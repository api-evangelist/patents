---
title: Round-trip engineering apparatus and methods for neural networks
abstract: Apparatus and methods for high-level neuromorphic network description (HLND) framework that may be configured to enable users to define neuromorphic network architectures using a unified and unambiguous representation that is both human-readable and machine-interpretable. The framework may be used to define nodes types, node-to-node connection types, instantiate node instances for different node types, and to generate instances of connection types between these nodes. To facilitate framework usage, the HLND format may provide the flexibility required by computational neuroscientists and, at the same time, provides a user-friendly interface for users with limited experience in modeling neurons. The HLND kernel may comprise an interface to Elementary Network Description (END) that is optimized for efficient representation of neuronal systems in hardware-independent manner and enables seamless translation of HLND model description into hardware instructions for execution by various processing modules.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09117176&OS=09117176&RS=09117176
owner: QUALCOMM TECHNOLOGIES INC.
number: 09117176
owner_city: San Diego
owner_country: US
publication_date: 20120315
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","COPYRIGHT","COMPUTER PROGRAM LISTING APPENDIX ON CD-ROM","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","DETAILED DESCRIPTION","Example 3","Example 4","Example 6","Example 8","Example 9","Example 9","Example 10","Example 11","Example 12","Example 13"],"p":["The present application is a continuation-in-part of U.S. patent application Ser. No. 13\/239,123, filed Sep. 21, 2011, entitled \u201cELEMENTARY NETWORK DESCRIPTION FOR NEUROMORPHIC SYSTEMS, which is expressly incorporated by reference herein.","This application is related to a co-owned U.S. patent application Ser. No. 13\/239,163 entitled \u201cELEMENTARY NETWORK DESCRIPTION FOR EFFICIENT IMPLEMENTATION OF EVENT-TRIGGERED PLASTICITY RULES IN NEUROMORPHIC SYSTEMS\u201d filed on Sep. 21, 2011, a co-owned U.S. patent application Ser. No. 13\/239,155 entitled \u201cELEMENTARY NETWORK DESCRIPTION FOR EFFICIENT MEMORY MANAGEMENT IN NEUROMORPHIC SYSTEMS\u201d filed on Sep. 21, 2011, and a co-owned U.S. patent application Ser. No. 13\/239,148 entitled \u201cELEMENTARY NETWORK DESCRIPTION FOR EFFICIENT LINK BETWEEN NEURONAL MODELS NEUROMORPHIC SYSTEMS\u201d filed on Sep. 21, 2011, each of the foregoing incorporated herein by reference in its entirety.","A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure, as it appears in the Patent and Trademark Office patent files or records, but otherwise reserves all copyright rights whatsoever.","The file of this patent includes duplicate copies of compact disc (CD-ROM) with forty six (46) read-only memory files in ASCII file format. The file details are presented in Table 1 below. These ASCII files contain lines of code which represent exemplary implementations of a Computer Program Listing for this disclosure. This CD-ROM and each of the files contained thereon and listed in Table 1 is incorporated herein by reference in its entirety.",{"@attributes":{"id":"p-0006","num":"0005"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"119pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}},{"entry":[{},"Creation","Creation","Size"]},{"entry":["Filename","Date","Time","(bytes)"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"119pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["synStructure_cpp.txt","6\/12\/2010","14:51","184"]},{"entry":["connStructurecopy_cpp.txt","7\/1\/2010","23:15","3,652"]},{"entry":["connStructure_cpp.txt","9\/21\/2010","17:13","3,694"]},{"entry":["randConnStructure_cpp.txt","9\/21\/2010","15:40","1,759"]},{"entry":["eContainer_cpp.txt","6\/4\/2010","\u20029:47","210"]},{"entry":["nStructure_cpp.txt","6\/24\/2010","17:06","226"]},{"entry":["noFactory_cpp.txt","6\/23\/2010","16:10","348"]},{"entry":["endO_cpp.txt","6\/10\/2010","\u20020:31","292"]},{"entry":["nO_cpp.txt","6\/30\/2010","12:43","237"]},{"entry":["endObjects_cpp.txt","9\/16\/2010","18:07","549"]},{"entry":["nContainer_cpp.txt","6\/9\/2010","22:36","352"]},{"entry":["main_cpp.txt","9\/28\/2010","11:57","3,274"]},{"entry":["simple_instances_two_units_xy_end.txt","6\/4\/2010","\u20029:47","53,710"]},{"entry":["simple_two_units_xy_end.txt","6\/4\/2010","\u20029:48","6,437"]},{"entry":["Hashdefs_h.txt","10\/8\/2010","10:39","222"]},{"entry":["gluSynapse_h.txt","9\/16\/2010","14:34","1,598"]},{"entry":["synStructure_h.txt","6\/22\/2010","11:42","1,152"]},{"entry":["gabaSynapse_h.txt","9\/16\/2010","14:34","1,619"]},{"entry":["inhNeuron_h.txt","7\/1\/2010","10:12","1,342"]},{"entry":["layer1exc_h.txt","9\/16\/2010","10:51","1,335"]},{"entry":["layer1to2exc_copy_h.txt","9\/21\/2010","17:03","3,321"]},{"entry":["excNeuron_h.txt","7\/1\/2010","10:12","1,340"]},{"entry":["layer2inh_h.txt","9\/16\/2010","10:52","1,333"]},{"entry":["layer2to2inh_copy_h.txt","9\/21\/2010","16:45","2,250"]},{"entry":["layer2to2inh_h.txt","9\/21\/2010","17:09","1,935"]},{"entry":["layerSynapse_h.txt","9\/28\/2010","12:00","3,194"]},{"entry":["eSynStruct_h.txt","6\/4\/2010","\u20029:47","2,548"]},{"entry":["endObjects_h.txt","9\/16\/2010","18:07","413"]},{"entry":["nContainer_h.txt","6\/23\/2010","20:04","1,593"]},{"entry":["newNetworkObject_h.txt","6\/25\/2010","\u20029:50","1,869"]},{"entry":["nGroup_h.txt","7\/1\/2010","10:12","1,825"]},{"entry":["endO_h.txt","9\/17\/2010","11:31","9,421"]},{"entry":["nO_h.txt","7\/1\/2010","10:08","4,626"]},{"entry":["eMCN_h.txt","6\/9\/2010","22:20","966"]},{"entry":["noFactory_h.txt","6\/23\/2010","16:10","941"]},{"entry":["SpNet_h.txt","9\/16\/2010","11:32","3,070"]},{"entry":["nSpace_h.txt","7\/1\/2010","10:09","686"]},{"entry":["eContainer_h.txt","6\/10\/2010","\u20020:06","1,912"]},{"entry":["nStructure_h.txt","7\/1\/2010","10:08","2,642"]},{"entry":["nSurface_h.txt","7\/1\/2010","10:09","694"]},{"entry":["connStructure_h.txt","9\/24\/2010","18:06","5,916"]},{"entry":["randConnStructure_h.txt","7\/12\/2010","\u20029:58","1,934"]},{"entry":["randSynStructure_h.txt","6\/22\/2010","18:26","3,121"]},{"entry":["cNO_h.txt","7\/1\/2010","10:09","681"]},{"entry":["layer1to2exc_h.txt","9\/21\/2010","17:05","2,953"]},{"entry":["testRand_h.txt","6\/30\/2010","22:53","1,049"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}]}}},"1. Field of the Disclosure","The present innovation relates to efficient design and implementation of artificial neural networks.","2. Description of Related Art","Most existing neuronal models and systems include networks of simple units, called neurons, which interact with each other via connections called synapses. The information processing in such neuronal systems may be carried out in parallel.","There are many specialized software tools that may help neuroscientists simulate models of neural systems. Examples of these tools may include high-level implementations such as one or more of NEURON, GENESIS, NEST, BRIAN, and\/or other high-level implementations may be designed primarily for use by neuroscientists. Such tools may typically require substantial specialized knowledge, may be cumbersome, and may require customization in order to achieve efficient performance during simulations that are executed using specific software and hardware engines, particularly when real-time performance is required, as in autonomous robotics applications.","Similarly, low-level implementations such as one or more of assembly languages, low-level virtual machine (LLVM) language, Java Bytecode, chip-specific instruction sets, and\/or other low-level implementations may be designed for efficient hardware implementations on x86, ARM\u2122, and\/or other silicon chips. However, such implementations may be unsuitable for parallel simulations of neuronal systems, mostly because the silicon chips are not designed for such parallel neuronal simulations.","Overall, existing approaches have substantial shortcomings as they do not provide sufficient flexibility in designing neural networks, require expert knowledge, and\/or platform specific customization in order to take advantage of specialized hardware.","Accordingly, there is a salient need for a universal high level network description for defining network architectures in a simple and unambiguous way that is both human-readable and machine-interpretable.","The present disclosure satisfies the foregoing needs by providing, inter alia, apparatus and methods for high level network description for neuromorphic systems.","One aspect of the disclosure relates to one or more systems and\/or computer-implemented methods for effectuating a neural network using an instruction set. In one implementation, the method may comprise providing a representation of the neural network. The representation may comprise a plurality of instructions of the instruction set. The representation may be compiled into to machine executable format. The instruction set may comprise a structured language consistent with English language structure and grammar.","In some implementations, the instruction set may comprise a first instruction adapted to cause generation of at least one node within the network. The first instruction may comprise a keyword including one or more of CREATE, MAKE, PUT, and\/or other keywords.","In some implementations, the at least one node may be characterized by a spatial coordinate. The first instruction may comprise a structured English language statement configured to assign a desired location within the network to the spatial coordinate. The assignment may be performed as a part of the generation of the at least one node. The first instruction may comprise a keyword including one or more of ON, IN, AT, and\/or other keywords.","In some implementations, the generation of at least one node may comprise generation of a first node and a second node. The instruction set may comprise a second instruction configured to cause a connection between the first node and the second node.","In some implementations, the connection may comprise a synapse, a junction, and\/or another connection.","In some implementations, the generation of at least one node may comprise generation of a plurality of nodes. The plurality may comprise the first node and the second node. The second instruction may comprise a Boolean expression configured to select two subsets within the plurality of nodes and\/or to cause a plurality of connections between nodes of one of the two subsets and nodes of another of the two subsets. The second instruction may comprise a keyword including one or more of CONNECT, LINK, PROJECT, and\/or other keywords. The Boolean expression may comprise a keyword including one or more of AND, NOT, OR, and\/or other keywords.","Some implementations may include a method of programming a computerized apparatus. In one implementation, the method may comprise representing a neural network using a plurality of instructions of an instruction set. The representation may be compiled into a machine representation for execution by the computerized apparatus. The instruction set may comprises a structured language consistent with English language structure and grammar.","In some implementations, the network may comprise a plurality of elements. Individual ones of the plurality of elements may have a tag associated therewith. The instruction set may comprise a first instruction configured to identify a subset of the plurality of elements based on the tag.","In some implementations, compiling of the instruction set may be effected by a database configured to store a plurality of tags. The plurality of tags may comprise the tag.","In some implementations, the instruction set may comprise a second instruction configured to effect assignment of a new tag to the subset. The second instruction may comprise a keyword including one or more of TAG, ASSIGN, MARK, and\/or other keywords.","In some implementations, the instruction set may comprise a Boolean expression consistent with a structured English representation. The structured English representation may enable machine execution of the Boolean expression by an implicit assignment of a logical AND operation between two Boolean variables of the Boolean expression. The Boolean variables may be separated by a separator keyword.","Some implementations may include a method of operating a neuromorphic computerized apparatus. In one implementation, the neuromorphic computerized apparatus may comprise a nonvolatile storage medium storing one or more of an instruction set, compiling kernel, processing module, and\/or other information. The method may comprise providing a representation of a neural network to the processing module. The representation may comprise a plurality of instructions included in the instruction set. The representation may be encoded into hardware independent format by the processing module using the kernel. The instruction set may comprise a structured language consistent with English language structure and grammar.","In some implementations, the hardware independent format may be configured to enable conversion of the representation into a plurality of machine executable instructions. The plurality of machine executable instructions may effect operation of the neural network.","In some implementations, the hardware independent format may comprise Elementary Network Description (END) format. The machine executable instructions may include one or more of central processing unit (CPU) instructions, graphics processing unit (GPU) instructions, field programmable gate array (FPGA) instructions, and\/or other machine executable instructions.","In some implementations, the machine format may comprise a plurality of functions configured for execution by the processing module.","In some implementations, the computerized apparatus may comprise a speech input apparatus. Providing the representation may be effected by a user of the computerized apparatus. The user may speak individual ones of the plurality of instructions such that a given spoken instruction is received by the speech input apparatus.","In some implementations, the speech input apparatus may be operably coupled to the kernel. The kernel may comprise a speech-recognition module configured to translate digitized user speech into one or more instructions of the instruction set.","In some implementations, the instruction set may comprise a first instruction configured to effect generation of at least one node within the network. The at least one node may be characterized by a spatial coordinate. The first instruction may comprise a structured English language statement configured to assign a desired location within the network to the spatial coordinate. The assignment may be effected substantially during creation of the at least one node.","In some implementations, the generation of the at least one node may comprise generation of a first node and a second node. The instruction set may comprise a second instruction configured to effect a connection between the first node and the second node.","Some implementations may include neuronal network logic. In one implementation, the neuronal network logic may comprise a series of computer program steps or instructions executed on a digital processor. The logic may comprise hardware logic. Hardware logic may be embodied in one or more of an ASIC, FPGA, and\/or other integrated circuit.","Some implementations may include a computer readable storage medium having at least one computer program stored thereon. The program may be executable to implement an artificial neuronal network.","Some implementations may include a system comprising one or more of an artificial neuronal (e.g., spiking) network having a plurality of nodes associated therewith, a controlled apparatus (e.g., robotic or prosthetic apparatus), and\/or other components.","These and other objects, features, and characteristics of the present disclosure, as well as the methods of operation and functions of the related elements of structure and the combination of parts and economies of manufacture, will become more apparent upon consideration of the following description and the appended claims with reference to the accompanying drawings, all of which form a part of this specification, wherein like reference numerals designate corresponding parts in the various figures. It is to be expressly understood, however, that the drawings are for the purpose of illustration and description only and are not intended as a definition of the limits of the disclosure. As used in the specification and in the claims, the singular form of \u201ca\u201d, \u201can\u201d, and \u201cthe\u201d include plural referents unless the context clearly dictates otherwise.","All Figures disclosed herein are \u00a9Copyright 2012 Brain Corporation. All rights reserved.","Implementations of the present disclosure will now be described in detail with reference to the drawings, which are provided as illustrative examples so as to enable those skilled in the art to practice the disclosure. Notably, the figures and examples below are not meant to limit the scope of the present disclosure to a single implementation, but other implementations are possible by way of interchange of or combination with some or all of the described or illustrated elements. Wherever convenient, the same reference numbers will be used throughout the drawings to refer to same or similar parts.","Where certain elements of these implementations can be partially or fully implemented using known components, only those portions of such known components that are necessary for an understanding of the present disclosure will be described, and detailed descriptions of other portions of such known components will be omitted so as not to obscure the disclosure.","In the present specification, an implementation showing a singular component should not be considered limiting; rather, the disclosure is intended to encompass other implementations including a plurality of the same component, and vice-versa, unless explicitly stated otherwise herein.","Further, the present disclosure encompasses present and future known equivalents to the components referred to herein by way of illustration.","As used herein, the term \u201cbus\u201d is meant generally to denote all types of interconnection or communication architecture that is used to access the synaptic and neuron memory. The \u201cbus\u201d may be optical, wireless, infrared, and\/or another type of communication medium. The exact topology of the bus could be for example standard \u201cbus\u201d, hierarchical bus, network-on-chip, address-event-representation (AER) connection, and\/or other type of communication topology used for accessing, e.g., different memories in pulse-based system.","As used herein, the terms \u201ccomputer\u201d, \u201ccomputing device\u201d, and \u201ccomputerized device\u201d may include one or more of personal computers (PCs) and\/or minicomputers (e.g., desktop, laptop, and\/or other PCs), mainframe computers, workstations, servers, personal digital assistants (PDAs), handheld computers, embedded computers, programmable logic devices, personal communicators, tablet computers, portable navigation aids, J2ME equipped devices, cellular telephones, smart phones, personal integrated communication and\/or entertainment devices, and\/or any other device capable of executing a set of instructions and processing an incoming data signal.","As used herein, the term \u201ccomputer program\u201d or \u201csoftware\u201d may include any sequence of human and\/or machine cognizable steps which perform a function. Such program may be rendered in a programming language and\/or environment including one or more of C\/C++, C#, Fortran, COBOL, MATLAB\u2122, PASCAL, Python, assembly language, markup languages (e.g., HTML, SGML, XML, VoXML), object-oriented environments (e.g., Common Object Request Broker Architecture (CORBA)), Java\u2122 (e.g., J2ME, Java Beans), Binary Runtime Environment (e.g., BREW), and\/or other programming languages and\/or environments.","As used herein, the terms \u201cconnection\u201d, \u201clink\u201d, \u201ctransmission channel\u201d, \u201cdelay line\u201d, \u201cwireless\u201d may include a causal link between any two or more entities (whether physical or logical\/virtual), which may enable information exchange between the entities.","As used herein, the term \u201cmemory\u201d may include an integrated circuit and\/or other storage device adapted for storing digital data. By way of non-limiting example, memory may include one or more of ROM, PROM, EEPROM, DRAM, Mobile DRAM, SDRAM, DDR\/2 SDRAM, EDO\/FPMS, RLDRAM, SRAM, \u201cflash\u201d memory (e.g., NAND\/NOR), memristor memory, PSRAM, and\/or other types of memory.","As used herein, the terms \u201cmicroprocessor\u201d and \u201cdigital processor\u201d are meant generally to include digital processing devices. By way of non-limiting example, digital processing devices may include one or more of digital signal processors (DSPs), reduced instruction set computers (RISC), general-purpose (CISC) processors, microprocessors, gate arrays (e.g., field programmable gate arrays (FPGAs)), PLDs, reconfigurable computer fabrics (RCFs), array processors, secure microprocessors, application-specific integrated circuits (ASICs), and\/or other digital processing devices. Such digital processors may be contained on a single unitary IC die, or distributed across multiple components.","As used herein, the term \u201cnetwork interface\u201d refers to any signal, data, and\/or software interface with a component, network, and\/or process. By way of non-limiting example, a network interface may include one or more of FireWire (e.g., FW400, FW800, etc.), USB (e.g., USB2), Ethernet (e.g., 10\/100, 10\/100\/1000 (Gigabit Ethernet), 10-Gig-E, etc.), MoCA, Coaxsys (e.g., TVnet\u2122), radio frequency tuner (e.g., in-band or OOB, cable modem, etc.), Wi-Fi (802.11), WiMAX (802.16), PAN (e.g., 802.15), cellular (e.g., 3G, LTE\/LTE-A\/TD-LTE, GSM, etc.), IrDA families, and\/or other network interfaces.","As used herein, the term \u201csynaptic channel\u201d, \u201cconnection\u201d, \u201clink\u201d, \u201ctransmission channel\u201d, \u201cdelay line\u201d, and \u201ccommunications channel\u201d include a link between any two or more entities (whether physical (wired or wireless), or logical\/virtual) which enables information exchange between the entities, and may be characterized by a one or more variables affecting the information exchange.","As used herein, the term \u201cWi-Fi\u201d includes one or more of IEEE-Std. 802.11, variants of IEEE-Std. 802.11, standards related to IEEE-Std. 802.11 (e.g., 802.11 a\/b\/g\/n\/s\/v), and\/or other wireless standards.","As used herein, the term \u201cwireless\u201d means any wireless signal, data, communication, and\/or other wireless interface. By way of non-limiting example, a wireless interface may include one or more of Wi-Fi, Bluetooth, 3G (3GPP\/3GPP2), HSDPA\/HSUPA, TDMA, CDMA (e.g., IS-95A, WCDMA, etc.), FHSS, DSSS, GSM, PAN\/802.15, WiMAX (802.16), 802.20, narrowband\/FDMA, OFDM, PCS\/DCS, LTE\/LTE-A\/TD-LTE, analog cellular, CDPD, satellite systems, millimeter wave or microwave systems, acoustic, infrared (i.e., IrDA), and\/or other wireless interfaces.","Overview","The present disclosure provides, among other things, a computerized high level network description apparatus and methods that may be configured to define neuronal network architectures in a simple and unambiguous way.","In some implementations, a computerized apparatus may be configured to implement a High Level Network Description (HLND) kernel. The HLND kernel may enable users to define neuromorphic network architectures using a unified and unambiguous representation that is both human-readable and machine-interpretable.","In some implementations, the HLND format may be used to define nodes types, node-to-node connection types, instantiate node instances for different node types, dynamically identify and\/or select subsets of the network using tags, generate instances of connection between these nodes using such subsets, and\/or other information associated with nodes and\/or tags.","The HLND format may provide some or all of the flexibility required by computational neuroscientists and may provide a user-friendly interface for users with limited experience in modeling neurons.","In some implementations, the HLND kernel may comprise an interface to Elementary Network Description (END). The END engine may be configured for efficient representation of neuronal systems in hardware-independent manner and\/or may enable seamless translation of HLND model description into hardware instructions for execution by various processing modules.","In some implementations, the HLND framework may comprise a graphical user interface (GUI) configured to enable users to, inter alia, create nodes, select node subsets, connect selected subsets using graphical actions via the GUI, and\/or perform other operations consistent with the disclosure. The GUI engine may be configured to generate HLND statements, which may correspond to the above user actions, without further input from the user. The HLND framework may be configured to convert HLND statements into a graphical representation of the network that is presented via the GUI. The HLND may include one or more components including (i) the network graphical depiction using the GUI, (ii) the corresponding list of HLND statements, and\/or other components. One or more components of the HLND may be configured to consistently represent the same information about the network, as changes in one representation may be consistently applied to the other representation thereby reflecting some or all of the modifications to the network.","In some implementations, the HLND may be applicable to arbitrary graph structures (e.g., neural network with arbitrarily complex architecture).","Detailed descriptions of the various implementations of the apparatuses and methods of the disclosure are now provided. Although certain aspects of the disclosure can best be understood in the context of the High Level Network Description format used for designing neural network architecture, the disclosure is not so limited and implementations of the disclosure may be used for implementing an instruction set that is optimized for efficient representation of other systems (e.g., biological or financial) in a hardware-independent manner.","Implementations of the disclosure may be, for example, deployed in a hardware and\/or software implementation of a neuromorphic computer system. In some implementations, a robotic system may include a processor embodied in an application specific integrated circuit, which may be adapted or configured for use in an embedded application (such as a prosthetic device).",{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIG. 1","FIG. 1","FIG. 1","FIG. 1","FIG. 1","FIG. 1","FIG. 1"],"b":["100","102","104","106","108","110","104","1","100","102","1","102","2","102","1","102","2"]},"HLND Framework Design Overview","According to one or more implementations, the exemplary HLND framework may be configured to facilitate design of neural networks, such as network  of . Some implementations may provide an ability to describe neuronal network of an arbitrary complexity. Some implementations may facilitate use of pre-defined node and\/or pre-defined connection types for the network generation process. That is, multiple instances of different node types may be generated, laid-out, and\/or connected using multiple instances of different connection types. Some implementations may provide a flexible definition of new node types such that a new node type may comprise an implementation of an elementary network description (END) unit class and\/or an implementation of a Network Object (e.g., a layout of nodes, a set of connectivity, and\/or a combination of these). The newly defined node types may be used in the network generation process. The END framework is described in U.S. patent application Ser. No. 13\/239,123 entitled \u201cELEMENTARY NETWORK DESCRIPTION FOR NEUROMORPHIC SYSTEMS\u201d, incorporated supra. Some implementations may provide a flexible definition of connection types. A connection type may include an implementation of an END Junction Class, an END Synapse Class, and\/or other classes. In one implementation, the newly defined connection types may be used in the network generation process. Some implementations may facilitate use of universal tags (or labels) for some or all of the network elements including nodes, connections, collections of nodes, and\/or other network elements. In some implementations, tags may be used to identify groups of nodes and\/or connections. Tags may be used to dynamically select parts of the network. One or more Boolean operations, such as AND, OR, NOT, and\/or other Boolean operations may be applied to tags. Some implementations may provide an ability to implement a HLND network using a graphical user interface (GUI). Individual description constructs may correspond to a user action in the GUI. In some implementations, models of moderate complexity may be built using a HLND GUI interface without requiring the use of a keyboard. In some implementations, the HLND GUI may be operated using a touch screen, a light pen input device, and\/or other input technique. Some implementations may facilitate presentation of HLND statements for defining network anatomy. Defining network anatomy may include laying out nodes and\/or connections in a user-readable (natural English) language to facilitate easy understanding for non-computer expert network designers. Some implementations may provide an ability to use HLND to generate END instances.","Network Definition Methods","Defining a neural network may comprise defining how many and\/or what type of nodes to create, how to lay out these nodes, how to connect these node instances (e.g., the network layout of ), and\/or other operations. In some implementations, the HLND definition method comprises (1) defining new node types and\/or connection types for the new node types, (2) defining a node layout within the network (e.g., how many and\/or what type of nodes to create, and how to arrange these nodes within the network that is being created), (3) defining how the nodes connect to one another, and\/or other operations. During neural network construction, the above steps 1-3 may be individually and\/or sequentially repeated multiple times. In some implementations, the above step 1 may be skipped and pre-defined classes (defining the desired node type) may be used instead in defining the network.","In some implementations, a dedicated software package may be configured to (i) process the HLND statements that define the network and\/or (ii) instantiate the network nodes and connections. This processing and\/or instantiation may be subject to one or more constraints including (i) only the defined node types and the defined connection types can be instantiated and used in the HLND network definition process, (ii) only the connections between the existing node instances can be instantiated, and\/or other constraints. In other words, only connections corresponding to the defined node instances may be used in the HLND process of defining the connections, in accordance with one or more implementations. In some implementations, the dedicated software package may comprise the END engine, which may be configured to generate the END instance of network models, as described in a co-owned U.S. patent application Ser. No. 13\/239,123 entitled \u201cELEMENTARY NETWORK DESCRIPTION FOR NEUROMORPHIC SYSTEMS\u201d, incorporated supra.","Defining Node Types","The definition of a node type may provide the implementation instructions of the node, which may be configured to instruct a network processing apparatus to perform specific steps during instantiation of the node in accordance with the node type. In some implementations, the node definition may further specify internal implementation of the node type (for example, specify the dynamics of the neuron type). In one or more implementations, the node definition may comprise definitions of input ports and\/or output ports for the node","In some implementations, the node type may be defined as a Simple Node, where the node definition specifies the \u201cinternals\u201d of that node. Internals of a node may include the implementation of an END unit (i.e., a neuron) and\/or an END implementation of a neuronal compartment.","In some implementations, the node type may be defined as a complex Network Object, which may provide instructions on how to instantiate pre-defined node types, instructions on how to connect the nodes, and\/or other instructions. Instructions on how to connect nodes may include a HLND description of a network of an arbitrary complexity, an algorithm configured to specify details of node and\/or connection instance generation, and\/or other instructions. It will be appreciated by those skilled in the arts that the term Network Object may be used to describe any network that is implementable using the HLND framework.","Within the HLND framework description, individual node types may comprise intra-node connections and may define interface (or interfaces) for incoming connections and\/or outgoing connections.","Node Type as an END Unit","In some implementations, the END Unit class may be generated using node implementation details (e.g., update rule, event rule). See, e.g., U.S. patent application Ser. No. 13,239,123 for additional detail related to the END Unit classes.","Node Type as a Network Object","In some implementations, the definition of a network object may be configured similarly to the definition of a network, with the main difference being that the network object is reusable. That is, multiple instances of the network object may be instantiated. Some or all elements of the network object (e.g., units, tags, and\/or other elements) may be scoped\u2014that is, they may have a finite lifetime associated with a particular scope. In some implementations, network objects may be configured to provide an I\/O interface that may be used to connect the network object with other nodes. A network object may be similar to a building block in a Simulink\u00ae model (see, e.g., http:\/\/www.mathworks.com\/products\/simulink\/index.html), a p-cell in a computer aided design (CAD) software, a function\/class in C++ language code, and\/or other programming elements.","In some implementations, a network object may be allowed to use pre-defined nodes that are defined externally (i.e., outside the scope of this node type). Thus, the parent node (i.e., the network object) and the child node(s) (i.e., the node types used in the network object) may not comprise nodes of the same type. In other words, the definition of an \u2018X\u2019-type node may not instantiate \u2018X\u2019 type nodes, according to some implementations.","Referring now to , one exemplary implementation of a network object is illustrated and described in detail. The network object  may comprise one or more of a network definition, specification of the object input\/output (I\/O) interface, and\/or other information.","The network definition may specify one or more steps of the object instance generation. The network definition may be implemented via a standard HLND network definition. A standard HLND network definition may comprise an instantiation and\/or layout of nodes that specifies the number of instances of each pre-defined node and\/or their spatial arrangement using pre-defined distribution functions. A standard HLND network definition may comprise a connectivity description, which may define connectivity between nodes. In some implementations, the connectivity description may define and\/or use spatial projection for the node. By way of non-limiting example, defining spatial projection for a node may include defining axonal and\/or dendritic projections for the nodes (e.g., (i) dendritic spread, (ii) distribution of synaptic boutons, (iii) distribution of axon terminals), defining how the axon of a node (e.g., model neuron) connects to the dendrite of another node (e.g., model neurons), and\/or defining other information associated with spatial projection for a node.","By way of non-limiting example, the standard HLND network definition may be used in (i) defining a specific layout of pre-defined nodes; (ii) defining a multi-compartmental neuron (e.g., a collection of pre-defined END units connected with pre-defined END junctions); (iii) defining an arbitrarily complex network comprising a plurality of neurons; and\/or defining other information associated with the standard HLND network definition. In some implementations, a network may comprise synapses and\/or junctions.","In some implementations, the definition of the network may comprise instance generation with an algorithm, which may be configured to describe one or more steps of Network Object instance generation, using the above Object definition. By way of non-limiting example, the algorithm may include one or more of (i) an algorithm that uses pre-defined node types and\/or defines the instance generation process of such node types; (ii) an arbitrary algorithm that uses pre-defined node types and\/or connection types and\/or define the instance generation process of such node and\/or connection types; (iii) an algorithm that defines a dendritic tree; and\/or other algorithms.","The above exemplary algorithms may utilize multiple pre-defined END unit types implementing neural compartments and\/or pre-defined END junction types designed to connect such compartments with the algorithm defining the layout of compartments and the connection between them. (See, e.g., Cuntz H., Forstner, F., Borst. A, and H\u00e4usser, M. (2010), \u201cOne Rule to Grow Them All: A General Theory of Neuronal Branching and Its Practical Application. 6(8), incorporated herein by reference in its entirety). The I\/O interface may specify an input\/output connection implemented for the network object.","Definition of Connection Types","Within HLND, definition of a connection type may provide the necessary implementation details (e.g., pre-event rules, post-event rules, update rules, and\/or other details) to generate a connection that comprises one or both of (i) an END synapse or (ii) an END junction, in accordance with one or more implementations.","Instantiating Nodes","In some implementations, the HLND may define rules that govern node instantiation. The HLND node instantiation instructions may be subsequently provided to a software package (e.g., the END kernel) that interprets the instructions and instantiates the appropriate nodes. According to some implementations, during instantiation, some or all node types may be treated equally, regardless of whether they are simple nodes (e.g., the END implementation of a neuron) or network objects (e.g., the entire network description). In some implementations, the following information may be needed to instantiate and lay out nodes: (i) node type to be instantiated, (ii) number of instances of the node type to be instantiated, and\/or other information.","In a basic form, an HLND instantiation statement may create n instances of a given node type using default definitions corresponding to the node type. During instantiation, additional parameters may be used to, inter alia, (i) set parameters used to initialize the instantiated node types, (ii) set how the instantiated nodes are laid out in space (e.g., how position tags are assigned), (iii) add additional tags to the new node instances, and\/or perform other operations associated with HLND instantiation. Within HLND, a defined node type available within the scope of operation may be instantiated without restriction.","In some implementations, position coordinates (i.e., spatial tags) may be assigned to the generated node instances during node instantiation. In order to implement this functionality, the HLND framework may support usage of predefined distribution functions that assign spatial tags to each instantiated node. Such a distribution function may be configured to sample n points from a given probability density function. By way of non-limiting example, the HLND statement:\n\n","In addition to the unique id tags that individual generated nodes may have, extra tags may be optionally assigned during the instantiation process and may be used to identify the set of newly instantiated nodes. In some implementations, special reserved tags (e.g., \u201cIN\u201d, \u201cOUT\u201d, or other special reserved tags) may be used to specify that the generated units are input or output interfaces for the network, therefore making these nodes accessible (readable and\/or writable) from outside. Exemplary HLND calls are shown in the Listing 1 below:",{"@attributes":{"id":"p-0105","num":"0106"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 1."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["a)","create 100 of the \u2018exc\u2019 node types (assuming that the \u2018exc\u2019 END"]},{"entry":[{},"class\/network object exists)"]},{"entry":[{},"\u2003\u2003example1_exc_neurons = (100, \u2018exc\u2019)"]},{"entry":["b)","create 200 of the \u2018exc\u2019 node types and distribute them using the"]},{"entry":[{},"given pdf:"]},{"entry":[{},"\u2003\u2003example2_exc_neurons = (200, \u2018exc\u2019, _exc_parameters_,"]},{"entry":[{},"\u2003\u2003pdf)"]},{"entry":["c)","create 1 node of a network object type that implements the retina"]},{"entry":[{},"(assuming that the \u2018retina\u2019 network object type has been"]},{"entry":[{},"implemented previously):"]},{"entry":[{},"\u2003\u2003example3_retina = (1, \u2018retina\u2019, _retina_parameters_)"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}},"br":{}},"In some implementations, morphology\/extensions may be used during connection instantiation. The morphology definition described with respect to  supra, specifies how an instantiated node may project and\/or extend into network space. That is, the morphology may define spatial extent where an instantiated node is allowed (i) to \u201creceive\u201d incoming connections from, and\/or (ii) to \u201csend\u201d outgoing connections to. Note that, according to some implementations, an addition of a node extension may not alter the size and\/or the location of the node. Instead, extensions may enable a node to \u201csearch\u201d for other nodes during instantiation of node interconnections. In other words, an extension may provide an additional \u201cview\u201d of a node, which may be used during the process of connecting nodes.","In some implementations, connecting nodes using spatial tags may only be allowed if the nodes are overlapping. Nodes may have a zero extension by default. In some implementations, only co-located nodes may be connected. In order to extend node connectivity, non-zero node input (dendrite) and node output (axon) projections may be defined. In some implementations, these projections may be used when connecting any two nodes. For example, the output projection of one node may be connected to the input projection of another node.","To create an extension, some or all of the following information may be required, in accordance with one or more implementations: (1) source tags used to identify the nodes for which the extension is created; (2) extension tags that are used to identify the extensions to be created; and\/or (3) define I\/O point distribution to define the space\/extension where the node can receive incoming connections, and define the space where the nodes can have outgoing connections.","For incoming extensions, the distribution of receiving terminals may be specified. The distribution of receiving terminals may be analogous to dendritic spread, and\/or distribution of synaptic boutons in case of a neuron. For an outgoing extension, the projection spread may be specified. The projection spread may be analogous to axon terminals.","For the distribution of receiving terminals and spread of projections, HLND may support predefined functions (e.g., bounded Gaussian and\/or uniform distribution). In general, arbitrary density functions may be used.","Exemplary HLND calls are shown in the Listing 2 below:",{"@attributes":{"id":"p-0112","num":"0113"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 2."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"example1_exc_neurons_axon = (\u2018example1_exc_neurons\u2019, pdf1)"},{"entry":"example2_exc_neurons_dendrite = (\u2018example2_exc_neurons\u2019, pdf2)"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}},"br":[{},{}]},"The HLND connection statement may comprise instructions configured to instantiate connections (of a given connection type) from one set of nodes to another set of nodes. In some implementations, some or all of the following information may be required in order to enable these node-to-node connections: (1) \u201cfrom subset\u201d, (2) \u201cto subset\u201d, and\/or (3) \u201cconnection type\u201d. A from subset may include a node subset selected by tags that uniquely identify source nodes\/extensions (e.g., nodes\/extensions that a connection will originate from). A to subset may include a node subset selected by tags that uniquely identify destination nodes\/extensions (e.g., nodes\/extensions that a connection will terminate at). A connection type may include the connection type used to connect <From Subset> nodes\/extensions to <To Subset> nodes\/extensions.","In some implementations, the HLND connection statement may direct connection instantiation with a given connection type from all available <From Tags> nodes to all available <To Tags> nodes. According to some implementations, the connection parameters may be used to filter out connections. That is, filter constrains may be applied to the some or all possible <From Tags> to <To Tags> connections. Therefore, a subset of possible <From Tags> to <To Tags> connections may be instantiated, which may allow instantiation of arbitrary connection maps from <From Tags> to <To Tags>. In some implementations, connections may be expressed as function calls. In some implementations, connections may be expressed using a table. In some implementations, a connection filter may be configured to generate all-to-all connections, where all of the <From Tags> are connected to all of the <To Tags>.","As an aside, two notation formats <From Tags> and <From Subset> may be used in various implementations, as both notations may cause the HLND to generate connections for a subset. For example, notation <\u2018FromTag\u2019 AND \u2018FromTag\u2019> may describe a collection of nodes (e.g., Collection) that have both tags \u2018FromTag\u2019 and \u2018FromTag\u2019. Accordingly, notation: <\u2018Collection\u2019> may be used instead while producing the same results.","By way of non-limiting example, the following connection statements may be employed:",{"@attributes":{"id":"p-0117","num":"0118"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u2003\u2003exc2exc = (pre=\u2019example1_exc_neurons_axon\u2019,"]},{"entry":[{},"post=\u2019example2_exc_neurons_dendrite\u2019, 100 connections \/ cell,"]},{"entry":[{},"SynType=\u2019GLU\u2019, _other_parameters_)."]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"In some implementations, the HLND connections statement may be configured to enable parameterized connection establishment, such that parameters may be passed to the connection type to set the connection variables in the connection instances. In some implementations, the connection parameters may be used to set the weights of the synaptic node connection. In some implementations, the node information (e.g., the position of the from-node and the to-node) may be used to set up connection weights based on the distance between the nodes.","By way of non-limiting example, one or more of the following connection statements may be employed: (1) connect each <From Nodes> node to N <To Nodes> node; (2) connect to each <To Nodes> node N <From Nodes> node; and\/or (3) randomly sample N from all possible connections.","In some implementations, nodes may comprise position tags and\/or may have zero default extension. Such nodes may connect to co-located nodes. Connecting nodes with spatial tags may require overlap so that overlapping nodes may be connected.","Referring now to , an exemplary implementation of an HLND framework node interconnection is shown and described in detail. The network  of  may comprise a group of A nodes  and a group of B nodes . For clarity, the network  may be configured using a one-dimensional configuration, so that nodes with matching X-coordinates  (i.e., the node index i=1:7 in ) are allowed to be connected via the connections . Specifically, a node a_i from the node group  may be allowed to be connected to the respective node to b_j of the node group  when i=j, such as, for example the nodes , , respectively, in . Several possible connections are illustrated by solid lines .","In some implementations, such as illustrated in , node extensions may be added to the nodes of the node groups, respectively, in order to implement complex connections and to enable more flexible connection mapping. The node extensions may be used to, inter alia, map spatial coordinates of the source nodes (e.g., the nodes A of the group  in ) to the receiving nodes (e.g., the node  in ). The extensions may be used to define the probability density function of potential connections, such as in the exemplary implementation illustrated in .",{"@attributes":{"id":"p-0123","num":"0124"},"figref":["FIG. 3B","FIG. 3B","FIG. 3B","FIG. 3B","FIG. 3B"],"b":["320","322","1","7","324","320","306","324","330","332","322","322","1","326","328","328","332","330","332","330"]},"The network configuration illustrated in  may allow connections between the node a_i of the A group  to the b_ node  when the axons overlap with the spatial dimension  of the dendrite . As illustrated in , the nodes a_, a_, a_ may be connected to the node b_ via the connections, depicted by solid arrows  in . Inactive (e.g., not allowed) connections  between other A nodes are depicted by dashed arrows in . The tags <\u2018axon\u2019> and <\u2018dendrite\u2019> may refer to another \u2018view\u2019 of the node.","As illustrated in , the following extension may be constructed: (i) a uniform circle extension (denoted as the Dendrite) to the B node  in ; (ii) a uniform circle extension (denoted as the Axon) to all A nodes in ; and\/or (iii) Connect <A Axon> to <B Dendrite>. The A to B node connection may be possible and may be instantiated because both the sending and receiving extensions may be uniform. In this example, the connect statement is looking for extension overlaps\u2014that is, whether the <Axon> extension of a node <A> overlaps with the <Dendrite> extension of a node <B>.","Within the HLND framework, nodes may comprise different views, such as, for example, Axons or Dendrites. Node tags \u2018axon\u2019 or \u2018dendrite\u2019 may be used in HLND to refer to the same node. The Axon\/Dendrite may have different spatial properties.",{"@attributes":{"id":"p-0127","num":"0128"},"figref":["FIG. 3C","FIG. 3C","FIG. 3C"],"b":["338","322","344","326","344","340","340","342","342"]},"The connectivity profile of the non-uniform extension  (see ) may be configured using a shape of the Gaussian distribution and\/or other distributions. The extension  may be centered at the node . The extension  may be characterized by a certain variance \u03c3and a radius .","Node connections via non-uniform extension are illustrated in . The network configuration illustrated in  may allow connections between the node a_i of the A group  to the b node  when the axons overlap with the spatial dimension  of the non-uniform dendrite . Possible connections in the network  include a_, a_, . . . , a_ axon to b dendrite, as a_ axon, a_ axon, and a_ axon do not overlap with the dendrite  spatial dimension .","The non-uniform extensions (e.g., the extension  in ) may bias the selection of connections towards axons that overlap with the highest probability area of the non-uniform dendrite. While the extension dimension overlap may be used to identify all of the possible connections, the sampling of the possible connections may follow the connectivity profile (probability) of the extensions. When selecting a subset of the possible connections the connectivity profile (shape) of the extension (that describes connection likelihood) may need to be taken into account. By way of non-limiting example, when connecting a single arbitrary <A axon>  to the <b dendrite>  (see ), the most likely outcome may be a connection between the node a_ and the node , rather than connections between nodes a_, a_ to the node .","In some implementations, the HLND exemplary operation sequence for connecting node populations using non-uniform extension may be: (1) add Gaussian extensions centered at the node with a fixed radius r to B nodes and tag these extensions as <Dendrite>; (2) add uniform extensions with a fixed radius rto A-nodes and tag these extension as <Axon>; and (3) connect N random <A Axon>s to <B Dendrite>s. The possible connections may be where the <Axon> extensions of <A> nodes overlap with the <Dendrite> extensions of <B> nodes. Instantiated connections may correspond to the highest connectivity. In some implementations, the highest connectivity may be determined based on a product of the Gaussian and the uniform functions.","I\/O for Network Objects","In some implementations of the HLND framework, network objects may comprise one or more members and an Input\/output (I\/O) interface. The I\/O interface may specify how to interface (e.g., establish connections) with other elements of the network. In some implementations, the members of the network object may comprise nodes. In some implementations, the members of the network object may comprise nodes and connections. The I\/O interface may define how the object members are accessible from outside the scope of the network object. During definition, individual members of the network object (and their values) may be declared as public or private. Private members may not be visible (i.e., directly not accessible) to external network elements outside the network object. The private object members may be accessible via the I\/O interface defined for that member. That is, private members of a network object may not be visible from outside the scope of the network object. In this case, the I\/O interface may be required to implement connections.","In some implementations, the network objects may be defined as \u2018open\u2019. Members of a network object defined as open may be public and visible from outside the scope of that network object. This may alleviate a requirement to publish the I\/O interface.","Public members of the network object may be visible and\/or accessible by external elements for input and\/or output connections. In some implementations, members of the network objects may be scoped by default. That is, some or all variables within the network object may be scoped inside the network object. Members of multiple instances of the same network object type may not interfere when these members use the same tags.","In some implementations, a network object may be defined as a \u2018macro\u2019. A network object defined as a macro may not be treated as scoped object. Such macro definition may allow some or all of the variables defined within the macro object to be accessible and\/or visible by external elements.","By way of non-limiting example, node_a may be a public member of network object NO, node_b (that is not a member of NO) may connect directly to the node_a, and\/or receive connections from the node_b. A member node_a in NO may be accessed by using the tags <NO> and <node_a>, and\/or with NO.node_a scoped notation. A node_c that is a private member of network object NO may not be visible and\/or accessible from outside unless the I\/O interface is defined for the node_c member of the NO, according to some implementations. An external node (that is not member of the NO) may not connect to and\/or receive a connection from the node_a member directly. In other words, a public member of a network object may be accessed using tags and\/or other publicly available information.","An exemplary implementation of the HLND network object, illustrating a public multi-compartment neuron (MCN), is shown and described in connection with . The MCN neuron  may comprise one or more public nodes, which may include dendritic compartments  and soma compartment . The term public may refer to members of the network object (e.g., the compartments , ) that are visible outside the scope of their definitions (e.g., outside the MCN ). Individual compartments  may be assigned two tags, which may include DENDRITE, COMP, and\/or other tags. Compartment  may be assigned three tags, which may include DENDRITE, COMP, SOMA, and\/or other tags. The compartment  and individual compartments  may be connected via junction connections .","By way of non-limiting example to illustrate functionality of public network elements in accordance with some implementations, two instances of public MCN neuron  may be considered. One instance may be tagged as the \u2018neuron_a\u2019, and another instance may be tagged as the \u2018neuron_b\u2019. The notation <neuron_a AND soma> collection may refer to the member  of MCN with the SOMA Tag in the neuron_a instance. The <neuron_b AND SOMA> collection may refer to the member  of MCN with the SOMA Tag in the neuron_b instance. As some or all members ,  of the MCN <neuron_a> may be public, they may be visible to outside entities (e.g., the MCN <neuron_b>), which may enable direct connection of the <neuron_a AND SOMA> to the <neuron_b AND DENDRITE AND COMP>.","In some implementations, connections from <neuron_a AND SOMA> to <neuron_b AND SOMA> may be instantiated using a given connection type.","As will be appreciated by those skilled in the arts, the above notation is exemplary and various other notations may be used in order to identify, select, and\/or access node members using their tags.",{"@attributes":{"id":"p-0141","num":"0142"},"figref":["FIG. 5","FIG. 5","FIG. 5"],"b":["500","520","504","524","504","524","508","526","524","508","504","524"]},"An exemplary pseudo-code, corresponding to the implementation illustrated in , is presented in . The statements  and  in  may be configured to generate the node instances  and  of , respectively. The statements  and  in  may be configured to define the out projections  and\/or the in projections  of , respectively. The last statement  may be configured to define the connection .",{"@attributes":{"id":"p-0143","num":"0144"},"figref":"FIG. 7","b":["700","704","702","702","704","700","704","702"]},"By way of non-limiting illustration, two instances of the private MCN  type may be considered. One instance may be tagged as \u201cneuron_a\u201d, and another instance may be tagged as \u201cneuron_b\u201d. As the MCN  type is defined as private, the MCN  members (e.g., the <neuron_a AND DENDRITE & COMP>, <neuron_a AND SOMA>, <neuron_b & DENDRITE & COMP>, <neuron_b & SOMA>, <SOMA> collections) may be invisible from outside the MCN . Directly connecting the <neuron_a AND SOMA> to the <neuron_b AND DENDRITE AND COMP> may not be permitted for the node configuration type illustrated in , in accordance with some implementations. In order to enable neuron external connectivity, the MCN  definition may comprise input ports  and , and the output  port, may be used to specify the I\/O interface for the MCN node type. The input interface of the MCN  may comprise direct internal connections to the private members of the MCN . Direct internal connections to the private members of the MCN  may include the connections , , and  from the input interface IN  to the private members  with tags \u201cDENDRITE\u201d and \u201cCOMP\u201d. Direct internal connections to the private members of the MCN  may include the connection  from the input interface IN  to the private member  with the tag \u201cSOMA\u201d. The link  may connect the private member  to the Output interface , which may allow the node  to be used for outgoing connections and\/or for incoming connections. The neuron_a.OUT may be configured to be linked\/connected to the neuron_b.IN. The neuron_b.OUT may be configured to be linked\/connected to the neuron_a.IN.",{"@attributes":{"id":"p-0145","num":"0146"},"figref":["FIG. 8","FIG. 8A","FIG. 7"],"b":["800","820","804","824","804","824","800","820","714","716","718","800","812"]},"Conversely, while individual ones of the members  of the node instance  may be kept private, an input interface  may be used to specify how the node members  connect to the input port , according to some implementations. Although a single port is illustrated in the implementation of , this is not intended to be limiting as multiple uniquely tagged input\/output ports may be used in some implementations.","As the output interface  of the node instance  and the input interface  of the node instance  may be exposed and\/or visible to external network elements, a link\/connection  may be established between the node instances of network objects  and  by using in and out interfaces  and , in a manner that is similar to the connection establishment described with respect to , , supra.",{"@attributes":{"id":"p-0148","num":"0149"},"figref":["FIG. 8B","FIG. 8A","FIG. 8B","FIG. 8A","FIG. 8B","FIG. 8A","FIG. 8B","FIG. 8A","FIG. 8A"],"b":["830","804","812","850","824","856","852","826","824","822","858"]},"In some implementations, a third network object  may create the instances inst_a and inst_b of types A  and B , respectively, and\/or may connect the inst_a instance to the inst_b instance using the a_out port of the node instance  and the in port of the node instance . The exemplary HLND definition steps, shown in , may include (1) creating an instance of A (inst_a), (2) creating an instance of B (inst_b), and\/or connecting and\/or linging inst_a to inst_b.","As the members of the node instances A and B may be private, the object C may not directly connect the members of the instance A to the members of the instance B. Instead, the object C may use the exposed ports inst_a.a_out to the inst_b.in. In the connection, declaration  may use the equal sign notation to denote that the inst_b.in is assigned to (e.g., is the same as) the inst_a.a_out. In some implementations, the HLND compiler may use the definition  and a definition of the private node B members to resolve the connection between the virtual b.in port to the actual members of the node B  by linking the inst_b.in with the corresponding member(s) of the node instance A \u2014that is, indirectly establishing the connection between the inst_a (of A) and the inst_b (of B). As illustrated in , the node type  and  may specify projection extensions and\/or synapse\/connection types that may be required to effect the node to node connection.","Tags","In accordance with some implementations, individual elements of the network (i.e., nodes, extensions, connections, I\/O ports) may be assigned at least one unique tag to facilitate the HLND operation and disambiguation. The tags may be used to identify and\/or refer to the respective network elements (e.g., a subset of nodes of the network that is within a specified area).","In some implementations, tags may be used to form a dynamic grouping of the nodes so that these dynamically created node groups may be connected with one another. That is, a node group tag may be used to identify a subset of nodes and\/or to create new connections within the network, as described in detail below in connection with . These additional tags may not create new instances of network elements, but may add tags to existing instances so that the additional tags are used to identify the tagged instances.",{"@attributes":{"id":"p-0153","num":"0154"},"figref":["FIG. 9","FIG. 9"],"b":["900","902","904","900","902","904"]},"Using the tag \u2018MyNodes\u2019, a node collection  may be selected. The node collection  may comprise individual ones of nodes  and\/or  (see, e.g., ). The node collection  may represent nodes tagged as <\u2018MyNodes\u2019 NOT \u2018Subset\u2019>. The node collection  may comprise individual ones of the nodes . The node collection  may represent the nodes tagged as \u2018Subset\u2019. The node collection  may comprise individual ones of the nodes  (see, e.g., ).","In some implementations, the HLND framework may use two types of tags, which may include string tags, numeric tags, and\/or other tags. In some implementations, the nodes may comprise arbitrary user-defined tags. Numeric tags may include numeric identifier (ID) tags, spatial tags, and\/or other tags.","Upon instantiating a node, the instantiated node may have a string tag (the node type) and a unique numerical tag (the unique numerical identifier). In some implementations, position tags may be assigned during the instantiation process.","Operations on Tags","As shown in , the tags may be used to identify a subset of the network. To implement this functionality, one or more Boolean operations may be used on tags. In some implementations, mathematical logical operations may be used with numerical tags. The < . . . > notation may identify a subset of the network, where the string encapsulated by the chevrons < > may define operations configured to identify and\/or select the subset. By way of non-limiting illustration, <\u2018MyTag\u2019> may select individual nodes from the network that have the tag \u2018MyTag\u2019; <\u2018MyTag\u2019 AND \u2018MyTag\u2019> may select individual members from the network that have both the \u2018MyTag\u2019 and the \u2018MyTag\u2019 string tags; <\u2018MyTag\u2019 OR \u2018MyTag\u2019> may select individual members from the network that has the \u2018MyTag\u2019 or the \u2018MyTag\u2019 string tags; <\u2018MyTag\u2019 NOT \u2018MyTag\u2019> may select individual members from the network that have the string tag \u2018MyTag\u2019 but do not have the string tag \u2018MyTag\u2019; and <\u2018MyTag\u2019 AND MyMathFunction(Spatial Tags)<NumericalValue> may select individual nodes from the network that have the string tag \u2018MyTag\u2019 and the output provided by MyMathFunction (as applied the spatial coordinates of the node) is smaller than NumericalValue. Note that this example assumes the existence of spatial tags, which are not mandatory, in accordance with various implementations.","Tag Inheritance","In some implementations, the HLND framework may comprise hierarchical tag inheritance. In some implementations, individual members that are instantiated within a network object may inherit string tags of its parent. For example, individual members of a network object with tags \u2018ParentTag\u2019 and \u2018ParentTag\u2019 may comprise the tags \u2018ParentTag\u2019 and \u2018ParentTag\u2019 in addition to the member-specific tags assigned, for example, during member instantiation.","In some implementations, member spatial tag data may refer to local coordinates (referenced relative to the space defined by the network object) of the member. In some implementations, global coordinates (referenced relative to the space of the entire network) may be inferred from the nested structure of network objects and\/or members.",{"@attributes":{"id":"p-0160","num":"0161"},"figref":["FIG. 10","FIG. 10","FIG. 10"],"b":["1002","1","2","1004","1006","1","2","1","2","1","2","1","2"]},"HLND tag properties and\/or characteristics, according to one or more implementations, may be summarized as: tag types may include string tags and numerical tags; numerical tags may comprise the numerical identifier; Boolean operations may be used on tags; math functions may be allowed on numerical tags; optional spatial and string tags may be assigned; individual node instances may comprise a unique numerical identifier tag; string tag inheritance may be hierarchical; spatial tags may refer to local coordinates; global tag coordinates may be inferred from the nested structure of nodes; and\/or other properties and\/or characteristics.","Tag Implementation","In some implementations, the HLND framework implementation of tags may be configured to require the following functionality: (i) an interface to tag-data generator and data handler; and (ii) implementation of nested objects so as to enable creation of complex network objects from any number of existing network objects.","In some implementations, the tag data handler may be implemented using a database, such as, e.g., MySQL. Instances of network objects may be generated using arbitrary string tags. In some implementations, the network objects may be generated using position tags as well as the string tags. Tag data may be placed into a database. Additional network data (e.g., instances of connections, such as junctions, synapses, etc.) may be generated. The instantiation of connections may depend on position tags and\/or query results. The new data may be stored in the database.","Tag implementation configuration may enable partitioning of the network software application into two parts, which may include a data generation block, a data storage block, and\/or other parts. The data generation block (e.g., implemented in c++) may be configured to generate data based on its own \u2018intelligence\u2019 and\/or by interacting with the database (e.g., MySQL). In some implementations, the data generator functionality may be embedded within the database server. The data generator may be implemented using server side procedures activated by triggers. Such triggers may include insert and connect call\/trigger procedures stored on the database server.","In some implementations, instantiating an END synapse\/junction may require information such as one or more of class and ID of pre-synaptic unit; class and id of post-synaptic unit; spatial position of pre\/unit and post\/unit, and spatial projection of pre\/unit.out and post\/unit.in.; and\/or other information.","A synapse\/junction instance may be generated. In some implementations, additional external parameters may be used for instantiation of an END synapse\/junction. Examples of the external parameters may include synaptic weights, synaptic delays, and\/or other external parameters. The use and functionality of synaptic weights and\/or delays in node-to-node connections is described in further detail in a co-owned U.S. patent application Ser. No. 13\/152,105, filed on Jun. 2, 2011, entitled \u201cAPPARATUS AND METHODS FOR TEMPORALLY PROXIMATE OBJECT RECOGNITION\u201d and\/or in a co-owned U.S. patent application Ser. No. 13\/152,084, filed on Jun. 2, 2011, entitled \u201cAPPARATUS AND METHODS FOR PULSE-CODE INVARIANT OBJECT RECOGNITION\u201d, each of the foregoing incorporated herein by reference in its entirety.","To connect different network objects that use different spatial coordinates, the coordinate system used for each network object may be published. That is, the coordinate system configuration may be available to individual nodes within a certain scope.","Within the HLND framework, connections between network objects may be established in one or more ways. In some implementations, the connection may be established based on an overlap between the axon terminal distribution and the synaptic bouton distribution. In some implementations, the overall connection map may be obtained using a joint probability distribution function (PDF) of the axon terminal distribution and the synaptic bouton distributions. The joint PDF may be used to establish the required connections (synapses). In some implementations, the HLND framework may be configured to distribute individual ones of the potential connection points. Connection points may be subject to one or more particular condition, such as, spatial coordinate and\/or other conditions. The HLND connection algorithm may be configured to select all (or a subset) of these connection points. The HLND connection algorithm may be configured to instantiate the corresponding connections. In some implementations, the HLND may be configured to generate arbitrary user-defined connection sets. According to some implementations, the HLND may be configured to generate all-to-all connections.","SQL-Like Format","In some implementations, the HLND may implemented entirely using SQL. According to some implementations, such SQL implementation may be effected using MySQL database and stored functions\/procedures. The HLND statements may be constructed according to the English language grammar.","Tag Examples","As described above, individual network elements defined within the HLND, regardless whether it is a node, unit, synapse, or junction, or just an empty placeholder, may comprise a tag. This property of the HLND network description may allow tagged elements to, inter alia, be addressed and manipulated as a group. In some implementations, spatial coordinates may be implemented using tags in the form (x,y,z).","By way of non-limiting example, a network unit may comprise one or more tags including the unit ID numerical identifier, \u2018QIF\u2019, \u2018soma\u2019, \u2018pyramidal\u2019, \u2018layer2\/3\u2019, \u2018V1\u2019, spatial coordinates tag (0.1,0.3,0.5), and\/or other tags. A synapse may have tags such as, UD, pre_neuron, post_neuron, and\/or other tags, denoting a pre-synaptic and a post-synaptic node IDs, respectively, \u2018apical\u2019, \u2018exc\u2019, \u2018glu\u2019, and a spatial coordinates tag (0.1,0.3,0.4).","Tagged Operator and Tag Filters","In some implementations, storing tags in a database may allow fast access to groups of elements. Individual database query statements that operate on tags may act as a tag filter (or a search statement) that selects particular elements (that match the query terms) from the database. For example, specifying <\u2018V1\u2019> in the query, may result in a selection of individual elements that comprise \u2018V1\u2019 in any of its tags, e.g., the entire V1 subset. Specifying (<\u2018V1\u2019 AND \u2018pyramidal\u2019 AND NOT \u2018layer2\/3\u2019) may result in individual pyramidal cells in V1 that are not located in the network layers 2 and 3.","In some implementations, an output of a tag query may be assigned its own tag as follows:",{"@attributes":{"id":"p-0174","num":"0175"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing. 3"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"<tag filter> TAGGED new_tag"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"Some implementations may allow addressing of the elements that satisfy the tag filter, without copying and pasting the filter statement.",{"@attributes":{"id":"p-0176","num":"0000"},"ul":{"@attributes":{"id":"ul0003","list-style":"none"},"li":"The following statement:\n    \n    "}},{"@attributes":{"id":"p-0177","num":"0000"},"ul":{"@attributes":{"id":"ul0005","list-style":"none"},"li":"The following statement:\n    \n    "}},"In some implementations, the expression",{"@attributes":{"id":"p-0179","num":"0182"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing. 4"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"n OF <tag filter>"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}},"br":{}},{"@attributes":{"id":"p-0180","num":"0183"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 5."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"100 OF cones TAGGED S_cones"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}},"br":{}},{"@attributes":{"id":"p-0181","num":"0184"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 6."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"300 OF (cones AND NOT S_cones) TAGGED M_cones"]},{"entry":[{},"cones AND NOT M_cones AND NOT S_cones TAGGED L_cones"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},"According to some implementations, a network comprising 800 excitory (exc) and 200 inhibitory (inh) neurons may be split into two equal sub-networks, subnetwork and subnetwork, each comprising 400 exc and 100 inh randomly selected neurons, as follows:",{"@attributes":{"id":"p-0183","num":"0186"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 7."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"400 OF exc TAGGED subnetwork1"]},{"entry":[{},"100 OF inh TAGGED subnetwork1"]},{"entry":[{},"400 OF (exc AND NOT subnetwork1) TAGGED subnetwork2"]},{"entry":[{},"100 OF (inh AND NOT subnetwork1) TAGGED subnetwork2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"Contrast the implementation of Listing 7 with the following statements:",{"@attributes":{"id":"p-0185","num":"0188"},"tables":{"@attributes":{"id":"TABLE-US-00010","num":"00010"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 8."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"500 OF (exc OR inh) TAGGED subnetwork1"]},{"entry":[{},"500 OF (exc OR inh) TAGGED subnetwork2."]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":[{},{}],"b":["1","2"]},"This PUT operator may be used to instantiate and tag network units as follows:",{"@attributes":{"id":"p-0187","num":"0190"},"tables":{"@attributes":{"id":"TABLE-US-00011","num":"00011"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 9."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"147pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"PUT n OF unit_class"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},{"@attributes":{"id":"p-0188","num":"0000"},"ul":{"@attributes":{"id":"ul0007","list-style":"none"},"li":"In accordance with some implementations, the statement"}},{"@attributes":{"id":"p-0189","num":"0192"},"tables":{"@attributes":{"id":"TABLE-US-00012","num":"00012"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 10."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"140pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"PUT 800 OF exc"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{},"ul":{"@attributes":{"id":"ul0008","list-style":"none"},"li":"The following statement"}},{"@attributes":{"id":"p-0190","num":"0194"},"tables":{"@attributes":{"id":"TABLE-US-00013","num":"00013"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 11."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"PUT 800 OF exc TAGGED exc_neurons"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},"In some implementations, the PUT operator may be used to create unit instances by using a filter parameter as follows:",{"@attributes":{"id":"p-0192","num":"0196"},"tables":{"@attributes":{"id":"TABLE-US-00014","num":"00014"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 12."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"PUT <tag filter> OF unit_class"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},{"@attributes":{"id":"p-0193","num":"0000"},"ul":{"@attributes":{"id":"ul0009","list-style":"none"},"li":"The statement"}},{"@attributes":{"id":"p-0194","num":"0198"},"tables":{"@attributes":{"id":"TABLE-US-00015","num":"00015"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 13."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"800 ON circle(1) TAGGED my_points \/\/ see below the definition of ON"},{"entry":"PUT my_points OF exc"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}},"br":[{},{}]},"The connect operator may be used to a synapse connection as:",{"@attributes":{"id":"p-0196","num":"0200"},"tables":{"@attributes":{"id":"TABLE-US-00016","num":"00016"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 14."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"196pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"CONNECT pre_tag TO post_tag WITH synapse_class"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},"In some implementations, the synapse_class may be replaced by the junction_class in the CONNECT statement so that the synaptic junctions may be generated. The construction function synapse_class may have access to individual tags of pre-synaptic and\/or post-synaptic elements. The construction function synapse_class may determine the delay and\/or other relevant parameters needed.","Some implementations may provide for the statements:",{"@attributes":{"id":"p-0199","num":"0203"},"tables":{"@attributes":{"id":"TABLE-US-00017","num":"00017"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 15."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"CONNECT N OF pre_tag TO post_tag WITH synapse_class"]},{"entry":[{},"CONNECT pre_tag TO N OF post_tag WITH synapse_class"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},"The statements in Listing 15 may use randomly selected subsets. This may be implemented by a random walk through the list of all pre_tagged units and random selection of elements of the subset.","Some implementations may provide for the statements:",{"@attributes":{"id":"p-0202","num":"0206"},"tables":{"@attributes":{"id":"TABLE-US-00018","num":"00018"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 16."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"n OF (CONNECT pre_tag TO post_tag WITH synapse_class)"},{"entry":"CONNECT pre_tag TO NEAREST post_tag WITH synapse_class"},{"entry":"CONNECT NEAREST pre_tag TO post_tag WITH synapse_class"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"The first statement in Listing 16 may be configured to instantiate synapses comprising a random subset of the fully-connected matrix of pre-to-post synapses. The term fully-connected matrix may be used to describe a network configuration where all of the pre-synaptic units are connected to all of the pus-synaptic units. Unlike the examples shown in Listing 15, the first statement in the Listing 16 does not guarantee that all pre-synaptic units or all post-synaptic units comprise the same number of synapses.","The second and\/or the third statement in Listing 16 may be configured to generate synaptic connections that are based on the coordinates of pre-synaptic units and the post-synaptic. The second statement may comprise a loop configured to connect each pre-synaptic unit to the nearest post-synaptic unit that satisfies the tag mask. The third statement may loop through each post_tag and finds the nearest pre_tag.","In some implementations, the parameter NEAREST 1 OF may be used in place of the parameter NEAREST in the statements of Listing 16.","In some implementations, individual pre-synaptic units may be connected to n nearest post-synaptic units using the statement:",{"@attributes":{"id":"p-0207","num":"0211"},"tables":{"@attributes":{"id":"TABLE-US-00019","num":"00019"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 17."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"CONNECT exc TO NEAREST n OF exc WITH glu"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":[{},{}]},"In some implementations, a generalized form of the OF selection operator may be configured as:",{"@attributes":{"id":"p-0209","num":"0213"},"tables":{"@attributes":{"id":"TABLE-US-00020","num":"00020"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 18."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"<tag filter 1> OF <tag filter 2>"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{},"b":["1","1","2","2","3","2","3","1","3","1","3"]},"In some implementations, a set of units may be tagged with \u2018cones\u2019. A set of random coordinates may be tagged with \u2018retina\u2019. Random retinal coordinates may be assigned to the cones using the expression:\n\n","The ON operator may be used to return a sample of n points from a probability density function defined by the parameter pdf as:\n\n",{"@attributes":{"id":"p-0212","num":"0000"},"ul":{"@attributes":{"id":"ul0014","list-style":"none"},"li":"Some implementations may provide for the statements:"}},{"@attributes":{"id":"p-0213","num":"0219"},"tables":{"@attributes":{"id":"TABLE-US-00021","num":"00021"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 19."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1000 ON segment(0,1) TAGGED rnd"]},{"entry":[{},"1000 ON circle(1) TAGGED cones"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},"In some implementations, the ON operator may use individual points returned by the function F:\n\n","The operator PER may be used to iterate through a list of tags specified, e.g., by a tag filter. For individual elements of the list, the operator PER may call the statement, passing to it all the tags of the list element. The format of the PER operator may be:\n\n","The PER operator may return a table comprising data describing generated network elements. In some implementations, the PER operator may be used to create multiple synapses per neuron. In some implementations, the PER operator may be used to create multiple neurons per location. In some implementations, the PER operator may be used to create multiple cortical columns per cortical surface.","Some implementations provide for the statements:",{"@attributes":{"id":"p-0218","num":"0226"},"tables":{"@attributes":{"id":"TABLE-US-00022","num":"00022"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 20."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1000 ON segment PER neuron"]},{"entry":[{},"1000 OF locations PER neuron"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},"In some implementations, which may be applicable to SPNET, the unit classes, exc and inh and the synaptic classes, glu and gaba may be defined within the SPNET definitions.",{"@attributes":{"id":"p-0220","num":"0228"},"tables":{"@attributes":{"id":"TABLE-US-00023","num":"00023"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 21."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"PUT 800 OF exc"]},{"entry":[{},"PUT 200 OF inh"]},{"entry":[{},"CONNECT exc TO 100 OF exc OR inh WITH glu"]},{"entry":[{},"CONNECT inh TO 100 OF exc WITH gaba"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"The first line of Listing 21 may be configured to generate 800 units of exc type. The second line of Listing 21 may be configured to generate 200 units of inh type. The third line of Listing 21 may be configured to connect individual units with tag \u2018exc\u2019 (note that the class type is automatically can be used as a tag) to 100 randomly selected units with tags \u2018exc\u2019 or \u2018inh\u2019 with connection type glu. The fourth line of listing 21 may be configured to connect individual units with tag \u2018inh\u2019 to 100 randomly selected units with tag \u2018exc\u2019 with connection type gaba.","Retinal Pixel to Cone Mapping","In some implementations, the HLND description may be used to describe retinal pixel-to-cone mapping. Generally speaking, the cone cells, or cones, may be or refer to photoreceptor cells in the retina of the eye that are responsible for color vision. Cone cells may be densely packed in the fovea, but gradually become sparser towards the periphery of the retina. Below several examples are provided that describe various aspects of the retinal mapping.",{"@attributes":{"id":"p-0223","num":"0231"},"tables":{"@attributes":{"id":"TABLE-US-00024","num":"00024"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Listing 22."},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\/\/ create a 100\u00d7100 square grid of pixel coordinates"},{"entry":"square_grid(100,100) TAGGED pixels"},{"entry":"\/\/create pixel_units at each pixel coordinate"},{"entry":"PUT pixels OF pixel_unit"},{"entry":"\/\/ create hexagonal grid of cone coordinates"},{"entry":"hexagonal_grid(100,100) TAGGED cones"},{"entry":"\/\/ mark a random subset of 10% of them as S cones"},{"entry":"\/\/ and create appropriate S_units"},{"entry":"SIZE(cones)*0.1 OF cones TAGGED S_cones"},{"entry":"PUT S_cones OF S_unit"},{"entry":"\/\/ mark a random subset of 30% of the remainder cones as M cones"},{"entry":"SIZE(cones)*0.3 OF cones AND NOT S_cones TAGGED M_cones"},{"entry":"PUT M_cones OF M_unit"},{"entry":"\/\/ the remainder of cones are L cones"},{"entry":"cones AND NOT S_cones AND NOT M_cones TAGGED L_cones"},{"entry":"PUT L_cones OF L_unit"},{"entry":"\/\/ connect each pixel to one nearest S cone by a junction"},{"entry":"CONNECT pixels TO NEAREST S_cones WITH p2S_junction"},{"entry":"\/\/ the same to M cones"},{"entry":"CONNECT pixels TO NEAREST M_cones WITH p2M_junction"},{"entry":"\/\/ the same to L cones"},{"entry":"CONNECT pixels to NEAREST L_cones WITH p2L_junction"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}},"br":{}},"In some implementations, assigning of the tag network subsets may be configured to enable representation of the network as a directed graph. A directed graph or digraph may comprise a pair G=(V,A) of elements, where a set V, whose elements may be called vertices or nodes, and a set A of ordered pairs of vertices, called arcs, directed edges, or arrows. In HLND, the term node may be used for vertices, and connections are the edges.","HLND and GUI","In some implementations, the HLND may comprise a Graphical User Interface (GUI). The GUI may be configured to translate user actions (e.g., commands, selections, etc.) into HLND statements using appropriate syntax. The GUI may be configured to update the GUI to display changes of the network in response to the HLND statements. The GUI may provide a one-to-one mapping between the user actions in the GUI and the HLND statements. Such functionality may enable users to design a network in a visual manner by, inter alia, displaying the HLND statements created in response to user actions. The GUI may reflect the HLND statements entered, for example, using a text editor module of the GUI, into graphical representation of the network.","This \u201cone-to-one mapping\u201d may allow the same or similar information to be unambiguously represented in multiple formats (e.g., the GUI and the HLND statement), as different formats are consistently updated to reflect changes in the network design. This development approach may be referred to as \u201cround-trip engineering\u201d.","User Actions in HLND","In some implementations, the GUI may support user actions including creating nodes, selecting one or more subsets of the network, connecting nodes, tagging nodes within a selected subset, and\/or other user actions. In some implementations, the GUI may support defining of network objects. Some exemplary user actions are described in detail below.","Create Nodes","Referring now to , node creation using GUI is illustrated, in accordance with one or more implementations. In some implementations, creating nodes of the neural network may require information including a type of node to be instantiated and\/or generated, a number of nodes to be created, and\/or other information. In some implementations, users may provide information including list TAGs to be assigned to the crated nodes, additional parameters for instantiating and\/or initializing nodes, and\/or other information. Additional parameters for instantiating and\/or initializing nodes may depend on a specific network implementation, such as, for example, instructions on how to lay out the nodes to be instantiated, that is, how to assign numerical spatial tags.","The above GUI Node Creation functionality may be supported by one or more appropriate instructions of the HLND kernel that implement node generation. See, for example, Listing 10, supra, for more detail. When a user enters the HLND node generation instruction (statement), the GUI may generate a graphical representation (e.g., a unique symbol, pictogram, and\/or an icon) in the graphical editor corresponding to the respective node (or nodes). Entry of HLND statements by users may be effected by various means, including but not limited to, text entry, speech, soft keys (icons), and\/or other means configured for HLND statement entry.","A user may employ the GUI  of  to perform node creation. According to some implementations, a user may select a node type from available list of node types (the list  in ); drag and drop (as illustrated via the arrow  in ) the selected node type (e.g., the type  in ) into an editor panel , where the node is represented with a unique node symbol ; provide additional parameters (if needed) in via a supplementary entry means (such as, for example, a pop-up menu  in ) that is associated with the specific node type ; and\/or perform other actions to create a node.","The search box  may allow the user to filter the list of displayed node types , , and  using one or more keywords. This may facilitate node type selection where there are large number of node types available. The pop-up menu  may enable the user to graphically specify a number of nodes , parameters for the node instantiation , layout process , additional tags , and\/or other information associated with node creation.","The GUI may allow the user to switch back and forth between the text editor (HLND statement ) and the GUI node creation. By way of non-limiting example, selecting different parameters\/option for the layout of the nodes in the GUI may update the HLND statement. Changing the additional TAGs assigned to the nodes created in the HLND statement may update this information in the GUI.","The GUI interface shown in  is not intended to be limiting as other implementations are contemplated and within the scope of the disclosure. For example, in some implementations, the GUI may include pull down lists, radio buttons, and\/or other elements.","Select Network Subset","Referring now to , A, and B, different exemplary implementations of node subset selection are shown and described in detail. The GUI  of  may comprise a network layout panel , two or more selection description panels  and , and\/or other components. The network, shown in the panel , may comprise a plurality of nodes with different tags , , and , depicted as \u2018\u25a1\u2019, \u2018\u0394\u2019, \u2018\u25ef\u2019, respectively. The selection description panels  and  may comprise a Boolean portion of HLND statements, corresponding to respective subset.","In some implementations, Select Network Subset user action may correspond to selecting members of the network using the GUI editor (e.g., the GUI of ). A user may select a subset of the network, for example, by using a mouse (or other pointing device, such as trackball, fingers on a touch-pad device like and iPad from Apple, light pen, and\/or other technology). The subset selection using GUI may be achieved via a selective clicking\/tapping graphical symbols corresponding to the desired members of the network, click and drag to select an area of the network, a combination thereof, and\/or other actions to select a subset. The subset selection action of the GUI may be supported by the respective instructions of the HLND kernel that implement subset selection. See, for example, listings 6-7, supra, for more detail.","As shown in , the node subset  may comprise nodes comprising the tag , while the subset  may comprise nodes having both tags  and . Once the subsets  and  are selected, the Boolean expressions in the panels  and  may be updated accordingly.","In some implementations, the network shown in the GUI  of  comprises two subsets including the subset  comprising tags  and , the subset  comprising the tag , and\/or other subsets. The selection description panel  may be updated to reflect the Boolean expression corresponding to the tag content of the subset . In some implementations, additional subsets may be generated by forming a subset  comprising an intersect between the subsets  and , as indicated by the Boolean expression .","By way of non-limiting example, responsive to a user entering the Boolean expression for the subset selection statement, e.g., the expression  in ), the GUI may display (for example, using a shaded rectangle in ) the corresponding selected members of the subset in the graphical editor. In some implementations, the GUI may generate a graphical representation in the graphical editor corresponding to the subset (or subsets) selection, as illustrated with respect to  below. Examples of graphical representations may include one or more of a unique symbol, pictogram, an icon, and\/or other graphical representations. In some implementations, a graphical representation may include a change in a graphical attribute including a change in one or more of color, shading pattern, and\/or other graphical attribute.",{"@attributes":{"id":"p-0239","num":"0247"},"figref":["FIG. 13A","FIG. 13A","FIG. 13A","FIG. 13A","FIG. 13A"],"b":["1330","1332","1334","1340","1342","1344","1346","1342","1344"]},"In some implementations, the GUI user actions may be represented using unique symbols  and , as illustrated in network  shown in . The unique symbols  and  may represent subsets  and , respectively, and may be subset specific. By way of non-limiting example, the color and\/or other identifying quality of the unique symbol may be configured in accordance with the tags that are used to identify the subset. The shape and position of the symbol in the graphical editor panel  may be configured according with the spatial tags of the members of the subset. This may be illustrated by symbols , , , , , , and , which may depict subsets of the network illustrated GUI implementations , , and . In some implementations, the symbol may be configured based on the element type. In some implementations, the symbol choice may depend on whether the subset comprises (i) only nodes, (ii) only connections; or (iii) nodes and connections.","In some implementations, the same network configuration (e.g., the subset  of ) may be represented within the GUI graphical panel (e.g., the panel  of ) using different symbols\/icons. In some implementations, which may correspond to a low-level detail network view (corresponding to, for example, no or low zoom), the subset may be represented using a symbol (e.g., the symbol  of ) without showing individual elements of the subset.","In some implementations, which may correspond, for example, to a network with limited processing capability or a network configured for batch updates, the subset may be represented using a symbol without showing individual elements of the subset.","In some implementations, which may be associated with a high-level detail network view (corresponding to, for example, to high zoom level, and\/or when processing resources are available to process information related to individual element of the subset), the subset may be represented using graphical depiction providing further detail of the subset (e.g., the representation  of  that illustrates individual subset elements at their appropriate locations.","In some implementations, the HLND framework selection operation may be performed to assign additional tags to the selected members, to use the selected members (nodes in this case) in a connection statement, and\/or to perform other actions.","The GUI may allow the user to switch back and forth between the text editor (HLND statement) and the GUI subset selection. By way of non-limiting example, selecting different node members using the GUI may cause updates of respective the HLND statements. Changing the selection in the text editor may update the selection in the graphical editor. In some implementations, updating the selection may include highlighting and\/or otherwise visually emphasizing the selected members.","Connect Nodes","The node connection user action may correspond to creating connections between network nodes. According to some implementations, in creating inter-node connections, the HLND kernel may require one or more of a first subset selection (e.g., a subset of nodes the connections will originate from), a second subset selection (e.g., a subset of nodes the connections will terminate at), a connection type used to connect the first subset to the second subset, and\/or other information associated with connecting nodes.","In some implementations, one or more additional parameters may be provided to the HLND kernel including one or more of parameters for setting the connectivity mapping (e.g., an all-to-all, one-to-one, one-to-nearest, and\/or other user defined connection mappings), parameters for instantiating and\/or initializing connection instances (e.g., initializing the synaptic weights), list TAGs to be assigned to the created connection instances, and\/or other parameters.","The HLND kernel may implement instructions configured to connect nodes in the network. (See, for example, listings 16-17, supra, for more detail). By way of non-limiting example, when a user enters the connection instruction, the GUI may create a corresponding graphical representation (e.g., draws a link\/arrow from source to destination selection of nodes) in the graphical editor to illustrate the connections.","In accordance with one or more implementations, the user may use the GUI to select a source subset selection of network members, select a destination selection of network members, drag and drop the source selection onto the destination selection to connect first selection to the second selection, and\/or perform other actions. The GUI may generate link\/arrow elements in the graphical view representing the respective connections between the source and the target members.","In some implementations, a pop-up menu associated with the connection elements (link\/arrow) may allow users to select a connection type from the available list of connection types. In some implementations, the pop-up menu may allow users to provide additional parameters for instantiating and\/or initializing connection instances. In some implementations, the pop-up menu may allow users to set parameters for setting the connectivity mapping.","The GUI may allow the user to switch back and forth between the text editor (HLND statement) and the GUI connection creation. By way of non-limiting example, selecting different node members using the GUI may cause updates of the HLND statement that are associated with the node description. Changing the selection in the text editor may update the selection (e.g., highlights the selected members) in the graphical editor.",{"@attributes":{"id":"p-0252","num":"0260"},"figref":["FIG. 14","FIG. 14"],"b":["1400","1402","1406","1404","1442","1444","1412","1414","1402","1404","1406","1412","1405","1414","1408","1410","1404","1406","1412","1414"]},"According to some implementations, additional tags may be assigned to the collections  and  by invoking, for example, right click on a selection to assign new tags for the selected members of the network. The HDLN statement for the tagging may be automatically generated.","A supplementary graphical data entry means (e.g., the pop-up menu  in ) may be invoked in some implementations using, e.g., a right click, as illustrated by the line . The menu  may be used to, inter alia, assign additional tags  or new tags  to the selected nodes.","In some implementations, by using drag-and-drop action, using a mouse and\/or a finger on a touch pad device, illustrated by the arrow  in  in the GUI field , the first selection may be \u2018dropped\u2019 onto the second selection, which may instruct the HLND engine to create connections between the nodes  of the collection  and the nodes  and  of the collection .","Additional supplementary graphical data entry means (e.g., the pop-up menu  in ) may be used to, inter alia, specify parameters for the connections. Specifying parameters for the connections may include one or more of setting connection type , initializing parameters  for the connection type, specifying connectivity pattern , assigning tags to the connections , and\/or other actions.","For one or more user actions performed with the node selections  and  using the GUI , corresponding HLND statements may be automatically generated and displayed in the statement fields  and , respectively.","The GUI interfaces shown in  are not intended to be limiting as other implementations are contemplated with within the scope of the disclosure. For example, some implementations may include pull down lists, radio buttons, and\/or other components.","Relation of the HLND to the END Format","The HLND format may be designed to be compatible and\/or to be used in conjunction with the Elementary Network Description (END) format, which is described in the U.S. patent application Ser. No. 13\/239,123 entitled \u201cELEMENTARY NETWORK DESCRIPTION FOR NEUROMORPHIC SYSTEMS\u201d filed on Sep. 21, 2011, incorporated supra. In some implementations, instances of END units may be generated based on a HLND description (e.g., END implementations of model neurons). Instances of END junctions and\/or END synapses may directionally connect the units. The HLND may define the anatomy, while the neural and synaptic dynamics may be defined in the applied END classes. HLND may hide complexity and\/or low-level difficulties of END, and may make network design a simple process.","The generated END instances may be used to generate a neural network engine that implements and\/or runs the specified model. That is, END instances may be used to generate an engine that implements the network defined by the HLND description and\/or the applied END classes. The engine may be executed on arbitrary hardware platform from PC, FPGA, any specialized END compatible hardware, and\/or other computer hardware.",{"@attributes":{"id":"p-0261","num":"0269"},"figref":["FIG. 11","FIG. 11"],"b":["1101","1111","1121"]},"In some implementations, individual basic structures (e.g., unit, doublet, and\/or triplet) may be implemented as a single thread on a multi-thread processor. In some implementations, individual structures may be implemented as a super-unit, super-doublet, and\/or super-triplet, which may comprise dedicated circuits configured to processing units, doublets, and\/or triplets respectively using time multiplexing. Some implementations may include three different circuits, one for each of units, doublets, and triplets.","In some implementations, unit  may represent a neuron and\/or a part of a neuron (e.g., a dendritic compartment). In another example, unit  may represent a population of neurons. The activity of the neuron may represent a \u201cmean-firing rate\u201d activity of the population and\/or some other mean-field approximation of the activity of the population. Individual units may be associated with memory variables and an update rule that describes what operations may be performed on its memory. The operations may be clock-based (i.e., executed every time step of the simulation) or they may be event-based (i.e., executed when certain events are triggered).","Depending on the values of the unit variables, the units may generate events (e.g., pulses or spikes) that trigger synaptic events in other units via doublets. For example, a unit  in  may influence unit  via the doublet , which may represent a synapse from pre-synaptic neuron (pre-synaptic unit ) to post-synaptic neuron (post-synaptic unit ).","Individual units may have after-event update rules, which may be triggered after the event is triggered. These rules may be responsible for modification of unit variables that are due to the events, e.g., the after-spike resetting of voltage variables.","Individual doublets may be associated with memory variables. Individual doublets may access variables of the post-synaptic unit. Such access may include read, write, and\/or access mechanisms. Individual doublets may be associated with a doublet event rule that makes a change to the doublet memory to implement synaptic plasticity. Individual doublets may be associated with a doublet event rule that makes a change to the post-synaptic unit memory to implement delivery of pulses. A doublet event rule may encompass some or all the synaptic rules described in the END formats above.","Because multiple doublets (e.g., - in ) may connect corresponding multiple pre-synaptic units - to a single post-synaptic unit , the doublets may modify the post-synaptic unit memory in parallel and\/or in arbitrary order. The result may be order-independent. This may be achieved when the operation on the post-synaptic unit memory is atomic addition (as in GPUs), atomic multiplication (which is equivalent to addition via logarithmic transformation), and\/or resetting to a value (with all the doublets trying to reset to the same value). The post-synaptic unit variable that is being modified by the doublet event rule may not be used in the rule. Otherwise, the result may depend on the order of execution of doublet event rules.","Referring now to , an exemplary implementation of a neural network definition system comprising HLND kernel and END description is shown and described in detail. In , the circles , , and  may represent different higher level network description methods or formats. The circle  may represent the END description of the network. Arrows from , , and  may denote the process of conversion to the END description. For example, a software that processes the HLND description of a network (e.g., the HLND statements) may generate the END description of the same network. The rectangles , , , and  in  may denote various hardware platform implementations of the network defined by the END description . The arrows between the circle  and the rectangles , , , and  may denote the engine generation process. By way of non-limiting example, the arrow between the END description  and the rectangle  may represent the process of generating an executable that implements the END network and is configured to runs on a CPU. The HLND definition  may be processed and converted into the END description . The END description may be configured to be processed (e.g., by various software applications) to generate platform specific machine executable instructions. The platform specific machine executable instructions may be configured to be executed on a variety of hardware platforms, including but not limited to, elements general purpose processor , graphics processing unit , ASIC , FPGA , and\/or other hardware platforms.","Other network description formats may be used with the process , such as, for example BRIAN  and\/or other neuromorphic network description format  (e.g., NEURON) configured to generate the END description of the network, as illustrated in .","Exemplary Implementation of Computerized HLND Apparatus","An exemplary implementation of a computerized network processing apparatus configured to utilize HLND framework in designing a neural network (e.g., the network  of ) is shown and described with respect to . The computerized apparatus  may comprise a processing block (e.g., a processor)  coupled to a nonvolatile storage device , random access memory (RAM) , user input\/output interface , and\/or other components. The user input\/output interface may include one or more of a keyboard\/mouse, a graphical display, a touch-screen input-output device, and\/or other components configured to receive input from a user and\/or output information to a user.","In some implementations, the computerized apparatus  may be coupled to one or more external processing\/storage devices via an I\/O interface , such as a computer I\/O bus (PCI-E), wired (e.g., Ethernet) or wireless (e.g., WiFi) network connection.","In some implementations, the input\/output interface may comprise a speech input device (e.g., a microphone) configured to receive voice commands from a user. The input\/output interface may comprise a speech recognition module configured to receive and recognize voice commands from the user. Various methods of speech recognition are considered within the scope of the disclosure. Examples of speech recognition may include one or more of linear predictive coding (LPC)-based spectral analysis algorithm run on a processor, spectral analysis comprising Mel Frequency Cepstral Coefficients (MFCC), cochlea modeling, and\/or other approaches for speech recognition. Phoneme\/word recognition may be based on HMM (hidden Markov modeling), DTW (Dynamic Time Warping), NNs (Neural Networks), and\/or other processes.","The END engine  may be configured to convert an HLND description of the network into a machine executable format, which may be optimized for the specific hardware or software implementation. The machine executable format may comprise a plurality of machine executable instructions that are executed by the processing block .","It will be appreciated by those skilled in the arts that various processing devices may be used with various implementations, including but not limited to, a single core\/multicore CPU, DSP, FPGA, GPU, ASIC, combinations thereof, and\/or other processors. Various user input\/output interfaces may be applicable to various implementations including, for example, an LCD\/LED monitor, touch-screen input and display device, speech input device, stylus, light pen, trackball, and\/or other user interfaces.","Execution of GUI User Actions","In some implementations, the network design system, e.g., the system  of , may automatically convert the GUI actions into HLND instructions and\/or into END statements. The HLND instruction may cause automatic updates of the GUI representation and\/or of the END description.",{"@attributes":{"id":"p-0276","num":"0284"},"figref":"FIG. 17","b":["1702","1702","1702"]},"As illustrated in , a single object (e.g., the object ) may have one or more representations related thereto, which may include GUI representation  (e.g., using the GUI editor  of ), the HLND representation  (e.g., using the HLND statement described supra with respect to ), the END representation  (see, e.g., ), and\/or other representations (depicted by the rectangle ). Responsive to an object property (i.e., the object data element) being generated and\/or updated, the corresponding representations of the object (e.g., the representations , , , and ) may be updated using bi-directional pathways , , , and , respectively.","In some implementations, responsive to a user GUI action modifying a selection, the corresponding HLND statement(s) (e.g., the HLND representation  in ) may be updated. In some implementations, the END instructions (e.g., the END representation  in ) may be updated.","In some implementations, the END instructions may be executed by the apparatus thereby enabling a more detailed and accurate representation of the network.","In some implementations, nodes may be rendered within the GUI using unique color symbol when the statement to create units is available in the network description framework.","In some implementations, nodes may be rendered within the GUI at their correct location with unique (for the whole subset) symbols responsive to the coordinates of the nodes being available\u2014that is, when the connections statement is at least partially processed.","In some implementations, connections between two subsets may be rendered by the GUI using, for example, a single line, when the connect instruction is available in the network description framework.","In some implementations, connections between two subsets may show the detailed connectivity structure once the pre-node and post-node information for the connection instances are available (i.e., have been previously generated)\u2014that is, once the connection statement is at least partially processed.","In some implementations, connections between two subsets may show the detailed connectivity structure with unique properties (e.g., line width representing a connection) per connection responsive to the pre-node information, the post-node information, and\/or the initial weight for the connection instances being available\u2014that is, once the connection statement is at least partially processed.","As is appreciated by those skilled in the arts, other representations (e.g., as depicted by the rectangle  in ) may exist and may be compatible with various implementations, provided they conform to the update framework described herein.","In some implementations, data exchange between different representations (e.g., the representations , , , and  in ) may be enabled via direct links denoted by arrows , , , , , and  in . For clarity, not all direct connections between the representations , , , and  are shown in .","It will be recognized that while certain aspects of the disclosure are described in terms of a specific sequence of steps of a method, these descriptions are only illustrative of the broader methods of the disclosure, and may be modified as required by the particular application. Certain steps may be rendered unnecessary or optional under certain circumstances. Additionally, certain steps or functionality may be added to the disclosed implementations, or the order of performance of two or more steps permuted. All such variations are considered to be encompassed within the disclosure disclosed and claimed herein.","Although the disclosure has been described in detail for the purpose of illustration based on what is currently considered to be the most practical and preferred implementations, it is to be understood that such detail is solely for that purpose and that the disclosure is not limited to the disclosed implementations, but, on the contrary, is intended to cover modifications and equivalent arrangements that are within the spirit and scope of the appended claims. For example, it is to be understood that the present disclosure contemplates that, to the extent possible, one or more features of any implementation can be combined with one or more features of any other implementation."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 3C"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 13A"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 13B"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 17"}]},"DETDESC":[{},{}]}
