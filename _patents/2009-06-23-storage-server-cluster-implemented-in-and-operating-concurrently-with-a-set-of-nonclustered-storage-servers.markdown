---
title: Storage server cluster implemented in and operating concurrently with a set of non-clustered storage servers
abstract: A storage controller is operable concurrently as an independent storage server and as at least a portion of a node of a storage server cluster. A network storage system comprises multiple storage controllers and multiple independent storage servers, each storage server implemented in a separate storage controller, each of the storage server configured to present to users a separate system image of stored data. The storage servers are independently operable and manageable. The system further includes a storage server cluster that includes cooperating storage server nodes, distributed among the storage controllers. The storage server cluster presents to users a single system image of data stored in the server cluster independently of the node or nodes in which said data resides. The storage server cluster is operable independently of the independent storage servers and is operable concurrently with operation of at least one of the independent storage servers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08156163&OS=08156163&RS=08156163
owner: NetApp, Inc.
number: 08156163
owner_city: Sunnyvale
owner_country: US
publication_date: 20090623
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","FIELD OF THE INVENTION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","Usage and Examples"],"p":["This Application incorporates by reference U.S. patent application Ser. No. 12\/287,099, filed Oct. 6, 2008, and entitled \u201cREAD-ONLY MIRRORING FOR LOAD SHARING\u201d","At least one embodiment of the present invention pertains to network storage systems, and more particularly, to a storage server cluster implemented in, and operating concurrently with, a set of non-clustered storage servers.","A storage controller is a physical processing device that is used to store and retrieve data on behalf of one or more hosts. A network storage controller can be configured (e.g., by hardwiring, software, firmware, or any combination thereof) to operate as a storage server that serves one or more clients on a network, to store and manage data in a set of mass storage devices, such as magnetic or optical storage-based disks or tapes. Some storage servers are designed to service file-level requests from hosts, as is commonly the case with file servers used in a network attached storage (NAS) environment. Other storage servers are designed to service block-level requests from hosts, as with storage servers used in a storage area network (SAN) environment. Still other storage servers are capable of servicing both file-level requests and block-level requests, as is the case with certain storage servers made by NetApp\u00ae, Inc. of Sunnyvale, Calif., employing the Data ONTAP\u00ae 7G or GX storage operating systems.","In at least one implementation, a storage server makes data available to a client (host) system by presenting or exporting to clients one or more logical containers of data, such as volumes. A \u201cvolume\u201d is an abstraction of physical storage, combining one or more physical mass storage devices (e.g., disks) or parts thereof into a single logical storage object, and which is managed as a single administrative unit, such as a single file system. A \u201cfile system\u201d is a structured (e.g., hierarchical) set of stored logical containers of data (e.g., volumes, logical unit numbers (LUNs), directories, files). Note that a file system does not have to include storage that is based on \u201cfiles\u201d per se.","From the perspective of a client system, each volume can appear to be a single disk drive. However, each volume can represent the storage space in a single physical storage device, a redundant array of independent disks (RAID) or a RAID group, an aggregate of some or all of the storage space in multiple physical storage devices, or any other suitable set of storage space. An \u201caggregate\u201d is a logical aggregation of physical storage; i.e., a logical container for a pool of storage, combining one or more physical mass storage devices (e.g., disks) or parts thereof into a single logical storage object, which contains or provides storage for one or more other logical data sets at a higher level of abstraction (e.g., volumes).","A network storage system can have a very simple architecture; for example, an individual storage server can provide one or more clients on a network with access to data stored in a mass storage subsystem. As another example, two or more individual storage servers may be connected and configured as failover partners, to increase reliability, such as in a storage failover (SFO) or controller failover (CFO) configuration. Recently, however, with storage capacity demands increasing rapidly in almost every business sector, there has been a trend towards the use of clustered network storage systems, to improve scalability.","In a clustered storage system, two or more storage server \u201cnodes\u201d are connected in a distributed architecture. The nodes are generally implemented by two or more storage controllers. Each storage server \u201cnode\u201d is in fact a storage server, although it is implemented with a distributed architecture. For example, a storage server node can be designed to include a network module (\u201cN-module\u201d) to provide network connectivity and a separate data module (e.g., \u201cD-module\u201d) to provide data storage\/access functionality, where the N-module and D-module communicate with each other over some type of physical interconnect. Two or more such storage server nodes are typically connected to form a storage \u201ccluster\u201d, where each of the N-modules in the cluster can communicate with each of the D-modules in the cluster.","A clustered architecture allows convenient scaling through the addition of more N-modules and D-modules, all capable of communicating with each other. Further, a storage cluster may present a single system image of stored data to clients and administrators, such that the actual location of data can be made transparent to clients and administrators. An example of a storage controller that is designed for use in a clustered system such as this is a NetApp FAS-series Filer which employs NetApp's Data ONTAP\u00ae GX storage operating system.","Both clustered and non-clustered network storage systems have advantages. However, in storage server products available today these two types of systems are mutually exclusive. That is, a storage server is generally designed to operate as part of a clustered storage system or as a non-clustered storage server, but not both. This places limitations upon storage system growth and performance over time and inhibits the most efficient use of storage resources.","For example, a company may use a non-clustered storage system with satisfactory results for a long time, but may conclude at some point in time (e.g., in anticipation of significant corporate growth) that it would be desirable to have a clustered storage system. With conventional technology, the company would have to replace its entire storage system with clustered-enabled devices, even though the existing non-clustered storage system it is using may be fully functional and adequate for many purposes. The company is therefore faced with either continuing to use the existing storage system which becomes less and less optimal over time, potentially adversely affecting business processes, or purchasing an expensive new system and giving up the use of the (also expensive) fully functional existing system. Either choice is likely to prove costly.","The techniques introduced here enable a storage controller to operate concurrently as an individual non-clustered storage server and as at least a portion of a node of a storage server cluster. As a result, a given number, N, of storage controllers can effectively operate as N+1 storage servers, i.e., as N individual storage servers plus a storage server cluster. Further, the techniques introduced here enable the ownership of any given data volume to be easily transitioned from exclusive ownership by an individual storage server to ownership by the cluster, or vice versa. These techniques collectively allow convenient, gradual transitioning of a non-clustered storage system into a clustered storage system in response to changing storage needs.","As described in greater detail below, in accordance with certain embodiments, a network storage system can comprise a plurality of storage controllers and a plurality of storage servers, where each of the storage servers is implemented in a separate one of the plurality of storage controllers, and where each of the storage servers is configured to present to users a separate system image of stored data. The system further can include a storage server cluster that includes a plurality of cooperating storage server nodes, distributed among the plurality of storage controllers, where the storage server cluster is configured to present to users a single system image of data stored in the storage server cluster independently of the storage server node or nodes in which said data resides, and where the storage server cluster is operable concurrently with operation of at least one of the plurality of storage servers.","The plurality of storage servers are independently operable and independently manageable. Further, the storage server cluster is operable independently of the plurality of storage servers.","The plurality of storage servers separately implement a plurality of distinct file systems, whereas the storage server nodes of the cluster collectively implement one or more file systems in a single global namespace. Each of the individual storage servers presents to users a separate system image of stored data, whereas data stored in the storage server cluster is presented to users as a single system image regardless of where said data stored in the storage server cluster physically resides. User data owned by any particular one of the plurality of storage servers can only be accessed through that particular storage server that owns the data, whereas user data owned by the cluster can be accessed through any of the plurality of storage server nodes.","Other aspects of the technique will be apparent from the accompanying figures and from the detailed description which follows.","References in this specification to \u201can embodiment\u201d, \u201cone embodiment\u201d, or the like, mean that the particular feature, structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment.","System Environment",{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 1","FIG. 1"],"b":["104","102","106","104","102","102","108","112","105","112"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 1","b":["102","111","108","105"]},"Although there is some level of cooperation between storage servers  for purposes of CFO, each storage server  is nonetheless operated and managed as a distinct, independent entity, which maintains its own separate file system(s). Hence, the network storage system of  is deemed a non-clustered system.","Each storage server  can be, for example, one of the FAS-series of storage server products available from NetApp, Inc. The client systems  are connected to the storage server  via the computer network , which can be a packet-switched network, for example, a local area network (LAN) or wide area network (WAN). Further, the storage server  can connected to the disks  via a switching fabric (not shown), which can be a fiber distributed data interface (FDDI) network, for example. It is noted that, within the network data storage environment, any other suitable numbers of storage servers and\/or mass storage devices, and\/or any other suitable network technologies, may be employed.","A storage server  can make some or all of the storage space on the disk(s)  available to the client systems  in a conventional manner. For example, each of the disks  can be implemented as an individual disk, multiple disks (e.g., a RAID group) or any other suitable mass storage device(s). A storage server  can communicate with the client systems  according to well-known protocols, such as the Network File System (NFS) protocol or the Common Internet File System (CIFS) protocol, to make data stored on the disks  available to users and\/or application programs. A storage server  can present or export data stored on the disk  as volumes and\/or qtrees, to each of the client systems . Various functions and configuration settings of either storage server  can be controlled by a user, e.g., a storage administrator, from a management station  coupled to the network .",{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 2","FIG. 2"],"b":["200","204","202","206","204","202","202","208","210","212","212"]},"Each node  essentially provides similar functionality to that of a storage server  in . However, unlike the storage servers  in , the nodes  are not operated and managed as distinct independent entities. Rather, they are operated and managed collectively as a single entity, i.e., a cluster . The cluster presents to users and administrators a single system image of all data stored by the cluster, regardless of where any particular data resides within the cluster.","Each of the nodes  is configured to include several modules, including an N-module , a D-module , and an M-host  (each of which can be implemented by using a separate software module) and an instance of a replicated database (RDB) . RDB  can be implemented as a number of individual databases, each of which has an instance located in each of the nodes . The N-modules  include functionality that enables their respective nodes  to connect to one or more of the client systems  over the network , while the D-modules  provide access to the data stored on their respective disks . The M-hosts  provide management functions for the clustered storage server system , including user interface functionality to enable an administrator to manage and control the cluster. Accordingly, each of the server nodes  in the clustered storage server arrangement provides the functionality of a storage server.","The RDB  is a database that is replicated throughout the cluster, i.e., each node  includes an instance of the RDB . The various instances of the RDB  are updated regularly to bring them into synchronization with each other. The RDB  provides cluster-wide storage of various information used by all of the nodes  and includes a volume location database (VLDB). The VLDB indicates the location within the cluster of each volume in the cluster (i.e., the owning D-module  for each volume) and is used by the N-modules  to identify the appropriate D-module  for any given volume to which access is requested. Each volume in the system is represented by a data set identifier (DSID) and a master data set identifier (MSID), each of which is stored in two places: on disk in the volume itself and in the VLDB. The DSID is a system internal identifier of a volume. The MSID is an external identifier for a volume, used in file handles (e.g., NFS) and the like. The VLDB stores the identity and mapping (MSIDs-to-DSIDs) of all volumes in the system.","The nodes  are interconnected by a cluster switching fabric , which can be embodied as a Gigabit Ethernet switch, for example. The N-modules  and D-modules  cooperate to provide a highly-scalable, distributed storage system architecture of a clustered computing environment implementing exemplary embodiments of the present invention. Note that while there is shown an equal number of N-modules and D-modules in , there may be differing numbers of N-modules and\/or D-modules in accordance with various embodiments of the technique described here. For example, there need not be a one-to-one correspondence between the N-modules and D-modules. As such, the description of a node  comprising one N-module and one D-module should be understood to be illustrative only.","Various functions and configuration settings of the cluster  can be controlled by a user, e.g., a storage administrator, from a management station  coupled to the network . A plurality of virtual interfaces (VIFs)  allow the disks  associated with the nodes  to be presented to the client systems  as a single shared storage pool.  depicts only the VIFs at the interfaces to the N-modules  for clarity of illustration.",{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 3","b":["108","208","301","301","320","340","370","380","390","370","208","208","270","214","216"]},"The storage controller  can be embodied as a single- or multi-processor storage system executing a storage operating system  that preferably implements a high-level module, called a storage manager, to logically organize the information as a hierarchical structure of named directories, files and special types of files called virtual disks (hereinafter generally \u201cblocks\u201d) on the disks. Illustratively, one processor  can execute the functions of the N-module  on the node  while another processor  executes the functions of the D-module .","The memory  illustratively comprises storage locations that are addressable by the processors and adapters , ,  for storing software program code and data associated with the present invention. The processor  and adapters may, in turn, comprise processing elements and\/or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system , portions of which is typically resident in memory and executed by the processing elements, functionally organizes the storage controller  by (among other things) invoking storage operations in support of the storage service provided by the node . It will be apparent to those skilled in the art that other processing and memory implementations, including various computer readable storage media, may be used for storing and executing program instructions pertaining to the technique introduced here.","The network adapter  includes a plurality of ports to couple the storage controller  to one or more clients  over point-to-point links, wide area networks, virtual private networks implemented over a public network (Internet) or a shared local area network. The network adapter  thus can include the mechanical, electrical and signaling circuitry needed to connect the storage controller  to the network . Illustratively, the network  can be embodied as an Ethernet network or a Fibre Channel (FC) network. Each client  can communicate with the node  over the network  by exchanging discrete frames or packets of data according to pre-defined protocols, such as TCP\/IP.","The storage adapter  cooperates with the storage operating system  The storage controller  to access information requested by the clients . The information may be stored on any type of attached array of writable storage media, such as magnetic disk or tape, optical disk (e.g., CD-ROM or DVD), flash memory, solid-state disk (SSD), electronic random access memory (RAM), micro-electro mechanical and\/or any other similar media adapted to store information, including data and parity information. However, as illustratively described herein, the information is stored on disks . The storage adapter  includes a plurality of ports having input\/output (I\/O) interface circuitry that couples to the disks over an I\/O interconnect arrangement, such as a conventional high-performance, Fibre Channel (FC) link topology.","Storage of information on disks  can be implemented as one or more storage volumes that include a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number (VBN) space on the volume(s). The disks  can be organized as a RAID group. One or more RAID groups together form an aggregate. An aggregate can contain one or more volumes\/file systems.","The storage operating system  facilitates clients' access to data stored on the disks . In certain embodiments, the storage operating system  implements a write-anywhere file system that cooperates with one or more virtualization modules to \u201cvirtualize\u201d the storage space provided by disks . In certain embodiments, a storage manager  () logically organizes the information as a hierarchical structure of named directories and files on the disks . Each \u201con-disk\u201d file may be implemented as set of disk blocks configured to store information, such as data, whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module(s) allow the storage manager  to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers (LUNs).",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 4","FIG. 2"],"b":["330","330","214","216","218","214","214"]},"The network protocol stack in the N-module  includes a network access layer  which includes one or more network drivers that implement one or more lower-level protocols to enable the processing system to communicate over the network , such as Ethernet, Internet Protocol (IP), Transport Control Protocol\/Internet Protocol (TCP\/IP), Fibre Channel Protocol (FCP) and\/or User Datagram Protocol\/Internet Protocol (UDP\/IP). The network protocol stack also includes a multi-protocol layer  which implements various higher-level network protocols, such as Network File System (NFS), Common Internet File System (CIFS), Hypertext Transfer Protocol (HTTP), Internet small computer system interface (iSCSI), etc. Further, the network protocol stack includes a cluster fabric (CF) interface module which implements intra-cluster communication with D-modules and with other N-modules.","In addition, the storage operating system  includes a set of data access layers organized to provide data paths for accessing information stored on the disks  of the node ; these layers in combination with underlying processing hardware forms the D-module . To that end, the data access layers include a storage manager module  that manages any number of volumes , a RAID system module  and a storage driver system module .","The storage manager  primarily manages a file system (or multiple file systems) and serves client-initiated read and write requests. The RAID system  manages the storage and retrieval of information to and from the volumes\/disks in accordance a RAID redundancy protocol, such as RAID-4, RAID-5, or RAID-DP, while the disk driver system  implements a disk access protocol such as SCSI protocol or FCP.","The data access layers also include a CF interface module to implement intra-cluster communication link  with N-modules and\/or other D-modules. The nodes  in the cluster  can cooperate, through their respective CF interface modules and , to provide a single file system image across all D-modules  in the cluster . Stated another way, the various D-modules  can implement multiple distinct file systems within a single global namespace. Thus, any N-module  that receives a client request can access any data container within the single file system image located on any D-module  of the cluster, and the location of that data container can remain transparent to the client and user.","The CF interface modules  implement the CF protocol to communicate file system commands among the modules of cluster over the cluster switching fabric  (). Such communication can be effected by a D-module exposing a CF application programming interface (API) to which an N-module (or another D-module) issues calls. To that end, a CF interface module  can be organized as a CF encoder\/decoder. The CF encoder of, e.g., CF interface on N-module  can encapsulate a CF message as (i) a local procedure call (LPC) when communicating a file system command to a D-module  residing on the same node or (ii) a remote procedure call (RPC) when communicating the command to a D-module residing on a remote node of the cluster. In either case, the CF decoder of CF interface on D-module  de-encapsulates the CF message and processes the file system command.","In operation of a node , a request from a client  is forwarded as a packet over the network  and onto the node , where it is received at the network adapter  (). A network driver of layer  processes the packet and, if appropriate, passes it on to a network protocol and file access layer for additional processing prior to forwarding to the storage manager . At that point, the storage manager  generates operations to load (retrieve) the requested data from disk  if it is not resident in memory . If the information is not in memory , the storage manager  indexes into a metadata file to access an appropriate entry and retrieve a logical VBN. The storage manager  then passes a message structure including the logical VBN to the RAID system ; the logical VBN is mapped to a disk identifier and disk block number (DBN) and sent to an appropriate driver (e.g., SCSI) of the disk driver system . The disk driver accesses the DBN from the specified disk  and loads the requested data block(s) in memory for processing by the node. Upon completion of the request, the node (and operating system) returns a reply to the client  over the network .","The data request\/response \u201cpath\u201d through the storage operating system  as described above can be implemented in general-purpose programmable hardware executing the storage operating system  as software or firmware. Alternatively, it can be implemented at least partially in specially designed hardware. That is, in an alternate embodiment of the invention, some or all of the storage operating system  is implemented as logic circuitry embodied within a field programmable gate array (FPGA) or an application specific integrated circuit (ASIC), for example.","The N-module  and D-module  can be implemented as processing hardware configured by separately-scheduled processes of storage operating system ; however, in an alternate embodiment, the modules may be implemented as processing hardware configured by code within a single operating system process. Communication between an N-module  and a D-module  is thus illustratively effected through the use of message passing between the modules although, in the case of remote communication between an N-module and D-module of different nodes, such message passing occurs over the cluster switching fabric . A known message-passing mechanism provided by the storage operating system to transfer information between modules (processes) is the Inter Process Communication (IPC) mechanism. The protocol used with the IPC mechanism is illustratively a generic file and\/or block-based \u201cagnostic\u201d CF protocol that comprises a collection of methods\/functions constituting a CF API.","The operating system  also includes M-host , which provides management functions for the cluster , including user interface functionality to enable an administrator to manage and control the cluster  (e.g., through management station ). Because each node  in the cluster  has an M-host , the cluster  can be managed via the M-host  in any node  in the cluster . The functionality of an M-host  includes generating a user interface, such as a graphical user interface (GUI) and\/or a command line interface (CLI), for a storage network administrator. The functionality of an M-host  can also include facilitating the provisioning of storage, creating and destroying volumes, installation of new software, controlling access privileges, scheduling and configuring data backups, scheduling data mirroring function, and other functions. The M-host  communicates with its local D-module  or with any remote D-module by using a set of APIs, the details of which are not germane to this disclosure. The M-host  includes network interfaces (not shown) to communicate with D-modules and to communicate with one or more external computers or input\/output terminals used by network administrators.","Concurrent Clustered and Non-Clustered Operation",{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIG. 5","FIG. 4"],"b":["530","530","330","208","530","208"]},"Storage operating system  can be assumed to be substantially identical to storage operating system  in all significant aspects, except as will be noted here. First, storage operating system  includes an additional protocol stack and user interface in its D-module . Specifically, the D-module  includes additional network access layer and protocol layer , which are substantially identical to those in the N-module , and it also includes additional user interface module . The network access layer and a protocol layer in a D-module  are logically coupled to (and below) the storage manager . The addition of network access layer , protocol layer and user interface module  enables D-module  to function as an independent (non-clustered) storage server while at the same time functioning as a cluster node. D-module  can function as a complete, independent storage server (which does not make use of N-module ), while at the same time functioning as the D-module of a cluster node .","User interface module  is also coupled logically to the storage manager . In one embodiment, user interface  provides a GUI and\/or CLI and facilitates management functions in a similar manner to M-host , except that it does not support management functions\/commands that are by definition cluster-related.","Storage manager  also includes an additional module, i.e., transition module . Transition module  is responsible for carrying out the transitioning of the contents of volumes from ownership by individual storage servers to ownership by the cluster, or vice versa, the manner discussed below. Transition module  can also transition an entire virtual filer (VFiler) to cluster ownership. Transition of a volume or VFiler is driven (commanded) from the M-Host .","Upon deploying a storage system, storage administrators may desire, at least initially, to operate the system in a non-clustered manner. Accordingly, by employing storage operating system , two or more storage controllers incorporating storage operating system  can be connected in the configuration of , but where each node  initially operates as an individual, non-clustered storage server. Note that this does not preclude, and is not inconsistent with, operating the nodes  as CFO partners, which is not \u201cclustering\u201d as that concept is used herein.","At some time after initial deployment of the storage system, however, a storage administrator may conclude it is desirable to have some of the benefits of clustering. For example, having data maintained by a storage cluster, rather than an individual storage server, enables more efficient load balancing and allows the location of data to be optimized, while keeping the location of data transparent to users. It also facilitates growth of the system as storage needs increase. However, the administrator may be reluctant to convert the entire system and all of its data into a clustered system all at one once, since doing so would require significant time and effort and could be disruptive to users in terms of locating and accessing data.","Accordingly, the techniques introduced here allow a storage cluster to be created dynamically (i.e., at run time) on the same physical hardware (the same storage controllers) as two or more existing non-clustered storage servers. The cluster operates concurrently with, yet independently of, the individual storage servers with which it shares the same hardware. The cluster can be utilized either immediately or at some convenient later time, and data volumes owned by the individual storage servers can be transitioned gradually to ownership by the cluster (and vice versa) on an individual basis, by adding volumes to the global namespace of the cluster. When the cluster is initially created, it does not have to own any data, i.e., it can be completely empty and remain empty until such time that a network administrator deems it appropriate to transition ownerships of one or more data volumes to the cluster. A data volume owned by the cluster can reside on any D-module  within the cluster, yet it can be accessed by a user in the same manner through any N-module  in the cluster, while the node in which that data volume resides can be kept completely transparent to users and administrators. In contrast, a data volume owned by an individual (non-clustered) storage server is accessible only through that particular storage server, and its location on that storage server is known to users and administrators.","Referring to , any data volume  which is owned by an individual storage server is accessed from the client side only through the network access layer and protocol layer in the D-module  of that individual storage server. Management functions related to such data are also performed exclusively through the user interface  in that D-module. On the other hand, any data volume  owned by the cluster is accessed from the client side through an N-module  (any N-module in the cluster), and management functions related to such data are performed through an M-host  (any M-host in the cluster). In both scenarios, back-end access to a volume's data in persistent storage is performed by the storage manager  via the RAID system  and storage driver system  in the D-module .","Cluster Creation and Volume Transition","The process of forming a cluster and transitioning ownership of volumes from individual storage servers to the cluster will now be described further with reference to . Note, however, that the techniques introduced here also enable transitioning of a volume owned by the cluster to ownership by an individual storage server, in a similar manner.  shows four storage controllers , i.e. storage controller A, storage controller B, storage controllers C, and storage controller D, each of which includes an independent (non-clustered) storage server . Each of the four storage servers  in  is assumed to include a storage operating system such as operating system  in  but is currently operating only as an independent (non-clustered) storage server. Two or more of the storage servers  may be configured to operate as CFO partners.","At some later time, it becomes desirable to create a cluster from the storage servers . Accordingly, a storage administrator first makes sure that all of the storage controllers  that are to be members of the cluster (in this example, storage controllers A, B, C and D) are physically connected through a cluster switching fabric, such as cluster switching fabric  in , which enables communication using the intra-cluster communication link . Then the storage administrator inputs a \u201cCluster Create\u201d command to the M-host  within any of these storage servers. In response to that command, the receiving M-host  causes a new cluster to be defined from the node  in which that M-host  is located. That is, the Cluster Create command creates a new storage cluster of one node. This command includes as a parameter the base license for the cluster. In addition, the Cluster Create command may include parameters to specify:","the name of the cluster to be created;","the IP address of the node's first cluster port;","the netmask assigned to the node's first cluster port;","the maximum transmission unit value (e.g., in bytes) assigned to the node's first cluster port;","the IP address of the node's second cluster port;","the netmask assigned to the node's second cluster port;","the maximum transmission unit value (e.g., in bytes) assigned to the node's second cluster port.","To add nodes to the cluster, another command, \u201cCluster Join\u201d, can be used (input to an M-Host ) to specify each additional node to be added.","As part of the creation of the new cluster, one or more entities called virtual servers (\u201cvservers\u201d) are created in the cluster. A vserver is a single file system namespace which is collectively implemented by all of the nodes in the cluster (i.e., it is global to the cluster). In this context, a vserver is also an instance of an application service provided to a client system. A cluster can be organized into any suitable number of vservers, although for purposes of this explanation, only one vserver is assumed to be created, which vserver is named \u201cfoo\u201d. Each vserver has a user domain and a security domain that are separate from the user and security domains of any other vserver. Moreover, each vserver is associated with one or more VIFs and can span, one or more physical nodes, each of which can hold one or more VIFs and storage associated with one or more vservers. Client systems can access the data on a vserver via any node of the cluster, but only through the VIFs associated with that vserver. Vservers are described further in co-pending U.S. patent application Ser. No. 12\/287,099, filed on Oct. 6, 2008 and entitled, \u201cRead-Only Mirroring for Load Sharing,\u201d by F. Rabii et al., which is incorporated herein by reference.","Referring now to , once the cluster has been created and a vserver has been defined for it, any volume can be transitioned so that it is owned by the cluster instead of by an individual storage server. For example, it may be desirable to transition volume VB, which is initially owned by storage server B, to ownership by the cluster. Accordingly, this can be accomplished by adding volume VB into the global namespace, vserver foo, as shown in . The process of transitioning the volume can be initiated by an administrator inputting into the M-host  a \u201cVolume Transition\u201d command specifying as parameters the volume or volumes to be transitioned. The manner in which this is accomplished is described further below. Likewise, any other volume owned by an individual storage server can be transitioned to cluster ownership in the same way, as shown in  regarding volume VD.","Once a volume is transitioned to cluster ownership, it can be accessed by users or administrators through any node (e.g., any storage server) in the cluster. The volume may continue to reside in the same server\/node immediately after the ownership transition, however, normal operation of the cluster may result in the volume being copied or moved to different nodes in the cluster for various reasons which are not germane here. Yet after the transition to cluster ownership, which node the volume resides in is transparent to users and administrators, unlike a volume owned by an individual storage server. In at least some embodiments, any given volume is owned exclusively by either one individual storage server or by the cluster, but not both. Note that after one or more volumes have been transitioned to cluster ownership, any volume still owned by an individual storage server can still be accessed through that individual storage server (and only that particular storage server), in the same manner as before the cluster was created.","Thus, a set of storage servers can operate as both a clustered system and a non-clustered system at the same time. Further, it can be seen that by using these techniques, any system of N independent storage servers (where N=4 in the illustrative embodiment of ) can, in effect, be converted into N+1 storage servers, where the (N+1)th storage server is a storage cluster.",{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 12"},"Initially, at  an M-host  receives from a storage administrator a Volume Transition command, specifying as parameters the volume (or volumes) to be transitioned. At  the M-host checks for feature compatibility of the volume, i.e., it determines whether all features in the volume are supported by the cluster or if they are not supported, can be disabled. Assuming all features are compatible (), the process continues to ; otherwise, the M-host  returns an error message to the administrator at , and the process ends.","At , the storage manager  of the storage server which owns the volume takes the specified volume off-line, completes all outstanding client operations (e.g., reads and or writes) while accepting no new client operations, and saves all volatile state for the volume to persistent storage (e.g., disks).","At  the storage manager  creates configuration information needed by the cluster for the volume. This includes storing a DSID and an MSID for the volume in two places: on disk in the volume itself and in the VLDB. The VLDB tracks the identities and mappings (MSIDs-to-DSIDs) of all volumes in the system, as noted above. This also includes storing in the VLDB the junction that points to the volume, if any, the vserver that owns the volume, and the export rules for the volume, along with its role as the root volume of a vserver namespace (if it is root, then there is no junction that points to it; otherwise, there is a junction inode in some other volume in the cluster that points to this volume).","After creating the configuration information, the storage manager  then marks the volume at  as a cluster-mode volume both on disk and in the VLDB and then brings the volume online at , using the newly created configuration information (for every volume in the storage system, the VLDB includes an indication of whether or not that volume is a cluster-mode volume). This completes the process. It will be recognized that the reverse transition, i.e. a transition from cluster ownership to ownership by a particular individual storage server, can be carried out in a similar manner, such as by reversing the above described process, subject to appropriate minor modifications that will be readily apparent to those skilled in the art.","The techniques introduced above can be implemented by programmable circuitry programmed or configured by software and\/or firmware, or they can be implemented by entirely by special-purpose \u201chardwired\u201d circuitry, or in a combination of such forms. Such special-purpose circuitry (if any) can be in the form of, for example, one or more application-specific integrated circuits (ASICs), programmable logic devices (PLDs), field-programmable gate arrays (FPGAs), etc.","Software or firmware for implementing the techniques introduced here may be stored on a machine-readable storage medium and may be executed by one or more general-purpose or special-purpose programmable microprocessors. A \u201cmachine-readable medium\u201d, as the term is used herein, includes any mechanism that can store information in a form accessible by a machine (a machine may be, for example, a computer, network device, cellular phone, personal digital assistant (PDA), manufacturing tool, any device with one or more processors, etc.). For example, a machine-accessible medium includes recordable\/non-recordable media (e.g., read-only memory (ROM); random access memory (RAM); magnetic disk storage media; optical storage media; flash memory devices; etc.), etc.","The term \u201clogic\u201d, as used herein, can include, for example, special-purpose hardwired circuitry, software and\/or firmware in conjunction with programmable circuitry, or a combination thereof.","Although the present invention has been described with reference to specific exemplary embodiments, it will be recognized that the invention is not limited to the embodiments described, but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly, the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["One or more embodiments of the present invention are illustrated by way of example and not limitation in the figures of the accompanying drawings, in which like references indicate similar elements and in which:",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIGS. 7 through 11"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 12"}]},"DETDESC":[{},{}]}
