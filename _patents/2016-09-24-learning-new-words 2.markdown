---
title: Learning new words
abstract: Systems and methods are disclosed for a server learning new words generated by user client devices in a crowdsourced manner while maintaining local differential privacy of client devices. A client device can determine that a word typed on the client device is a new word that is not contained in a dictionary or asset catalog on the client device. New words can be grouped in classifications such as entertainment, health, finance, etc. A differential privacy system on the client device can comprise a privacy budget for each classification of new words. If there is privacy budget available for the classification, then one or more new terms in a classification can be sent to new term learning server, and the privacy budget for the classification reduced. The privacy budget can be periodically replenished.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09645998&OS=09645998&RS=09645998
owner: Apple Inc.
number: 09645998
owner_city: Cupertino
owner_country: US
publication_date: 20160924
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATIONS","TECHNICAL FIELD","BACKGROUND","SUMMARY OF THE DESCRIPTION","DETAILED DESCRIPTION"],"p":["This application claims priority under 35 U.S.C. \u00a7119(e) of U.S. Patent Application No. 62\/348,988, filed Jun. 12, 2016, and entitled, \u201cLEARNING NEW WORDS,\u201d and U.S. Patent Application No. 62\/371,657, filed Aug. 5, 2016, entitled \u201cLEARNING NEW WORDS,\u201d both of which are incorporated herein by reference to the extent that they are consistent with this disclosure.","This application is related to U.S. patent application Ser. No. 15\/275,356, filed Sep. 24, 2016, and entitled \u201cLEARNING NEW WORDS,\u201d which is incorporated herein by reference to the extent that is it consistent with this disclosure.","This disclosure relates to the field of a server learning new words generated on a client device.","A user of a client device relies on one or more dictionaries of words for spell checking, suggesting words during typing, and other uses of known words. Such client dictionaries are difficult to keep updated with new words that may become popular through crowdsourced usage of words without compromising privacy.","Current servers can learn the words that users are typing by examining clear text that users have typed when utilizing the servers. For example, some prior art text message services and email services (collectively, messages) receive messages in clear text. Message servers that route messages to client devices can read the clear text and use the words obtained from the clear text of user messages to present advertising to the users. However the server-learned words remain on the server and do not update an on-device dictionary to include the new words. Also, usage of clear text by servers compromises the privacy of a user. In addition, new words generated on a client device, such as words that are used within documents on the client device and are not transmitted to a server, cannot be learned by the server because the words are localized to the client device. Further, if the client device utilizes an end-to-end encrypted messaging service, such as Apple\u00ae iMessage, then a server cannot learn the words contained in the user message at all and thus a server cannot update a user client dictionary using crowdsourced data.","Systems and methods are disclosed for a server learning new words generated by user client devices in a crowdsourced manner while maintaining local differential privacy of client devices. In a crowdsourced, client\/server environment, local differential privacy introduces randomness into user data prior to a client sharing the user data with a server. A server can learn from the aggregation of the crowdsourced data of all clients, but the server cannot learn the data provided by any particular client.","Local differential privacy introduces, in one embodiment, randomness to client user data prior to sharing the user data. Instead of having a centralized data source D={d1, . . . , dn}, each data entry dbelongs to a separate client i. Given the transcript Tof the interaction with client i, it is not possible for an adversary to distinguish Tfrom the transcript that would have been generated if the data element were to be replaced by null. The degree of indistinguishability is parameterized by \u03b5, typically considered to be a small constant. The following is a formal definition of local differential privacy.","Let n be the number of clients in a client-server system, let \u0393 be the set of all possible transcripts generated from any single client-server interaction, and let Tbe the transcript generated by a differential privacy algorithm A while interacting with client i. Let d\u2208S be the data element for client i. Algorithm A is \u03b5-locally differentially private if, for all subsets T\u0393, the following holds:",{"@attributes":{"id":"p-0010","num":"0009"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["\u2200","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"i","mo":"\u2208","mrow":{"mo":["[","]"],"mi":"n"}}},{"mi":"d","mo":"\u2208","mrow":{"mrow":[{"mi":"S","mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"ln","mo":"\u2062","mfrac":{"mrow":[{"mi":"Pr","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":{"mrow":{"mi":["Ti","\u03b5","\u0393"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}]},"mo":"|","mi":"di"},"mo":"=","mi":"d"}}},{"mi":"PR","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":{"mrow":{"mi":["Ti","\u03b5","\u0393"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}]},"mo":"|","mi":"di"},"mo":"=","mi":"null"}}}]}}}},{"mi":"\u025b","mo":"."}],"mo":"\u2264"}}],"mo":","}}}},"Here, d=null refers to the case where the data element for client i is removed.","The systems and methods disclosed herein include an \u03b5-local differentially private count-median-sketch (CMS) and a Hadamard \u03b5-local differentially private count-median-sketch (CMS) that compare favorably to prior art methods, with respect to error, communication load, space used, and client and server computation, while preserving user privacy, as shown in the table below.",{"@attributes":{"id":"p-0013","num":"0012"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"center"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}},{"entry":[{},{},{},{},"Client","Server"]},{"entry":[{},{},"Commun-",{},"Compu-","Compu-"]},{"entry":[{},"Error","ication","Space","tation","tation"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Applicant's CMS",{"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"\u0398","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mn":"1","msqrt":{"mi":"n"}}}}}}},"O({square root over (n)})","O({square root over (n)})","O({square root over (n)})","O({square root over (n)})"]},{"entry":{}},{"entry":["Applicant's Hadamard CMS",{"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"\u0398","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mn":"1","msqrt":{"mi":"n"}}}}}}},"O(1)","O({square root over (n)})","O(log n)","O({square root over (n)})"]},{"entry":{}},{"entry":["Prior art (Bassily & Smith)",{"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"\u0398","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mn":"1","msqrt":{"mi":"n"}}}}}}},"O(1)","O(n)","O(log n)","O(n)"]},{"entry":{}},{"entry":["Prior art (Hsu, Khanna, Roth)",{"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"O","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mn":"1","msup":{"mi":"n","mfrac":{"mn":["1","6"]}}}}}}}},"O(n)","O(n)","O(log n)","O(n)"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}}}}},"In an embodiment, a client device can determine that a word typed on the client device is a new word that is not contained in a dictionary or asset catalog on the client device. New words can be associated with classifications such as entertainment, health, finance, etc. A classification is a conglomeration of similar types of information. In an embodiment, each classification can be associated with one or more sessions, wherein each session can be associated with an application or product type. For example, the health classification can be associated with a personal fitness or health application. New words generated by the health or fitness application can be classified as health words. For example, \u201czika,\u201d a trending medical term, could be classified as a health word. Similarly, the classification can be associated with a finance application or a finance tab of a browser session. New words generated by a finance application or finance tab can be classified in the finance classification. For example, \u201ccorporate inversion,\u201d a trending financial term, could be classified in the financial classification. A differential privacy system on the client device can comprise a privacy budget for each classification of new words. New words generated by a user can be stored in a transmission buffer in preparation for being transmitted to a new word learning server. Words can be stored in the transmission buffer, organized by classification. It is possible to store more words in the transmission buffer than there is privacy budget available to transmit the words. To preserve the privacy budget, the transmission buffer can be periodically sampled to obtain a word for transmission to the new word learning server. If there is sufficient privacy budget available for the classification, the sampled word can be segmented into n-grams, an n-gram can be selected from the n-grams and processed using local differential privacy, then transmitted to a server. N-grams can be selected to be a particular length, such as 1 character (one-gram), 2 characters (bi-gram), etc. Throughout the disclosure, the term \u201cn-gram\u201d is used to generically refer to a sequence of characters having a specified length for a process. In an embodiment, a length of 2 is selected (bi-gram) to reduce search space complexity. Longer, or shorter, n-grams can be used. In an embodiment, an n-gram length can be selected based upon the language of the words to learn. In an embodiment, the client device can use local differential privacy to introduce randomness in the client data prior to sharing the data with a server that will learn the new words. In an embodiment, a server can test differentially private data received from a plurality of clients to determine whether the amount of randomization in the differentially private data is sufficient to maintain differential privacy of client data.","In an embodiment a non-transitory computer readable medium can store executable instructions, that when executed by a processing system, can perform any of the functionality described above.","In yet another embodiment, a processing system coupled to a memory programmed with executable instructions can, when the instructions are executed by the processing system, perform any of the functionality described above.","Some embodiments described herein can include one or more application programming interfaces (APIs) in an environment with calling program code interacting with other program code being called through the one or more interfaces. Various function calls, messages or other types of invocations, which further may include various kinds of parameters, can be transferred via the APIs between the calling program and the code being called. In addition, an API may provide the calling program code the ability to use data types or classes defined in the API and implemented in the called program code.","Other features and advantages will be apparent from the accompanying drawings and from the detailed description.","The present disclosure recognizes that the use of personal information data collected from a large population of users, in the present technology, can be used to the benefit of all or many users. For example, the words that are introduced to the popular lexicon can be identified and included in on-device dictionaries. Accordingly, use of such personal information data enables calculated control of the delivered content. Further, other uses for personal information data that benefit the user are also contemplated by the present disclosure.","The present disclosure further contemplates that the entities responsible for the collection, analysis, disclosure, transfer, storage, or other use of such personal information data will comply with well-established privacy policies and\/or privacy practices. In particular, such entities should implement and consistently use privacy policies and practices that are generally recognized as meeting or exceeding industry or governmental requirements for maintaining personal information data private and secure. For example, personal information from users should be collected for legitimate and reasonable uses of the entity and not shared or sold outside of those legitimate uses. Further, such collection should occur only after receiving the informed consent of the users. Additionally, such entities would take any needed steps for safeguarding and securing access to such personal information data and ensuring that others with access to the personal information data adhere to their privacy policies and procedures. Further, such entities can subject themselves to evaluation by third parties to certify their adherence to widely accepted privacy policies and practices.","Despite the foregoing, the present disclosure also contemplates embodiments in which users selectively block the use of, or access to, personal information data. That is, the present disclosure contemplates that hardware and\/or software elements can be provided to prevent or block access to such personal information data. For example, in the case of advertisement delivery services, the present technology can be configured to allow users to select to \u201copt in\u201d or \u201copt out\u201d of participation in the collection of personal information data during registration for services. In another example, users can select not to provide location information for targeted content delivery services. In yet another example, users can select to not provide precise location information, but permit the transfer of location zone information.","In the following detailed description of embodiments, reference is made to the accompanying drawings in which like references indicate similar elements, and in which is shown by way of illustration manners in which specific embodiments may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention, and it is to be understood that other embodiments may be utilized and that logical, mechanical, electrical, functional and other changes may be made without departing from the scope of the present disclosure. The following detailed description is, therefore, not to be taken in a limiting sense, and the scope of the present invention is defined only by the appended claims.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 1","b":"100"},"Client devices , each associated with a user in a large plurality of users (crowdsource), can be coupled to a one or more new term learning server(s)  (\u201cterm learning server \u201d) via network . Each client device  can segment a new word into n-grams and send a differentially private sketch of the word and n-grams of the word to a term learning server. A sketch of a word is a computed, encoded representation of the word. The purpose of the sketch is to transmit the encoded representation of the word (sketch) to the server rather that the clear text of the word, so that the server cannot directly learn the word transmitted by only the one client. In a crowdsourced, client\/server environment, a local differential privacy system generates the encoded representation such that randomness is introduced to client data (word) prior to a client sharing the word with a server. A server can learn the word from the aggregation of the crowdsourced data of all clients, but cannot learn the word provided by any particular client. Collectively, the differentially private sketches received from the large plurality of client devices  comprise crowdsourced data from which term learning server  can learn new words used among the large plurality of client devices , while maintaining privacy of each of the client devices . Client-side (local) differential privacy implemented in a crowdsourced data environment ensures that the term learning server  learns the new words of all client devices  without exposing whether any particular client device  uses the new words. Client device  can comprise any type of computing device such as a desktop computer, a tablet computer, a smartphone, a television set top box, or other computing device  such as iPhone\u00ae, Apple\u00ae Watch, Apple\u00ae TV, etc., as described below with reference to .","Network  can be any type of network, such as Ethernet, WiFi, Token Ring, Firewire, USB, Fiber Channel, or other network type.","Term learning server  can comprise one or more hardware processors, memory, storage devices such as one or more hard disks, solid state storage devices, CD-ROM storage, DVD-ROM storage, storage appliances, etc. Exemplary components of term learning server  are described below with reference to .",{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 2","b":"100"},"Internal components of client device  can include a plurality of storages -, a differential privacy engine (DPE)  that can comprises a differential privacy daemon  and a differential privacy framework or application programming interface (API) , and a plurality of applications , e.g. App 1, App2, and App 3. APIs are described in detail, below, with reference to .","Storages - can include a blacklist , a term learning buffer , an asset catalog , and private dictionary . Blacklist  can be used to determine words that are not to be sent to term learning server . A user may prefer to blacklist certain words as having a high level of privacy to the user such that the user does not want to transmit word to the term learning server  no matter how great the guarantee of privacy from the term learning server . Such words may include proper names, e.g. family members or surnames, proprietary technology words, and other words a user may proactively choose to keep private using blacklist .","Blacklist storage  can be used to store words that have been previously transmitted by client device  to term learning server , but the client device  has not yet received an updated asset catalog  from new term learning server  to replace the client device asset catalog . In an embodiment, differential privacy engine  can check the blacklist storage  before processing a word (e.g. generating differentially private n-grams). In an embodiment, differential privacy engine (DPE)  of a client device  sends a word to term learning server  only once. To preserve a privacy budget of the client device , a word that has been added to transmitted words storage  may not be re-sent to the term learning server . Once the client receives an updated asset catalog , words that appear within blacklist storage , that are now contained in the updated asset catalog , can be deleted from blacklist storage .","Privacy budget is a quantity that ensures the privacy of an individual is not compromised after repeated donation of information to the term learning server . A privacy budget E quantifies the amount of information leaked by a client device  to a server by providing the differentially private information to the server. Every submission to a server of differentially private information, e.g. a new word, consumes a portion \u03b5 of the privacy budget E for the client device . If a client device  submits k pieces of information through a privacy channel to a server, then \u03b5\u2266E\/k to ensure that the overall privacy budget E is not violated. A separate privacy budget is allocated to each classification of information. Each time a word is transmitted to term learning server , a privacy budget for a classification of the word is charged or reduced by some amount. For example, in the keyboard usage classification, if a client device transmits the words \u201czika\u201d and \u201cebola\u201d to the term learning server , the client device keyboard classification budget would be charged a portion of the privacy budget E for the keyboard classification for each transmitted word.","When data for a classification is purged from term learning server , it is possible to replenish or increase the privacy budget for the classification on a client device . Alternatively, the privacy budget for the classification can be replenished periodically on the client device . In an embodiment, replenishment of the client device  privacy budget for a classification can be synchronized with purging of client device data for one or more client devices on term learning server  or purging of all client device data on the term learning server . In an embodiment, replenishment of a client device privacy budget for a classification of words can be asynchronous with term learning server  purging client device data for a plurality of client devices .","A term learning buffer  can comprise a storage that holds candidate words for transmission to term learning server . A user may generate more new words than can be sent within a privacy budget for a classification of words. Thus, DPE  can store candidate words in term learning buffer , then sample the buffer later to determine a random candidate word to send to term learning server . Term learning buffer  can also store words that have been sampled from the candidate words and selected for transmission to the term learning server . In an embodiment, words are stored in term learning buffer  by classification. Each classification can have a privacy budget.","Client device  can further include a private dictionary  that stores words that a user of a client device  may want to consider familiar or frequent, i.e., known to the particular client device . In an embodiment, the user can designate a word in private dictionary  as eligible, or ineligible, for sending to the term learning server . Differential privacy engine  can receive a word from an application  and access the private dictionary  to determine whether the word is eligible to be sent to term learning server .","Term learning server  can comprise a module to receive data , a module to classify received data  according to a classification system, and a job to learn new words  from received, de-identified sketch data. Term learning server  can further include one or more storages, including an n-gram\/position frequencies storage , an asset catalog , and an updated asset catalog . A module to update clients  can publish the asset catalog update  to one or more client devices .","Receive module  can asynchronously receive sketches of n-grams of new words for a large plurality of client devices  (\u201ccrowdsourced data\u201d). Receive module  can remove from the received sketch data any latent identifiers, such as IP address, meta data, session identifier, or other data that might identify a particular client device  that sent the sketch data.","Classify received data module  can extract classification data from the received sketches and group received sketch data by classification. For example, classify received data module  can receive sketches for the new words and group these sketches according to the keyboard usage classification.","Learn new terms job  can periodically process the received, de-identified, and classify sketch data received from the large plurality of client devices . Learn new terms job  can include operations that include accumulating frequencies of received n-grams, generating permutations of n-grams, trimming the permutations of n-grams, and determining candidate new words from the permutations of n-grams. Learn new terms job  can also update asset catalog  to generate asset catalog update with updated frequencies of known words.",{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIGS. 3A and 3B","b":"300"},"In operation , differential privacy engine (DPE)  can receive a new word from an application . The application  can identify a new word by comparing the new word to dictionaries included on the client device . If a word is not included in the dictionaries, then application  can determined that the new word is to be sent to the DPE . An application can be an email application, a messaging application, a word processing application, a web browser, a client device browser, an online store, or any other application. An application  can determine a classification (class) for the word. A class can be a language, e.g. English or Chinese. In an embodiment, a class can be shared by a plurality of applications . In an embodiment, a class can be health, finance, legal terms, or other use case classification. As an example, DPE  can receive the word \u201czika\u201d from a messaging application  and determine that the word is associated with the keyboard usage classification. As another example, the DPE  can receive the number of steps a user takes over a period of time from a fitness application and determine that the number of steps is associated with a health classification. Each classification of words can have its own privacy budget.","In operation , DPE  can access asset catalog  to determine whether the word received in operation  is already known to term learning server , as evidenced by the presence of the word in asset catalog  or private dictionary . If the word is in the asset catalog  or in the private dictionary , then the method  ends. Otherwise the method  continues at operation .","In operation , application  or DPE  can determine whether the word is stored in blacklist storage . If the word is stored in blacklist storage  then method  ends. Otherwise, method  resumes at operation .","In operation , DPE  can determine whether the word has been previously processed by DPE . A previously processed word can include a term that has been previously transmitted to term learning server  by this client device  but is not yet found in an updated asset catalog  on the client device . A word that has been previously processed can also be a word that is stored in the learning buffer  that has not yet been transmitted to term learning server , but has been processed by DPE  on client device . If the word has been previously processed, then the method  ends. Otherwise, method  resumes at operation .","In operation , the word can be stored in a sample buffer or queue in learning buffer . After operation , method  resumes at operation  as described below with reference to .","Words can be held in learning buffer  such that a batch of words is gathered together for sending to term learning server  within a time interval. Each time a word is sent, a portion of the privacy budget for a classification is charged. To preserve privacy budget for each classification of words, terms are held in a learning buffer , then, after an interval of time, a word is selected from a classification in the learning buffer  for processing. In an embodiment, the words in the buffer are processed in a queue order. In embodiment, a word is selected at random from the buffer in accordance with a policy. This process slows the rate at which new words are sent to the term learning server  and extends the life of the privacy budget. In an embodiment, DPE  can contain logic that determines when a privacy budget for a classification is depleted. DPE  can then monitor the elapsed time before the privacy budget is replenished. The time interval between client intervals of processing can be extended or contracted, based upon the amount of privacy budget available at any time. Before selecting a word, it can be determined whether there is privacy budget available to send the word to the new term learning server . A word may not be processed if there is no privacy budget available for the classification of the word.","In , in operation , it can be determined whether an update interval has expired. If not, then the update interval can be periodically rechecked in operation  until the interval has expired. The update interval can be used to meter the donation of information from the client device  to the new term frequency server , to preserve privacy budget.","If in operation , the update interval has expired, then method  resumes at operation .","In operation , a word can be selected from the sample buffer in learning buffer . In an embodiment, the sample buffer can hold a plurality of words, optionally organized by classification, such that a word can be selected at random from the sample buffer for processing in preparation for transmission to term learning server . In an embodiment, words can be selected from the sample buffer in a queue order. In an embodiment, words can be selected from the sample buffer in a random order. In an embodiment, selection of words from the sample buffer can be performed in accordance with a policy. A policy can be determined per application, or per classification of words.","In operation , the selected word is processed for sending to term learning server . Operation  is described in detail below with reference to .","In operation , the processed word can be stored in a buffer in learning buffer  for transmission to new term learning server .","In operation , it can be determined whether there are more words in the sample buffer to consider for processing and transmission to term learning server . If so, then method  resumes at operation , otherwise method  resumes at operation .","In operation , an output buffer of processed words can be transmitted to term learning server .","In operation , client device can optionally receive an updated asset catalog from term learning server. The updated asset catalog  can have one or more new terms added by the term learning server  in response to crowdsourced data received by term learning server .","In an embodiment, the words that were processed and transmitted to term learning server  in operation  can be stored in blacklist storage . In an embodiment, application  that initially selected the word for processing in operation  of  can determine whether the word should be added to blacklist storage .",{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 4","b":"400"},"In operation , the new word can be segmented into n-grams. In an embodiment, an n-gram can be a single character in length (\u201cone-gram\u201d). A single n-gram length may be appropriate for a language such as Chinese, wherein a single symbol can represent one or more words. In another embodiment, n-gram length can be two characters (\u201cbi-gram\u201d). In an embodiment, an n-gram can be three, or four, characters long. Each n-gram has a position in a word. For example, if a new word is, \u201cbazinga,\u201d and the n-gram length is two, then a first n-gram would comprise \u201cba,\u201d a second n-gram would comprise \u201czi,\u201d a third n-gram would comprise \u201cng,\u201d and a fourth n-gram would comprise \u201ca<null>.\u201d","In operation , a number can be generated that is a hash of the new word and associated with the new word and each n-gram of the new word (\u201ca puzzle piece\u201d). In an embodiment, the hash can comprise the SHA256 hash algorithm, or other hash algorithm. The term learning server  can use the number as a puzzle piece to associate n-grams together in combinations to identify new words at the new term learning server .","In operation , DPE  can apply a differential privacy algorithm to the new word and to a selected n-gram of the new word. Operation  is described in detail with reference to , below.","In operation , DPE  can transmit the differentially private word and selected differentially private n-gram of the word to term learning server , along with selected n-gram position data, and class information of the new word.","In operation , DPE  can charge the client device privacy budget for the classification of the new word transmitted to the term learning server . For example, after the DPE  transmits the differentially private data to the term learning server , the privacy budget for the classification of the new term can be reduced or adjusted to reflect the transmission.","In operation , DPE  can periodically replenish or increase the privacy budget for the classification on the client device . In an embodiment, replenishing or increasing the privacy budget for a classification is asynchronous with the transmission of new term learning information in operation .",{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 5","b":"500"},"A sketch provides a succinct data structure to maintain a frequency of a domain of elements S={s, . . . , s} present in a data stream D={d, . . . }. Let H={h, . . . , h} be a set of k pair-wise independent hash functions such that each h\u2208H is h: S\u2192[m]. Client and server differential privacy algorithms can agree on a common set of k pair-wise independent hash functions H={h, . . . , h} which map to [0 . . . m). In an embodiment, m can be \u221a{square root over (n)}, wherein n is a number of client samples of data to be collected by the server. The value m can be a nearest power of 2 to the value of \u221a{square root over (n)}. In an embodiment, k can be approximately 8\u00b7ln(p), wherein p is approximately equal to |S|; the count of data items in S for the classification of terms.","A client-side local differentially private sketch can be one of two types: (1) an \u03b5-local differentially private sketch, A, or (2) a Hadamard \u03b5-local differentially private sketch, A.","In operation , DPE  can receive the new word and n-grams as an ordered set. In an example, the candidate new word is \u201cbazinga,\u201d a word made popular by a television show. In the example, n-grams are length 2 (bi-grams), such that an ordered set of n-grams for the word \u201cbazinga\u201d is: \u201cba\u201d \u201czi\u201d \u201cng\u201d and \u201ca<null>,\u201d where null signifies the end of the word \u201cbazinga.\u201d Bazinga has been determined to be a candidate new word as shown in , above. Bazinga was not in the blacklist , bazinga was not in the learning buffer storage , bazinga was not in the asset catalog , and bazinga was not found in the private dictionary .","In operation , DPE  can convert the word to a numeric value by taking a hash of the string representation of the word, d=H(word), e.g. d=SHA256(word). The word is encoded as a number, d, in the range of 0 . . . m, using H, wherein m is the square root of the estimated size |S| of the vocabulary S of the classification. In an embodiment, d=H(word) modulo m, such that d\u03b5[0, m). The size of a vocabulary for a classification can vary by classification.","Operations  and  differ slightly as between client-side local differentially private sketch algorithms Aand A. The operations  and  for Awill be described first.","Input for the client-side \u03b5-local differentially private algorithm, A, can include: (1) privacy parameter, \u03b5; (2) hashing range, m; (3) k pair-wise independent hashing functions H={h, . . . , h} with each h: S\u2192[m]; and (4) data element: d\u2208S.","For algorithm A, in operation , a noise constant",{"@attributes":{"id":"p-0081","num":"0080"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["c","\u025b"]},"mo":"\u2190","mfrac":{"mrow":[{"msup":{"mi":["\u2147","\u025b"]},"mo":"+","mn":"1"},{"msup":{"mi":["\u2147","\u025b"]},"mo":"-","mn":"1"}]}},"mo":","}}},"br":{},"sub":["\u03b5","\u03b5"],"sup":"m"},"In operation , the sketch for the word (or n-gram) can be generated with Awith the following operations.\n\n",{"@attributes":{"id":"p-0083","num":"0084"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mfrac":{"msup":{"mi":["\u2147","\u025b"]},"mrow":{"msup":{"mi":["\u2147","\u025b"]},"mo":"+","mn":"1"}}}},"ul":{"@attributes":{"id":"ul0003","list-style":"none"},"li":{"@attributes":{"id":"ul0003-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0004","list-style":"none"},"li":"3."}}}},{"@attributes":{"id":"p-0084","num":"0086"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["v","priv"]},"mo":"=","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mo":["(",")"],"mfrac":{"mrow":{"mrow":{"mrow":[{"mi":"v","mo":"\u2061","mrow":{"mo":["[","]"],"mi":"j"}},{"mi":"b","mo":"\u2061","mrow":{"mo":["[","]"],"mi":"j"}}],"mo":"*"},"mo":"+","mn":"1"},"mn":"2"}},{"mo":"\u2200","mrow":{"mi":"j","mo":"\u2208","mrow":{"mo":["[","]"],"mi":"m"}}}],"mo":","}}}}},"ul":{"@attributes":{"id":"ul0005","list-style":"none"},"li":{"@attributes":{"id":"ul0005-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0006","list-style":"none"},"li":"Return vector vand the chosen hash function h."}}}},"If the client, instead, generates the Hadamard version of the \u03b5-local differentially private sketch using the Aalgorithm, the inputs to Acan be: (1) privacy parameter, \u03b5; (2) hashing range m; (3) k pair-wise independent hashing functions H={h, . . . , h} with each h: S\u2192[m]; and (4) data element d\u2208S.","Operations  and , below, form a part of the operations of the algorithm Athat generates the Hadamard version of the \u03b5-local differentially private sketch.","In operation , a constant",{"@attributes":{"id":"p-0088","num":"0091"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["c","\u025b"]},"mo":"\u2190","mfrac":{"mrow":[{"msup":{"mi":["\u2147","\u025b"]},"mo":"+","mn":"1"},{"msup":{"mi":["\u2147","\u025b"]},"mo":"-","mn":"1"}]}}}},"br":{},"sup":"m"},"In operation , the algorithm Afurther includes the following operations:\n\n",{"@attributes":{"id":"p-0090","num":"0095"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["v","Hadamard"]},"mo":"\u2190","mrow":{"mfrac":{"mn":"1","msqrt":{"mi":"m"}},"mo":["\u00b7","\u00b7"],"msub":{"mi":["H","m"]},"mi":"v"}},"mo":","}}},"br":{},"sub":"m ","ul":{"@attributes":{"id":"ul0009","list-style":"none"},"li":{"@attributes":{"id":"ul0009-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0010","list-style":"none"},"li":"3. Sample an index j, independent and identically distributed in [m] and a bit b\u2208{\u22121,1} such that b is \u201c1\u201d with probability"}}}},{"@attributes":{"id":"p-0091","num":"0097"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"msup":{"mi":["\u2147","\u025b"]},"mrow":{"msup":{"mi":["\u2147","\u025b"]},"mo":"+","mn":"1"}},"mo":"."}}},"ul":{"@attributes":{"id":"ul0011","list-style":"none"},"li":{"@attributes":{"id":"ul0011-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0012","list-style":"none"},"li":"4. Return c\u00b7b\u00b7v[j], the selected hash function h, and the selected index j."}}}},"In operation , an n-gram is randomly selected of the new word \u201cbazinga\u201d from the set of n-grams {ba, zi, ng, a<null>} that make up the word. For example, n-gram \u201cba\u201d can be randomly selected from the set of n-grams for the new word, \u201cbazinga.\u201d In an embodiment, the set of n-grams can be an ordered set.","In operation , DPE  can convert the randomly selected n-gram to a numeric value by taking a hash of the string representation of the n-gram with the puzzle piece (PP) prepended, d_n-gram=H(PP, n-gram), e.g. d_n-gram=SHA256(PP, n-gram). The n-gram and puzzle piece can be encoded as a number, d_n-gram in the range of 0 . . . m using H. In an embodiment, d_n-gram=H(PP, n-gram) modulo m, such that d_n-gram \u2208 [0, m).","In operation , DPE  can initialize a sketch of the d_n-gram with a noise constant, analogous to the operation  described above.","In operation , DPE  can generate a differentially private sketch of the d_n-gram as described above in operations  above.",{"@attributes":{"id":"p-0096","num":"0103"},"figref":"FIG. 6","b":"600"},"In operation , term learning server  de-identifies word and n-gram sketch data received from clients . De-identification can include removing an internet protocol (IP) address from the received data, removing any metadata that identifies, or can be used to identify, a particular client with reasonable specificity.","In operation , term learning server  selects a batch of batch of differentially private words and n-grams for a large plurality of clients from the received and de-identified client data .","In operation , term learning server  can generate, or retrieve, a sketch for each known n-gram. A sketch of an n-gram can be used as an index to match a received n-gram sketch with a known n-gram sketch so that the frequency of the n-gram at a position can be accumulated. In an embodiment, frequently occurring n-grams can have a pre-generated sketch stored in tuple\/position database . For example, in a first position, the n-gram, \u201cth\u201d is a commonly occurring n-gram as it starts many words. In an embodiment, a sketch of each n-gram is stored in tuple\/position database  for all n-grams, e.g. \u201caa,\u201d \u201cab,\u201d \u201cac,\u201d etc.","In operation , learn new words job  can select all n-grams from the received data  that are in the first position of the term from which the n-gram was extracted, e.g. the first position bi-gram in the word \u201cterm\u201d is \u201cte.\u201d","In operation , term learning data can be reset. Term learning data can include a histogram of n-gram frequencies per n-gram position, a histogram of new term frequencies, an n-gram permutations data structure, and other data structures necessary to implement the word learning logic herein.","In operation , for each selected n-gram sketch for the position, a matching sketch is looked up and incremented in a histogram of n-grams for the position. The specific operations for updating the histogram of n-grams at a position can depend upon whether the client used the \u03b5-local differentially private sketch algorithm, A, or the Hadamard \u03b5-local differentially private sketch algorithm A. The operations for each are described below.","In the case that the client used the Aalgorithm to generate the selected n-gram sketch, then in operation , the selected sketch data, vector v, is added to the matching sketch data W, as follows:","For row W, corresponding to the selected hash function that was used to generate v, set Wto W+v.","In the case that the client used the Aalgorithm to generate the selected sketch, then in operation , the selected sketch data, vector v, is added to the matching sketch data W, as follows:\n\n","In operation , the frequency histogram of n-grams for the position can be ordered by n-gram frequency, from highest to lowest. A noise floor frequency value can be determined. N-grams in the ordered histogram for the position having a frequency below the noise floor can be discarded and excluded from further processing. In an embodiment, if there are \u201cn\u201d samples of n-gram data in the histogram, then the noise floor=",{"@attributes":{"id":"p-0107","num":"0117"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"c","mo":"*","mfrac":{"msqrt":{"mi":"n"},"mi":"\u025b"}},"mo":","}}},"br":{}},"In operation , it can be determined whether there are more positions of clients' differentially private words and n-grams to process. If so, then the method  continues at operation . Otherwise the method  continues at operation .","In operation , learn new terms job  can learn new words for the selected class of words using the accumulated differentially private n-gram sketches and word sketches. Operation  is described in detail, below, with reference to .","In operation , learn new terms job  can optionally purge some or all of the received differentially private n-gram and word sketch data to help maintain the differential privacy of users of the client devices .",{"@attributes":{"id":"p-0111","num":"0121"},"figref":["FIG. 7","FIG. 6"],"b":["700","632"]},"In operation , new term learning server  can generate an n-gram sketch as H(PP, n-gram) for each PP and each possible n-gram. Any particular generated H(PP, n-gram) is the same as if H(PP, n-gram) were generated on a client device . Server H(PP, n-gram) values can be stored in association with the puzzle piece used to generate H(PP, n-gram), so that H(PP, n-gram) values can be easily grouped by puzzle piece by new term learning server .","In operation , new term learning server  can use the histogram of operation  of  to determine a frequency of received from crowdsourced client data of each H(PP, n-gram) at each position in the histogram. New term learning server  can group H(PP, n-gram) at each position by puzzle piece (PP) and n-gram sketch such that the frequency of a particular n-gram sketch having a particular puzzle piece can be determined.","In operation , a number of puzzle piece groups of n-grams, \u201cx\u201d, having a highest frequency at a position can be determined that represents the number of puzzle piece groups of n-grams to use at each position for generating candidate words. The value x can depend upon the a maximum number of candidate words that the term learning server  is configured to process. For example, if a server is configured to process a maximum of ten million words comprising a maximum of 7 bi-grams (therefore a maximum word length of 14 symbols), then the value x can be determined as:",{"@attributes":{"id":"p-0115","num":"0125"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"x","mo":"=","mrow":{"mroot":{"mrow":{"mn":["10","000","000"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":","},{"mtext":","}]},"mn":"7"},"mo":"=","mrow":{"mn":"10","mo":["\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":"bi-grams"}]}}}}},"br":{}},"In operation , up to \u201cx\u201d puzzle piece groups of n-grams having the highest frequency for the position can be selected for generating candidate words.","In operation , a set of ordered combinations of n-gram s can be generated from the selected puzzle piece groups of n-grams at each position to generate candidate new words. Each of the \u201cx\u201d n-grams having the same puzzle piece selected at the first position can be permuted with each of the \u201cx\u201d n-grams having the same puzzle piece at the second position, etc., for all positions that the server is configured to process. The puzzle piece signifies an n-gram at a position that belongs with an n-gram at another position based on the n-grams having been obtained from the same word having the same puzzle piece value.","In operation , candidate words having a frequency less than a noise floor can be discarded. In an embodiment, the noise floor is calculated similarly to the noise floor for n-grams, scaled by the maximum number of configured n-grams per candidate word maximum.","The search space of candidate words can alternatively, or in addition, be trimmed by a number of techniques. In an embodiment, natural language processing (NLP) techniques can be applied to trim combinations that are illogical or do not occur in a particular vocabulary. For example, an n-gram \u201ccc\u201d would not be followed by any of the following n-grams: \u201coo\u201d \u201cii\u201d \u201cuu\u201d \u201czz\u201d, etc. Permutations of such n-gram sequences can be trimmed from the set of combinations.","In an embodiment, an A* search can be performed, wherein the shortest combinations are traversed first to determine candidate words. NLP can be used to find combinations that are not viable and thus can be trimmed from the set of candidate combinations.","The above methods of trimming the search space of candidates can be used alone, or in combination, to trim the search space before using the n-grams to learn new words.","In operation , a candidate word can be selected from the candidate words.","In operation , it can be determined whether the candidate new word is in an existing asset catalog or dictionary of words. Theoretically, a newly found candidate word should not be found in an asset catalog or dictionary on new term learning server . Client devices should have the latest asset catalog containing the latest known words, and the asset catalog is used by the client device to help eliminate known words from client and server processing. It is possible, however, that a client may not have upgraded to the latest asset catalog and thus the client may have sent a word that appears new to the client but is known to the server.","If, in operation , it is determined that the candidate word is in an existing asset catalog or dictionary of words, then in an embodiment, a frequency of the word can optionally be increased.","If, in operation , it is determined that the candidate word is a new word, then the new word can be added to an updated asset catalog .","In operation , it can be determined whether the permutations tree traversal is complete. If not, then method  continues at operation , otherwise method  continues at operation .","In operation , term learning server  can optionally transmit an updated asset catalog to one or more client devices.",{"@attributes":{"id":"p-0128","num":"0138"},"figref":"FIG. 8","b":"800"},"In operation , a differentially private sketch can be selected for each of \u201cn\u201d clients represented in the received and de-identified data  on new term learning server . A bit is generated from the sketch of each client by XORing the 1-bit vector of each row of the sketch for the client.","In operation , a loop iterator variable, i, is set to 1.","In operation , each bit b. . . bof a row of sketch i, compute:\n\n.\n","In operation , increment client counter variable i.","In operation , it can be determined whether there are more client sketches to process. If so, then method  continues at operation . Otherwise, method  continues at operation .","In operation , a sum of bits is computed using all of the B=1 . . . n, computed above for one sketch for each client i, of n clients. The sum A is computed as:",{"@attributes":{"id":"p-0135","num":"0145"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"A","mo":"=","mrow":{"mfrac":{"mn":"1","mi":"n"},"mo":"\u00b7","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msub":{"mi":["B","i"]}}}}}}},"In operation , a randomization tolerance is computed and it can be determined whether the randomization of clients is within tolerance. In an embodiment, the randomization tolerance can be computed as:",{"@attributes":{"id":"p-0137","num":"0147"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"If","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"A","mo":"-","mfrac":{"mn":["1","2"]}}}},{"mfrac":{"mn":"3","mrow":{"mn":"2","mo":"\u2062","msqrt":{"mi":"n"}}},"mo":"+","msup":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mfrac":{"mn":"2","mrow":{"mn":"1","mo":"+","msup":{"mi":["\u2147","\u025b"]}}}}},"mi":"k"}}],"mo":"\u2265"}}},"br":{}},"In an embodiment, randomization tolerance can be computed as:",{"@attributes":{"id":"p-0139","num":"0149"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"A","mo":"\u2208","mrow":{"mfrac":{"mi":"n","mn":"2"},"mo":"\u00b1","msqrt":{"mi":"n"}}}}}},"If",{"@attributes":{"id":"p-0141","num":"0151"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":{"mrow":{"mrow":{"mo":["\uf603","\uf604"],"mi":"A"},"mo":"\u2208"},"mo":")"},"mo":"\u2062","mfrac":{"mi":"n","mn":"2"}},"mo":"\u00b1","msqrt":{"mi":"n"}}}},"br":{}},"If, in operation , randomization tolerance succeeds, then in operation , a message can be generated to a user interface of new term learning server  indicating that randomization is within tolerance, otherwise in operation  and message can be generated to the user interface of new term learning server  indicating that randomization is not within tolerance, indicating that measures need to be taken to further ensure client device differential privacy. Measures could include modifying a randomization algorithm, purging more client device data and\/or purging client device data more frequently.","In  (\u201cSoftware Stack\u201d), an exemplary embodiment, applications can make calls to Services 1 or 2 using several Service APIs and to Operating System (OS) using several OS APIs. Services 1 and 2 can make calls to OS using several OS APIs.","Note that the Service 2 has two APIs, one of which (Service 2 API 1) receives calls from and returns values to Application 1 and the other (Service 2 API 2) receives calls from and returns values to Application 2, Service 1 (which can be, for example, a software library) makes calls to and receives returned values from OS API 1, and Service 2 (which can be, for example, a software library) makes calls to and receives returned values from both as API 1 and OS API 2, Application 2 makes calls to and receives returned values from as API 2.",{"@attributes":{"id":"p-0145","num":"0155"},"figref":["FIG. 10","FIG. 10","FIG. 10"],"b":"1000"},"Computing system  includes bus  or other communication device to communicate information, and processor  coupled to bus  that may process information.","While computing system  is illustrated with a single processor, computing system  may include multiple processors and\/or co-processors . Computing system  further may include random access memory (RAM) or other dynamic storage device  (referred to as main memory), coupled to bus  and may store information and instructions that may be executed by processor(s) . Main memory  may also be used to store temporary variables or other intermediate information during execution of instructions by processor .","Computing system  may also include read only memory (ROM) and\/or other static storage device  coupled to bus  that may store static information and instructions for processor(s) . Data storage device  may be coupled to bus  to store information and instructions. Data storage device  such as flash memory or a magnetic disk or optical disc and corresponding drive may be coupled to computing system .","Computing system  may also be coupled via bus  to display device , such as a cathode ray tube (CRT) or liquid crystal display (LCD), to display information to a user. Computing system  can also include an alphanumeric input device , including alphanumeric and other keys, which may be coupled to bus  to communicate information and command selections to processor(s) . Another type of user input device is cursor control , such as a touchpad, a mouse, a trackball, or cursor direction keys to communicate direction information and command selections to processor(s)  and to control cursor movement on display . Computing system  may also receive user input from a remote device that is communicatively coupled to computing system  via one or more network interfaces .","Computing system  further may include one or more network interface(s)  to provide access to a network, such as a local area network. Network interface(s)  may include, for example, a wireless network interface having antenna , which may represent one or more antenna(e). Computing system  can include multiple wireless network interfaces such as a combination of WiFi, Bluetooth\u00ae and cellular telephony interfaces. Network interface(s)  may also include, for example, a wired network interface to communicate with remote devices via network cable , which may be, for example, an Ethernet cable, a coaxial cable, a fiber optic cable, a serial cable, or a parallel cable.","In one embodiment, network interface(s)  may provide access to a local area network, for example, by conforming to IEEE 802.11 b and\/or IEEE 802.11 g standards, and\/or the wireless network interface may provide access to a personal area network, for example, by conforming to Bluetooth standards. Other wireless network interfaces and\/or protocols can also be supported. In addition to, or instead of, communication via wireless LAN standards, network interface(s)  may provide wireless communications using, for example, Time Division, Multiple Access (TDMA) protocols, Global System for Mobile Communications (GSM) protocols, Code Division, Multiple Access (CDMA) protocols, and\/or any other type of wireless communications protocol.","In the foregoing specification, the invention has been described with reference to specific embodiments thereof. It will, however, be evident that various modifications and changes can be made thereto without departing from the broader spirit and scope of the invention. The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Embodiments of the invention are illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings in which like reference numerals refer to similar elements.",{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIGS. 3A and 3B"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 10"}]},"DETDESC":[{},{}]}
