---
title: Efficient use of user-defined shaders to implement graphics operations
abstract: User-defined shaders are constructed from fragments. The shaders are identified by tags. At run-time, the tag is used to determine whether the user-defined shader has been previously compiled. If it has, the compiled version is executed. If it has not, the fragments are assembled to form the shader and the shader is run-time compiled. The compiled shader can be stored for subsequent reuse, with the tag serving as an index to the compiled version.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07015909&OS=07015909&RS=07015909
owner: Aechelon Technology, Inc.
number: 07015909
owner_city: San Carlos
owner_country: US
publication_date: 20020319
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["1. Field of the Invention","This invention relates generally to computer graphics and, more particularly, to user-defined shaders that implement graphics operations.","2. Description of the Related Art","Ever since 3D computer graphics evolved beyond wireframe rendering, shading has been a principal area of research and development. In the early days, shading primarily concerned processes by which pixel colors were applied to a surface. These days, the terms shading and shader are much broader and generally refer to any types of 3D graphics operation. Code which implements such graphics operations is commonly referred to as a shader. Examples of graphics operations that can be implemented by shaders include coordinate transformation, lighting, and determining the pixel colors across a surface. Shaders can also be used to produce geometric effects, such as skeletal animation, particle systems, or other dynamics such as textile modeling. Shaders are widely used for simulating the reflectance properties of surfaces, ranging from simple shaders describing a pattern on a surface to more sophisticated shaders modeling human skin, granite, velvet, etc. Shaders can also be used to simulate the optics in a camera lens through which a scene is viewed or to simulate the illumination properties of lights in a scene. Other examples will be apparent.","In 1988, Pixar's Renderman renderer became available. Renderman was the first widely used rendering application that supported programmable shading, although the technique was introduced commercially by Pixar with their Chap Reyes rendering system in 1986 and academically by Robert L. Cook in 1984 (\u201cShade Trees\u201d, Robert L. Cook, Computer Graphics Siggraph 1984 proceedings). Prior to programmable shading, a user of a graphics system (e.g., an applications developer) was limited to a predefined set of shading operations, which shall be referred to as \u201cstandard operations.\u201d All graphics had to be rendered using only the standard operations. If an effect was not supported by the standard operations, then the user either had to skip the effect or, if the effect was important enough, lobby the manufacturer of the graphics system to expand the set of standard operations to include the desired effect. In contrast, programmable shading allowed users to mathematically define shading functions using their own code. This resulted in a nearly infinite number of shading possibilities to simulate virtually every conceivable type of surface, lighting, atmosphere or other effect. Essentially, users could define their own shaders.","The shading techniques described above were typically first implemented as software running on general purpose computers. Such rendering software is generally used for off-line rendering, in which rendering times for each frame of a computer graphics movie can vary from seconds to days, depending on the processor performance and scene complexity. Later, as semiconductor performance increased, many shading techniques were implemented in hardware for real-time applications. In real-time applications, scenes must be rendered at interactive rates, which is usually somewhere between 10 and 100 Hz.","Due to the difficulty in meeting this performance requirement, advances in shading technology are implemented in off-line rendering systems significantly before they reach real-time renderingsystems. For example, an early implementation of real-time texture mapping occurred in the 1980's in General Electric's CompuScene III real time image generator. An early implementation of rudimentary real-time programmable shading was nVidia's Geforce3 accelerator, released in 2001. These dates are significantly later than the corresponding dates for off-line rendering systems.","Like their off-line rendering ancestors, prior to programmable shading, real-time graphics systems were based upon a predefined set of standard operations and a corresponding application programming interface (API). This predefined set of operations is also known as the fixed-function pipeline. It will also be referred to as the fixed-function mode for the graphics system. Examples of APIs that include a fixed function pipeline are OpenGL 1.1 and DirectX. Older APIs include IRISGL (SGI's API prior to OpenGL), Glide (by 3dfx), and PHIGS. The OpenGL specification describes a pipelined architecture for real-time 3D rendering. The pipeline includes stages for vertex processing, primitive processing, rasterization, texture mapping, and fragment processing. Each stage in the pipeline can implement a finite number of standard operations and the operations to be performed are described by states that are set by the user (including, for example, matrices, and lighting and material parameters).","For example, in the geometry processing stage (a combination of vertex processing and primitive assembly), the user might set state(s) to describe how texture coordinates are generated. Texture coordinates may, for example, be explicitly specified in source geometry, derived by means of a linear equation from the vertex positions of source geometry, transformed by a matrix, etc. The user sets the appropriate state(s) for the generation of texture coordinates and the graphics processor then executes the corresponding standard operation(s).","One important property of the standard operations is that they are typically \u201corthogonal.\u201d Two graphics operations are orthogonal if the state of one operation does not affect the state of the other operation. For example, consider texture coordinate generation and texture coordinate transformation. The former describes how texture coordinates are initially generated; the latter describes a matrix transformation applied to the coordinates. These two operations are orthogonal because the transformation operation functions the same regardless of how the texture coordinates are initially generated, and vice versa.","One advantage of orthogonality for users is that it simplifies the use of the graphics system because the interplay between different graphics operations is reduced. This makes it easier to understand the graphics system and also makes incremental development possible. One disadvantage of orthogonality for manufacturers of graphics systems is that each additional graphics operation supported by the fixed function pipeline geometrically increases the number of combinations of possible states that the user may set.","Take the geometry processing stage as an example. Here, the addition of new graphics operations and the corresponding proliferation of states have led to the adoption of \u201cfast paths.\u201d Modern geometry processing stages are typically implemented using programmable processors that execute microcode. The microcode implements the standard operations of the geometry processing stage of the fixed function pipeline. It is fixed function because the user cannot easily alter the microcode (e.g., it may be preloaded by the graphics system manufacturer) and therefore can only perform the standard operations supported by the microcode. The microcode authors usually start by creating a \u201cslow path,\u201d which is an all-inclusive microprogram that is capable of handling every possible combination of states supported by the fixed function pipeline. This generalized microprogram is not optimized. For example, if the user disables texture coordinate transformation, rather than skipping this operation, the generalized microprogam typically would still perform the coordinate transformation but set the transformation matrix to the identity matrix so that no actual coordinate transformation occurred.","Because most applications use only a small subset of the possible combinations of states, the microcode authors often implement \u201cfast path\u201d microprograms for specific cases. For example, if flat-shaded wireframe rendering is used frequently in CAD applications, the authors may create an optimized microprogram to implement this combination of states more efficiently. Or if a popular computer game renders textured polygons with one diffuse light and fog enabled, the authors may create another optimized microprogram to implement this combination. The graphics driver typically chooses the appropriate fast path by analyzing the state settings made by the application. If no fast path is available, the generalized slow path is executed.","The programmable pipeline or programmable mode goes one step further. In the fixed function mode, the user sets states and, based on the states, a fast path microprogram is executed if one is available. In the programmable mode, the user supplies his own microprogram (i.e., a user-defined shader). The programmable pipeline simplifies the graphics system manufacturer's job because the user (e.g., an application developer) can create shaders optimized for his particular application and can also create shaders to implement graphics operations which are not supported by the fixed function pipeline. Furthermore, the user does this without affecting the fixed function pipeline or the corresponding graphics API. Early examples of the programmable pipeline include Direct3D Vertex Shaders (a.k.a. Vertex Programs in OpenGL) and Direct3D Pixel Shaders (a.k.a. Texture Shaders and Register Combiners in OpenGL). These allow the user to write shaders (vertex shaders and pixel shaders in the examples given above) that essentially bypass the API abstraction layer and operate directly with the underlying graphics hardware (or which are optimized to run on general CPUs if there is no direct hardware support).","While the programmable pipeline gives users the flexibility to create custom shaders, it comes at a price.  (prior art) is a functional diagram of a graphics system  with a fixed function mode  and a programmable mode . Typically, the programmable pipeline  and the fixed function pipeline  are mutually exclusive. Using the programmable pipeline  means that many of the standard operations of the fixed function pipeline  are not available. For example, when a Direct3D Vertex Shader is enabled, it completely replaces the vertex processing stage of the fixed function pipeline. Suppose a user simply wants to implement a new method for deriving texture coordinates from source geometry and uses the programmable pipeline to do so. By invoking the programmable pipeline for this one operation, the user can no longer take advantage of the texture matrix, geometry transformation, lighting, or any other standard vertex operations available from the fixed function pipeline. Rather, the user must supply all of these operations himself in additional user-defined shaders. In the case of Vertex\/Pixel Shaders, some non-programmable functions of the fixed function pipeline, such as clipping and depth testing, remain when the programmable pipeline is invoked.","In other words, using shaders and the programmable pipeline shifts the burden of managing many of the features of the graphics pipeline from the graphics system manufacturer to the user. The problem of proliferating graphics operations and states now becomes the user's problem. As a result, there is a substantial barrier to entry to using shaders and there is a need for an approach which allows users to take advantage of the flexibility of the programmable pipeline while significantly reducing this barrier to entry.","The present invention overcomes the limitations of the prior art by providing user-defined shaders that are constructed from fragments. The shaders are identified by tags. At run-time, the tag is used to determine whether the user-defined shader has been previously compiled. If it has, the compiled version is executed. If not, the fragments are assembled to form the shader and the shader is run-time compiled. The compiled shader can be stored for subsequent reuse, with the tag serving as an index to the compiled version.","The present invention is particularly advantageous because it provides a way for real-time graphics applications to be constructed using programmable shading technology while maintaining the advantages of orthogonality. Furthermore, it provides the automatic creation of \u201cfast-paths\u201d for different combinations of states. It also allows users to use multiple shaders in tandem, as well as combine shaders with functionality equivalent to that provided by the fixed function pipeline. This approach also scales efficiently as the number of possible shaders multiplies exponentially. It is applicable to graphics applications based on a variety of application architectures, including scene graphs.","Specific implementations may include one or more of the following variations. In one variation, the tag includes a state vector indicating which fragment(s) are included in the shader. In another variation, a table contains records that associate previously compiled shaders with their corresponding tags. The table is consulted to determine whether it contains the tag of the current shader. If it does, it means there is a previously compiled version. If it does not, after compiling the current shader, its tag is added to the table. In one implementation, the table is a hash table. In another variation, the shader and tag represent the combination of two or more constituent shaders that are to be applied to an object.","In another aspect of the invention, a system for compiling user-defined shaders for implementing graphics operations includes control logic, a library of fragments and a fragment assembler. The control logic determines, based on the tag identifying the shader, whether the shader has been previously compiled. The fragment assembler communicates with the control logic and can access the library of fragments. If the shader has not been previously compiled, the fragment assembler assembles the fragment(s) included in the shader. The system optionally also includes a run-time compiler that compiles the assembled fragment(s).","In another aspect of the invention, a library of fragments is for building user-defined shaders which are compatible with a predefined set of standard operations (e.g., as for a fixed function pipeline). For those graphics operations that are implemented by both a standard operation and by the library of fragments, there is a substantial one to one correspondence between the standard operations and fragments in the library.","In yet another aspect of the invention, a set of graphics operations is to be performed by a graphics system having a programmable mode and a fixed function mode. The fixed function mode is for performing a predefined set of standard operations. The programmable mode is capable of executing user-defined shaders. It is determined whether the set of graphics operations is to be executed in programmable mode or in fixed function mode. If the fixed function mode is selected, the appropriate standard operations are executed. If the programmable mode is selected, the appropriate user-defined shader is executed using the techniques described above. In one implementation, a state vector identifies the specific graphics operations to be performed and the state vector is used to determine whether the set of graphics operations can be implemented by one or more standard operations.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 1B","b":["112","100","100"]},"Computer system  includes one or more central processing units (CPU), such as CPU , and one or more graphics subsystems, such as graphics pipeline . One or more CPUs  and one or more graphics pipelines  can execute software and\/or hardware instructions to implement the graphics functionality described herein. Graphics pipeline  can be implemented, for example, on a single chip, as part of CPU , or on one or more separate chips. Each CPU  is connected to a communications infrastructure , e.g., a communications bus, crossbar, network, etc. Those of skill in the art will appreciate after reading the instant description that the present invention can be implemented on a variety of computer systems and architectures other than those described herein.","Computer system  also includes a main memory , such as random access memory (RAM), and can also include input\/output (I\/O) devices . I\/O devices  may include, for example, an optical media (such as DVD) drive , a hard disk drive , a network interface , and a user I\/O interface . As will be appreciated, optical media drive  and hard disk drive  include computer usable storage media having stored therein computer software and\/or data. Software and data may also be transferred over a network to computer system  via network interface .","In one embodiment, graphics pipeline  includes frame buffer , which stores images to be displayed on display . Graphics pipeline  also includes a geometry processor  with its associated instruction memory . In one embodiment, instruction memory  is RAM. The graphics pipeline  also includes rasterizer , which is communicatively coupled to geometry processor , frame buffer , texture memory  and display generator . Rasterizer  includes a scan converter , a texture unit , which includes texture filter , fragment operations unit , and a memory control unit (which also performs depth testing and blending) . Graphics pipeline  also includes display generator  and digital to analog converter (DAC) , which produces analog video output  for display . Digital displays, such as flat panel screens can use digital output, bypassing DAC . Again, this example graphics pipeline is illustrative of the context of the present invention and not intended to limit the present invention.",{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 2","b":["200","200","200","200"]},"Shader  is an example written in the assembly language used in nVidia OpenGL Vertex Programs. In alternate embodiments, the shader may be written in other assembly languages or in a higher level shading language such as those supported by compilers such as the Stanford Shading Compiler or SGI's OpenGL Shader system. The vertex shader  computes the per-vertex attributes for cubic reflection mapping. For the purposes of this example, the shader  has been decomposed into eight shader fragments A\u2013H, surrounded by a standard header  and footer . Generally speaking, user-defined shaders can include one or more shader fragments. One advantage of defining shaders as a combination of shader fragments is that shader fragments can be reused. They also simplify the process of combining shaders, as will be further explained below.","In shader , the three fragments A\u2013C implement graphics operations which are part of the fixed function pipeline (i.e., they implement standard operations). It is also expected that many different user-defined shaders will use these shader fragments. The four fragments D\u2013G implement graphics operations which do not map uniquely to any part of the fixed function pipeline but which are expected to be frequently used in other shaders nonetheless. Fragment H is specific to this shader  and it is unlikely that other shaders would use this code.","Shaders can be decomposed into shader fragments in more than one way. For example, shader  could have been decomposed into a different number of shader fragments and\/or differently defined shader fragments. The decomposition of a shader into its constituent fragments can be done by hand but preferably is automated. For example, nVidia's NVASM shader assembler is advertised as being able to perform this task. Shaders preferably will be decomposed into shader fragments in a manner that permits significant reuse of shader fragments, fast compilation, combining and execution of shaders, and consistency between shader fragments and the standard operations of the fixed function pipeline (see  below). Put in another way, the shaders used in an application are built up from a library of shader fragments and the library preferably is selected to achieve the goals described above. The library itself may be entirely coded from scratch by the user, contain previously coded libraries (either personal or possible commercially available ones) or both.","In decomposing shaders into their constituent fragments, several issues typically are important. First, it is important to identify conflicts between different shaders. For example, two shaders might use the same texture coordinate for different purposes or in an inconsistent manner. These conflicts typically must be resolved before the shaders are compiled and preferably before run time. If the conflict between the shaders cannot be resolved through automated means, then human intervention may be required to resolve the conflict. It is even possible that the conflict is unresolvable, meaning that the shaders cannot both be used and an alternate solution is required. Second, in order to increase the modularity of the shader fragments, it is important to identify commonalities and differences between the shaders. Commonly used graphics operations preferably are coded once as a single fragment that will be included in multiple shaders. Fragments A\u2013G are examples of this type of fragment. Differences are coded as fragments that are unique to one shader. In the example of , fragment H is a shader-specific fragment.","As mentioned previously, the use of shaders and the programmable pipeline has many advantages. For example, the programmable pipeline has more flexibility and freedom, allowing the user to implement new graphical effects. The flexibility of vertex shaders allows users to implement graphics operations such as procedural geometry (e.g., cloth simulation and soap bubbles), advanced vertex blending for skinning and vertex morphing (i.e., tweening), particle systems, advanced lighting models, advanced keyframe interpolation (e.g., for complex facial expressions and speech), and real-time modifications of the perspective view (e.g., lens effects). Another advantage is that shaders can be more portable than applications based on the fixed function pipeline. The shader approach can more easily take advantage of advances in hardware capability and the addition of new instructions and registers.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":["FIG. 3","FIG. 4"],"b":["300","300","300","310","320","330","340","300","350","360","370","360"]},"In , with the exception of the fragment library , all of the components are shown as being able to communicate with each other and the picture suggests some sort of bus-like communications mechanism. Fragment library  is shown as being accessible only by the fragment assembler . These communications links are shown for convenience and are not intended to limit the architecture  to certain implementations. Alternate embodiments may couple the components in a different manner and\/or use different communications mechanisms.","First consider each component individually. The control logic  generally controls the process of compiling and executing shaders, in this example according to method . The control logic  does not necessarily have sole control over the entire process. At various points, control may be shared or transferred to other components. In some embodiments, the control logic  may also detect and\/or resolve conflicts at run time. It may also combine multiple shaders into a larger shader and then execute the larger shader (which shall be referred to as a composite shader) instead of the many constituent shaders. For example, if multiple shaders are to be applied to the same object, the control logic  might construct a single composite shader that has the same effect as the original multiple shaders. The fragment assembler  is responsible for assembling shaders to be executed from their constituent fragments. The run-time compiler  is responsible for compiling shaders at run time. The graphics engine  executes the compiled shaders.","With respect to implementation, graphics engine  typically is implemented in hardware, although it could be a software implementation or a combination of hardware and software (e.g., a chip and a low level driver). Examples of graphics engine  include graphics processors, DSPs and general-purpose microprocessors (especially if optimized for graphics processing or coupled with graphics drivers). The three components , ,  typically are implemented in software. This software could run on the graphics engine  or on other processors.","Turning to the data structures, the fragment library  is a data structure that contains the shader fragments that will be used to build shaders. The compiled shaders database  contains shaders which have been previously compiled. The table  is an index into the compiled shaders database . In one implementation, each shader is identified by a tag and each record in table  lists a tag  and a pointer  to the location in database  of the corresponding compiled shader. The data structures ,  and  are referred to as library, database and table, but this is solely for convenience. They can be implemented using any appropriate type of data structures, including for example arrays, linked-lists or hash tables.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 4","b":["400","300","410","310","310","410","310","310","310"]},"The tag can also take different forms. It can be a descriptive label or some other name, for example \u201cLighting\u201d for a shader that implements lighting. In an alternate embodiment, the tag includes a state vector that indicates which fragments are included in the shader. For composite shaders, the tag may define the shader by identifying its constituent shaders.","Once the control logic  receives  the tag, it determines , based on the tag, whether the corresponding shader has been previously compiled. In architecture , the records in table  contain the tags for shaders that have been previously compiled. In this case, control logic  references the table  and determines whether the tag for the current shader is already contained in table . If it is, then the shader has been previously compiled. The control logic  retrieves  the previously compiled shader from database  and provides  the compiled shader to the graphics engine , which executes  the shader in real time.","If the tag is not in table , the shader must be compiled before it can be executed. In this case, the control logic  instructs the fragment assembler  to retrieve the appropriate fragments from fragment library  and assemble  the fragments in the correct order. The fragment assembler  may also add syntax such as headers and footers.","The run-time compiler  compiles  the assembled shader and provides  the compiled shader to the graphics engine  for execution  in real time. The control logic  also stores  the compiled shader in database  and adds  a corresponding record to table . Hence, if the same shader is encountered later, it can be retrieved from the database  rather than recompiled.","Method  is applied to each shader in the application. If the implementation is pipelined, multiple shaders can be processed concurrently.",{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIG. 5","FIG. 5"],"b":["500","300","340","540","530","540","530","530","330","310","320","510","520","530"]},"The data structures are implemented as follows. In this system, shaders executed in the programmable pipeline are assigned handles, also known as id's. The compiled shaders are stored by driver  in program memory  and the handles are passed back to the user software module via the OpenGL API. In other words, the compiled shader database  is implemented in program memory  and maintained by driver . The tags for shaders are bit-based state vectors, as will be further described below, and table  associates the state vectors (i.e., tags) with the corresponding handles (i.e., pointers). If there are a large number of state vectors, a hash table A can be used to index into the complete table B. The control logic software  maintains the hash table A and the complete table B. The fragment library  is implemented as a library  of individual ASCII files, one file per fragment. The fragments are defined prior to run time and loaded into the fragment library  for use at run time.","System  includes a fixed function mode as well as a programmable mode.  is a flow diagram illustrating operation of both the fixed function mode and the programmable mode. The graphics operations requested by the user application are described by states, as described previously. These states can include both states associated with user-defined shaders and states associated with the fixed function pipeline. The states are received by the control software  which converts  them to the corresponding state vector.","In this implementation, the state vector is bit-based. Each bit (or group of bits) indicates whether certain shaders are enabled. For example, if there are 32 possible different shaders, the state vector could be a 32-bit state vector. Each bit corresponds to a shader, which in turn includes one or more fragments. The value of the bit indicates whether that shader (and the corresponding fragments) are included in the composite shader, thus representing over 4 billion (2) possible composite shaders. For example, bit =1 might indicate that shader  is included in the composite shader and bit =0 indicates that shader  is not included. If shader  includes fragments A, B and C, then bit =1 would cause fragments A, B and C to be included in the composite shader. If bit =0, fragments A, B and C will not be included unless another enabled shader calls for their inclusion. In an alternate embodiment, the shaders can be mapped to the state vector in different ways. In a common approach, multiple bits may be used to represent groups of shaders. For example, if the application is limited to one light in a scene, but there are three different shaders representing three different light types (e.g., directional diffuse, local specular\/diffuse, and ambient only), then only two bits are needed to represent which light, if any, is enabled. For example, 00 could mean no lighting, 01 directional diffuse lighting, 10 local specular\/diffuse, and 11 ambient only. Not all bits in the state vector need be assigned, thus allowing the future addition of new shaders and fragments. In a preferred embodiment, bits are used in order, starting with the least significant bit.","Each bit of the state vector is determined by querying or otherwise determining the state that the application has specified should be applied. In scenegraph applications, this data is readily available from a state manager or node data structure. In an application built directly on top of a lower-level graphics API such as OpenGL, it is possible to query the driver immediately prior to object rendering to obtain object state associated with the fixed-function pipeline, if the data is not available through more efficient means. The result of each state query is inserted into the corresponding bit(s) of the state vector.","In this implementation, the control software  also combines multiple shaders that are to be applied to the same object, forming a single state vector that represents all of the graphics operations to be applied to the object. In this process, fragments that appear in more than one shader typically will appear only once in the combined shader. Conflicts between shaders typically are resolved at this stage if they have not been resolved before run time. Fragment assembler  maintains information on which fragments are included in each shader, including any requirements on the order in which fragments must be executed. Fragments that are not required by any of the constituent shaders are not included in the composite shader, thus making the entire process more efficient.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":["FIG. 7","FIG. 7"],"b":["710","510","710"]},"Returning to , the state vector for a shader (whether it be for a single shader or a composite shader) represents the graphics operations to be applied. The control software  determines , based on the state vector, whether the shader is to be executed using the fixed function pipeline or the programmable pipeline. In this implementation, if the state vector indicates that only standard operations are required (i.e., no custom shaders are enabled), the fixed function pipeline is used  to render the object.","If the programmable pipeline is used, execution proceeds according to . In particular, the state vector is hashed and compared  against the hash table . If there is a match, the corresponding handle is passed ,  by the control logic  to the driver , which executes  the previously compiled shader.","If there is no match for the state vector, then the required shader is run-time compiled. The fragment assembler  retrieves and assembles  the fragments indicated by the state vector. In this implementation, the assembler  does so by traversing the list of fragments required if all shaders are enabled and assembling only those required by shaders enabled in the state vector. It is usually important to preserve the order of the fragments since some fragments may depend on the output of other fragments. If the vector state represents the combination of multiple shaders, the order of the fragments in the combined shader preferably is consistent with the order in the individual shaders. Continuing the example of , assume shader X requires fragments A, B, D in the order A-B-D, and shader Y requires fragments B, E, H in the order E-B-H. The composite shader  of A-E-B-D-H is consistent with the orderings in the constituent shaders. However, shaders A-B-D-E-H and A-H-D-B-E are not.","In compilation , a handle for the user-defined shader is requested from the driver  and the assembled fragments are handed to the driver . The driver  includes a run-time compiler that compiles  the shader, which can then be executed . The driver  also returns the handle to the control software .","The control software  indexes the state vector and corresponding handle into the hash table  for future use. Other objects in the same scene may reuse the compiled shader in the same frame and any object, including the original object, may reuse the compiled shader in subsequent frames. If all objects requiring the compiled shader disappear from view, the compiled shader may remain in the hash table  and program memory  (this is generally preferred). Alternately, a garbage collection scheme may be used to clean out shaders that are no longer needed. Because most graphics drivers that have a programmable mode automatically allocate scarce resources to shaders which are in use, it is generally more efficient to retain compiled shaders in case they are needed again later.","The process described above is repeated for each object in the scene that may have shaders applied. The various data structures are maintained on a global basis, rather than on a per-object basis, and may be used by multiple objects. It may be desirable to have multiple sets of data structures, corresponding to different sets of fragments. For example, one class of objects may have certain characteristics that are best served by a certain library of fragments, with its corresponding data structures ,  and . Another class of objects may be better served by a different library of fragments, as opposed to expanding the first library to cover both classes of objects. This approach reduces the size of the state vectors and works well when the two libraries are significantly different.","Shader parameters, such as light colors, positions, bump-map scales, etc. are managed using a state management system in parallel with the fixed-function pipeline state management infrastructure of the application. For example, if the application uses a scenegraph with hierarchical state management (i.e., state attributes can be at any level in the graph), custom attributes for shader-specific parameters are added, and some fixed-function attributes may be supplemented with attributes that map the fixed-function parameters into parameters addressable by the shader engine (referred to as program parameters by nVidia's OpenGL Vertex Programs, for example). An example of states defined by the fixed-function pipeline is texture coordinate generation mode. A stock scenegraph supporting different texture coordinate generation modes includes a mechanism for keeping track of what texture coordinate generation mode is used for each object in the scene. States associated with specific user-defined shaders (e.g., index of refraction) are not known to such a stock scenegraph. The scenegraph is extended to support user-defined states. For an application using a scenegraph or other scene structure with leaf-node state management (such as SGI's IrisPerformer's geoState mechanism), additional parameters may be added to the \u201cgeoStates\u201d to support user-defined shaders.","For the example of OpenGL Vertex Programs, states are passed to user-defined shaders through 96 program parameter registers, each of which comprises four IEEE floating-point components. Both fixed-function and user-defined states are mapped into this address space such that each shader fragment may access the parameters that affect its operation. The available shader parameter address space can be allocated as necessary for all the possible shader combinations. This is achieved by filling in the address space starting with zero with the parameters for all the shaders that may be used concurrently. If there are several disjoint sets of shaders, wherein each set describes some subset of all the shaders that may be used concurrently, each set may have its own parameter mapping. This is only necessary if the number of parameters needed by all the shaders exceeds the available address space.","Returning to , the determination  of whether to use the fixed function pipeline versus the programmable pipeline is made in this implementation based on the state vector. As a result, it is advantageous to select the user-defined shaders so that they overlap in functionality with the standard operations from the fixed function pipeline. In other words, there are certain graphics operations which will be implemented by both standard operations and by user-defined shaders. Preferably, for at least a substantial number of these graphics operations, there is a specific user-defined shader that corresponds directly to the standard operation.","For example, assume that there are three standard operations A, B and C, each of which has two subparts as follows:",{"@attributes":{"id":"p-0072","num":"0071"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Standard Operation","Subparts"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"A","A1 + A2"]},{"entry":[{},"B","B1 + B2"]},{"entry":[{},"C","C1 + C2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}},"br":{}},{"@attributes":{"id":"p-0073","num":"0072"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"140pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Shader","Subparts"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"X","A1 + A2"]},{"entry":[{},"Y","B1 + B2"]},{"entry":[{},"Z","C1 + C2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}},"br":{}},{"@attributes":{"id":"p-0074","num":"0073"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"140pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Fragment","Subparts"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"T","A1 + B2"]},{"entry":[{},"U","B1 + C1 + C2"]},{"entry":[{},"V","A2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}},"br":{}},{"@attributes":{"id":"p-0075","num":"0074"},"figref":["FIG. 8","FIG. 8"]},"State vector  requires graphics operations A, C and E. Since E is a user-defined operation, state vector  is executed via the programmable pipeline. The composite shader defined by shaders X, Z and E is executed. Now assume that the user (e.g., an applications programmer) makes a change to state vector  by disabling operation E. The resulting state vector  only requires operations A and C, both of which are standard operations. As a result, the state vector  can be executed by the fixed function pipeline. The transition from programmable pipeline to fixed function pipeline is efficient due to the one to one correspondence between fragments X\u2013Z and standard operations A\u2013C.","Although the invention has been described in considerable detail with reference to certain preferred embodiments thereof, other embodiments will be apparent. Therefore, the scope of the appended claims should not be limited to the description of the preferred embodiments contained herein. For example, the functionality described here can be implemented in various combinations of hardware and software, including implementation in software of different levels.","As another example, vertex shaders are used in many of the examples but other types of shaders are also suitable for use with the invention. For example, pixel shaders can be processed in an analogous manner. Furthermore, the invention can also be used with other shaders, such as clipping, fragment or camera projection shaders, including shaders which are not currently available today. If multiple types of shaders are in use, a correlation between different types of shaders can be established since there may be a correspondence between fragments. For example, if a pixel shader fragment for per pixel normal perturbation via a \u201cbump map\u201d texture is used, a corresponding vertex shader fragment may be required to set up the vertex parameters properly. As a result, it is possible to have different types of shaders share common bits in the shader state vector."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The invention has other advantages and features which will be more readily apparent from the following detailed description of the invention and the appended claims, when taken in conjunction with the accompanying drawings, in which:",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 1B"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 5","FIG. 3"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 6","FIG. 5"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
