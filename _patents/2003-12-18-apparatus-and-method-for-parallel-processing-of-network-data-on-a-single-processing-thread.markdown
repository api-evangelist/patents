---
title: Apparatus and method for parallel processing of network data on a single processing thread
abstract: A method and apparatus for handling multiple processing streams in parallel on a single thread of a processing device. In one embodiment, a parallel processing agent includes a scheduler that multiplexes a number of processing streams, or pipelines, on a single thread of execution.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07000048&OS=07000048&RS=07000048
owner: Intel Corporation
number: 07000048
owner_city: Santa Clara
owner_country: US
publication_date: 20031218
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["The invention relates generally to networking and, more particularly, to an apparatus and method for processing network data in parallel on a single execution thread of a processing device.","Computer networks, as well as other communication networks, routinely exchange information in units\u2014commonly referred to as packets\u2014corresponding to a known format. For example, a network may exchange packets associated with the TCP\/IP protocols. See, e.g., Internet Engineering Task Force Request for Comment (IETF RFC) 791, , and IETF RFC 793. In addition to packets, other network data may also be exchanged over a computer network, including host requests\/descriptors, timeout requests, transmission requests, as well as others.","Generally, operations performed on network data include receive processing, transmit processing, and timer processing. For example, when a packet is received at an end node of a network, such as a server or server cluster (or an intermediate device such as a router), one or more operations are usually performed with respect to the received packet. This packet processing may include, by way of example, accessing source and destination addresses and\/or port numbers in the header of a received packet to make classification and\/or routing decisions, flow control operations (e.g., sending a TCP ACK packet, error detection, etc.), queuing, as well as procedures associated with establishing or tearing down a connection.","Traditionally, processing of packets and other network data was performed on a general purpose processor supporting a single execution thread. Often times, the processing of a packet requires a large number of memory accesses to system memory or other off-chip memory. The single threaded processor performs operations sequentially and, therefore, it can stall during memory accesses and other slow operations while waiting for such operations to complete. Each stall in the processing of a packet due to a memory access (or other relatively slow process) wastes a significant number of clock cycles. The combination of a large number of unutilized clock cycles with the sequential nature of a single threaded processor creates an inefficient scheme for handling packets and other network data.","Processor clock cycles are shrinking at a much greater rate than memory access latencies. Thus, the number of clock cycles that may be wasted during a stall (e.g., for a memory access) is rapidly increasing, which in turn has caused the execution of packet processing\u2014as well as the processing of other network data\u2014on general purpose CPUs (central processing units) to rapidly decrease in efficiency, resulting in a failure to utilize the computational potential offered by high frequency processing devices. As equipment vendors strive to increase the speed and performance of general purpose computers (e.g., a client or server), the effects of the above-described failure to harness the abilities of high speed processors is becoming more profound.","The traditional method of using caches to reduce the frequency of memory accesses in application code is not very effective for processing packets and other network data due to a very low re-use of cached parameters and data. Also, conventional software multi-threading schemes do not provide a viable solution for the processing of network data. Today's multitasking operating systems (OS) may utilize methods of software multi-threading to share one or more processor execution threads between the many programs that may be executing on a computer simultaneously. However, OS multi-threading exhibits a very high overhead (e.g., thousands of clock cycles) because the OS implements a software scheduler common to all programs running on a system and, therefore, the OS has to deal not only with switching between threads but also with swapping program operating environments and contexts in and out of the CPU hardware in order to support the threads. Thus, due to these high thread switching latencies that typically consume thousands of clock cycles on conventional CPUs, such software multi-threading schemes cannot be utilized to hide memory accesses and other stalls that typically consume a few hundred clock cycles.","The masking of memory access latencies (and other stalls) experienced while processing packets and other network data may be achieved using multi-threaded hardware. Generally, hardware multi-threading schemes replicate certain hardware resources to facilitate parallel streams of execution. The use of multi-threaded processing hardware has been shown to be an effective method of hiding stalls for memory accesses and other slow operations. However, use of multi-threaded processors adds significant hardware complexity, while also increasing cost, real estate, and power consumption of the processing system. At the same time, these multi-threaded processors only provide a significant performance advantage for the few applications and operations that do not achieve effective use of cache. In addition, today's high volume (both in terms of manufacture and use) general purpose processors\u2014e.g., those used for desk top and lap top computers\u2014obtain less advantage from multi-threading than lower volume processors\u2014e.g., those used for servers and workstations. Furthermore, it should be noted that, from a cost and power consumption standpoint, the use of lower complexity, higher volume, single threaded general purpose processing devices for desk top, lap top, server, work station, and packet processing alike, is desirable.","In the arena of network data processing, the use of a specialized packet processor supporting multiple threads of execution has been proposed. A multi-threaded packet processor can process multiple packets in parallel and very effectively reduce the performance cost of memory access stalls. However, these specialized packet processors suffer from many of the above-described disadvantages. Costs are increased due to added hardware complexity and lower volume markets. Further, in comparison to single threaded high volume processors (such as those used in desk top and lap top computers), the multi-threaded packet processor will have higher power requirements, increased cooling loads, and may be more difficult to program.","Disclosed are embodiments of a method and apparatus for processing network data, such as packets, in parallel on a single thread of a processing device. The disclosed embodiments are described below in the context of processing multiple packets in parallel on a single processing thread. However, it should be understood that the disclosed embodiments are applicable to the processing of any type of network data, whether packetized data or other type of data (e.g., host requests\/descriptors, timeout requests, transmission requests, etc.) and, further, irrespective of whether the data processing is receive, transmit, or timer processing. As used herein, a \u201cpacket\u201d is any network communication having a header and a payload (e.g., a TCP\/IP packet). The disclosed embodiments are also described below in the context of an end node (e.g., a server) on a network. However, it should be understood that the disclosed embodiments are applicable to any device on a communications network (e.g., a router) that receives and handles network data.","Referring to , illustrated is an embodiment of a network . The network  may comprise any type of network, such as a Local Area Network (LAN), a Metropolitan Area Network (MAN), a Wide Area Network (WAN), a Wireless LAN (WLAN), or other network. The network  comprises a number of end nodes, including nodes  (i.e., nodes ) and node . At least the node  incorporates an embodiment of a disclosed parallel processing agent, as will be described in greater detail below. A node ,  may comprise any addressable device. For example, a node ,  may comprise a computer system or other computing device, such as a server, a desktop computer, a laptop computer, or a hand-held computing device (e.g., a personal digital assistant or PDA).","Each of the nodes ,  is coupled with a router  via a link . Node  is coupled with router  via link , whereas node is coupled with router  via link , node is coupled with router  via link , and so on. Each of the links may be established over any suitable medium\u2014e.g., wireless, copper wire, fiber optic, or a combination thereof\u2014supporting the exchange of information via any suitable protocol\u2014e.g., TCP\/IP (Transmission Control Protocol\/Internet Protocol), HTTP (Hyper-Text Transmission Protocol), as well as others.","The router  couples the network  with another network (or networks) , such as, by way of example, the Internet and\/or another LAN, MAN, WAN, or WLAN. The router  comprises any device (or combination of devices) that can receive from the other network  packets (or other network data) that are destined for the network  and route these packets to the appropriate destination within network . Router  also receives packets (or other network data) from the nodes ,  within the network  and forwards these communications to the other network . The router  may be coupled with the other network  via any suitable medium, including a wireless, copper wire, and\/or fiber optic connection using any suitable protocol (e.g., TCP\/IP, HTTP, etc.).","It should be understood that the network  shown in  is intended to represent an exemplary embodiment of such a system and, further, that the network  may have any suitable configuration. For example, the network  may include additional nodes , subnets, and\/or other devices (e.g., switches, routers, hubs, etc.), which have been omitted from  for ease of understanding. Further, it should be understood that the network  may not include all of the components shown in .","As noted above, at least the node  implements an embodiment of a disclosed parallel processing agent, as will be described below. In one embodiment, the node  comprises any suitable computer system (e.g., a server), and the parallel processing agent comprises a software application that may be implemented or executed on this computer system. An embodiment of such a computer system is illustrated in .","Referring to , the computer system  includes a bus  to which various components are coupled. Bus  is intended to represent a collection of one or more buses\u2014e.g., a system bus, a Peripheral Component Interconnect (PCI) bus, a Small Computer System Interface (SCSI) bus, etc.\u2014that interconnect the components of computer system . Representation of these buses as a single bus  is provided for ease of understanding, and it should be understood that the computer system  is not so limited. Those of ordinary skill in the art will appreciate that the computer system  may have any suitable bus architecture and may include any number and combination of buses.","Coupled with bus  is a processing system . The processing system  may comprise any suitable processing device or combination of processing devices, and embodiments of the processing system  are illustrated below in each of  and the accompanying text.","Computer system  also includes system memory  coupled with bus . The system memory  may comprise any suitable type and number of random access memories, such as static random access memory (SRAM), a dynamic random access memory (DRAM), a synchronous DRAM (SDRAM), and\/or a double data rate DRAM (DDRDRAM). During operation of computer system , an operating system , a parallel processing agent , as well as other programs  may be resident in the system memory . Computer system  may further include a read-only memory (ROM)  coupled with the bus . During operation, the ROM  may store instructions and variables for processing system , and ROM  may also have resident thereon a system BIOS (Basic Input\/Output System). The computer system  may also include a storage device  coupled with the bus . The storage device  comprises any suitable non-volatile memory, such as, for example, a hard disk drive. The parallel processing agent , as well as operating system  and other programs , may be stored in the storage device . Further, a device  for accessing removable storage media (e.g., a floppy disk drive or a CD ROM drive) may be coupled with bus .","The computer system  may include one or more input devices  coupled with the bus . Common input devices  include keyboards, pointing devices such as a mouse, and scanners or other data entry devices. One or more output devices  may also be coupled with the bus . Common output devices  include video monitors, printing devices, and audio output devices.","Computer system  further comprises a network interface  coupled with bus . The network interface  comprises any suitable hardware, software, or combination of hardware and software capable of coupling the computer system  with the network (or networks)  (and\/or with router ).","It should be understood that the computer system  illustrated in  is intended to represent an exemplary embodiment of such a computer system and, further, that this computer system may include many additional components, which have been omitted for clarity and ease of understanding. By way of example, the computer system  may include a DMA (direct memory access) controller, a chip set associated with the processing system , additional memory (e.g., a cache memory), as well as additional signal lines and buses. Also, it should be understood that the computer system  may not include all of the components shown in .","In one embodiment, the parallel processing agent  comprises a set of instructions (e.g., a software application) run on a computer system\u2014e.g., the computer system  of  or other suitable computing device. The set of instructions may be stored locally in storage device  or, alternatively, the instructions may be stored in a remote storage device (not shown in figures) and accessed via network  (or from another network ). During operation, the set of instructions may be executed on processing system , wherein the instructions (or a portion thereof) may be resident in system memory .","In another embodiment, the parallel processing agent  comprises a set of instructions stored on a machine accessible medium, such as, for example, a magnetic media (e.g., a floppy disk or magnetic tape), an optically accessible disk (e.g., a CD-ROM disk), a flash memory device, etc. To run parallel processing agent  on, for example, computer system , the device  for accessing removable storage media may access the instructions on the machine accessible medium, and the instructions may then be executed in processing system . In this embodiment, the instructions (or a portion thereof) may again be downloaded to system memory .","In another embodiment, the parallel processing agent  is implemented in hardware or a combination of hardware and software (e.g., firmware). For example, the parallel processing agent  may be implemented in an ASIC (Application Specific Integrated Circuit), an FPGA (Field Programmable Gate Array), or other similar device that has been programmed in accordance with the disclosed embodiments.","As previously noted, embodiments of the processing system  are illustrated in  and the accompanying text. It should be understood, however, that the processing systems shown in  are but a few embodiments of a processing system upon which the disclosed embodiments of a parallel processing agent  may be implemented. Those of ordinary skill in the art will appreciate that the disclosed embodiments of parallel processing agent  may be implemented on many other types of processing systems and\/or processor architectures.","Referring first to , the processing system  includes a host processing device  and a network data processing device , both of which are coupled with bus . The host processing device  may comprise any suitable processing device, such as a microprocessor, a network processor, an ASIC, an FPGA, or similar device. In one embodiment, the host processing device  comprises a general purpose processing system that may execute operating system  (and other programs ). Although a single processing device  is shown in , it should be understood that the disclosed embodiments are not so limited, and in a further embodiment, the processing system  includes multiple processing devices .","The network data processing device  comprises any suitable processing device, such as a microprocessor, a network processor, an ASIC, an FPGA, or similar device. In one embodiment, the network processing device  comprises a general purpose processing system that supports a single thread , and the parallel processing agent  can be executed on this single thread. Although a single network data processing device  is illustrated in , it should be understood that the disclosed embodiments are not so limited, and in another embodiment, the processing system  includes multiple network data processing devices , each supporting a single thread, wherein one (or more) of the multiple network data processing devices can execute the parallel processing agent .","As noted above, the network data processing device  may support a single thread of execution. A \u201cthread\u201d is a separate flow of control within a computer program. The thread shares the resources that are allocated to the program (e.g., address space, global variables, etc.) with other parts of\u2014and\/or other threads associated with\u2014the program. The thread also has its own resources (e.g., execution stack, stack pointers, program counters, etc.) that are typically not shared with other components of the program. As used herein, the term \u201cthread\u201d encompasses a flow of control within a program and the resources allocated to that thread. Furthermore, a processing device is said to \u201csupport\u201d a thread if that device includes at least some of the hardware (e.g., registers, memory space, as well as other circuitry) needed for the thread to utilize the resources available to it. In addition, the phrases \u201cthread\u201d, \u201cexecution thread\u201d, \u201cthread of execution\u201d, \u201cprocessing thread\u201d, and the like are used herein interchangeably.","Referring next to , another embodiment of the processing system  is illustrated. The embodiment of  is similar to that shown in ; however, the network data processing device  supports multiple threads. For example, as shown in , the network data processing device  may support four threads , , , and . Any one of the threads may execute the parallel processing agent , whereas each of the remaining threads can be allocated to other tasks. In another embodiment, two or more of the threads are each separately executing the parallel processing agent . Again, as noted above, in other embodiments, the processing system  may include multiple processing devices  and\/or multiple network data processing devices , each of the network data processing devices  providing multiple threads (or a single thread) of execution.","Turning now to , a further embodiment of the processing system  is illustrated. In the embodiment of , the processing system  comprises a single device. Referring to this figure, the processing system  includes a local bus  to which various functional units are coupled. Bus  is intended to represent a collection of one or more on-chip buses that interconnect the various functional units of processing system . Representation of these local buses as a single bus  is provided for ease of understanding, and it should be understood that the processing system  is not so limited. Those of ordinary skill in the art will appreciate that the processing system  may have any suitable bus architecture and may include any number and combination of buses.","A core  and a processing engine  are coupled with the local bus . In one embodiment, the core  comprises a general purpose processing system, which may execute operating system . Core  may also control operation of processing system  and perform a variety of management functions, such as dispensing instructions to the processing engine  for execution. The processing engine  comprises any suitable processing system, and it may include an arithmetic and logic unit (ALU), a controller, and a number of registers (for storing data during read\/write operations). The processing engine  supports a single thread of execution , and the parallel processing agent  may be executed on this thread. In another embodiment, the processing engine  supports multiple execution threads (e.g., four), and the parallel processing agent  may be executed on any one of these threads (the remaining threads being allocated, in one embodiment, to other tasks).","Also coupled with the local bus  is an on-chip memory subsystem . Although depicted as a single unit, it should be understood that the on-chip memory subsystem  may\u2014and, in practice, likely does\u2014comprise a number of distinct memory units and\/or memory types. For example, such on-chip memory may include SDRAM, SRAM, and\/or flash memory (e.g., FlashROM). It should be understood that, in addition to on-chip memory, the processing system  may be coupled with off-chip memory (e.g., ROM , off-chip cache memory, etc.).","Processing system  further includes a bus interface  coupled with local bus . Bus interface  provides an interface with other components of computer system , including bus . For simplicity, bus interface  is depicted as a single functional unit; however, it should be understood that, in practice, the processing system  may include multiple bus interfaces. For example, the processing system  may include a PCI bus interface, an IX (Internet Exchange) bus interface, as well as others, and the bus interface  is intended to represent a collection of one or more such interfaces.","It should be understood that the embodiment of processing system  illustrated and described with respect to  (and D) is but one example of a processing system that may find use with the disclosed embodiments of a parallel processing agent and, further, that the processing system  may have other components in addition to those shown in , which components have been omitted for clarity and ease of understanding. For example, the processing system  of  may include other functional units (e.g., an instruction decoder unit, an address translation unit, etc.), a thermal management system, clock circuitry, additional memory, and registers. Also, it should be understood that a processing system may not include all of the elements shown in .","Yet another embodiment of the processing system  is illustrated in . The embodiment of  is similar to the embodiment of ; however, the processing system of  includes a number of processing engines  (e.g., processing engines , , . . . , ) that are coupled with the local bus . Each of the processing engines comprises any suitable processing system, and each may include an arithmetic and logic unit (ALU), a controller, and a number of registers (for storing data during read\/write operations). In one embodiment, each of the processing engines supports a single thread of execution, and the parallel processing agent  may be executed on any one of the processing engines (the other threads being allocated, in one embodiment, to other tasks). In another embodiment, the parallel processing agent  is executed separately on two or more of the processing engines . In yet a further embodiment, at least one of the processing engines supports multiple threads (e.g., four), and the parallel processing agent  may be executed on any one of these threads (the other threads perhaps being allocated to other tasks).","In the embodiments of , the operating system can be executed on a host processing device  that is separate (in one embodiment) from the network data processing device  upon which the disclosed parallel processing agent may be executed. Similarly, in the embodiments of , the operating system can be executed on a core  that is separate (in one embodiment) from a processing engine  upon which the disclosed parallel processing agent may be executed. However, in another embodiment, the parallel processing agent may be executed on the same processing device and\/or core upon which the host operating system is running. Thus, in another embodiment, as shown in , the processing system  comprises a host CPU having a single processing thread  upon which an operating system (as well as other applications) and the disclosed parallel processing agent may be executed. In one embodiment, the host CPU  comprises a general purpose processing device. For the embodiment of , the operating system and parallel processing agent are time multiplexed on the execution thread .","In yet another embodiment, as shown in , the host CPU  may include multiple processing threads, including, for example, processing threads through . In the embodiment of , the parallel processing agent may be executed on one of the threads , whereas the other of threads may be allocated to other tasks (e.g., running the operating system and\/or other applications).","The parallel processing agent  decomposes the processing of packets into a number of processing streams\u2014each processing stream being referred to herein as a \u201cpipeline\u201d\u2014and execution of these separate processing streams is interleaved on a single thread of a processing device (e.g., the packet processing device  of , one of the processing engines  of  or D, or the host CPU  of either  or F). Each separate processing stream is capable of processing a received packet (or other network data) and, therefore, multiple packets can be handled by the parallel processing agent  in parallel on a single execution thread. Through this interleaving of separate processing streams to provide for the parallel processing of multiple packets\u2014or, more generally, multiple instances of network data\u2014memory access latencies (as well as other latencies) can be masked, and greater efficiency is achieved by minimizing the number of clock cycles that are \u201clost\u201d or \u201cwasted\u201d due to memory accesses and other operations.","Because the disclosed parallel processing agent enables the parallel processing of network data on a single thread, the disclosed embodiments are amenable to a system architecture in which host CPU processing is partitioned from the processing of network data. An embodiment of such an architecture  for the computer system  of  is illustrated in . It should, however, be understood that the architecture  shown in  is but one example of a computer architecture with which the disclosed embodiments may be used.","Referring to , the architecture  includes one or more applications and a socket API (Application Programming Interface) . Through the socket API , the applications access the services provided by a TCP\/IP stack , or other protocol stack. The host OS  lies at a layer between the applications (and\/or socket API ) and the TCP\/IP stack .","The network data processing system carries out TCP\/IP stack processing and the processing of other network data, where the network data processing system comprises, for example, the network data processing device  of  or B, the processing engine  of  or one of the processing engines of , or the host CPU  itself (using, in one embodiment, a time multiplexing scheme), as shown in  or F. Other processing (e.g. the OS  and applications ) is performed by the host CPU, the host CPU comprising, for example, the host processing device  of  or B, the core  of  or D, or the host CPU  of  or F. Thus, processing of network data may be performed in parallel and asynchronously with other processing that is performed by the host CPU. The host CPU and network data processing system interface with one another through direct transport interfaces (DTIs) . The socket APIs may be implemented as an adaptation layer that translates API calls into DTI setup, control, and access operations. The applications may exchange data directly (through a direct user socket I\/O) with the DTIs , and the network data processing system performs network and transport layer processing (and\/or other network data processing). For legacy socket applications, the architecture  may include a legacy sockets proxy .","The architecture  also includes a kernel web server  and kernel space applications , both of which may exchange data with the DTIs . The kernel space applications may include its own socket I\/O , as shown in . In addition, an OS vendor (OSV) file system  may exchange data with the DTIs  via an IP storage driver . Architecture  further includes a media interconnect  (comprising, for example, the network interface  of ) providing for the exchange of data with the network  and, perhaps, with a storage network  and\/or a system area network .","Turning now to , illustrated are embodiments of the parallel processing agent . Referring first to , in one embodiment, the parallel processing agent  comprises a scheduler  and a number of pipelines , including pipelines , , . . . , . Each of the pipelines comprises a series of operations and, for any packet received at node  (or other network data), at least a subset of these operation can be performed with respect to the received packet. The pipelines can share a single thread of the processing system  (see ), and the pipelines are multiplexed on this single thread by the scheduler . In one embodiment, the scheduler  also shares this single thread with the pipelines . The parallel processing agent , including scheduler  and pipelines , can be implemented using any suitable software, hardware, or combination of software and hardware. In one embodiment, as shown in , the parallel processing agent  is implemented as a series of software routines executed by processing system .","Referring next to , an embodiment of the parallel processing agent  is illustrated in greater detail. In one embodiment, as shown in , each of the pipelines comprises a series of \u201cstates\u201d through (STATE  through STATE X). For any received packet (or other network data), at least a subset (i.e., one or more) of these states are performed to process the packet. Each of the states comprises a sequence of one or more operations that lead up to a specific outcome or decision point. A state may perform operations related to, for example, TCP\/IP stack processing (e.g., flow control, sending TCP ACK packets, error checking, etc.), drivers for accessing network hardware (e.g., network interface  and network ), and interfacing with a host processor (e.g., the host processing device  of  or the core  of ).","The entire set of possible states within the pipelines is generally the same within each pipeline (although it is within the scope of the disclosed embodiments that the set of possible states vary between pipelines). However, as previously suggested, a packet (or other network data) will progress through a subset of these states while being transferred between the host processor  and the network , or between the network and the host processor. The actual sequence of states for any given packet (or other instance of network data) is dependent upon decisions made during processing of that packet, and this subset of the states for a given packet will be dependent upon a variety of factors. For example, the subset of the states that a packet progresses through may depend on the direction of transfer (e.g., is the packet arriving from a host processor or from a network), the state of the connection over which the packet was or will be transmitted (e.g., is the connection an established connection, a connection in the process of being set up, or a connection being torn down), and the contents of the packet's header.","As noted above, each of the states comprises a sequence of one or more operations that can be performed with respect to a packet (or other instance of network data). When a state is executed for a packet, the processing of that packet progresses through the sequence of operations associated with that state to reach a specified outcome or decision point. The outcome or decision point reached during execution of any given state generally coincides with a memory access or other operation that requires a relatively large number of clock cycles to complete, and processing of a packet on that pipeline cannot resume until the operation has been completed. Such events are referred to herein as \u201cyield points\u201d or \u201cyield events.\u201d When a pipeline reaches a yield point\u2014which typically, but not necessarily, corresponds with the end of a state\u2014the scheduler  will scheduler another one of the pipelines  for execution on the thread. Potential yield events include memory accesses (e.g., for non-cached data), the end of a network operation (e.g., the end of processing a packet), TCP context lookups and\/or accesses, host interface interactions such as reading a queue, resource management tasks such as tracking counters and timers, asynchronous interactions with other components, and various hardware operations such as DMA (direct memory access) transfers, network interface interactions (e.g., descriptor, packet, and header fetches), virtual to physical address translation look-ups, etc. A yield event may also be an explicit yield performed by software (e.g., the parallel processing agent ). For example, an explicit yield may be carried out to perform a speculative memory prefetch.","The number of pipelines needed for efficient operation is generally a function of the relationship between the number of clock cycles necessary for the scheduler to perform a pipeline switch (e.g., 28 clock cycles), the number of clock cycles needed to process the individual states, and the number of clock cycles necessary to perform a memory access or other asynchronous operation (e.g., 250 clock cycles). Generally, multiple pipelines will be needed to reduce or \u201chide\u201d the effects of memory access latencies (or latencies associated with other operations) and to insure the number of unutilized clock cycles is minimized. In one embodiment, the number of pipelines comprises between two (2) and eight (8) pipelines; however, it should be understood that any suitable number of pipelines (e.g., >8) may be employed.","The scheduler  multiplexes the pipelines on the single thread being used by the parallel processing agent . The scheduler  includes a number of state pointers  (including state pointers , , . . . , ) and a pipeline pointer , as well as sequence logic . Each of the state pointers corresponds to one of the pipelines (i.e., state pointer corresponds with pipeline , and so on), and the state pointer  associated with any given pipeline  keeps track of the current state within that pipeline. When one pipeline reaches a yield point, and the scheduler switches allocation of the thread to another pipeline, the state pointer  of the stalled pipeline will indicate which of the states is to be executed when that pipeline is resumed. In other words, the state pointer  for a pipeline  is a placeholder for the pipeline, such that the next state to execute on the pipeline is known when that pipeline is again allocated control of the thread.","The pipeline pointer  identifies the pipeline that currently has control of the thread upon which the parallel processing agent  is executing. When a pipeline  yields (e.g., when a state has completed), the sequence logic  will determine which of the other pipelines  is next to be allocated the thread. The sequence logic  may employ any suitable scheduling scheme, such as a round-robin scheduling scheme in which the pipelines  are sequenced for execution one by one in order, a scheme based on cache line locality, or a scheme based on a quantum. Further, the scheduling of the pipelines  may performed synchronously, asynchronously, or using some combination of both synchronous and asynchronous scheduling.","As noted above, the pipelines  may yield and be re-scheduled in a synchronous or asynchronous manner. For example, if a particular operation (e.g., a particular memory access) has a high probability of always causing a stall and the approximate length of the stall is always known, the pipelines may be configured to always yield at that point. Accordingly, a pipeline may be synchronously re-scheduled after a given time period, without regard for whether the stall event has completed. On the other hand, for operations where the probability of causing a stall is low or unknown and the length of the stall may vary widely or is unknown, the pipelines may be configured to either always yield or yield only if a stall event occurs. If a pipeline yields, it may then be asynchronously re-scheduled only after the stall event has completed. In one embodiment, rather than waiting a minimum time period for a memory access (or other operation), the scheduler  first checks for completion of the operation and, if not complete, the scheduler advances to the next pipeline. In another embodiment, if a pipeline is not be ready to commence execution when the thread has been allocated to it (e.g., because the memory access or other operation that earlier caused the pipeline to yield has not yet been completed), the pipeline may simply wait for the operation to complete while maintaining control of the thread (which, in a further embodiment, may occur after synchronous scheduling of the pipeline). Thus, it should be understood that the scheduler  and pipelines  may utilize any combination of synchronous and asynchronous yielding and re-scheduling methods, depending upon the nature of the stall events being avoided.","In another embodiment, as noted above, the sequence logic  may base scheduling decisions on cache line locality. In this scheme, the next pipeline  to be allocated the execution thread is determined according to which of the pipelines has the data it needs to resume execution in cache memory. The cache memory may comprise, for example, an on-chip memory of the network data processing device  of  or B, or other cache memory associated with this processing element. Such a cache memory may also comprise an on-chip memory coupled with the processing engine  of  or coupled with one of the processing engines of  (or shared between two or more of these processing engines). By way of further example, the cache memory may comprise an on-chip memory of the host CPU  of  or F, or other cache memory associated with this processing element.","In a further embodiment, which was also noted above, the sequence logic  may schedule pipelines on the basis of a quantum assigned to each pipeline . A quantum may be expressed as a certain quantity of resources (e.g., a specified number of clock cycles, a certain number of operations, etc.) allocated to a pipeline during any given opportunity to control the thread (each pipeline being given one opportunity to control the thread during a scheduling round). During any given scheduling round, a pipeline  cannot exceed its quantum of allocated resources. In this scheme, the quantum for each of the pipelines may be equal or, alternatively, the quantum may vary for the pipelines to allow some pipelines to obtain more resources than others (e.g., as part of a quality of service or resource reservation policy).","The parallel processing agent  also includes a context  associated with each of the pipelines  (e.g., context is associated with pipeline , and so on). The context  of each pipeline  stores state information regarding the packet (or other network data) currently being processed on that pipeline. The context of each of the pipelines , respectively, may be shared between the pipeline and the scheduler . When a pipeline  executes one of the states , the context  associated with that pipeline may be updated to reflect the outcome of that state. It should be understood that, although illustrated in  as being associated with the scheduler , in another embodiment, each of the state pointers may be thought of as being a part of the respective context for the associated pipeline ","The data structures\u2014e.g., state pointers , pipeline pointer , contexts \u2014associated with the parallel processing agent  may be stored in any suitable memory and, in one embodiment, these data structures are stored in a memory directly accessible by the processing system. For example, these data structures (or a portion thereof) may be stored in on-chip memory, such as a SRAM, DRAM, SDRAM, DDRDRAM, or other memory local to the packet processing device  (see ), comprising part of the on-chip memory subsystem  (see ), or local to the host CPU  (see ). However, in another embodiment, these data structures (or a portion thereof) are resident in off-chip memory (e.g., system memory ).","Illustrated in  is an embodiment of a method  for the parallel processing of network data, as may be performed by parallel processing agent . Again, the network data may comprise a packet or other type of network data. Referring to this figure, a number of processing streams , , . . . , are shown, each of the processing streams corresponding to a pipeline (e.g., the pipelines of ). The processing streams are multiplexed on a single thread, and each can process a packet in parallel with the other processing streams.","As set forth in block , network data (e.g., a packet) is received in the first pipeline , and if the thread is allocated to the first pipeline\u2014see block \u2014the network data is processed on this pipeline, as set forth in block . Referring to block , if a yield event occurs in the first pipeline, control of the thread is returned to the scheduler, as set forth in block . The scheduler will then shift control of the thread to the next scheduled pipeline (e.g., one of pipelines through ). With reference to block , if the processing of the network data is complete, the network data is disposed of, as set forth in block . For example, if the network data is a packet, the packet may be consumed or queued. If processing of the network data is not complete (see block ), the first pipeline is idle until this pipeline is again allocated control of the thread (see block ). New network data brought into the first pipeline (see block ) is processed in a similar manner.","Each of the other processing streams through handles network data in a similar fashion. When any one of the processing streams yields, control of the thread is returned to the scheduler (see , item ), which determines the next pipeline to which control of the thread will be allocated. As suggested above, by multiplexing the thread amongst multiple processing pipelines, the thread resources are efficiently utilized (e.g., wasted clock cycles minimized).","Referring now to , illustrated is another embodiment of a method  for the parallel processing of network data, as may be performed by parallel processing agent . Referring to block  in this figure, network data (e.g., a packet or packets) is received into an idle pipeline (or pipelines). An \u201cidle\u201d pipeline is a pipeline not currently processing network data. Network data may be received from a network (e.g., the network  in ), or the network data may be received from a host processing device (e.g., the host processing device  in , the core  in , or the host CPU  in ).","As set forth in block , the current pipeline is set to the pipeline pointer (see , item ). Referring to block , the current state is set to the state pointer of the current pipeline (see , items ). The current state is then executed, as set forth in block . Generally, each of the states in a pipeline will terminate at a yield event, such as a memory access, as noted above, and the scheduler will allocate the thread to another pipeline when this yield event occurs. In one embodiment, as set forth in block , prior to terminating the current state and switching thread control to another pipeline, a memory prefetch (or DMA start or other operation) is issued by the current state to insure data needed by the current pipeline in the future will be available when the current pipeline is again allocated control of the thread. In response to execution of the current state (see block ), the context (see , items ) of the current pipeline may be updated, as set forth in block .","Referring to block , the next state for the current pipeline is determined. As noted above, the subset of states through which any network data progresses within a pipeline will depend on decisions made during processing of that data. Factors which can effect the next state of a pipeline (and the subset of states that the network data will progress through) include the direction of transfer of the network data (i.e., ingress vs. egress), the state of the connection over which the network data was or will be transmitted (i.e., established connection vs. a connection being set up or torn down), and the contents of the network data (e.g., a packet's header). The state pointer of the current pipeline is then incremented to the next state, which is set forth at block . When this pipeline again resumes control of the thread, the next state will then become the current state.","In one embodiment, prior to switching control of the thread to another pipeline, it is determined whether there is a cache hit for data needed by the current pipeline (i.e., data needed by the next state), as set forth in block . In one embodiment, a cache hit occurs when a memory access for the needed data can be completed from an on-chip memory. If there is a cache hit, the current state of the current pipeline is set to the state pointer (which was previously incremented), and this state is executed (see block  and ). The context of the current pipeline is again updated (see block ), and the next state is determined (see block ) and the state pointer of the current pipeline incremented to the next state (see block ).","If, however, there was not a cache hit (see block ), the current pipeline will yield, as set forth in block . When the current pipeline yields, control of the thread is returned to the scheduler (see , item ), which can perform a pipeline switch. In one embodiment, the current pipeline explicitly returns control of the thread to the scheduler. To reallocate the thread, the scheduler determines the next pipeline that is to be given control of the thread, as set forth in block . Again, scheduling of the next pipeline may be performed synchronously, asynchronously, or according to some combination of synchronous and asynchronous scheduling. In another embodiment, scheduling of the next pipeline may be a function of the current pipeline. For example, the current pipeline may determine using a shared schedule which pipeline is to be scheduled next, and the current pipeline may then transfer control of the thread directly to the next pipeline. For this embodiment, the next pipeline is determined (see block ) prior to yielding the current pipeline (see block ). Referring to block , the pipeline pointer (see , item ) is then set to point to the next pipeline.","Referring to block , if the end of processing the network data on the current pipeline has been reached, the network data is disposed of, as set forth at block . For example, if the network data is a packet, the packet may be consumed or queued. Upon disposition of the completed network data, another set of network data can be received in this pipeline (refer again to block ). If the end of processing the network data has not been reached (or after disposing of the completed network data), the current pipeline is set\u2014refer again to block \u2014to the pipeline identified by the pipeline pointer (which was previously set to the next pipeline in block ), and the above-described processes (i.e., blocks  through ) is repeated for the pipeline that is now the current pipeline.","The above-described methods for parallel processing of network data on a single thread may be better understood with reference to the timing diagram  illustrated in . Referring to this figure, the timing diagram  includes a horizontal axis  representing time and a vertical axis  corresponding to the various entities (e.g., pipelines and the scheduler) that may control the thread. For the embodiment shown in , the parallel processing agent  includes four separate pipelines , , , and (labeled \u201cP\u201d through \u201cP\u201d). However, it should be understood that the disclosed embodiments encompass a parallel processing agent  having any suitable number of pipelines.","During a first time period , the scheduler  is performing a thread switch and determining which pipeline the thread should be allocated to. At a second time period , the first pipeline executes a state (i.e., a state \u201cS\u201d). After the first pipeline completes execution of the state, control of the thread is again returned to the scheduler (for time period ) to determine which pipeline to next schedule on the thread (or, in an alternative embodiment, during time period , the current pipeline determines the next pipeline using a shared schedule and transfers control of the thread directly to the next pipeline). Employing, for example, a round-robin scheduling scheme, the second pipeline is given control of the thread, and this pipeline executes a state (i.e., S) during a fourth time period . During the next time period , control of the thread is returned to the scheduler to determine the next pipeline (or, according to the alternative embodiment, the current pipeline determines the next pipeline and transfers control of the thread directly to the next pipeline). During time period , the third pipeline executes a state (i.e., S) and, after the scheduler (or current pipeline) performs pipeline switching during the next time period , the fourth pipeline executes a state (i.e., S) during a subsequent time period , after which control of the thread is again returned to the scheduler for time period .","The above-described sequence is then repeated, wherein control of the thread is continually being multiplexed between the scheduler and the pipelines in a round-robin fashion (or according to a different scheduling scheme in other embodiments). It should be noted that the time periods , , , . . . ,  during which the pipelines are executing a state are not necessarily equal, as the time required for execution may vary among the states. Also, as noted above, the states that a pipeline progresses through while processing any given network data will vary with the decisions made during processing of that data, and a given set of network data will typically progress through a subset of all possible states within a pipeline. This is also illustrated in , where it can be observed that pipeline progresses from state S to S, pipeline progresses from S to S, and so on.","As described above with respect to , in one embodiment, if a cache hit occurs on a pipeline that has executed a state, rather than switching control of the thread to the next pipeline, the current pipeline is allowed to retain control of the thread and advance to the next state in sequence (see block  and accompany text above). This embodiment is also illustrate in . Referring to the sequence of states for the second pipeline , during the time period , this pipeline executes a state S. However, a cache hit occurred, and at point , rather than returning control of the thread back to the scheduler (or otherwise performing scheduling of the next pipeline), the second pipeline is allowed to execute another state S.","Embodiments of a parallel processing agent  providing multiple processing streams that are multiplexed on a single thread have been disclosed. However, it should be understood that the disclosed embodiments are not limited to the processing of network data. Rather, the disclosed embodiments for multiplexing multiple processing streams on a single thread may find application where it is desirable to process any type of data in parallel on a single thread of a processing device. Any application can utilize the disclosed pipeline\/state structure with a dedicated scheduler in order to reduce sensitivity to memory access latencies and various other stall events.","Embodiments of a parallel processing agent capable of handling multiple processing streams in parallel on a single thread\u2014as well as embodiments of a method of processing network data in parallel on a single thread\u2014having been described above, those of ordinary skill in the art will appreciate the advantages of the disclosed embodiments. A pipeline can be entered at any appropriate state, executed on the thread until a yield event occurs, and then control of the thread returned to the scheduler to schedule the next pipeline (or transferred directly to the next pipeline). Thus, control of the thread can be continuously multiplexed amongst all pipelines. By multiplexing a single thread between a number of processing pipelines, clock cycles which might otherwise be wasted during a memory access are utilized for network data processing. Thus, greater efficiency and throughput are achieved. Further, because the disclosed parallel processing scheme can be executed on a single thread, the parallel processing agent is amenable to execution on a low cost and less power hungry general purpose processors that support a single thread of execution. Accordingly, efficient parallel processing is realized without the added cost and complexity of multi-threaded or multi-processor systems (although the disclosed parallel processing agent may, in some embodiments, be implemented on such systems).","The foregoing detailed description and accompanying drawings are only illustrative and not restrictive. They have been provided primarily for a clear and comprehensive understanding of the disclosed embodiments and no unnecessary limitations are to be understood therefrom. Numerous additions, deletions, and modifications to the embodiments described herein, as well as alternative arrangements, may be devised by those skilled in the art without departing from the spirit of the disclosed embodiments and the scope of the appended claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":["FIG. 2B","FIG. 2A"]},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIGS. 3A\u20133F","FIG. 2A"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIGS. 4A\u20134B"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 5","FIGS. 4A\u20134B"]},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 6","FIGS. 4A\u20134B"]},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 7","FIGS. 4A\u20134B"]}]},"DETDESC":[{},{}]}
