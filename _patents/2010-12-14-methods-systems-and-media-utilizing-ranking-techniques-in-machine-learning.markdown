---
title: Methods, systems and media utilizing ranking techniques in machine learning
abstract: Methods, systems and media are taught utilizing ranking techniques in machine learning to learn a ranking function. Specifically, ranking algorithms are applied to learn a ranking function that advantageously minimizes ranking error as a function of targeted ranking order discrepancies between a predetermined first ranking of a training plurality of data elements and a second ranking of the training plurality of data elements by the ranking function. The ranking algorithms taught may be applied to ranking representations of chemical structures and may be particularly advantageous in the field of drug discovery, e.g., for prioritizing chemical structures for drug screenings.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08862520&OS=08862520&RS=08862520
owner: Massachusetts Institute of Technology
number: 08862520
owner_city: Cambridge
owner_country: US
publication_date: 20101214
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATION","STATEMENT OF GOVERNMENT INTEREST","BACKGROUND","SUMMARY","DESCRIPTION OF EXEMPLARY EMBODIMENT(S)"],"p":["The present application claims the benefit of provisional patent application entitled \u201cMethods Systems and Media Utilizing Ranking Techniques in Machine Learning For Prioritizing Representations of Chemical Structures\u201d which was filed on Dec. 14, 2009 and assigned Ser. No. 61\/286,362. The entire contents of the foregoing provisional patent application are incorporated herein by reference.","This invention was made with government support under Grant No. DMS0732334 awarded by the National Science Foundation. The government has certain rights in this invention.","Methods, systems and media utilizing ranking techniques in machine learning are taught herein.","Ranking problems may arise in a wide variety of information retrieval (IR) applications such as ranking data elements, for example, documents, websites, or the like, according to relevance to a query. In exemplary IR applications, it may be desirable to utilize ranking algorithms which are adapted for ranking chemical structures or representations of chemical structures. The cost of developing a new drug today is estimated to be over one billion dollars. See Shekhar, C. In silico pharmacology: Computer-aided methods could transform drug development, Chemical Biology 2008, 15, 413-414. A large part of this cost is due to failed molecules, i.e., chemical structures that appear to be promising drug candidates during initial stages of screening, but after several rounds of expensive pre-clinical and clinical testing, turn out to be unsuitable for further development. With chemical libraries today containing millions of structures for screening, there is an increasing need for computational methods that can help alleviate some of these challenges. See Shekhar, C (2008); Jorgensen, W. L. The many roles of computation in drug discovery, Science 2004, 303, 1813-1818; and Bajorath, J. Chemoinformatics: Concepts, Methods, and Tools for Drug Discovery; Humana Press, 2004. Particularly, there is a need for computational tools that can rank chemical structures, e.g., according to their chances of clinical success.","Notably, ranking problems are mathematically distinct from the classical learning problems of classification and regression, and require distinct analysis and distinct algorithms. See, e.g., Cortes, C.; Mohri, M., AUC optimization vs. error rate minimization, Advances in Neural Information Processing Systems 16, 2004.","Advantageous methods, systems and media are taught herein utilizing ranking techniques in machine learning. The ranking techniques disclosed herein may be applied to and\/or tailored for a wide variety of IR applications. For example, the ranking techniques may be adapted for ranking data elements in a chemical structure domain, a database domain, an Internet domain, an Intranet domain or other information domain.","The methods for learning a ranking function may generally comprise (i) determining, for example, with a computing process on a computing device, ranking error as a function of ranking order discrepancies between a predetermined first ranking of a plurality of data elements (for example, a plurality of documents or a plurality of representations of chemical structures) and a second ranking of the plurality of data elements determined by the ranking function; and (ii) solving, e.g., with the computing process on the computing device, the ranking error function for the ranking function, whereby the ranking error is minimized with respect to the second ranking of the plurality of data elements.","Methods are also disclosed herein for ranking a plurality of data elements using a learned ranking function. Methods for ranking a plurality of data elements typically comprise (i) providing a ranking function which minimizes ranking error as a function of ranking order discrepancies between a predetermined first ranking of a training plurality of data elements and a second ranking of the training plurality of the data elements determined by the ranking function, (ii) applying the ranking function to rank a test plurality of data elements, for example, in the same domain as the training plurality of data elements, and (iii) identifying from the ranking of the test plurality of data elements, a data element having a characteristic consistent with the training plurality of data elements.","System implementations and non-transitory storage medium embodiments for the forgoing methods for learning a ranking function and methods for ranking are also disclosed herein. System implementation generally comprises a computing device programmed to carry out a computing process including the steps of the forgoing methods. Similarly, non-transitory storage medium embodiments typically involve a non-transitory storage medium storing computer executable instructions for carrying out the steps of the foregoing methods.","Methods for ranking a plurality of data elements are also disclosed herein generally comprising (i) receiving a query characterizing a ranking problem for a set of data elements, (ii) processing the query to learn a ranking function, (iii) applying the ranking function to rank a test plurality of the data elements, and (iv) returning a result to the query based on the ranking of the test plurality of data elements Learning the ranking function may generally include (i) determining, e.g., with a computing process on a computing device, ranking error as a function of ranking order discrepancies between a predetermined first ranking of a training plurality of the data elements and a second ranking of the training plurality of the data elements determined by the ranking function; and (ii) solving, e.g., with the computing process on the computing device, the ranking error function for the ranking function, whereby the ranking error is minimized with respect to the second ranking of the training plurality of data elements.","System implementations for the previous methods are also disclosed generally including input means for inputting the query, a processor for processing the query by executing a computing process, storage means storing computer executable instructions for the computing process, and output means for returning the result to the query.","Methods for ranking a plurality of data elements are also disclosed generally comprising (i) providing a known ranking for a plurality of data elements in a training set, (ii) deriving a ranking function that minimizes ranking error for the ranking function with respect to the known ranking of the plurality of data elements in the training set, and (iii) ranking a plurality of data elements in a test set using the derived ranking function.","Additional features, functions and benefits of the disclosed methods, systems and media will be apparent from the description which follows, particularly when read in conjunction with the appended figures.","Methods, systems and media are taught herein utilizing ranking techniques in machine learning to facilitate\/optimize prioritization of data elements. In some exemplary embodiments, data elements may, for example, be in or from a chemical structure domain such as data representations of chemical structures. Thus, exemplary methods, systems and media are taught herein for applying ranking techniques in machine learning to the task of ranking representations of chemical structures. The exemplary ranking algorithms taught herein may be particularly advantageous in the field of drug discovery, e.g., for prioritizing chemical structures for drug screenings. The exemplary ranking algorithms taught herein are not limited to chemical structure applications, but may also be applied to any number of IR applications that include data different from data representative of chemical structures. Indeed, in some exemplary embodiments, data elements may, for example, may be documents, text files, mark up language files and other types of data.","The exemplary methods, systems and media taught herein may advantageously utilize ranking algorithms which, contrasted with conventional SVM and SVR algorithms, minimize ranking loss rather than classification\/regression loss. The ranking algorithms taught herein may be derived, e.g., from either bipartite or real-value labeled sets of training data and may be used, e.g., in both virtual screening and QSAR ranking applications. The bipartite and real-value labeled sets of training data may advantageously represent a predetermined ranking for a plurality of representations of chemical structures. Thus, the ranking algorithms may advantageously determine ranking functions such that targeted ranking discrepancies are minimized relative to the predetermined ranking. The effectiveness of exemplary ranking algorithms, taught herein, was evaluated for both bipartite and real-value labeled sets of training data, e.g., relative to conventional classification based approaches (SVM) in virtual screening and conventional regression based approaches (SVR) in QSAR ranking.","In exemplary embodiments, the ranking algorithms taught herein may account for preferences, e.g., pair wise preferences (e.g., more active than\/less active than relationships), preferences characterized by bipartite labels (e.g., active\/inactive), or preferences characterized by real-valued labels (representing numerical activity levels). The preferences may advantageously represent objective or subjective criteria as between data elements. In exemplary embodiments, preferences may be used to supplement bipartite or real-value labeled sets of training data. Thus, for example, in the chemical structure domain, preferences may be used to distinguish, e.g., on the basis of cost, availability, potential side effects, etc., between two compounds having similar rankings (e.g., wherein the difference in activity level is below a certain threshold). Alternatively, preferences, e.g., pair-wise preferences, may be used to indicate a predetermined ranking for a plurality data elements. For example, preferences may be used to indicate a predetermined ranking of representations of chemical structure with respect to activity level. Advantageously, ranking loss may be approximated, even where only relative preferences between instances, e.g., relative activity levels of chemical structures, are known. In general, preferences present a strong tool for optimizing, fine tuning, and\/or tailoring ranking loss for a particular set of circumstances. Thus, in exemplary embodiments ranking loss may account for and weight multiple preferences based on a number of difference criteria.","Real-value labeled ranking algorithms utilized by the exemplary methods, systems and non-transitory media taught herein may prove particularly advantageous. Specifically, in exemplary embodiments, the real valued ranking algorithms taught herein weight incorrectly raked pairs of data elements by the disparity in relevance between the data elements in each pair. Thus, for example, in the chemical structure domain the ranking algorithms may be designed to focus the most attention on correctly ranking instance pairs that exhibit the greatest disparity in activity levels (since errors in ranking chemical structures with similar activity levels are often less crucial than errors in ranking chemical structures with a greater differences in activity levels). The effectiveness of various exemplary real-value labeled ranking algorithms, taught herein was evaluated relative to conventional algorithms for various data sets.","In exemplary embodiments, ranking algorithms may minimize average pair-wise loss. For example, an error function may be selected to advantageously normalize aggregate pair-wise loss, e.g., aggregate relevancy weighted pair-wise loss, by the number of pairs. In other exemplary embodiments, ranking algorithms may minimize a maximum pair-wise loss, e.g., a maximum relevancy weighted pair-wise loss. For example, an error function may be selected to advantageously represent the maximum ranking margin across all pairs. The effectiveness of both exemplary ranking algorithms minimizing average pair-wise loss and exemplary ranking algorithms minimizing maximum pair wise-loss was evaluated relative to conventional algorithms for various data sets.","In practice, ranking accuracy at a top portion of a ranked list may be more important than accuracy at the bottom of the list. Thus, the ranking algorithms and methods, systems and non-transitory media taught herein may incorporate constraints that encourage accuracy in a top portion of the learned ranking, for example in a top x percentage (such as top 50%, top 25%, top 10%, top 5% or the like) of the learned ranking or top n number of results (such as top 100, top 50, top 10, or the like) in the learned ranking. In exemplary embodiments, an error function may be selected to advantageously account for the fraction of positives ranked below the highest ranking negative. Such an error function may be minimized so as to maximize the number of positives that appear before the first negative in the ranked list. In other exemplary embodiments, ranking algorithms may incorporate constraints that give greater weight to ranking data elements with higher relevancy. For example, each pair-wise loss for a given data element or aggregate pair wise loss for a given data element may be normalized relative to a characterization of relevancy for the data element such as a real-value label for the data element, or the inverse of a preferred rank position of the data element. The effectiveness of various exemplary ranking algorithms, taught herein, which encourage accuracy at the top was evaluated relative to conventional algorithms for various data sets.","In ranking representations of chemical structures, it may also be desirable to obtain as many different lead structures at a top of the ranking as possible. Thus the ranking algorithms taught herein may incorporate diversity constraints, e.g., by including a term in the error function that encourages diversity among the chemical structures in a top portion of the learned ranking. Thus, in exemplary embodiments ranking algorithms may optimize a suitable trade-off between ranking accuracy (e.g., high-activity chemical structures at the top of the list) and diversity (e.g., diverse chemical structures at the top of the list). Different diversity criteria may be used depending on the particular chemical structure collection and specific requirements of the prioritization task at hand.","In exemplary embodiments, the ranking algorithms may be kernel-based ranking algorithms. Experimental results indicate that depending on the kernel selection, such kernel-based ranking algorithms outperform both conventional SVR and SVM algorithms. Furthermore, the kernel-based ranking algorithms taught herein may be solved efficiently using gradient-based optimization methods. In particular, in exemplary embodiments, the kernel-based ranking algorithms taught herein involve solving a quadratic program (QP) resulting from the Lagrangian dual of the actual optimization problem. Standard implementations of SVM type algorithms resulting in similar QPs generally use a standard QP solver, which takes O(n) time to solve a QP in n variables. The optimization problems taught herein, however, may involve mvariables, where m is in some embodiments the number of training chemical structures and in some embodiments the number of training documents or other data. The resulting O(m) computation time, is prohibitive for large data sets. Thus, in exemplary embodiments, a gradient projection algorithm may be used to solve the optimization problems presented taking only O(m\/\u03b5) time to converge to an \u03b5-accurate solution, a significant improvement over a standard QP solver.","As used herein the term instance generally refers to a data element in a ranking.","Referring now to , a block flow diagram  is presented utilizing machine learning to apply the ranking algorithms taught herein. Specifically, the block flow diagram  (which in system implementations may be a computer implemented computing process executed by one or more computational devices) includes steps for learning a ranking function for ranking data elements. In some embodiments the data elements are representations of chemical structures. In some embodiments the data elements are documents. Thus, the block flow diagram  generally includes a step  of deriving\/selecting a ranking error function for a ranking algorithm. A ranking error is determined as a function of ranking order discrepancies between a predetermined first ranking for a training set of data elements and a second ranking of the training plurality of data elements determined by the ranking function. The error function may further be tailored to meet specific needs of an application at hand. Thus, for example, in the chemical structure domain, the error function may be designed, e.g., to emphasize ranking accuracy at the top of the ranked list and\/or to encourage diversity.","Once an appropriate ranking error function is determined, machine learning may be utilized in step  to derive the ranking function so as to minimize the ranking error. In exemplary embodiments, the ranking function is learned from a reproducing kernel Hilbert space (RKHS). The kernel may advantageously be selected to best suit the data type being investigated (e.g., a Tanimoto kernel for binary fingerprint vectors in virtual screening and a RBF kernel for descriptor vectors in QSAR ranking). In the chemical structure domain, the ranking function may alternatively be derived using chemical similarity functions designed specifically to measure similarities between chemical structures.","Often, a discrete nature of the ranking error function prevents direct minimization thereof. , depicts an exemplary block flow diagram  for deriving a ranking function in such instances, as taught herein. In step , a regularized version of a convex upper bound of the error function is derived. In step , a ranking function is derived by solving the optimization problem presented by the regularized version of the convex upper bound of the error function, e.g., to minimizing the regularized upper bound of the error function. For example, if the hinge ranking loss is selected as the convex upper bound the resulting optimization problem is a convex quadratic program (QP) which can be solved efficiently.","Referring now to , an exemplary block flow diagram  is presented for deriving and applying a ranking function as taught herein. In step , a known ranking for a training plurality of data elements, e.g., representations of chemical structures, is provided. In step , a ranking function is derived that minimizes ranking error for the ranking function with respect to the known ranking of the training plurality of data elements. Finally, in step , a test plurality of data elements are ranked using the derived ranking function.","Referring now to , an exemplary block flow diagram  is presented for processing a query related to a ranking problem as taught herein. In step , a query is received characterizing a ranking problem for data. In the chemical structure domain, the query may, for example, designate a database of representations of chemical structures, a training plurality of representations of chemical structures having a predetermined\/known ranking, and ranking criteria (e.g., regarding focus on diversity, accuracy at the top of the ranking, etc.). In step , the query is processed to learn a ranking function. Exemplary steps for learning a ranking function are described herein, e.g., with respect to  and step  of . In step , the learned ranking function is applied to rank a test plurality of data elements. In some embodiments, the test plurality of data elements are representations of chemical structures. In some embodiments, the test plurality of data elements are documents. Finally, in step , a result to the query is returned based on the ranking of the test plurality of data elements.","The rankings algorithms taught herein may likewise be included in system implementations e.g. involving a computer or distributed computer architecture such as a computer network or a multiple core processor. Systems may generally be utilized to input constraints, process data, and output results. System implementations are discussed in greater detail later herein.","The following sections describe in greater detail exemplary applications of the ranking techniques presented herein. A bipartite ranking setting is discussed first, followed by a real-valued label ranking setting. For both settings exemplary ranking algorithms are presented and evaluated with respect to past methodologies such as SVM and SVR. It is noted that the specific ranking error terms optimized and methods\/algorithms used for optimization are exemplary embodiments and are not limiting of the scope of the methods, systems and media taught herein.","Bipartite Ranking:","Bipartite ranking problems have received significant attention over past few years. In the bipartite setting data elements are assigned to one of two categories\/classes, e.g., positive or negative, active or inactive, etc. Thus, training data including examples of data elements labeled in a bipartite manner, e.g., as positive or negative may be used to derive a ranking function, e.g., such that positive data elements are ranked higher than negative ones.","An exemplary bipartite ranking problem can be described as follows. The learner is given a training sample S=(S,S) including a sequence of positive examples S=(x, . . . , x) and a sequence of negative examples the S=(x, . . . , x) xand xbeing instances in some instance space X, and the goal is to learn a real-valued ranking function \u0192:X\u2192 that ranks accurately future instances in X; in other words, that assigns higher scores to positive instances than to negative ones. The ranking quality of \u0192 in this setting is often measured in terms of the bipartite ranking error. Equation 1 presents one embodiment of a bipartite ranking error function:",{"@attributes":{"id":"p-0047","num":"0046"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"err","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","S"],"mo":";"}}},{"mfrac":{"mn":"1","mi":"mn"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"<"}}},"mo":"+","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"="}}}}}}}}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}}},"where Iis 1 if \u2212\u03c6 is true and 0 otherwise; this is simply the expected fraction of data element pairs incorrectly ranked by \u0192, e.g., where a negative data element is ranked higher than a positive data element by \u0192. Note that the",{"@attributes":{"id":"p-0049","num":"0048"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mi":"\u03d5"}}}}},"br":{}},"A number of learning algorithms have been developed for minimizing bipartite ranking error on the training sample. In exemplary embodiments, Bipartite ranking error may be determined as one minus the AUC. The AUC is often used to measure the ranking quality in the binary\/bipartite settings. Thus, ranking algorithms, such as those taught herein, may be viewed conceptually as, inter alia, maximizing (an approximation of) the training AUC.","An exemplary ranking algorithm for a bipartite setting is presented below. The algorithm advantageously learns a ranking function from a reproducing kernel Hilbert space (RKHS), much as SVMs and SVR do in the case of classification and regression. The discrete nature, of the bipartite ranking error function presented in Equation 1, cannot be minimized by an algorithm directly. Rather, a (bipartite) kernel-based ranking algorithm may minimize instead a regularized version of the convex depicted in .",{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0053","num":"0052"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"<"}}},"mo":"+","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"="}}}}},"mo":","}}},"br":{},"sub":["i","j","i","j"],"sup":["+","\u2212","+","\u2212"]},{"@attributes":{"id":"p-0054","num":"0053"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mfrac":{"mn":"1","mi":"mn"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msub":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"-"}}}},"mo":"+"}}}},{"mrow":[{"mi":"where","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"msub":{"mi":"a","mo":"+"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mi":"a"},{"mrow":{"mrow":{"mi":["if","a"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":">","mn":"0"}}]},{"mtd":[{"mn":"0"},{"mrow":{"mi":"otherwise","mo":"."}}]}]}}],"mo":"="}],"mo":[",","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}}},"This is similar to the use of the hinge loss as a convex upper bound on the zero-one classification loss in SVMs, and is graphically depicted in .","In exemplary embodiments, instances in X, are described by d-dimensional vectors (such as chemical descriptor vectors as is the case for virtual screening in the chemical structure domain). Thus, X for some appropriate d. For the bipartite example presented above, given a training sample S=(S,S)\u03b5X\u00d7X, the (linear) kernel-based ranking algorithm learns a linear ranking function \u0192:X\u2192 given by \u0192(x)=w\u00b7x, wherein the weight vector w\u03b5 is selected as follows:",{"@attributes":{"id":"p-0057","num":"0056"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"munder":{"mi":"min","mrow":{"mi":"w","mo":"\u2208","msup":{"mi":["\u211d","d"]}}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":"1","mi":"mn"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msub":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"w","mo":"\u00b7","msubsup":{"mi":["x","i"],"mo":"+"}},{"mi":"w","mo":"\u00b7","msubsup":{"mi":["x","j"],"mo":"-"}}],"mo":"-"}}}},"mo":"+"}}}},{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","mi":"C"}},"mo":"\u2062","msup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"w"},"mn":"2"}}],"mo":"+"}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}}},"where \u2225w\u2225 denotes the Euclidean norm of w and C>0 is an appropriate regularization parameter.","More generally, if  is an RKHS of real-valued functions on X, the kernel-based ranking algorithm learns a ranking function \u0192\u03b5 as follows:",{"@attributes":{"id":"p-0060","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"munder":{"mi":"min","mrow":{"mi":["f","\u2131"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":"1","mi":"mn"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msub":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"-"}}}},"mo":"+"}}}},{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":"\u2062","mi":"C"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"f"},"mi":"\u2131","mn":"2"}}],"mo":"+"}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}}},"where \u2225\u0192\u2225 denotes the RKHS norm of \u0192 in . In practice, the above optimization problem may be solved by reduction to a convex quadratic program (QP), much as is done in the case of SVMs for classification. In particular, if K:X\u00d7X\u2192 is the kernel function associated with  then the above optimization problem reduces to the following QP over mn variables \u03b1(1\u2266i\u2266m, 1\u2266j\u2266n):",{"@attributes":{"id":"p-0062","num":"0061"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":["min","\u03b1"]},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"msub":{"mi":["\u03b1","ij"]},"mo":"\u2062","mrow":{"msub":{"mi":["\u03b1","ki"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["x","i"],"mo":"+"},{"mi":["x","k"],"mo":"+"}],"mo":","}}},{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["x","i"],"mo":"+"},{"mi":["x","i"],"mo":"-"}],"mo":","}}},{"mi":"K","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["x","j"],"mo":"-"},{"mi":["x","k"],"mo":"+"}],"mo":","}}},{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["x","j"],"mo":"-"},{"mi":["x","i"],"mo":"-"}],"mo":","}}}],"mo":["-","-","+","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"26.9em","height":"26.9ex"}}}]}}}}}}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msub":{"mi":["\u03b1","ij"]}}}],"mo":"-"}}},{"mrow":[{"mrow":[{"mi":["subject","to"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mn":"0"},{"mfrac":{"mi":["C","mn"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["for","all","i"]}],"mo":["\u2264","\u2264"],"msub":{"mi":["\u03b1","ij"]}},{"mi":"j","mo":"."}],"mo":","}],"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}}]}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}}},"Given the solution \u03b1 to the above QP, the solution to Equation 4 is given by:",{"@attributes":{"id":"p-0064","num":"0063"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"mrow":{"msub":{"mi":["\u03b1","ij"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","i"],"mo":"+"},"mo":",","mi":"x"}}},{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","j"],"mo":"-"},"mo":",","mi":"x"}}}],"mo":"-"}}},"mo":"."}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}}},"Thus, given a training sample S=(S,S)\u03b5X\u00d7Xand a kernel function K corresponding to an RKHS  the bipartite kernel-based ranking algorithm learns a ranking function \u0192:X\u2192 in  by solving the QP in Equation 5, and then constructing \u0192 as in Equation 6.","In practice, ranking accuracy at the top of a ranked list may often be more important than accuracy at the bottom of the list. Indeed, in virtual screening, e.g., where the goal may be to identify active chemical structures in a large chemical library containing mostly inactive chemical structures, it is especially important that active chemical structures are returned at the top of the ranking. Taught herein is an exemplary ranking algorithm that focuses on ranking accuracy at the top in the context of bipartite ranking. Note that a similar extension is possible for ranking with real-valued labels.","As above, consider the bipartite ranking setting in which the input consists of a training sample S=(S,S) including a sequence of positive examples S=(x, . . . , x) and a sequence of negative examples S=(x, . . . , x), wherein xand xare instances in some instance space X and wherein the goal is to learn a real-valued ranking function \u0192:X\u2192 that ranks accurately future instances in X. For the situation where correct ranking is particularly important at the top of the list, the ranking quality of \u0192 can be measured as follows:",{"@attributes":{"id":"p-0068","num":"0067"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":["err","new"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","S"],"mo":";"}}},{"munder":{"mi":"max","mrow":{"mn":"1","mo":["\u2264","\u2264"],"mi":["j","n"]}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mfrac":{"mn":"1","mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"<"}}},"mo":"+","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"="}}}}}}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"\u2062","mi":"a"}}}]}}}}},"where Iis 1 if \u03c6 is true and 0 otherwise. Equation 1a advantageously accounts for the fraction of positives ranked below the highest ranking negative (in expectation, assuming ties are broken uniformly at random). Therefore, minimizing such corresponds to maximizing the number of positives that appear before the first negative in the ranked list. Similar to Equation 1, Equation 1a may be minimized using an appropriately regularized version of the following convex upper bound:",{"@attributes":{"id":"p-0070","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":{"mi":"max","mrow":{"mn":"1","mo":["\u2264","\u2264"],"mi":["j","n"]}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mfrac":{"mn":"1","mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","msub":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"-"}}}},"mo":"+"}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mn":"2","mo":"\u2062","mi":"a"}}}]}}}}},"wherein,",{"@attributes":{"id":"p-0072","num":"0071"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"a","mo":"+"},"mo":"=","mrow":{"mo":"{","mtable":{"mtr":[{"mtd":[{"mi":"a"},{"mrow":{"mrow":{"mi":["if","a"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":">","mn":"0"}}]},{"mtd":[{"mn":"0"},{"mrow":{"mi":"otherwise","mo":"."}}]}]}}}}}},"Where  is an RKHS of real-valued functions on X, an algorithm may be designed to minimize a regularized version of the above quantity over \u0192\u03b5 as follows:",{"@attributes":{"id":"p-0074","num":"0073"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":{"mi":"min","mrow":{"mi":["f","\u2131"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"munder":{"mi":"max","mrow":{"mn":"1","mo":["\u2264","\u2264"],"mi":["j","n"]}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mfrac":{"mn":"1","mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"-"}}}},"mo":"+"}}}}},{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"C"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"f"},"mi":"\u2131","mn":"2"}}],"mo":"+"}}}},{"mrow":{"mo":["(",")"],"mrow":{"mn":"3","mo":"\u2062","mi":"a"}}}]}}}}},"wherein \u2225\u0192\u2225 denotes the RKHS norm of \u0192 in  and wherein C>0 is an appropriate regularization parameter. As in the previous example, the above optimization problem may be solved by considering the dual of an equivalent optimization problem. In particular, if K:X\u00d7X\u2192 is the kernel function associated with , the above optimization problem may be rewritten, with the help of slack variables and taking the Lagrangian dual, as the following problem over mn variables \u03b1(1\u2266i\u2266m, 1\u2266j\u2266n):",{"@attributes":{"id":"p-0076","num":"0075"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":["min","\u03b1"]},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["\u03b1","ij"]},"mo":"\u2062","mrow":{"msub":{"mi":["\u03b1","kl"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["x","i"],"mo":"+"},{"mi":["x","k"],"mo":"+"}],"mo":","}}},{"mi":"K","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["x","i"],"mo":"+"},{"mi":["x","l"],"mo":"-"}],"mo":","}}},{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["x","j"],"mo":"-"},{"mi":["x","k"],"mo":"+"}],"mo":","}}},{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["x","j"],"mo":"-"},{"mi":["x","l"],"mo":"-"}],"mo":","}}}],"mo":["-","-","+"]}}}}}}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msub":{"mi":["\u03b1","ij"]}}}],"mo":"-"}}},{"mi":["subject","to"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mrow":{"mn":"0","mo":["\u2264","\u2264"],"msub":{"mi":["\u03b1","ij"]},"mrow":{"msub":{"mi":["\u03b3","j"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["for","all","i"]}},"mo":",","mi":"j"},{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msub":{"mi":["\u03b3","j"]}},{"mfrac":{"mi":["C","m"]},"mo":"."}],"mo":"="}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},{"mtext":{}},{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},{"mtext":{}},{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}}]}},{"mrow":{"mo":["(",")"],"mrow":{"mn":"4","mo":"\u2062","mi":"a"}}}]}}}}},"In contrast with the previously presented bipartite kernel-based ranking algorithm, the above dual is not a quadratic program (QP) due to the form of the constraints. A gradient projection method may still be used solve the above dual (in this case, the projection step is more complicated, but can be performed using an lprojection. With this projection, the algorithm converges to an \u03b5-accurate solution in O(((m+n)mn log (mn))\/\u03b5) time. Given the solution \u03b1 to the above optimization problem, the solution to Equation. (3a) is given by:",{"@attributes":{"id":"p-0078","num":"0077"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"msub":{"mi":["\u03b1","ij"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","i"],"mo":"+"},"mo":",","mi":"x"}}},{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","j"],"mo":"-"},"mo":",","mi":"x"}}}],"mo":"-"}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mn":"5","mo":"\u2062","mi":"a"}}}]}}}}},"Thus, given a training sample S=(S,S)\u03b5X\u00d7Xand a kernel function K corresponding to an RKHS  the above modified bipartite kernel-based ranking algorithm learns a ranking function \u0192:X\u2192 in  by solving the optimization problem in Equation 4a, and then constructing \u0192 as in Equation 5a.","Ranking with Real-Valued Labels:","In many cases, a training set of data elements may be provided where the data elements are known (or expected) to have a real-valued labels pertinent to relevance. For example, representation of chemical structures may be provided where all of the chemical structures are known (or expected) to have varying degrees of a desired characteristic structure and\/or property with respect to a particular target. In such cases it is of interest to establish a fine-grained ranking among data elements in order to identify the most promising candidates. Thus, in exemplary embodiments a training set of representation of chemical structures may be provided where each chemical structure is associated, e.g., with an activity level such as an experimentally determined level of biological activity. The goal in such circumstances is to derive a ranking function whereby those chemical structures with higher degrees of a desirable characteristic structure and\/or property are ranked higher.","Contrasted with binary ranking methods where data elements are associated with one of only two states (e.g., active or inactive for chemical structures), real-value based ranking associates data elements with real-valued labels (ranking with real-valued labels has been studied by Agarwal, S.; Niyogi, P., Generalization bounds for ranking algorithms via algorithmic stability, Journal of Machine Learning Research 2009, 10, 441-474). Real-value based ranking can generally be described as follows: A labeled training sample S=((x,y), . . . , (x,y)), is provided, wherein xis an instance in some instance space X and the yis a real-valued labels denoting relevance of a corresponding x. The label training Sample may be advantageously analyzed to determine a ranking function \u0192:X\u2192 that ranks accurately future instances in X by relevance, e.g., ranks instances with higher relevance values higher. The ranking quality of \u0192 in this setting can be measured in terms of the following relevance-weighted ranking error:",{"@attributes":{"id":"p-0083","num":"0082"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"err","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","S"],"mo":";"}}},{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"mi":"P"}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2208","mi":"P"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","j"]}],"mo":"-"}},{"mo":["[","]"],"mrow":{"msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","j"]}}}],"mo":"<"}}},"mo":"+","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","j"]}}}],"mo":"="}}}}}}],"mo":"\u2061"}}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}}},"where P={(i,j)|y>y} denotes a pair of instances in S. The error contribution as between each pair of instances may be weighted by the difference in relevance between the instances, i.e., (y\u2212y). Thus, incorrectly ranked pairs of instances with a large differences in relevance values (e.g., pairs of chemical structures with large differences in activities) result in a greater error contribution than incorrectly ranked pairs of instances with smaller differences in relevance values (e.g., pairs of chemical structures with similar activities).","An exemplary kernel-based ranking algorithm for minimizing the relevance-weighted ranking error of Equation 7 is taught herein. As in the bipartite case, the ranking error in Equation 7 cannot be minimized directly. Thus, the kernel-based ranking algorithm minimizes a regularized version of the following convex upper bound on the ranking error:",{"@attributes":{"id":"p-0086","num":"0085"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"mi":"P"}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2208","mi":"P"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","j"]}],"mo":"-"}},{"mo":["(",")"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","j"]}}}],"mo":"-"}}],"mo":"-"}},"mo":"+"},"mo":"."}}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}}},{"@attributes":{"id":"p-0087","num":"0086"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0088","num":"0087"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","j"]}],"mo":"-"}},{"mo":["[","]"],"mrow":{"msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"<"}}},"mo":"+","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"="}}}}}}],"mo":"\u2061"}}},"br":{},"sub":["i","i","j","j","i","j","i","j","i","j"]},"Given a training sample S\u03b5(X\u00d7)and an RKHS  of real-valued functions on X, the kernel-based ranking algorithm taught herein advantageously selects a ranking function \u0192\u03b5 as follows:",{"@attributes":{"id":"p-0090","num":"0089"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"munder":{"mi":"min","mrow":{"mi":["f","\u2131"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"mi":"P"}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2208","mi":"P"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","j"]}],"mo":"-"}},{"mo":["(",")"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","j"]}}}],"mo":"-"}}],"mo":"-"}},"mo":"+"}}},{"mfrac":{"mn":"1","mrow":{"mn":"2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"C"}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"f"},"mi":"\u2131","mn":"2"}}],"mo":"+"}},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}}},"It can be shown that if K:X\u00d7X\u2192 is the kernel function associated with , then the above optimization problem reduces to the following convex QP in |P| variables \u03b1(i,j)\u03b5P):",{"@attributes":{"id":"p-0092","num":"0091"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":["min","\u03b1"]},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2208","mi":"P"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["k","l"],"mo":","}},"mo":"\u2208","mi":"P"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"msub":{"mi":["\u03b1","ij"]},"mo":"\u2062","mrow":{"msub":{"mi":["\u03b1","kl"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["x","i"]},{"mi":["x","k"]}],"mo":","}}},{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["x","i"]},{"mi":["x","l"]}],"mo":","}}},{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["x","j"]},{"mi":["x","k"]}],"mo":","}}},{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["x","j"]},{"mi":["x","l"]}],"mo":","}}}],"mo":["-","-","+"]}}}}}}},{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2208","mi":"P"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["\u03b1","ij"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","j"]}],"mo":"-"}}}}],"mo":"-"}}},{"mi":["subject","to"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mn":"0","mo":["\u2264","\u2264"],"msub":{"mi":["\u03b1","ij"]},"mrow":{"mfrac":{"mi":"C","mrow":{"mo":["\uf603","\uf604"],"mi":"P"}},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}],"mi":["for","all"],"mrow":{"mrow":{"mo":["(","\u2062",")"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":["i","j"],"mo":","}},"mo":"."}}}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},{"mtext":{}},{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}}]}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}}},"Given the solution \u03b1 to the above QP, the solution to Equation 9 is given by:",{"@attributes":{"id":"p-0094","num":"0093"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2208","mi":"P"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"msub":{"mi":["\u03b1","ij"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["x","i"]},"mo":",","mi":"x"}}},{"mi":"K","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["x","j"]},"mo":",","mi":"x"}}}],"mo":"-"}}},"mo":"."}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}}},"Thus, given a training sample S\u03b5(X\u00d7)and a kernel function K corresponding to an RKHS , the kernel-based ranking algorithm for the real-valued labels setting learns a ranking function \u0192:X\u2192 in  A by solving the QP in Equation 10, and then constructing \u0192 as in Equation 11.","Virtual Screening Experiments in the Chemical Structure Domain:","In virtual screening experiments conducted, the data sets included five sets of representations of chemical structures that each target a different protein, and a background set of chemical structures assumed to be inactive. The five active sets contained representations of fifty chemical structures each in the following classes: reversible inhibitors of cyclin-dependent kinase 2 (CDK2), cyclooxygenase-2 (COX2), factor Xa (FXa), phosphodiesterase-5 (PDE5), and reversible antagonists of the \u03b1adrenoceptor (\u03b1AR). The \u201cinactive\u201d set contained representations of 1,892 chemical structures drawn from the National Cancer Institute (NCI) diversity set. Thus, for each of the five targets, a total of fifty active chemical structures were represented and 2092 inactives (including the 1,892 background chemical structures and the 200 chemical structures belonging to the other four active sets) were represented. Additional details on the data sets may be found in Jorissen, R. N.; Gilson, M. K., Virtual screening of molecular databases using a support vector machine, Journal of Chemical Information and Modeling 2005, 45, 549-561.","Given the prevalence of binary molecular fingerprints as chemical descriptors in virtual screening (see, e.g., Wassermann, A. M.; Geppert, H.; Bajorath, J., Searching for target-selective chemical structures using different combinations of multi-class support vector machine ranking methods, kernel functions, and fingerprint descriptors, Journal of Chemical Information and Modeling 2009, 49, 582-592; Wilton, D.; Willett, P., Comparison of ranking methods for virtual screening in lead-discovery programs, Journal of Chemical Information and Computer Sciences 2003, 43, 469-474; and Geppert, H.; Humrich, J.; Stumpfe, D.; G{umlaut over ( )}artner, T.; Bajorath, J., Ligand prediction from protein sequence and small molecule information using support vector machines and fingerprint descriptors, Journal of Chemical Information and Modeling 2009, 49, 767-779) each chemical structure in the above data sets was represented using Molprint2D fingerprints (see, e.g., Bender, A.; Mussa, H. Y; Glen, R. C.; Reiling, S., Molecular similarity searching using atom environments, information-based feature selection, and a naive Bayesian classifier, Journal of Chemical Information and Computer Sciences 2004, 44, 170-178), which can be computed using publicly available software (e.g., MOLPRINT 2D, http:\/\/www.molprint.com) and have been shown recently to give superior performance compared to other types of fingerprints in SVM-based ranking applications. A Molprint2D fingerprint associated with a chemical structure includes of a set of strings describing the atom environments present in the chemical structure. These strings were converted to bit vectors by enumerating all the atom environment strings present in the data sets and assigning a unique fingerprint position to each such string. For each chemical structure, the bit corresponding to a given string was set to 1 if the associated atom environment was present in the chemical structure, and 0 otherwise. For the experiments conducted the result was a fingerprint representation with a total of 17,221 bit positions.","Three different ways were considered to split representations of the active chemical structures into training and test sets. In particular, representations of the fifty active chemical structures for each target were listed in a manner such that chemical structures with similar chemistries were grouped together. The chemical structures were then split in two ways. In the first split, representations of the top twenty-five chemical structures in each list were separated from the bottom twenty-five, with one half going to the training set and the other half going to the test set; in this case, representations of the active chemical structures in the training and test sets were mostly quite different. In the second split, representations of the odd-numbered chemical structures in each list were separated from representations of the even-numbered chemical structures, again with one half going to the training set and the other half going to the test set; in this case, the active chemical structures associated with the training and test sets were highly similar. In each case, the representations of background chemical structures were partitioned in a fixed manner (odd-numbered entries in a fixed ordering were separated from even-numbered entries), with representations of half the chemical structures going to the training set and representations of half going to the test set. The above two splits are both extremes, representing the worst-case and best-case possibilities, respectively. In practice, the training set is likely to be more representative of the test set than in the first (worst-case) split above, and less representative than in the second (best-case) split. To simulate this, a third split was considered, in which the active and background sets were each partitioned randomly, with one half going to the training set and the other half going to the test set.","In addition, to evaluating the impact of the training set size, experiments in which 20%, 40%, 60%, 80% and 100% of the training chemical structures were used. This process was repeated with ten random partitions of the representations of chemical structures into training and test sets.","As discussed above the performance of the exemplary bipartite kernel-based ranking algorithm was compared with that of a conventional SVM-based ranking method on the above data sets. Thus the following performance measures were considered:","1. Bipartite ranking error: as (approximately) optimized by the bipartite kernel-based ranking algorithm. Specifically, given a ranking function \u0192:X\u2192 and a test sample T=(T,T) including m positive instances T=(x, . . . , x)\u03b5Xand n negative instances T=(x, . . . , x)\u03b5Xthe bipartite ranking error of \u0192 with respect to T measures the fraction of positive-negative pairs in T mis-ranked by \u0192, and is given by:",{"@attributes":{"id":"p-0103","num":"0102"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"err","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},{"mfrac":{"mn":"1","mi":"mn"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"mrow":{"mo":["[","]"],"mrow":{"msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"<"}}},"mo":"+","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"="}}}}}},"mo":"."}}}}],"mo":"="}}}},"2. Area under the ROC curve (AUC): simply one minus the bipartite ranking error; this measure was included since it has the intuitive interpretation of being the fraction of positive-negative pairs in T that are ranked correctly by \u0192, and is widely used as a ranking performance measure in the binary setting (for more information about the AUC see Hanley, J. A.; McNeil, B. J., The meaning and use of the area under a receiver operating characteristic (ROC) curve, Radiology 1982, 143, 29-36).","3. Average precision: the precision at a given position r is widely used in information retrieval to measure the concentration of relevant results in the top r results in a ranked list. Applied to the virtual screening experiments disclosed herein, the precision of \u0192 at position r in T for 1\u2266r\u2266m+n is defined as the proportion of actives in the top r chemical structures returned by \u0192:",{"@attributes":{"id":"p-0106","num":"0105"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"msub":{"mi":["prec","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},{"mfrac":{"mn":"1","mi":"r"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"r"},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"msub":[{"mi":["y","\u03c0"]},{"mn":"1","mrow":{"mo":["(",")"],"mi":"i"}}],"mo":"-"},{"mo":"+","mn":"1"}],"mo":"="}}}}}],"mo":"="},"mo":","}}}},"where yis the label (+1 for active and \u22121 for inactive) of the ith ranking instance returned by \u0192. In particular, if T\u2032=((xy), . . . , (x,y)) is constructed from T according to",{"@attributes":{"id":"p-0108","num":"0107"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":["x","k"]},{"mi":["y","k"]}],"mo":","}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","k"],"mo":"+"},"mo":",","mrow":{"mo":"+","mn":"1"}}}},{"mrow":{"mrow":{"mrow":{"mi":["for","k"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":"=","mn":"1"},"mo":[",","\u2062",","],"mi":["\u2026","m"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}}]},{"mtd":[{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":"x","mrow":{"mi":["k","m"],"mo":"-"},"mo":"-"},"mo":",","mrow":{"mo":"-","mn":"1"}}}},{"mrow":{"mrow":[{"mrow":[{"mi":["for","k"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mi":"m","mo":"+","mn":"1"}],"mo":"="},{"mi":["m","n"],"mo":"+"}],"mo":[",","\u2062",",",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}}]}]}}],"mo":"="}}}},"then \u03c0(k) to denote the position of xin the ranking returned by \u0192, and \u03c0(i) to denote the index in T\u2032 of the ith ranking instance returned by \u0192. Precision is closely related to the enrichment factor (EF) used for example by Jorissen et al. (2005) to evaluate rankings in virtual screening. In particular, the enrichment factor at a given position r measures the proportion of actives in the top r chemical structures relative to the overall proportion of actives in the complete sample:",{"@attributes":{"id":"p-0110","num":"0109"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"AP","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},{"mfrac":{"mn":"1","mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"r","mo":"=","mn":"1"},{"mi":["m","n"],"mo":"+"}]},"mo":"\u2062","mrow":{"mrow":{"mo":["[","]"],"mrow":{"mrow":{"msub":{"mi":["prec","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},"mo":"\u00d7","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"msub":[{"mi":["y","\u03c0"]},{"mn":"1","mrow":{"mo":["(",")"],"mi":"i"}}],"mo":"-"},{"mo":"+","mn":"1"}],"mo":"="}}}}},"mo":"."}}}],"mo":"="}}}},"Unlike bipartite ranking error and AUC, which measure global ranking quality of a ranked list, the average precision emphasizes ranking quality at the top of a ranked list. We note that if the ranking function f assigns distinct scores to all the chemical structures in T, then the average precision can also be written as:",{"@attributes":{"id":"p-0112","num":"0111"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"AP","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},{"mfrac":{"mn":"1","mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mrow":{"mo":["[","]"],"mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","j"]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}}],"mo":"\u2265"}}}},{"munderover":{"mo":"\u2211","mrow":[{"mi":"k","mo":"=","mn":"1"},{"mi":["m","n"],"mo":"+"}]},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","k"]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}}],"mo":"\u2265"}}}}]}},"mo":"."}}}],"mo":"="}}}},"See also, the modified enrichment factor taught by Jorissen et al. (2005) which when evaluated over the complete sample is inversely proportional to the average rank (AR) of the actives:",{"@attributes":{"id":"p-0114","num":"0113"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msup":{"mi":["EF","\u2032"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},{"mfrac":{"mrow":[{"mi":["m","n"],"mo":"+"},{"mn":"2","mo":"\u2062","mrow":{"mi":"AR","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}}}]},"mo":"=","mrow":{"mfrac":{"mrow":[{"mi":["m","n"],"mo":"+"},{"mfrac":{"mn":"2","mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"k","mo":"=","mn":"1"},{"mi":["m","n"],"mo":"+"}]},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","k"]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}}],"mo":"\u2265"}}}}}}}]},"mo":"."}}],"mo":"="}}}},"4. Number of actives in the top twenty-five: A simple measure of the quality of a ranking at a cutoff r is the number of actives returned in the top r chemical structures:",{"@attributes":{"id":"p-0116","num":"0115"},"maths":{"@attributes":{"id":"MATH-US-00027","num":"00027"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["act","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"r"},"mo":"\u2062","mrow":{"msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"msub":[{"mi":["y","\u03c0"]},{"mn":"1","mrow":{"mo":["(",")"],"mi":"i"}}],"mo":"-"},{"mo":"+","mn":"1"}],"mo":"="}}},"mo":"."}}],"mo":"="}}}},"The number of actives in the top twenty-five chemical structures, act(\u0192; T) is reported herein. Note that the precision at r is simply:",{"@attributes":{"id":"p-0118","num":"0117"},"maths":{"@attributes":{"id":"MATH-US-00028","num":"00028"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["prec","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},{"mfrac":{"mrow":{"msub":{"mi":["act","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},"mi":"r"},"mo":"."}],"mo":"="}}}},"Together with precision, another measure frequently reported when evaluating a ranking at a cutoff r is the recall, which is the proportion of all actives in the sample that are returned in the top r chemical structures:",{"@attributes":{"id":"p-0120","num":"0119"},"maths":{"@attributes":{"id":"MATH-US-00029","num":"00029"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["recall","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},{"mfrac":{"mrow":{"msub":{"mi":["act","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},"mi":"m"},"mo":"."}],"mo":"="}}}},"In the present data sets, the total number of actives in each test sample is m=twenty five. In an ideal ranking in our case, all twenty-five active chemical structures would be placed above all the inactive chemical structures, which would give act(\u0192; T)=25.","5. Number of active in the top one-hundred: the number of actives in the top one-hundred chemical structures, act(f; T). Since the total number of actives in each test sample in is m=25 (see above):\n\nact(\u0192;)\u2266act(\u0192;)\u226625.\n","As discussed above the performance of an exemplary bipartite kernel-based ranking algorithm was compared with that of SVM-based ranking on five data sets including actives with respect to CDK2, COX2, FXa, PDE5, and \u03b1AR, and a background set of inactives. Each representation of a chemical structure in these data sets was represented using a binary molecular fingerprint vector with 17; 221 bit positions; thus the instance space in this case was X={0,1}for d=17; 221.","Experiments were conducted for two types of kernels: the linear kernel given by:\n\n(x,x\u2032)=x\u00b7x\u2032,\n","and the Tanimoto kernel that is often used with binary fingerprint vectors:",{"@attributes":{"id":"p-0126","num":"0125"},"maths":{"@attributes":{"id":"MATH-US-00030","num":"00030"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["K","Tanimoto"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":",","msup":{"mi":["x","\u2032"]}}}},{"mfrac":{"mrow":[{"mi":"x","mo":"\u00b7","msup":{"mi":["x","\u2032"]}},{"mrow":[{"mi":["x","x"],"mo":"\u00b7"},{"msup":[{"mi":["x","\u2032"]},{"mi":["x","\u2032"]}],"mo":"\u00b7"},{"mi":"x","mo":"\u00b7","msup":{"mi":["x","\u2032"]}}],"mo":["+","-"]}]},"mo":"."}],"mo":"="}}}},"For the SVM algorithm, Joachim's SVMlight software was used (SVMlight, http:\/\/svmlight.joachims.org.; see also Joachims, T., Making large-scale SVM learning practical, In Advances in kernel Methods\u2014Support Vector Learning; MIT Press, 1999; Chapter 11, pp 169-184). The regularization parameter C in each training run was selected by 5-fold cross-validation from the range {0.1, 1, 10, 100, 1000}; in each case, the value of C that gave the lowest average bipartite ranking error (equivalently, the highest average AUC) across the five folds was used in training. An exemplary bipartite kernel-based ranking algorithm was also run through SVMlight (using the \u2018\u2212z p\u2019 software option; see SVMlight, http:\/\/svmlight.joachims.org). The regularization parameter C was selected as above.","As described above, three different splits of the data sets into training and test sets were considered. The results on the 1st\/2nd splits are shown in Table 1, for the linear kernel, and in Table 2 for the Tanimoto kernel. In each case, the better performance is shown in boldface. While not much can be said about the superiority of one algorithm over the other from these results, one observation is that in the case of kernel-based ranking algorithms, the use of the Tanimoto kernel almost consistently improves performance over the linear kernel when measured in terms of the bipartite ranking error or AUC. SVM based ranking did not exhibit similar improvement.",{"@attributes":{"id":"p-0129","num":"0128"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"364pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Virtual screening results on 1st\/2nd splits with the linear kernel."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},{},{},{},"Actives in","Actives in"]},{"entry":[{},{},"Ranking error","AUC","Average precision","top 25","top 100"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Data","Train\/test",{},"Rank",{},"Rank",{},"Rank",{},"Rank",{},"Rank"]},{"entry":["set","split","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["CDK2","1st\/2nd","0.3141",{"b":"0.2723"},"0.6859",{"b":"0.7277"},{"b":"0.3414"},"0.1885",{"b":"8"},"\u20027",{"b":"11"},"10"]},{"entry":[{},"2nd\/1st",{"b":"0.3209"},"0.3495",{"b":"0.6791"},"0.6505",{"b":"0.3623"},"0.3233",{"b":"9"},"\u20027",{"b":"12"},"11"]},{"entry":["COX2","1st\/2nd","0.1545",{"b":"0.1455"},"0.8455",{"b":"0.8545"},{"b":"0.7405"},"0.7397",{"b":"18"},{"b":"18"},{"b":"19"},{"b":"19"}]},{"entry":[{},"2nd\/1st","0.2463",{"b":"0.2462"},"0.7537",{"b":"0.7538"},"0.3161",{"b":"0.3162"},{"b":"6"},{"b":"\u20026"},{"b":"13"},{"b":"13"}]},{"entry":["FXa","1st\/2nd",{"b":"0.0019"},"0.0060",{"b":"0.9981"},"0.9940",{"b":"0.9495"},"0.9090",{"b":"22"},{"b":"22"},{"b":"25"},{"b":"25"}]},{"entry":[{},"2nd\/1st","0.1188",{"b":"0.1159"},"0.8812",{"b":"0.8841"},"0.7194",{"b":"0.7206"},{"b":"18"},{"b":"18"},{"b":"18"},{"b":"18"}]},{"entry":["PDE5","1st\/2nd","0.2067",{"b":"0.1994"},"0.7933",{"b":"0.8006"},"0.0996",{"b":"0.1002"},{"b":"3"},{"b":"\u20023"},{"b":"14"},"13"]},{"entry":[{},"2nd\/1st",{"b":"0.1591"},"0.1595",{"b":"0.8409"},"0.8405","0.1139",{"b":"0.1140"},{"b":"2"},{"b":"\u20022"},{"b":"11"},{"b":"11"}]},{"entry":["\u03b1AR","1st\/2nd","0.0743",{"b":"0.0546"},"0.9257",{"b":"0.9454"},"0.6927",{"b":"0.7259"},"15",{"b":"16"},"19",{"b":"21"}]},{"entry":[{},"2nd\/1st","0.0302",{"b":"0.0263"},"0.9698",{"b":"0.9737"},"0.6527",{"b":"0.7193"},"14",{"b":"17"},"20",{"b":"22"}]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0130","num":"0129"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"364pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Virtual screening results on 1st\/2nd splits with the Tanimoto kernel."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},{},{},{},"Actives in","Actives in"]},{"entry":[{},{},"Ranking error","AUC","Average precision","top 25","top 100"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Data","Train\/test",{},"Rank",{},"Rank",{},"Rank",{},"Rank",{},"Rank"]},{"entry":["set","split","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["CDK2","1st\/2nd","0.2616",{"b":"0.2315"},"0.7384",{"b":"0.7685"},{"b":"0.2841"},"0.1572",{"b":"8"},"6",{"b":"13"},{"b":"13"}]},{"entry":[{},"2nd\/1st",{"b":"0.2734"},"0.2807",{"b":"0.7266"},"0.7193",{"b":"0.3428"},"0.3356",{"b":"7"},{"b":"7"},{"b":"13"},"12"]},{"entry":["COX2","1st\/2nd","0.1688",{"b":"0.1574"},"0.8312",{"b":"0.8426"},"0.6914",{"b":"0.7265"},{"b":"18"},{"b":"18"},{"b":"19"},{"b":"19"}]},{"entry":[{},"2nd\/1st","0.2672",{"b":"0.2279"},"0.7328",{"b":"0.7721"},"0.2891",{"b":"0.3299"},{"b":"6"},{"b":"6"},"12",{"b":"15"}]},{"entry":["FXa","1st\/2nd",{"b":"0.0026"},"0.0031",{"b":"0.9974"},"0.9969",{"b":"0.9305"},"0.9162",{"b":"22"},{"b":"22"},{"b":"25"},{"b":"25"}]},{"entry":[{},"2nd\/1st","0.1101",{"b":"0.0610"},"0.8899",{"b":"0.9390"},"0.6927",{"b":"0.6938"},"17",{"b":"18"},"18",{"b":"19"}]},{"entry":["PDE5","1st\/2nd",{"b":"0.1467"},"0.1666",{"b":"0.8533"},"0.8334",{"b":"0.1226"},"0.1050",{"b":"4"},"3",{"b":"15"},"14"]},{"entry":[{},"2nd\/1st","0.2180",{"b":"0.1241"},"0.7820",{"b":"0.8759"},"0.0935",{"b":"0.1104"},"1",{"b":"2"},{"b":"12"},"\u20029"]},{"entry":["\u03b1AR","1st\/2nd","0.0810",{"b":"0.0519"},"0.9190",{"b":"0.9481"},"0.6875",{"b":"0.7091"},{"b":"16"},{"b":"16"},"20",{"b":"21"}]},{"entry":[{},"2nd\/1st","0.0291",{"b":"0.0268"},"0.9709",{"b":"0.9732"},"0.6271",{"b":"0.6801"},"14",{"b":"15"},"20",{"b":"22"}]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}}]}}},"Tables 3 and 4 show the results on the Odd\/Even splits, using the linear kernel and the Tanimoto kernel, respectively. As taught herein the use of the Tanimoto kernel leads to near consistent improvement in kernel-based ranking algorithm performance over the linear kernel in terms of the bipartite ranking error or AUC. A second observation is the drastic improvement in performance on the Odd\/Even splits (Tables 3-4) as compared to the 1st\/2nd splits (Tables 1-2), regardless of the particular algorithm\/kernel used. This is consistent with the 1st\/2nd splits representing a worst-case scenario and the Odd\/Even splits representing a best-case scenario.",{"@attributes":{"id":"p-0132","num":"0131"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"364pt","align":"center"}},"thead":{"row":{"entry":"TABLE 3"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Virtual screening results on Odd\/Even splits with the linear kernel."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},{},{},{},"Actives in","Actives in"]},{"entry":[{},{},"Ranking error","AUC","Average precision","top 25","top 100"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Data","Train\/test",{},"Rank",{},"Rank",{},"Rank",{},"Rank",{},"Rank"]},{"entry":["set","split","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}},{"entry":["CDK2","Odd\/Even","0.0353",{"b":"0.0298"},"0.9647",{"b":"0.9702"},{"b":"0.8945"},"0.8302",{"b":"21"},"19",{"b":"23"},"22"]},{"entry":[{},"Even\/Odd",{"b":"0.0119"},"0.0307",{"b":"0.9881"},"0.9693",{"b":"0.9089"},"0.8799",{"b":"22"},"20","23",{"b":"24"}]},{"entry":["COX2","Odd\/Even","0.0068",{"b":"0.0026"},"0.9932",{"b":"0.9974"},"0.9285",{"b":"0.9511"},{"b":"22"},{"b":"22"},"24",{"b":"25"}]},{"entry":[{},"Even\/Odd","0.0299",{"b":"0.0073"},"0.9701",{"b":"0.9927"},"0.8803",{"b":"0.9111"},"21",{"b":"22"},{"b":"24"},{"b":"24"}]},{"entry":["FXa","Odd\/Even",{"b":"0.0007"},"0.0052",{"b":"0.9993"},"0.9948",{"b":"0.9731"},"0.9073",{"b":"22"},"21",{"b":"25"},{"b":"25"}]},{"entry":[{},"Even\/Odd","0.0005",{"b":"0.0004"},"0.9995",{"b":"0.9996"},"0.9802",{"b":"0.9860"},{"b":"23"},{"b":"23"},{"b":"25"},{"b":"25"}]},{"entry":["PDE5","Odd\/Even",{"b":"0.0005"},"0.0039",{"b":"0.9995"},"0.9961",{"b":"0.9856"},"0.9531",{"b":"24"},"23",{"b":"25"},"24"]},{"entry":[{},"Even\/Odd",{"b":"0.0026"},"0.0047",{"b":"0.9974"},"0.9953",{"b":"0.9498"},"0.9367",{"b":"23"},"22",{"b":"25"},"24"]},{"entry":["\u03b1AR","Odd\/Even",{"b":"0.0143"},"0.0151",{"b":"0.9857"},"0.9849",{"b":"0.9201"},"0.9135",{"b":"22"},{"b":"22"},{"b":"24"},{"b":"24"}]},{"entry":[{},"Even\/Odd",{"b":"0.0023"},"0.0025",{"b":"0.9977"},"0.9975",{"b":"0.9353"},"0.9276",{"b":"22"},{"b":"22"},{"b":"25"},{"b":"25"}]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0133","num":"0132"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"364pt","align":"center"}},"thead":{"row":{"entry":"TABLE 4"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Virtual screening results on Odd\/Even splits with the Tanimoto kernel."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},{},{},{},"Actives in","Actives in"]},{"entry":[{},{},"Ranking error","AUC","Average precision","top 25","top 100"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Data","Train\/test",{},"Rank",{},"Rank",{},"Rank",{},"Rank",{},"Rank"]},{"entry":["set","split","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["CDK2","Odd\/Even","0.0217",{"b":"0.0180"},"0.9783",{"b":"0.9820"},"0.8936",{"b":"0.9113"},{"b":"21"},"20",{"b":"24"},{"b":"24"}]},{"entry":[{},"Even\/Odd",{"b":"0.0084"},"0.0261",{"b":"0.9916"},"0.9739",{"b":"0.9243"},"0.8775",{"b":"22"},"20",{"b":"24"},{"b":"24"}]},{"entry":["COX2","Odd\/Even","0.0030",{"b":"0.0021"},"0.9970",{"b":"0.9979"},"0.9430",{"b":"0.9575"},{"b":"22"},{"b":"22"},{"b":"25"},{"b":"25"}]},{"entry":[{},"Even\/Odd",{"b":"0.0044"},"0.0276",{"b":"0.9956"},"0.9724","0.8922","0.8219",{"b":"22"},"20",{"b":"25"},"23"]},{"entry":["FXa","Odd\/Even",{"b":"0.0010"},"0.0049",{"b":"0.9990"},"0.9951",{"b":"0.9623"},"0.8985",{"b":"22"},"21",{"b":"25"},{"b":"25"}]},{"entry":[{},"Even\/Odd","0.0008",{"b":"0.0003"},"0.9992",{"b":"0.9997"},"0.9683",{"b":"0.9862"},{"b":"23"},{"b":"23"},{"b":"25"},{"b":"25"}]},{"entry":["PDE5","Odd\/Even","0.0021",{"b":"0.0016"},"0.9979",{"b":"0.9984"},"0.9709",{"b":"0.9736"},{"b":"24"},{"b":"24"},{"b":"25"},{"b":"25"}]},{"entry":[{},"Even\/Odd","0.0025",{"b":"0.0019"},"0.9975",{"b":"0.9981"},"0.9503",{"b":"0.9576"},{"b":"23"},{"b":"23"},{"b":"25"},{"b":"25"}]},{"entry":["\u03b1AR","Odd\/Even",{"b":"0.0072"},"0.0082","0.9928","0.9918","0.8916",{"b":"0.9079"},"20",{"b":"21"},{"b":"24"},{"b":"24"}]},{"entry":[{},"Even\/Odd","0.0063",{"b":"0.0035"},"0.9937","0.9965","0.8932",{"b":"0.9206"},"20",{"b":"21"},{"b":"24"},{"b":"25"}]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}}]}}},"As previously discussed, the third group of train\/test splits (in which the data sets were randomly partitioned into training and test sets) was designed to simulate a situation in which the training set would be more representative of a real world situation (as opposed to best and worst case scenarios). Results for the random splits, using linear and Tanimoto kernels, are shown in Tables 5 and 6, respectively. Each number shown in these tables is the average over ten different random splits. In addition, experiments were conducted in which increasing fractions of the training set were used to train the algorithms.","There are several observations made: First, as in the other two splits above, the Tanimoto kernel near consistently showed improved performance compared to the linear kernel in the case of the kernel-based ranking algorithm, when measured in terms of the bipartite ranking error or AUC. Second, the performance on the test set improves with an increase in the training set size; this was true for all five data sets and regardless of the algorithm\/kernel used. Third, when the complete training set including representations of twenty-five actives and representations of 1046 inactives was used (same training set size as in the 1st\/2nd and Odd\/Even splits above), the performance in the case of these random splits lies between the best-case and worst-case performance levels observed with respect to the 1st\/2nd and Odd\/Even splits. Finally, for random train\/test splits using the Tanimoto kernel, the ranking performance of the exemplary kernel-based ranking algorithm, as taught herein, in terms of the bipartite ranking error and AUC is in generally superior to conventional SVM-based ranking. This is also illustrated in , which depicts the performance of each algorithm (using the Tanimoto kernel) averaged over the five data sets. This figure also shows that when measured in terms of the other performance measures such as average precision or number of actives retrieved in the top portion of the ranking, the performance of the two algorithms is similar. Indeed the exemplary kernel-based ranking algorithms considered with respect to the virtual screening experiments reported herein optimize (approximately) the bipartite ranking error or AUC, which are global measures of ranking performance, whereas the other three measures focus on local ranking accuracy at the top of a ranking.",{"@attributes":{"id":"p-0136","num":"0135"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"364pt","align":"center"}},"thead":{"row":{"entry":"TABLE 5"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Virtual screening results on random splits with the linear kernel."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},{},{},{},"Actives in","Actives in"]},{"entry":[{},{},"Ranking error","AUC","Average precision","top 25","top 100"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Data","No. train",{},"Rank",{},"Rank",{},"Rank",{},"Rank",{},"Rank"]},{"entry":["set","(act-inact)","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["CDK2","\u20025-209","0.2859",{"b":"0.2813"},"0.7141",{"b":"0.7187"},{"b":"0.3606"},"0.3436",{"b":"8.5"},"\u20028.2",{"b":"12.5"},"12.1"]},{"entry":[{},"10-418","0.1976",{"b":"0.1834"},"0.8024",{"b":"0.8166"},{"b":"0.5572"},"0.5238",{"b":"13.7"},"13.1",{"b":"16.8"},"16.3"]},{"entry":[{},"15-627","0.1521",{"b":"0.1381"},"0.8479",{"b":"0.8619"},{"b":"0.6512"},"0.6489","16.0",{"b":"16.3"},"18.3",{"b":"18.4"}]},{"entry":[{},"20-836",{"b":"0.0779"},"0.0787",{"b":"0.9221"},"0.9213",{"b":"0.7657"},"0.7206",{"b":"18.9"},"17.3",{"b":"21.1"},"21.0"]},{"entry":[{},"25-1046",{"b":"0.0626"},"0.0632",{"b":"0.9374"},"0.9368",{"b":"0.7998"},"0.7612",{"b":"20.0"},"18.3",{"b":"22.0"},"21.8"]},{"entry":["COX2","\u20025-209",{"b":"0.1950"},"0.1966",{"b":"0.8050"},"0.8034",{"b":"0.6235"},"0.6230","14.9",{"b":"15.0"},{"b":"17.1"},"16.9"]},{"entry":[{},"10-418",{"b":"0.1189"},"0.1200",{"b":"0.8811"},"0.8800","0.7292",{"b":"0.7334"},{"b":"17.7"},"17.4","19.5",{"b":"19.9"}]},{"entry":[{},"15-627","0.0759",{"b":"0.0684"},"0.9241",{"b":"0.9316"},{"b":"0.8021"},"0.7982",{"b":"19.1"},"18.9","20.9",{"b":"21.1"}]},{"entry":[{},"20-836","0.0463",{"b":"0.0379"},"0.9537",{"b":"0.9621"},"0.8352",{"b":"0.8417"},"19.7",{"b":"19.9"},"22.3",{"b":"22.7"}]},{"entry":[{},"25-1046","0.0248",{"b":"0.0170"},"0.9752",{"b":"0.9830"},"0.8737",{"b":"0.8870"},"20.7",{"b":"20.9"},"23.4",{"b":"23.6"}]},{"entry":["FXa","\u20025-209","0.0680",{"b":"0.0664"},"0.9320",{"b":"0.9336"},"0.7847",{"b":"0.7893"},"19.2",{"b":"19.8"},{"b":"21.4"},"21.0"]},{"entry":[{},"10-418","0.0515",{"b":"0.0437"},"0.9485",{"b":"0.9563"},{"b":"0.8621"},"0.8551",{"b":"20.5"},{"b":"20.5"},{"b":"22.5"},{"b":"22.5"}]},{"entry":[{},"15-627",{"b":"0.0242"},"0.0293",{"b":"0.9758"},"0.9707",{"b":"0.8879"},"0.8783",{"b":"20.8"},{"b":"20.8"},{"b":"23.3"},"23.2"]},{"entry":[{},"20-836","0.0282",{"b":"0.0261"},"0.9718",{"b":"0.9739"},{"b":"0.8915"},"0.8877",{"b":"21.1"},"20.9",{"b":"23.4"},"23.3"]},{"entry":[{},"25-1046","0.0221",{"b":"0.0195"},"0.9779",{"b":"0.9805"},{"b":"0.9012"},"0.8943",{"b":"21.5"},"21.0",{"b":"23.8"},"23.1"]},{"entry":["PDE5","\u20025-209","0.2258",{"b":"0.2251"},"0.7742",{"b":"0.7749"},{"b":"0.4719"},"0.4472",{"b":"11.3"},"10.9",{"b":"14.7"},"14.2"]},{"entry":[{},"10-418",{"b":"0.0934"},"0.0984",{"b":"0.9066"},"0.9016",{"b":"0.7469"},"0.7404","17.2",{"b":"17.5"},{"b":"20.7"},"20.2"]},{"entry":[{},"15-627","0.0558",{"b":"0.0486"},"0.9442",{"b":"0.9514"},"0.8408",{"b":"0.8485"},"19.8",{"b":"20.0"},{"b":"22.4"},"22.3"]},{"entry":[{},"20-836",{"b":"0.0296"},"0.0325",{"b":"0.9704"},"0.9675",{"b":"0.8764"},"0.8690",{"b":"20.5"},"20.1",{"b":"23.3"},"23.2"]},{"entry":[{},"25-1046",{"b":"0.0214"},"0.0267",{"b":"0.9786"},"0.9733",{"b":"0.9033"},"0.8876",{"b":"21.7"},"21.1",{"b":"23.5"},"23.1"]},{"entry":["\u03b1AR","\u20025-209",{"b":"0.1023"},"0.1045",{"b":"0.8977"},"0.8955",{"b":"0.6782"},"0.6637",{"b":"15.7"},"15.2",{"b":"19.6"},"19.5"]},{"entry":[{},"10-418",{"b":"0.0511"},"0.0571",{"b":"0.9469"},"0.9429",{"b":"0.7415"},"0.7323",{"b":"16.3"},{"b":"16.3"},{"b":"22.1"},"21.6"]},{"entry":[{},"15-627","0.0388",{"b":"0.0386"},"0.9612",{"b":"0.9614"},{"b":"0.7890"},"0.7788",{"b":"18.1"},"17.4",{"b":"22.4"},{"b":"22.4"}]},{"entry":[{},"20-836","0.0217",{"b":"0.0173"},"0.9783",{"b":"0.9827"},"0.8277",{"b":"0.8584"},"18.9",{"b":"19.5"},"23.2",{"b":"23.8"}]},{"entry":[{},"25-1046",{"b":"0.0130"},"0.0134",{"b":"0.9870"},"0.9866","0.8498",{"b":"0.8609"},"19.3",{"b":"19.9"},{"b":"23.8"},"23.7"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0137","num":"0136"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"364pt","align":"center"}},"thead":{"row":{"entry":"TABLE 6"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Virtual screening results on random splits with the Tanimoto kernel."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},{},{},{},"Actives in","Actives in"]},{"entry":[{},{},"Ranking error","AUC","Average precision","top 25","top 100"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Data","No. train",{},"Rank",{},"Rank",{},"Rank",{},"Rank",{},"Rank"]},{"entry":["set","(not-inact)","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM","SVM"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"11","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["CDK2","\u20025-209","0.3088",{"b":"0.2608"},"0.6912",{"b":"0.7392"},"0.3302",{"b":"0.3318"},"7.8",{"b":"7.9"},"11.5",{"b":"11.7"}]},{"entry":[{},"10-418","0.1880",{"b":"0.1706"},"0.8120",{"b":"0.8294"},{"b":"0.5392"},"0.5110",{"b":"13.2"},"12.6",{"b":"16.6"},"16.3"]},{"entry":[{},"15-627","0.1504",{"b":"0.1261"},"0.8496",{"b":"0.8739"},"0.6168",{"b":"0.6486"},"14.9",{"b":"16.1"},"17.7",{"b":"18.4"}]},{"entry":[{},"20-836","0.1070",{"b":"0.0661"},"0.8930",{"b":"0.9339"},"0.7189",{"b":"0.7294"},{"b":"17.3"},"17.2","20.0",{"b":"21.2"}]},{"entry":[{},"25-1046","0.0632",{"b":"0.0542"},"0.9368",{"b":"0.9458"},{"b":"0.7751"},"0.7507",{"b":"18.5"},"18.0","21.6",{"b":"21.9"}]},{"entry":["COX2","\u20025-209","0.2208",{"b":"0.1961"},"0.7792",{"b":"0.8039"},"0.6188",{"b":"0.6281"},"15.1",{"b":"15.4"},{"b":"17.1"},{"b":"17.1"}]},{"entry":[{},"10-418","0.1315",{"b":"0.1165"},"0.8685",{"b":"0.8835"},"0.7079",{"b":"0.7325"},"17.2",{"b":"17.6"},"19.1",{"b":"19.5"}]},{"entry":[{},"15-627",{"b":"0.0653"},"0.0654",{"b":"0.9347"},"0.9346",{"b":"0.8032"},"0.8013",{"b":"19.4"},"19.2",{"b":"21.5"},{"b":"21.5"}]},{"entry":[{},"20-836","0.0709",{"b":"0.0345"},"0.9291",{"b":"0.9655"},"0.8251",{"b":"0.8505"},"19.9",{"b":"20.1"},"22.3",{"b":"23.0"}]},{"entry":[{},"25-1046","0.0306",{"b":"0.0201"},"0.9694",{"b":"0.9799"},{"b":"0.8613"},"0.8556",{"b":"20.4"},"20.1",{"b":"23.6"},"23.3"]},{"entry":["FXa","\u20025-209","0.0751",{"b":"0.0597"},"0.9249",{"b":"0.9403"},"0.7428",{"b":"0.7489"},"19.0",{"b":"19.2"},"20.8",{"b":"21.0"}]},{"entry":[{},"10-418","0.0589",{"b":"0.0398"},"0.9411",{"b":"0.9602"},{"b":"0.8189"},"0.8146","20.1",{"b":"20.3"},"22.0",{"b":"22.2"}]},{"entry":[{},"15-627","0.0280",{"b":"0.0222"},"0.9720",{"b":"0.9778"},{"b":"0.8549"},"0.8475","20.6",{"b":"20.7"},{"b":"23.1"},{"b":"23.1"}]},{"entry":[{},"20-836",{"b":"0.0164"},"0.0176",{"b":"0.9836"},"0.9824",{"b":"0.8706"},"0.8602",{"b":"20.9"},"20.7",{"b":"23.8"},"23.5"]},{"entry":[{},"25-1046","0.0298",{"b":"0.0150"},"0.9702",{"b":"0.9850"},{"b":"0.8693"},{"b":"0.8693"},{"b":"21.1"},"21.0","23.7",{"b":"23.8"}]},{"entry":["PDE5","\u20025-209","0.2362",{"b":"0.2103"},"0.7638",{"b":"0.7897"},{"b":"0.4288"},"0.4120",{"b":"10.7"},"10.5","13.2",{"b":"13.7"}]},{"entry":[{},"10-418","0.1093",{"b":"0.0903"},"0.8907",{"b":"0.9097"},"0.7034",{"b":"0.7174"},{"b":"16.7"},{"b":"16.7"},{"b":"20.3"},"20.0"]},{"entry":[{},"15-627","0.0462",{"b":"0.0390"},"0.9538",{"b":"0.9610"},"0.8188",{"b":"0.8320"},"19.4",{"b":"19.6"},"22.2",{"b":"22.3"}]},{"entry":[{},"20-836","0.0323",{"b":"0.0243"},"0.9677",{"b":"0.9757"},"0.8599",{"b":"0.8688"},{"b":"20.2"},{"b":"20.2"},{"b":"23.4"},"23.2"]},{"entry":[{},"25-1046",{"b":"0.0192"},{"b":"0.0192"},{"b":"0.9808"},{"b":"0.9808"},{"b":"0.8988"},"0.8976",{"b":"21.4"},"21.1","23.5",{"b":"23.7"}]},{"entry":["\u03b1AR","\u20025-209",{"b":"0.1047"},"0.1052",{"b":"0.8953"},"0.8948",{"b":"0.6420"},"0.6367","14.4",{"b":"14.5"},"18.4",{"b":"18.7"}]},{"entry":[{},"10-418",{"b":"0.0564"},"0.0565",{"b":"0.9436"},"0.9435",{"b":"0.7106"},"0.7081","15.8",{"b":"15.9"},"21.2",{"b":"21.3"}]},{"entry":[{},"15-627",{"b":"0.0331"},"0.0340",{"b":"0.9669"},"0.9660",{"b":"0.7702"},"0.7658",{"b":"17.2"},"17.1",{"b":"22.4"},{"b":"22.4"}]},{"entry":[{},"20-836","0.0286",{"b":"0.0168"},"0.9714",{"b":"0.9832"},"0.7902",{"b":"0.8269"},"18.1",{"b":"18.7"},"22.3",{"b":"23.4"}]},{"entry":[{},"25-1046","0.0195",{"b":"0.0113"},"0.9805",{"b":"0.9887"},"0.8061",{"b":"0.8525"},"18.5",{"b":"19.5"},"23.3",{"b":"23.7"}]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}}]}}},"QSAR Ranking Experiments in the Chemical Structure Domain:","In QSAR ranking experiments, using exemplary algorithms taught herein two QSAR data sets were used which included inhibitors of dihydrofolate reductase (DHFR) and cyclooxygenase-2 (COX2), respectively, together with corresponding biological activities represented as pICvalues. The DHFR inhibitor data set contained representations of 361 chemical structures, with pICvalues ranging from 3:3 to 9:8. The COX2 inhibitor data set contained representations of 282 chemical structures, with pICvalues ranging from 4:0 to 9:0. Each chemical structure in the above data sets was represented using 2.5D chemical descriptors such as disclosed in Sutherland et al. (2004). The 2.5D chemical descriptors advantageously included 2D descriptors that were calculated simply from the connection graph of a molecule (such as X indices, counts of rotatable bonds, molecular weight and E-state indices), as well as whole-molecule 3D descriptors (such as molecular volume and charged partial surface area descriptors). Details regarding the descriptors can be found in aforementioned Sutherland et al. (2004). The DHFR inhibitor data set contained a total of seventy descriptors; the COX2 inhibitor data set contained seventy-four descriptors. All descriptors were real-valued descriptors; scaled between 0 and 1.","Two different types of train\/test splits were considered for the above data sets. The first split involved, for each data set, selecting approximately one-third of the representations of chemical structures using a maximum dissimilarity algorithm for the test set, and using the remaining two-thirds of the representations of chemical structures as the training set. Thus, this split resulted in a training set of 237 representations of chemical structures and test set of 124 representations of chemical structures for the DHFR inhibitor data set, and a training set of 188 representations of chemical structures and test set of ninety-four representations of chemical structures for the COX2 inhibitor data set. Again, the above split represents a worst-case possibility, with the test set being maximally diverse and requiring considerable extrapolation from the training set. A random split was also considered, in which each data set was partitioned randomly into training and test sets of the same sizes as above. In addition, to evaluate the impact of the training set size, experiments were conducted in which each of 10%, 20%, 30%, . . . ,100% of the representations of training chemical structures were used. The above experiments were repeated with ten random partitions of the representations of chemical structures into training and test sets.","As discussed above the performance of an exemplary real-value kernel-based ranking algorithm, as taught herein, was compared with that of a conventional SVR-based ranking method on the above data sets. Thus, for each train\/test split of the QSAR data sets described above, the performance of the ranking function learned by each algorithm on the training set was evaluated on the corresponding test set. The following performance measures were considered with respect to the real value ranking problems presented:","1. Ranking error: (approximately) optimized by the exemplary kernel-based ranking algorithm, as taught herein, in the real-valued labels setting described above. Specifically, given a ranking function \u0192:X\u2192 and a test sample T=((x,y), . . . , (xY))\u03b5(X\u00d7), the ranking error of \u0192 with respect to T measures the average relevance-weighted ranking loss of \u0192 on preference pairs in T:",{"@attributes":{"id":"p-0143","num":"0142"},"maths":{"@attributes":{"id":"MATH-US-00031","num":"00031"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mi":"err","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"mi":"P"}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2208","mi":"P"}},"mo":"\u2062","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["y","j"]}],"mo":"-"}},{"mo":["[","]"],"mrow":{"msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","j"]}}}],"mo":"<"}}},"mo":"+","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","j"]}}}],"mo":"="}}}}}}],"mo":"\u2061"}}}],"mo":"="},"mo":","}}}},"where P={(i,j)|y>y} denotes the set of preference pairs in T.","2. Correlation: Two performance measures that have frequently been used in evaluating QSAR models are the root mean squared error (RMSE) and correlation. The RMSE measures the predictive (in)accuracy of a model; the correlation, on the other hand, can be viewed as a measure of ranking performance. Specifically, the (Pearson) correlation between the vector f=(\u0192(x), . . . , \u0192(x)) of predicted activities (or in the discussed situation scores assigned by the learned function) and the vector y=(y, . . . , y) of actual activities is given by:",{"@attributes":{"id":"p-0146","num":"0145"},"maths":{"@attributes":{"id":"MATH-US-00032","num":"00032"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":{"mi":"corr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},"mo":"=","mfrac":{"mrow":[{"mfrac":{"mn":"1","mrow":{"mi":"m","mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mrow":{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},"mo":"-","msub":{"mi":["\u03bc","f"]}}},{"mo":["(",")"],"mrow":{"msub":[{"mi":["y","i"]},{"mi":["\u03bc","y"]}],"mo":"-"}}],"mo":"\u2062"}}},{"msub":[{"mi":["\u03c3","f"]},{"mi":["\u03c3","y"]}],"mo":"\u2062"}]}},"mo":","}}}},"where \u03bc,\u03c3are the mean and standard deviation of \u0192, respectively, and \u03bc,\u03c3are similarly the mean and standard deviation of y, respectively. This measures the strength of a linear relationship between the two vectors. In particular, if the relative differences in the scores\/activities of chemical structures under the two vectors are roughly proportional to each other (resulting in a similar ranking), the correlation will be high. Note however that if the rankings are similar but the scores have a non-linear relationship, then the correlation may not be an accurate measure of ranking performance. The correlation lies between \u22121 and 1, with 1 representing a perfect positive linear relationship and \u22121 representing a perfect negative linear relationship.","3. Kendall's \u03c4 rank correlation coefficient: a rank correlation coefficient used to measure the agreement between two rankings; it effectively measures the fraction of pairs of data elements on which two rankings agree. In the present case, Kendall's \u03c4 rank correlation was used to measure the expected agreement between the learned ranking of the representations of chemical structures in T and the true ranking based on their biological activities, assuming that ties in the learned ranking are broken uniformly at random:",{"@attributes":{"id":"p-0149","num":"0148"},"maths":{"@attributes":{"id":"MATH-US-00033","num":"00033"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"\u03c4","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},{"mrow":{"mn":"2","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"mi":"P"}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2208","mi":"P"}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","j"]}}}],"mo":">"}}},"mo":"+","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msub":{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","j"]}}}],"mo":"="}}}}}}}}}},"mo":"-","mn":"1."}],"mo":"="}}}},"The Kendall \u03c4 coefficient takes a value between \u22121 and 1, with 1 representing perfect agreement between the rankings and \u22121 representing perfect disagreement. For more information see Kendall, M., A new measure of rank correlation, Biometrika 1938, 30, 81-89.","4. Spearman's \u03c1 rank correlation coefficient: another rank correlation coefficient used to measure the agreement between two rankings; it measures the standard Pearson correlation between the vectors of ranks of data elements (i.e., their positions in sorted order) resulting from two rankings. Spearman's \u03c1 rank correlation coefficient was used to measure the correlation between the learned ranking of the representations of chemical structures in T and the true ranking based on biological activities. If \u03b2(i) denotes the rank of xin the ranking returned by \u0192 (such that chemical structures that are assigned the same score by \u0192 receive the average rank among them), and \u03b2(i) denotes similarly the rank of xin the ranking based on the actual activities, then the Spearman \u03c1 coefficient is given by the Pearson correlation between the vectors and \u03b2=(\u03b2(1), . . . , \u03b2(m)) and \u03b2=(\u03b2(1), . . . , \u03b2(m)):",{"@attributes":{"id":"p-0152","num":"0151"},"maths":{"@attributes":{"id":"MATH-US-00034","num":"00034"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":{"mi":"\u03c1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},"mo":"=","mfrac":{"mrow":[{"mfrac":{"mn":"1","mrow":{"mi":"m","mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["\u03b2","f"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}},"mo":"-","msubsup":{"mi":["\u03bc","f","\u2032"]}}},{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["\u03b2","y"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}},"mo":"-","msubsup":{"mi":["\u03bc","y","\u2032"]}}}],"mo":"\u2062"}}},{"msubsup":[{"mi":["\u03c3","f","\u2032"]},{"mi":["\u03c3","y","\u2032"]}],"mo":"\u2062"}]}},"mo":","}}}},"where \u03bc\u2032,\u03c3\u2032 are mean and standard deviation of \u03b2, respectively, and \u03bc\u2032,\u03c3\u2032are similarly the mean and standard deviation of \u03b2, respectively. The Spearman \u03c1 coefficient also takes a value between \u22121 and 1, with 1 representing perfect agreement and \u22121 representing perfect disagreement. For more information see Spearman, C., The proof and measurement of association between two things, American Journal of Psychology 1904, 15, 72-101.","5. Normalized discounted cumulative gain (NDCG): a measure of ranking performance in information retrieval, where the relevance of the top few items returned by a ranking is especially important, and is used instead of the average precision when there are more than two possible relevance levels. As with average precision, NDCG places greater emphasis on ranking accuracy at the top of a ranked list: starting from the top, it accumulates a gain value for each data element in the list that is based on the relevance of the data element, but applies a (logarithmic) discounting factor that reduces the gain for data elements lower down in the list. For the present case, NDCG was used to evaluate the ranked list of chemical structures in T returned by \u0192. The gain value of each representation of chemical structure in the list was based on the biological activity of the chemical structure. In particular, if \u03c0(i) denotes the index of the chemical structure in T that appears in the ith position in the ranking returned by \u0192, then NDCG is given by",{"@attributes":{"id":"p-0155","num":"0154"},"maths":{"@attributes":{"id":"MATH-US-00035","num":"00035"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mi":"NDCG","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","T"],"mo":";"}}},{"mfrac":{"mn":"1","mi":"Z"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mfrac":{"mn":"1","mrow":{"msub":{"mi":"log","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"i","mo":"+","mn":"1"}}}},"mo":"\u00b7","mrow":{"mo":["(",")"],"mrow":{"msup":{"mn":"2","mrow":{"msub":[{"mi":["y","\u03c0"]},{"mn":"1","mrow":{"mo":["(",")"],"mi":"i"}}],"mo":"-"}},"mo":"-","mn":"1"}}}}}],"mo":"="},"mo":","}}}},"where 1\/log(i+1) is the discounting factor for the ith-ranking instance, and Z is a normalization constant chosen so that NDCG is at most 1. Higher values of NDCG correspond to better ranking performance.","As discussed previously, the performance of the kernel-based ranking algorithm for ranking with real valued labels was compared with that of SVR-based ranking on two QSAR data sets containing inhibitors of DHFR and COX2, respectively. For both algorithms, experiments were conducted using two types of kernels: the simple linear kernel described above, and the Gaussian or radial basis function (RBF) kernel that may be used with real-valued descriptor vectors:\n\n(\u2032)=,\n","where \u03b3>0 is a parameter for the RBF kernel.","For the SVR algorithm, Joachim's SVMlight software was used. The regularization parameter C and the sensitivity parameter \u03b5 in each training run were selected by 5-fold cross-validation from the ranges {0.1, 1, 10, 100, 1000, 10000} and {0.01, 0.05, 0.1, 0.5, 1} respectively. In addition, when using the RBF kernel, the parameter was similarly selected from the range",{"@attributes":{"id":"p-0160","num":"0159"},"maths":{"@attributes":{"id":"MATH-US-00036","num":"00036"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mo":["{","}"],"mrow":{"mfrac":[{"mn":["1","16"]},{"mn":["1","4"]}],"mo":[",",",",",",","],"mn":["1","4","16"]}},"mo":"."}}},"br":{}},"The kernel-based ranking algorithm described with respect to real-valued rankings was not supported in the SVMlight software and was consequently implemented in C++, i.e., rather than use a standard QP solver which can be slow even for moderate-sized ranking problems. Thus, a gradient projection algorithm was implemented to solve the QP involved in training an exemplary real-value kernel-based ranking algorithm (see Equation 10). Since the constraints on the variables \u03b1 in the QP were simple box constraints, the projection step in this case is simple and efficient. The gradient projection algorithm has two additional parameters, namely the number of iterations t and the learning rate \u03b7. The regularization parameter C, optimization parameters t and \u03b7 (and the kernel parameter \u03b3 when using the RBF kernel), were all selected in each training run by 5-fold cross-validation as above. The parameters C and \u03b3 were chosen from the same ranges as above; the parameters t and \u03b7 were selected from the ranges {100, 250, 500, 750, 1000} and {10, 10, 10, 10, 10}, respectively.","Two different splits of the data sets into training and test sets were considered for the real-valued ranking experiments: an original split where roughly one-third of each data set was chosen to form the test set using a maximum dissimilarity algorithm, and a second split in which each data set was randomly partitioned into training and test sets of the same sizes as in the original split. The results on the original split are shown in Table 7 for the linear kernel, and in Table 8 for the RBF kernel. The performance of the RanksSVM algorithm on these splits is similar overall. Furthermore, both the kernel-based ranking algorithm and the conventional SVM algorithm demonstrated improvement in performance when using the RBF kernel as compared to the linear kernel.",{"@attributes":{"id":"p-0163","num":"0162"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"378pt","align":"center"}},"thead":{"row":{"entry":"TABLE 7"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"QSAR ranking results on original splits with the linear kernel."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"63pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},{},"Ranking error","Correlation","Kendall's \u03c4","Spearman's \u03c1","NDCG"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Data","Train\/test",{},"Rank",{},"Rank",{},"Rank",{},"Rank",{},"Rank"]},{"entry":["set","split","SVR","SVM","SVR","SVM","SVR","SVM","SVR","SVM","SVR","SVM"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"9","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"11","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["DHFR","Original",{"b":"0.1914"},"0.1959",{"b":"0.7302"},"0.7190",{"b":"0.5516"},"0.5450",{"b":"0.7495"},"0.7464",{"b":"0.8599"},"0.8377"]},{"entry":["COX2","Original","0.3235",{"b":"0.3217"},"0.5558",{"b":"0.5635"},{"b":"0.4195"},"0.4149",{"b":"0.6060"},{"b":"0.6060"},"0.8906",{"b":"0.8921"}]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0164","num":"0163"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"378pt","align":"center"}},"thead":{"row":{"entry":"TABLE 8"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"QSAR ranking results on original splits with the RBF kernel."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"63pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},{},"Ranking error","Correlation","Kendall's \u03c4","Spearman's \u03c1","NDCG"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Data","Train\/test",{},"Rank",{},"Rank",{},"Rank",{},"Rank",{},"Rank"]},{"entry":["set","split","SVR","SVM","SVR","SVM","SVR","SVM","SVR","SVM","SVR","SVM"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"9","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"11","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["DHFR","Original","0.1837",{"b":"0.1726"},"0.7519",{"b":"0.7618"},"0.5571",{"b":"0.5747"},"0.7552",{"b":"0.7758"},"0.8540",{"b":"0.8632"}]},{"entry":["COX2","Original",{"b":"0.3138"},"0.3173",{"b":"0.5836"},"0.5703",{"b":"0.4351"},"0.4346","0.6100",{"b":"0.6174"},{"b":"0.9399"},"0.9231"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}}]}}},"Similar to the first split in the bipartite ranking experiments, the original splits for the real-value ranking experiments represented a worst-case scenario, i.e., since the test chemical structures were selected to be maximally diverse and therefore were most likely to require considerable extrapolation from the training set. By comparison, random splits were designed to simulate a situation in which the training set would be more representative of the test set. The results on these random splits, using the linear and RBF kernels, are shown in Tables 9 and 10, respectively. Each number shown in these tables is the average over ten different random splits. In each case, 237 out of 361 representations of chemical structures in the DHFR data set and 188 of 292 representations of chemical structures in the COX2 data set were randomly selected to form the training set; the remaining representations of chemical structures were assigned to the test set (same sizes as in the original splits as above). In addition, in order to evaluate the impact of the training set size, experiments were conducted using increasing fractions of the training set.",{"@attributes":{"id":"p-0166","num":"0165"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"364pt","align":"center"}},"thead":{"row":{"entry":"TABLE 9"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"QSAR ranking results on random splits with the linear kernel."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"63pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},{},"Ranking error","Correlation","Kendall's \u03c4","Spearman's \u03c1","NDCG"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Data","No.",{},"Rank",{},"Rank",{},"Rank",{},"Rank",{},"Rank"]},{"entry":["set","train","SVR","SVM","SVR","SVM","SVR","SVM","SVR","SVM","SVR","SVM"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"11","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["DHFR","24","0.4967",{"b":"0.4742"},{"b":"0.0889"},"0.0519",{"b":"0.0524"},"0.0329",{"b":"0.0763"},"0.0480",{"b":"0.6899"},"0.6855"]},{"entry":[{},"48",{"b":"0.4014"},"0.4043","0.1527",{"b":"0.1626"},"0.1047",{"b":"0.1151"},"0.1562",{"b":"0.1692"},"0.6918",{"b":"0.6938"}]},{"entry":[{},"72",{"b":"0.2900"},"0.4974",{"b":"0.2326"},"0.2044",{"b":"0.1584"},"0.1369",{"b":"0.2312"},"0.2037",{"b":"0.7092"},"0.6974"]},{"entry":[{},"96",{"b":"0.2531"},"0.4650",{"b":"0.2893"},"0.2204",{"b":"0.2005"},"0.1418",{"b":"0.2935"},"0.2086",{"b":"0.7154"},"0.7127"]},{"entry":[{},"120",{"b":"0.2074"},"0.2207",{"b":"0.2995"},"0.2718",{"b":"0.2047"},"0.1834",{"b":"0.3003"},"0.2702",{"b":"0.7186"},"0.7032"]},{"entry":[{},"144","0.2005",{"b":"0.1951"},"0.2849",{"b":"0.2925"},"0.1892",{"b":"0.1955"},"0.2808",{"b":"0.2892"},"0.7048",{"b":"0.7062"}]},{"entry":[{},"168","0.1973",{"b":"0.1970"},"0.2914",{"b":"0.2968"},"0.1942",{"b":"0.1974"},"0.2864",{"b":"0.2916"},"0.7056",{"b":"0.7080"}]},{"entry":[{},"192",{"b":"0.1974"},"0.2352",{"b":"0.2927"},"0.2823",{"b":"0.2004"},"0.1906",{"b":"0.2955"},"0.2831",{"b":"0.7085"},"0.6997"]},{"entry":[{},"216","0.1921",{"b":"0.1869"},{"b":"0.2904"},"0.2834","0.1911",{"b":"0.1915"},"0.2828",{"b":"0.2830"},{"b":"0.7024"},"0.7020"]},{"entry":[{},"237",{"b":"0.1961"},"0.3407",{"b":"0.2866"},"0.2806",{"b":"0.1882"},"0.1850",{"b":"0.2767"},"0.2728","0.7016",{"b":"0.7078"}]},{"entry":["COX2","19",{"b":"0.3979"},"0.3983","0.0803",{"b":"0.1095"},"0.0715",{"b":"0.0978"},"0.1032",{"b":"0.1426"},{"b":"0.7746"},"0.7719"]},{"entry":[{},"38","0.3717",{"b":"0.3655"},{"b":"0.1561"},"0.1559","0.1315",{"b":"0.1368"},"0.1918",{"b":"0.1979"},{"b":"0.7896"},"0.7893"]},{"entry":[{},"57",{"b":"0.3365"},"0.3580","0.1683",{"b":"0.1690"},"0.1278",{"b":"0.1367"},"0.1925",{"b":"0.1985"},"0.7883",{"b":"0.7901"}]},{"entry":[{},"76","0.3023",{"b":"0.2999"},"0.1674",{"b":"0.1728"},"0.1379",{"b":"0.1445"},"0.2008",{"b":"0.2098"},{"b":"0.7951"},"0.7925"]},{"entry":[{},"95",{"b":"0.2639"},"0.4334",{"b":"0.1789"},"0.0822",{"b":"0.1522"},"0.0784",{"b":"0.2188"},"0.1131",{"b":"0.7858"},"0.7801"]},{"entry":[{},"114",{"b":"0.2581"},"0.2645",{"b":"0.1884"},"0.1872","0.1515",{"b":"0.1525"},{"b":"0.2204"},"0.2202","0.7938",{"b":"0.8006"}]},{"entry":[{},"133",{"b":"0.2669"},"0.2734",{"b":"0.1879"},"0.1794",{"b":"0.1626"},"0.1459",{"b":"0.2316"},"0.2100","0.7903",{"b":"0.8037"}]},{"entry":[{},"152",{"b":"0.2535"},"0.2775",{"b":"0.2045"},"0.1945",{"b":"0.1693"},"0.1534",{"b":"0.2434"},"0.2202","0.7984",{"b":"0.8161"}]},{"entry":[{},"171",{"b":"0.2439"},"0.3012",{"b":"0.2105"},"0.1712",{"b":"0.1727"},"0.1393",{"b":"0.2465"},"0.1986","0.8035",{"b":"0.8152"}]},{"entry":[{},"188",{"b":"0.2429"},"0.2441","0.1902",{"b":"0.1934"},"0.1575",{"b":"0.1591"},"0.2243",{"b":"0.2272"},{"b":"0.8071"},"0.8065"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0167","num":"0166"},"tables":{"@attributes":{"id":"TABLE-US-00010","num":"00010"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"364pt","align":"center"}},"thead":{"row":{"entry":"TABLE 10"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"QSAR ranking results on random splits with the RBF kernel."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"63pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},{},"Ranking error","Correlation","Kendall's \u03c4","Spearman's \u03c1","NDCG"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Data","No.",{},"Rank",{},"Rank",{},"Rank",{},"Rank",{},"Rank"]},{"entry":["set","train","SVR","SVM","SVR","SVM","SVR","SVM","SVR","SVM","SVR","SVM"]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"12"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"8","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"9","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"10","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"11","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"12","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["DHFR","24","0.4755",{"b":"0.4655"},"0.0982",{"b":"0.1005"},{"b":"0.0727"},"0.0687",{"b":"0.1082"},"0.1012",{"b":"0.7055"},"0.6974"]},{"entry":[{},"48",{"b":"0.3430"},"0.3512",{"b":"0.1857"},"0.1782",{"b":"0.1225"},"0.1164",{"b":"0.1813"},"0.1739",{"b":"0.7084"},"0.7057"]},{"entry":[{},"72","0.2840",{"b":"0.2701"},{"b":"0.2355"},"0.2252",{"b":"0.1552"},"0.1526",{"b":"0.2274"},"0.2231",{"b":"0.7123"},"0.7073"]},{"entry":[{},"96","0.2483",{"b":"0.2363"},"0.2489",{"b":"0.2528"},{"b":"0.1715"},"0.1666",{"b":"0.2506"},"0.2442",{"b":"0.7116"},"0.7057"]},{"entry":[{},"120","0.2171",{"b":"0.2123"},"0.2519",{"b":"0.2622"},"0.1692",{"b":"0.1746"},"0.2466",{"b":"0.2550"},"0.7107",{"b":"0.7115"}]},{"entry":[{},"144","0.2023",{"b":"0.1968"},"0.2409",{"b":"0.2484"},"0.1667",{"b":"0.1668"},{"b":"0.2439"},"0.2425",{"b":"0.7085"},"0.7076"]},{"entry":[{},"168","0.2019",{"b":"0.1821"},"0.2480",{"b":"0.2776"},"0.1657",{"b":"0.1868"},"0.2407",{"b":"0.2721"},"0.7109",{"b":"0.7157"}]},{"entry":[{},"192","0.1808",{"b":"0.1742"},"0.2706",{"b":"0.2802"},"0.1830",{"b":"0.1852"},"0.2683",{"b":"0.2708"},"0.7146",{"b":"0.7212"}]},{"entry":[{},"216","0.1816",{"b":"0.1727"},"0.2596",{"b":"0.2679"},"0.1772",{"b":"0.1799"},"0.2543",{"b":"0.2651"},{"b":"0.7223"},"0.7107"]},{"entry":[{},"237","0.1714",{"b":"0.1702"},"0.2606",{"b":"0.2711"},"0.1754",{"b":"0.1810"},"0.2575",{"b":"0.2660"},{"b":"0.7089"},"0.7075"]},{"entry":["COX2","19","0.4362",{"b":"0.4294"},{"b":"0.1194"},"0.1093","0.0937",{"b":"0.0987"},"0.1423",{"b":"0.1494"},{"b":"0.7845"},"0.7782"]},{"entry":[{},"38","0.3777",{"b":"0.3717"},"0.1605",{"b":"0.1609"},"0.1257",{"b":"0.1379"},"0.1813",{"b":"0.1985"},"0.7882",{"b":"0.7963"}]},{"entry":[{},"57","0.3325",{"b":"0.3249"},{"b":"0.1745"},"0.1632",{"b":"0.1478"},"0.1386",{"b":"0.2146"},"0.1950",{"b":"0.7924"},"0.7902"]},{"entry":[{},"76","0.3046",{"b":"0.2953"},"0.1722",{"b":"0.1726"},"0.1444",{"b":"0.1448"},"0.2080",{"b":"0.2085"},"0.7880",{"b":"0.7963"}]},{"entry":[{},"95","0.2667",{"b":"0.2635"},{"b":"0.1960"},"0.1844",{"b":"0.1615"},"0.1513",{"b":"0.2313"},"0.2171","0.7853",{"b":"0.7911"}]},{"entry":[{},"114","0.2633",{"b":"0.2620"},"0.1840",{"b":"0.1858"},"0.1455",{"b":"0.1508"},"0.2096",{"b":"0.2145"},{"b":"0.7906"},"0.7895"]},{"entry":[{},"133","0.2760",{"b":"0.2705"},"0.1957",{"b":"0.1958"},{"b":"0.1560"},"0.1559",{"b":"0.2247"},"0.2241","0.7885",{"b":"0.7950"}]},{"entry":[{},"152",{"b":"0.2520"},"0.2579",{"b":"0.2011"},"0.1951",{"b":"0.1626"},"0.1560",{"b":"0.2340"},"0.2233","0.7925",{"b":"0.8028"}]},{"entry":[{},"171",{"b":"0.2392"},"0.2403","0.1988",{"b":"0.2140"},"0.1650",{"b":"0.1734"},"0.2356",{"b":"0.2460"},"0.8001",{"b":"0.8060"}]},{"entry":[{},"188",{"b":"0.2153"},"0.2255",{"b":"0.1912"},"0.1851",{"b":"0.1553"},"0.1546",{"b":"0.2198"},"0.2184","0.7958",{"b":"0.8005"}]},{"entry":{"@attributes":{"namest":"1","nameend":"12","align":"center","rowsep":"1"}}}]}}]}}},"The first observation is that as with the original splits above, the use of the RBF kernel leads to an overall improvement in performance over the linear kernel. Notably the improvement in overall performance is more pronounces in the in the case of the kernel-based ranking algorithm when compared to the conventional SVR algorithm. The exemplary kernel-based ranking algorithm, as taught herein, with the linear kernel shows some unstable behavior, where an increase in training set size sometimes leads to deterioration. In contrast, the kernel-based ranking algorithm with RBF kernel, is not only stable, but also exhibits better performance.","A second observation is that in most cases, an increase in training set size leads to improvement in performance on the test set, particularly when measured in terms of the ranking error (except in the anomalous linear kernel instances noted above).","A third observation is that when the complete training set (237 representations of chemical structures for DHFR and 188 representations of chemical structures for COX2) is used, the performance on the COX2 data set is considerably better than in the original split (using any algorithm\/kernel) while for the DHFR data set, the performance is roughly the same (except in unstable linear kernel case). Thus, the ranking for the COX2 data set appears to suffer from the use of maximum dissimilarity algorithms in constructing the test set while the ranking for the DHFR data set does not.","Finally, we observe that on these random train\/test splits, when the RBF kernel is used, the ranking performance of the kernel-based ranking algorithm in terms of the ranking error (and to some degree, the correlation) is, in general, superior to conventional SVR-based ranking. This improvement may be attributed to the fact that the kernel-based ranking algorithm optimizes the relevance-weighted ranking error which takes into account the actual activity values of the chemical structures.","Discussion Regarding Results of Bipartite and OSAR Ranking Experiments:","In view of the forgoing results for both the virtual screening experiments and the QSAR ranking experiments, it is noted the use of a kernel function more suited to the chemical structure data type (e.g., the Tanimoto kernel for binary fingerprint vectors in virtual screening and the RBF kernel for descriptor vectors in QSAR ranking) showed clear performance gains with ranking techniques when compared to conventional SVM and SVR techniques. Furthermore, since the kernel function in RKHS can be viewed as a similarity function, chemical similarity measures between chemical structures may be applied. See, e.g., Willett, P., Chemical similarity searching, Journal of Chemical Information and Computer Sciences 1998, 38, 983-996 and Nikolova, N.; Jaworska, J., Approaches to measure chemical similarity\u2014a review, QSAR & Combinatorial Science 2003, 22, 1006-1026. While not all such chemical similarity measures satisfy the requirements for a kernel function (namely, the kernel function must be symmetric and positive semi-definite), in cases where the library of representations of chemical structures to be ranked is known in advance, one can construct a chemical similarity graph over the representations of chemical structures in the training set and test library, and then use graph-based ranking methods that effectively construct a kernel function from such a similarity graph. See, e.g., Agarwal, S. (2006). As taught herein, using a kernel function corresponds to constructing an implicit representation of each chemical structure in a new (often high-dimensional) space. Thus, constructing a chemically-inspired kernel function in this manner can be thought of as obtaining an implicit chemical representation of the chemical structures.","The ranking approach as taught herein outperformed classification and regression approaches in terms of the bipartite ranking error or AUC in virtual screening and in terms of the ranking error in QSAR ranking. This can be attributed to the fact that the kernel-based ranking algorithms used in the experiments optimized precisely these criteria. The algorithms, however, may be modified to optimize any desired performance measures, such as average precision and number of actives in the top portion of a list in the bipartite setting, and the rank correlation coefficients and NDCG in the real-valued labels setting.","Evaluation of the Infinite Push Algorithm:","Empirical studies were also conducted evaluating the performance of a ranking algorithm (the Infinite Push algorithm) derived from an error function:",{"@attributes":{"id":"p-0177","num":"0176"},"maths":{"@attributes":{"id":"MATH-US-00037","num":"00037"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"msub":{"mi":["R","\u221e"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","S"],"mo":";"}}},{"munder":{"mi":"max","mrow":{"mn":"1","mo":["\u2264","\u2264"],"mi":["j","n"]}},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mn":"1","mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mn":"1","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"<"}}}}}}}],"mo":"="},"mo":","}}}},"Notably the error function for the Infinite Push algorithm is similar and minimized using the same techniques as the error function in equation (1a), above. The error function for the Infinite Push algorithm, similar to the error function in equation (1a), accounts for the fraction of positives ranked below the highest ranked negative. Therefore, minimizing the error function corresponds to maximizing the number of positives that appear before the first negative in the ranked list.","In experiments conducted, the Infinite Push algorithm was analyzed together with a number of other ranking algorithms including RankSVM (see Joachims, T.: Optimizing search engines using clickthrough data. In: Proceedings of the 8th ACM Conference on Knowledge Discovery and Data Mining (2002) and Herbrich, R., Graepel, T., Obermayer, K.: Large margin rank boundaries for ordinal regression, Advances in Large Margin Classifiers (2000) 115-132); SVMMAP (see Yue, Y., Finley, T., Radlinski, F., Joachims, T.: A support vector method for optimizing average precision, In: Proceedings of the 30th ACM SIGIR Conference on Research and Development in Information Retrieval. (2007)), RankBoost (see Freund, Y., Iyer, R., Schapire, R. E., Singer, Y.: An efficient boosting algorithm for combining preferences, Journal of Machine Learning Research 4 (2003) 933-969), and the P-Norm Push algorithm (see C. Rudin. The P-norm push: A simple convex ranking algorithm that concentrates at the top of the list. Journal of Machine Learning Research, 10:2233{2271, 2009).","Performance of these algorithms was evaluated for three different data sets: the Ionosphere dataset (see A. Frank and A. Asuncion. UCI Machine Learning Repository, 2010 http:\/\/archive.ics.uci.edu\/ml.); the Spambase dataset (Id.); and a cheminformatics data set that has been used to test virtual screening algorithms for drug discovery (see R. N. Jorissen and M. K. Gilson. Virtual screening of molecular databases using a support vector machine. Journal of Chemical Information and Modeling, 45:549{561, 2005).","In each case, four different performance measures were used in the comparison:","1. the area under the ROC curve (AUC);","2. the number of positives retrieved at the absolute top;","3. the average precision (AP); and","4. the discounted cumulative gain (DCG).","Note that the average precision and DCG for a ranking function \u0192:X\u2192 and sample S=(S,S)\u03b5X\u00d7Xare defined",{"@attributes":{"id":"p-0187","num":"0186"},"maths":[{"@attributes":{"id":"MATH-US-00038","num":"00038"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"as","mo":"\u2062","mstyle":{"mtext":":"}}}},{"@attributes":{"id":"MATH-US-00038-2","num":"00038.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mrow":[{"mi":"AP","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","S"],"mo":";"}}},{"mfrac":{"mn":"1","mi":"m"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mfrac":{"mn":"1","mrow":{"msub":{"mi":["r","f"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","msub":{"mn":"1","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","k"],"mo":"+"}}}],"mo":"<"}}}}}}}],"mo":"="},{"mrow":[{"mi":"DCG","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["f","S"],"mo":";"}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mfrac":{"mn":"1","mrow":{"msub":{"mi":"log","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["r","f"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}},"mo":"+","mn":"1"}}}}}],"mo":"="}],"mo":[",","\u2062",",","\u2062"],"mstyle":[{"mtext":{}},{"mtext":{}}],"mi":"wherein"}}},{"@attributes":{"id":"MATH-US-00038-3","num":"00038.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["r","f"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}},{"mn":"1","mo":["+","+"],"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","msub":{"mn":"1","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","k"],"mo":"+"}}}],"mo":"<"}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msub":{"mn":"1","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","i"],"mo":"+"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","j"],"mo":"-"}}}],"mo":"<"}}}}]}],"mo":"="}}}],"br":{},"sub":"i","sup":"+ "},"Note that average precision and DCG also emphasize ranking accuracy at the top. For all four performance measures, a larger value corresponds to better ranking performance.","The first data used was the Ionosphere data set, containing 351 data elements representing radar signals collected from a phased array of antennas; of these, 225 are \u2018good\u2019 (positive), representing signals that react back toward the antennas and indicating structure in the ionosphere, and 126 are \u2018bad\u2019 (negative), representing signals that pass through the ionosphere. Each data element in the data set was described by thirty-three features each scaled to lie in [0; 1]. To train the algorithms, the data set was divided into two-thirds for training and one-third for testing (subject to both having the same proportion of positives); this was repeated 10 times. In each run, the number of iterations tin the gradient projection algorithms for RankSVM and the Infinite Push was fixed to 1000; the parameters C and 0 were selected from the ranges {0:1; 1; 10; 100; 1000} and {10; 10; 10; 10; 10}, respectively, using 5-fold cross validation on the training set (in each case, the parameters giving the highest average precision across the 5 folds were selected). The parameter C for SVMMAP was selected similarly. The number of iterations tin the P-Norm Push was fixed to 100.","The results are shown in Table 11 (each value is an average over 10 random trials):",{"@attributes":{"id":"p-0191","num":"0190"},"tables":{"@attributes":{"id":"TABLE-US-00011","num":"00011"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 11"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Results on the Ionosphere Data Set"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},{},"Average",{}]},{"entry":["Training method","AUC","Positives at Top","Precision","DCG"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["RankSVM",{"b":"0.9271"},"12.1",{"b":"0.9330"},"16.6200"]},{"entry":["SVMMAP","0.9207",{"b":"16.5"},{"b":"0.9357"},{"b":"16.7940"}]},{"entry":["P-Norm Push","0.9052","10.7","0.9086","16.4462"]},{"entry":["(p = 1; RankBoost)",{},{},{},{}]},{"entry":["P-Norm Push (p = 4)","0.8789","6.7","0.8849","16.2743"]},{"entry":["P-Norm Push (p = 16)","0.9070","13.9","0.9218",{"b":"16.6527"}]},{"entry":["P-Norm Push (p = 64)","0.8917","9.9","0.9064","16.4970"]},{"entry":["Infinite Push",{"b":"0.9237"},{"b":"14.7"},"0.9328","16.6336"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"For each performance measure, the two algorithms giving the best performance in terms of that measure are highlighted in bold typeface. As expected, the RankSVM algorithm performs well in terms of the AUC. On this data set, the SVMMAP algorithm gives the highest number of positives at the top, but the Infinite Push algorithm follows closely behind. In particular, the Infinite Push gives more positives at the top than both RankSVM and the P-Norm Push for various values of p. ROC curves for the four algorithms on a sample run (using the best value of p, in terms of number of positives at the top, for the P-Norm Push) are shown in .","The second data set used was the Spambase data set, containing 4601 email messages, of which 1813 are spam. If spam messages are treated as positives, the goal is to learn a ranking that maximizes the number of positives at the top. Each email message is represented as a 57-dimensional feature vector representing various word frequencies and other attributes; each feature was scaled to lie in [0; 1]. In this case only a small randomly divided fraction (5%) of the data set was used for training and the rest for testing. The parameters for the various algorithms were selected same as for the Ionosphere dataset.","The results are shown in Table 12 (again, each value is an average over 10 random trials):",{"@attributes":{"id":"p-0195","num":"0194"},"tables":{"@attributes":{"id":"TABLE-US-00012","num":"00012"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 12"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Results on the Spambase Data Set"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"Positives","Average",{}]},{"entry":["Training method","AUC","at Top","Precision","DCG"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["RankSVM",{"b":"0.9418"},"22.2",{"b":"0.9010"},{"b":"189.6650"}]},{"entry":["SVMMAP","0.9319",{"b":"48.6"},"0.8957","189.6074"]},{"entry":["P-Norm Push","0.9194","12.5","0.8714","188.2330"]},{"entry":["(p = 1; RankBoost)",{},{},{},{}]},{"entry":["P-Norm Push (p = 4)","0.8024","5.0","0.6905","180.4931"]},{"entry":["P-Norm Push (p = 16)","0.8293","24.1","0.7707",{"b":"185.4531"}]},{"entry":["P-Norm Push (p = 64)","0.8490","31.4","0.8143","187.3132"]},{"entry":["Infinite Push",{"b":"0.9388"},{"b":"49.9"},{"b":"0.9028"},{"b":"189.8070"}]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"As before, for each performance measure, the two algorithms giving the best performance in terms of that measure are highlighted in bold typeface. Again, RankSVM gives the best performance in terms of AUC. In this case, however, the Infinite Push algorithm gives the highest number of positives at the top; it also gives the best performance in terms of the other measures emphasizing accuracy at the top, namely the average precision and DCG. ROC curves for the four algorithms on a sample run (using the best p for the P-Norm Push) are shown in .","The third data used in the Infinite Push Algorithm experiments was a cheminformatics data typically used to test virtual screening algorithms in drug discovery. The data set contained five sets of fifty representations of chemical compounds (each set targeting a different protein) and a background set of 1892 representations of always inactive chemical compounds. Thus, for each of the five targets, there were fifty representations of active compounds, and a total of 2092 representations of inactives (including the 1892 representations of background compounds and the 200 representations of compounds belonging to the other four target sets). Each chemical compound in the above data set was represented using a molecular fingerprint representation (specifically, a 1021-bit vector FP2 fingerprint available with the OpenBabel chemical informatics software package). To train the algorithms, for each target, the fifty representations of actives and 2092 representations of inactives were randomly divided into a small fraction (10%) for training and the rest (90%) for testing (subject to both having the same proportion of actives and inactives); this was repeated ten times. The parameters for the various algorithms were selected as before. In each case, given the training set, the goal was to rank representations of compounds in the test set such that active compounds would be retrieved at the top of the ranked list.","The results are shown in Table 13 (in this case, there were ten random trials for each of the five targets; therefore each value is first averaged over the ten random trials corresponding to each target, and then averaged across the five targets):",{"@attributes":{"id":"p-0199","num":"0198"},"tables":{"@attributes":{"id":"TABLE-US-00013","num":"00013"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 13"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Results on the Chemiformatics Data Set"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},{},"Average",{}]},{"entry":["Training method","AUC","Positives at Top","Precision","DCG"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":["RankSVM",{"b":"0.8881"},{"b":"6.00"},"0.4708","6.4877"]},{"entry":["SVMMAP","0.8852","5.80",{"b":"0.4778"},{"b":"6.5580"}]},{"entry":["P-Norm Push","0.8753","3.31","0.3714","5.9695"]},{"entry":["(p = 1; RankBoost)",{},{},{},{}]},{"entry":["P-Norm Push (p = 4)","0.8525","3.16","0.3178","5.5329"]},{"entry":["P-Norm Push (p = 16)","0.8786","3.89","0.4005","6.1465"]},{"entry":["P-Norm Push (p = 64)","0.8805","3.96","0.3997","6.1685"]},{"entry":["Infinite Push",{"b":"0.8991"},{"b":"6.34"},{"b":"0.4910"},{"b":"6.6052"}]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"As before, for each performance measure, the two algorithms giving the best performance in terms of that measure are highlighted in bold typeface. In this case, the Infinite Push algorithm gives the best performance in terms of all four performance measures. The RankSVM performs well in terms of the AUC; SVMMAP performs well in terms of the average precision. ROC curves for the four algorithms on a sample run (using the best p for the P-Norm Push) are shown in .","The above experiments suggest that in the linear case, the Infinite Push algorithm is superior in terms of accuracy at the top when compared to RankSVM and the P-Norm Push for various values of p, and is comparable to SVMMAP (which is an adaptation of structural SVMs to optimizing the average precision). One benefit of the Infinite Push algorithm over SVMMAP, however, is that it can be applied efficiently with nonlinear kernels as well; for SVMMAP, efficient algorithms are known primarily in the linear case.","Evaluation of Exemplary Real-Value Label Algorithms:","Three exemplary real-value label ranking algorithms derived according too the ranking techniques presented herein, were considered. The ranking algorithms learned a linear ranking function \u0192w: by solving an optimization problem of the following form:",{"@attributes":{"id":"p-0204","num":"0203"},"maths":{"@attributes":{"id":"MATH-US-00039","num":"00039"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munder":{"mi":["min","w"]},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"w"},"mn":"2"}},{"mfrac":{"mi":["C","m"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"w","mo":",","msup":{"mi":["S","i"]}}}}}}],"mo":"+"}}}}}},"where L(w; S) denotes a query-level loss function incurred by a ranking function fon the training sample S(S=((\u03c6,y), . . . , (\u03c6,y))) associated with the ith query (\u03c6\u2261\u03c6(q,d)). The function L(w; S) is constructed from relevance-weighted pair-wise hinge loss lwhich may be viewed as a convex upper bound on the following relevance-weighted ranking error:\n\n(,(\u03c6),(\u03c6))=||1((\u03c6\u2212\u03c6)<0)\n","Notably, land therefore linclude a relevancy weighting |y\u2212y| such that mis-ranking a pair of instances with relevance labels 1 and 5 incurs a larger penalty than mis-ranking a pair of instances with relevance labels 1 and 2.","The first algorithm considered (RankMM-1) was constructed using average pair-wise loss:",{"@attributes":{"id":"p-0208","num":"0207"},"maths":{"@attributes":{"id":"MATH-US-00040","num":"00040"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msubsup":{"mi":"L","mn":"1","mrow":{"mi":["H","rel"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"w","mo":",","msup":{"mi":["S","i"]}}}},{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["R","i"]}}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["j","k"],"mo":","}},"mo":"\u2208","msub":{"mi":["R","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"msub":{"mi":"l","mrow":{"mi":["H","rel"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"w","mo":[",",","],"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["\u03d5","j","i"]},{"mi":["y","j","i"]}],"mo":","}},{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["\u03d5","k","i"]},{"mi":["y","k","i"]}],"mo":","}}]}}},"mo":"."}}}],"mo":"="}}}},"Thus in addition to the relevance weighting in lthe loss functions differs from the earlier SVM ranking algorithms in that the loss may be normalized by query, taking into account different numbers of data element pairs for different queries.","Stochastic subgradient methods (such as presented in Shalev-Shwartz, S., Singer, Y., Srebro, N.: Pegasos: Primal estimated sub-gradient solver for SVM. In: Proceedings of the 24th International Conference on Machine Learning. (2007)) may be used to directly solve the resulting optimization problem. However, to facilitate development of similar algorithms using other loss formulations a dual version is considered herein. Using standard techniques involving the introduction of slack variables, the minimization problem corresponding to the above loss may be written as:",{"@attributes":{"id":"p-0211","num":"0210"},"maths":[{"@attributes":{"id":"MATH-US-00041","num":"00041"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munder":{"mi":"min","mrow":{"mi":["w","\u03be"],"mo":","}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"w"},"mn":"2"}},{"mfrac":{"mi":["C","m"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["R","i"]}}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["j","k"],"mo":","}},"mo":"\u2208","msub":{"mi":["R","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["\u03be","jk","i"]}}}}}],"mo":"+"}}}}},{"@attributes":{"id":"MATH-US-00041-2","num":"00041.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":["subject","to"],"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":":"}]}}},{"@attributes":{"id":"MATH-US-00041-3","num":"00041.3"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"msubsup":{"mi":["\u03be","jk","i"]},"mo":"\u2265","mn":"0"}},{"mrow":{"mrow":{"mo":"\u2200","mi":"i"},"mo":[",",","],"mi":["j","k"]}}]},{"mtd":[{"mrow":{"msubsup":{"mi":["\u03be","jk","i"]},"mo":"\u2265","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["y","j","i"]},{"mi":["y","k","i"]}],"mo":"-"}},{"mi":"w","mo":"\u00b7","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["\u03d5","j","i"]},{"mi":["\u03d5","k","i"]}],"mo":"-"}}}],"mo":"-"}}},{"mrow":{"mrow":[{"mo":"\u2200","mi":"i"},{"mi":"k","mo":"."}],"mo":[",",","],"mi":"j"}}]}]}}}]},"Introducing Lagrange multipliers and taking the Lagrangian dual then results in the following convex QP:",{"@attributes":{"id":"p-0213","num":"0212"},"maths":[{"@attributes":{"id":"MATH-US-00042","num":"00042"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munder":{"mi":["min","\u03b1"]},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["j","k"],"mo":","}},"mo":"\u2208","msub":{"mi":["R","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"msup":{"mi":["i","\u2032"]},"mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":["j","\u2032"]},{"mi":["k","\u2032"]}],"mo":","}},"mo":"\u2208","msub":{"mi":"R","msup":{"mi":["i","\u2032"]}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":[{"mi":["\u03b1","jk","i"]},{"mi":"\u03b1","mrow":{"msup":[{"mi":["j","\u2032"]},{"mi":["k","\u2032"]}],"mo":"\u2062"},"msup":{"mi":["i","\u2032"]}},{"mi":"Q","mrow":[{"mi":"jk","mo":",","mrow":{"msup":[{"mi":["j","\u2032"]},{"mi":["k","\u2032"]}],"mo":"\u2062"}},{"mi":"i","mo":",","msup":{"mi":["i","\u2032"]}}]}],"mo":["\u2062","\u2062"]}}}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["j","k"],"mo":","}},"mo":"\u2208","msub":{"mi":["R","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":["\u03b1","jk","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["y","j","i"]},{"mi":["y","k","i"]}],"mo":"-"}}}}}],"mo":["-","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"26.7em","height":"26.7ex"}}}]}}}}},{"@attributes":{"id":"MATH-US-00042-2","num":"00042.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mo":"\u2062","mrow":{"mi":["subject","to"],"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":":"}]}}}},{"@attributes":{"id":"MATH-US-00042-3","num":"00042.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mo":"\u2062","mrow":{"mrow":{"mn":"0","mo":["\u2264","\u2264"],"msubsup":{"mi":["\u03b1","jk","i"]},"mrow":{"mfrac":{"mi":"C","mrow":{"mi":"m","mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["R","i"]}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":"\u2200","mi":"i"}}},"mo":[",",",",","],"mi":["j","k"]}}}}]},"wherein Q=(\u03c6\u2212\u03c6)\u00b7(\u03c6\u2212\u03c6).","A simple stochastic gradient projection method may be used to solve the QP which starts with some initial values \u03b1for \u03b1 and on each iteration t, randomly selects a single query i and updates the corresponding |R| variables \u03b1using a gradient projection step:\n\n\u03b1\u2190(\u03b1\u2212\u03b7\u2207),\n\n\u03b1\u2190\u03b1\n\nfor i\u2032\u2260i,\n","wherein \u03b7>0 is a learning rate; \u2207\u03b5 is the partial gradient of the objective function in the above QP with respect to \u03b1, evaluated at \u03b1;",{"@attributes":{"id":"p-0217","num":"0216"},"maths":{"@attributes":{"id":"MATH-US-00043","num":"00043"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["\u03a9","i"]},"mo":"=","mrow":{"mo":["{","}"],"mrow":{"msup":{"mi":["\u03b1","i"]},"mo":"\u2208","mrow":{"msup":{"mi":"\u211d","mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["R","i"]}}},"mo":":","mrow":{"mn":"0","mo":["\u2264","\u2264"],"msubsup":{"mi":["\u03b1","jk","i"]},"mrow":{"mfrac":{"mi":"C","mrow":{"mi":"m","mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["R","i"]}}}},"mo":"\u2062","mrow":{"mo":"\u2200","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["j","k"],"mo":","}},"mo":"\u2208","msub":{"mi":["R","i"]}}}}}}}}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},"br":{},"sup":"i ","img":{"@attributes":{"id":"CUSTOM-CHARACTER-00041","he":"3.13mm","wi":"4.91mm","file":"US08862520-20141014-P00006.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},"sub":"i"},"The second algorithm considered (RankMM-2) was constructed using a maximum pair-wise loss:",{"@attributes":{"id":"p-0219","num":"0218"},"maths":{"@attributes":{"id":"MATH-US-00044","num":"00044"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msubsup":{"mi":"L","mn":"2","mrow":{"mi":["H","rel"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"w","mo":",","msup":{"mi":["S","i"]}}}},{"munder":{"mi":"max","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["j","k"],"mo":","}},"mo":"\u2208","msub":{"mi":["R","i"]}}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"msub":{"mi":"l","mrow":{"mi":["H","rel"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"w","mo":[",",","],"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["\u03d5","j","i"]},{"mi":["y","j","i"]}],"mo":","}},{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["\u03d5","k","i"]},{"mi":["y","k","i"]}],"mo":","}}]}}}}}],"mo":"="}}}},"wherein ranking margin w is defined as (j,k)\u03b5Ras w\u00b7(\u03c6\u2212\u03c6) if w\u00b7(\u03c6\u2212\u03c6)<(y\u2212y), and (y\u2212y) otherwise. Thus, the resulting algorithm maximizes the minimum ranking margin across all instance pairs associated with each query.","Introducing slack variables, the corresponding loss minimization problem may be written as:",{"@attributes":{"id":"p-0222","num":"0221"},"maths":[{"@attributes":{"id":"MATH-US-00045","num":"00045"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munder":{"mi":"min","mrow":{"mi":["w","\u03be"],"mo":","}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"w"},"mn":"2"}},{"mfrac":{"mi":["C","m"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":["\u03be","i"]}}}],"mo":"+"}}}}},{"@attributes":{"id":"MATH-US-00045-2","num":"00045.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":["subject","to"],"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":":"}]}}},{"@attributes":{"id":"MATH-US-00045-3","num":"00045.3"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"msup":{"mi":["\u03be","i"]},"mo":"\u2265","mn":"0"}},{"mrow":{"mo":"\u2200","mi":"i"}}]},{"mtd":[{"mrow":{"msup":{"mi":["\u03be","i"]},"mo":"\u2265","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["y","j","i"]},{"mi":["y","k","i"]}],"mo":"-"}},{"mi":"w","mo":"\u00b7","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["\u03d5","j","i"]},{"mi":["\u03d5","k","i"]}],"mo":"-"}}}],"mo":"-"}}},{"mrow":{"mrow":[{"mo":"\u2200","mi":"i"},{"mi":"k","mo":"."}],"mo":[",",","],"mi":"j"}}]}]}}}]},"Introducing Lagrange multipliers and taking the Lagrangian dual then results in the following convex QP (after an appropriate scaling of variables, and introduction of an additional variable \u03b1for each i:",{"@attributes":{"id":"p-0224","num":"0223"},"maths":[{"@attributes":{"id":"MATH-US-00046","num":"00046"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munder":{"mi":["min","\u03b1"]},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["j","k"],"mo":","}},"mo":"\u2208","msub":{"mi":["R","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"msup":{"mi":["i","\u2032"]},"mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":["j","\u2032"]},{"mi":["k","\u2032"]}],"mo":","}},"mo":"\u2208","msub":{"mi":"R","msup":{"mi":["i","\u2032"]}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":[{"mi":["\u03b1","jk","i"]},{"mi":"\u03b1","mrow":{"msup":[{"mi":["j","\u2032"]},{"mi":["k","\u2032"]}],"mo":"\u2062"},"msup":{"mi":["i","\u2032"]}},{"mi":"Q","mrow":[{"mi":"jk","mo":",","mrow":{"msup":[{"mi":["j","\u2032"]},{"mi":["k","\u2032"]}],"mo":"\u2062"}},{"mi":"i","mo":",","msup":{"mi":["i","\u2032"]}}]}],"mo":["\u2062","\u2062","\u2062"],"mfrac":{"mi":["C","m"]}}}}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["j","k"],"mo":","}},"mo":"\u2208","msub":{"mi":["R","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":["\u03b1","jk","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["y","j","i"]},{"mi":["y","k","i"]}],"mo":"-"}}}}}],"mo":["-","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"26.7em","height":"26.7ex"}}}]}}}}},{"@attributes":{"id":"MATH-US-00046-2","num":"00046.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mo":"\u2062","mrow":{"mi":["subject","to"],"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":":"}]}}}},{"@attributes":{"id":"MATH-US-00046-3","num":"00046.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mo":"\u2062","mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":["\u03b1","i"],"mn":"0"},"mo":"+","mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["j","k"],"mo":","}},"mo":"\u2208","msub":{"mi":["R","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["\u03b1","jk","i"]}}},"mo":"=","mn":"1"}},{"mrow":{"mo":"\u2200","mi":"i"}}]},{"mtd":[{"mrow":{"msubsup":{"mi":["\u03b1","jk","i"]},"mo":",","mrow":{"msubsup":{"mi":["\u03b1","i"],"mn":"0"},"mo":"\u2265","mn":"0"}}},{"mrow":{"mrow":[{"mo":"\u2200","mi":"i"},{"mi":"k","mo":"."}],"mo":[",",","],"mi":"j"}}]}]}}}}]},"The constraints in the above problem force \u03b1to lie in the simplex \u0394of distributions over |R|+1 elements for each i. This allows for derivation of an efficient exponentiated gradient (EG) algorithm which, in this case, starts with an initial set of distributions \u03b1\u03b5\u0394\u00d7 . . . \u00d7\u0394, and on each iteration t, updates the distribution associated with a single randomly chosen query i using an exponentiated gradient step:",{"@attributes":{"id":"p-0226","num":"0225"},"maths":{"@attributes":{"id":"MATH-US-00047","num":"00047"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mrow":[{"msup":{"msubsup":{"mi":["\u03b1","jk","i"]},"mrow":{"mo":["(",")"],"mrow":{"mi":"t","mo":"+","mn":"1"}}},"mo":"\u2190","mfrac":{"mrow":{"msup":{"msubsup":{"mi":["\u03b1","jk","i"]},"mrow":{"mo":["(",")"],"mi":"t"}},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":"-","msub":{"mi":"\u03b7","mn":"0"}},"mo":"\u2062","msup":{"msubsup":{"mo":"\u2207","mi":["jk","i"]},"mrow":{"mo":["(",")"],"mi":"t"}}}}}},"msup":{"mi":"Z","mrow":{"mo":["(",")"],"mi":"t"}}}},{"msup":{"msubsup":{"mi":["\u03b1","i"],"mn":"0"},"mrow":{"mo":["(",")"],"mrow":{"mi":"t","mo":"+","mn":"1"}}},"mo":"\u2190","mfrac":{"msup":[{"msubsup":{"mi":["\u03b1","i"],"mn":"0"},"mrow":{"mo":["(",")"],"mi":"t"}},{"mi":"Z","mrow":{"mo":["(",")"],"mi":"t"}}]}}],"mo":";"},{"msup":{"msup":{"mi":"\u03b1","msup":{"mi":["i","\u2032"]}},"mrow":{"mo":["(",")"],"mrow":{"mi":"t","mo":"+","mn":"1"}}},"mo":"\u2190","mrow":{"mrow":{"msup":[{"msup":{"mi":"\u03b1","msup":{"mi":["i","\u2032"]}},"mrow":{"mo":["(",")"],"mi":"t"}},{"mi":["i","\u2032"]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"for"},"mo":"\u2260","mi":"i"}}],"mo":[",","\u2062",","],"mstyle":{"mtext":{}}}}}},"wherein \u03b7>0 is a constant learning rate; \u2207is the partial derivative of the objective in the above QP with respect to \u03b1, evaluated at \u03b1; and Zchosen to ensure \u03b1\u03b5\u0394.","The third algorithm considered (RankMM-3) was constructed using a hybrid of average and maximum pair-wise loss:",{"@attributes":{"id":"p-0229","num":"0228"},"maths":{"@attributes":{"id":"MATH-US-00048","num":"00048"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"msubsup":{"mi":"L","mn":"3","mrow":{"mi":["H","rel"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"w","mo":",","msup":{"mi":["S","i"]}}}},{"munder":{"mi":"max","mrow":{"mi":"k","mo":":","mrow":{"mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["P","ik"]}},"mo":">","mn":"0"}}},"mo":"\u2062","mrow":{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["P","ik"]}}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":"P","mrow":{"mi":["i","k"],"mo":","}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"l","mrow":{"mi":["H","rel"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"w","mo":[",",","],"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["\u03d5","j","i"]},{"mi":["y","j","i"]}],"mo":","}},{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["\u03d5","k","i"]},{"mi":["y","k","i"]}],"mo":","}}]}}}}}}],"mo":"="},"mo":","}}}},"This distinguishes ranking errors at the top of the list from ranking errors at the bottom of the list since, in practice ranking errors at the top of the list may often be more costly. Thus, P={j|y>y}","denotes the set of instances that are preferred to d.","To see why this loss term might penalize ranking errors at the top more heavily than ranking errors at the bottom, note that the cost of each \u2018mis-ranking up\u2019 of a data element is inversely weighted by the number of data elements preferred to it; therefore, \u2018mis-ranking up\u2019 a lower-relevance data element by a few positions is less costly than \u2018mis-ranking up\u2019 a higher-relevance data element. By minimizing the largest such \u2018mis-ranking up\u2019 cost over all data elements, the resulting algorithm should therefore discourage mis-ranking of higher-relevance data elements, resulting in good accuracy at the top of the returned ranking.","Introducing slack variables, the corresponding loss minimization problem may be written as:",{"@attributes":{"id":"p-0234","num":"0233"},"maths":{"@attributes":{"id":"MATH-US-00049","num":"00049"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"munder":{"mi":"min","mrow":{"mi":["w","\u03be"],"mo":","}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msup":{"mrow":{"mo":["\uf605","\uf606"],"mi":"w"},"mn":"2"}},{"mfrac":{"mi":["C","m"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","msup":{"mi":["\u03be","i"]}}}],"mo":"+"}}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mrow":{"mi":["subject","to"],"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":":"}]}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mrow":{"msup":{"mi":["\u03be","i"]},"mo":"\u2265","mrow":{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["P","ik"]}}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":["P","ik"]}}},"mo":"\u2062","msubsup":{"mi":["\u03be","jk","i"]}}}}},{"mrow":{"mrow":{"mo":"\u2200","mi":"i"},"mo":",","mi":"k"}}]},{"mtd":[{"mrow":{"msubsup":{"mi":["\u03be","jk","i"]},"mo":"\u2265","mn":"0"}},{"mrow":{"mrow":{"mo":"\u2200","mi":"i"},"mo":[",",","],"mi":["j","k"]}}]},{"mtd":[{"mrow":{"msubsup":{"mi":["\u03be","jk","i"]},"mo":"\u2265","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["y","j","i"]},{"mi":["y","k","i"]}],"mo":"-"}},{"mi":"w","mo":"\u00b7","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["\u03d5","j","i"]},{"mi":["\u03d5","k","i"]}],"mo":"-"}}}],"mo":"-"}}},{"mrow":{"mrow":[{"mo":"\u2200","mi":"i"},{"mi":"k","mo":"."}],"mo":[",",","],"mi":"j"}}]}]}}}},"Introducing Lagrange multipliers and taking the Lagrangian dual then results in the following convex optimization problem (after an appropriate scaling of variables):",{"@attributes":{"id":"p-0236","num":"0235"},"maths":[{"@attributes":{"id":"MATH-US-00050","num":"00050"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munder":{"mi":"min","mrow":{"mi":["\u03b1","\u03b3"],"mo":","}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["j","k"],"mo":","}},"mo":"\u2208","msub":{"mi":["R","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"msup":{"mi":["i","\u2032"]},"mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":["j","\u2032"]},{"mi":["k","\u2032"]}],"mo":","}},"mo":"\u2208","msub":{"mi":"R","msup":{"mi":["i","\u2032"]}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":[{"mi":["\u03b1","jk","i"]},{"mi":"\u03b1","mrow":{"msup":[{"mi":["j","\u2032"]},{"mi":["k","\u2032"]}],"mo":"\u2062"},"msup":{"mi":["i","\u2032"]}}],"mo":["\u2062","\u2062"],"mfrac":{"msubsup":{"mi":"Q","mrow":[{"mi":"jk","mo":",","mrow":{"msup":[{"mi":["j","\u2032"]},{"mi":["k","\u2032"]}],"mo":"\u2062"}},{"mi":"i","mo":",","msup":{"mi":["i","\u2032"]}}]},"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"msub":{"mi":["P","ik"]}},{"mo":["\uf603","\uf604"],"msub":{"mi":"P","mrow":{"msup":[{"mi":["i","\u2032"]},{"mi":["k","\u2032"]}],"mo":"\u2062"}}}],"mo":"\u2062"}}}}}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mo":"\u2211","mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":["j","k"],"mo":","}},"mo":"\u2208","msub":{"mi":["R","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":["\u03b1","jk","i"]},"mo":"\u2062","mfrac":{"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["y","j","i"]},{"mi":["y","k","i"]}],"mo":"-"}},{"mo":["\uf603","\uf604"],"msub":{"mi":["P","ik"]}}]}}}}],"mo":"-"}}}}},{"@attributes":{"id":"MATH-US-00050-2","num":"00050.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mo":"\u2062","mi":"wherein"}}},{"@attributes":{"id":"MATH-US-00050-3","num":"00050.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mo":"\u2062","mtable":{"mtr":[{"mtd":[{"mrow":{"mn":"0","mo":["\u2264","\u2062","\u2264"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":[{"mi":["\u03b1","jk","i"]},{"mi":["\u03b3","k","i"]}]}},{"mrow":{"mrow":{"mo":"\u2200","mi":"i"},"mo":[",",","],"mi":["j","k"]}}]},{"mtd":[{"mrow":{"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"k","mo":":","mrow":{"mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["P","ik"]}},"mo":">","mn":"0"}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msubsup":{"mi":["\u03b3","k","i"]}},"mo":"=","mfrac":{"mi":["C","m"]}}},{"mrow":{"mo":"\u2200","mrow":{"mi":"i","mo":"."}}}]}]}}}}]},"The constraints in this case can be interpreted as a set of constraints on the l-norm of the \u03b1for each i, together with non-negativity constraints. Algorithms for calculating the l-norm projection are presented in Quattoni, A., Carreras, X., Collins, M., Darrell, T.: An efficient projection for 11; 1 regularization; In: Proceedings of the 26th International Conference on Machine Learning. (2009). By obtaining the l-norm projection a stochastic gradient projection method similar to above may be applied.","The RankMM-1, RankMM-2 and RankMM-3 algorithms were evaluated for an OHSUMED data set, using a benchmark data set for IR ranking algorithms available publicly as part of the LETOR distribution (LETOR 3.0). The data set consisted of 106 medical queries. Each query was associated with a number of documents, each of which was judged by human experts as being either definitely relevant to the query (label 2), partially relevant (label 1), or not relevant (label 0). There were a total of 16,140 such query-document pairs with relevance judgments (an average of roughly 152 judged documents per query). Each query-document pair was represented as a vector of 45 features.","There were five folds provided in the data set; each fold consists of a split of the queries into roughly 60% for training, 20% for validation, and 20% for testing. The algorithms were evaluated on these five folds and their performance compared to several other ranking algorithms: regression, RankSVM (see Joachims, T.: Optimizing search engines using clickthrough data. In: Proceedings of the 8th ACM Conference on Knowledge Discovery and Data Mining (2002) and Herbrich, R., Graepel, T., Obermayer, K.: Large margin rank boundaries for ordinal regression, Advances in Large Margin Classifiers (2000) 115-132), RankBoost (see Freund, Y., Iyer, R., Schapire, R. E., Singer, Y.: An efficient boosting algorithm for combining preferences, Journal of Machine Learning Research 4 (2003) 933-969), ListNet (See Cao, Z., Qin, T., Liu, T. Y., Tsai, M. F., Li, H.: Learning to rank: From pairwise approach to listwise approach. In: Proceedings of the 24th International Conference on Machine Learning (2007)), two versions of AdaRank (Xu, J., Li, H.: AdaRank: A boosting algorithm for information retrieval, In: Proceedings of the 30th ACM SIGIR Conference on Research and Development in Information Retrieval, (2007)), and SVMMAP (see Yue, Y., Finley, T., Radlinski, F., Joachims, T.: A support vector method for optimizing average precision, In: Proceedings of the 30th ACM SIGIR Conference on Research and Development in Information Retrieval. (2007)).","The following performance measures were used to evaluate the algorithms:","1. NDCG@k: The NDCG@k is simply NDCG truncated to the top k documents returned by ranking function \u0192. See J{umlaut over ( )}rarvelin, K., Kek{umlaut over ( )}al{umlaut over ( )}ainen, J.: Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems 20(4) (2002) 422-446.","2. Prec@k: Prec@k is the proportion of relevant documents in the top k documents returned by the ranking function \u0192.","3. Average Precision (AP or MAP): MAP is the average Prec@k over all positions k occupied by relevant documents.","The results are shown in . Of the algorithms shown for comparison, regression is a point-wise algorithm that predicts labels of individual documents; RankSVM and RankBoost are pair-wise ranking algorithms; and the remainder are list-wise ranking algorithms, with the last three directly optimizing the MAP or NDCG. Other than RankBoost, which uses thresholded features as weak rankers, all algorithms learn a linear ranking function. The performance of the algorithms, particularly RankMM-1 and RankMM-3, were considerably superior to the standard pair-wise (and point-wise) ranking algorithms2, and in many cases, is comparable to the performance of the algorithms that directly optimize the MAP or NDCG. The RankMM-2 appears not to have been as suited to IR performance measures; RankMM-3 appears to have been particularly suited to MAP. The best overall performance (for this data set) was obtained using RankMM-1.","Machine Embodiments:","It is contemplated that the methods, systems and non-transitory media presented herein may be carried out, e.g., via one or more programmable processing units having associated therewith executable instructions held on one or more non-transitory computer readable medium, RAM, ROM, harddrive, and\/or hardware for solving for, deriving and\/or applying ranking functions according to the algorithms taught herein. In exemplary embodiments, the hardware, firmware and\/or executable code may be provided, e.g., as upgrade module(s) for use in conjunction with existing infrastructure (e.g., existing devices\/processing units). Hardware may, e.g., include components and\/or logic circuitry for executing the embodiments taught herein as a computing process.","Displays and\/or other feedback means may also be included to convey detected\/processed data. Thus, in exemplary embodiments, ranking results may be displayed, e.g., on a monitor. The display and\/or other feedback means may be stand-alone or may be included as one or more components\/modules of the processing unit(s). In exemplary embodiments, the display and\/or other feedback means may be used to facilitate selection of one or more chemical structures or portions of chemical structures for as drug candidates.","The software code or control hardware which may be used to implement some of the present embodiments is not intended to limit the scope of such embodiments. For example, certain aspects of the embodiments described herein may be implemented in code using any suitable programming language type such as, for example, C or C++ using, for example, conventional or object-oriented programming techniques. Such code is stored or held on any type of suitable non-transitory computer-readable medium or media such as, for example, a magnetic or optical storage medium.","As used herein, a \u201cprocessor,\u201d \u201cprocessing unit,\u201d \u201ccomputer\u201d or \u201ccomputer system\u201d may be, for example, a wireless or wireline variety of a microcomputer, minicomputer, server, mainframe, laptop, personal data assistant (PDA), wireless e-mail device (e.g., \u201cBlackBerry\u201d trade-designated devices), cellular phone, pager, processor, fax machine, scanner, or any other programmable device configured to transmit and receive data over a network. Computer systems disclosed herein may include memory for storing certain software applications used in obtaining, processing and communicating data. It can be appreciated that such memory may be internal or external to the disclosed embodiments. The memory may also include non-transitory storage medium for storing software, including a hard disk, an optical disk, floppy disk, ROM (read only memory), RAM (random access memory), PROM (programmable ROM), EEPROM (electrically erasable PROM), etc.","Referring now to , an exemplary computing environment suitable for practicing exemplary embodiments is depicted. The environment may include a computing device  which includes one or more non-transitory media for storing one or more computer-executable instructions or code for implementing exemplary embodiments. For example, memory  included in the computing device  may store computer-executable instructions or software, e.g. instructions for implementing and processing an application  for solving for, deriving, and\/or applying a ranking algorithm for ranking data elements, as taught herein. For example, execution of application  by processor  may programmatically (i) determine a ranking error as a function of ranking order discrepancies between a first predetermined ranking of a plurality of data elements and a second ranking of the plurality of data elements by a ranking function, and (i) solve the ranking error function for the ranking function, whereby the ranking error is minimized with respect to the first ranking of the plurality of data elements. In some embodiments, execution of application  by processor  may apply a ranking function minimizing ranking error between a first predetermined ranking of a training plurality of data elements and a second ranking of the training plurality of data elements by the ranking function, as taught herein, to rank a test set of data elements, for example in response to a query.","The computing device  also includes processor , and, one or more processor(s) \u2032 for executing software stored in the memory , and other programs for controlling system hardware. Processor  and processor(s) \u2032 each can be a single core processor or multiple core ( and \u2032) processor. Virtualization can be employed in computing device  so that infrastructure and resources in the computing device can be shared dynamically. Virtualized processors may also be used with application  and other software in storage . A virtual machine  can be provided to handle a process running on multiple processors so that the process appears to be using only one computing resource rather than multiple. Multiple virtual machines can also be used with one processor. Other computing resources, such as field-programmable gate arrays (FPGA), application specific integrated circuit (ASIC), digital signal processor (DSP), Graphics Processing Unit (GPU), and general-purpose processor (GPP), may also be used for executing code and\/or software. A hardware accelerator , such as implemented in an ASIC, FPGA, or the like, can additionally be used to speed up the general processing rate of the computing device .","The memory  may comprise a computer system memory or random access memory, such as DRAM, SRAM, EDO RAM, etc. The memory  may comprise other types of memory as well, or combinations thereof. A user may interact with the computing device  through a visual display device , such as a computer monitor, which may display one or more user interfaces . The visual display device  may also display other aspects or elements of exemplary embodiments, e.g., databases, ranking results, etc. The computing device  may include other I\/O devices such a keyboard or a multi-point touch interface  and a pointing device , for example a mouse, for receiving input from a user. The keyboard  and the pointing device  may be connected to the visual display device . The computing device  may include other suitable conventional I\/O peripherals. The computing device  may further comprise a storage device , such as a hard-drive, CD-ROM, or other storage medium for storing an operating system  and other programs, e.g., application  characterized by computer executable instructions solving for, deriving, and\/or applying a ranking algorithm for ranking data elements, as taught herein.","The computing device  may include a network interface  to interface to a Local Area Network (LAN), Wide Area Network (WAN) or the Internet through a variety of connections including, but not limited to, standard telephone lines, LAN or WAN links (e.g., 802.11, T1, T3, 56 kb, X.25), broadband connections (e.g., ISDN, Frame Relay, ATM), wireless connections, controller area network (CAN), or some combination of any or all of the above. The network interface  may comprise a built-in network adapter, network interface card, PCMCIA network card, card bus network adapter, wireless network adapter, USB network adapter, modem or any other device suitable for interfacing the computing device  to any type of network capable of communication and performing the operations described herein. Moreover, the computing device  may be any computer system such as a workstation, desktop computer, server, laptop, handheld computer or other form of computing or telecommunications device that is capable of communication and that has sufficient processor power and memory capacity to perform the operations described herein.","The computing device  can be running any operating system such as any of the versions of the Microsoft\u00ae Windows\u00ae operating systems, the different releases of the Unix and Linux operating systems, any version of the MacOS\u00ae for Macintosh computers, any embedded operating system, any real-time operating system, any open source operating system, any proprietary operating system, any operating systems for mobile computing devices, or any other operating system capable of running on the computing device and performing the operations described herein. The operating system may be running in native mode or emulated mode.",{"@attributes":{"id":"p-0255","num":"0254"},"figref":"FIG. 13","b":["150","150","152","154","156","158","160","152","154","156","158","102","118","102","152","154","156","158","160","160","160","160"]},"In the network environment , the servers  and  may provide the clients  and  with software components or products under a particular condition, such as a license agreement. The software components or products may include one or more components of the application . For example, the client  may solve for and\/or derive a ranking function which is subsequently applied over the server  for ranking representations of chemical structures.","Although the teachings herein have been described with reference to exemplary embodiments and implementations thereof, the disclosed methods, systems and media are not limited to such exemplary embodiments\/implementations. Rather, as will be readily apparent to persons skilled in the art from the description taught herein, the disclosed methods, systems and media are susceptible to modifications, alterations and enhancements without departing from the spirit or scope hereof. Accordingly, all such modifications, alterations and enhancements within the scope hereof are encompassed herein."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 13"}]},"DETDESC":[{},{}]}
