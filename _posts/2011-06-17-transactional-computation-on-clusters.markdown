---
title: Transactional computation on clusters
abstract: Computations are performed on shared datasets in a distributed computing cluster using aggressive speculation and a distributed runtime that executes code transactionally. Speculative transactions are conducted with currently available data on the assumption that no dependencies exist that will render the input data invalid. For those specific instances where this assumption is found to be incorrect—that the input data did indeed have a dependency (thereby impacting the correctness of the speculated transaction)—the speculated transaction is aborted and its results (and all transactions that relied on its results) are rolled-back accordingly for re-computation using updated input data. In operation, shared state data is read and written using only the system's data access API which ensures that computations can be rolled-back when conflicts stemming from later-determined dependencies are detected.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08661449&OS=08661449&RS=08661449
owner: Microsoft Corporation
number: 08661449
owner_city: Redmond
owner_country: US
publication_date: 20110617
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Distributed computing is the use of multiple autonomous computers (or processors) to solve computational problems by dividing the problem into many sub-problems that are then solved by one or more of the autonomous computers (or nodes) in a cluster of computers. Distributed shared memory (DSM) is an aspect of distributed computing where each node of a cluster has access to shared memory that is distributed across the cluster and which may comprise local memory, remote memory, or both with regard to each node. Certain performance-critical workloads\u2014such as those requiring more memory or processing power than is available in a single computer\u2014can benefit from the abstraction of a mutable (changeable) shared state that is made possible via distributed computing systems employing distributed shared memory.","To perform computations on very large datasets, distributed computing clusters (comprising, for example, tens to thousands of autonomous computers) employing distributed shared memory may be utilized. However, many computations (and their associated algorithms) may be challenging to parallelize because of the possibility of dependencies existing between certain sub-computations and the inherent difficulty in determining these dependencies in advance. A conservative approach to handling possible dependencies is to wait for prior sub-computations to complete before running subsequent sub-computations that possibly depend on those prior sub-computations, but such an approach requires a substantial amount of synchronization between parallel processes (the autonomous computers via the computer network, for example) which in turn become a substantial bottleneck to efficient computational performance.","Computational dependencies, however, may be relatively rare for certain useful computations compared to the total number of computations over an entire large dataset, and in such instances the vast majority of individual computations (and sub-computations) will have no such dependencies. Thus most computations (and sub-computations) need not be delayed since they do not depend on the completion of other computations, but the seeming difficulty lies in the inability to identify and separate in advance the computations having dependencies from the computations that do not.","To provide high-performance computing over distributed shared memory, various implementations disclosed herein are directed to performing computations on shared datasets in a distributed computing cluster using aggressive speculation and a distributed runtime (an instance of a distributed execution engine) that executes code transactionally. Various implementations conduct speculative transactions with currently available data on the assumption that no dependencies exist that will render the input data invalid. For those specific instances where this assumption is found to be incorrect\u2014that the input data did indeed have a dependency (thereby impacting the correctness of the speculated transaction)\u2014the speculated transaction is aborted and its results (and, generally, all transactions that relied on its results) are rolled-back accordingly for re-computation using updated input data. For these various implementations, shared state data is read and written using only the system's data access API which is specifically designed to ensure that computations can be rolled-back when conflicts stemming from later-determined dependencies are detected.","In addition, since a speculated transaction can be aborted and\/or rolled-back if the input data it utilizes turns out to be inaccurate, these various implementations make it possible to aggressively pre-fetch data and complete speculative transactional computations that, much more often than not, will ultimately be found to have no dependencies and thus be correct. This approach, in turn, results in reduced latency of synchronization since it is not necessary to wait to ensure all possible dependencies are completed before completing machine-level computations\u2014which, again, will be correct except when a dependency impacting the input data is later found (which may be relatively rare). Similarly, the runtime for several implementations further reduces synchronization latency by executing subsequent children transactions speculatively and, if conflicts are later found with the parent speculative transactional computation, the runtime aborts or rolls-back these children computations.","This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the detailed description. This summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","Distributed computing is a form of computing in which many calculations are carried out simultaneously on the premise that large problems can often be divided into smaller problems which can be solved concurrently (\u201cin parallel\u201d) for efficiency. However, distributed computing programs are more difficult to write than sequential ones because concurrency (\u201coperating in parallel\u201d) introduces several new classes of potential bugs, conflicts, data access issues, and utilization inconsistencies, as well as a host of other potential errors. Consequently, communication and synchronization between the different concurrent processes can be major challenges to achieving good efficiency and consistency. For this reason, most distributed computing systems use some kind of consistency model that defines rules for how distributed computing operations occur and how results are produced.","There are several different forms of distributed computing, such as bit-level, instruction level, task level, and data parallelism. This last form, data parallelism, focuses on distributing the data across different computing nodes (machines or processors) to be processed in parallel such as, for example, parallelism inherent in program loops where similar (though not necessarily identical) operation sequences or functions are being performed on elements of a large data structure (which are common to many scientific and engineering applications).","A multicore processor is a processor that includes multiple execution units (\u201ccores\u201d) on the same chip. As such, a multicore processor can issue multiple instructions per cycle from multiple instruction streams. A multiprocessor computer, in comparison, is a stand-alone computer system (or \u201cmachine\u201d) with multiple processors that share memory and may connect via a bus, point-to-point links, or other high-speed means; however, \u201cbus contention\u201d (where more than one processor attempts to use the bus at the same time) and similar limitations may prevent these computer systems from scaling to more than thirty-two (32) processors. As such, a multiprocessor computer may comprise one or more multicore processors for adding computational power.","A distributed computer (sometime referred to as a distributed memory multiprocessor) is comprised of multiple processors connected by a network (and thus is highly scalable) to solve computational problems using parallel computing (where a problem is divided into many sub-problems, each of which is solved by different processor). For example, a massively parallel processor (MPP) is a single stand-alone computer with many networked processors using specialized high-speed interconnect networks where generally each processor has its own memory, copy of the operating system, and copy of the application(s). In contrast, a cluster (or cluster computer system) is a distributed computer comprising multiple computer systems (each a \u201ccluster computer,\u201d \u201cautonomous computer,\u201d or a \u201cmachine\u201d) connected by a network where each machine has its own processing elements, memory, operating system, and applications, and the network generally comprises commodity networking hardware. A grid computer system (or grid) is similar to a cluster but where the networked computers communicate over the Internet which, because of its relatively low bandwidth and high latency, are the most distributed form of parallel computing and typically deals only with \u201cembarrassingly parallel\u201d problems, that is, problems that are easily split into parallel tasks that require little or no communication between such tasks.","A distributed computer\u2014whether an MPP, a cluster, or a grid\u2014may comprise one or more multiprocessor computers and\/or comprise one or more multicore processors. There are also several specialized parallel\/distributed computer systems based on reconfigurable computing systems with field-programmable gate arrays, general-purpose computing systems on graphics processing units, application-specific integrated circuits, vector processors, to name a few, for example.","Notwithstanding the foregoing, the terms \u201cconcurrent,\u201d \u201cparallel,\u201d and \u201cdistributed\u201d strongly overlap, and are used interchangeably herein such that a same system may be characterized as \u201cparallel\u201d and\/or \u201cdistributed\u201d without loss of generality such that processors in a distributed system run concurrently in parallel. Where distinctions are necessary and the terms are in disjunctive and in obvious conflict to a person of ordinary skill in the relevant art, then the term \u201cparallel\u201d as used in parallel computing shall refer to all processors having access to a shared memory that can be used to exchange information between processors, whereas the term \u201cdistributed\u201d as used in distributed computing shall refer to each processor having its own private memory (a part of the \u201cdistributed memory\u201d) where information is exchanged by passing messages between the processors (presumably through an intermediary of some kind).","While various implementations disclosed herein are described in terms of a distributed computing system and, more specifically, in terms of a cluster computer system (or \u201cdistributed computing cluster\u201d), skilled artisans will readily recognize that such implementations can readily be implemented on other types of distributed computing systems, and nothing is intended to limit the implementations disclosed herein to any specific distributed computer type nor to any specific configuration of processors such as multiprocessors but, instead, are intended to be given the widest interpretations possible.",{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 1","b":["100","100","110","140","120","120","110","140","120","110","140"]},"As shown in , and for several implementations disclosed herein, the communication server  may be part of a distributed computing cluster  comprising the communication server  and other computers (or processors) in a processing cluster  comprising a plurality of cluster machines (or simply \u201cservers\u201d) -, -, . . . , -(each also referred to interchangeably as a \u201cmachine\u201d, \u201ccluster server\u201d, \u201ccluster computer,\u201d or \u201cautonomous computer\u201d) interconnected by a network \u2032. The communication server  may be a separate machine from the machines in the processing cluster  (as shown) or the communication server  may also comprise a machine in the processing cluster . Moreover, the network \u2032 may be local network of some kind (e.g., a local area network or LAN) or it may be an extension of a larger network such as network .","In some implementations, the client  may include a desktop personal computer, workstation, laptop, PDA, cell phone, smart phone, or any WAP-enabled device or any other computing device capable of interfacing directly or indirectly with the network  such as a computing device  illustrated in . The client  may run an HTTP client, e.g., a browsing program, such as MICROSOFT INTERNET EXPLORER or other browser, or a WAP-enabled browser in the case of a cell phone, PDA or other wireless device, or the like, allowing a user of the client  to access information available to it at the communication server  or to provide information to the communication server . Other applications may also be used by the client  to access or provide information to the communication server , for example. In some implementations, the server  may be implemented using one or more general purpose computing systems such as the computing device  illustrated in .",{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 2","b":["200","200","210","220","230","240","250","260","270","1","270","2","270"],"i":"n. "},"The execution engine  is a software layer that coordinates the execution of multiple dependent programs (processes) running on a computer cluster and communicating with each other through messaging channels. The execution engine  may also handle the reliable execution of the cluster servers -, -, . . . , -via the cluster storage  and cluster services  layers\u2014that is, the execution engine  may schedule computations to the cluster computers, monitor their execution, collects and reports statistics, and handle transient failure in the cluster by re-executing failed or slow computations.","The compiler , aided by functionality provided by the front-end library  and API , translates specific computations into jobs that can be executed on a cluster by the execution engine . The compiler  provides a set of operators that perform computations on collections of values where, for certain implementations, the specific language may be similar to the SQL database language and the collections are the equivalent of database tables. In addition, collections may be partitioned by the compiler , with different partitions residing on different machines in the cluster. During computation, the collection elements may be moved between computers, so the in-memory data structures are serialized to a shared physical medium such as a disk or the network. To this end, the compiler  automatically generates serialization and de-serialization code, although for certain implementations a user might replace the default serialization routines with custom routines.","For some implementations\u2014and unlike SQL\u2014the compiler  may be \u201cembedded\u201d within, for example, the \u201c.NET languages\u201d such that there are operators for C#, VB, and F#, for example. For several implementations, the basic compiler  operations may comprise (1) applying a transformation (function) to all elements in a collection, (2) filtering elements according to a predicate, (3) grouping elements by a common key, (4) aggregating the elements according to some function (e.g., addition), and (5) joining the elements in two collections using a common key. Thus, for a user\/programmer of the system, a benefit of the compiler  is the provision of a single high-level programming language (e.g., a \u201c.NET language\u201d) to write the application and blending seamlessly the local and distributed parts of the computation in a single program due to tight embedding of the compiler  within these well-known languages.","In several implementations, the compiler  not only generates jobs but also generates parallel multi-threaded code for each of the processes in a job graph to utilize multiple processing cores. In certain such implementations, the parallelization across cores may use similar techniques to the parallelization across machines. The compiler  translates the operations on the large collections into operators on individual partitions which are then further partitioned across cores and processed in parallel. Moreover, for some implementations, the compiler  may provide a generalization of the map-reduce computation model\u2014comprising a particular sequence of operators such as, for example, SelectMany, GroupBy, and Aggregate\u2014that is elsewhere implemented in both proprietary and open-source computation stacks known to those of skill in the art.","To perform computations on very large datasets, distributed computing clusters (comprising, for example, tens to thousands of autonomous computers) employing distributed shared memory may be utilized. However, many computations (and their associated algorithms) may be challenging to parallelize because of the possibility of dependencies existing between certain sub-computations and the inherent difficulty in determining these dependencies in advance. For example, an algorithm that performs fine-grain updates to a large shared data structure may benefit from such a system, although a challenge arises when these updates have an order that is difficult to predict statistically. A conservative approach to handling possible dependencies is to wait for prior sub-computations to complete before running subsequent sub-computations that possibly depend on those prior sub-computations, but such an approach requires a substantial amount of synchronization between parallel processes (the autonomous computers via a computer network, for example) which in turn become a substantial bottleneck to efficient computational performance.","Computational dependencies, however, may be relatively rare compared to the total number of computations over an entire large dataset, and in most instances the vast majority of individual computations (and sub-computations) have no such dependencies. Thus most computations (and sub-computations) need not be delayed since they do not depend on the completion of other computations, but a seeming difficulty lies in the inability to identify and separate in advance the computations having dependencies from the computations that do not.","To provide high-performance computing over distributed shared memory, various implementations disclosed herein are directed to systems and methods for performing computations on shared datasets in a distributed computing cluster using aggressive speculation and a distributed runtime (an instance of a distributed execution engine) that executes code transactionally (which, as used herein, means atomically and in isolation, with effects of a parallel execution of multiple transactions equivalent to those of a serial execution). These various implementations utilize aggressive speculation to hide the communication latency stemming from inter-nodal communications largely comprising remote memory access and synchronization of computations. This latency is frequently a bottleneck to performance gains when using distributed shared memory. Since the speculation will be correct in most instances, substantial parallelism can be gained. As for the instances where mis-speculation occurs (i.e., when a conflict arises when a dependency is discovered), such corresponding computations are aborted and\/or rolled-back for re-computation. Distributed software transactional memory is used to detect dependencies and correct the corresponding conflicts stemming from aggressive speculation in such various implementations.","Various such implementations conduct speculative transactions with currently available data on the assumption that no dependencies exist that will render the input data invalid. For those specific instances where this assumption is found to be incorrect\u2014that the input data did indeed have a dependency (thereby impacting the correctness of the speculated transaction)\u2014the speculated transaction is aborted and its results (and all transactions that relied on its results) are rolled-back accordingly for re-computation using updated input data. For these various implementations, shared state data is read and written using only the system's data access API which is specifically designed to ensure that computations can be rolled-back when conflicts stemming from later-determined dependencies are detected.","Since a speculated transaction can be aborted and\/or rolled-back if the input data it utilizes turns out to be inaccurate, these various implementations make it possible to aggressively pre-fetch data and complete speculative transactional computations that, much more often than not, will ultimately be found to have no dependencies and thus be correct. This approach, in turn, results in reduced latency of synchronization since it is not necessary to wait to ensure all possible dependencies are completed before completing machine-level computations\u2014which, again, will be correct except when a dependency impacting the input data is later found (which may be relatively rare). Similarly, the runtime for several implementations further reduces synchronization latency by executing subsequent children transactions speculatively and, if conflicts are later found with the parent speculative transactional computation, the runtime aborts or rolls-back these children computations.","Certain implementations execute transactions in parallel even when program order specifies a control-flow dependency (i.e., even though a dependency is pre-identified) because control flow dependencies can be safely ignored (except to the extent control flow dependencies are needed to perform commits and roll-backs). Transactions that operate on the same elements of shared state, comprising the expected conflicts (which may also be relatively rare even though pre-identified), are handled the same as unexpected conflicts\/dependencies where the subject computations are cancelled or rolled-back accordingly. Similarly, this approach gains the benefits of parallelism from the much larger portion of the workload ultimately having no dependencies and, thus, where the speculative results are ultimately correct.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 3","FIG. 1"],"b":["300","300","152","1","152","2","152","310","1","310","2","310","310","310","320","1","320","2","320","320","320","312","322","320","340","310"],"i":["n ","n ","n "]},"A control system  manages the operations of compute nodes  (constituting a distributed computing subsystem) and their interactions with the storage nodes  (constituting the shared distributed storage or system \u201cstate\u201d), and thus the control system  comprises the API  to perform such interactions (such as reads and writes). The control system  may also comprise an ordering node (ON) , a transaction coordinator (TC) , a transaction commit ordering (TCO) module , a global scheduling node (GSN) , a roll-back subsystem (RBS) , and a global pool of transactions (GPT or \u201cpool\u201d) . Each of these components of the control system  may reside centrally for certain implementations or, alternately, in a distributed fashion (e.g., with the compute nodes) in other implementations. The pool  contains the tasks  that, together with a task graph (described in more detail later herein), represent the program to be executed by the system  over the shared distributed memory or state maintained in the storage nodes . These components are described in more detail herein.",{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 4","b":["400","400","402","404"]},"As each transaction completes, at , the transaction is then eventually designated as committable based on a traversal of a task graph corresponding to a dependency ordering of the plurality of tasks discussed later herein. At , each committable transaction is then assessed to determine if there are any conflicts using one of several methodologies discussed later herein. If at  no conflicts are found, then at  the transaction is committed and its resources are released for use in processing other uncommitted transactions at .","On the other hand, if there are conflicts at , then at  the transaction is aborted and its resources are released. Then, if at  the task corresponding to that transaction has not yet committed, that task is re-executed as a new transaction at ; otherwise, it is unnecessary to re-execute the task (presumably because it was successfully executed and committed on another compute node), so the compute node can process another task at  until there are no more tasks left to commit.","With regard to the foregoing, and referring again to , several implementations disclosed herein are directed to computation on a distributed shared memory system comprising several elements. For these implementations, the system may distribute a plurality of tasks (representative of a computational program) to a plurality of compute nodes for execution on the distributed shared memory system (which, in turn, comprises the storage nodes holding the versioned data objects that together constitute system state data). The system\u2014namely the compute nodes\u2014then execute each task from among the plurality of tasks (presumably from the pool) as a transaction without any regard to possible dependencies (i.e., as a \u201cspeculative transaction\u201d). As each transaction completes and is then designated as \u201ccommittable\u201d (presumably based on a traversal of a task graph corresponding to a dependency ordering of the plurality of tasks), the system then determines whether the committable transaction has any conflicts. If there are no conflicts, then the system can commit that transaction, and the system also indicates that the corresponding task for that transaction has been committed and no longer needs to be processed (and thus permits any running processes for that task to be aborted). Conversely, if the transaction is found to have at least one conflict, then the system aborts that transaction and, if the corresponding task has not yet been committed by another transaction somewhere in the system, then that task may be re-executed as a new transaction.","As noted earlier herein, each compute node comprises a local cache and, for certain such implementations, the compute node may further comprise a read set and write set for each transaction executing on the compute node, as well as a dependency graph for tracking local dependencies as they arise. The cache stores the copied versioned data objects the compute node reads from the storage nodes (as well as the new versions written by the storage nodes transactions). Regarding the latter, and for certain implementations, when a versioned data object is to be used for a computation for a transaction but a copy of the versioned data object does not already reside in the local cache, the compute node reads the versioned data object from its storage node and stores a copy of that versioned data object in the local cache and, for some implementations, may also update a read set corresponding to the transaction, and then the system uses the copy of the versioned data object from the local cache to complete the computation for the transaction.","When making an \u201cupdate\u201d to the versioned data object for a transaction, an approach for several implementations may be to write a new updated copy of the versioned data object to the local cache (with an undetermined version number) while leaving the original versioned data object in the local cache unchanged (to support roll-backs as well as version selection by subsequent transactions). The compute node may also modify the dependency graph with the new updated copy of the versioned data object and its dependency on the copy of the versioned data object to which the update was applied, as well as update a write set corresponding to the transaction to reflect the transaction's write.","For conflict detection, several implementations may first determine if any conflicts exist in the dependency graph, and then determine if any conflicts exist in the read set. To accomplish the latter, such implementations might first, for each versioned data object in the read set or the write set (or both), lock the corresponding versioned data objects in their storage nodes. Once successfully locked, then a comparison of each versioned data object in the read set to the corresponding versioned data object in the storage node can be made and, if all of the versioned data objects in the read set are the same version (i.e., have the same version number) as the corresponding data objects in the storage nodes, then return that no dependency conflicts exist so that a commit operation can finish. Otherwise, if any of the versioned data objects in the read set are not the same version as the corresponding data objects in the storage node, then return that at least one conflict exists so that the abort operation can commence. In some implementations the storage node may indicate which objects were in conflict, to aid the compute node in refreshing its cache.","To commit an updated version of a versioned data object from a write set to its storage node, several implementations may overwrite the corresponding versioned data object in that storage node with the versioned data object in the write set while incrementing accordingly the version indicator (or version number) for that versioned data object in the storage node. Once all overwrites for the write set have been completed (and now irrevocably committed), each versioned data object in the read set or the write set (or both) is then unlocked in their respective storage nodes. In addition, some such implementations may update the task graph for the task corresponding to the transaction (as \u201ccommitted\u201d) as well as update the dependency graph for each versioned data object in the write set to reflect the new committed version of the versioned data object (and possibly leading to some selective aborting of transactions accordingly). Conversely, aborting a transaction in such implementations might be accomplished by (in no particular order): (a) unlocking the versioned data objects in the storage nodes that were previously locked (i.e., for each versioned data object in the read set or the write set or both); (b) aborting the transaction; and (c) aborting any subsequent transactions dependent upon the transaction as indicated by the dependency graph.","With regard to the system of , and for various implementations disclosed herein, the distributed shared memory may comprise a plurality of storage nodes for storing versioned data objects constituting the system's \u201cstate\u201d (a.k.a., \u201cshared state\u201d). As such, the distributed computing subsystem may then comprise compute nodes for executing a program that has been divided into a plurality of tasks for execution as transactions in accordance with a task graph describing acceptable ordering of the tasks. Moreover, to discover additional parallelism, the transactions are executed speculatively, that is, without regard to dependency conflicts (including ignoring the task graph) and effectively presuming (i) that the transaction's corresponding task has no dependencies and (ii) that any versioned data objects used are valid. Of course, one or both presumptions may not hold true, but when they do, there is a gain in discovered parallelism and when they do not, they are aborted and rolled-back accordingly. In addition, certain implementations may also comprise a control subsystem to, for example, determine when a task is committable and either committed or aborted. The control subsystem, in turn, may comprise a roll-back subsystem for aborting a task and, when appropriate, any additional tasks dependent upon the aborted task.","With regard to transaction reads and writes, several implementations may utilize an local-cache-centric approach where each compute node reads a versioned data object from a corresponding storage node from among the plurality of storage nodes and stores a copy of the versioned data object to the local cache if and only if a transaction needs the versioned data object and a copy of the versioned data object is not already present in the local cache. By focusing on the local cache contents, such implementations can restrict transactions such that they can only read the copies of versioned data objects residing in the local cache and, similarly, can only write updated versioned data objects to the local cache. Of course, such implementations may also store a read set and a write set for each transaction on the compute node, as well as a dependency graph for transaction on the compute node and an ordered list of each versioned data object read and written to the local cache.","For certain implementations, the control subsystem permits each compute node to attempt to commit a transaction corresponding to a task from among a plurality of tasks if and only if the transaction has completed running, the task corresponding to the transaction has no preceding tasks in the task graph that have not been committed, and the transaction has no preceding transactions in the dependency graph that have not been committed. The commit will succeed if every copy of the versioned data objects in the transaction's read set are the same version as the versioned data objects in the plurality of storage nodes. For other implementations, the control subsystem may be distributed across the plurality of compute nodes and thus comprise, for example, a distributed transactional commit protocol to detect dependency conflicts.","With regard to the system of , several implementations may comprise three high-level operations as follows: (1) for each task from among the plurality of tasks, repeatedly executing the task as a set of transactions comprising at least one transaction until the task is committed, wherein each such transaction runs without regard to possible dependencies; (2) designating a transaction as committable when all tasks in the task graph that precede the transaction's corresponding task have been committed; and (3) either committing each transaction designated as committable if it has no uncommitted dependency, or aborting each transaction having a conflict as well as aborting each transaction depending upon an aborted transaction (thus enabling the detection of a dependency conflict, aborting the transaction for which the conflict was found, and then aborting one or more additional subsequent transactions that inherently depended upon the aborted transaction). With regard to the third element, on one hand the commit ordering protocol may determine when it is safe to submit a given transaction for commit, while on the other hand the commit node may use local information to detect conflicts and abort transactions as a result (without necessarily communicating with other nodes). Moreover, this \u201cother hand\u201d (a.k.a., a \u201clocal abort\u201d) may be optional because the commit protocol itself would abort the transactions anyway since they have a conflict, although aborting them earlier may save resources. In addition, several such implementations may maintain a task graph corresponding to a dependency ordering of the plurality of tasks, as well as track each transaction's reads in a read set and writes in a write set, to support dependency conflict detection and rolling-back (by selective removals of bad data). Such implementations may also maintain, for each versioned data object in a local cache of a compute node, a sorted list of object versions to likewise assist with dependency conflict detection and support rolling-back (again by selective removal of bad data).","When processors are plentiful in a cluster, often the main bottleneck for performance is communication which is limited in both bandwidth and latency. To conserve bandwidth, compiling functional programs into dataflow graphs can yield some efficiencies; however, this approach is not well-suited for algorithms that are difficult to express functionally. For example, an algorithm that performs fine-grain updates to a large shared data-structure in an unpredictable order is difficult to express functionally.","Even though an algorithm is hard to express functionally, however, there may be a large amount of potential parallelism since many atomic operations (or \u201ctasks\u201d) that comprise such programs operate on unrelated (or disjoint) elements of the data structure, and thus could possibly be executed in any order with respect to the other operations. Yet because the inter-relationship between operations that are disjoint versus operations that depend upon one another is extremely difficult to predict in advance, frequent synchronization is seemingly required to detect dependencies real-time in order to ensure correctness of the program execution, and the unavoidable latency of such extensive communication necessary for this level of synchronization is often more detrimental than any potential benefits that could be gained from parallel execution.","However, as embodied by several implementations disclosed herein, the performance penalty of this \u201csynchronization latency\u201d can be significantly mitigated in certain workloads using \u201caggressive speculation\u201d to forge ahead with parallel executions on an assumption of correctness and then detect and undo any incorrect work. To aid in this detection, various implementations disclosed herein utilize software transactional memory and interfaces, thereby making \u201ctransactions\u201d an implementation mechanism for speculation. Moreover, for the various implementations disclosed herein, the programming code executes as transactions, and an explicit graph structure is used to encode the control-flow dependencies between the individual transactions that comprise a program. In other words, the graph encodes the set of legal serialization orders for the transactions making up a program. This in turn permits the use of efficient algorithms known to skilled artisans to determine commit order, the side effects a transaction abort will have on its successor transactions, and so forth.","In addition, several implementations may annotate transactions with hints about the data they will access. This may also net additional benefits stemming from dataflow scheduling when data dependencies are known in advance since a scheduler can in principle inspect the task and transaction graph and make intelligent decisions about placement of computation and pre-fetching of remote data. In addition, data hints may be used to help select from the task graph the computation node for a given group of task (aggregating tasks that are likely to read each other's data) as well as assist in the selection of the computation node for a specific single task such as by placing that task on a computer that is near the storage node holding the data it needs. Moreover, data hints on the transaction graph might also be used to determine which transaction's data to pre-fetch first.","For various implementations disclosed herein, a program for execution is reduced to a set of atomic code fragments called \u201ctasks\u201d that are arrangeable in a directed-acyclic graph (DAG) referred to as the \u201ctask graph\u201d representative of flow control. Thus the task graph imposes a partial order on the tasks that make up a program, and the valid execution of a program will be indistinguishable from a serialization of the tasks which respects the partial order. In addition, the task graph initially constructed to describe a program may be modified by a task during its execution by inserting (\u201cforking\u201d or \u201cfork\u201d) a subgraph between itself and its immediate successors in the task graph\u2014in effect, enabling the task to divide itself up into smaller tasks that could potentially be run in parallel.","For several implementations, whenever a task is added to the task graph the task may be assigned a unique task identification (TID), a generation number, and\/or an ancestor vector. Moreover, when a task is executed, the execution of that task (referred to as a \u201ctransaction\u201d) may be assigned a unique execution id (EID). For example, the generation number and ancestor vector might be computed as follows: if the task has generation number 1.3.4 and ancestor vector 0.53.98 and the transaction has EID 134, then the second new task added by original task (i.e., the original task's second call to \u201cfork\u201d) will have generation number 1.3.4.2 and ancestor vector 0.53.98.134. The ancestor vector can be used directly to determine the set of tasks forked by a particular transaction. Together with the generation number, the ancestor vector can be used to construct a test of partial ordering between two tasks.","For certain such implementations, the programs may be represented with a task graph that consists of a dummy task L (with TID 0, generation number 0, and ancestor vector 0), a root task R (with TID 1, generation number 1, and empty ancestor vector), and dummy task T (with TID 2, generation number 2, and empty ancestor vector) ordered so that L precedes R in the task graph, and R precedes T in the task graph.","For several implementations, each task may include a set of \u201carguments\u201d (read-only for some implementations) that are specified at the time the task is forked. From that point forward, however, no other communications may occur between tasks except what can be achieved using the global set of shared date organized as versioned data objects (VDOs) that can potentially be created, read, written, or deleted by any task in accordance with the system's APIs and policies. Each VDO may have a unique identifier (OID) assigned at creation time and can be used to located the VDO among the system's storage nodes. In addition, tasks (and, specifically, a task's transaction) may generate \u201cside-effects\u201d that, for various implementations, may only comprise (a) modifications (create, update, or deletion) VDOs and (b) forking new tasks. Some implementations may allow additional side-effects, and using compensating actions certain implementations may thus effectively deal with some file creation\/modification\/deletion side effects.","The model for transactional execution employed by various implementations herein disclosed is based on software transactional memory designs but with the addition of speculation. For such implementations, each task is executed as a transaction which will either (a) be included in the serialization of the program as though it executed atomically or (b) will be aborted leaving no side-effects. As used herein, the term \u201ctransaction\u201d is used to refer to an execution of a task. Each task may be executed multiple times, and two transactions of the same task may be executed concurrently (presumably on different compute nodes, although it is possible for both to execute on the same compute node). However, no more than one transaction per task will be committed, and the program completes successfully when every task in the task graph has been committed. Thus a task is deemed committed when its first (and only) corresponding transaction is committed.","For various implementations, a transaction can exist in one of four states. When created\u2014and a new transaction for any uncommitted task can be created at any time\u2014that transaction starts in a Running state. Then, when the transaction finishes execution, it is in the Completed state. Although not committed, for some implementations the side-effects of the transactions' execution (e.g., new tasks or updated VDOs) may be made visible to other transactions when the transaction enters the Completed state; in other implementations, side-effects may be made visible to other transactions as they occur while the transaction is in the Running state. Either way, the system permits other transactions to speculation that these side-effects will be committed and to execute accordingly (and later be aborted if the speculation was incorrect).","When an uncommitted side-effect is used by second transaction\u2014such as using a first transaction's updated VDO\u2014a dependency is formed between the second transaction and the first transaction. These updates form a directed graph referred to as a \u201cdependency graph\u201d (DG). If a transaction aborts, then in some implementations all of its successors in the dependency graph are aborted (although this does not necessarily include all the tasks successors in the task graph), while in other alternative implementations some successors may survive if the re-execution of the original transaction yields the same value.","For various implementations, a transaction may transition from Completed to Committed if and only if all of its predecessors (preceding tasks) in both the task graph and dependency graph are already Committed. Of course, this condition could cause a deadlock (i.e., two tasks waiting for the other to commit before it can commit), but the system could detect this situation and use policy to select one transaction to abort in order to resolve the conflict and permit one transaction to commit (and the other transaction to re-execute and later commit). Regardless, for such implementations, when a transaction transitions from Completed to Committed, its side-effects are irrevocable and that transaction is removed from the dependency graph.","Many different approaches to detecting dependency conflicts are possible. Several implementations disclosed herein use versioning for shared objects (i.e., the versioned data objects, or VDOs, maintained by the storage nodes) such that, at any given time, there is only one committed version of each VDO, although there may be multiple visible but uncommitted versions of the VDO stored throughout the system (namely in compute nodes' caches) corresponding to updates made by transactions (and tasks) that have not yet been committed. In such implementations, versions can improve the effectiveness of speculation by helping maintain several possible updates to a VDO such that the correct one\u2014whichever one that is\u2014can be written to the VDO when the corresponding transaction commits. There is also a possibility of increasing the likely effectiveness of speculation by intelligently choosing which version a running transaction to utilize. In other words, when a transaction reads a VDO (presumably from the local cache), there may be multiple versions of that VDO available that were written by earlier transactions, and the system may use predetermined policies to intelligently decide which version the transaction to use in order to maximize its chances for success and minimize its risk of being aborted. However, there is a tradeoff between the efficiency gains from reducing unnecessary aborts versus the overhead incurred for making the reductions.","Certain implementations may also maintain, for each uncommitted transaction, (a) a ReadSet describing the set of VDOs read by the transaction and (b) a WriteSet containing the objects updated by the transaction. Although each committed VDO has a version number stored along with the object, when a transaction updates that VDO and adds it to its WriteSet the updated VDO initially has an undefined version number. Then, when the transaction is committed, the corresponding ReadSet and WriteSet are submitted to the transactional memory implementation. If, for every VDO in the transaction's ReadSet, the version number in the ReadSet is the same as the version number of the committed VDO still stored in the storage node, then the transaction succeeds and the VDOs in that transaction's WriteSet are atomically committed to the storage nodes (effectively overwriting the old version of the VDOs) and, during the commit, each new VDO (from the transaction's WriteSet) is allocated a new version number that is incremented from the previous VDO's version number. In addition, the committed version number for an updated VDO (the one just committed) is also stored into the ReadSet of any uncommitted transaction which referenced the updated VDO before it was committed, replacing the previously undefined version number value with the newly determined version number value. This in turn ensures that, since no transaction can commit before all of its predecessors in the dependency graph have committed, at commit time every VDO in the transaction's ReadSet will have a version number.","Of course, a transaction commit may fail due to a conflict that was not present in the dependency graph, such as when two transactions on different compute nodes read the same version of a VDO and then both later attempt to update the VDO such that the first transaction to do so will succeed but the second one will not (and will be re-executed using the new, updated version of the VDO provided by the first transaction). Yet this approach can detect these conflicts and correct them accordingly.","As previously discussed with regard to , several implementations disclosed herein run on a cluster of server computers and comprise a set of storage nodes (SN), a set of compute nodes (CN), and an ordering node (ON) that may be centralized in some implementations or distributed among the CNs (for example) in other implementations. Both SNs and CNs may be processes, in which case each computer may run more than one node; however, a typical deployment in several implementations may be to allocate an SN and a CN to every computer in the cluster, and then coordinate task scheduling and transaction commit ordering using the ON.","The shared objects (VDOs) are stored on the SNs, and each VDO may be assigned to a single SN at allocation time. The location of a VDO may be fixed throughout the program execution for certain implementations, while in other implementations the VDO might migrate between SNs. SNs store VDOs (the object's data and its version number) and may participate in a distributed transactional commit protocol, but do not directly compute or speculate as CNs do.","In contrast, a CN may (a) store a portion of the task graph (in distributed implementations) and execute transactions for the subset of the task graph that it stores; (b) maintain a local cache of committed and uncommitted VDOs; and (c) maintain in the local cache certain data structures to track subsets of the dependency graph.","The ON may store, for various implementations, the connectivity information for the entire task graph, as well as each task's TID, generation number, ancestor vector, and the CN that task is assigned to, as well as pointers to the task's immediate predecessors and successors in the task graph. Using the ON (centralized or distributed), if a first transaction executing a first task a completed at a CN forks a set of subsequent tasks and then the first transaction and first task are subsequently aborted, then the subsequent tasks (along with any tasks recursively forked by them) are aborted and removed from the task graph. The ON keeps state for each task and tracks if the task is Uncommitted, Committable, or Committed, and any task in the task graph whose predecessors are all Committed becomes eligible for commit and is marked Committable by the ON and the ON informs the CN hosting a transaction corresponding to the task. Conversely, whenever a CN successfully commits a transaction it informs the ON which marks the task corresponding to that transaction as Committed which, in turn, may then cause its immediate successor tasks to become Committable.","In numerous implementations, each CN may cache multiple versions of a VDO which may include the most recent known committed version of the VDO as well as versions contained in the WriteSets of uncommitted transactions at CN. These versions of a VDO may be maintained in a sorted list along with an auxiliary graph. (Both the sorted list and the auxiliary graph are local to the CN and each CN keeps its own independent data structures. When a transaction reads a VDO, the CN inspects the VDO's version list and, if empty, the CN fetches (reads) the VDO from the SN (which is the most recently committed version of the VDO), copies it to the local cache, and lists it in the version list. Otherwise, the CN locates an appropriate version of the VDO from the list. When a transaction writes (updates) a VDO, the VDO is buffered in the local cache until the transaction completes, at which point each VDO in the transaction's WriteSet is inserted into that VDO's sorted list.","As previously mentioned, a transaction may commit only if the ON has told the CN that the transaction's task is Committable. Moreover, a transaction may commit only if (a) it is the first transaction in the value-list for every object in the transaction's WriteSet, and (b) all transactions that wrote versions in the transactions ReadSet have committed. At this point, the CN may perform a two-phase distributed commit that includes all the SNs that store any VDO in either the transaction's ReadSet or WriteSet or both. In the first phase, the OID and version number of every object in the ReadSet, and the OID of every object in the WriteSet, is sent to the SN storing that object. The SNs acquire locks on the object (in OID order to prevent deadlock) and return success if objects in the ReadSet used their respective most recent committed versions, and return failure otherwise (which may lead to an abort in some implementations). If all SNs return success for the locks, then the second phase commits the new versions of the objects (VDOs) in the WriteSet and releases the locks. The CN can then commit the transaction.",{"@attributes":{"id":"p-0070","num":"0069"},"figref":"FIG. 5"},"Computer-executable instructions, such as program modules, being executed by a computer may be used. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. Distributed computing environments may be used where tasks are performed by remote processing devices that are linked through a communications network or other data transmission medium. In a distributed computing environment, program modules and other data may be located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing aspects described herein includes a computing device, such as computing device . In its most basic configuration, computing device  typically includes at least one processing unit  and memory . Depending on the exact configuration and type of computing device, memory  may be volatile (such as random access memory (RAM)), non-volatile (such as read-only memory (ROM), flash memory, etc.), or some combination of the two. This most basic configuration is illustrated in  by dashed line .","Computing device  may have additional features\/functionality. For example, computing device  may include additional storage (removable and\/or non-removable) including, but not limited to, magnetic or optical disks or tape. Such additional storage is illustrated in  by removable storage  and non-removable storage .","Computing device  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by device  and includes both volatile and non-volatile media, removable and non-removable media.","Computer storage media include volatile and non-volatile, and removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Memory , removable storage , and non-removable storage  are all examples of computer storage media. Computer storage media include, but are not limited to, RAM, ROM, electrically erasable program read-only memory (EEPROM), flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computing device . Any such computer storage media may be part of computing device .","Computing device  may contain communications connection(s)  that allow the device to communicate with other devices. Computing device  may also have input device(s)  such as a keyboard, mouse, pen, voice input device, touch input device, etc. Output device(s)  such as a display, speakers, printer, etc. may also be included. All these devices are well known in the art and need not be discussed at length here.","It should be understood that the various techniques described herein may be implemented in connection with hardware or software or, where appropriate, with a combination of both. Thus, the methods and apparatus of the presently disclosed subject matter, or certain aspects or portions thereof, may take the form of program code (i.e., instructions) embodied in tangible media, such as floppy diskettes, CD-ROMs, hard drives, or any other machine-readable storage medium where, when the program code is loaded into and executed by a machine, such as a computer, the machine becomes an apparatus for practicing the presently disclosed subject matter.","Although exemplary implementations may refer to utilizing aspects of the presently disclosed subject matter in the context of one or more stand-alone computer systems, the subject matter is not so limited, but rather may be implemented in connection with any computing environment, such as a network or distributed computing environment. Still further, aspects of the presently disclosed subject matter may be implemented in or across a plurality of processing chips or devices, and storage may similarly be effected across a plurality of devices. Such devices might include personal computers, network servers, and handheld devices, for example.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["To facilitate an understanding of and for the purpose of illustrating the present disclosure and various implementations, exemplary features and implementations are disclosed in, and are better understood when read in conjunction with, the accompanying drawings\u2014it being understood, however, that the present disclosure is not limited to the specific methods, precise arrangements, and instrumentalities disclosed. Similar reference characters denote similar elements throughout the several views. In the drawings:",{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
