---
title: Hybrid traffic system and associated method
abstract: A traffic sensing system for sensing traffic at a roadway includes a first sensor having a first field of view, a second sensor having a second field of view, and a controller. The first and second fields of view at least partially overlap in a common field of view over a portion of the roadway, and the first sensor and the second sensor provide different sensing modalities. The controller is configured to select a sensor data stream for at least a portion of the common field of view from the first and/or second sensor as a function of operating conditions at the roadway.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08849554&OS=08849554&RS=08849554
owner: Image Sensing Systems, Inc.
number: 08849554
owner_city: St. Paul
owner_country: US
publication_date: 20111115
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The present invention relates generally to traffic sensor systems and to methods of configuring and operating traffic sensor systems.","It is frequently desirable to monitor traffic on roadways and to enable intelligent transportation system controls. For instance, traffic monitoring allows for enhanced control of traffic signals, speed sensing, detection of incidents (e.g., vehicular accidents) and congestion, collection of vehicle count data, flow monitoring, and numerous other objectives.","Existing traffic detection systems are available in various forms, utilizing a variety of different sensors to gather traffic data. Inductive loop systems are known that utilize a sensor installed under pavement within a given roadway. However, those inductive loop sensors are relatively expensive to install, replace and repair because of the associated road work required to access sensors located under pavement, not to mention lane closures and traffic disruptions associated with such road work. Other types of sensors, such as machine vision and radar sensors are also used. These different types of sensors each have their own particular advantages and disadvantages.","It is desired to provide an alternative traffic sensing system. More particularly, it is desired to provide a traffic sensing system that allows for the use of multiple sensing modalities to be configured such that the strengths of one modality can help mitigate or overcome the weaknesses of the other.","In one aspect, a traffic sensing system for sensing traffic at a roadway according to the present invention includes a first sensor having a first field of view, a second sensor having a second field of view, and a controller. The first and second fields of view at least partially overlap in a common field of view over a portion of the roadway, and the first sensor and the second sensor provide different sensing modalities. The controller is configured to select a sensor data stream for at least a portion of the common field of view from the first and\/or second sensor as a function of operating conditions at the roadway.","In another aspect, a method of normalizing overlapping fields of view of a traffic sensor system for sensing traffic at a roadway according to the present invention includes positioning a first synthetic target generator device on or near the roadway, sensing roadway data with a first sensor having a first sensor coordinate system, sensing roadway data with a second sensor having a second sensor coordinate system, detecting a location of the first synthetic target generator device in the first sensor coordinate system with the first sensor, displaying sensor output of the second sensor, selecting a location of the first synthetic target generator device on the display in the second sensor coordinate system, and correlating the first and second coordinate systems as a function of the locations of the first synthetic target generator device in the first and second sensor coordinate systems. The sensed roadway data of the first and second sensors overlap in a first roadway area, and the first synthetic target generator is positioned in the first roadway area.","Other aspects of the present invention will be appreciated in view of the detailed description that follows.","While the above-identified drawing figures set forth embodiments of the invention, other embodiments are also contemplated, as noted in the discussion. In all cases, this disclosure presents the invention by way of representation and not limitation. It should be understood that numerous other modifications and embodiments can be devised by those skilled in the art, which fall within the scope and spirit of the principles of the invention. The figures may not be drawn to scale, and applications and embodiments of the present invention may include features and components not specifically shown in the drawings.","In general, the present invention provides a traffic sensing system that includes multiple sensing modalities, as well as an associated method for normalizing overlapping sensor fields of view and operating the traffic sensing system. The system can be installed at a roadway, such as at a roadway intersection, and can work in conjunction with traffic control systems. Traffic sensing systems can incorporate radar sensors, machine vision sensors, etc. The present invention provides a hybrid sensing system that includes different types of sensing modalities (i.e., different sensor types) with at least partially overlapping fields of view that can each be selectively used for traffic sensing under particular circumstances. These different sensing modalities can be switched as a function of operating conditions. For instance, machine vision sensing can be used during clear daytime conditions and radar sensing can be used instead during nighttime conditions. In various embodiments, switching can be implemented across an entire field of view for given sensors, or can alternatively be implemented for one or more subsections of a given sensor field of view (e.g., to provide switching for one or more discrete detector zones established within a field of view). Such a sensor switching approach is generally distinguishable from data fusion. Alternatively, different sensing modalities can work simultaneously or in conjunction as desired for certain circumstances. The use of multiple sensors in a given traffic sensing system presents numerous challenges, such as the need to correlate sensed data from the various sensors such that detections with any sensing modality are consistent with respect to real-world objects and locations in the spatial domain. Furthermore, sensor switching requires appropriate algorithms or rules to guide the appropriate sensor selection as a function of given operating conditions. In operation, traffic sensing allows for the detection of objects in a given field of view, which allows for traffic signal control, data collection, warnings, and other useful work. This application claims priority to U.S. Provisional Patent Application Ser. No. 61\/413,764, entitled \u201cAutoscope Hybrid Detection System,\u201d filed Nov. 15, 2010, which is hereby incorporated by reference in its entirety.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 1","b":["30","32","32","34","36","34","34","34","34","30","34","38","38","30","38","38","34"],"sub":["1 ","S ","R ","L1","L2","L3 ","L4 ","A"]},"It should be noted that while  specifically identifies elements of the intersection  and the traffic sensing system  for a single direction of approach, a typical application will involve multiple sensor assemblies , with at least one sensor assembly  for each direction of approach for which it is desired to sense traffic data. For example, in a conventional four-way intersection, four sensor assemblies  can be provided. At a T-shaped, three-way intersection, three sensor assemblies  can be provided. The precise number of sensor assemblies  can vary as desired, and will frequently be influenced by roadway configuration and desired traffic sensing objectives. Moreover, the present invention is useful for applications other than strictly intersections. Other suitable applications include use at tunnels, bridges, toll stations, access-controlled facilities, highways, etc.","The hybrid sensor assembly  can include a plurality of discrete sensors, which can provide different sensing modalities. The number of discrete sensors can vary as desired for particular applications, as can the modalities of each of the sensors. Machine vision, radar (e.g., Doppler radar), LIDAR, acoustic, and other suitable types of sensors can be used.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 2","b":["30","34","1","34","2","34","3","34","34","1","34","2","34","1","34","3","34","2","34","1","34","2","34","3","34","1","34","2","34","3","34","1","34","2","34","3","34","1","34","2","34","3"],"sub":["1 ","2 "]},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 3","b":["34","32","40","42","40","40","42","40","42","44","40","42","36","40","42","40","42","40","42","46","40","42","40","42","46","40","42","40","34"],"sub":["1 ","2 ","1 ","2 "]},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 4A","b":["34","40","50","52","54","50","52","54","54","56","58","40"]},"Furthermore, in the illustrated embodiment, the second sensor  is a machine vision device and includes a vision sensor (e.g., CCD or CMOS array) , an A\/D converter , and a DSP . Output from the vision sensor  is sent to the A\/D converter , which sends a digital signal to the DSP . The DSP  communicates with the processor (CPU) , which in turn is connected to the I\/O mechanism .",{"@attributes":{"id":"p-0039","num":"0038"},"figref":["FIG. 4B","FIG. 4B","FIG. 4A"],"b":["34","52","62","54","64","56","40","42","52","62","54","64","56","34"]},"Internal sensor algorithms can be the same or similar to those for known traffic sensors, with any desired modifications of additions, such as queue detection and turning movement detection algorithms that can be implemented with a hybrid detection module (HDM) described further below.","It should be noted that the embodiment illustrated in  is shown merely by way of example, and not limitation. In further embodiments, other types of sensors can be utilized, such as LIDAR, etc. Moreover, more than two sensors can be used, as desired for particular applications.","In a typical installation, the hybrid sensor assembly  is operatively connected to additional components, such as one or more controller or interfaces boxes and a traffic controller (e.g., traffic signal system).  is a schematic block diagram of one embodiment of the traffic sensing system , which includes four hybrid sensor assemblies A-D, a bus , a hybrid interface panel box , and a hybrid traffic detection system box . The bus  is operatively connected to each of the hybrid sensor assemblies A-D, and allows transmission of power, video and data. Also connected to the bus  is the hybrid interface panel box . A zoom controller box  and a display  are connected to the hybrid interface panel box  in the illustrated embodiment. The zoom controller box  allows for control of zoom of machine vision sensors of the hybrid sensor assemblies A-D. The display  allows for viewing of video output (e.g., analog video output). A power supply  is further connected to the hybrid interface panel box , and a terminal  (e.g., laptop computer) can be interfaced with the hybrid interface panel box . The hybrid interface panel box  can accept 110\/220 VAC power and provides 24 VDC power to the sensor assemblies A-D. Key functions of the hybrid interface panel box  are to deliver power to the hybrid sensor assemblies A-D and to manage communications between the hybrid sensor assemblies A-D and other components like the hybrid traffic detection system box . The hybrid interface panel box  can include suitable circuitry, processors, computer-readable memory, etc. to accomplish those tasks and to run applicable software. The terminal  allows an operator or technician to access and interface with the hybrid interface panel box  and the hybrid sensor assemblies A-D to perform set-up, configuration, adjustment, maintenance, monitoring and other similar tasks. A suitable operating system, such as WINDOWS from Microsoft Corporation, Redmond, Wash., can be used with the terminal . The terminal  can be located at the roadway intersection , or can be located remotely from the roadway  and connected to the hybrid interface panel box  by a suitable connection, such as via Ethernet, a private network or other suitable communication link. The hybrid traffic detection system box  in the illustrated embodiment is further connected to a traffic controller , such as a traffic signal system that can be used to control traffic at the intersection . The hybrid detection system box  can include suitable circuitry, processors, computer-readable memory, etc. to run applicable software, which is discussed further below. In some embodiments, the hybrid detection system box  includes one or more hot-swappable circuitry cards, with each card providing processing support for a given one of the hybrid sensor assemblies A-D. In further embodiments, the traffic controller  can be omitted. One or more additional sensors  can optionally be provided, such as a rain\/humidity sensor, or can be omitted in other embodiments. It should be noted that the illustrated embodiment of  is shown merely by way of example. Alternative implementations are possible, such as with further bus integration or with additional components not specifically shown. For example, an Internet connection that enables access to third-party data, such as weather information, etc., can be provided.",{"@attributes":{"id":"p-0043","num":"0042"},"figref":["FIG. 5B","FIG. 5B","FIG. 5A"],"b":["32","32","32","32","88","74","76","88","34","86"]},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 6","b":["32","32","90","1","90","92","94","96","98","90","1","90","40","42","30","40","42","94","96","90","1","90","92","90","1","90","94","96","90","1","90","94","96","84","80","32","32","100","90","1","90","102","104","106","90","1","90","102","102","94","96","92","90","1","90","76","32","88","32"],"i":["n ","n ","n ","n ","n ","n ","n ","n "]},"The radar and video subsystems  and  process and control the collection of sensor data, and transmit outputs to the HDSM . The video subsystem  (utilizing appropriate processor(s) or other hardware) can analyze video or other image data to provide a set of detector outputs, according to the user's detector configuration created using the detector editor  and saved as a detector file. This detector file is then executed to process the input video and generate output data which is then transferred to the associated HDM - to -for processing and final detection selection. Some detectors, such as queue size detector and detection of turning movements, may require additional sensor information (e.g., radar data) and thus can be implemented in the HDM - to -where such additional data is available.","The radar subsystem  can provide data to the associated HDMs - to -in the form of object lists, which provide speed, position, and size of all objects (vehicles, pedestrians, etc.) sensed\/tracked. Typically, the radar has no ability to configure and run machine vision-style detectors, so the detector logic must generally be implemented in the HDMs - to -. Radar-based detector logic in the HDMs - to -can normalize sensed\/tracked objects to the same spatial coordinate system as other sensors, such as machine vision devices. The system  or \u2032 can use the normalized object data, along with detector boundaries obtained from a machine vision (or other) detector file to generate detector outputs analogous to what a machine vision system provides.","The state block  provides indication and output relative to the state of the traffic controller , such as to indicate if a given traffic signal is \u201cgreen\u201d, \u201cred\u201d, etc.","The hybrid GUI  allows an operator to interact with the system  or \u2032, and provides a computer interface, such as for sensor normalization, detection domain setting, and data streaming and collection to enable performance visualization and evaluation. The configuration wizard  can include features for initial set-up of the system and related functions. The detector editor  allows for configuration of detection zones and related detection management functions. The GUI , configuration wizard  and detector editor  can be accessible via the terminal  or a similar computer operatively connected to the system . It should be noted that while various software modules and components have been described separately, it should be noted that these functions can be integrated into a single program or software suite, or provided as separate stand-alone packages. The disclosed functions can be implemented via any suitable software in further embodiments.","The GUI  software can run on a Windows\u00ae PC, Apple PC or Linux PC, or other suitable computing device with a suitable operating system, and can utilize Ethernet or other suitable communication protocols to communicate with the HDMs - to -. The GUI  provides a mechanism for setting up the HDMs - to -, including the video and the radar subsystems  and  to: (1) normalize\/align fields of view from both the first and second sensors  and ; (2) configure parameters for the HDSM  to combine video and radar data; (3) enable visual evaluation of detection performance (overlay on video display); and (4) allow collection of data, both standard detection output and development data. A hybrid video player of the GUI  will allow users to overlay radar-tracking markers (or markers from any other sensing modality) onto video from a machine vision sensor (see ). These tracking markers can show regions where the radar is currently detecting vehicles. This video overlay is useful to verify that the radar is properly configured, as well as to enable users to easily evaluate the radar's performance in real-time. The hybrid video player of the GUI  can allow a user to select from multiple display modes, such as: (1) Hybrid\u2014shows current state of the detectors determined from hybrid decision logic using both the machine vision and radar sensor inputs; (2) Video\/Vision\u2014shows current state of the detectors using only machine vision input; (3) Radar\u2014shows current state of the detectors using only radar sensor input; and\/or (4) Video\/Radar Comparison\u2014provides a simple way to visually compare the performance of machine vision and radar, using a multi-color scheme (e.g., black, blue, red and green) to show all of the permutations of when the two devices agree and disagree for a given detection zone. In some embodiments, only some of the display modes described above can be made available to users.","The GUI  communicates with the HDMs - to -via an API, namely additions to a client application programming interface (CLAPI), which can go through the comserver , and eventually to the HDMs - to -. An applicable communications protocol can send and receive normalization information, detector output definitions, configuration data, and other information to support the GUI .","Functionality for interpreting, analyzing and making final detections or other such functions of the system are primarily performed by the hybrid detection state machine . The HDSM  can take outputs from detectors, such as machine vision detectors and radar-based detectors, and arbitrates between them to make final detection decisions. For radar data, the HDSM  can, for instance, retrieve speed, size and polar coordinates of target objects (e.g., vehicles) as well as Cartesian coordinates of tracked objects, from the radar subsystem  and the corresponding radar sensors - to -. For machine vision, the HDSM  can retrieve data from the detection state block  and from the video subsystem  and the associated video sensors (e.g., camera) - to -. Video data is available at the end of every video frame processed. The HDSM  can contain and perform sensor algorithm data switching\/fusion\/decision logic\/etc. to process radar and machine vision data. A state machine to determine which detection outcomes can be used, based on input from the radar and machine vision data and post-algorithm decision logic. Priority can be given to the sensor believed to be most accurate for the current conditions (time of day, weather, video contrast level, traffic level, sensor mounting position, etc.).","The state block  can provide final, unified detector outputs to a bus or directly to the traffic controller  through suitable ports (or wirelessly). Polling at regular intervals can be used to provide these detector outputs from the state block . Also, the state block can provide indications of each signal phase (e.g., red, green) of the signal controller  as an input.","Numerous types of detection can be employed. Presence or stop-line detectors identify the presence of a vehicle in the field of view (e.g., at the stop line or stop bar); their high accuracy in determining the presence of vehicles makes them ideal for signal-controlled intersection applications. Count and speed detection (which includes vehicle length and classification) for vehicles passing along the roadway. Crosslane count detectors provide the capability to detect the gaps between vehicles, to aid in accurate counting. The count detectors and speed detectors work in tandem to perform vehicle detection processing (that is, the detectors show whether or not there is a vehicle under the detector and calculate its speed). Secondary detector stations compile traffic volume statistics. Volume is the sum of the vehicles detected during a time interval specified. Vehicle speeds can be reported either in km\/hr or mi\/hr. and can be reported as an integer. Vehicle lengths can be reported in meters or feet. Advanced detection can be provided for the dilemma zone (primarily focusing on presence detection, speed, acceleration and deceleration). The \u201cdilemma zone\u201d is the zone in which drivers must decide to proceed or stop as the traffic control (i.e., traffic signal light) changes from green to amber and then red. Turning movement counts can be provided, with secondary detector stations connected to primary detectors to compile traffic volume statistics. Volume is the sum of the vehicles detected during a time interval specified. Turning movement counts are simply counts of vehicles making turns at the intersection (not proceeding straight through the intersection). Specifically, left turning counts and right turning counts can be provided separately. Often, traffic in the same lane may either proceed straight through or turn and this dual lane capability must be taken into account. Queue size measurement can also be provided. The queue size can be defined as the objects stopped or moving below a user-defined speed (e.g., a default 5 ml\/hr threshold) at the intersection approach; thus, the queue size can be the number of vehicles in the queue. Alternately, the queue size can be measured from the stop bar to the end of the upstream queue or end of the furthest detection zone, whichever is shortest. Vehicles can be detected as they approach and enter the queue, with continuous accounting of the number of vehicles in the region defined by the stop line extending to the back of the queue tail.","Handling of errors is also provided, including handling of communication, software errors and hardware errors. Regarding potential communication errors, outputs can be set to place a call to fail safe in the following conditions: (i) for failure of communications between hardware circuitry and the associated radar sensors (e.g., first sensors ) and only outputs associated with that radar sensor, the machine vision outputs (e.g., second sensors ) can be used instead, if operating properly; (ii) for loss of a machine vision output and only outputs associated with that machine vision sensor; and (iii) for loss of detector port communications\u2014associated outputs will be placed into call or fail safe for the slave unit whose communications is lost. A call is generally an output (e.g., to the traffic controller ) based on a detection (i.e., a given detector triggered \u201con\u201d), and a fail-safe call can default to a state that corresponds to a detection, which generally reduces the likelihood of a driver being \u201cstranded\u201d at an intersection because of a lack of detection. Regarding potential software errors, outputs can be set to place call to fail safe if the HDM software - to -is not operational. Regarding potential hardware errors, selected outputs can be set to place call (sink current), or fail safe, in the following conditions: (i) loss of power, all outputs; (ii) failure of control circuitry, all outputs; and (iii) failure of any sensors of the sensor assemblies A-D, only outputs associated with failed sensors.","Although the makeup of software for the traffic sensing system  or \u2032 has been described above, it should be understood that various other features not specifically discussed can be incorporated as desired for particular applications. For example, known features of the Autoscope\u00ae system and RTMS\u00ae system, both available from Image Sensing Systems, Inc., St. Paul, Minn., can be incorporated. For instance, such known functionality can include: (a) a health monitor\u2014monitors the system to ensure everything is running properly; (b) a logging system\u2014logs all significant events for troubleshooting and servicing; (c) detector port messages\u2014for use when attaching a device (slave) for communication with another device (master); detector processing of algorithms\u2014for processing the video images and radar outputs to enable detection and data collection; (d) video streaming\u2014for allowing the user to see an output video feed; (e) writing to non-volatile memory\u2014allows a module to write and read internal non-volatile memory containing a boot loader, operational software, plus additional memory that system devices can write to for data storage; (f) protocol messaging\u2014message\/protocol from outside systems to enable communication with the traffic sensing system  or \u2032; (g) a state block\u2014contains the state of the I\/O; and (h) data collection\u2014for recording I\/O, traffic data, and alarm states.","Now that basic components of the traffic sensing system  and \u2032 have been described, a method of installing and normalizing the system can be discussed. Normalization of overlapping sensor fields of view of a hybrid system is important so that data obtained from different sensors, especially those using different sensing modalities, can be correlated and used in conjunction or interchangeably. Without suitable normalization, use of data from different sensors would produce detections in disparate coordinate systems preventing a unified system detection capability.",{"@attributes":{"id":"p-0057","num":"0056"},"figref":["FIG. 7","FIGS. 2 and 8","FIG. 1"],"b":["32","32","30","100","34","74","76","88","34","34","36","34","102","34"],"sub":["1","S","A","R","L1 ","L2","S "]},"After physical positions have been measured, orientations of the sensor assemblies  and the associated first and second sensors  and  can be determined (step ). This orientation determination can include configuration of azimuth angles \u03b8, elevation angles \u03b8, and rotation angle. The azimuth angle \u03b8 for each discrete sensor  and  of a given hybrid sensor assembly  can be a dependent degree of freedom, i.e., azimuth angles \u03b8and \u03b8are identical for the first and second sensors  and , given the mechanical linkage in the preferred embodiment. The second sensor  (e.g., machine vision device) can be configured such that a center of the stop-line for the traffic approach  substantially aligns with a center of the associated field of view -. Given the mechanical connection between the first and second sensors  and  in a preferred embodiment, one then knows that alignment of the first sensor  (e.g., a bore sight of a radar) has been properly set. The elevation angle \u03b2 for each sensor  and  is an independent degree of freedom for the hybrid sensor assembly , meaning the elevation angle \u03b2of the first sensor  (e.g., radar) can be adjusted independently of the elevation angle \u03b2of the second sensor  (e.g., machine vision device).","Once sensor orientation is known, the coordinates of that sensor can be rotated by the azimuth angle \u03b8 so that axes align substantially parallel and perpendicular to a traffic direction of the approach . Adjustment can be made according to the following equations (1) and (2), where sensor data is provided in x, y Cartesian coordinates:\n\n\u2032=cos(\u03b8)*\u2212sin(\u03b8)*\u2003\u2003(1)\n\n\u2032=sin(\u03b8)*+cos(\u03b8)*\u2003\u2003(2)\n","Also a second transformation can be used to harmonize axis-labeling conventions of the first and second sensors  and , according to equations (3) and (4):\n\n\u2003\u2003(3)\n\n\u2003\u2003(4)\n","A normalization application (e.g., the GUI  and\/or the configuration wizard ) can then be opened to begin field of view normalization for the first and second sensors  and  of each hybrid sensor assembly  (step ). With the normalization application open, objects are positioned on or near the roadway of interest (e.g., roadway intersection ) in a common field of view of at least two sensors of a given hybrid sensor assembly  (step ). In one embodiment, the objects can be synthetic target generators, which, generally speaking, are objects or devices capable of generating a recordable sensor signal. For example, in one embodiment a synthetic target generator can be a Doppler generator that can generate a radar signature (Doppler effect) while stationary along the roadway  (i.e., not moving over the roadway ). In an alternative embodiment using an infrared (IR) sensor, synthetic target generator can be a heating element. Multiple objects can be positioned simultaneously, or alternatively one or more objects can be sequentially positioned, as desired. The objects can be positioned on the roadway in a path of traffic or on a sidewalk, boulevard, curtilage or other adjacent area. Generally at least three objects are positioned in a non-collinear arrangement. In applications where the hybrid sensor assembly  includes three or more discrete sensors, the objects can be positioned in an overlapping field of view of all of the discrete sensors, or of only a subset of the sensors at a given time, though eventually an objects should be positioned within the field of view of each of the sensors of the assembly . Objects can be temporarily held in place manually by an operator, or can be self-supporting without operator presence. In still further embodiments, the objects can be existing objects positioned at the roadway , such as posts, mailboxes, buildings, etc.","With the object(s) positioned, data is recorded for multiple sensors of the hybrid sensor assembly  being normalized, to capture data that includes the positioned objects in the overlapping field of view, that is, multiple sensors sense the object(s) on the roadway within the overlapping fields of view (step ). This process can involve simultaneous sensing of multiple objects, or sequential recording of one or more objects in different locations (assuming no intervening adjustment or repositioning of the sensors of the hybrid sensor assembly  being normalized). After data is captured, an operator can use the GUI  to select one or more frames of data recorded from the second sensor  (e.g., machine vision device) of the hybrid sensor assembly  being normalized that provide at least three non-collinear points that correspond to the locations of the positioned objects in the overlapping field of view of the roadway , and selects those points in the one or more selected frames to identify the objects' locations in a coordinate system for the second sensor  (step ). Selecting the points in the frame(s) from the second sensor  can be done manually, through a visual assessment by the operator and actuation of an input device (e.g., mouse-click, touch screen contact, etc.) to designate the location of the objects in the frame(s). In an alternate embodiment, a distinctive visual marking can be provided to attached to the object(s) and the GUI  can automatically or semi-automatically search through frames to identify and select the location of the markers and therefore also the object(s). The system  or \u2032 can record the selection in the coordinate system associated with second sensor , such as pixel location for output of a machine vision device. The system  or \u2032 can also perform an automatic recognition of the objects relative to another coordinate system associated with the first sensor , such as in polar coordinates for output of a radar. The operator can select the coordinates of the coordinate system of the first sensor  from an object list (due to the possibility that other objects may be sensed on the roadway  in addition to the object(s)), or alternatively automated filtering could be performed to select the appropriate coordinates. The selected coordinates of the first sensor  can be adjusted (e.g., rotated) in accordance with the orientation determination of step  described above. The location selection process can be repeated for all applicable sensors of a given hybrid sensor assembly  until locations of the same object(s) have been selected in the respective coordinate systems for each of the sensors.","After points corresponding to the locations of the objects have been selected in each sensor coordinate system, those points are translated or correlated to common coordinates used to normalize and configure the traffic sensing system  or \u2032 (step ). For instance, radar polar coordinates can be mapped, translated or correlated to pixel coordinates of a machine vision device. In this way, a correlation between data of all of the sensors of a given hybrid sensor assembly , so that objections in a common, overlapping field of view of those sensors can be identified in a common coordinate system, or alternatively in a primary coordinate system and mapped into any other correlated coordinate systems for other sensors. In one embodiment, all sensors can be correlated to a common pixel coordinate system.","Next, a verification process can be performed, through operation of the system  or \u2032 and observation of moving objects traveling through the common, overlapping field of view of the sensors of the hybrid sensor assembly  being normalized (step ). This is a check on the normalization already performed, and an operator can adjust or clear and perform again the previous steps to obtain a more desired normalization.","After normalization of the sensor assembly , an operator can use the GUI  to identify one or more lanes of traffic for one or more approaches  on the roadway  in the common coordinate system (or in one coordinate system correlated to other coordinate systems) (step ). Lane identification can be performed manually by an operator drawing lane boundaries on a display of sensor data (e.g., using a machine vision frame or frames depicting the roadway ). Physical measurements (from step ) can be used to assist the identification of lanes. In alternative embodiments automated methods can be used to identify and\/or adjust lane identifications.","Additionally, an operator can use the GUI  and\/or the detection editor  to establish one or more detection zones (step ). The operator can draw the detection zones on a display of the roadway . Physical measurements (from step ) can be used to assist the establishment of detection zones.","The method illustrated in  is shown merely by way of example. Those of ordinary skill in the art will appreciate that the method can be performed in conjunction with other steps not specifically shown or discussed above. Moreover, the order of particular steps can vary, or can be performed simultaneously, in further embodiments. Further details of the method shown in  will be better understood in relation to additional figures described below.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":["FIG. 8","FIG. 8"],"b":["30","34","40","40","34","1","130","34","40","130","36","30","32","32"],"sub":["S","1 "]},{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 9","b":["140","102","34","142","142","30","142","142","30","130","142","142","38","38"]},"The objects A-F can each be synthetic target generators (e.g., Doppler generators, etc.). In general, synthetic target generators are objects or devices capable of generating a recordable sensor signal, such as a radar signature (Doppler effect) generated while the object is stationary along the roadway  (i.e., not moving over the roadway ). In this way, a stationary object on the roadway  can given the appearance of being a moving object that can be sensed and detected by a radar. For instance, mechanical and electrical Doppler generators are known, and any suitable Doppler generator can be used with the present invention as a synthetic target generator for embodiments utilizing a radar sensor. A mechanical or electro-mechanical Doppler generator can include a spinning fan in a slit enclosure having a slit. An electrical Doppler generator can include a transmitter to transmit an electromagnetic wave to emulate a radar return signal (i.e., emulating a reflected radar wave) from a moving object at a suitable or desired speed. Although a typical radar cannot normally detect stationary objects, a synthetic target generator like a Doppler generator makes such detection possible. For normalization as described above with respect to , stationary objects are much more convenient than moving objects. Alternatively, the objects A-F can be objects that move or are moved relative to the roadway , such as corner reflectors that halp provide radar reflection signatures.","Although six objects A-F are shown in , only a minimum of three non-collinearly positioned objects need to be positioned in other embodiments. Moreover, as noted above, not all of the objects A-F need to be positioned simultaneously.",{"@attributes":{"id":"p-0072","num":"0071"},"figref":["FIG. 10","FIG. 10"],"b":["146","42","148","1","148","2","148","3","102","148","4","148","5"]},"As an alternative to having an operator manually draw the stop line boundary -, an automatic or semi-automatic process can be used in further embodiments. The stop line position is usually difficult to find, because there is only one somewhat noisy indicator: where objects (e.g., vehicles) stop. Objects are not guaranteed to stop exactly on the stop line (as designated on the roadway  by paint, etc.); they could stop up to several meters ahead or behind the designated stop line on the roadway . Also, some sensing modalities, such as radar, can have significant errors in estimating positions of stopped vehicles. Thus, an error of +\/\u2212 several meters can be expected in a stop line estimate. The stop line position can be found automatically or semi-automatically by averaging a position (e.g., a y-axis position) of a nearest stopped object in each measurement\/sensing cycle. Taking only the nearest stopped objects helps eliminate undesired skew caused by non-front objects in queues (i.e., second, third, etc. vehicles in a queue). This dataset will have some outliers, which can be removed using an iterative process (similar to one that can be used in azimuth angle estimates):","(a) Take a middle 50% of samples nearest a stop line position estimate (inliers), and discard the other 50% of points (outliers). An initial stop line position estimate can be an operator's best guess, informed by any available physical measurements, geographic information system (GIS) data, etc.","(b) Determine a mean (average) of the inliers, and consider this mean the new stop line position estimate.","(c) Repeat steps (a) and (b) until method converges (e.g., 0.0001 delta between steps (a) and (b)) a threshold number of iterations of steps (a) and (b) have been reached (e.g., 100 iterations). Typically, method should converge within around 10 iterations. After convergence or reaching the iteration threshold, a final estimate of this the stop line boundary position is obtained. A small offset can be applied, as desired.","It is generally necessary to provide orientation information to the system  or \u2032 to allow suitable recognition of the orientation of the sensors of the hybrid sensor assembly  relative to the roadway  desired to be sensed. Two possible methods for determining orientation angles are illustrated in , B and C.  is a view of a normalization display  for one form of sensor orientation detection and normalization. As shown in the illustrated embodiment of , a radar output (e.g., of the first sensor ) is provided in a first field of view - for four lanes of traffic Lto Lof the roadway . Numerous objects  (e.g., vehicles) are detected in the field of view -, and a movement vector - is provided for each detected object. It should be noted that it is well-known for radar sensor systems to provide vector outputs for detected moving objects. By viewing the display  (e.g., with the GUI ), an operator can adjust an orientation of the first sensor  recognized by the system  or \u2032 such that vectors - substantially align with the lanes of traffic Lto L. Lines designating lanes of traffic Lto Lcan be manually drawn by an operator (see ). This approach assumes that sensed objects travel substantially parallel to lanes of the roadway . Operator skill can account for any outliers or artifacts in data used for this process.",{"@attributes":{"id":"p-0078","num":"0077"},"figref":["FIG. 11B","FIG. 11B"],"b":["150","150","42","154","1","40","150","154","1","154","2","150"]},{"@attributes":{"id":"p-0079","num":"0078"},"figref":["FIG. 11C","FIG. 11C"],"b":["150","150","102","156","40","158","1","158","2","158","3","158","4","158","5","159"]},"Steps of the auto-normalization algorithm can be as described in the following embodiment. The azimuth angle \u03b8 is estimated first. Once the azimuth angle \u03b8 is known, the object coordinates for the associated sensor (e.g., the first sensor ) can be rotated so that axes of the associated coordinate system align parallel and perpendicular to the traffic direction. This azimuth angle \u03b8 simplifies estimation of the stop line and lane boundaries. Next, the sensor coordinates can be rotated as a function of the azimuth angle \u03b8 the user entered as an initial guess. The azimuth angle \u03b8 is computed by finding an average direction of travel of the objects (e.g., vehicles) in the sensor's field of view. It is assumed that on average objects will travel parallel to lane lines. Of course, vehicles executing turning maneuvers or changing lanes will violate this assumption. Those types of vehicles produce outliers in the sample set that must be removed. Several different methods are employed to filter outliers. As an initial filter, all objects with speed less than a given threshold (e.g., approximately 24 km\/hr or 15 ml\/hr) can be removed. Those objects are considered more likely to be turning vehicles or otherwise not traveling parallel to lane lines. Also, any objects with a distance outside of approximately 5 to 35 meters past the stop line are removed; objects in this middle zone are considered the most reliable candidates to be accurately tracked while travelling within the lanes of the roadway . Because the stop line location is not yet known, the operator's guess can be used at this point. Now using this filtered dataset, an angle of travel for each tracked object is computed by taking the arctangent of the associated x and y velocity components. An average angle of all the filtered, tracked objects produces an azimuth angle \u03b8 estimate. However, at this point, outliers could still be skewing the result. A second outlier removal step can now be employed as follows:","(a) Take a middle 50% of samples nearest the azimuth angle \u03b8 estimate (inliers), and discard the other 50% of points (outliers);","(b) Take the mean of the inliers, and consider this the new azimuth angle \u03b8 estimate; and","(c) Repeat steps (a) and (b) until the method converges (e.g., 0.0001 delta between steps (a) and (b)) or a threshold number of iterations of steps (a) and (b) have been reached (e.g., 100 iterations). Typically, this method should converge within around 10 iterations. After converging or reaching the iteration threshold, the final azimuth angle \u03b8 estimate is obtained. This convergence can be graphically represented as a histogram, if desired.",{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIGS. 12A-12E","FIG. 12E","FIG. 12E"],"b":["30","170","32","32"]},{"@attributes":{"id":"p-0085","num":"0084"},"figref":"FIG. 13","b":["180","106","32","32","180","182","30","30","182","184","186","188","180","32","32"]},{"@attributes":{"id":"p-0086","num":"0085"},"figref":["FIG. 14","FIG. 14"],"b":["190","32","32","40","42","184","184","30","192","194","184","184","184","196","38"]},"As already noted, the present invention allows for switching between different sensors or sensing modalities based upon operating conditions at the roadway and\/or type of detection. In one embodiment, the traffic sensing system  or \u2032 can be configured as a gross switching system in which multiple sensors run simultaneously (i.e., operate simultaneously to sense data) but with only one sensor being selected at any given time for detection state analysis. The HDSMs - to -carry out logical operations based on the type of sensor being used, taking into account the type of detection.","One embodiment of a sensor switching approach is summarized in Table 1, which applies to post-processed data from the sensors - to -and - to -from the hybrid sensor assemblies . A final output of any sensor subsystem can simply be passed through on a go\/no-go basis to provide a final detection decision. This is in contrast to a data fusion approach that makes detection decisions based upon fused data from all of the sensors. The inventors have developed rules in Table 1 based on comparative field-testing between machine vision and radar sensing, and discoveries as to beneficial uses and switching logic. All the rules of Table 1 assume use of a radar deployed for detection up to 50 m after (i.e., upstream from) a stop line and then machine vision is relied upon past that 50 m region. Other rules can be applied under different configuration assumptions. For example, with a narrower radar antenna field of view, the radar could be relied upon at relatively longer ranges than machine vision.",{"@attributes":{"id":"p-0089","num":"0088"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["DETECTOR",{}]},{"entry":["TYPE","RULES"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["COUNT","For mast-arm installations, use Machine Vision"]},{"entry":[{},"For luminaire installations, use Radar by default"]},{"entry":[{},"If low contrast, use Radar"]},{"entry":[{},"Use a combination of Machine Vision & Radar to"]},{"entry":[{},"identify and remove outliers"]},{"entry":["SPEED","For dense traffic or congestion, use Machine Vision"]},{"entry":[{},"For low contrast (night-time, snow, fog, etc.),"]},{"entry":[{},"use Radar"]},{"entry":["STOP LINE","By default, use Machine Vision,"]},{"entry":["DETECTOR","EXCEPT:"]},{"entry":[{},"When strong shadows are detected, use Radar"]},{"entry":[{},"For low contrast (nighttime, snow, fog, etc.),"]},{"entry":[{},"use Radar"]},{"entry":["PRESENCE","By default, use the Machine Vision,"]},{"entry":[{},"For Directional, use a combination of Machine"]},{"entry":[{},"Vision & Radar to identify and remove occlusion"]},{"entry":[{},"and\/or cross traffic"]},{"entry":[{},"EXCEPT:"]},{"entry":[{},"When strong shadows are detected, use Radar"]},{"entry":[{},"For low contrast (night-time, snow, fog, etc.),"]},{"entry":[{},"use Radar"]},{"entry":["QUEUE","Use Radar for queues up to 100 m, informed by"]},{"entry":[{},"Machine Vision"]},{"entry":[{},"EXCEPT:"]},{"entry":[{},"For dense traffic or congestion, use Machine Vision"]},{"entry":[{},"When strong shadows are detected, use Radar"]},{"entry":[{},"For low contrast (night-time, snow, fog, etc.),"]},{"entry":[{},"use Radar"]},{"entry":["TURN","Use the Radar"]},{"entry":["MOVEMENT","Optionally use Machine Vision for inside inter-"]},{"entry":[{},"section delayed turns"]},{"entry":["VEHICLE","Use Machine Vision"]},{"entry":["CLASSIFICATION","EXCEPT:"]},{"entry":[{},"For nighttime, low contrast and poor weather"]},{"entry":[{},"conditions, use Radar"]},{"entry":["DIRECTIONAL","Use Radar"]},{"entry":"WARNING"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 15","b":["32","32","34","200","202","202","204","32","32","206","32","32","208","202","210","32","32","212","34","214","216","218"]},"If all of the sensors are working (i.e., none have failed), the system  or \u2032 can enter a hybrid detection mode that can take advantage of sensor data from all sensors (step ). A check of detector distance from the radar sensor (i.e., the hybrid sensor assembly ) is performed (step ). Here, detector distance can refer to a location and distance of a given detector defined within a sensor field of view in relation to a given sensor. If the detector is outside the radar beam, the system  or \u2032 can use only video sensor data for the detector (step ), or if the detector is inside the radar beam then a hybrid detection decision can be made (step ). Time of day is determined (step ). During daytime, a hybrid daytime processing mode (see ) is entered (step ), and during nighttime, a hybrid nighttime processing mode (see ) is entered (step ).","The process described above with respect to  can be performed for each frame analyzed. The system  or \u2032 can return to step  for each new frame of sensor data analyzed. It should be noted that although the disclosed embodiment refers to machine vision (video) and radar sensors, the same method can be applied to systems using other types of sensing modalities. Moreover, those of ordinary skill in the art will appreciate the disclosed method can be extended to systems with more than two sensors. It should further be noted that sensor modality switching can be performed across an entire common, overlapping field of view of the associated sensors, or can be localized for switching of sensor modalities for one or more portions of the common, overlapping field of view. In the latter embodiment, different switching decisions can be made for different portions of the common, overlapping field of view, such as to make different switching decisions for different detector types, different lanes, etc.",{"@attributes":{"id":"p-0093","num":"0092"},"figref":["FIG. 16","FIG. 16","FIG. 15"],"b":["32","32","230"]},"For each new frame (step ), a global contrast detector, which can be a feature of a machine vision system, can be checked (step ). If contrast is poor (i.e., low), then the system  or \u2032 can rely on radar data only for detection (step ). If contrast is good, that is, sufficient for machine vision system performance, then a check is performed for ice and\/or snow buildup on the radar (i.e., radome) (step ). If there is ice or snow buildup, the system  or \u2032 can rely on machine vision data only for detection (step ).","If there is no ice or snow buildup on the radar, then a check can be performed to determine if rain is present (step ). This rain check can utilize input from any available sensor. If no rain is detected, then a check can be performed to determine if shadows are possible or likely (step ). This check can involve a sun angle calculation or use any other suitable method, such as any described below). If shadows are possible, a check is performed to verify if strong shadows are observed (step ). If shadows are not possible or likely, or if no strong shadows are observed, then a check is performed for wet road conditions (step ). If there is no wet road condition, a check can be performed for a lane being susceptible to occlusion (step ). If there is no susceptibility to occlusion, the system  or \u2032 can reply on machine vision data only for detection (step ). In this way, machine vision can act as a default sensing modality for daytime detection. If rain, strong shadows, wet road, or lane occlusion conditions exist, then a check can be performed for traffic density and speed (step ). For slow moving and congested conditions, the system  or \u2032 can rely on machine vision data only (go to step ). For light or moderate traffic density and normal traffic speeds, a hybrid detection decision can be made (step ).",{"@attributes":{"id":"p-0096","num":"0095"},"figref":["FIG. 17","FIG. 17","FIG. 15"],"b":["32","32","232"]},"For each new frame (step ), a check is performed for ice or snow buildup on the radar (i.e., radome) (step ). If ice or snow buildup is present, the system  or \u2032 can rely on machine vision data only for detection (step ). If no ice or snow buildup is present, the system  or \u2032 can rely on the radar for detection (step ). When radar is used for detection, machine vision can be used for validation or other purposes as well in some embodiments, such as to provide more refined switching.","Examples of possible ways to measure various conditions at the roadway  are summarized in Table 2, and are described further below. It should be noted that the examples given in Table 2 and accompanying description generally focus on machine vision and radar sensing modalities, other approaches can be used in conjunction with out types of sensing modalities (LIDAR, etc.), whether explicitly mentioned or not.",{"@attributes":{"id":"p-0099","num":"0098"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 2"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["CONDITION","MEASUREMENT METHOD(S)"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Strong Shadows","Sun angle calculation"]},{"entry":[{},"Image processing"]},{"entry":[{},"Sensing modality count delta"]},{"entry":["Nighttime","Sun angle calculation"]},{"entry":[{},"Time of day"]},{"entry":[{},"Image processing"]},{"entry":["Rain\/wet road","Image processing (rain)"]},{"entry":[{},"Image processing (wet road)"]},{"entry":[{},"Rain signature in radar return"]},{"entry":[{},"Rain\/humidity sensor"]},{"entry":[{},"Weather service link"]},{"entry":["Occlusion","Geometry"]},{"entry":["Low contrast","Machine vision global contrast detector"]},{"entry":["Traffic Density","Vehicle counts"]},{"entry":["Distance","Measurement"]},{"entry":["Speed","Radar speed"]},{"entry":[{},"Machine vision speed detector"]},{"entry":["Sensor Movement","Machine vision movement detector"]},{"entry":[{},"Vehicle track to lane alignment"]},{"entry":["Lane Type","User input"]},{"entry":[{},"Inference from detector layout and\/or configuration"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},"Strong Shadows","A strong shadows condition generally occurs during daytime when the sun is at such an angle that objects (e.g., vehicles) cast dynamic shadows on a roadway extending significantly outside of the object body. Shadow can cause false alarms with machine vision sensors. Also, applying shadow false alarm filters to machine vision systems can have an undesired side effect of causing missed detections of dark objects. Shadows generally produce no performance degradation for radars.","A multitude of methods to detect shadows with machine vision are known, and can be employed in the present context as will be understood by a person of ordinary skill in the art. Candidate techniques include spatial and temporal edge content analysis, uniform biasing of background intensity, and identification of spatially connected inter-lane objects.","One can also exploit information from multiple sensor modalities to identify detection characteristics. Such methods can include analysis of vision versus radar detection reports. If shadow condition is such that vision-based detection results in a high quantity of false detections, an analysis of vision detection to radar detection count differentials can indicate a shadow condition. Presence of shadows can also be predicted through knowledge of a machine vision sensor's compass direction, latitude\/longitude, and date\/time, and use of those inputs in a geometrical calculation to find the sun's angle in the sky and to predict if strong shadows will be observed.","Radar can be used exclusively when strong shadows are present (assuming the presence of shadows can reliably be detected) in a preferred embodiment. Numerous alternative switching mechanisms can be employed for strong shadow handling, in alternative embodiments. For example, a machine vision detection algorithm can instead assign a confidence level indicating the likelihood that a detected object is a shadow or object. Radar can be used as a false alarm filter when video detection has low confidence that the detected object is an object and not a shadow. Alternatively, radar can provide a number of radar targets detected in each detector's detection zone (radar targets are typically instantaneous detections of moving objects, which are clustered over time to form radar objects). A target count is an additional parameter that can be used in the machine vision sensor's shadow processing. In a further alternative embodiment, inter-lane communication can be used, using the assumption is that a shadow must have an associated shadow-casting object nearby. Moreover, in yet another embodiment, if machine vision is known to have a bad background estimate, radar can be used exclusively.","Nighttime","A nighttime condition generally occurs when the sun is sufficiently far below the horizon so that the scene (i.e., roadway area at which traffic is being sensed) becomes dark. For machine vision systems alone, the body of objects (e.g., vehicles) becomes harder to see at nighttime, and primarily just vehicle headlights and headlight reflections on the roadway (headlight splash) stand out to vision detectors. Positive detection generally remains high (unless the vehicle's headlights are off). However, headlight splash often causes an undesirable increase in false alarms and early detector actuations. The presence of nighttime conditions can be predicted through knowledge of the latitude\/longitude and date\/time for the installation location of the system  or \u2032. These inputs can be used in a geometrical calculation to find when the sun drops below a threshold angle relative to a horizon.","Radar can be used exclusively during nighttime, in one embodiment. In an alternative embodiment, radar can be used to detect vehicle arrival, and machine vision can be used to monitor stopped objects, therefore helping to limit false alarms.","Rain\/Wet Road","Rain and wet road conditions generally include periods during rainfall, and after rainfall while the road is still wet. Rain can be categorized by a rate of precipitation. For machine vision systems, rain and wet road conditions cause are typically similar to nighttime conditions: a darkened scene with vehicle headlights on and many light reflections visible on the roadway. In one embodiment, rain\/wet road conditions can be detected based upon analysis of machine vision versus radar detection time, where an increased time difference is an indication that headlight splash is activating machine vision detection early. In an alternative embodiment, a separate rain sensor  (e.g., piezoelectric or other type) is monitored to identify when a rain event has taken place. In still further embodiments, rain can be detected through machine vision processing, by looking for actual raindrops or optical distortions caused by the rain. Wet road can be detected through machine vision processing by measuring the size, intensity, and edge strength of headlight reflections on the roadway (all of these factors should increase while the road is wet). Radar can detect rain by observing changes in the radar signal return (e.g., increased noise, reduced reflection strength from true vehicles). In addition, rain could be identified through receiving local weather data over an Internet, radio or other link.","In a preferred embodiment, when a wet road condition is recognized, the radar detection can be used exclusively. In an alternative embodiment, when rain exceeds a threshold level (e.g., reliability threshold), machine vision can be used exclusively, and when rain is below the threshold level but the road is wet, radar can be weighted more heavily to reduce false alarms, and switching mechanisms described above with respect to nighttime conditions can be used.","Occlusion","Occlusion refers generally to an object (e.g., vehicle) partially or fully blocking a line of sight from a sensor to a farther-away object. Machine vision may be susceptible to occlusion false alarms, and may have problems with occlusions falsely turning on detectors in adjacent lanes. Radar is much less susceptible to occlusion false alarms. Like machine vision, though, radar will likely miss vehicles that are fully or near fully occluded.","The possibility for occlusion can be determined through geometrical reasoning. Positions and angles of detectors, and a sensor's position, height H, and orientation, can be used to assess whether occlusion would be likely. Also, the extent of occlusion can be predicted by assuming an average vehicle size and height.","In one embodiment, radar can be used exclusively in lanes where occlusion is likely. In another embodiment, radar can be used as a false alarm filter when machine vision thinks an occlusion is present. Machine vision can assign occluding-occluded lane pairs, then when machine vision finds a possible occlusion and matching occluding object, the system can check radar to verify whether the radar only detects an object in the occluding lane. Furthermore, in another embodiment, radar can be used to address a problem of cross traffic false alarms for machine vision.","Low Contrast","Low contrast conditions generally exist when there is a lack of strong visual edges in a machine vision image. A low contrast condition can be caused by factors such as fog, haze, smoke, snow, ice, rain, or loss of video signal. Machine vision detectors occasionally lose the ability to detect vehicles in low-contrast conditions. Machine vision systems can have the ability to detect low contrast conditions and force detectors into a failsafe always-on state, though this presents traffic flow inefficiency at an intersection. Radar should be largely unaffected by low-contrast conditions. The only exception for radar low contrast performance is heavy rain or snow, and especially snow buildup on a radome of the radar; the radar may miss objects in those conditions. It is possible to use an external heater to prevent snow buildup on the radome.","Machine vision systems can detect low-contrast conditions by looking for a loss of visibility of strong visual edges in a sensed image, in a known manner. Radar can be relied upon exclusively in low contrast conditions. In certain weather conditions where the radar may not perform adequately, those conditions can be detected and detectors placed in a failsafe state rather than relying on the impaired radar input, in further embodiments.","Sensor Failure","Sensor failure generally refers to a complete dropout of the ability to detect for a machine vision, radar or any other sensing modality. It can also encompass partial sensor failure. A sensor failure condition may occur due to user error, power outage, wiring failure, component failure, interference, software hang-up, physical obstruction of the sensor, or other causes. In many cases, the sensor affected by sensor failure can self-diagnose its own failure and provide an error flag. In other cases, the sensor may appear to be running normally, but produce no reasonable detections. Radar and machine vision detection counts can be compared over time to detect these cases. If one of the sensors has far less detections than the other, that is a warning sign that the sensor with less detections may not be operating properly. If only one sensor fails, the working (i.e., non-failed) sensor can be relied upon exclusively. If both sensors fail, usually nothing can be done with respect to switching, and outputs can be set to a fail-safe, always on, state.","Traffic Density","Traffic density generally refers to the rate of vehicles passing through an intersection or other area where traffic is being sensed. Machine vision detectors are not greatly affected by traffic density. There are an increased number of sources of shadows, headlight splash, or occlusions in high traffic density conditions, which could potentially increase false alarms. However, there is also less practical opportunity for false alarms during high traffic density conditions because detectors are more likely to be occupied by a real object (e.g., vehicle). Radar generally experiences reduced performance in heavy traffic, and is more likely to miss objects in heavy traffic conditions. Traffic density can be measured by common traffic engineering statistics like volume, occupancy, or flow rate. These statistics can easily be derived from radar, video, or other detections. In one embodiment, machine vision can be relied upon exclusively when traffic density exceeds a threshold.","Distance","Distance generally refers to real-world distance from the sensor to the detector (e.g., distance to the stop line D). Machine vision has decent positive detection even at relatively large distances. Maximum machine vision detection range depends on camera angle, lens zoom, and mounting height H, and is limited by low resolution in a far-field range. Machine vision usually cannot reliably measure vehicle distances or speeds in the far-field, though certain types of false alarms actually become less of a problem in the far-field because the viewing angle becomes nearly parallel to the roadway, limiting visibility of optical effects on the roadway. Radar positive detection falls off sharply with distance. The rate of drop-off depends upon the elevation angle \u03b2 and mounting height of the radar sensor. For example, a radar may experience poor positive detection rates at distances significantly below a rated maximum vehicle detection range. The distance of each detector from the sensor can be readily determined through the system's  or \u2032 calibration and normalization data. The system  or \u2032 will know the real-world distance to all corners of the detectors. Machine vision can be relied on exclusively when detectors exceed a maximum threshold distance to the radar. This threshold can be adjusted based on the mounting height H and elevation angle \u03b2 of the radar.","Speed","Speed generally refers to a speed of the object(s) being sensed. Machine vision is not greatly affected by vehicle speed. Radar is more reliable at detecting moving vehicles because it generally relies on the Doppler effect. Radar is usually not capable of detecting slow-moving or stopped objects (below approximately 4 km\/hr or 2.5 ml\/hr). Missing stopped objects is less than optimal, as it could lead an associated traffic controller  to delay switching traffic lights to service a roadway approach , delaying or stranding drivers. Radar provides speed measurements each frame for each sensed\/tracked object. Machine vision can also measure speeds using a known speed detector. Either or both mechanism can be utilized as desired. Machine vision can be used for stopped vehicle detection, and radar can be used for moving vehicle detection. This can limit false alarms for moving vehicles, and limit missed detections of stopped vehicles.","Sensor Movement","Sensor movement refers to physical movement of a traffic sensor. There are two main types of sensor movement: vibrations, which are oscillatory movements, and shifts, which are a long-lasting change in the sensor's position. Movement can be caused by a variety of factors, such as wind, passing traffic, bending or arching of supporting infrastructure, or bumping of the sensor. Machine vision sensor movement can cause misalignment of vision sensors with respect to established (i.e., fixed) detection zones, creating a potential for both false alarms and missed detections. Image stabilization onboard the machine vision camera, or afterwards in the video processing, can be used to lessen the impact of sensor movement. Radar may experience errors in its position estimates of objects when the radar is moved from its original position. This could cause both false alarms and missed detections. Radar may be less affected than machine vision by sensor movements. Machine vision can provide a camera movement detector that detects changes in the camera's position through machine vision processing. Also, or in the alternative, sensor movement of either the radar or machine vision device can be detected by comparing positions of radar-tracked vehicles to the known lane boundaries. If vehicle tracks don't consistently align with the lanes, then it is likely a sensor's position has been disturbed.","If only one sensor has moved, then the other sensor can be used exclusively. Because both sensors are linked to the same enclosure, it is likely both will move simultaneously. In that case, the least affected sensor can be weighted more heavily or even used exclusively. Any estimates of the motion as obtained from machine vision or radar data can be used to determine which sensor is most affected by the movement. Otherwise, radar can be used as the default when significant movement occurs. Alternatively, a motion estimate based on machine vision and radar data can be used to correct the detection results of both sensors, in an attempt to reverse the effects of the motion. For machine vision, this can be done by applying transformations to the image (e.g., translation, rotation, warping). With radar, it can involve transformations to the position estimate of vehicles (e.g., rotation only). Furthermore, if all sensors have moved significantly such that part of the area-of-interest is no longer visible, then affected detectors can be placed in a failsafe state (e.g., a detector turned on by default).","Lane Type","Lane type generally refers to the type of the lane (e.g., thru-lane, turn-lane, or mixed use). Machine vision is usually not greatly affected by the lane type. Radar generally performs better than machine vision for thru-lanes. Lane type can be inferred from phase number or relative position of the lane to other lanes. Lane type can alternatively be explicitly defined by a user during initial system setup. Machine vision can be relied upon more heavily in turn lanes to limit misses of stopped objects waiting to turn. Radar can be relied upon more heavily in thru lanes.","Concluding Summary","The traffic sensing system  can provide improved performance over existing products that rely on video detection or radar alone. Some improvements that can be made possible with a hybrid system include improved traditional vehicle classification accuracy, speed accuracy, stopped vehicle detection, wrong way vehicle detection, vehicle tracking, cost savings, and setup. Also, improved positive detection, decreased false detection is made possible. Vehicle classification is difficult during nighttime and poor weather conditions because machine vision may have difficulty detecting vehicle features; however, radar is unaffected by most of these conditions and thus can generally improve upon basic classification accuracy during such conditions despite known limitations of radar at measuring vehicle length. While one version of speed detector integration improves speed measurement through time of day, distance and other approaches, another syllogism can further improve speed detection accuracy by seeking out a combination process for using multiple modalities (e.g., machine vision and radar) simultaneously. For stopped vehicles, a \u201cdisappearing\u201d vehicle in Doppler radar (even with tracking enabled) often occurs when an object (e.g., vehicle) slows to less than approximately 4 km\/hr. (2.5 ml\/hr.), though integration of machine vision and radar technology can help maintain detection until the object starts moving again and also to provide the ability to detect stopped objects more accurately and quickly. For wrong way objects (e.g., vehicles), the radar can easily determine if an object is traveling the wrong way (i.e., in the wrong direction on a one-way roadway) via Doppler radar, with a small probability of false alarm. Thus, when normal traffic is approaching from, for example, a one-way freeway exit, the system could provide an alert alarm when a driver inadvertently drives the wrong way onto the freeway exit ramp. For vehicle tracking through data fusion, the machine vision or radar outputs are chosen, depending on lighting, weather, shadows, time of day and other factors, enabling the HDM - to -to map coordinates of radar objects into a common reference system (e.g., universal coordinate system), in the form of a post-algorithm decision logic. Increased system integration can help limit cost and improve performance. The cooperation of radar and machine vision while sharing common components such as power supply, I\/O and DSP in further embodiments can help to reduce manufacturing costs further while enabling continued performance improvements. With respect to automatic setup and normalization, the user experience is benefited by a relatively simple and intuitive setup and normalization process.","Any relative terms or terms of degree used herein, such as \u201csubstantially\u201d, \u201capproximately\u201d, \u201cessentially\u201d, \u201cgenerally\u201d and the like, should be interpreted in accordance with and subject to any applicable definitions or limits expressly stated herein. In all instances, any relative terms or terms of degree used herein should be interpreted to broadly encompass any relevant disclosed embodiments as well as such ranges or variations as would be understood by a person of ordinary skill in the art in view of the entirety of the present disclosure, such as to encompass ordinary manufacturing tolerance variations, sensor sensitivity variations, incidental alignment variations, and the like.","While the invention has been described with reference to an exemplary embodiment(s), it will be understood by those skilled in the art that various changes may be made and equivalents may be substituted for elements thereof without departing from the scope of the invention. In addition, many modifications may be made to adapt a particular situation or material to the teachings of the invention without departing from the essential scope thereof. Therefore, it is intended that the invention not be limited to the particular embodiment(s) disclosed, but that the invention will include all embodiments falling within the scope of the appended claims. For example, features of various embodiments disclosed above can be used together in any suitable combination, as desired for particular applications."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4A"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 4B"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 11A"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 11B"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 11C"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIGS. 12A-12E"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 17"}]},"DETDESC":[{},{}]}
