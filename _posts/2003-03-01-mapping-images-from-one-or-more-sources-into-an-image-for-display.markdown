---
title: Mapping images from one or more sources into an image for display
abstract: The present invention provides systems and methods that provide images of an environment to the viewpoint of a display. The systems and methods define a mapping surface at a distance from the image source and display that approximates the environment within the field of view of the image source. The system methods define a model that relates the different geometries of the image source, display, and mapping surface to each other. Using the model and the mapping surface, the systems and methods tile images from the image source, correlate the images to the display, and display the images. In instants where two image sources have overlapping fields of view on the mapping surface, the systems and methods overlap and stitch the images to form a mosaic image. If two overlapping image sources each have images with unique characteristics, the systems and methods fuse the images into a composite image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07619626&OS=07619626&RS=07619626
owner: The Boeing Company
number: 07619626
owner_city: Chicago
owner_country: US
publication_date: 20030301
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["1. Field of Invention","The present invention relates to vision display systems used to provide a user with a visual display of a field of interest, and more particularly to systems and methods that combine images from a plurality of sources to provide a coherent view of a field of interest.","2. Description of Related Art","Despite the advent of many flight navigational aids, one of the most important tools for navigation of aircraft remains visual navigation. Many of today's aircrafts include various safety features such as on board radar, ground proximity warning systems, etc. that provide a pilot with added information about the airspace surrounding the aircraft. These systems are a tremendous resource to aid the pilot in obtaining better situational awareness during flight, by allowing the pilot to further interpret what he or she is visually observing. However, there are instances where these various instruments become the pilot's only resource for information because the pilot's vision is hindered.","Visual hindrances may be due to bad weather, such as fog, snow, or rain, or they may be due to the time of day, such as night, dawn, or dusk. Further, some visual hindrances are due to the field of view limitations of the aircraft itself. Many aircraft cockpits have a field of view that is typically limited to a forward facing area that does not provide the pilot with adequate visualization to the sides and rear of the aircraft and also does not provide adequate vertical visualization above and below the aircraft.","Obstructed vision is an important safety concern in aircraft navigation, and there has been considerable effort devoted to providing systems that increase or enhance a pilot's view from the cockpit. Systems have been developed that include the use of one or more sensors that are located on the aircraft. The sensors are directed toward a selected field of view and provide images to a display system in the cockpit, where they are, in turn, displayed to the pilot. The sensors may be video cameras, infrared cameras, radar, etc. The systems allow the pilot to choose the types of images to view. For example, in nighttime flight or fog conditions, the pilot may opt to view images from the infrared and radar sensors, while under clear conditions, the pilot may use video camera feeds.","One such system is disclosed in U.S. Pat. No. 5,317,394 to Hale et al., which is incorporated herein by reference. In this system, sensors are positioned on the exterior of the aircraft such that adjacent sensors have overlapped fields of view. Images from these various sensors are provided to a display system in the aircraft, where they are displayed to the pilot. The images are displayed in an overlapped configuration so as to provide a composite or mosaic image.","A more advanced system is disclosed in U.S. patent application Ser. No. 09\/608,234, entitled: Exterior Aircraft Vision System Using a Helmet-Mounted Display, which is incorporated herein by reference. The Ser. No. 09\/608,234 application discloses a system that includes a helmet-mounted display for displaying images from various sensors located on the aircraft. Importantly, this system includes a helmet-tracking device that tracks movement of the pilot's head in order to determine the pilot's current line of sight (LOV) and field of view (FOV). Using this directional information, the system retrieves image data from the sensors that represent the field of view in which the pilot is staring and displays this image on the helmet display. The image is updated as the pilot turns his head to different lines of sight.","In general, these and other conventional systems provide fairly accurate visual images to the pilot, and thereby increase flight safety. However, there are some limitations to these systems that can cause the images provided to the pilot to either be less accurate or include anomalies that may distract the pilot's view. For example, one issue relates to the spacing of sensors relative to each other and relative to the pilot's position or in the case of stored synthetic data, the difference in perspective between the synthetic data and the other sensors and pilot's position. This physical distance between the sources relative to each other and the cockpit may cause a skewing of the images provided to the pilot. Specifically, the distance creates a visual skewing in the images referred to as parallax. Parallax is an apparent change in the direction of an object caused by a change in observational position that provides a new line of sight. In these conventional systems, the sensor or sensors and\/or synthetic data sources each have a different line of sight with regard to a scene of view from that of the pilot. As such, when viewing the images from the sensor point of view, the pilot is not provided with an accurate representation of the location of the object relative to his position in the aircraft in their overlapping field of view.","Another issue relates to tiling of several images together to create a composite image. In many conventional systems, images from adjacent cameras are displayed adjacent to each other. The edges of the two images appear as visual seams in the display. These seams disrupt viewing of the composite image and can make it harder for the pilot to view the image as a whole. To correct this problem, some prior art systems overlap the edges of adjacent images in an attempt to blend the images together. While this technique is an improvement over the conventional technique of abutting adjacent images, there may still be perceived discontinuity between the images.","A further issue relates to the limitations of a particular type of sensor to provide the best imaging for a given situation. For example, in twilight conditions, a video camera will still provide a discernable visual image, but the image will be degraded in detail due to the low light or obstructed conditions. Further, an infrared sensor will provide imaging based on heat sensing, but the image from an infrared sensor will not have the benefit of the ambient light still available at twilight. In many conventional systems, the pilot must select between these images, instead having an image available that incorporates the advantages of both sensors. As such, the pilot does not have the best images available for viewing.","The present invention provides systems and methods for use in enhanced vision displays. The systems and methods of the present invention receive image data from a plurality of sensors located at different positions on a vehicle or in an area of interest. The system may also receive synthetic image data from a database source, such as a terrain database. The systems and methods assimilate the images from each sensor\/source into a composite or mosaic image. The systems and methods of the present invention detect the current line of sight of the user of the system. Based on this line of sight (LOS), the systems and methods display and image to the user representing the user's current field of view (FOV).","The systems and methods of the present invention reduce issues with regard to parallax in the displayed images. Specifically, the systems and methods of the present invention create one or more artificial mapping surfaces at a selected distance in front of the vehicle or point of interest. The artificial mapping surface allows the various images from the different sources to be correlated by a common frame of reference, with all of the images from each source being projected onto the mapping surface in the form of a tile that represents the desired surface area coverage. The mapping surface creates a frame of reference for correlating the difference in line of sight to an object of interest between the observer and the sources due to the distance separation between the observer and the sources.","Further, the sources, display, and mapping surface all have different coordinate systems. The systems and methods of the present invention create a common or primary coordinate system and transform the images from each of these components to the common or primary coordinate system. This, in turn, allows the systems and methods of the present invention to correlate the images from the various sources and provide them to the user in the viewer's frame of reference to thereby reduce and possibly eliminate parallax.","In addition to correcting for parallax, the systems and methods of the present invention also provide improved methods for stitching distributed aperture images together into a seamless mosaic. The systems and methods of the present invention overlap the edges of tile images received from the sources of the same type that are taken of adjacent or overlapping fields of view. In the overlapped region between the two tile images, the systems and methods of the present invention define blend zones that consist of inner and outer edges. In the blend zones, the systems and methods of the present invention alter the intensity of individual pixels of each tile image so as to blend the two tile images together at the blend zones. The pixels of the left image in the blend zone are tapered from an intensity value of one (1) for the pixels adjacent to the left edge of the blend zone to an intensity value of zero (0) for the pixels adjacent to the right edge of the blend zone. In an opposite manner, the pixels of the right image in the blend zone are tapered from an intensity value of zero (0) for the pixels adjacent to the left edge of the blend zone to an intensity value of one (1) for the pixels adjacent to the right edge of the blend zone. This alteration of pixel intensity values in the blend zone provides an enhanced stitch between the two images. Top and bottom stitching is performed in the same manner.","The systems and methods of the present invention also provide a method for overlaying or fusing images from different types of sources. Specifically, the system of the present invention may include two different types of sources having either the same or overlapped fields of view. The sources provide different images of the same field of view; each source having associated advantages and disadvantages. For example, one source could be a video camera that provides images that may be affected by the amount of light or visibility and the other source may be an infrared sensor that provides images based on heat sensing. The systems and methods of the present invention provide a method for overlaying or fusing the images from these sources together to provide an enhanced image.","The systems and methods include two alternate methods for fusing the images. In the first alternative, the systems and methods of the present invention assign a percentage value to each tile image with regard to intensity. For example, one tile image may be defined with a 30% intensity and the other a 70% intensity. The images are then summed together in this intensity percentage ratio, thereby providing the user with the benefit of both images. In the second alterative, each tile image is displayed based on its content. For example, given a visible and an infrared image covering similar fields of view, the images can be combined at pixel level, where priority can be given to the infrared image based upon its pixel intensity. In this case, if the infrared pixel is at 75% of maximum, then the resulting pixel would be composed from 75% of the IR pixel intensity and 25% visible pixel intensity.","The systems and methods of the present invention further include methods for reducing the number of anomalies in a given displayed image. The systems and methods of the present invention evaluate the intensity value associated with each pixel in a tile image. If a pixel or a series of pixels have intensity values that are excessive compared to neighboring pixels, the systems and methods of the present invention may decrease their intensity based on an average intensity value from neighboring pixels.","The present invention now will be described more fully hereinafter with reference to the accompanying drawings, in which preferred embodiments of the invention are shown. This invention may, however, be embodied in many different forms and should not be construed as limited to the embodiments set forth herein; rather, these embodiments are provided so that this disclosure will be thorough and complete, and will fully convey the scope of the invention to those skilled in the art. Like numbers refer to like elements throughout.","The present invention provides systems and methods for use in enhanced vision displays. The systems and methods of the present invention receive image data from a plurality of sources located at different positions on a vehicle or in an area of interest. The systems and methods assimilate the images from each source into a composite image. The system may also be provided with synthetic data from a data source. Both sensors and synthetic data sources are referred to herein as a source. The systems and methods of the present invention detect the current line of sight of the user of the system. Based on this line of sight (LOS), the systems and methods display and image to the user representing the user's current field of view (FOV).","The systems and methods of the present invention reduce issues with regard to parallax in the displayed images. Specifically, the systems and methods of the present invention create one or more artificial mapping surfaces at a selected distance in front of the vehicle or point of interest. This artificial mapping surface allows the various images from the different sources to be correlated by a common frame of reference, with all of the images from each source being projected onto the mapping surface. The mapping surface creates a frame of reference for correlating the difference in line of sight to an object of interest between the observer and the sources due to the distance separation between the observer and the sources and the distance between the sources themselves.","Further, the sources, display, and mapping surface all have different coordinate systems. The systems and methods of the present invention create a common coordinate system and transform the images from each of these components to the common coordinate system. This, in turn, allows the systems and methods of the present invention to correlate the images from the various sources and provide them to the user in the viewer's frame of reference to thereby reduce and possibly eliminate parallax.","In addition to correcting for parallax, the systems and methods of the present invention also provide improved methods for stitching various tile images together into a composite or mosaic image. The systems and methods of the present invention overlap the edges of tile images received the same type of sources that are taken of adjacent or overlapping fields of view. In the overlapped region between the two tile images, the systems and methods of the present invention define blend zones that have two vertical edges. In the blend zones, the systems and methods of the present invention alter the intensity of individual pixels of each tile image so as to blend the two tile images together. The pixels of the left image in the blend zone are tapered from an intensity value of one (1) for the pixels adjacent to the left edge of the blend zone to an intensity value of zero (0) for the pixels adjacent to the right edge of the blend zone. In an opposite manner, the pixels of the right image in the blend zone are tapered from an intensity value of zero (0) for the pixels adjacent to the left edge of the blend zone to an intensity value of one (1) for the pixels adjacent to the right edge of the blend zone. This alteration of pixel intensity values in the blend zone provides an enhanced stitch between the two images. Top and bottom stitching is performed in the same manner.","The systems and methods of the present invention also provide a method for overlaying or fusing images from different types of sources. Specifically, the system of the present invention may include two different types of sources having either the same or overlapped fields of view. The sources provide different images of the same field of view; each source having associated advantages and disadvantages. For example, one source could be a video camera that provides images that may be affected by the amount of light or visibility and the other source may be an infrared source that provides images based on heat sensing. The systems and methods of the present invention provide a method for overlaying or fusing the images from these sources together to provide an enhanced image.","The systems and methods include two alternate methods for fusing the images. In the first alternative, the systems and methods of the present invention assign a percentage value to each tile image with regard to intensity. For example, one tile image may be defined with a 30% intensity and the other a 70% intensity. The images are then summed together in this intensity percentage ratio, thereby providing the user with the benefit of both images. In the second alterative, each tile image is displayed based on its content. For example, given a visible and an infrared image covering similar fields of view, the images can be combined at pixel level, where priority can be given to the infrared image based upon its pixel intensity. In this case, if the infrared pixel is at 75% of maximum, then the resulting pixel would be composed from 75% of the IR pixel intensity and 25% visible pixel intensity.","The systems and methods of the present invention further include methods for minimizing anomalies in a given displayed image. The systems and methods of the present invention evaluate the intensity values associated with local areas in adjacent images. The systems and methods of the present invention provide the means for local brightness discontinuity correction for adjacent as well as similar field of view images.","As summarized above, the systems and methods of the present invention provide an enhanced vision system. The various aspects of the present invention are provided in greater detail below.","It is first important to note, that the systems and methods of the present invention can be used in any environment where visual situational awareness is a concern, be it in an aircraft, automobile, or other type of vehicle or in a specified location or environment, such as a secured or surveillance area. In the below embodiments, the systems and methods are disclosed with regard to an aircraft. The aviation environment is a dynamic environment and aids in illustrating the robustness of the system. It is understood, however, that this is only one example of the use of the system and methods and that a wide variety of other applications are envisioned for use of the invention.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIGS. 1A and 1B","b":["10","12","14","16","18","20","21"]},"As illustrated in , the various sensors , (such as various image sources and radar sources as depicted in ), are electrically connected to an image processor  located on the aircraft. The image processor of the present invention is responsible for assimilating the various images from the sensors for display. Also connected to the image processor is a navigation system  for providing positional information, (i.e., longitude, latitude, pitch, roll, yaw, etc.), related to the aircraft. A navigational database  may also be available for providing synthetic navigational data to the system. Synthetic navigational data is typically D graphic data that simulates the terrain of other points of interest in a geographic location.","Importantly, also connected to the image processor is a display . In this particular embodiment, the display is a helmet-mounted display located in the helmet of the pilot or other viewer. Associated with the helmet is a helmet-tracking device . The helmet-tracking device provides information to the image processor concerning the present line of sight of the user. As illustrated in , a series of sensors or reflectors are located at various positions on the helmet  worn by the user. The tracking device  is located at a position relative to the helmet and tracks the movement an orientation of the sensor to determine the line of sight of the user relative to the vehicle. Such tracking devices are available from military suppliers such as BAE Systems in Santa Monica, Cal., or commercial suppliers such as Ascension Technology Corporation located in Milton, Vt., as well as several other suppliers not mentioned here. It is understood here that a helmet-tracking device is not required for the invention. Instead of use such a device, the orientation of the vehicle itself or a joystick control, etc. could be used to determine the user's line of sight. Similarly, in a security or surveillance location environment, the line of sight could be a fixed value trained on the view of interest.",{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 3","b":["22","36","38","12","40","42","44","30","24","26","38","46","48","50"]},{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIG. 4","FIG. 4"],"b":["40","12","42","200","36","210"]},"The central processor establishes a raytracing model involving vector transformations between the various image source, mapping surface, and display spaces (see step ). Each image tile vertex is mapped into its associated source image, defining a texture coordinate. These vertex and texture coordinates, along with other components yet to be explained, are sent to the graphics processor (step ) which renders these descriptions as 3D video-textured triangles.","In particular, the central processor of one embodiment of the present invention communicates with the graphics accelerator using an OpenGL Application Programming Interface. For each vertex of the mapping surface, the central processor provides three vector-valued data sets to the graphics processor  using the OpenGL command structure. The central processor provides: 1) a vertex vector representing a 3D coordinate point on the geometric mapping space, 2) the texture coordinate vector representing the associated texture image location corresponding to the vertex, and 3) a color vector that indicates a particular color (red, green, and blue) and an alpha-blending value, all used to determine how the associated pixels are to be displayed.","This vertex, texture, and color information is sent to the graphics processor as collections describing triangle meshes. The graphics processor uses these constructs to render the triangle meshes, stitching individual tiled images together at their respective edges to form a composite image. (See step ). Specifically, edges of each image are overlapped with adjacent tiles. The vertex associated with texture coordinates in these overlapped regions is manipulated to ensure coherent blending of adjacent images. To accomplish this procedure, the central processor alters the intensity a value associated with the color for the vertex when the vertex information is provided to the graphics processor. The procedures for stitching images together are discussed more fully below.","Although the systems and methods of the disclosed embodiment use an OPEN GL command structure to communication with the graphics processor, it must be understood other command structures could be used and are contemplated herein.","The systems and methods of the present invention also provide the ability to fuse images of similar fields of view together. (See step ). Specifically, the system may include one of more different types of sources with overlapping field of views. These sources create different images of the same view based on the characteristic of the sources. The graphics processor blends these images together such that the benefits of both images are incorporated into the displayed image. The procedures for fusing images together are discussed more fully below.","In addition to correlating the texture coordinates of each image to the vertices of the geometric mapping surface, the central processor also determines from the head-tracking device  the line of sight of the user. (See step ). The central processor loads those tile images stored in the processor memory  that are within the field of view for the current line of sight into the memory  of the graphics accelerator . (See step ). As each collection of vertex values is received from the central processor, the graphics processor associates the data from the texture images stored in memory  with each vertex of the geometric space that is in the field of view of the user and renders the image data into the display space. (See step ). The rendered data is then provided to the display via the graphics processor video output electronics. (See step ). This process is continually looped at the display video vertical frame rate using the freshest input imagery that is available.","As mentioned, a major issue with vision systems is the phenomenon of parallax, which is caused by the physical separation of the sources from the viewer, as well as the separation between the sources themselves. This separation causes the sources to have a different perspective relative to an object from that of the position of the user. To remedy these issues, the systems and methods of the present invention provide two procedures that reduce, if not eliminate issues with parallax. Specifically, the systems and methods of the present invention first define geometric mapping space that allows the images from the various sources to be mapped to a common space. Secondly, the systems and methods of the present invention create a common reference coordinate system and transform the sources, mapping surface, and display to the common or primary coordinate system.","With regard to the geometric mapping surface, the systems and methods of the present invention approximate the geometry of the real-world environment being imaged by the sources with a continuous mapping surface. The images from the sources are mapped to the mapping surface and are then mapped to the display. In this manner, all of the images of the system are correlated to a common or primary coordinate system for viewing.","With reference to , the geometric mapping surface is defined as a series of mesh triangles in the 3D space. Each triangle is defined by three (3) vertices. As illustrated in , for each vertex in the geometric space, the central processor provides texture coordinates that relate the vertex to the coordinates to a location within the associated input image. (See step  of ). In this way, the vertices are raytraced from the mapping surface into the source image to determine the associated texture coordinates for rendering. (See step ). The image is then rendered and displayed.","With reference to , a geometric mapping surface is selected in order to determine a vector field for each source. In other words, a common outer boundary is chosen for all sources. The images from each source are then mapped to the geometric surface and then to the display for a coherent image of the area surrounding the aircraft. The selection of the position of the geometric surface relative to the position of the sources is critical to reducing the issues with parallax.  illustrates two objects, (square object  and triangle object ), in space at different distances from the two sensors, Sensor A and Sensor B. The sensors are slightly \u201ctowed in\u201d relative to each other in order to create an increased overlap of the sensors' fields of view. (This is typically done with the sensors  of the system for this reason.) For accurate mapping of the two objects, a mapping surface  should be chosen so that the two objects will appear appropriately to the observer.  illustrate the issues involved with choosing the location of the mapping surface.","Specifically,  illustrate the problems associated with locating the surface  at the location of the square object . In this instance, after the surface is chosen, the square object  is projected onto the same location of the surface  for each sensor. For Sensor A, the triangle object maps to position , while for Sensor B, the triangle object maps to surface at position . As illustrated in , when the images from the two sensors are combined, it will appear to the observer that there are two triangles in the field of view.","With reference to , in a similar manner, if the surface  is selected at the position of the triangle object , the triangle object  will be mapped to the same location of the surface . For Sensor A, the square object maps to position , while for the Sensor B, the square object maps to surface at position . As illustrated in , in this instance, when the tile images from the two views are combined, it will appear as though there are two squares in the field of view.","As illustrated in , the selection of the position of geometric mapping surface is a critical issue in reducing parallax anomalies. For example, in the case illustrated in , the appropriate mapping surface location is theoretically indeterminate for objects at different ranges along the same ray. In other words, the location of a mapping surface is a function of the relative 3D geometry expressed from the viewpoint perspective, which is discontinuous at best, and indeterminate at worst. This is why the surface is chosen to approximate the geometry being imaged in order to obtain a solution that minimizes the parallax anomalies.","In the case of an aircraft a sphere may be used as the geometric mapping surface. At a significant altitude, parallax anomalies tend toward zero, allowing the geometric mapping surface to be chosen at some theoretical maximum distance. However, as the aircraft descends and approaches the ground, the distance between the aircraft location and the geometric mapping surface must be decreased to more closely approximate the distance between the aircraft and surrounding objects, such as the terrain. Thus, for low altitude flight, landing, and taxiing or in the case of ground vehicles or ground applications, a flat geometric surface may be used either with or as opposed to a sphere to approximate the surface of the ground. The approximation can also be further specified based upon apriori knowledge of the geometry being imaged, at the cost of computation complexity.","For example, some synthetic navigation data include terrain data that is represented in 3D by using polygons to represent various terrain features. The polygons could be used for mapping surfaces. In this instance, the system would include various geometric surfaces, and the surface that is closest to a display would be used as the mapping surface. As the aircraft moves, the geometric mapping surface would change to the then closest geometric surface from the synthetic data.","In another example, the system could receive data from a ranging device that indicates a range to terrain and other features. This range data can be used to construct the geometric mapping surface.","Once the geometric mapping surface is selected, it is then important to create a common or primary coordinate system for transforming between the coordinates systems of the sources, mapping surface, and display.  illustrates a spherical geometric mapping surface  that has been selected relative to a source (sensor ) and an object , with the object projected onto the surface at \u2032. As can be seen in this figure, the source (sensor ) has a coordinate system , the display has a coordinate system , and the mapping surface has a coordinate system  that are all different from each other. The objective is to create a reference coordinate system  that allows for mapping between the vector spaces of the source (sensor ), the geometric mapping surface, and that of the display. This collection of surfaces, coordinate systems, and geometric mapping through vector spaces is referred to as the raytracing model.","The raytracing model uses a Cartesian reference space , typically tied to the coordinates of the vehicle. It is important to account for as many factors as possible to ensure an accurate transformation is performed. Specifically, each source and display is modeled as a full six-degree-of-freedom component to account for the x, y, and z position of the optical exit or entrance pupil, as well as pitch, roll, and yaw of the external optical axis. Further, the optics associated with each source also affects the perspective of the image received by the source, and there also may be distortions caused by the optics themselves, or by the pixel distribution (raster) of the source or display. In light of this, the transformations include modeling each source and display in the system as full six-degree-of-freedom component with non-linear transformations for optical and raster distortions.","This transformation process for sources and displays is more specifically illustrated . Specifically, the reference coordinates for each source or display is defined by a six-degrees-of-freedom 6DOF reference. Each component also has a viewing volume  in pupil space that is frustrum shaped. The frustum is defined by near and far planes, and , and left, right, top, and bottom sides, -, that are half-angles from the line of sight  associated with the component. The line of sight is chosen along the x-axis of the coordinate reference of the component.","For sensor components, when an image is received by a sensor, the image passes through the optics of the sensor, which distorts the image . As such, the image is first transformed (f) from the pupil space to the image space to account for these non-linear transformations due to the optics. Following this transformation, the image is then transformed (f) to raster space, which establishes raster space origin and scaling for display on the raster . The transformations for a display are in the reverse order. Specifically, the raster space is first transformed to the image space (f), which, in turn, is transformed to the pupil space (f). These non-linear transformations can all be used in both the forward and inverse directions within the raytracing model.","The geometric mapping surface intersections can also transformed to the reference coordinate space  of . This is accomplished using a geometry-based transformation related to the specific geometric shape of the surface. Specifically, as illustrated in , from a given position P a line of sight Uis defined. A surface intersection S is defined where the line of sight intersects the mapping surface . The point is transformed to local Euclidean space and mapped to the surface f(U, V). The surface intersection is then mapped from the surface f(V, V) and transformed to the reference coordinate system.","The above discussed transformations for the sources, mapping surface, and display are used to properly correlate the images captured by each source, so that the images can be properly aligned and displayed.","As mentioned, the central processor of the present invention associates each image from each source with the mapping surface. This procedure is referred to as tiling. For each image, an area or tile is defined on the mapping surface, typically by constant azimuth and constant elevation boundaries. Tiling not only allows for aesthetic shaping of the image boundaries, but also allows for accurate stitching and fusion blend coefficient definitions for each vertex of the mapping surface that is constantly rendered by the graphics processor.",{"@attributes":{"id":"p-0079","num":"0078"},"figref":"FIG. 10","b":["60","84","86","12","60","68","90","90"]},{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIG. 11","FIG. 11"],"b":["12","64","86","60","84","28","66","94","12","96","94"]},"If the display  has linear display optics, the vertices  defining the tile are rendered as 3D textured polygons to thereby provide a perspective projection from the display pupil space to the display raster space . If the display has non-linear optics, the tile vertices are mapped to the display raster space to establish 2D vertex coordinates. The tile vertices are then rendered as 2D textured polygons. This is an orthographic projection from display image space to display raster space.","In addition to accurately mapping each image to a common mapping surface and providing transforms for relating the images, mapping surface, and display to a common coordinate system, the systems and method of the present invention also provide techniques for enhancing the images as displayed. As earlier stated, the systems and methods of the present invention provide methods for stitching adjacent image tiles together to form a composite image. (See step , ).  illustrate this stitching process.","In particular,  illustrates a series of image tiles -that collectively define a horizontal mosaic image array . Each of these tiles was taken with sources that slightly overlap one another such that the adjacent edges of each tile contain identical imagery. For example, an edge portion  of tile is a display of the same view as is displayed in edge portion  of the tile . The same is true for all other adjacent portions of each tile. This applies to two dimensional mosaic arrays as well, where the top and bottom edges of vertically adjacent tiles are blended.","Given that the images are the same at the edge of each tile, the tiles are overlapped at their adjacent edges so that the common portions of the image are overlay each other. This relationship is established mathematically by the central processor. Specifically, the central processor, for each tile, defines the edge portions of each tile to have the same mapping surface vertices. For example, a pixel in the edge portion  will have the same designated mapping surface vertices as a pixel in the edge portion . More specifically, when the central processor sends information to the graphics processor for rendering the first tile , it will send vertex, texture, and color information for the triangle strip representing the edge portion . When the central processor sends information for rendering the second tile , it will send vertex, texture, and color information for the triangle strip representing the edge portion . Because the two pixels, and , for the different tiles are determined from independent but identical vertex strips, the graphics processor will render and display the two pixels at the same location on the display. This is done for each corresponding pixel in each adjacent tile where the images are the same so as to create an overlap between adjacent tiles. Note that this is done for all adjacent edges as is illustrated in . By overlapping the tiles -, the composite mosaic image  can be displayed.","Although overlapping adjacent edges of tiles provides some level of stitching, merely overlapping alone does not provide a seamless image. For this reason, the systems and methods of the present invention further manipulate the images to create a hidden seam. With reference to , the systems and methods of the present invention leverage the texture modulation and pixel blending capabilities of OpenGL to hide the seams. Specifically, as shown in , a series of images -are rendered on the mapping surface  to form the composite image . The systems and methods of the present invention define blending zones  on the mapping surface  at the location where the images overlap. Each blending zone defines a plurality of vertices on the mapping surface.","In order to blend the two images, the central processor of the present invention alters the intensity value of the vertex colors in the blend zone. Specifically, as stated previously, for each vertex, the central processor defines vertex coordinates, texture coordinates, and color. Associated with the color is an alpha blending value, which is used by the graphics processor to determine how the currently rendered pixel contributes to the contents of the frame buffer. In one embodiment of the present invention, the central processor defines vertex colors such that the texture modulation and blending capabilities of the graphics processor creates a tapering effect in the blend zones so as to blend the two tile images together. With texture modulation, the resultant pixel color begins as a solid color interpolated from the associated vertices, but is further modified by modulating this color with the associated texture image content. For example a triangle with all red vertices would normally be rendered as a solid red triangle, but when modulated by an associated monochrome texture image, the result is the appearance of a red triangular image. This modulation is performed independently for the red, green, and blue pixel values, thus a blue triangle with a green texture would result in a black pixel.","The blending functions involve weighted summation of the rendered pixel with the existing pixel in the frame buffer, performed independently for the red, green, and blue components. The first blending function (f( )) uses the interpolated vertex alpha value for the pixel to determine the weightings as\n\nDisplay=1( )=\u03b1*Image +(1\u2212\u03b1)*Image \n\nThe second blending function (f( )) uses the interpolated alpha only for the source pixel ImageA, and uses unity for the ImageB weighting, i.e.\n\nDisplay=2( )=\u03b1*Image +(1)*Image \n","When rendering the two triangle strips of a particular mosaic edge blend zone, the first strip is rendered using f( ), then the second strip is rendered using f( ). With some basic arithmetic, and assuming the frame buffer initially contains the pixel value Display, the resulting pixel value can be shown to be\n\nDisplay2=\u03b1*Color*Image+\u03b1*Color*Image+(1\u2212\u03b1)*Display0\n\nSince the ColorA and ColorB are defined by the blend zone color taper as unity sum,\n\nColor1\u2212Color\n\nDisplay2=\u03b1*(Image\u2212Color*Image+Color*Image)+(1\u2212\u03b1)*Display0\n\nFurthermore, assuming perfect image registration, or ImageB=ImageA, and the result is\n\nDisplay2=\u03b1*Image+(1\u2212\u03b1)*Display0\n\nIt can also be shown that the pixel value error due to misregistration can be expressed as\n\nDisplayError=\u03b1*Color*(Image\u2212Image)\n\nThis is true for every display pixel in the blend zone region since the triangle strips from each contributing source image is defined with identical vertex locations and unity-sum color gradients.\n","Although the blend zone definitions are designed to have unity sum, brightness mismatch between adjacent source images can still cause discontinuities in the composite image. This discontinuity is minimized by reducing the color magnitude of the inner blend zone vertices of the brighter image. The magnitude scale factor is calculated as the ratio of the relative brightness ratio of the adjacent images at the point in question. A simple local-area average is calculated at each blend zone texture coordinate for each source image. For example, as shown in , when two images are stitched together at a seam , a difference in brightness  between the images may occur. To remedy this, the intensity magnitudes for the brighter image may be scaled down by the relative intensity ratios, resulting in a more pleasing brightness match .","In addition to providing methods for blending of adjacent tile edges, the present invention also provides methods for fusing images from different types of sources. Image fusion allows the user to view a composite image that includes the unique advantages offered by each contributing source. An example is fusion of image from a video camera and an infrared source, where the fused image benefits from visual characteristics of the camera and heat visualization of the infrared source.",{"@attributes":{"id":"p-0091","num":"0090"},"figref":"FIG. 15","b":["36","118","118","1"],"i":["a ","b","=f","A"],"sub":["blend","blend"],"br":{},"in-line-formulae":[{},{}]},"As an alternative to percentage-based fusion, content-based fusion may be used. This method involves a similar percentage ratio blending, however, rather than using an alpha (\u03b1) value assigned to the tile vertices, the blending coefficient is determined from the source pixel intensity. This is referred to as a pixel driven alpha written in equation form as:\n\nDisplay1=(Image 2)*Image +(1\u2212Image 2)*Display0\n\n","Other fusion techniques are contemplated for use with the present invention. These include, for example, arithmetic operations, frequency domain manipulation, and object identification and extraction.","In addition to the mosaic image stitching and image fusion, the image tiling capability also provides the ability to present picture-in-picture virtual displays distributed throughout the viewable space. For instance, it may be desirable to have a moving map presented in the lower display areas, similar to having a paper map in your lap. Another desire might be to have a rear-facing camera mapped to a tile in the upper display area, similar to a rear-view mirror. These concepts are depicted in , which shows a stitched horizontal infrared image array  in the forward view of the vehicle, a rear-view mirror tile  above, and a moving map tile  below. This imagery can be further augmented with a synthetic image source, such as a head-tracked 3D terrain rendering correlated with vehicle position.","In this instance, the central processor receives the output of a synthetic vision system that generates 3D terrain and graphics from a navigation database. The synthetic vision image is transformed to the mapping surface and rendered similar to the other source images, including the image fusion function. For example,  illustrates a synthetic image fused with the central area of the composite view of .","In addition to moving map data, other types of display data could also be displayed in a separate tile or fused with the images. For example, the aircrafts HSI and ADI displays, altimeters, airspeed, etc. could be displayed on the display as a tile or fused with an image to provide an integrated view allowing the pilot to view instrument readings while also viewing the environment surrounding the aircraft.","While not illustrated, the systems and methods also include the ability to provide picture-in-picture tiling, zooming, panning etc.","U.S. patent application Ser. No. 10\/377,412, entitled: SYSTEMS AND METHODS FOR PROVIDING ENHANCED VISION IMAGING WITH DECREASED LATENCY, and filed concurrently herewith describes an image display system that uses a field programmable gate array or similar parallel processing device; the contents of which are incorporated herein by reference.","Many modifications and other embodiments of the invention will come to mind to one skilled in the art to which this invention pertains having the benefit of the teachings presented in the foregoing descriptions and the associated drawings. Therefore, it is to be understood that the invention is not to be limited to the specific embodiments disclosed and that modifications and other embodiments are intended to be included within the scope of the appended claims. Although specific terms are employed herein, they are used in a generic and descriptive sense only and not for purposes of limitation."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Having thus described the invention in general terms, reference will now be made to the accompanying drawings, which are not necessarily drawn to scale, and wherein:",{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIGS. 1A and 1B"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIGS. 6A-6D"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 19"}]},"DETDESC":[{},{}]}
