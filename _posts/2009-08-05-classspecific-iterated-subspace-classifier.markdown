---
title: Class-specific iterated subspace classifier
abstract: A method is provided for calculating a class-specific iterated subspace for a classification system utilized in a computing system. Training data in the specific class for the class-specific iterated subspace is collected. A linear orthogonal transform is applied transforming the data into at least one bin. Magnitude squared bins are calculated and used as columns of a matrix. Orthonormal vectors of this matrix are selected and a J function is calculated. The J function and orthonormal starting vectors are used to obtain the class-specific iterated subspace for each class. The method further applies these class-specific iterated subspaces in a classification system for determining the most likely class of a data signal of interest.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08185490&OS=08185490&RS=08185490
owner: The United States of America as represented by the Secretary of the Navy
number: 08185490
owner_city: Washington
owner_country: US
publication_date: 20090805
---

{"@attributes":{"id":"description"},"GOVINT":[{},{}],"heading":["STATEMENT OF GOVERNMENT INTEREST","CROSS-REFERENCE TO RELATED PATENT APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["The invention described herein may be manufactured and used by or for the Government of the United States of America for governmental purposes without the payment of any royalties thereon or therefor.","None.","(1) Field of the Invention","The present invention generally relates to a class-specific signal analysis method using a subspace that maximizes class-specific J-functions.","(2) Description of the Prior Art","Characterizing an input signal using automated data processing systems is a common problem in many fields. In sonar, it is often desirable to separate natural sources from manmade sources. This is also true in radar. In speech recognition, it is desirable to recognize phonemes so that speech can be turned into text. In virtually all state-of-the-art methods, the process of characterizing the data is divided into two separate stages. In the first stage, it is necessary to extract features (useful information in the form of a compact set of parameters) from the input data that is useable by automatic recognition algorithms. In the second stage, an algorithm, usually a probabilistic model, decides which type of signal is present based on the features.","An example of such a system is automatic speech recognition (ASR) system as implemented on a computer. In the first stage of a state-of-the-art ASR system, the speech signal is divided into equal-sized segments, from which features are extracted. These features are usually extracted in mel-scale cepstral format because this format focuses on the frequency response of human hearing.","The mel-scale cepstrum is calculated by taking the Fourier transform of a time domain signal to produce a spectrum. Powers of the spectrum are mapped onto the mel scale. Logarithms are taken of the powers at each of the mel frequencies. A discrete cosine transform is calculated for the logarithms of the mel powers. The mel-scale cepstral coefficients are the calculated discrete cosine transform coefficients. In ASR systems, the mel-scale cepstral coefficients are used as the feature set for recognizing phonemes.","In mathematical terms, one may write the MEL cepstrum features as\n\n=DCT(log())\u2003\u2003(1)\n\nwhere vector y is the length\u2212N\/2+1 spectral vector, the magnitude-squared DFT output and the columns of A are the MEL band functions, and the \u201cprime\u201d notation indicates the transpose of the matrix A. The logarithm and the discrete cosine transform (DCT) are invertible functions. There is no dimension reduction or information loss so they may be considered a feature conditioning step, which results in more Gaussian-like and independent features.\n","Other approaches of feature set development are taught in the prior art. The use of signal-dependent or class-dependent features for classification, known as the class-specific method or CSM, is covered in U.S. Pat. No. 6,535,641, \u201cClass-Specific Classifier\u201d. The probability density function (PDF) projection theorem (PPT) is disclosed in Baggenstoss, \u201cThe PDF Projection Theorem and the Class-Specific Method\u201d, IEEE Transactions on Signal Processing, Vol. 51, No. 3 (March 2003) which is incorporated by reference. The probability density function projection theorem eliminates the need for sufficient statistics and allows the use of class-dependent reference hypotheses, improving the performance of any classification system using class-dependent features. U.S. Pat. No. 6,466,908, entitled \u201cSystem and Method for Training a Class-specific Hidden Markov Model Using a Modified Baum-Welch Algorithm\u201d alleviates the need for a common feature set in a HMM.","The key operation here is dimension reduction by linear projection onto a lower-dimensional space. Now, with the introduction of the class-specific method (CSM) and the PDF projection theorem (PPT), one is free to explore class dependent features within the rigid framework of Bayesian classification. Some work has been done in class-dependent features; however, existing approaches are only able to use different features by using compensation factors to make likelihood comparisons fair. Such approaches work if the class-dependent feature transformations are restricted to certain limited sets. Both methods fall short of the potential of the PPT, which makes no restriction on the type of feature transformations available to each phoneme. Under CSM, the \u201ccommon feature space\u201d is the time-series (raw data) itself. Feature PDFs, evaluated on different feature spaces are projected back to the raw data space where the likelihood comparison is done. Besides its generality, the CSM paradigm has many additional advantages as well. For example, there is a quantitative class-dependent measure to optimize that allows the design of the class-dependent features in isolation, without regard to the other classes.","A prior art classifier is shown in . The classifier  receives data from a data source . Data source  is joined to a feature transformation module  for developing a feature set. The feature set is provided to pattern match processors , which correspond to each data class. Pattern match processors  provide an output measuring the developed feature set against trained data. The pattern match processor  outputs are compared in a comparison  and the highest output is selected.",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2","b":["10","12","14","16","12","18","18","14","18","12","20","18","20","20","20","22","20","22","24","20","24","26","12","16","16","24"],"sub":["j","j"]},"Accordingly, there is provided a method for calculating a class-specific iterated subspace for a classification system utilized in a computing system. Training data in the specific class for the class-specific iterated subspace is collected. A linear orthogonal transform is applied transforming the data into at least one bin. Magnitude squared bins are calculated and used as columns of a matrix. Orthonormal vectors of this matrix are selected and a J function is calculated. The J function and orthonormal starting vectors are used to obtain the class-specific iterated subspace for each class. The method further applies these class-specific iterated subspaces in a classification system for determining the most likely class of a data signal of interest.","When applying the class specific method, one must find class-dependent signal processing to produce features that characterize each class. This invention applies specifically to class-specific classifiers in which the features are produced by these three steps (1) applying a Fourier transform or discrete Fourier transform to the input data to obtain a power spectral vector y, (2) then the multiplication of a spectral vector y by a dimension-reducing matrix A, then (3) optionally applying a feature conditioning transformation. While the Fourier transform and discrete Fourier transform are explicitly mentioned here, it is understood by those skilled in the art that other transforms could be used for this. These transforms include the discrete cosine transform, wavelet transform and the like. We seek an automatic means of optimizing the matrix A for a given class. We first review the class specific method.","Let there be M classes among which we would like to classify. The class-specific classifier, based on the PPT, is given by",{"@attributes":{"id":"p-0022","num":"0021"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"arg","mo":["\u2062","\u2062"],"munder":{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mo":"\u2062","mi":"max"},"mi":"m"},"mrow":{"msub":{"mi":["p","p"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"\u2758","msub":{"mi":["H","m"]}}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":[{},{},{}],"sub":["p","m","p","m","m","m","0,m","m","m","m","m"],"in-line-formulae":[{},{}],"i":["p","x|H","J","x,A",",H","p","z","|H"]},{"@attributes":{"id":"p-0023","num":"0022"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["J","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":[",",","],"msub":[{"mi":["A","m"]},{"mi":"H","mrow":{"mn":"0","mo":",","mi":"m"}}]}}},"mo":"=","mfrac":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"\u2758","msub":{"mi":"H","mrow":{"mn":"0","mo":",","mi":"m"}}}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["z","m"]},{"mi":"H","mrow":{"mn":"0","mo":",","mi":"m"}}],"mo":"\u2758"}}}]}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":[{},{},{}],"sub":["0,m ","0","m ","m","m","m","m","0,m"],"in-line-formulae":[{},{}],"i":["z","=C","A\u2032","y"]},"It is the \u201ccompensation term\u201d that allows feature PDFs from various feature spaces to be compared fairly because the resulting log-likelihood function is a PDF on the raw data space x. The J function is a generalization of the determinant of the Jacobian matrix in the case of a 1:1 transformation. The PPT guarantees that p(x|H) given by (3) is a PDF, so it integrates to 1 over x regardless of the reference hypothesis Hor the feature transformation producing zfrom x. It is up to the designer to choose Hand Ato make p(x|H) as good an estimate of p(x|H) as possible. The designer is guided by the principle that if zis a sufficient statistic for Hthen p(x|H) will equal p(x|H) (provided {circumflex over (p)}(z|H) is a good estimate). We can also think of it as a way of imbedding a low-dimensional PDF within a high-dimensional PDF. We have good reason, as we shall see, to use a common reference hypothesis, Hwhich simplifies the classifier to\n\narg max J(x,A,H)p(z|H)\u2003\u2003(6)\n\nwhere the J function, J(x), now depends only on A. Note that in contrast to other class-dependent schemes using pairwise or tree tests, the class specific method is a Bayesian classifier and has the promise of providing a \u201cdrop-in\u201d replacement to the MEL cepstrum based feature processors in existing ASR systems. The J function for this specific feature set is covered in Steven M. Kay and Albert H. Nuttall and Paul M. Baggenstoss, , IEEE Trans. Signal Processing, October, 2001, which is incorporated by reference herein.\n","We are interested in adapting the matrix A to an individual class. We propose the strategy of selecting Ato maximize the total log-likelihood of the training data using the projected PDF. Let",{"@attributes":{"id":"p-0026","num":"0025"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"x","mn":"1"},"mo":",","mrow":{"mrow":{"msup":[{"mi":"x","mn":"2"},{"mi":["x","k"]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026"},"mo":";","msub":{"mi":["A","m"]}}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"K"},"mo":"\u2062","mrow":{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["p","p"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":["x","i"]},"mo":"\u2758","msub":{"mi":["H","m"]}}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":{},"sub":["p","m"]},{"@attributes":{"id":"p-0027","num":"0026"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"msub":{"mi":["p","p"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"\u2758","msub":{"mi":["H","m"]}}}},{"mrow":[{"mo":["[","]"],"mfrac":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"\u2758","msub":{"mi":"H","mn":"0"}}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["z","m"]},{"mi":"H","mn":"0"}],"mo":"\u2758"}}}]}},{"mover":{"mi":"p","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["z","m"]},{"mi":["H","m"]}],"mo":"\u2758"}}}],"mo":"\u2062"}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}},"br":[{},{}],"sub":["0 ","0","m","m","m","m","0"],"in-line-formulae":[{},{}]},"It is difficult to determine how the first term {circumflex over (p)}(z|H) is affected by changing A. To determine the effect of changing A, new feature vectors zneed to be calculated for each training sample, then the PDF needs to be re-estimated and {circumflex over (p)}(z|H) needs to be evaluated for each training sample. On the other hand, given the simplicity of the reference hypothesis H, the second term p(z|H) can be known, either in analytic form or in an accurate analytic approximation. This is taught by Kay et al., \u201cMultidimensional Probability Density Function Approximations for Detection Classification, and Model Order Selection,\u201d IEEE Transactions on Signal Processing, Vol. 49, No. 10, pp. 2240-2252, (October 2001), which is incorporated by reference herein. Furthermore, the first term can be made nearly independent of A, by requiring A, to be orthonormal. We proceed, then by ignoring the term {circumflex over (p)}(z|H) and maximizing the function",{"@attributes":{"id":"p-0029","num":"0028"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"Q","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"x","mn":"1"},"mo":",","mrow":{"mrow":{"msup":[{"mi":"x","mn":"2"},{"mi":["x","K"]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026"},"mo":";","msub":{"mi":["A","m"]}}}}},{"mo":"-","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"K"},"mo":"\u2062","mrow":{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["z","m","i"]},"mo":"\u2758","msub":{"mi":"H","mn":"0"}}}},"mo":"."}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}}},"The change in {circumflex over (p)}(z|H) can be minimized as Ais changed by insisting on an orthonormal form for A. Thus, by maximizing L (7) under the restriction that Ais orthonormal, we approximately maximize L. Constraining Asuch that the columns of Aare an orthonormal set of vectors. We use a orthonormality under the inner product:",{"@attributes":{"id":"p-0031","num":"0030"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mo":"<","mi":"x"},{"mi":"y","mo":">=","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"i","mo":"=","mn":"0"},{"mi":"N","mo":"\/","mn":"2"}]},"mo":"\u2062","mrow":{"msub":[{"mi":["\u025b","i"]},{"mi":["x","i"]},{"mi":["y","i"]}],"mo":["\u2062","\u2062"]}}}],"mo":[",",","]}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}},"br":{},"sub":["i ","i ","m "]},"Ais further constrained with respect to energy sufficiency. The energy sufficiency constraint means that the total energy in x,",{"@attributes":{"id":"p-0033","num":"0032"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"E","mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","msubsup":{"mi":["x","i"],"mn":"2"}}}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}},"br":{}},{"@attributes":{"id":"p-0034","num":"0033"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mfrac":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"\u2758","msub":{"mi":"H","mn":"0"}}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["z","m"]},{"mi":"H","mn":"0"}],"mo":"\u2758"}}}]}},{"mrow":{"mo":["(",")"],"mn":"13"}}]}}}},"br":{},"sub":["0 ","1","1","m ","1"]},"Since we would like the feature set created by projecting onto the columns of A to characterize the statistical variations within the class, a natural first step is to use principal component analysis (PCA). To do this, we arrange the spectral vectors from the training set into a matrix\n\nY=[yy. . . y]\u2003\u2003(14)\n\nwhere K is the number of training vectors. To meet the energy sufficiency constraint, we fix the first column of A to be the normalized eidentified as {tilde over (e)}:\n",{"@attributes":{"id":"p-0036","num":"0035"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mover":{"mi":"e","mo":"~"},"mn":"1"},"mo":"=","mfrac":{"msub":{"mi":"e","mn":"1"},"mrow":{"mo":["\uf605","\uf606"],"msub":{"mi":"e","mn":"1"}}}}},{"mrow":{"mo":["(",")"],"mn":"15"}}]}}}},"br":{},"sub":["1","1","n","1","n","n","n","1","m "]},"This method has been used with known experimental data (the TIMIT data set) as a source of phonemes. The data consists of sampled time-series (in 16 kHz .wav files) of scripted sentences read by a wide variety of speakers and includes index tables that point to start and stop samples of each spoken phoneme in the text. In TIMIT, each speaker is identified by the dialect region speaker, and phoneme. Dialect region takes values from 1-8. The speaker is identified by a 5 character code such as FDAWO or MGRLO. The initial letter F or M indicates the sex of the speaker. There are 61 phonemes in the database, having a 1 to 4 character code. We use the term dataclass to represent the collection of all the phonemes of a given type from a given speaker. The average number of utterances of a given speaker\/phoneme combination is about 10 and ranges from 1 up to about 30 for some of the most common phonemes. Speaker\/phoneme combinations with no fewer than 10 samples were used.","In all of our classification experiments, the utterances of a given speaker\/phoneme were divided into two sets, even (samples 2, 4, 6 . . . ) and odd (samples 1, 3, 5 . . . ). We conducted two sub-experiments, training on even, testing on odd, then training on odd, testing on even. We reported the sum of the classification counts from the two experiments.","We now describe the processing for the features of the MEL cepstrum classifier (MCC) and CSIS. In order to concentrate on the basic dimension reduction step (equation 2), the simplest possible processing and PDF modeling was used. Each step in the processing is described below, in the order in which it is processed.","The phoneme data was pre processed by resampling from 16 kHz down to 12 kHz. Phoneme endpoints were correspondingly converted and used to select data from the 12 kHz time-series. The phoneme data was also truncated to a multiple of 384 samples by truncating the end. Those phoneme events that were below 384 samples at 12 kHz were not used. Doing this allowed us to use fast Fourier transform (FFT) sizes of 48, 64, 96, 128, or 192 samples, which are all factors of 384.","We computed non-overlapped unshaded (rectangular window function) FFTs resulting in a sequence of magnitude-squared FFT spectral vectors of length N\/2+1, where N is the FFT size. The number of FFTs in the sequence depended on how many non-overlapped FFTs fit within the truncated phoneme utterance.","Spectral vectors were normalized after FFT processing. For non-speaker-dependent (MEL cepstrum) features, the spectral vectors were normalized by the average spectrum of all available data. For class specific iterated subspace (CSIS) (speaker-dependent) features, the spectral values for each speaker\/phoneme combination were normalized by the average spectrum for that speaker\/phoneme. In classification experiments, the average spectrum was computed from the training data to avoid issues of data separation.","Next, the spectral vectors, denoted by y, were projected onto a lower dimensional subspace by a matrix as in (2) resulting in feature vectors, denoted by w. For the mel cepstrum classifier, the columns of A were mel frequency band functions. The number of columns in matrix A was N+2 including the zero and Nyquist half-bands. (see ). For CSIS, A was an orthonormal matrix determined from the optimization algorithm. For CSIS, the number of columns of A was P+1 where P is the number of basis functions in addition to the first column {tilde over (e)}.","From a statistical point of view, feature conditioning has effect on the information content of the features. It does, however, make probability density function (PDF) estimation easier if the resulting features are approximately independent and Gaussian. For MCC, the features were conditioned by taking the logarithm and discrete cosine transform as in (1). For CSIS, features were conditioned first by dividing features 2 through P+1 by the first feature. This effectively normalizes the features since the first feature, being a projection onto e, is a power estimate for the segment. Lastly, the logarithm of the first feature is taken. Mathematically, we have for CSIS\n\nw=A\u2032y\u2003\u2003(16)\n\n=log()\u2003\u2003(17)\n\n2, 3, . . . 1\u2003\u2003(18)\n","J-function contributions must be included for FFT magnitude-squared, spectral normalization, matrix multiplication, and feature conditioning.","We used a simple multivariate Gaussian PDF model, or equivalently a Gaussian mixture model (GMM) with a single mixture component. We assume independence between the members of the sequence within a given utterance, thus disregarding the time ordering. The log-likelihood value of a sample was obtained by evaluating the total log-likelihood of the feature sequence from the phoneme utterance. The reason we used such simplified processing and PDF models was to concentrate our discussion on the features themselves.","Classification was accomplished by maximization of log-likelihood across class models. For CSS and CSIS, we added the log J-function value to the log-likelihood value of the Gaussian mixture model, implementing (6) in the logarithm domain.",{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 3","b":["30","32","30","34","34","36","38","40","42","30","46","42","40","40","38","46","40"]},"The CS feature transform section  includes a multiplier  receiving output (y) from spectral vector  and multiplying it by the CS band functions  for the associated class (A) producing a CS modified spectral vector. The CS band functions are class specific versions of the MEL band functions A used in conventional MEL cepstrum analysis. The CS band functions  are computed as disclosed above. The CS modified spectral vector is provided to normalization and conditioning section . Normalization and conditioning can be by conventionally known methods such as discrete cosine transforms, discrete Fourier transforms and the like. It is preferred that this section provide valid results for negative inputs, so logarithmic processing is not desired. Normalization and conditioning section  produces CS transformed data (z). The CS transformed data is provided as input to the CS probability function section . CS probability function section  provides an output indicating how well the CS transformed data matches the associated class. CS J function section  receives input from spectral vector  and calculates a CS correction vector. Multiplier  receives the CS probability function section  output and the CS correction vector from the CS J function section . As an output, multiplier  provides a measure of how well the CS transformed data matches the associated class that is comparable among all of the classes. Comparator  receives the comparable matching data and provides an output signal indicating the class that is most likely to match that of the incoming spectral data."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing features of the present invention will become more readily apparent and may be understood by referring to the following detailed description of an illustrative embodiment of the present invention, taken in conjunction with the accompanying drawings, in which:",{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 3"}]},"DETDESC":[{},{}]}
