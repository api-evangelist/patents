---
title: Hierarchical approach for the statistical vowelization of Arabic text
abstract: Advantageously, the text is completed according to a model hierarchy giving higher priority to longer chunks of text, ie sentences () then multiword phrases (), then words () and finally character groups ().
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08069045&OS=08069045&RS=08069045
owner: Nuance Communications, Inc.
number: 08069045
owner_city: Burlington
owner_country: US
publication_date: 20040923
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["1. BACKGROUND OF THE INVENTION","2. SUMMARY AND ADVANTAGES OF THE INVENTION","4. DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT","Details of the Sentence and Phrase Vowelizer Embodiment","Details of the Word Vowelizer Embodiment","Details of the Character Vowelizer Embodiment"],"p":["1.1. Field of the Invention","The present invention relates to the field of computer-aided text and speech processing, and in particular to a method and respective system for converting an input text given in an incomplete language, into speech, wherein a computer-aided grapheme-phoneme conversion is used.","1.2. Description and Disadvantages of Prior Art","The term \u201cincomplete\u201d language used herein shall be understood to be a language, which does not necessarily contain a complete syntactic description of phrases in its textual representation. Good examples are the \u201cnatural\u201d semitic languages (such as Arabic and Hebrew), in which written text often lack vowels. Other examples are \u201cartificial\u201d languages, which may be used to abbreviate complete text.","The present invention will thus be defined from prior art by aid of the Arabic language, as it can be very advantageously applied to the processing of Arabic, a member of the family of Semitic languages, that does only occasionally make use of vowels when written.","1.2.1 Introduction to Arabic Language","Arabic is one of the Semitic languages and is an important language to religion and literature. Arabic is spoken by almost 200 million people in more than twenty two countries.","Arabic Text","The most striking difference between Arabic and other languages is that Arabic text is usually presented without vowels and other diacritical marks. Vowels, when used, are presented by diacritical marks, placed above or below the character. The process of adding vowels and other diacritical marks to Arabic text can be called Diacritization or, for simplicity, Vowelization. Vowelization defines the sense of each word, and how it will be pronounced. However, the use of vowels and other diacritics has lapsed in modern Arabic writing. It is, therefore, the norm for an entire formal Arabic newspaper to have only a dozen or so thoughtfully-used vowels and diacritics placed only where there is not enough context to know what is intended.","These zero-width optional elements are used occasionally to disambiguate homographs when there is insufficient context for the reader to do so. A good reader anticipates these potential ambiguities and inserts shorts vowels and diacritics as needed, such as to disambiguate the Arabic for \u201cAmman\u201d and \u201cOman\u201d, or to indicate the passive voice. Occasionally one hears professional news announcers pause and backtrack to re-read a passage with a different \u201cvocalization\u201d of a word.","Vocalization","In any vocalized language vowels play an important role since they are-the most prominent and central sound of a syllable. The vowels help us to join consonants to achieve a full sound. In English a, e, i, o and u (also y) are the vowels which are clearly spelled out in a text, whereas in Arabic they are not.","Arabic Vowels and other Diacritics","Arabic has three short vowels:\n\n","In addition there are three kinds of diacritics:\n\n","In the remainder of this patent application we will distinguish between vowel signs and other diacritical marks only if it is required for purposes of illustration or in cases of exception. In general, we will refer to both groups of marks as vowels, and will refer to written text that makes use of any vowel signs and\/or diacritical marks as vowelized text. In contrast, all other written text is referred to as un-vowelized text or simply as text, which is often used as input to the inventional method.","Problems in Automatic Speech and Natural Language Processing","As mentioned above, almost all written Arabic text\u2014for example all newspaper text\u2014is un-vowelized, which may lead to ambiguity in meaning and different possibilities of pronunciation. A normal Arabic speaker can put vowels \u201con the fly\u201d while reading to get the intended meaning; readers usually apply their linguistic and semantic knowledge to resolve ambiguities.","While humans perform quite well on this task, the omission of vowels in written text leads to some serious problems for automatic speech and language processing systems. For example, the language model component of an automatic speech recognition system requires vowelized text in order resolve ambiguities and to achieve very high transcription accuracy.","Even more obvious is the fact that on-line vowelization of written text is indispensable for a text-to-speech (TTS) system, in order to correctly pronounce the input text.","For the construction of such speech technology components, current state-of-the-art speech recognition applications usually use manually vowelized text, which is tedious and error prone to create and results in less reliable components.","State-of-the-art TTS systems, as represented simplified in  may be used for a large variety of applications A, B, etc. Examples are telephony applications used for call-centres and elsewhere. A text written in a Semitic language is input into a vowelization tool , which is used and developed by a plurality of highly skilled developers , who apply an ensemble of morpho-syntactical rules, usually including a determination of etymological roots of text components, the determination of casus, numerus, and genus, and part-of-speech tagging, i.e. the identification of word types (nomen, verbum, adjective, etc.). Usually, also a semantic analysis is performed in order to determine the meaning of a word in its context. By applying this ensemble of rules the missing vowels and other diacritical marks are inserted into the original input text.","Those rules are depicted by reference sign , and an exemplary plurality of exceptions is depicted with reference sign  in order to illustrate the empiric character of this rule collection. Rules and large exception dictionaries are often stored electronically as part of the front-end component  of a text-to-speech (TTS) system. As depicted in , which is a schematic block diagram representation illustrating some more details of the prior art TTS conversion, the TTS-front-end also generates a phonetic description (also known as \u201cbaseform\u201d) and a prosodic description (aka intonation contour) of the input text.","The TTS back-end component  generates synthetic speech signals  from the above-mentioned phonetic and prosodic description for outputting via a speaker system.","The above-mentioned TTS engine including the front-end  and back-end  thereof is implemented in prior art usually as a component of a prior art voice server, which is schematically illustrated in . As  illustrates, such prior art voice server further comprises a speech recognition engine , as most of the applications are operating fully bidirectional, i.e. they convert speech to text and vice-versa. Further, the voice server  comprises a voice browser  for rendering the acoustic or textual information, connected to a telephony stack , which handles the incoming calls. A JAVA speech application programming interface (JSAPI) component  receives audio signals, words and grammars corresponding to a respective telephone call structure from the voice browser component. The JSAPI component co-operates with the speech recognition engine and the text-to-speech engine  as it is known from prior art. The present invention basically improves the TTS engine ,  and improves the training, the speech recognition engine  is based on.","Further, according to , the prior art environment comprises an interface to a co-operating web application server  which may be implemented on a PC system, either desktop or portable, possibly together with the before-mentioned applications or the respective application components and the voice server itself, or which may run in a client-server environment.","As mentioned above, the use of un-vowelized text is common in written Arabic. An average Arabic speaker or reader will add vowels on the fly while reading to get the intended meaning. In contrast, the use of un-vowelized text by a computer program that performs any kind of natural language processing is almost impossible, because such text is highly ambiguous: without a mechanism for vowelization, the system would simply behave unpredictable.","For an illustration of the problem, consider the following example that\u2014for the purpose of explanation\u2014is given in English: Imagine that English in its written form uses only consonants, but no vowels. In this case, the two words \u201cwrite\u201d and \u201cwrote\u201d will both be written as \u201cwrt\u201d. When \u201cwrt\u201d appears in a sentence, a reader will have at least two choices:\n\n","A prior art morphological analyzer can only propose these two solutions, and more information is needed for disambiguation. For example, the consideration of the syntactic sentence structure can be used to obtain the correct vowelizations (\u201cI will write a letter.\u201d vs. \u201cYesterday I wrote a letter.\u201d).","While this simple example illustrates the ambiguity problem of non-vowelized text, Arabic has the additional problem of non-diacritized text, where the same vowelization can lead to different meanings dependent on additional diacritical marks. For example, in Arabic a triliteral word \u201cK T B\u201d could have any of the following semantics:\n\n","In prior art, a morphological analyzer analyzes the word \u201cK T B\u201d and will offer the 5 different vowelization patterns and solutions above; ambiguity in this case must be resolved from a combination of syntax and semantics according to the pre-established collection of rules , mentioned above with reference to . These rules , including the empirically found exceptions  are, however, disadvantageously difficult to maintain up-to-date, or to extend to a new application due to the productive nature of any \u201cliving\u201d spoken language.","Further disadvantages of prior art are due to the fact that the development of morphological and syntactical analyzers requires a lexicon database. The lexicon should cover the entire language, and therefore its collection is not a trivial task, and requires the expertise of computational linguists. In previous efforts done by IBM Cario Scientific Center (1987) morphological, syntactical, and some semantical features for about 5700 Arabic roots have been collected.","Morphology is important because a \u201cliving\u201d language is \u201cproductive\u201d: In any given text one will encounter words and word forms that have not been seen before and that are not in any precompiled dictionary. Many of these new words are morphologically related to known words. It is important to be able to handle morphology in almost all languages, but it is absolutely essential for highly inflected languages.","The major types of morphological processes are inflection, derivation, and compounding: Inflections are the systematic modifications of a root form by means of prefixes and suffixes to indicate grammatical distinctions like singular and plural. Derivation is less systematic. It usually results in a more radical change of syntactic category, and it often involves a change in meaning. Compounding refers to the merging of two or more words into a new word. Further, words are organized into phrases, groupings of words that are clumped as a unit. Syntax is the study of the regularities and constraints of word order and phrase structure.","The above-mentioned prior art morpho-syntactical analyzer is able to handle only two types of the Arabic sentences, namely the Arabic verbal and nominal sentences, and generating the corresponding parse trees. These two sentence types can also be vowelized completely with a certain degree of ambiguity that needs to be resolved through a semantical analyzer.","Sakhr (www.sakhr.com), a Middle East based company, has developed a system for automatic diacritization of Arabic that depends on various levels for language processing and analysis. Starting from a morphological level and ending with disambiguation of word meanings, the method relies on an extensive basic research in the area of Natural Language Processing (NLP) and large linguistic databases that Sakhr has developed over many years. Disadvantageously, in this approach the databases can be maintained up-to-date only with a large amount of manual work and highly skilled staff due to the \u201cproductive\u201d nature of any language, as it was described above, and due to the even more problematic fact that Arabic is a highly inflected language.","1.3. Objectives of the Invention","It is thus an objective of the present invention to help overcome the above mentioned disadvantages.","This objective of the invention is achieved by the features stated in enclosed independent claims. Further advantageous arrangements and embodiments of the invention are set forth in the respective subclaims. Reference should now be made to the appended claims.","According to the broadest aspect of the invention a method for converting an input text given in an incomplete language into speech is disclosed, in which method a computer-aided graphem-phonem conversion is used, which is characterized by the steps of:\n\n","An \u201cincomplete\u201d language is hereby to be understood as a natural or artificial language, in which text is used, which does not contain a complete syntactic and semantic description of phrases. Thus, there is a lack of information, ie a gap between text in its usual written form and the fully elaborated, disambiguated version thereof, which is necessary for correctly converting said text into speech.","Also, a method for training a speech recognizer with an input text given in an incomplete language and corresponding speech data is disclosed, characterized by the steps of:\n\n","An advantageous additional step usable in both before-mentioned methods is done, when the text is completed according to a hierarchy of models giving higher priority to longer chunks of text, ie sentences, then multiword phrases, then words, and finally character groups.","This means in more detail to evaluate a given, pre-compiled so-called \u201cenrichment corpus\u201d for completing said input text of said incomplete language, wherein said corpus comprises a collection of language-specific characters, a collection of relevant character combinations ie words, and a collection of relevant sequences of predetermined character combinations, ie phrases and sentences according to a hierarchical evaluation scheme. This helps to make the inventional method efficient in runtime behaviour and to save storage space.","The core idea of the present invention is based on the assumption that, if a huge vowelized Arabic text corpus\u2014the before-mentioned enrichment corpus\u2014where all possible contexts of Arabic words and hence (implicitly) all possible Arabic sentences occur is available, we can exploit this corpus in order to vowelize any given un-vowelized Arabic text. However, in practise it is neither possible to find nor practical to store such a huge corpus; therefore, we propose to capture the statistics of a respective different corpus on various levels, namely on the sentence level, the phrase level, and the word and character level as well. During the process of automatic vowelization according to the present invention, the main task is to find the longer units first, ie sentences and phrases, and\u2014if not found\u2014fall back to statistical models for shorter units, i.e. words, and finally groups of characters, in that particular order.","In short words the invention provides a method based on an automatic statistical approach for the vowelization of Arabic text that may serve as input to the text-to-speech (TTS) component of a natural language processing system or may be used to improve prior art speech recognition systems during a training phase thereof.","Advantageously, due to its automatic runtime behaviour with no human intervention the inventional method allows the on-line addition of vowels to Arabic text without using any linguistic information and highly-skilled staff.","Statistical methods for the evaluation of the before-mentioned enrichment corpus include N-gram statistics, where N is preferably in the range from 1 to 5 including the interval limits.","In contrast to prior art, in this invention an entirely statistics based algorithm is disclosed for the automatic vowelization of Arabic text. The inventional method is thus a valuable tool for any automatic processing of the Arabic language and can overcome the drawbacks associated with both, the use of manually vowelized text and prior art rule-based vowelization methods.","With general reference to the figures and with special reference now to  a rough structure diagram is given for the purpose of a clear overview and contrast to prior art telling the skilled reader how the present invention is embedded into the prior art structure shown in . Instead of morpho-syntactic rules developed and maintained in prior art by highly-skilled language experts, the present invention applies automatic vowelization, which needs a training phase, before it can be applied in runtime. This training phase is symbolically depicted with the structural features of decision trees , a database  comprising language models generally abbreviated in the drawings as LM\u2014for words or characters and\/or for classes of words or characters, as well as a database  for storing frequently used phrases and sentences. During the training phase yet unvowelized text is vowelized according to the inventional method, checked for correctness and inserted into respective database fields of before-mentioned databases  and .","Then, the vowelization databases may be used during runtime together with the inventional vowelization algorithms of above-mentioned applications, which is symbolised by the central sequence of blocks , ,  and , identical to .","An additional preferred feature of the present invention is a phonetic check algorithm , which tests after the grapheme to phoneme conversion , if a generated phonetic description follows the syllable structure, which is specific for the language in use, ie in this embodiment the Arabic language.","Further advantageously, in order to adapt the contents of the above-mentioned words and phrases databases  and , respectively, additional short-term databases  and  and application-specific language models A, B are filled with correctly tested words and phrases respectively, together with the corresponding unvowelized words or phrases. These databases are preferably operated with dynamic updates and dynamic data delivery to the static words and phrases dictionaries (databases) , , in order to keep the contents of the dynamic databases small and concise.","With reference to  an overview representation for the training phase functional components according to this inventional embodiment is given next.","In the training phase vowelized text is basically compared to its unvowelized form, and scores are provided for each of the possible vowelized forms in proportion to the occurrences of a respective vowelized form. Thus, if in said training text a certain first vowelized word has 30 occurrences, and a different, second vowelized form of the same unvowelized text item has only a number of 10 occurrences, then said first vowelized form is awarded a three times higher score than the second one. Thus, for each unvowelized text item, ie sentences, phrases words and character groups smaller than entire words the different vowelized forms are stored together with their respective scores. A full set of such datasets is referred to then as a language model, as it is known from the field of speech recognition in a different context.","Following this principle or similar ones, a relatively large vowelized Arabic text corpus  is used to train the different statistical language models, abbreviated as LM, ie the word-level models stored in database A, and the character-level models stored in database B. It should be added that both models can also be based on class structures, which can be automatically derived with prior art methods. The skillful reader can think of the syntactical class (or part-of-speech) of a word (like e.g. noun, verb, etc.) as an example; character classes can be derived, for example, by grouping individual characters that represent similar speech sounds when spoken (e.g. plosive sounds, nasals, fricatives, etc.).","The before-mentioned data sources  and  are used as input for the training of stochastic language models, which uses preferably the formulae given later below with equations (3) to (7) and equations (10) to (12).","Further, a pre-established, application-specific set of sentences and phrases is stored in the static sentence and phrase dictionary .","These modelling methods result in the before-mentioned word and character language models and respective filled databases A and B. Preferably, these language models are also used to provide scores  for the selection of sentences and phrases that are stored in a dynamic dictionary . Those scores are provided on each level (sentence, phrase, word, character), and are initially computed in the training phase, but are repeatedly computed during runtime for an update of the dynamic dictionary as it is described in more detail further below. Further details for the application of the above-mentioned formulae are given later below.","The vowelization phase can be started after the training phase has led to dictionaries and stochastic models having a satisfying content good enough for the specific needs of the underlying application. In this phase vowels are added to non-vowelized sentences, phases, words or character groups. In particular, first, the sentence\/phrase models are utilised, if negative, then word models and finally character models. As an alternative to the stochastic N-gram language model, decision trees may by used, which is symbolized by the \u201cdecision tree\u201d symbol  in .",{"@attributes":{"id":"p-0067","num":"0084"},"figref":"FIG. 6"},"In particular, and with additional reference to , showing the control flow of the respective inventional method, the unvowelized Arabic text symbolized by the English transcription thereof is first fed, steps , , to a sentence comparator accessing the static sentence database , to check, step , if the whole sentence already exists in the reference sentences database. In case of a match\u2014see the YES branch of , the vowelized sentence is retrieved for selection to speech output; otherwise\u2014see the NO branch of , the sentence is passed to the phrase vowelizer, where it is chopped, step , into phrases that are compared to the phrases in the databases. A scoring technique is preferably used in order to favour the chunks of longer phrases relative to shorter ones. In case of a respective match of a phrase\u2014see the YES branch of decision , the vowelized phrase is retrieved for selection to speech output; otherwise\u2014see the NO branch of decision , a feedback to chop step  is done, in order to match a shorter phrase, cutting preferably the word at the current end of the phrase off from evaluation. This procedure is continued until phrases are matched, step , and picked for selection to speech output, or at least and preferably words are left unvowelized.","At the end of this stage, these unvowelized words will be fed, step  to a word vowelizer, which consults a respective N-gram\u2014here a bigram\u2014word vowelization model and outputs the vowelization of the word, step , if found, step .","Otherwise, ie, in case there will be any words left out non-vowelized, these words will be fed to a character vowelizer, where a word is chopped into characters, and N-gram character language models or decision trees are consulted to obtain a vowelization. Again, longer groups of characters are preferred for priority vowelization, according to the before-mentioned longest match principle. Thus, for each input sequence of characters the vowel sequence having a highest posterior probability is determined, and is issued at step , or, if no group of characters is found, a single character is output for synthetic speech output, step .","Optionally, the retrieved sentences phrases and words are fed back to the dynamic respective databases , , respectively, see back to , in order to store it in a cache status of quickly repeatable access, and the scores are updated in those databases preferably according to the degree how often they were found.","In summary, in this inventional embodiment the vowelization is performed on four levels; first on the sentence level, then on the phrase level with an optional focus on bigram evaluation, then on the word level and finally on the character level. All evaluation is done on a fully automized computer program level. The details of the vowelizations on the respective levels will be described in more detail next below. Such implementation details are of course only exemplary in nature.","The main task of the sentence and phrase vowelizer is to provide a highly accurate vowelization of entire sentences or phrases that are frequently used and\/or critical for a given application. For a fast lookup during application runtime, these sentences and phrases are stored in their unvowelized form in tree-structured dictionaries. The vowelized text can be obtained from the trees' leaves that are reached by traversing the trees given the unvowelized input text.","Advantageously, two different types of sentence and phrase vowelizers are provided. One or more static dictionaries will provide vowelized text for a small set of application-specific sentences and phrases that are always needed and always known to the system. For example, the Text-to-Speech output component of a natural language interface will always need to synthesise a couple of sentences or phrases, such as greeting (e.g. \u201cWelcome to the IBM customers helpline! How may I help you?\u201d) or goodbye (e.g. \u201cThank you for using the IBM customer's helpline! Goodbye!\u201d). These sentences and many more of them can be selected and vowelized by the evaluation of a representative training corpus using a prior art stochastic language model technology or they are supplied by an application developer.","A dynamic or cache dictionary will store vowelizations for sentences and phrases that are likely to be used by a particular application. Again, stochastic language modelling can be used to provide an initial set of phrases and their vowelization, but it is important to understand that the content of the dynamic dictionary and the associated vowelization can vary over time due to the update of the initial language model. Promising methods for the replacement of a phrase or sentence from the dynamic dictionary are: replacement of the most recently used entity, replacement of the least frequently used entity, replacement of the least probable entity, or random replacement of an entity.","The main task of the word vowelizer is to map a given unvowelized word onto a vowelized word. The basic inventional approach of a statistical approach to word vowelization will be described in more detail next below.","Denoting the sequence of unvowelized words as U=(u, . . . , u), and the sequence of vowelized words as V=(v, . . . , v), it is a well known fact from statistical pattern recognition theory, that a decision for the sequence\n\nmax()}\u2003\u2003(1)\n\nminimizes the average expected error. Applying Bayes' rule we can rewrite equation (1) as:\n",{"@attributes":{"id":"p-0078","num":"0095"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"msup":{"mi":"V","mo":"*"},"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mrow":[{"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},{"msub":{"mi":["max","V"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["V","U"],"mo":","}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"U"}}],"mo":"\/"}}}]}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"msub":{"mi":["max","V"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"V"}},{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["U","V"],"mo":"\u2758"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"U"}}],"mo":"\/"}],"mo":"\u00b7"}}}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{},"sub":["1","n"]},{"@attributes":{"id":"p-0079","num":"0096"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"v","mn":"1"},{"mi":["v","n"]}],"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"v","mn":"1"}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"2"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["v","i"]},{"mi":"v","mn":"1"}],"mo":"\u2758"},"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"msub":{"mi":"v","mrow":{"mi":"i","mo":"-","mn":"1"}}}}}}],"mo":"\u00b7"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}},"br":{},"sup":"25 ","sub":"i ","ul":{"@attributes":{"id":"ul0012","list-style":"none"},"li":{"@attributes":{"id":"ul0012-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0013","list-style":"none"},"li":"the unigram language model, where all histories are equivalent:"}}}},{"@attributes":{"id":"p-0080","num":"0098"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"v","mn":"1"},{"mi":["v","n"]}],"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"v","mn":"1"}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"2"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["v","i"]}}}}],"mo":"\u00b7"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"ul":{"@attributes":{"id":"ul0014","list-style":"none"},"li":{"@attributes":{"id":"ul0014-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0015","list-style":"none"},"li":"the bigram language model, where histories are equivalent, if they end in the same word:"}}}},{"@attributes":{"id":"p-0081","num":"0100"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"v","mn":"1"},{"mi":["v","n"]}],"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"v","mn":"1"}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"2"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["v","i"]},{"mi":"v","mrow":{"mi":"i","mo":"-","mn":"1"}}],"mo":"\u2758"}}}}],"mo":"\u00b7"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}},"ul":{"@attributes":{"id":"ul0016","list-style":"none"},"li":{"@attributes":{"id":"ul0016-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0017","list-style":"none"},"li":"the trigram language model, where histories are equivalent, if they end in the same two words:"}}}},{"@attributes":{"id":"p-0082","num":"0102"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"v","mn":"1"},{"mi":["v","n"]}],"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"v","mn":"1"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"v","mn":"1"},{"mi":"v","mn":"2"}],"mo":"\u2758"}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"3"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["v","i"]},{"mi":"v","mrow":{"mi":"i","mo":"-","mn":"2"}}],"mo":"\u2758"},"mo":",","msub":{"mi":"v","mrow":{"mi":"i","mo":"-","mn":"1"}}}}}}],"mo":["\u00b7","\u00b7"]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}}},"Note that by these approximations the number of probabilities to be estimated from a training corpus reduces significantly, e.g. to approximately 4\u00b710for a vocabulary size of |Q|=20000 for a bigram language model. Both, a further reduction and a better reliability for the prediction of text from unseen domains can be achieved by using a set of pair wise disjoint word classes, e.g. parts of speech, rather than words. For example, when using a set of 256 classes for a vocabulary of |Q|=20000 words, the number of parameters for the bigram class language model reduces to only 65536. It is also state of the art in stochastic language modeling to use n-gram mixture language models, which compute the probability of a sequence V as a weighted sum of (arbitrary) language models,",{"@attributes":{"id":"p-0084","num":"0104"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"v","mn":"1"},{"mi":["v","n"]}],"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},{"munder":{"mo":"\u2211","mi":"k"},"mo":"\u2062","mrow":{"msub":{"mi":["\u03c9","k"]},"mo":"\u2062","mrow":{"msub":{"mi":["P","k"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"v","mn":"1"},{"mi":["v","n"]}],"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":{},"sub":"k "},{"@attributes":{"id":"p-0085","num":"0105"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"munder":{"mo":"\u2211","mi":"k"},"mo":"\u2062","msub":{"mi":["\u03c9","k"]}},"mo":"=","mn":"1"}}},"br":{}},"Using a word bigram language model (cf. Equation (5)) for the estimation of P(v, . . . , v), assuming that the vowelization of udepends only on v, and taking into account that the maximization is independent of P(U), the joint probability in Equation (2) can be rewritten as:",{"@attributes":{"id":"p-0087","num":"0107"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"msup":{"mi":"V","mo":"*"},"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mrow":[{"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},{"msub":{"mi":["max","V"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"v","mn":"1"}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"2"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["v","i"]},{"mi":"v","mrow":{"mi":"i","mo":"-","mn":"1"}}],"mo":"\u2758"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["U","V"],"mo":"\u2758"}}}],"mo":"\u00b7"}}],"mo":"\u00b7"}}}]}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mrow":[{"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},{"msub":{"mi":["max","V"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"v","mn":"1"}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"2"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["v","i"]},{"mi":"v","mrow":{"mi":"i","mo":"-","mn":"1"}}],"mo":"\u2758"}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["u","i"]},{"mi":["v","i"]}],"mo":"\u2758"}}}}],"mo":"\u00b7"}}],"mo":"\u00b7"}}}]}}}}]}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}}},"According to the last line of Equation (8), the probabilities that must be estimated according to the invention from the vowelized corpus are the probabilities for each word of the corpus P(v), the word-to-word transition probabilities P(v|v), and the probabilities P(u|v) that a vowelized word vgenerates the unvowelized word us. For that purpose, a rich set of maximum likelihood methods for the estimation and smoothing of n-gram probabilities are available from the literature. Algorithms that efficiently search for the sequence V* can also be found in the literature; a dynamic programming based Viterbi beam-search or a stack search algorithm (A*-algorithm) are two frequently methods used therefore.","While in Equation (8) the idea of statistical vowelization was demonstrated using a single-word bigram language model, we propose the use of any n-gram language model, either word or class based, and the use of any combination of such language models according to Equation (7). Furthermore, we propose the use of domain specific vowelized text corpora for parameter estimation that allow obtaining more accurate vowelizations for a particular application. Further, we propose the dynamic update of parameters during the lifetime of the application by means of so-called cache language models.","The character vowelizer according to this preferred embodiment of the invention is used for the vowelization of unknown words, ie. unvowelized words (or sequences of characters) ufor which no vowelized form vis observable in the training corpora.","Let L=(l, . . . l) denote a sequence of letters of length n, where ldenotes the letter in the i-th position of an unvowelized word, let S=(s, . . . , s) denote a sequence of vowels, and let S=(S, . . . , S) denote the set of Arabic vowels (including the diacritical marks). Furthermore, like in the examples given in the introductory section for the purpose of explanation assume that Arabic words v are written using the usual in-line notation of Western languages, rather than putting vowels above or below the consonants, i.e. v=lsls. . . ls.","Again, we determine the best sequence of vowels for a given set of letters as the sequence having maximum probability given the observed sequence of letters:",{"@attributes":{"id":"p-0093","num":"0113"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"msup":{"mi":"S","mo":"*"},"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"msub":{"mi":["max","S"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["S","L"],"mo":"\u2758"}}}}}}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mrow":[{"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},{"msub":{"mi":["max","S"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["S","L"],"mo":","}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"L"}}],"mo":"\/"}}}]}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"msub":{"mi":["max","S"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["s","i"]},{"mi":"s","mrow":{"mi":"i","mo":"-","mn":"1"}}],"mo":"\u2758"},"mo":[",","\u2062",",",","],"mi":["\u2026","L"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"msub":{"mi":"s","mn":"1"}}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"L"}}],"mo":"\/"}}}}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}}},"Under the assumption that the i-th vowel depends only on L, but not on the i\u22121 previous vowels, and taking into account that the search in Equation (9) does not depend on P(L), the application of Bayes' rule gives the intuitive form",{"@attributes":{"id":"p-0095","num":"0115"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"S","mo":"*"},"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mrow":{"msub":{"mi":["max","S"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["s","i"]},{"mi":"s","mrow":{"mi":"i","mo":"-","mn":"1"}}],"mo":"\u2758"},"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"msub":{"mi":"s","mn":"1"}}}},{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["s","i"]},"mo":"\u2758","mi":"L"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["s","i"]}}}],"mo":"\/"}],"mo":"\u00b7"}}}}}}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}}},"Following the argument in the previous section, cf. Equations (3)-(6), the vowel sequence probability has to be computed from a limited history, i.e.: P(s|s, . . . s)=P(s, . . . , s), and the N-gram language modelling techniques described further above can be used.","A similar argument holds for the estimation of P(s|L): Since there are far too many letter sequences of length n, a limited context has to be used in order to obtain reliable estimates. For that purpose, we preferably limit L to the K letters that succeed and follow the letter land compute",{"@attributes":{"id":"p-0098","num":"0118"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["s","i"]},"mo":"=","mrow":{"msub":{"mi":["S","k"]},"mo":"\u2758","mi":"L"}}}},{"mi":"P","mo":"(","mrow":{"mrow":{"msub":{"mi":["s","i"]},"mo":"=","mrow":{"msub":[{"mi":["S","k"]},{"mi":"l","mrow":{"mi":["i","K"],"mo":"-"}}],"mo":"\u2758"}},"mo":[",",",","\u2062",",",",",",",",","\u2062",","],"msub":[{"mi":"l","mrow":{"mi":["i","K"],"mo":["-","+"],"mn":"1"}},{"mi":"l","mrow":{"mi":"i","mo":"-","mn":"1"}},{"mi":["l","i"]},{"mi":"l","mrow":{"mi":"i","mo":"+","mn":"1"}}],"mi":["\u2026","\u2026"],"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}]}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mi":{},"mo":["\u2062",")"],"mrow":{"msub":[{"mi":"l","mrow":{"mi":["i","K"],"mo":["+","-"],"mn":"1"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":"+"}}],"mo":","}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mfrac":{"mtable":[{"mtr":[{"mtd":{"mrow":{"mi":"C","mo":"(","mrow":{"msub":[{"mi":"l","mrow":{"mi":["i","K"],"mo":"-"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":["-","+"],"mn":"1"}},{"mi":"l","mrow":{"mi":"i","mo":"-","mn":"1"}},{"mi":["l","i"]},{"mi":["S","k"]},{"mi":"l","mrow":{"mi":"i","mo":"+","mn":"1"}}],"mo":[",",",","\u2062",",",",",",",",",",","\u2062",","],"mi":["\u2026","\u2026"],"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}]}}}},{"mtd":{"mrow":{"mrow":{"msub":[{"mi":"l","mrow":{"mi":["i","K"],"mo":["+","-"],"mn":"1"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":"+"}}],"mo":","},"mo":")"}}}]},{"mtr":[{"mtd":{"mrow":{"munder":{"mo":"\u2211","mrow":{"msub":{"mi":["s","j"]},"mo":"\u2208","mi":"S"}},"mo":"\u2062","mrow":{"mi":"C","mo":"(","mrow":{"msub":[{"mi":"l","mrow":{"mi":["i","K"],"mo":"-"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":["-","+"],"mn":"1"}},{"mi":"l","mrow":{"mi":"i","mo":"-","mn":"1"}},{"mi":["l","i"]},{"mi":["S","j"]},{"mi":"l","mrow":{"mi":"i","mo":"+","mn":"1"}}],"mo":[",",",","\u2062",",",",",",",",",",","\u2062",","],"mi":["\u2026","\u2026"],"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}]}}}}},{"mtd":{"mrow":{"mrow":{"msub":[{"mi":"l","mrow":{"mi":["i","K"],"mo":["+","-"],"mn":"1"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":"+"}}],"mo":","},"mo":")"}}}]}]}}}}]}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}},"br":{}},"Another advantageous option is to include the already inserted vowels into the stochastic model. In this case Equation (11) becomes",{"@attributes":{"id":"p-0100","num":"0120"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["s","i"]},"mo":"=","mrow":{"msub":{"mi":["S","k"]},"mo":"\u2758","mi":"L"}},"mo":[",",",","\u2062",","],"msub":[{"mi":"s","mn":"1"},{"mi":"s","mrow":{"mi":"i","mo":"-","mn":"1"}}],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},{"mi":"P","mo":"(","mrow":{"mrow":{"msub":{"mi":["s","i"]},"mo":"=","mrow":{"msub":[{"mi":["S","k"]},{"mi":"l","mrow":{"mi":["i","K"],"mo":"-"}}],"mo":"\u2758"}},"mo":[",",","],"msub":{"mi":"s","mrow":{"mi":["i","K"],"mo":"-"}}}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"msub":[{"mi":"l","mrow":{"mi":["i","K"],"mo":["-","+"],"mn":"1"}},{"mi":"s","mrow":{"mi":["i","K"],"mo":["-","+"],"mn":"1"}},{"mi":"l","mrow":{"mi":"i","mo":"-","mn":"1"}},{"mi":"s","mrow":{"mi":"i","mo":"-","mn":"1"}},{"mi":["l","i"]},{"mi":["S","k"]}],"mo":[",",",","\u2062",",",",",",",",",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}}},{"mtd":{"mrow":{"mi":{},"mo":["\u2062",")"],"mrow":{"msub":[{"mi":"l","mrow":{"mi":"i","mo":"+","mn":"1"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":["+","-"],"mn":"1"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":"+"}}],"mo":[",","\u2062",",",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mfrac":{"mtable":[{"mtr":[{"mtd":{"mrow":{"mi":"C","mo":"(","mrow":{"msub":[{"mi":"l","mrow":{"mi":["i","K"],"mo":"-"}},{"mi":"s","mrow":{"mi":["i","K"],"mo":"-"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":["-","+"],"mn":"1"}},{"mi":"s","mrow":{"mi":["i","K"],"mo":["-","+"],"mn":"1"}},{"mi":"l","mrow":{"mi":"i","mo":"-","mn":"1"}}],"mo":[",",",",",",",","\u2062",",",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}}},{"mtd":{"mrow":{"mrow":{"msub":[{"mi":"s","mrow":{"mi":"i","mo":"-","mn":"1"}},{"mi":["l","i"]},{"mi":["S","k"]},{"mi":"l","mrow":{"mi":"i","mo":"+","mn":"1"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":["+","-"],"mn":"1"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":"+"}}],"mo":[",",",",",",",","\u2062",",",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}},"mo":")"}}}]},{"mtr":[{"mtd":{"mrow":{"munder":{"mo":"\u2211","mrow":{"msub":{"mi":["s","j"]},"mo":"\u2208","mi":"S"}},"mo":"\u2062","mrow":{"mi":"C","mo":"(","mrow":{"msub":[{"mi":"l","mrow":{"mi":["i","K"],"mo":"-"}},{"mi":"s","mrow":{"mi":["i","K"],"mo":"-"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":["-","+"],"mn":"1"}},{"mi":"s","mrow":{"mi":["i","K"],"mo":["-","+"],"mn":"1"}}],"mo":[",",",",",",",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}}}},{"mtd":{"mrow":{"mrow":{"msub":[{"mi":"l","mrow":{"mi":"i","mo":"-","mn":"1"}},{"mi":["l","i"]},{"mi":"s","mrow":{"mi":"i","mo":"-","mn":"1"}},{"mi":["l","i"]},{"mi":["S","j"]},{"mi":"l","mrow":{"mi":"i","mo":"+","mn":"1"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":["+","-"],"mn":"1"}},{"mi":"l","mrow":{"mi":["i","K"],"mo":"+"}}],"mo":[",",",",",",",",",",",","\u2062",",",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}},"mo":")"}}}]}]}}}}]}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}}},"A further reduction of the number of parameters can be achieved by either the creation of letter classes, or by the clustering of similar letter contexts. The rational behind the latter is that some letter sequences will predict the same vowel or at least will behave similar. Decision trees and maximum entropy methods that are well documented in the statistical speech recognition literature will be used for this purpose.","Finally, Equation (10) requires a search algorithm that finds the best sequence of vowels among all candidate sequences. A Viterbi-like procedure can be readily implemented using P(s|s, . . . , s) as transition probabilities, and P(s|L)\/P(s) as output probabilities.","Further, advantageously, either an A* search algorithm or a N-best implementation of the Viterbi\u2014search can be applied to give the N-best vowel sequences; see Reference 4 for a detailed description of state-of-the-art search techniques. Then, in a post-poned evaluation step, an application-specific best-hit-selector may select the best match and provide an application specific resolve of ambiguities.","Optional Phonetic Check:",{"@attributes":{"id":"p-0104","num":"0000"},"ul":{"@attributes":{"id":"ul0018","list-style":"none"},"li":["The final vowelized text is passed to a phonetic transcriber, where the phonetic baseform of each word is obtained. The main idea here is to check the correctness of the obtained consonant-vowel-sequences by exploiting the fact that the syllabic structures in Arabic language are limited in number and easily detectable;","Every syllable begins with a consonant \/C\/ followed by a vowel \/V\/ which is called, sometimes, the nucleus of the syllable. These features facilitate the process of syllabification. In case the syllabification does not work for a phonetic transcription of a word, this means that the vowels sequence is not correct and the sequence should be fixed, e.g. by removing it from the n-best list of candidate vowelizations."]}},"The present invention can be realized in hardware, software, or a combination of hardware and software. A vowelization tool according to the present invention can be realized in a centralized fashion in one computer system or in a distributed fashion where different elements are spread across several interconnected computer systems. Any kind of computer system or other apparatus adapted for carrying out the methods described herein is suited. A typical combination of hardware and software could be a general purpose computer system with a computer program that, when being loaded and executed, controls the computer system such that it carries out the methods described herein.","It should be understood that a voice server improved according to the present invention may be operated in cooperation with a client computer system via an electronic network, e.g. Intranet or Internet, or also as a \u201cstand-alone\u201d system cooperating with an analogue or digital telephony interface and respective telephony applications.","The present invention can also be embedded in a computer program product, which comprises all the features enabling the implementation of the methods described herein, and which\u2014when loaded in a computer system\u2014is able to carry out these methods.","Computer program means or computer program in the present context mean any expression, in any language, code or notation, of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following:\n\n"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"3. BRIEF DESCRIPTION OF THE DRAWINGS","p":["The present invention is illustrated by way of example and is not limited by the shape of the figures of the drawings in which:",{"@attributes":{"id":"p-0049","num":"0066"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0050","num":"0067"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0051","num":"0068"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0052","num":"0069"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0053","num":"0070"},"figref":["FIG. 5","FIG. 4"]},{"@attributes":{"id":"p-0054","num":"0071"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0055","num":"0072"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
