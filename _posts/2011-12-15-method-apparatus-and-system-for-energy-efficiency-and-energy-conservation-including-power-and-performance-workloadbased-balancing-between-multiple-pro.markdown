---
title: Method, apparatus, and system for energy efficiency and energy conservation including power and performance workload-based balancing between multiple processing elements
abstract: An apparatus, method and system is described herein for efficiently balancing performance and power between processing elements based on measured workloads. If a workload of a processing element indicates that it is a bottleneck, then its performance may be increased. However, if a platform or integrated circuit including the processing element is already operating at a power or thermal limit, the increase in performance is counterbalanced by a reduction or cap in another processing elements performance to maintain compliance with the power or thermal limit. As a result, bottlenecks are identified and alleviated by balancing power allocation, even when multiple processing elements are operating at a power or thermal limit.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09304570&OS=09304570&RS=09304570
owner: Intel Corporation
number: 09304570
owner_city: Santa Clara
owner_country: US
publication_date: 20111215
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD","BACKGROUND","DETAILED DESCRIPTION"],"p":["This disclosure pertains to energy efficiency and energy conservation in integrated circuits, as well as code to execute thereon, and in particular but not exclusively, to balancing power and performance between processing elements.","Advances in semi-conductor processing and logic design have permitted an increase in the amount of logic that may be present on integrated circuit devices. As a result, computer system configurations have evolved from a single or multiple integrated circuits in a system to multiple hardware threads, multiple cores, multiple devices, and\/or complete systems on individual integrated circuits. Additionally, as the density of integrated circuits has grown, the power requirements for computing systems (from embedded systems to servers) have also escalated. Furthermore, software inefficiencies, and its requirements of hardware, have also caused an increase in computing device energy consumption. In fact, some studies indicate that computers consume a substantial amount of the entire electricity supply for the United States of America.","As a result, there is a vital need for energy efficiency and conservation associated with integrated circuits. And as servers, desktop computers, notebooks, ultrabooks, tablets, mobile phones, processors, embedded systems, etc. become even more prevalent (from inclusion in the typical computer, automobiles, and televisions to biotechnology), the effect of computing device sales stretches well outside the realm of energy consumption into a substantial, direct effect on economic systems.","Furthermore, as integrated circuit density has grown, the ability to consolidate multiple devices in a single integrated circuit and\/or package has similarly increased. As an illustrative example, system on chip (SOC) devices have recently gained popularity. However, often SOCs (and like components) integrate multiple devices into a single device\/circuit, but they also inherit each individual device's power, performance, and thermal constraints. Therefore, a device with a host processor and a graphics processor usually implements power savings policies individually and separately. And even if a power policy is defined for the entire package, it often only contemplates assumed power requirements for each of the individual devices without balancing power and performance amongst asymmetric cores to achieve maximum overall performance.","In the following description, numerous specific details are set forth, such as examples of specific types of specific processor and system configurations, specific hardware structures, specific architectural and micro architectural details, specific register configurations, specific structures for determining workloads, specific modules to determine and set performance\/power limits, specific types of performance limits, specific types of processing devices, specific types of program code, specific performance bottleneck assumptions, specific devices including asymmetric processors, etc. in order to provide a thorough understanding of the present invention. It will be apparent, however, to one skilled in the art that these specific details need not be employed to practice the present invention. In other instances, well known components or methods, such as specific and alternative processor architectures, specific logic circuits\/code for described algorithms, specific firmware code, specific interconnect operation, specific workload determination, specific frequency setting techniques, specific power limit determination and implementation algorithms, and other specific operational details of processors haven't been described in detail in order to avoid unnecessarily obscuring the present invention.","Although the following embodiments are described with reference to energy conservation and energy efficiency in specific integrated circuits, such as in computing platforms or microprocessors, other embodiments are applicable to other types of integrated circuits and logic devices. Similar techniques and teachings of embodiments described herein may be applied to other types of circuits or semiconductor devices that may also benefit from better energy efficiency and energy conservation. For example, the disclosed embodiments are not limited to desktop computer systems. And may be also used in other devices, such as handheld devices, systems on a chip (SOC), and embedded applications. Some examples of handheld devices include cellular phones, Internet protocol devices, digital cameras, personal digital assistants (PDAs), and handheld PCs. Embedded applications typically include a microcontroller, a digital signal processor (DSP), a system on a chip, network computers (NetPC), set-top boxes, network hubs, wide area network (WAN) switches, or any other system that can perform the functions and operations taught below. Moreover, the apparatus', methods, and systems described herein are not limited to physical computing devices, but may also relate to software optimizations for energy conservation and efficiency. As will become readily apparent in the description below, the embodiments of methods, apparatus', and systems described herein (whether in reference to hardware, firmware, software, or a combination thereof) are vital to a \u2018green technology\u2019 future balanced with performance considerations.","The method and apparatus described herein are for balancing power between multiple asymmetric devices to achieve maximum overall performance. Specifically, power balancing and allocation is primarily discussed below in reference to an integrated circuit and\/or package including a central\/host processor (or cores thereof) and a graphics processor (or cores thereof). Yet, the apparatuses and methods described herein are not so limited, as they may be implemented in conjunction with any integrated circuit device including any type of asymmetric or symmetric processing cores. For example, power balancing and allocation, as described herein, may be utilized between two host processing cores, between processing cores and memory, and\/or between integrated I\/O devices. And as discussed above, it may be utilized in servers, desktops, ultrabooks, tablets, small form-factor devices, handheld devices, SOCs, or embedded applications.","Referring to , an embodiment of an integrated circuit (or package) including multiple devices\/cores is illustrated. Integrated Circuit (IC)  includes any processor or processing device, such as a microprocessor, an embedded processor, a digital signal processor (DSP), a network processor, a handheld processor, an application processor, a co-processor, or other device to execute code. Processor , in one embodiment, includes at least two central, host, or processing cores\u2014core  and , which may be asymmetric cores or symmetric cores (the illustrated embodiment), as well as an Input\/Output (I\/O) device , such as a graphics processor (or cores thereof). However, IC  may include any number of processing elements that may be symmetric or asymmetric.","In one embodiment, a processing element refers to hardware or logic to support a software thread. Or in another embodiment a processing element refers to a core or processing logic of a device, such as a graphics processor\/core, network controller, embedded processor, microcontroller, etc. Other Examples of hardware central processing elements include: a thread unit, a thread slot, a thread, a process unit, a context, a context unit, a logical processor, a hardware thread, a core, and\/or any other element, which is capable of holding a state for a processor, such as an execution state or architectural state. In other words, a processing element, in one embodiment, refers to any hardware capable of being independently associated with code, such as a software thread, operating system, application, or other code. A physical processor typically refers to an integrated circuit, which potentially includes any number of other processing elements, such as cores or hardware threads.","A processing (or central processing unit) core often refers to logic located on an integrated circuit capable of maintaining an independent architectural state, wherein each independently maintained architectural state is associated with at least some dedicated execution resources. In contrast to cores, a hardware thread typically refers to any logic located on an integrated circuit capable of maintaining an independent architectural state, wherein the independently maintained architectural states share access to execution resources. As can be seen, when certain resources are shared and others are dedicated to an architectural state, the line between the nomenclature of a hardware thread and core overlaps. Yet often, a core and a hardware thread are viewed by an operating system as individual logical processors, where the operating system is able to individually schedule operations on each logical processor.","Physical package , as illustrated in , includes two processing cores, core  and . Here, core  and  are considered symmetric cores, i.e. cores with the same configurations, functional units, and\/or logic. In another embodiment, core  includes an out-of-order processor core, while core  includes an in-order processor core. However, cores  and  may be individually selected from any type of core, such as a native core, a software managed core, a core adapted to execute a native Instruction Set Architecture (ISA), a core adapted to execute a translated Instruction Set Architecture (ISA), a co-designed core, or other known core. Yet to further the discussion, the functional units illustrated in core  are described in further detail below, as the units in core  operate in a similar manner.","As depicted, core  includes two hardware threads and , which may also be referred to as hardware thread slots and . Therefore, software entities, such as an operating system, in one embodiment potentially view processor  as four separate processors, i.e. four logical processors or processing elements capable of executing four software threads concurrently. As alluded to above, a first thread is associated with architecture state registers , a second thread is associated with architecture state registers , a third thread may be associated with architecture state registers , and a fourth thread may be associated with architecture state registers . Here, each of the architecture state registers (, , , and ) may be referred to as processing elements, thread slots, or thread units, as described above. As illustrated, architecture state registers are replicated in architecture state registers , so individual architecture states\/contexts are capable of being stored for logical processor and logical processor . In core , other smaller resources, such as instruction pointers and renaming logic in rename allocater logic  may also be replicated for threads and . Some resources, such as re-order buffers in reorder\/retirement unit , ILTB , load\/store buffers, and queues may be shared through partitioning. Other resources, such as general purpose internal registers, page-table base register(s), low-level data-cache and data-TLB, execution unit(s) , and portions of out-of-order unit  are potentially fully shared.","Processor  often includes other resources, which may be fully shared, shared through partitioning, or dedicated by\/to processing elements. In , an embodiment of a purely exemplary processor with illustrative logical units\/resources of a processor is illustrated. Note that a processor may include, or omit, any of these functional units, as well as include any other known functional units, logic, or firmware not depicted. As illustrated, core  includes a simplified, representative out-of-order (OOO) processor core. But an in-order processor may be utilized in different embodiments. The OOO core includes a branch target buffer  to predict branches to be executed\/taken and an instruction-translation buffer (I-TLB)  to store address translation entries for instructions.","Core  further includes decode module  coupled to fetch unit  to decode fetched elements. Fetch logic, in one embodiment, includes individual sequencers associated with thread slots , , respectively. Usually core  is associated with a first Instruction Set Architecture (ISA), which defines\/specifies instructions executable on processor . Often machine code instructions that are part of the first ISA include a portion of the instruction (referred to as an opcode), which references\/specifies an instruction or operation to be performed. Decode logic  includes circuitry that recognizes these instructions from their opcodes and passes the decoded instructions on in the pipeline for processing as defined by the first ISA. For example, as discussed in more detail below decoders , in one embodiment, include logic designed or adapted to recognize specific instructions, such as transactional instruction. As a result of the recognition by decoders , the architecture or core  takes specific, predefined actions to perform tasks associated with the appropriate instruction. It is important to note that any of the tasks, blocks, operations, and methods described herein may be performed in response to a single or multiple instructions; some of which may be new or old instructions.","In one example, allocator and renamer block  includes an allocator to reserve resources, such as register files to store instruction processing results. However, threads and are potentially capable of out-of-order execution, where allocator and renamer block  also reserves other resources, such as reorder buffers to track instruction results. Unit  may also include a register renamer to rename program\/instruction reference registers to other registers internal to processor . Reorder\/retirement unit  includes components, such as the reorder buffers mentioned above, load buffers, and store buffers, to support out-of-order execution and later in-order retirement of instructions executed out-of-order.","Scheduler and execution unit(s) block , in one embodiment, includes a scheduler unit to schedule instructions\/operation on execution units. For example, a floating point instruction is scheduled on a port of an execution unit that has an available floating point execution unit. Register files associated with the execution units are also included to store information instruction processing results. Exemplary execution units include a floating point execution unit, an integer execution unit, a jump execution unit, a load execution unit, a store execution unit, and other known execution units.","Lower level data cache and data translation buffer (D-TLB)  are coupled to execution unit(s) . The data cache is to store recently used\/operated on elements, such as data operands, which are potentially held in memory coherency states. The D-TLB is to store recent virtual\/linear to physical address translations. As a specific example, a processor may include a page table structure to break physical memory into a plurality of virtual pages.","Here, cores  and  share access to higher-level or further-out cache , which is to cache recently fetched elements. Note that higher-level or further-out refers to cache levels increasing or getting further way from the execution unit(s). In one embodiment, higher-level cache  is a last-level data cache\u2014last cache in the memory hierarchy on processor \u2014such as a second or third level data cache. However, higher level cache  is not so limited, as it may be associated with or include an instruction cache. A trace cache\u2014a type of instruction cache\u2014instead may be coupled after decoder  to store recently decoded traces.","In the depicted configuration, processor  also includes bus interface module . Historically, controller , which is described in more detail below, has been included in a computing system external to processor . In this scenario, bus interface  is to communicate with devices external to processor , such as system memory , a chipset (often including a memory controller hub to connect to memory  and an I\/O controller hub to connect peripheral devices), a memory controller hub, a northbridge, or other integrated circuit. And in this scenario, bus  may include any known interconnect, such as mutli-drop bus, a point-to-point interconnect, a serial interconnect, a parallel bus, a coherent (e.g. cache coherent) bus, a layered protocol architecture, a differential bus, and a GTL bus.","Memory  may be dedicated to processor  or shared with other devices in a system. Common examples of types of memory  include dynamic random access memory (DRAM), static RAM (SRAM), non-volatile memory (NV memory), and other known storage devices. Similar to the discussion above, I\/O device(s)  have been traditionally included in a computing system external to a central processing unit. Here, device  may include a graphic accelerator, processor or card coupled to a memory controller hub, data storage coupled to an I\/O controller hub, a wireless transceiver, a flash device, an audio controller, a network controller, or other known device.","Note however, that in the depicted embodiment, the controller  and I\/O device  are illustrated as part of processor . Recently, as more logic and devices are being integrated on a single die, such as System on a Chip (SOC), each of these devices may be incorporated on processor  to form a single package or single integrated circuit with multiple devices. For example in one embodiment, memory controller hub  and a graphics processor\/device  are included on the same package and\/or die with a central processor unit to form IC , which may be referred to herein as processor  that includes an integrated memory controller  and graphics device . Here, a portion of the core (an on-core portion) includes one or more controller(s)  for interfacing with other devices such as memory  and\/or a graphics device . The configuration including an interconnect and controllers for interfacing with such devices is often referred to as an on-core (or un-core configuration). As an example, bus interface  includes a ring interconnect with a memory controller for interfacing with memory  and a graphics controller for interfacing with graphics processor . And graphics processor  may even have direct access to cache . Yet, in the SOC environment, even more devices, such as the network interface, co-processors, memory , graphics processor , and any other known computer devices\/interface may be integrated on a single die or integrated circuit to provide small form factor with high functionality and low power consumption. And as discussed in more detail below, when power policies are developed for the entire IC , the techniques for power balancing between prominent, integrated devices, such as cores - and graphics device  become even more important.","In one embodiment, processor  is capable of executing a compiler, optimization, and\/or translator code  to compile, translate, and\/or optimize application code  to support the apparatus and methods described herein or to interface therewith. A compiler often includes a program or set of programs to translate source text\/code into target text\/code. Usually, compilation of program\/application code with a compiler is done in multiple phases and passes to transform hi-level programming language code into low-level machine or assembly language code. Yet, single pass compilers may still be utilized for simple compilation. A compiler may utilize any known compilation techniques and perform any known compiler operations, such as lexical analysis, preprocessing, parsing, semantic analysis, code generation, code transformation, and code optimization.","Larger compilers often include multiple phases, but most often these phases are included within two general phases: (1) a front-end, i.e. generally where syntactic processing, semantic processing, and some transformation\/optimization may take place, and (2) a back-end, i.e. generally where analysis, transformations, optimizations, and code generation takes place. Some compilers refer to a middle, which illustrates the blurring of delineation between a front-end and back end of a compiler. As a result, reference to insertion, association, generation, or other operation of a compiler may take place in any of the aforementioned phases or passes, as well as any other known phases or passes of a compiler. As an illustrative example, a compiler potentially inserts operations, calls, functions, etc. in one or more phases of compilation, such as insertion of calls\/operations in a front-end phase of compilation and then transformation of the calls\/operations into lower-level code during a transformation phase. Note that during dynamic compilation, compiler code or dynamic optimization code may insert such operations\/calls, as well as optimize the code for execution during runtime. As a specific illustrative example, binary code (already compiled code) may be dynamically optimized during runtime. Here, the program code may include the dynamic optimization code, the binary code, or a combination thereof.","Similar to a compiler, a translator, such as a binary translator, translates code either statically or dynamically to optimize and\/or translate code. Therefore, reference to execution of code, application code, program code, or other software environment may refer to: (1) execution of a compiler program(s), optimization code optimizer, or translator either dynamically or statically, to compile program code, to maintain software structures, to perform other operations, to optimize code, or to translate code; (2) execution of main program code including operations\/calls, such as application code that has been optimized\/compiled; (3) execution of other program code, such as libraries, associated with the main program code to maintain software structures, to perform other software related operations, or to optimize code; or (4) a combination thereof.","In one embodiment, power control module , which may include hardware, firmware, software, or a combination thereof, is able to balance and allocate power\/frequency between devices of IC  to achieve maximum performance; even when IC  is under a power limit. Additionally, such allocation, in one scenario, is based on actual workload of devices, instead of assumptions of workload or power consumption. For example, a power limit (either a maximum power limit or a reduced limit during power saving modes) is provided for IC . Previously, a maximum current (and as a corollary frequency) was reduced for all processing elements of IC  to meet the power limit. Instead, in this scenario, a device that is determined to be a bottleneck for performance, such as throughput performance as viewed from the perspective of an application, is allocated more current\/frequency, while the other competing device is capped\/limited to ensure the power limit is still met. In other words, a dynamic balance is continuously stuck between devices to ensure the device that needs more resources for better overall performance is allocated such extra resources, while other devices are \u2018balanced\u2019 (reduced in performance) to meet a given power limit.","As a specific illustrative example, assume that I\/O device  is a graphics processor. Here, processing cores ,  and graphics device  form a producer and consumer relationship. Cores ,  receive and process instructions; some of which are offloaded to graphics processor  for processing (e.g. CPU cores ,  send a pixel to GPU  to be textured, GPU  calculates a texture of the pixel to obtain a texel). Therefore, if the CPU cores ,  are overworking (i.e. producing too much work for the GPU  to handle at its current performance level), then overall performance from a maximum potential is degraded (the extra performance that the CPU cores - are utilizing to overproduce could have been allocated to GPU . Note that this overproducing scenario potentially occurs even when CPU cores ,  are operating at the same or lower frequency than GPU . And inversely, if CPU cores ,  are under-producing (i.e. not generating enough work to keep GPU  busy), then once again the maximum potential performance is likely not being achieved. Moreover, the producer-consumer relations may also be view from the other perspective, where CPU cores ,  are to consume graphics output from producer  before CPU cores ,  can make further forward progress.","Therefore, in one embodiment, processing workload of cores - and graphics workload of graphics device  are determined. For example, an amount of activity (e.g. number of cycles active) over a quantum or period of time is determined for both cores - and graphics core . And based on those workloads, performance of cores - and graphics core  are balanced to achieve maximum overall performance. As an illustrative example, if one of the devices (CPU cores - or GPU cores ) are working at or near the maximum, while the other device is not fully occupied, then the near maximum device is allocated more performance (e.g. more frequency).","Note that when a power limit exists and IC  is not up against the power limit, the immediately aforementioned increase in performance may not include a counterbalancing reduction in the other device. For example, assume GPU cores  are fully occupied (all cycles during a period of time are active) and CPU cores - are only active 70% of the cycles during the period of time. In this scenario, CPU cores - are not working at their full capacity, but are still overproducing for GPU . When IC  is not at a set power limit, then GPU  frequency, in one embodiment, is increased to meet the overproduction of CPU -. However, once GPU  starts consuming at a higher rate, CPU cores - are able to produce at even a higher rate. And if a power or thermal limit for IC  is reached, then if GPU  is to increase frequency (e.g. from the same assumptions above), then CPU -, in one embodiment, are limited\/capped to ensure the power or thermal limit is not exceeded. However, the power and performance within that limit are intelligently allocated to ensure maximum, \u2018balanced\u2019 performance.","Referring to , embodiments of a computer system configuration adapted to include processors to balance performance and power between processing element\/cores on integrated circuits is illustrated. In reference to , an illustrative example of a two processor system  with an integrated memory controller and Input\/Output (I\/O) controller in each processor ,  is illustrated. Note that processors ,  each may include integrated graphics processors\/cores, as discussed above. Although not discussed in detail to avoid obscuring the discussion, platform  illustrates multiple interconnects to transfer information between components. For example, point-to-point (P2P) interconnect , in one embodiment, includes a serial P2P, bi-directional, cache-coherent bus with a layered protocol architecture that enables high-speed data transfer. Moreover, a commonly known interface (Peripheral Component Interconnect Express, PCIE) or variant thereof is utilized for interface  between I\/O devices , . However, any known interconnect or interface may be utilized to communicate to or within domains of a computing system.","Turning to  a quad processor platform  is illustrated. As in , processors - are coupled to each other through a high-speed P2P interconnect . And processors - include integrated controllers -.  depicts another quad core processor platform  with a different configuration. Here, instead of utilizing an on-processor I\/O controller to communicate with I\/O devices over an I\/O interface, such as a PCI-E interface, the P2P interconnect is utilized to couple the processors and I\/O controller hubs . Hubs  then in turn communicate with I\/O devices over a PCIE-like interface.","Referring next to , an embodiment of a logical representation of modules to balance power between multiple processing elements to achieve maximum performance is depicted. Although only two cores are illustrated in , any type of processing element and number thereof may be utilized. For example, in the most commonly discussed scenario herein (a CPU and GPU), both the CPU and GPU potentially includes multiple cores. And as a result, workload determination (which is described in more detail below) may be in reference to individual cores or groupings of cores, such as a workload in regard to all CPU cores in comparison to all GPU cores.","However, to further the discussion in regards to , core  is referred to below as a CPU core (or collection of CPU cores), while core  is referred to as a GPU core (or collection of GPU cores). It is also worth noting that although the discussion herein primarily focuses on power balancing between asymmetric cores; such balancing, in one embodiment, is also utilized between cores of the same type. For example, assume cores  and  are both CPU cores and are executing related threads (e.g. producer and consumer related threads, such as a main thread of a single application executing on core  and a run-ahead helper thread for the single application execution on core , where the run-ahead helper thread produces information to be consumed by the main thread). Consequently, power\/frequency balancing may be performed to achieve maximum performance between the threads (e.g. if core  executing the helper thread is running too far ahead, such as acquiring too many future locks or prefetching too much information, then core 's frequency is decreased, while core 's frequency is increased to consume at a higher rate and catch up with the prefetching of core ).","Continuing the discussion of , workload module  is to determine a workload of core , core , or both. Determining a workload includes any known method for determining activity of hardware, a workload of software, a number of instructions executed or to execute, or any other metric for determining use of a processing element. As one example, a workload for core  includes an amount time core  is active during a period of time. Activity may be determined as when core  is in an active performance state versus a sleep or low power state or a number of cycles executing or performing operations.","As a corollary to the multitude of potential workload measurements, the components to measure such workload may also vary. To further discussion of the potential components for workload measurement, it's assumed below that workload is measured as a number of active cycles over a period of time (i.e. a total number of cycles for measurement). As a first example, a hardware register, counter, and\/or accumulator associated with each of cores ,  track active cycles. And upon expiration of a period of measurement, the number of active cycles is determined for the period. As another example, a microcontroller executes code that causes hardware to track active cycles over a period. Moreover, microcode, privileged-level code, application code, or other code may be executed to similarly cause hardware to track active cycles over the period of measurement.","In one embodiment, power module  is to dynamically balance performance between cores ,  based on workload determination from workload module . Balancing \u2018performance\u2019 between cores includes modifying any metric or factor that affects core performance and\/or power. For example, if separate power delivery is provided for CPU core  from GPU core , then the power delivery or limits thereof may be altered to balance performance there between. However, often when CPU core  and GPU core  are included on the same package, power delivery is not separate. Therefore, as another example frequency between the individual cores is balanced. In this scenario, power module  is to dynamically tune frequency allocation between core  and core  based on a power limit for an integrated circuit or package including cores ,  and one or more workloads as determined by workload module . Note that frequency may not be directly altered in some embodiments, but rather an underlying metric, such as current, is altered to affect the change in frequency.","As one example, power module  is to increase frequency for core  and\/or decrease frequency for core  in response to the dynamically determined workload for core  being greater the workload for core . Here, if one core is demonstrating a greater need of performance due to a higher workload, then its frequency is increased. And when an integrated circuit including cores  and  are up against a power limit, then the core demonstrating less need for performance requirements is decreased in frequency to counterbalance\/compensate for the increase in frequency to the other core. In another example, power module  is to increase frequency for core  and\/or decrease frequency for core  in response to determining core  is a performance bottleneck. In other words, core  is limiting overall performance, because core  is relying on output from core  to continue further execution. But core  is operating to slowly. Then, core 's frequency is increased to ensure overall performance is accelerated. Yet again, if a power limit is encountered, then core 's frequency may be similarly decreased to ensure the power limit is enforced. Note that in some embodiment, hysteresis may be utilized to avoid a \u2018see-saw\u2019 effect, where power\/performance balance bounces between settings. For example, CPU frequency is raised quickly (e.g. upon first detection of requiring more performance\/frequency), while reduction of frequency from lack of demand is determined over multiple consecutive (or non-consecutive but closely related) periods of measurement.","In one embodiment, a basic assumption of potential bottlenecks allows the process to be simplified. For example, assume core  is a collection of CPU cores and core  is a collection of GPU cores. Here, as an exemplary base assumption, it's assumed that when GPU cores  are operating at a maximum (i.e. fully occupied), then GPU cores  are the performance bottleneck (i.e. GPU is working as fast as it can under the current limits, but it's not completing the work fast enough). As a result of the assumption, if GPU cores  are operating over a workload\/activity threshold, it's considered to be a bottleneck. So, the frequency of the GPU is increased. And if a power limit is reached, the CPU frequency is capped (or decreased) to ensure the power limit is upheld. However, if workload module  determines the GPU 's frequency is below a threshold (i.e. not working\/consume at maximum capacity), then any CPU 's frequency cap or limit is increased to cause more production, such that GPU cores  workload increases accordingly.","Any hardware, firmware, microcode, software, or combination thereof may perform the balancing, tuning, and\/or allocating described above. As one example, the balancing policies are implemented in a Power Control Unit (PCU), which often includes a microcontroller configured to execute collocated power code. Here, the power code, when executed by the microcontroller, loads workload information from workload module , determines an appropriate balancing action (e.g. increasing frequency of cores ), and initiates the balancing action (e.g. indicates the frequency increase, enabling hardware\/logic to actually increase the frequency). As another example, code and hardware are both utilized for balancing. In one scenario, a graphics driver for GPU cores  loads the graphics workload information from workload module  (e.g. hardware activity tracking logic). And either the device driver, a privileged level power service routine (e.g. OS service routine), or other code\/hardware determines if a balancing action is to be taken at least based on the graphics workload information. Additionally, CPU core  workload information may also be taken into account to determine the appropriate balancing action.","In one embodiment, the balancing decision maker also takes into account a software hint to determine how performance\/power is to be balanced among processing elements. For example, GPU cores  workload information is determined, as discussed above. And an executing application provides a software hint of whether a future workload (i.e. a portion of the application) is to include a larger or smaller graphics workload in comparison to a previous portion of the application that resulted in the measured workload. Here, if the GPU is close to a threshold (or close to fully occupied) and the applicant hints that a next period is associated with a greater graphics workload, then the power module  may determine that GPU frequency is to be increased and CPU frequency is to be capped; even if the previous activity has not reached an activity threshold. As a result, the hint aids in predicting future activity; instead of relying purely on previous, measured workload information. Note that a hint may be provided through an Instruction Set Architecture defining such a hint, an Application Program Interface (API), or other known structure\/method for communicating information from software to hardware.","Turning to , another embodiment of a logical representation of modules to balance power among processing devices to achieve maximum overall performance is illustrated. In one embodiment, Graphics Processing Unit (GPU) cores  and Central Processing Unit (CPU) cores  are on the same die, integrated circuit, package, or other grouping, which may be subject to an overall power or performance limit. For example, a thermal design power (TDP) power limit is often provided for an integrated circuit or package. In other words, a power limit is provided for a maximum thermal dissipation capacity of the integrated circuit. However, in another embodiment, GPU cores  and CPU cores  are separate integrated circuits to be inserted in a single platform, which may similarly be subject to an overall platform power and\/or thermal limit.","Workload monitor  is illustrated as associated with GPU cores , and workload monitor  is illustrated as associated with CPU cores . As stated above, workload monitor  and  may be implemented in hardware associated with GPU cores  and CPU cores , workload code to be executed by a micro-controller, and\/or code (e.g. micro-code, driver code, other code) executed on GPU Cores  and\/or CPU cores .","Device driver , which may be executing on CPU cores , is to read\/load workload information from workload monitor . For example, a number of cycles GPU cores  are active is held in a register, counter, and\/or accumulator in GPU . Device driver  loads the count every period and determines GPU  activity. Here, in one embodiment, device driver  makes the balancing decision based on the graphics activity from workload monitor . In one scenario, if the workload activity is greater than a threshold, then driver  indicates that GPU cores  are occupied (i.e. a fully active and may be a bottleneck). In this embodiment, power management\/balancing module  (hardware, firmware, code, or a combination thereof) receives the occupied indication and performs the appropriate balancing action (e.g. capping CPU cores  frequency and\/or increasing GPU core frequency ). As specific illustrative examples, power module  includes a PCU, power code, microcode, application code, privileged level code, an Operating System (OS) power management service, etc.","In another embodiment, power management module  (e.g. such as a privileged power management service) loads CPU activity information from monitor(s) . And with GPU cores  activity information either directly from workload monitor  or device driver , power management module  makes the balancing decision. And module  also either directly implements the balancing decision or indicates that the balancing is to be performed by some other module, unit, or entity.","As one example, power management module , which in some embodiments encompasses device driver , sets a performance limit for one or more of the CPU cores  and\/or GPU cores . Here, assume that an overall power or thermal limit for GPU cores  and CPU cores  has been reached. And during workload measurement, it's determined that GPU cores  activity exceeds an occupied threshold. As an illustrative example of a threshold, a number of cycles to consider a GPU occupied may range from over 50% of cycles in a period active to 100% of cycles in a period active. For example, assume the activity threshold is 95% activity, then if workload monitor  detects 96%-100% active cycles, then it's determined the activity threshold is exceeded.","As a result, a performance limit (e.g. a maximum operating frequency for CPU cores , a maximum turbo frequency for CPU cores , a maximum current for CPU cores , and a maximum energy consumption for CPU cores ) is set. This action alone may be enough to balance the production of CPU cores  to match the consumption rate of GPU . However, in another embodiment, a performance metric for GPU cores , which may be the same performance metric limited for CPU cores  or a different performance metric, is increased. Here, the increase for GPU  is intelligently selected to balance with CPU cores 's increase, such that the power or thermal limit that has already been encountered is not exceeded. Inversely, if the GPU workload is below a threshold and a cap\/limit has been placed on CPU cores  from a previous period, then that cap is either increased or removed. Essentially, if GPU cores  is being under-utilized, it's assumed that CPU  is not producing at a fast enough rate to fully utilize GPU . Therefore, CPU  is increased or allowed to run-to-completion to generate more work for GPU cores . Also much like the discussion above, if a power limit is reached, then GPU 's performance metric, such as frequency, may be reduced, since it doesn't need full performance to complete its workload that was below an activity threshold.","As can be seen from above, the power consumption that has reached a limit is intelligently redistributed among processing elements to achieve an optimal balance that provides the best overall performance. In contrast, if power wasn't balanced as discussed herein, then the power limit is enforced but one of the processing elements may act as a performance bottleneck that limits overall performance. In contrast, such intelligent redistribution enables bottlenecks to be alleviated, even under power constraints, which enables maximum performance under the most efficient energy consumption constraints.","Moving to , an embodiment of modules and\/or a representation of a flow diagram for balancing power between a processor and graphics processor to achieve maximum performance within a power limit is depicted. Note that the flows (or modules) are illustrated in a substantially serial fashion. However, both the serial nature of these flows, as well as the depicted order, is not required. Also, flows are illustrated in a substantially linear. However, the flows may be performed in parallel over the same time period. In addition, any of the illustrated flows or logical blocks may be performed within hardware, software, firmware, or a combination thereof. As stated above and below, each flow, in one embodiment, represents a module, portion of a module, or overlap of modules. Moreover, any program code in the form of one or more instructions or operations, when executed, may cause a machine to perform the flows illustrated and described below. For example, in one embodiment, program code includes a device driver, such as a graphics driver; a privileged service routine, a portion of power code for a Power Control Unit, a portion of micro-code for a processor, or a combination thereof.","In flow , a graphics processor and or central processor workload over a period of time is received (or loaded) from one or more workload monitors. As one example, an activity count is read\/loaded from a hardware register in the devices. Yet, any known method of tracking workload or activity over a period of time may be utilized.","Then in flow , it's determined if a limit, such as a thermal design power (TDP) limit, is reached. Although any power or energy limit may be encountered, often a power limit based on electrical and thermal limits for a package including the GPU and CPU (i.e. TDP limit for the package) is utilized. If the power limit has not been reached and either the CPU or GPU workload indicates an increase in performance is needed (flow ), then the power or performance metric, such as frequency, is increased up to a maximum allowable to accommodate the workload in flow . Here, if there is no concern of hitting a power limit, then a potential bottleneck is eliminated through purely providing more performance. In contrast, if no performance increase is needed, as indicated by the workloads from flow , then the flow resets to measure workload and read once again in flow  after a subsequent period. Note that in flow  it may instead be determined that performance of a device, such as a GPU, may be lowered and the GPU would still be able to accommodate the measured workload. As a result, GPU performance is reduced, which in turn decreases power consumption, while maintaining workload performance.","However, if a limit has been encountered, then the power and\/or performance is balanced to alleviate as much of a bottleneck as possible. Therefore, in flow  it's determined if the loaded GPU workload is greater than a workload\/activity threshold. If the GPU workload is above the threshold, then in flow  a performance limit is imposed on the CPU. In other words, the GPU is being overworked and the CPU's production is capped. As can be seen, one way to alleviate the GPUs workload is to cap CPU production. However, to further alleviate the bottleneck, GPU performance, such as frequency is increased. Because of the increase in GPU frequency, the GPU power consumption is increased. And without the decrease or capping of CPU frequency, which results in lower CPU power consumption, the encountered TDP limit would be exceeded. However, the balancing of capping CPU performance and the similar increase in GPU frequency ensures the TDP limits are not exceeded. But instead, the available power is intelligently redistributed among the GPU and CPU to obtain the best performance based on actual, measured GPU workload.","Yet, if the GPU workload is not greater than a threshold, then in flow  its determined if a previous CPU limit has been imposed. If no limit has been imposed, then the CPU continues to operate according to existing polices in flow . However, if the GPU is below an activity threshold and the CPU has a current performance limit, then the limit is increased and\/or removed in flow . As discussed above, in this scenario the GPU is not working hard, so the production of the CPU is increased. Consequently, if no CPU cap is in place, the low graphics activity may be due to an application that is not graphics intensive. In contrast, if a CPU performance limit has been imposed, then it is increased to potentially also increase the CPU's production for the GPU.","Referring to , another embodiment of modules and\/or a representation of a flow diagram for balancing power between a processor and graphics processor to achieve maximum performance within a power limit is depicted. Similar to  and flow , a processing workload activity for a central processor and a graphics workload activity for a graphics processor is determined. And in flow , as in flow , it's determined if a platform, package, integrated circuit, or other form holding the central processor and graphics processor has reached a power limit, a thermal limit, or combination thereof (e.g. a TDP limit).","In flow , it's determined if the graphics processor is a performance bottleneck based at least on the graphics workload activity determined in flow . As one example, such a determination is based on whether the graphics workload activity is greater than a graphics activity threshold. Here, if the activity is greater than a threshold it's assumed that the GPU is a bottleneck. However, if the activity is less than the threshold, then it's assumed for flow  that the CPU is the bottle neck. However, such comparisons of activity to a threshold are not required. For example, workloads may be directly compared. And if one is higher than the other, it is determined to be the bottleneck.","Therefore, in flow , a processor performance limit for the central processor is set below a previous processor performance limit in response to determining the graphics processor is the performance bottleneck in flow . Or in flow , a graphics performance limit for the graphics processor is set below a previous graphics processor performance limit in response to determining the central processor is the performance bottleneck in flow . And similar to the discussion above, the competing device may be increased to further alleviate any bottleneck.","Although the discussion has primarily focused on competing devices being central processors and graphics processors, any devices that have a producer-consumer relationship, contend for power or performance resources, or are regulated under an overall power or thermal limit may similarly have power\/performance balanced between them. Therefore, available power is intelligently allocated between devices to provide the best possible performance from an overall perspective, instead of each individual component being charged with its own power and performance regulation without being cognizant of their application level contribution or monopolization of resources.","A module as used herein refers to any combination of hardware, software, and\/or firmware. As an example, a module includes hardware, such as a micro-controller, associated with a non-transitory medium to store code adapted to be executed by the micro-controller. Therefore, reference to a module, in one embodiment, refers to the hardware, which is specifically configured to recognize and\/or execute the code to be held on a non-transitory medium. Furthermore, in another embodiment, use of a module refers to the non-transitory medium including the code, which is specifically adapted to be executed by the microcontroller to perform predetermined operations. And as can be inferred, in yet another embodiment, the term module (in this example) may refer to the combination of the microcontroller and the non-transitory medium. Often module boundaries that are illustrated as separate commonly vary and potentially overlap. For example, a first and a second module may share hardware, software, firmware, or a combination thereof, while potentially retaining some independent hardware, software, or firmware. In one embodiment, use of the term logic includes hardware, such as transistors, registers, or other hardware, such as programmable logic devices.","A value, as used herein, includes any known representation of a number, a state, a logical state, or a binary logical state. Often, the use of logic levels, logic values, or logical values is also referred to as 1's and 0's, which simply represents binary logic states. For example, a 1 refers to a high logic level and 0 refers to a low logic level. In one embodiment, a storage cell, such as a transistor or flash cell, may be capable of holding a single logical value or multiple logical values. However, other representations of values in computer systems have been used. For example the decimal number ten may also be represented as a binary value of 1010 and a hexadecimal letter A. Therefore, a value includes any representation of information capable of being held in a computer system.","Moreover, states may be represented by values or portions of values. As an example, a first value, such as a logical one, may represent a default or initial state, while a second value, such as a logical zero, may represent a non-default state. In addition, the terms reset and set, in one embodiment, refer to a default and an updated value or state, respectively. For example, a default value potentially includes a high logical value, i.e. reset, while an updated value potentially includes a low logical value, i.e. set. Note that any combination of values may be utilized to represent any number of states.","The embodiments of methods, hardware, software, firmware or code set forth above may be implemented via instructions or code stored on a machine-accessible, machine readable, computer accessible, or computer readable medium which are executable by a processing element. A non-transitory machine-accessible\/readable medium includes any mechanism that provides (i.e., stores and\/or transmits) information in a form readable by a machine, such as a computer or electronic system. For example, a non-transitory machine-accessible medium includes random-access memory (RAM), such as static RAM (SRAM) or dynamic RAM (DRAM); ROM; magnetic or optical storage medium; flash memory devices; electrical storage devices; optical storage devices; acoustical storage devices; other form of storage devices for holding information received from transitory (propagated) signals (e.g., carrier waves, infrared signals, digital signals); etc, which are to be distinguished from the non-transitory mediums that may receive information there from.","Reference throughout this specification to \u201cone embodiment\u201d or \u201can embodiment\u201d means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Thus, the appearances of the phrases \u201cin one embodiment\u201d or \u201cin an embodiment\u201d in various places throughout this specification are not necessarily all referring to the same embodiment. Furthermore, the particular features, structures, or characteristics may be combined in any suitable manner in one or more embodiments.","In the foregoing specification, a detailed description has been given with reference to specific exemplary embodiments. It will, however, be evident that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the appended claims. The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense. Furthermore, the foregoing use of embodiment and other exemplarily language does not necessarily refer to the same embodiment or the same example, but may refer to different and distinct embodiments, as well as potentially the same embodiment."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The present invention is illustrated by way of example and not intended to be limited by the figures of the accompanying drawings.",{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
