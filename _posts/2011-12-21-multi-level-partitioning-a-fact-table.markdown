---
title: Multi level partitioning a fact table
abstract: Inputs are received. The inputs are a star schema data model comprising a non-partitioned fact table (F) comprising fields (f, f, . . . f); a set of queries (Q=q, q, . . . q); and a set of weights for each query in Q. Each weight defines the priority of a respective query in a workload. The inputs are pre-processed to produce a most-granular-partitioning. The most-granular-partitioning is processed to produce an initial multi-level partitioned primary index, the processing including determining the scan cost of one of the queries in Q. The initial multi-level partitioned primary index is processed to produce a final multi-level partitioned primary index, the processing including determining the query cost of one of the queries in Q. F is partitioned using the final multi-level partitioned primary index. A query from Q is run against F to produce a result. The result is stored.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08768916&OS=08768916&RS=08768916
owner: Teradata US, Inc.
number: 08768916
owner_city: Dayton
owner_country: US
publication_date: 20111221
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Database tables in a star schema, i.e., databases with a fact table and a plurality of dimension tables, are often partitioned to improve performance. Partitioned Index (or \u201cPI\u201d) refers to the hash partitioning of a table. Partitioned primary index (or \u201cPPI\u201d) refers to an optional horizontal partitioning performed on top of the hash partitioning (specified by PI). A PPI can have a single level, in which partitioning is performed based on one field, or it can have multiple levels, sometimes nested, in which partitioning is performed based on a plurality of fields. The latter partitioning technique is called multi-level PPI or MLPPI. PPI and MLPPI can be used to reduce the amount of data read from a storage system in response to a query. In general, only data blocks with matching partition values are read. With correct partitioning, the query workload and disk I\/O can be reduced. Selecting the MLPPI columns is a challenge.","In general, in one aspect, the invention features a method. The method includes receiving inputs. The inputs are a star schema data model comprising a non-partitioned fact table (F) comprising fields (f, f, . . . f); a set of queries (Q=q, q, . . . q)); and a set of weights for each query in Q. Each weight defines the priority of a respective query in a workload. The method further includes preprocessing the inputs to produce a most-granular-partitioning. The method further includes processing the most-granular-partitioning to produce an initial multi-level partitioned primary index, the processing including determining the scan cost of one of the queries in Q. The method further includes processing the initial multi-level partitioned primary index to produce a final multi-level partitioned primary index, the processing including determining the query cost of one of the queries in Q. The method further includes partitioning F using the final multi-level partitioned primary index. The method further includes running a query from Q against F to produce a result. The method further includes storing the result.","Implementations of the invention may include one or more of the following. Preprocessing the inputs to produce a most granular partitioning may include:\n\n",{"@attributes":{"id":"p-0005","num":"0013"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"77pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"77pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]},{"entry":[{},"f","f",". . .","f"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"r[1, 1]","r[2, 1]",". . .","r[n, 1]"]},{"entry":[{},"r[1, 2]","r[2, 2]",". . .","r[n, 2]"]},{"entry":[{},". . .",". . .",". . .",". . ."]},{"entry":[{},"r[1, last]",". . .",". . .",". . ."]},{"entry":[{},{},"r[2, last]",". . .",". . ."]},{"entry":[{},{},{},{},"r[n, last]"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}}},"ul":{"@attributes":{"id":"ul0004","list-style":"none"},"li":{"@attributes":{"id":"ul0004-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0005","list-style":"none"},"li":[{"@attributes":{"id":"ul0005-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0006","list-style":"none"},"li":["where\n            \n            ","constructing a map, M2, between R and P, of the form:\n            \n            ","where\n            \n            ","splitting overlapping ranges in R to produce R_updated, R_updated being the most-granular-partitioning;","constructing a map, M, between R_updated and Q, of the form:","\u2003<q, ranges from fields in conjunct predictes in q>\n            \n            "]}},"for each field in R:\n        \n        ","update R with L.\n\nProcessing the most granular partitioning to produce an initial multi-level partitioned primary index may include:\n","(a) determining that the product:\n        \n        ","is greater than T, the number of partitions allowed by a database system, and, in response:\n        \n        ","(b) repeating (a) until the product S is less than or equal to T;\n\nFinding a scan cost for a query q\u03b5Q, and merge-candidate ranges rand r, rand rbeing ranges associated with qin M may include:\n","creating range ras the merger of ranges rand r;","L\u2190ranges associated with q;","removing rand rfrom L and adding rto L;","s\u2190\u201cExplain SELECT * FROM F WHERE\u201d;","for each range r\u03b5L:\n        \n        ","make an API call to the database system with s to produce a result;","extract a spool size in bytes from the result; and","compute a number of blocks from the spool size.\n\nProcessing the initial multi-level partitioned primary index to produce a final multi-level partitioned primary index may include:\n","(c) finding two ranges associated with a query belonging to Q and a variable in the query that, when merged, have the smallest weighted query cost;","(d) merging the two ranges in R_update; and","repeating (c) and (d) for a number of iterations;","partitioning F using R_update as a guide.\n\nFinding a query cost for a query q\u03b5Q, with merge-candidate ranges rand r, rand rbeing ranges associated with qin S may include:\n","creating in a database system a shadow table ST as F with no data;","propagating the statistics of F to ST;","creating range ras the merger of ranges rand r;","copying ranges associated with q into L;","removing rand rfrom L and adding rto L;","computing a partitioning clause PC by:\n        \n        ","altering ST based on PC;","creating an explain statement for qand","determining a query cost of executing qagainst ST by\n        \n        "]}}}},"In general, in one aspect, the invention features a database system. The database system includes one or more nodes. The database system further includes a plurality of CPUs, each of the one or more nodes providing access to one or more CPUs. The database system further includes a plurality of virtual processes. Each of the one or more CPUs provides access to one or more virtual processes. Each virtual process is configured to manage data, including rows from the set of database table rows, stored in one of a plurality of data-storage facilities. The database system further includes a process. The process receive as inputs:\n\n","In general, in one aspect, the invention features a computer program stored in a non-transitory tangible computer readable storage medium. The program includes executable instructions that cause a computer to receive as inputs:\n\n","The MLPPI tool disclosed herein has particular application, but is not limited, to large databases that might contain many millions or billions of records managed by a database system (\u201cDBS\u201d) , such as a Teradata Active Data Warehousing System available from the assignee hereof.  shows a sample architecture for one node of the DBS . The DBS node includes one or more processing modules , connected by a network , that manage the storage and retrieval of data in data-storage facilities . Each of the processing modules may be one or more physical processors or each may be a virtual processor, with one or more virtual processors running on one or more physical processors.","For the case in which one or more virtual processors are running on a single physical processor, the single physical processor swaps between the set of N virtual processors.","For the case in which N virtual processors are running on an M-processor node, the node's operating system schedules the N virtual processors to run on its set of M physical processors. If there are 4 virtual processors and 4 physical processors, then typically each virtual processor would run on its own physical processor. If there are 8 virtual processors and 4 physical processors, the operating system would schedule the 8 virtual processors against the 4 physical processors, in which case swapping of the virtual processors would occur.","Each of the processing modules manages a portion of a database that is stored in a corresponding one of the data-storage facilities . Each of the data-storage facilities includes one or more disk drives. The DBS may include multiple nodes in addition to the illustrated node , connected by extending the network .","The system stores data in one or more tables in the data-storage facilities . The rows of the tables are stored across multiple data-storage facilities to ensure that the system workload is distributed evenly across the processing modules . A parsing engine  organizes the storage of data and the distribution of table rows among the processing modules . The parsing engine  also coordinates the retrieval of data from the data-storage facilities in response to queries received from a user at a mainframe  or a client computer . The DBS  usually receives queries and commands to build tables in a standard format, such as SQL.","In one implementation, the rows are distributed across the data-storage facilities by the parsing engine  in accordance with their primary index. The primary index defines the columns of the rows that are used for calculating a hash value. The function that produces the hash value from the values in the columns specified by the primary index is called the hash function. Some portion, possibly the entirety, of the hash value is designated a \u201chash bucket\u201d. The hash buckets are assigned to data-storage facilities and associated processing modules by a hash bucket map. The characteristics of the columns chosen for the primary index determine how evenly the rows are distributed.","In addition to the physical division of storage among the storage facilities illustrated in , each storage facility is also logically organized. One implementation divides the storage facilities into logical blocks of storage space. Other implementations can divide the available storage space into different units of storage. The logical units of storage can ignore or match the physical divisions of the storage facilities.","In one example system, the parsing engine  is made up of three components: a session control , a parser , and a dispatcher , as shown in . The session control  provides the logon and logoff function. It accepts a request for authorization to access the database, verifies it, and then either allows or disallows the access.","Once the session control  allows a session to begin, a user may submit a SQL query, which is routed to the parser . As illustrated in , the parser  interprets the SQL query (block ), checks it for proper SQL syntax (block ), evaluates it semantically (block ), and consults a data dictionary to ensure that all of the objects specified in the SQL query actually exist and that the user has the authority to perform the request (block ). Finally, the parser  runs an optimizer (block ), which develops the least expensive plan to perform the request and produces executable steps to execute the plan. A dispatcher  issues commands to the processing modules to implement the executable steps.","Choosing the proper MLPPI columns can be a challenge for a database administrator (\u201cDBA\u201d). For example, the number of possible fields from which to choose for partitioning a table could be large. Moreover, some of the chosen fields may have a wide range of values making it challenging to find a proper range for those fields.","The granularity of the MLPPI for a specific workload may also be an issue. Fine-grained partitions potentially can lead to less I\/O, since the amount of data to be accessed is reduced. However, numerous partitions can have potential overhead from an optimization and execution perspective. Coarse-grained partitions may result in the opposite condition. Consider a simple workload Q (this example workload will be used throughout this application):","q1:\n\n","q2:\n\n","Q is based on the SSB benchmark, O'Neil, P., O'Neil, B., and Chen, X. The Star Schema Benchmark (SSB), 2007, where the fact table is lineorder while the others, including ddate and customer mentioned above, are dimension tables.","To partition the lineorder table, a DBA may note the following fact table fields: lo_orderdate, lo_custkey, l.lo_discount and lo_quantity. The lo_orderdate and lo_custkey fields are used as join attributes, while the l.lo_discount and lo_quantity fields are single table predicates. Since the first two join fields will be subject to a full scan, the DBA's interests in partitioning can be narrowed down to the latter two fields. The DBA, nevertheless, may struggle to divide the fields l.lo_discount and lo_quantity into proper regions.","There may be quite a few ways, but one way of partitioning fields is to leverage single table predicates in their entirety. With regard to the chosen fields, we can observe the following query conditions: l.lo_discount IN (1, 4, 5), l.lo_discount>=7, l.lo_quantity<=30 and lo_quantity>=25 AND l.lo_quantity<=35. However, lo_quantity field has overlapped conditions in q1 and q2 (i.e., l.lo_quantity<=30 in q1 and l.lo_quantity>=25 AND l.lo_quantity<=35 in q2); thus, a split around the medium value, 30, can be made to separate the common part on that field. As a result, partitioning solution A by this approach can be recommended as shown below:",{"@attributes":{"id":"p-0030","num":"0125"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"PARTITION BY (",{}]},{"entry":[{},{},"\u2003CASE_N (",{}]},{"entry":[{},{},"\u2003\u2003lo_discount = 1,",{}]},{"entry":[{},{},"\u2003\u2003lo_discount >= 4 AND lo_discount <= 5,",{}]},{"entry":[{},{},"\u2003\u2003lo_discount >= 7,",{}]},{"entry":[{},{},"\u2003\u2003NO CASE OR UNKNOWN),",{}]},{"entry":[{},{},"\u2003CASE_N (",{}]},{"entry":[{},{},"\u2003\u2003lo_quantity >= 25 AND lo_quantity <= 30,",{}]},{"entry":[{},{},"\u2003\u2003lo_quantity > 30 AND lo_quantity <= 35,",{}]},{"entry":[{},{},"\u2003\u2003NO CASE OR UNKNOWN)",{}]},{"entry":[{},{},")"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}}}},"The above example shows a MLPPI with two levels derived based on the chosen fields. The total number of partitions made in A is equal to number of ranges for lo_discount\u00d7number of ranges for lo_quantity=4 (=1, >=4 AND <=5, >=7, NO CASE)\u00d73 (>=25 AND <=30, >30 AND <=35, NO CASE)=12.","According to the query conditions, q1 will access only two partitions to satisfy the first or second cases on lo_discount and the first case on lo_quantity, and q2 will also access only two partitions to meet the third cases on lo_discount and the first or second cases on lo_quantity. In particular, these partitions are very favorable to the workload, because the accessed partitions will contain only the rows exactly needed by each query. Smaller and larger partitions than A can also be made. For instance, fewer partitions are created by taking the minimum and maximum values for each field. In this coarse-grained way, partitioning solution B can be derived as follows:",{"@attributes":{"id":"p-0033","num":"0128"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"PARTITION BY (",{}]},{"entry":[{},{},"\u2003CASE_N (",{}]},{"entry":[{},{},"\u2003\u2003lo_discount >= 1 AND lo_discount <= 7,",{}]},{"entry":[{},{},"\u2003\u2003NO CASE OR UNKNOWN),",{}]},{"entry":[{},{},"\u2003CASE_N (",{}]},{"entry":[{},{},"\u2003\u2003lo_quantity >= 25 AND lo_quantity <= 35,",{}]},{"entry":[{},{},"\u2003\u2003NO CASE OR UNKNOWN)",{}]},{"entry":[{},{},")"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}}}},"The biggest advantage of the approach is that the solution B makes only 4 partitions in total, which are much less than those of A. The disadvantage is that each query cannot avoid reading rows that need not be in their answers, compared with the solution A. Bigger partitions can also be generated by focusing on each individual value present in the workload. In Q, lo_discount is associated with 1, 4, 5 and 7, and lo_quantity involves 25, 30 and 35. Hence, a fine-grained solution C could be recommended as follows:",{"@attributes":{"id":"p-0035","num":"0130"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"PARTITION BY (",{}]},{"entry":[{},{},"\u2003CASE_N (",{}]},{"entry":[{},{},"\u2003\u2003lo_discount = 1,",{}]},{"entry":[{},{},"\u2003\u2003lo_discount = 4,",{}]},{"entry":[{},{},"\u2003\u2003lo_discount = 5,",{}]},{"entry":[{},{},"\u2003\u2003lo_discount >= 7,",{}]},{"entry":[{},{},"\u2003\u2003NO CASE OR UNKNOWN),",{}]},{"entry":[{},{},"\u2003CASE_N (",{}]},{"entry":[{},{},"\u2003\u2003lo_quantity < 25,",{}]},{"entry":[{},{},"\u2003\u2003lo_quantity >= 25 AND lo_quantity <= 30,",{}]},{"entry":[{},{},"\u2003\u2003lo_quantity > 30 AND lo_quantity <= 35,",{}]},{"entry":[{},{},"\u2003\u2003lo_quantity > 35,",{}]},{"entry":[{},{},"\u2003\u2003NO CASE OR UNKNOWN)",{}]},{"entry":[{},{},")"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}}}},"Similar to A, C can help q1 and q2 retrieve the rows that are exactly needed and included in only a few partitions. However, C requires more partitions (5\u00d75=25) than A and B.","In one embodiment, an MLPPI wizard recommends a specific MLPPI scheme for the fact table in a star schema workload. One embodiment of the tool uses a greedy algorithm for search space enumeration. In one embodiment, the space is driven by the predicates in the queries. This fits an MLPPI scheme based on a general framework allowing general expressions, ranges and case expressions for partition definitions. The workload Q, described above, will be used as an example. In one embodiment, a cost model (in one embodiment, a cost model from TERADATA CORPORATION\u00ae) is used to prune the search space to reach an efficient solution. In one embodiment, the tool is completely on the client side and uses existing server application programming interfaces (\u201cAPIs\u201d) to simplify the workload queries, capture fact table predicates and compute scan and query costs.","In one embodiment, the tool is independent of the DBMS. In one embodiment, the tool makes API calls to the server and receives necessary information only for the partition solution produced in each phase, as discussed below. In one embodiment, the wizard does not require recompilation of the DBMS source code when modified. This flexibility results in little impact on the DBMS, thereby leading to a lightweight design.","In one embodiment, the tool  has three incremental phases, as shown in . In one embodiment, given a set of queries and weights, the tool  produces an MLPPI recommendation that is the customized for the workload. In one embodiment, interacting with the DBMS optimizer , as illustrated in , it passes through a series of phases: a preprocessing phase , an initial phase , and an optimization phase . In one embodiment, as phases proceed, a less granular partitioning solution is made, and eventually the final MLPPI is reached.","In one embodiment of the preprocessing phase , for each query the tool  makes an API call to the DBMS in order to extract a list of predicates of that query. Then, in one embodiment, an association between the query and the predicates is made and managed by the tool . In this way, all of the mappings can be achieved on the entire query set.","A predicate typically contains a field and a value or values to restrict the field. In one embodiment, it is possible to construct a candidate partition from the predicate. However, there may be many predicates found in the given workload. Some of the predicates could be duplicate, overlapped, or very distinct. In one embodiment, the predicates are classified by fact table field and then simplified them by removing the duplicates or overlaps within the same field if necessary. In one embodiment, this is accomplished by converting each predicate into a corresponding range, and splitting any overlapping ranges. Thereafter, in one embodiment, the tool  remaps from a query to ranges through the predicates by transitive property, so that it can proceed to further phases.","In one embodiment, this process produces the Most Granular Partitioning (\u201cMGP\u201d) for a fact table, which contains partitions that could be made initially for each (found) field. Depending upon how many predicates appear in the workload, the partitions by MGP can be less or more than the maximum number of partitions allowed in the DBMS  (in the current TERADATA CORPORATION system, the maximum is 65,536). In one embodiment, if the number of partitions is greater than the limit, an initial phase  is performed. If, in one embodiment, the number of partitions is fewer than the limit, the initial phase  is skipped and an optimized phase , described below, is performed. Otherwise, in one embodiment, the tool  performs the initial phase  with the MGP.","In the initial phase , in one embodiment, the passed MGP is further refined. In one embodiment, to reduce the search space by the MGP, a pair of consecutive ranges is examined to compute how much I\/O cost would be increased once the pair is merged. This I\/O cost is called scan cost, which, in one embodiment, is obtained by sending a scan cost query to and receiving the estimate from the DBMS . In one embodiment, after all pairs are evaluated, a range pair with the least scan cost sum of all queries is chosen at each iteration, in an approach called a \u201cgreedy\u201d approach. In one embodiment, the mapping from query to ranges is also updated by the selection. In this way, in one embodiment, pairs of adjacent ranges are incrementally consolidated. In one embodiment, the initial phase  ends if the number of ongoing partitions drops below the maximum allowed by the DBMS .","In one embodiment, the optimization phase  attempts to reduce the number of partitions. In one embodiment, reducing the number of partitions is beneficial to lessening the overhead of the optimizer  in query processing. In one embodiment, the tool  examines all remaining ranges in a similar way as is done in the initial phase . In one embodiment, the optimization phase  uses the query cost of a query, estimated processing time, rather than the scan cost. In one embodiment the query cost is estimated by sending a shadow query to and receiving its estimated processing time from the DBMS . In one embodiment, the shadow query is built by executing the original query against a hypothesized MLPPI assuming the merger of a range pair. In one embodiment, a range pair with the minimum query cost sum of all queries is selected. In one embodiment, if merger of the chosen range pair on a particular iteration does not improve the total workload cost, then the task is considered complete and the tool  recommends the final MLPPI.","In one embodiment, given a workload Q, the tool  produces an MLPPI recommendation for a non-partitioned fact table through the three phases described above. In one embodiment, the tool  interacts actively with the DBMS optimizer , and suggests the final MLPPI by taking advantage of received predicates or cost estimates. Pseudo-code set 1 is for one embodiment of the tool :","Pseudo-Code Set 1: The MLPPI Wizard (Tool )\n\n","Preprocessing Phase ():\n\n","Initial Phase ():\n\n","Optimization Phase ():\n\n","The preprocessing phase  is described in more detail below in the discussion of . The initial phase  is described in more detail below in the discussion of . The optimization phase  is described in more detail below in the discussion of .","Preprocessing Phase","Receive Inputs.","In one embodiment, illustrated in , the preprocessing phase  begins with the receipt of the data model (i.e., the star schema described above), a set of queries Q, and weights for each of the queries (block ).","Query Simplification.","In one embodiment, if queries in a given workload have redundant conditions or unfolded views, the queries can be simplified by removing duplicates, deriving new predicates, and folding the views via an API call made to the DBMS before further processing (block ). This extra work is not required in the example because q1 and q2 are already clean. Hence, this step can be ignored for the example workload Q.","Conjunct Extraction.","In one embodiment, the next task is to locate and extract all the conjunct predicates in the simplified workload (block ). In one embodiment, the conjunct extraction is done through an API call to the DBMS  that returns the query XML tree. In general, in one embodiment, the conjuncts are in the form:","<variable> <op> <constant>,","where <variable> is one of the fields in the fact table, <op> is in","{=, <, <=, >=, >, IN},","and <constant> represents a constant. All <op>s are self-explanatory. In particular, in one embodiment, \u2018IN\u2019 is a list predicate implying \u2018OR\u2019 operator. In one embodiment, a list of predicates P={p1, . . . , p4} is extracted from Q. In one embodiment, the predicates comply with the conjunct form as shown below:","p1: l.lo_discount IN (1, 4, 5)","p2: l.lo_discount>=7","p3: l.lo_quantity<=30","p4: l.lo_quantity>=25 and l.lo_quantity<=35","Next, in one embodiment, a bi-directional map between Q and P, called M1, is constructed so that for each query the corresponding predicate(s) can be found directly and vice versa (block ). The following association entries, for example, are included in M1:","<q1, {p1, p3}>","<q2, {p2, p4}>","Conversion to Range from Conjunct.","It can be challenging to derive a partition recommendation from pure conjuncts. To make it possible to perform arithmetic operations on conjuncts, in one embodiment, each conjunct is transformed into an equivalent range (block ).","Assume I to be a list of the fact table fields referenced by P. There are two fields in Q used by P: lo_discount, lo_quantity. These are added to I.","In one embodiment, for each field in I, raw ranges are constructed from the predicates in P. In one embodiment, the outcome is a 2-dimensional array, called R, which captures the range representation by the predicates (block ). Each entry in R represents a pair of values for the range. Given Q, R is formed by the entries shown below:",{"@attributes":{"id":"p-0067","num":"0171"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"lo_discount","lo_quantity"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[1, 1]","[\u221e, 30]"]},{"entry":[{},"[4, 5]","[25, 35]"]},{"entry":[{},"[7, 1]"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"In the range representation, infinity (1) or -infinity (\u22121) can be used for unbounded ranges. \u201c[\u201d or \u201c]\u201d are used for closed ranges, and \u201c(\u201d or \u201c)\u201d for open ranges.","In one embodiment, R is associated with P, thereby leading to another two-way map, called M2 (block ). Let r[i, j] denote the range value by the i-th field's j-th range in R. r[1, 3], for instance, indicates lo_discount[7,1]. In the example M2 will be formed by the entries shown below:","<p1, {r[1, 1],r[1, 2]}>","<p2, r[1, 3]>","<p3, r[2, 1]>","<p4, r[2, 2]>","Non-Overlapping Range Construction.","In one embodiment, raw ranges per field are sorted in range order. In the later phases (as discussed below), costs of all range pairs are computed, and the range pair with the least cost is selected for merge in each iteration (block ). The process assumes the ranges do not overlap with each other, and they are sorted. Therefore, overlapping ranges are split.","One embodiment of the technique for splitting overlapping ranges (block ) is shown in pseudo-code set 2, which is illustrated in :",{"@attributes":{"id":"p-0076","num":"0180"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"161pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"Pseudo-code set 2: Splitting overlapping ranges",{}]},{"entry":[{},{},"input : R",{}]},{"entry":[{},{},"output: R with consecutive, non-overlapping ranges",{}]},{"entry":[{},{},"for each field i \u03b5 R (blocks 605, 606) do",{}]},{"entry":[{},{},"\u2003L \u2190 Get a list of field i's ranges sorted by start",{}]},{"entry":[{},{},"\u2003value (block 610)",{}]},{"entry":[{},{},"\u2003while L has overlapping ranges (block 615) do",{}]},{"entry":[{},{},"\u2003\u2003r1, r2 \u2190 Consecutive ranges in L (block 620)",{}]},{"entry":[{},{},"\u2003\u2003if r1.end \u2267 r2.start (block 625) then",{}]},{"entry":[{},{},"\u2003\u2003\u2003Split the overlapping range (block 630).",{}]},{"entry":[{},{},"\u2003\u2003\u2003Insert into L an intermediate range if",{}]},{"entry":[{},{},"\u2003\u2003\u2003needed (block 635).",{}]},{"entry":[{},{},"\u2003\u2003\u2003Resort ranges in L (block 640).",{}]},{"entry":[{},{},"\u2003\u2003end",{}]},{"entry":[{},{},"\u2003\u2003else",{}]},{"entry":[{},{},"\u2003\u2003\u2003r1 \u2190 r2, r2 \u2190 r3 (blocks 645, 650); \/* r3:",{}]},{"entry":[{},{},"\u2003\u2003\u2003next range *\/",{}]},{"entry":[{},{},"\u2003\u2003end",{}]},{"entry":[{},{},"\u2003end",{}]},{"entry":[{},{},"\u2003Update R with the final L (block 655).",{}]},{"entry":[{},{},"end (block 660)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}}},"br":{},"b":"630"},"a. r1.start, r2.start;","b. r2.start+1, r2.end;","c. r2.end+1, r1.end.","A similar split is made if r1 is contained in r2. In the second case, in which r2 overlaps but is not contained within r1 (i.e., r2.end>r1.end), the split ranges are:","a. r1.start, r2.start;","b. r2.start+1, r1.end;","c. r1.end+1, r2.end.","In the example, the lo_quantity field in the example does not have disjoint ranges. The overlap can be removed as described above, and the intermediate range, r[2, 2]=[25, 30] is created. Eventually the updated R (R_updated) is obtained:",{"@attributes":{"id":"p-0084","num":"0188"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"133pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"lo_discount","lo_quantity"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[1, 1]","[\u221e, 25]"]},{"entry":[{},"[4, 5]","[25, 30]"]},{"entry":[{},"[7, 1]","[31, 35]"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"In one embodiment, the existing map M2 between P and R is updated to reflect the split. As a result, p1 and p2 referencing lo_discount are unchanged, but p3 and p4 affected by the split now get associated with {r[2, 1] and r[2, 2]} and {r[2, 2] and r[2, 3]}, respectively. Therefore, the updated M2 is obtained:","<p1, {r[1, 1],r[1, 2]}>","<p2, r[1, 3]>","<p3, r[2, 1], r[2, 2]>","<p4, r[2, 2], r[2, 3]>","Query-to-Range Map Construction.","In one embodiment, the determination of which query in the workload contains which ranges is accomplished by the transitive property from M1 to M2; namely, a query-to-range map between Q and R_updated, called M, can be built from M1 and M2 (block ). M for the example is shown below:","<q1, {r[1,1], r[1,2], r[2,1], r[2,2]}>","<q2, {r[1,3], r[2,2], r[2,3]}>","Input Partitions.","In some cases, in one embodiment, R_updated can be used to define an MLPPI using each field as one level. In one embodiment, if the number of such partitions (i.e., the product of the number of partitions in each level) is less than or equal to the maximum number of partitions allowed by the DBMS (block ), then the initial phase can be skipped and processing can move directly to the optimization phase (\u201cN\u201d branch from block  through the \u201cC\u201d connector). Otherwise, in one embodiment, processing moves to the initial phase (\u201cY\u201d branch from block  through the \u201cA\u201d connector). Processing returns from the initial phase through the \u201cB\u201d connector.","For example, suppose that the maximum limit of partitions is 15. From R in the example, (# ranges in lo_discount)\u00b7(# ranges in lo_quantity)=(3+1)\u00b7(3+1)=16 partitions are obtained. Since this exceeds the limit, the initial phase is executed, in which a feasible partition recommendation with the number of partitions less than or equal to the limit can be made. Otherwise, is the number of partitions is smaller than the maximum limit, processing can move directly to the optimization phase.","Note that, in one embodiment, one range is added to each field in the calculation. The reason is that some columns may allow for a NULL value or have a value that does not fall into any range in R. To put it another way, in one embodiment, a default partition is reserved for a field having such a value. Thus, it should also be taken into account for the total partition counts.","In one embodiment, assuming that the current R surpasses the partition limit, processing is now ready to proceed to the initial and\/or optimization phases with the prepared M (query-to-range map) and R (input range set per field). M will be used for updating R, and ultimately R will be evolving to a partition recommendation.","Initial Phase","In one embodiment in the initial phase, a merge is incrementally done on a range pair in R until the number of the current partitions is dropped below the max number of partitions allowed. In one embodiment, the incremental consolidation is limited to merging two consecutive ranges, because for each field the range set is sorted in non-decreasing order.","In general, the overall I\/O cost may be increased by the merge. In one embodiment, every row is hash-sorted in the DBMS . This means that once a query is passed to a certain partition, nonqualified rows belonging to the same partition are read, for a full scan is done on the partition to retrieve the target rows. Therefore, in one embodiment, a heuristic approach is used to pick the range pair that incurs the least I\/O cost increase.","To select the best merge yielding the lowest I\/O cost, in one embodiment, the scan cost is determined for each query assuming the merge is applied. The scan cost is defined as the number of blocks to be read for the target query, and, in one embodiment, it is obtained by making an API call to the DBMS  with a corresponding scan cost query, as described below.","In one embodiment, the API call is made with an \u201cexplain\u201d tool to exploit a cost-based optimizer  in the DBMS . In one embodiment, the explain tool provides the tool  with a variety of estimates, such as estimated spool size\/processing time\/result cardinality\/CPU cost, etc. Among the estimates, in one embodiment, the tool  leverages the spool size in bytes so that the block counts can be derived from it. In the initial phase , in one embodiment, an MLPPI is not derived directly from R which produces more partitions than the maximum limit. Therefore, the spool size is selected for the heuristic. In one embodiment, in the optimization phase , the tool  will be able to use the estimated processing time, which is more accurate heuristic than the spool size.","In one embodiment, the scan cost query \u201cs\u201d is built as follows:","SELECT * FROM F WHERE predicate-list,","where \u201cpredicate-list\u201d is a list of predicates for the ranges belonging to q according to M. In one embodiment, if q has ranges forming a range pair in examination, then the consolidated range pair substitutes for the ranges temporarily, and it is taken into account in making the predicates of s. Otherwise, in one embodiment, the predicates are constructed from the current ranges mapped to q.","To illustrate, consider a range pair r[2, 2] and r[2, 3]. The pair leads to a merge of ranges [25, 30] and [31, 35] of lo_quantity field. The consolidated range is [25, 35]. In one embodiment, the corresponding scan cost queries, s1 and s2, are built as below:","s1:\n\n","s2:\n\n","In one embodiment, s1 and s2 are sent to the server, and block counts associated with each will be returned. In one embodiment, if the range merge impacts neither q1 nor q2, the above queries s1 and s2 would not be needed; accordingly, the formerly computed scan costs of q1 and q2 will be reused for the current range pair. In one embodiment, the tool  will recompute the scan cost of a query if and only if it is affected by a merge of two consecutive ranges.","Pseudo-code set 3, illustrated in , shows one embodiment of the scan cost computation on a range pair that influences a query.","Pseudo-Code Set 3: Scan Cost Calculation\n\n","In this way, in one embodiment, the weighted sum of scan cost of queries, T, can be computed for each range pair, assuming that each query has its own weight, which is regarded as priority or frequency, in a workload. In one embodiment, T is defined as follows:",{"@attributes":{"id":"p-0111","num":"0236"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"T","mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mrow":{"mi":"scan_cost","mo":"\u2062","mrow":{"mrow":[{"mo":["(",")"],"msub":{"mi":["q","i"]}},{"mo":["(",")"],"msub":{"mi":["w","i"]}}],"mo":"\u00b7"}}}}}}}},"where n is the number queries in Q, qis a query in Q, and wdenotes the weight of qi, which falls within 0 and 1.","Once the merge in the example is eventually chosen among all range pairs by the least T, both R and M are updated accordingly, as shown below for the example:","R:",{"@attributes":{"id":"p-0115","num":"0240"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"133pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"lo_discount","lo_quantity"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[1, 1]","[\u221e, 25]"]},{"entry":[{},"[4, 5]","[25, 35]"]},{"entry":[{},"[7, \u221e]"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"M:\n\n","Note that q2 may have the most customized partition based on the current R. The partition formed by r[1, 3]\u00d7r[2, 2] has all qualified rows for q2; thus, most of partitions can be eliminated when q2 is served. In contrast, q1 cannot benefit from R as much as q2. Answering q1 would require accessing the entire partitions formed by (R[1, 1]|R[1, 2])\u00d7R[2, 2], which contain both qualified and non-qualified rows for q1. This fact addresses the aforementioned trade-off between fine-grained and coarse-grained partitions addressed. Again, the goal of the tool  is to produce the MLPPI with the minimum total cost given a workload.","Now that the total number of partitions by the ongoing R is 12, it is below the partition limit (15). The tool  can proceed to the optimization phase with R without repeating the initial phase.","One embodiment uses the heuristic that if several range pairs end up leading to the same, least T, the range pair that produces the fewest partitions once applied is chosen. For instance, assume that the number of partitions by the ongoing R is higher than the limit. Then in the continuing example a merge m1 of r[1, 2] and r[1, 3] and another merge m2 of r[2, 1] and r[2, 2] can be considered, and they happen to produce the same T. Since m2 can reduce more partitions than m1 when selected (3\u00d73 by m1 vs. 4\u00d72 by m2), it is chosen as the best merge instead of m1.","Pseudo-code set 4, which is illustrated in , describes the initial phase .",{"@attributes":{"id":"p-0121","num":"0248"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"168pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Pseudo-code set 4: Initial Phase",{}]},{"entry":[{},"input : M (query-to-range map) and R (input range set)",{}]},{"entry":[{},"output: MLPPI such that # partitions by R is \u2266 LIMIT",{}]},{"entry":[{},"while calculateTotalPartitions (R) > LIMIT do",{}]},{"entry":[{},"\u2003\/* r1 and r2 are consecutive, adjacent *\/",{}]},{"entry":[{},"\u2003for each a range pair of (r1, r2) in R do (blocks 805",{}]},{"entry":[{},"\u2003and 810)",{}]},{"entry":[{},"\u2003r \u2190 Merge r1 and r2 (block 815).",{}]},{"entry":[{},"\u2003\u2003T \u2190 0 ; (block 815) \/* Sum of weighted scan cost",{}]},{"entry":[{},"\u2003\u2003of Q *\/",{}]},{"entry":[{},"\u2003\u2003for each q in Q do (blocks 820 and 825)",{}]},{"entry":[{},"\u2003\u2003\u2003w \u2190 q's weight (block 830)",{}]},{"entry":[{},"\u2003\u2003\u2003if (r1 or r2) \u03b5 q then (block 835) \/*",{}]},{"entry":[{},"\u2003\u2003\u2003Affected *\/",{}]},{"entry":[{},"\u2003\u2003\u2003\u2003T += w \u00b7 (sc(q, r1, r2)) ; (block (840)\/*",{}]},{"entry":[{},"\u2003\u2003\u2003\u2003See pseudo-code set 3 *\/",{}]},{"entry":[{},"\u2003\u2003\u2003else T += w \u00b7 (q's existing scan cost) (block",{}]},{"entry":[{},"\u2003\u2003\u2003845)",{}]},{"entry":[{},"\u2003\u2003end",{}]},{"entry":[{},"\u2003\u2003Associated T with the range pair of r1 and r2",{}]},{"entry":[{},"\u2003\u2003(block 850).",{}]},{"entry":[{},"\u2003end",{}]},{"entry":[{},"\u2003Find the best range pair with the least T. (block 855)",{}]},{"entry":[{},"\u2003(Select the range pair to make less partitions in the",{}]},{"entry":[{},"\u2003case of the same least T.)",{}]},{"entry":[{},"\u2003Update R and M using the chosen range pair (block",{}]},{"entry":[{},"\u2003855).",{}]},{"entry":[{},"end (return to FIG. 5 through connector B)"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}},"br":{}},"In one embodiment, at the beginning of the optimization phase  the tool  has arrived at an initial MLPPI solution, e.g., in the example an initial MLPPI solution for the lineorder fact table. The solution can be used as is, but, in one embodiment, decreasing the number of partitions through further merges can enhance the overall workload performance. First, multiple file contexts by many partitions can incur overhead that impacts on the optimizer . Second, in one embodiment, the optimizer has a limit of partitions that it can actually access while in execution even though the DBMS itself can allow much more partitions.","In one embodiment, a similar algorithm to that used in the initial phase  is applied in the optimization phase . In one embodiment, the optimization phase  uses an estimated query cost, which is the estimated processing time of a query against the fact table partitioned by an ongoing MLPPI solution and dimension tables.","In one embodiment, for each range pair, the tool  determines whether a given query contains one of the ranges in the pair, just as in the initial phase . In one embodiment, if the query does have the range(s) in the pair, then its query cost is recomputed, applying the merged range. Otherwise, in one embodiment, the tool uses the previous query cost of the query as determined in the initial phase .","In one embodiment, the query cost for each query in a workload is computed as follows. First, in one embodiment, a fictitious partitioning is made based on R (or R\u2032 applying a target merge of two ranges in R for the re-computation case). In one embodiment, this is done by creating an empty shadow table S with the same definition as F, propagating the table statistics from F to S, and altering the resulting table by an MLPPI to reflect R (R\u2032). Next, in one embodiment, for each query q a shadow query s is built by replacing F exposed in q with S, and then an API call with s is made to the optimizer . Lastly, in one embodiment, the tool  fetches the estimated processing time of s and adds it to the total cost in the workload. Again, in one embodiment, the re-computation of query cost is limited to a query that is affected by the merge of a range pair.","Pseudo-code set 5, shown in , illustrates how the query cost is computed given a range pair affecting a query, as described above.","Pseudo-Code Set 5: Query Cost Computation\n\n","In one embodiment, the weighted sum of the query cost of queries, T, is computed for each range pair. In one embodiment, T is defined similarly as shown in the initial phase.","Suppose in the example the tool  decides to consolidate r[1, 1] and r[1, 2]. Then, in one embodiment, T computed for the merge is stored for the next iteration, and R is also updated along with the merge. In one embodiment, the tool  continues to find another range pair for the next merge. In one embodiment, if the chosen merge fails to enhance the existing T, then the wizard completes the optimization phase .","Pseudo-code set 6, shown in , illustrates the optimization phase . In one embodiment, the same heuristic described above for the initial phase  may be used in the optimization stage  to pick the best range pair for a merge.",{"@attributes":{"id":"p-0131","num":"0270"},"tables":{"@attributes":{"id":"TABLE-US-00010","num":"00010"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"14pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"Pseudo-code set 6: Optimization Phase",{}]},{"entry":[{},{},"input : R (input range set) output: MLPPI such that the total query cost is minimal",{}]},{"entry":[{},{},"Tp \u2190 Compute and add up all query costs using R (block ",{}]},{"entry":[{},{},"1005).",{}]},{"entry":[{},{},"while specified iterations or no more range in R do (blocks ",{}]},{"entry":[{},{},"1010 and 1015)",{}]},{"entry":[{},{},"\u2003for each a range pair of (r1, r2) in R do (blocks 1020",{}]},{"entry":[{},{},"\u2003and 1025)",{}]},{"entry":[{},{},"\u2003\u2003r \u2190 Merge r1 and r2 (block 1030).",{}]},{"entry":[{},{},"\u2003\u2003R0 \u2190 Update R with r, temporarily (block 1030).",{}]},{"entry":[{},{},"\u2003\u2003T \u2190 0 ; (block 1030)\/* Weighted query cost sum of",{}]},{"entry":[{},{},"\u2003\u2003Q *\/",{}]},{"entry":[{},{},"\u2003\u2003for each q in Q do (blocks 1035 and 1040)",{}]},{"entry":[{},{},"\u2003\u2003\u2003w \u2190 q's weight (block 1045)",{}]},{"entry":[{},{},"\u2003\u2003\u2003if (r1 or r2) \u03b5 q then (block 1050)\/*",{}]},{"entry":[{},{},"\u2003\u2003\u2003Affected *\/",{}]},{"entry":[{},{},"\u2003\u2003\u2003\u2003T += w \u00b7 (qc(q, r1, r2)) (block 1055); \/*",{}]},{"entry":[{},{},"\u2003\u2003\u2003\u2003See pseudo-code set 5 *\/",{}]},{"entry":[{},{},"\u2003\u2003\u2003else T += w \u00b7 (q's existing query cost) (block",{}]},{"entry":[{},{},"\u2003\u2003\u20031060)",{}]},{"entry":[{},{},"\u2003\u2003end",{}]},{"entry":[{},{},"\u2003\u2003Map T to the current range pair (of r1 and r2)",{}]},{"entry":[{},{},"\u2003\u2003(block 1065).",{}]},{"entry":[{},{},"\u2003end",{}]},{"entry":[{},{},"\u2003Find the best range pair with the least T, or Tc",{}]},{"entry":[{},{},"\u2003(block 1070).",{}]},{"entry":[{},{},"\u2003(Select the range pair to make less partitions in the",{}]},{"entry":[{},{},"\u2003case of the same Tc.)",{}]},{"entry":[{},{},"\u2003Update R using the chosen range pair (block 1070).",{}]},{"entry":[{},{},"\u2003if Tc > Tp (block 1075) then Done (block 1080).",{}]},{"entry":[{},{},"\u2003else Tp \u2190 Tc (block 1085)",{}]},{"entry":[{},{},"end while (block 1090)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}}}},"In the example, the tool makes the following the final MLPPI recommendation for the lineorder fact table:","ALTER TABLE lineorder","MODIFY PRIMARY INDEX","PARTITION BY (\n\n","In this section, the time complexity of the proposed algorithms for the initial or optimized phase is analyzed. For this purpose, time is interpreted as the number of API calls made to the DBMS.","Lemma 5.1. The time complexity for the initial phase or optimization phase is O(M\u00b7N), where M is the number of range pairs, and N is the number of queries in a given workload.","Proof. Suppose that a merge for any range pair affects all given queries because every query has any of range elements in the pair. Every iteration the scan or query costs in both of the phases is recomputed for every query. In the first iteration, the cost paid is as much as M\u00b7N. Once a merge is chosen, then M\u22121 range pairs are remaining in the next iteration, the cost will be equal to (M\u22121)\u00b7N. In this way, we may end up consolidating all range pairs and thus having no partition for the fact table. In this worst case, the total calls made to the server can be increased up to:",{"@attributes":{"id":"p-0139","num":"0285"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mrow":[{"mi":["M","N"],"mo":"\u00b7"},{"mrow":{"mo":["(",")"],"mrow":{"mi":"M","mo":"-","mn":"1"}},"mo":"\u00b7","mi":"N"}],"mo":["+","+","+"],"mi":["\u2026","N"]},{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"M"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"i"}},"mo":"\u00b7","mi":"N"},{"mfrac":{"mrow":{"mi":"M","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"M","mo":"-","mn":"1"}}},"mn":"2"},"mo":"\u00b7","mi":"N"}],"mo":"="}],"mo":"="}}},"br":{},"sup":"2"},"The inventor showed that the complexity, M\u00b7N is overly pessimistic. In practice, the number of calls made was much fewer than the theoretic bound. The Experiment section that follows describes how many API calls would be made on the workloads used for evaluation.","Experiment","Environment.","The tools  was developed on Windows XP platform (although that platform is not necessary) and written in the Java programming language. JDK (Java SE Development Kit) 6 Update 26 version was used for the implementation. All evaluations on the tool's MLPPI recommendation were done on a Teradata DBMS (ver 13.10) server machine running Unix.","Workloads were created using a random star schema query generator. Assuming that 1) operators, 2) fields and 3) minimum and maximum values per field are already known, the generator first determines how many predicates are needed along with the number of fields in a fact table. It then selects a field and a operator arbitrarily. If the IN operator is chosen, the query generator determines how many values are added to the list and then randomly picks up the corresponding number of values between the minimum and maximum values of the chosen field. Otherwise, just a single random value is used on the chosen operator. The generator builds a query having the desired predicates.","Each query created by the generator is based on a template query that joined the lineorder and the date dimension with constraints on selected fields of the lineorder table. This template is common in customer cases like reports and form templates. Based on the template, two sets of 10 and 20 queries were generated by varying the constraints on lineorder.","Statistics.","While producing the recommendations for the two sets of workload, a variety of statistics about input partitions, range pairs, number of API calls and total iterations were collected. The statistics for the workloads is shown in Table 1.",{"@attributes":{"id":"p-0146","num":"0292"},"tables":{"@attributes":{"id":"TABLE-US-00011","num":"00011"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"91pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"center"}}],"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":[{},"Initial Phase ","Optimization Phase"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"10 queries","20 queries","10 queries","20 queries"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["# Input partitions","1,008,000 ","110,739,200","51,840","59,520"]},{"entry":["# Total iterations","15","55 ","1","1 "]},{"entry":["# Input range ","441 ","4,180","24","48"]},{"entry":["pairs",{},{},{},{}]},{"entry":["# Total API calls","1,305 ","31,915","78 ","388 "]},{"entry":["(worst calls by","(1,944,810)","(349,448,000)","(5,760)","(46,080)"]},{"entry":["Lemma 5.1)",{},{},{},{}]},{"entry":["Average API","2.96","7.63","3.25","8.08"]},{"entry":["calls per range",{},{},{},{}]},{"entry":"pair"},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"About 1 million partitions were derived initially from 10 queries, whereas roughly 0.11 billion partition from 20 queries. The workload size doubled, but the total number of partitions was increased exponentially. Because the number and variety of predicates contained in the 20-query workload quantitatively overwhelmed those of the 10-query-workload, two orders of magnitude more ranges were observed. The initial search space can be sensitive to the size of workload, but nevertheless, we will show that the tool  is capable of producing good quality recommendation.","The total number of iterations tended to be proportional to the input partitions given to the initial phase across the workloads. There was only one iteration in the optimized phase. The partitions produced by the initial phase were complete enough, so more iterations were not necessary to optimize them through the further phase.","The number of collected range pairs per field was relatively small, compared with the input partitions. The range pairs were considered for a merge in the phases, and they were reduced as a pick is made in each iteration.","The inventors doubted that the running time complexity expression of O(M\u00b7N) derived above, because it would be very rare for a range pair to be associated with all queries in a workload. As illustrated in Table 1, the total calls made to the server per workload were far fewer than the theoretical bounds, which are shown in parentheses and computed using number of queries and range pairs in each phase. In particular, the average number of API calls per range pair was limited within 10 across workloads and phases.","Performance Evaluation.","The main goal of the evaluation was to see the quality of an MLPPI recommendation made by the tool  compared with that of no partitioning (NO PPI) and the partitioning done similarly by previous work (Agrawal, S., Narasayya, V., and Yang, B., Integrating Vertical and Horizontal Partitioning into Automated Physical Database Design, Proceedings of the 2004 ACM SIGMOD (2004), ACM, pp. 359-370; Zilio, D., Rao, J., Lightstone, S., G., L., Storm, A., C., G. A., and Fadden, S., DB2 Design Advisor: Integrated Automatic Physical Database, Proceedings of the 2004 VLDB (2004), Morgan Kaufmann Publishers Inc., pp. 1087-1097) (EXP). The results are summarized in , showing the total run time in seconds. Overall, the tool's recommendation was very effective at partitioning the fact table using the referenced fields and restricting values in the workloads. The total speedups made by the MLPPI recommendations were two or three times more efficient than those by the NOPPI and EXP schemes across workloads and fact table scales. Furthermore, the tool's solutions scaled well with increasing data size and workload size. Although scaling up the fact table from 1 TB and 3 TB, we did not observe any significant rise in the total cost of each workload. Increasing workloads could not quite affect the total cost against the same fact table size. These results ensure the superiority of the MLPPI algorithms against the existing work.","The foregoing description of the preferred embodiment of the invention has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. It is intended that the scope of the invention be limited not by this detailed description, but rather by the claims appended hereto."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0008","num":"0088"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0009","num":"0089"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0010","num":"0090"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0011","num":"0091"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0012","num":"0092"},"figref":"FIGS. 5-10"},{"@attributes":{"id":"p-0013","num":"0093"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
