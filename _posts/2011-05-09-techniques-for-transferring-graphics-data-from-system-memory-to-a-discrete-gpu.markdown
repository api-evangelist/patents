---
title: Techniques for transferring graphics data from system memory to a discrete GPU
abstract: A method for transferring graphics data includes receiving graphics data in the system memory. The graphics data may be loaded into system memory by and application from a mass storage device. One or more graphics commands associated with the graphics data may also be received. The graphics commands may also be received from the application. The graphics data in system memory is compressed in response to receipt of the one or more graphics commands before the graphics data is transferred to a discrete graphics processing unit. The one or more received graphics commands are transferred to the discrete graphics processing unit. The one or more graphics commands include an operation to copy the compressed graphics data to the discrete graphics processing unit. The compressed graphics data is copied from the system memory to memory of the graphics processing. The compressed graphics data is then decompressed by the graphics processing unit. Thereafter, the discrete graphics processing unit may perform one or more graphics operations on the transferred graphics data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08780122&OS=08780122&RS=08780122
owner: Nvidia Corporation
number: 08780122
owner_city: Santa Clara
owner_country: US
publication_date: 20110509
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["This application is a continuation-in-part of U.S. patent application Ser. No. 12\/649,317 filed Dec. 29, 2009, and U.S. patent application Ser. No. 12\/651,395 filed Dec. 21, 2009, both of which claim the benefit of U.S. Provisional Patent Application No. 61\/243,155 filed Sep. 16, 2009 and U.S. Provisional Patent Application No. 61\/243,164 filed Sep. 17, 2009, all of which are incorporated in their entirety herein by reference.","Conventional computing systems may include a discrete graphics processing unit (dGPU) or an integral graphics processing unit (iGPU). The discrete GPU and integral GPU are heterogeneous because of their different designs. The integrated GPU generally has relatively poor processing performance compared to the discrete GPU. However, the integrated GPU generally consumes less power compared to the discrete GPU.","The conventional operating system does not readily support co-processing using such heterogeneous GPUs. Referring to , a graphics processing technique according to the conventional art is shown. When an application  starts, it calls the user mode level runtime application programming interface (e.g., DirectX API d3d9.dll)  to determine what display adapters are available. In response, the runtime API  enumerates the adapters that are attached to the desktop (e.g., the primary display ). A display adapter , , even recognized and initialized by the operating system, will not be enumerated in the adapter list by the runtime API  if it is not attached to the desktop. The runtime API  loads the device driver interface (DDI) (e.g., user mode driver (umd.dll))  for the GPU  attached to the primary display . The runtime API  of the operating system will not load the DDT of the discrete GPU  because the discrete GPU  is not attached to the display adapter. The DDI  configures command buffers of the graphics processor  attached to the primary display . The DDI  will then call back to the runtime API  when the command buffers have been configured.","Thereafter, the application  makes graphics requests to the user mode level runtime API (e.g., DirectX API d3d9.dll)  of the operating system. The runtime  sends graphics requests to the DDI  which configures command buffers. The DDI calls to the operating system kernel mode driver (e.g., DirectX driver dxgkrnl.sys) , through the runtime API , to schedule the graphics request. The operating system kernel mode driver then calls to the device specific kernel mode driver (e.g., kmd.sys)  to set the command register of the GPU  attached to the primary display  to execute the graphics requests from the command buffers. The device specific kernel mode driver  controls the GPU  (e.g., integral GPU) attached to the primary display .","In U.S. patent application Ser. No. 12\/649,317 filed Dec. 29, 2009, and U.S. patent application Ser. No. 12\/651,395 filed Dec. 21, 2009 various techniques for using the discrete GPU to process graphics data and commands are disclosed. When using the discrete GPU, graphics data and commands are transferred across one or more communication links such as peripheral component interface express buss. The transfer of graphics data and commands, however, may result in a bandwidth bottleneck on the communication link. Accordingly, there is a continuing need for improved techniques for transferring graphics data and commands across a communication link to the discrete graphics processing unit.","The present technology may best be understood by referring to the following description and accompanying drawings that are used to illustrate embodiments of the present technology.","Embodiments of the present technology are directed toward techniques for transferring graphics data from system memory to a discrete graphics processing unit. In one embodiment, a method for transferring graphics data includes receiving graphics data in the system memory. The graphics data may be loaded into system memory by an application from a mass storage device. One or more graphics commands associated with the graphics data may also be received from the application. The graphics data in system memory is compressed in response to receipt of the one or more graphics commands before the graphics data is transferred to the discrete graphics processing unit. The one or more received graphics commands are transferred to the discrete graphics processing unit. The one or more graphics commands include an operation to copy the compressed graphics data to the discrete graphics processing unit. In response to the copy operation in the graphics command, the compressed graphics data is copied from the system memory to memory of the discrete graphics processing unit. In one implementation, the data is compressed to copy the data across a relatively slow communication link, such as a 1X peripheral component interface express (PCIe) bus. The compressed graphics data is then decompressed by the graphics processing unit. The method may be implemented in software (e.g., computing device executable instructions stored in computing device readable media which when executed perform the method), firmware, hardware, or a combination of one or more thereof.","In another embodiment, the method of transferring graphics data includes receiving graphics data in the system memory. One or more graphics commands associated with the graphics data may also be received. Receiving the graphics data may include writes of the entire graphics data to system memory, or one or more update write to the graphics data in system memory. An amount of one or more portions of the graphics data that have changed in the system memory is determined. If the one or more portion of the graphics data that have changed are more than a predetermined amount, the one or more portions of the graphics data are compressed. The one or more received graphics commands are transferred to a discrete processing unit. One or more of the received graphics command include an operation to copy the compressed graphics data to the discrete graphics processing unit. If the one or more portion of the graphics data that have changed are more than the predetermined amount, the compressed one or more portions of the graphics data from the system memory are copied to memory of the discrete graphics processing unit. If the one or more portions of the graphics data that have changed are more than the predetermined amount, the compressed one or more portion of the data may be decompressed in-line with copying into the memory of the discrete graphics processing unit. If the one or more portions of the graphics data that have changed are not more than the predetermined amount, the one or more portion of the graphics data may be copied from the system memory to the memory of the graphics processing without compression. Again, the method may be implemented in software, firmware, hardware, or a combination of one or more thereof.","Reference will now be made in detail to the embodiments of the present technology, examples of which are illustrated in the accompanying drawings. While the present technology will be described in conjunction with these embodiments, it will be understood that they are not intended to limit the invention to these embodiments. On the contrary, the invention is intended to cover alternatives, modifications and equivalents, which may be included within the scope of the invention as defined by the appended claims. Furthermore, in the following detailed description of the present technology, numerous specific details are set forth in order to provide a thorough understanding of the present technology. However, it is understood that the present technology may be practiced without these specific details. In other instances, well-known methods, procedures, components, and circuits have not been described in detail as not to unnecessarily obscure aspects of the present technology.","Some embodiments of the present technology which follow are presented in terms of routines, modules, logic blocks, and other symbolic representations of operations on data within one or more electronic devices. The descriptions and representations are the means used by those skilled in the art to most effectively convey the substance of their work to others skilled in the art. A routine, module, logic block and\/or the like, is herein, and generally, conceived to be a self-consistent sequence of processes or instructions leading to a desired result. The processes are those including physical manipulations of physical quantities. Usually, though not necessarily, these physical manipulations take the form of electric or magnetic signals capable of being stored, transferred, compared and otherwise manipulated in an electronic device. For reasons of convenience, and with reference to common usage, these signals are referred to as data, bits, values, elements, symbols, characters, terms, numbers, strings, and\/or the like with reference to embodiments of the present technology.","It should be borne in mind, however, that all of these terms are to be interpreted as referencing physical manipulations and quantities and are merely convenient labels and are to be interpreted further in view of terms commonly used in the art. Unless specifically stated otherwise as apparent from the following discussion, it is understood that through discussions of the present technology, discussions utilizing the terms such as \u201creceiving,\u201d and\/or the like, refer to the actions and processes of an electronic device such as an electronic computing device that manipulates and transforms data. The data is represented as physical (e.g., electronic) quantities within the electronic device's logic circuits, registers, memories and\/or the like, and is transformed into other data similarly represented as physical quantities within the electronic device.","In this application, the use of the disjunctive is intended to include the conjunctive. The use of definite or indefinite articles is not intended to indicate cardinality. In particular, a reference to \u201cthe\u201d object or \u201ca\u201d object is intended to denote also one of a possible plurality of such objects. It is also to be understood that the phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting.","Referring to , a graphics co-processing computing platform, in accordance with one embodiment of the present technology is shown. The exemplary computing platform may include one or more central processing units (CPUs) , a plurality of graphics processing units (GPUs) , , volatile and\/or non-volatile memory (e.g., computer readable media) , , one or more chip sets , , and one or more peripheral devices , - communicatively coupled by one or more busses. The GPUs include heterogeneous designs. In one implementation, a first GPU may be an integral graphics processing unit (iGPU) and a second GPU may be a discrete graphics processing unit (dGPU). The chipset ,  acts as a simple input\/output hub for communicating data and instructions between the CPU , the GPUs ,  the computing device-readable media , , and peripheral devices , -. In one implementation, the chipset includes a northbridge  and southbridge . The northbridge  provides for communication between the CPU , system memory  and the southbridge . In one implementation, the northbride  includes an integral GPU. The southbridge  provides for input\/output functions. The peripheral devices , - may include a display device , a network adapter (e.g., Ethernet card) , CD drive, DVD drive, a keyboard, a pointing device, a speaker, a printer, and\/or the like. In one implementation, the second graphics processing unit is coupled as a discrete GPU peripheral device  by a bus  such as a Peripheral Component Interconnect Express (PCIe) bus.","In a number of computing platforms, the discrete graphics processing unit is coupled by a relatively high speed communication link (e.g., 16X PCIe bus). However, in some computing platforms, such laptop computers, tablet computers, netbooks, smartphones and the like, a slower communication link (e.g., 1X PCIe bus) may be used to reduce costs. For example, the discrete GPU  may be coupled to system memory , the CPU , and the chipset ,  by a 1x PCIe bus . The 1x PCIe bus  provides a limited bandwidth (e.g., 200 MB\/sec) as compared to a faster 16x PCIe link (e.g., 3 GB\/sec).","The computing device-readable media ,  may be characterized as primary memory and secondary memory. Generally, the secondary memory, such as a magnetic and\/or optical storage, provides for non-volatile storage of computer-readable instructions and data for use by the computing device. For instance, the disk drive  may store the operating system (OS), applications, utilities, routines, drivers, and data. The primary memory, such as the system memory  and\/or graphics memory, provides for volatile storage of computer-readable instructions and data for use by the computing device. For instance, the system memory  may temporarily store a portion of the operating system, a portion of one or more applications, utilities, routines, drivers and associated data that are currently used by the CPU , GPU  and the like. In addition, the GPUs ,  may include integral or discrete frame buffers , .","Referring to , a graphics co-processing technique, in accordance with one embodiment of the present technology, is shown. When an application  starts, it calls the user mode level runtime application programming interface (e.g., DirectX API d3d9.dll)  to determine what display adapters are available. In addition, an application initialization routine is injected when the application starts. In one implementation, the application initialization routine is a short dynamic link library (e.g., appin.dll). The application initialization routine injected in the application includes some entry points, one of which includes a call (e.g., set_dll_searchpath( )) to change the search path for the display device driver interface. During initialization, the search path for the device driver interface (e.g., c:\\windows\\system32\\ . . . \\umd.dll) is changed to the search path of a shim layer library (e.g., c:\\ . . . \\coproc\\ . . . \\umd.dll). Therefore the runtime API  will search for the same DDI name but in a different path, which will result in the runtime API  loading the shim layer .","The shim layer library  has the same entry points as a conventional display driver interface (DDI). The runtime API  passes one or more function pointers to the shim layer  when calling into the applicable entry point (e.g., OpenAdapter( )) in the shim layer . The function pointers passed to the shim layer  are call backs into the runtime API . The shim layer  stores the function pointers. The shim layer  loads and initializes the DDI on the primary adapter . The DDI on the primary adapter  returns a data structure pointer to the shim layer  representing the attached adapter. The shim layer  also loads and initializes the device driver interface on the unattached adapter  by passing two function pointers which are call backs into local functions of the shim layer . The DDI on the unattached adapter  also returns a data structure pointer to the shim layer  representing the unattached adapter. The data structure pointers returned by the DDI on the primary adapter  and unattached adapter  are stored by the shim layer . The shim layer  returns to the runtime API  a pointer to a composite data structure that contains the two handles. Accordingly, the DDI on the unattached adapter  is able to initialize without talking back to the runtime API .","In one implementation, the shim layer  is an independent library. The independent shim layer may be utilized when the primary GPU\/display and the secondary GPU are provided by different vendors. In another implementation, the shim layer  may be integral to the display device interface on the unattached adapter. The shim layer integral to the display device driver may be utilized when the primary GPU\/display and secondary GPU are from the same vendor.","The application initialization routine (e.g., appin.dll) injected in the application also includes other entry points, one of which includes an application identifier. In one implementation, the application identifier may be the name of the application. The shim layer  application makes a call to the injected application initialization routine (e.g., appin.dll) to determine the application identifier when a graphics command is received. The application identifier is compared with the applications in a white list (e.g., a text file). The white list indicates an affinity between one or more applications and the second graphics processing unit. In one implementation, the white list includes one or more applications that would perform better if executed on the second graphics processing unit.","If the application identifier is not on the white list, the shim layer  calls the device driver interface on the primary adapter . The device driver interface on the primary adapter  sets the command buffers. The device driver interface on the primary adapter then calls, through the runtime  and a thunk layer , to the operating system kernel mode driver (e.g., DirectX driver dxgkrnl.sys) . The operating system kernel mode driver  in turn schedules the graphics command with the device specific kernel mode driver (e.g., kmd.sys)  for the GPU  attached to the primary display . The GPU  attached to the primary display  is also referred to hereinafter as the first GPU. The device specific kernel mode driver  sets command register of the GPU  to execute the graphics command on the GPU  (e.g., integral GPU) attached to the primary display .","If the application identifier is a match to one or more identifiers on the white list, the handle from the runtime API  is swapped by the shim layer  with functions local to the shim layer . For a rendering command, the local function stored in the shim layer  will call into the DDI on the unattached adapter  to set the command buffer. In response, the DDI on the unattached adapter  will call local functions in the shim layer  that route the call through the thunk layer  to the operating system kernel mode driver  to schedule the rendering command. The operating system kernel mode driver  calls the device specific kernel mode driver (e.g., dkmd.sys)  for the GPU on the unattached adapter  to set the command registers. The GPU on the unattached adapter  (e.g., discrete GPU) is also referred to hereinafter as the second GPU. Alternatively, the DDI on the unattached adapter  can call local functions in the thunk layer . The thunk layer  routes the graphics request to the operating system kernel mode driver (e.g., DirectX driver dxgkrnl.sys) . The operating system kernel mode driver  schedules the graphics command with the device specific kernel mode driver (e.g., dkmd.sys)  on the unattached adapter. The device specific kernel mode driver  controls the GPU on the unattached adapter .","For a display related command (e.g., Present( )), the shim layer  splits the display related command received from the application  into a set of commands for execution by the GPU on the unattached adapter  and another set of commands for execution by the GPU on the primary adapter . In one implementation, when the shim layer  receives a present call from the runtime , the shim layer  calls to the DDI on the unattached adapter  to cause a copy from the frame buffer  of the GPU on the unattached adapter  to a corresponding buffer in system memory . The shim layer  will also call the DDI on the primary adapter  to cause a copy from the corresponding buffer in system memory  to the frame buffer  of the GPU on the attached adapter  and then a present by the GPU on the attached adapter . The memory accesses between the frame buffers ,  and system memory  may be direct memory accesses (DMA). To synchronize the copy and presents on the GPUs , , a display thread is created, that is notified when the copy to system memory by the second GPU  is done. The display thread will then queue the copy from system memory  and the present call into the GPU on the attached adapter .","In another implementation, the operating system (e.g., Window7Starter) will not load a second graphics driver . Referring now to , a graphics co-processing technique, in accordance with another embodiment of the present technology, is shown. When the operation system will not load a second graphics driver, the second GPU  is tagged as a non-graphics device adapter that has its own driver . Therefore the second GPU  and its device specific kernel mode driver  are not seen by the operating system as a graphics adapter. In one implementation, the second GPU  and its driver  are tagged as a memory controller. The shim layer  loads and configures the DDI  for the first GPU  on the primary adapter and the DDI  for the second GPU  If there is a specified affinity for executing rendering commands from the application  on the second GPU , the shim layer  intercepts the rendering commands sent by the runtime API  to the DDI on the primary adapter , calls the DDI on the unattached adapter to set the commands buffers for the second GPU , and routes them to the driver  for the second GPU . The shim layer  also intercepts the callbacks from the driver  for the second GPU  to the runtime . In another implementation, the shim layer  implements the DDI  for the second GPU . Accordingly, the shim layer  splits graphics command and redirects them to the two DDIs , .","Accordingly, the embodiments described with reference to , enables the application to run on a second GPU instead of a first GPU when the particular version of the operating system will allow the driver for the second GPU to be loaded but the runtime API will not allow a second device driver interface to be initialized. The embodiments described with reference to  enables an application to run on a second GPU, such as a discrete GPU, instead of a first GPU, such as an integrated GPU, when the particular version of the operation system (e.g., Win7Starter) will not allow the driver for the second GPU to be loaded. The DDI  for the second GPU  cannot talkback through the runtime  or the thunk layer  to a graphics adapter handled by an OS specific kernel mode driver.","Referring now to , a method of transferring graphics data from system memory to a discrete GPU, in accordance with one embodiment of the present technology, is shown. The method begins with receiving graphics data in a system memory, at . In one implementation, an application causes graphics data to be loaded into system memory. For example, an application may cause graphics data from a mass storage device such as a hard disk drive, optical disk drive (e.g., DVD drive) to be loaded into a vertex buffer, an index buffer, a shading\/lighting buffer, a texture buffer, and\/or the like, of the system memory. At , one or more graphics commands associated with the graphics data are received. The graphics commands received from the application specify one or more graphics operations utilizing the graphics data, including an operation to copy the graphics data in system memory to the discrete GPU.","At , the graphics data in the system memory is compressed before the graphics data is copied to the discrete GPU, in response to receiving the one or more graphics operations associated with the graphics data. In one implementation, the user mode driver applies Lempel-Ziv-Oberhumer (LZO) or similar compression to the graphics data in the system memory. The LZO compression is a lossless block compression algorithm that provides approximately a 40% reduction in the graphics data.","In an optional implementation, the graphics data in system memory may be compressed as slices. Each slice may be compressed by a corresponding thread. Alternatively, each slice may be compressed by a corresponding CPU core. Compressing slices of graphics data speeds up the compression process.","At , the one or more received graphics commands are transferred to the discrete GPU. In one implementation, one or more command buffers of the discrete GPU are loaded with byte code for executing the one or more graphics commands associated with the graphics data. For example, a user mode driver (umd), operating system, kernel mode driver (kmd), and\/or the like, loads one or more command buffers of the discrete GPU with a collection of identifiers and arguments specifying an operation to copy the compressed graphics data in system memory into a frame buffer of the discrete GPU. One or more arguments may specify the location (e.g., base address and offset) of the graphics data in system memory. One or more other arguments may specify a location in a frame buffer of the discrete GPU for storing the graphics data. One or more identifiers may specify a direct memory access blitting of the graphics data from the location in system memory to the location in the frame buffer. One or more other identifiers may specify that the graphics data in system memory is compressed and optionally the type of compression.","At , the compressed graphics data is copied from the system memory to memory of the discrete GPU. In one implementation, a copy engine of the discrete GPU performs a direct memory access (DMA) transfer of the compressed graphics data in system memory in response to the applicable identifies and arguments in the command buffer. At , the compressed graphics data copied to memory of the discrete GPU is decompressed. In one implementation, a decoder engine or similar engine of the discrete GPU applies Lempel-Ziv-Oberhumer (LZO) decompression or another corresponding decompression algorithm to the graphics data. The decompressed graphics data may be stored in the frame buffer at the location specified by one or more identifiers and\/or arguments in the command buffer.","Although the copy process and the decompression process are described as separate processes, it is appreciated that LZO decompression and other corresponding decompression algorithms can be performed in-line with the copy operation so that the decompressed graphics data is stored directly in the frame buffer. In addition, if the graphics data was compressed as slices, the compressed slices of graphics data may be decompressed in corresponding order. Furthermore, the decompression can be performed by the discrete GPU in parallel with some ongoing rendering.","If the discrete GPU is coupled to the CPU and system memory by a relatively slow communication link, such as a 1x PCIe bus, the compression advantageously reduces the time that it takes to copy the graphics data from system memory to the discrete GPU. For example, if 100 MB of uncompressed graphics data is transferred at 20 MB\/sec across a 1X PCIe, it will take approximately 0.5 seconds to copy the graphics data from the system memory to the frame buffer of the discrete GPU. However, the 40 MB of compressed graphics data in accordance with embodiments of the present technology may be transferred across the same 20 MB\/sec 1X PCIe link in approximately 0.2 seconds.","At , the method may continue with performing one or more operations on the decompressed graphics data. In one implementation, one or more engines of the discrete GPU perform one or more operations on the decompressed graphics data in the frame buffer. For example, the operations may render graphics primitives from the graphics data in the frame buffer.","Referring now to , a method of transferring graphics data in system memory to a discrete GPU, in accordance with another embodiment of the present technology, is shown. The method begins with receiving updates to graphics data in a system memory, at . In some cases the graphics data may involve a large amount of data. However, only a small fraction of the data might have actually have been modified. Therefore it may be advantageous to track the updates to the graphics data. In one implementation, a lock call and a write\/watch call may be received prior to one or more writes to portion of the graphics data. After the updates to the graphics data are written to the system memory a stop\/watch call and a unlock call are received. In response to the write\/watch call a set of dirty bits are set to their modified state for each page that is the target of a write operation. At , one or more graphics commands associated with the graphics data are received. The graphics commands received from the application specify one or more graphics operations utilizing the graphics data. At , the amount of graphics data that has been updated in the system memory is determined. In one implementation, the graphics driver (e.g., umd, kmd, and or the like) reads the dirty bits in response to the unlock call to determine which pages were modified. The graphics driver may be configured with a minimum block size (e.g., number of pages) for transfer operations and may initiate a compressed or uncompressed direct memory access (DMA) blit, as described below, for each block of pages that contains at least one modified page. The minimum block size may be selected based on an efficiency tradeoff between using a large number of small direct memory access blit operations.","If one or more updated portions of the graphics data are not more than a predetermined amount, the one or more received graphics commands are transferred to the discrete GPU, at . In one implementation, one or more command buffers of the discrete GPU are loaded with byte code for executing the one or more graphics commands associated with the graphics data including an operation to copy the compressed graphics data in system memory to the discrete GPU.","If one or more updated portions of the graphics data are not more than a predetermined amount, the graphics data is copied from the system memory to the discrete GPU, at . In one implementation, a copy engine of the discrete GPU performs a direct memory access (DMA) transfer of the graphics memory in system memory in response to the applicable identifies and arguments in the command buffer. At , the method may continue with performing operations on the graphics data. In one implementation, one or more engines of the discrete GPU perform one or more operations on the graphics data in the frame buffer. For example, the operations may render graphics primitives from the graphics data in the frame buffer.","If the one or more updated portions of the graphics data are more than a predetermined amount, the updated portions of the graphics data in the system memory are compressed, at , before being copied to the discrete GPU. The compression may be triggered upon an unlock call from the application, or by memory watching assisted by some operating system primitive. In one implementation, the user mode driver applies Lempel-Ziv-Oberhumer (LZO) or similar compression to the updated portions of the graphics data in the system memory. The LZO compression is a lossless block compression algorithm that provides approximately a 40% reduction in data.","In an optional implementation, the updated portions of the graphics data may be compressed as slices. Each slice may be compressed by a corresponding thread. Alternatively, each slice may be compressed by a corresponding CPU core. Compressing slices of data speeds up the compression process.","If the one or more updated portions of the graphics data are more than a predetermined amount, the one or more received graphics commands are transferred to the discrete GPU, at . The transferred graphics commands include an operation to copy the compressed graphics data updates in system memory to the discrete GPU. In one implementation, one or more command buffers of the discrete GPU are loaded with byte code for executing the one or more graphics commands associated with the graphics data. For example, a user mode driver (umd), operating system, kernel mode driver (kmd), and\/or the like, loads one or more command buffers of the discrete GPU with a collection of identifiers and arguments specifying an operation to copy the compressed graphics data updates in system memory into the frame buffer of the discrete GPU. One or more arguments may specify the location (e.g., base address and offset) of the compressed graphics data updates in system memory. One or more other arguments may specify a location in a frame buffer of the discrete GPU for storing the graphics data updates. One or more identifiers may specify a direct memory access transfer of the graphics data updates from the location in system memory to the location in the frame buffer. One or more other identifiers may specify that the one or more graphics data updates in memory are compressed and optionally the type of compression.","If the one or more updated portions of the graphics data are more than a predetermined amount, the one or more compressed graphics data updates are copied from the system memory to the discrete GPU, at . In one implementation, a copy engine of the discrete GPU performs a direct memory access (DMA) transfer of the compressed graphics data updates in system memory in response to the applicable identifies and arguments in the command buffer. At , the one or more compressed graphics data updates copied to the discrete GPU are decompressed. In one implementation, a decoder engine or similar engine of the discrete GPU applies Lempel-Ziv-Oberhumer (LZO) decompression or another corresponding decompression algorithm to the graphics data updates. The decompressed graphics data updates may be stored in the frame buffer at the location specified by one or more identifiers and\/or arguments in the command buffer.","Although the copy process and decompression process are described as separate processes, it is appreciated that LZO decompression and other corresponding decompression algorithms can be performed in-line with the copy operation so that the decompressed graphics data updates are stored directly in the frame buffer. In addition, if the graphics data updates were compressed as slices, the compressed slices of graphics data may be decompressed in corresponding order. Furthermore, the decompression can be performed by the discrete GPU in parallel with some ongoing rendering.","Again, if the discrete GPU is coupled to the CPU and system memory by a relatively slow communication link, such as a 1x PCIe bus, the compression advantageously reduces the time that it takes to copy the graphics data from system memory to the discrete GPU.","Again, the method may continue with performing operations on the graphics data, at . In one implementation, one or more engines of the discrete GPU perform one or more operations on the graphics data including one or more updates. For example, the operations may render graphics primitives from the graphics data in the frame buffer.","Accordingly, embodiments of the present technology utilize compression of graphics data in the system memory before copying from system memory to memory of a discrete GPU to advantageously reduce traffic on the bus coupling the CPU and system memory to the discrete GPU.","Generally, display data is generated by the discrete GPU after processing the graphics data and commands. If the discrete GPU is attached to a display device, the display data may be output by the discrete GPU to the display device. If the discrete GPU is not attached to the display device, the display data may be transferred to the integrated GPU for output on the display attached to the integrated GPU. Referring now to , a method of synchronizing the copy and present operations on the first and second GPUs is shown. The method is illustrated in  with reference to an exemplary set of render and display operations, in accordance with one embodiment of the present technology. At , the shim layer  receives a plurality of rendering - and display operations for execution by the GPU on the unattached adapter . At , the shim layer  splits each display operation into a set of commands including 1) a copy - from a frame buffer  of the GPU on the unattached adapter  to a corresponding buffer in system memory  having shared access with the GPU on the attached adapter , 2) a copy ,  from the buffer in shared system memory  to a frame buffer of the GPU on the primary adapter , and 3) a present ,  on the primary display  by the GPU on the primary adapter . At , the copy and present operations on the first and second GPUs ,  are synchronized.","The frame buffers ,  and shared system memory  may be double or ring buffered. In a double buffered implementation, the current rendering operations is stored in a given one of the double buffers  and the other one of the double buffers is blitted to a corresponding given one of the double buffers of the system memory. When the rendering operation is complete, the next rendering operation is stored in the other one of the double buffers and the content of the given one of the double buffers is Witted  to the corresponding other one of the double buffers of the system memory. The rendering and blitting alternate back and forth between the buffers of the frame buffer of the second GPU . The blit to system memory is executed asynchronously. In another implementation, the frame buffer of the second. GPU  is double buffered and the corresponding buffer in system memory  is a three buffer ring buffer.","After the corresponding one of the double buffers of the frame buffer  in the second GPU  is blitted  to the system memory , the second GPU  generates an interrupt to the OS. In one implementation, the OS is programmed to signal an event to the shim layer  in response to the interrupt and the shim layer  is programmed to wait on the event before sending a copy command  and a present command  to the first GPU . In a thread separate from the application thread, referred to hereinafter as the display thread, the shim layer waits for receipt of the event indicating that the copy from the frame buffer to system memory is done, referred to herein after as the copy event interrupt. A separate thread is used so that the rendering commands on the first and second GPUs ,  are not stalled in the application thread while waiting for the copy event interrupt. The display thread may also have a higher priority than the application thread.","A race condition may occur where the next rendering to a given one of the double buffers for the second GPU  begins before the previous copy from the given buffer is complete. In such case, a plurality of copy event interrupts may be utilized. In one implementation, a ring buffer and four events are utilized.","Upon receipt of the copy event interrupt, the display thread queues the blit from system memory  and the present call into the first GPU . The first GPU  blits the given one of the system memory  buffers to a corresponding given one of the frame buffers of the first GPU . When the blit operation is complete, the content of the given one of the frame buffers of the first GPU  is presented on the primary display . When the next copy and present commands are received by the first GPU , the corresponding other of the system memory  buffers is blitted into the other one of the frame buffers of the first GPU  and then the content is presented on the primary display . The blit and present alternate back and forth between, the double buffered frame buffer of the first GPU . The copy event interrupt is used to delay programming, thereby effectively delaying the scheduling of the copy from system memory  to the frame buffer of the first GPU  and presenting on the primary display .","In one implementation, a notification on the display side indicates that the frame has been present on the display  by the first GPU . The OS is programmed to signal an event when the command buffer causing the first GPU  to present its frame buffer on the display is done executing. The notification maintains synchronization where an application runs with vertical blank (vblank) synchronization.","Referring now to , an exemplary set of render and display operations, in accordance with another embodiment of the present technology, is shown. The rendering and copy operations executed on the second GPU  may be performed by different engines. Therefore, the rendering and copy operations may be performed substantially simultaneously in the second GPU .","Generally, the second GPU  is coupled to the system memory  by a bus having a relatively high bandwidth. However, in some systems the bus coupling the second GPU  may not provide sufficient bandwidth for blitting the frame buffer  of the second GPU  to system memory . For example, an application may be rendered at a resolution of 1280\u00d71024 pixels. Therefore, approximately 5 MB\/frame of RGB data is rendered. If the application renders at 100 frames\/sec, than the second GPU needs approximately 500 MB\/sec for blitting upstream to the system memory . However, a peripheral component interface express (PCIe) 1x bus typically used to couple the second GPU  system memory  has a bandwidth of approximately 250 MB\/sec in each direction. Referring now to , a method of compressing rendered data, in accordance with one embodiment of the present technology is shown. The second GPU  renders frames of RGB data, at . At , the frames of RGB data are converted using a pixel shader in the second GPU  to YUV sub-sample data. The RGB data is processed as texture data by the pixel shader in three passes to generate YUV sub-sample data. In one implementation, the U and V components are sub-sampled spatially, however, the Y is not sub-sampled. The RGB data may be converted to YUV data using the 4.2.0 color space conversion algorithm. At , the YUV sub-sample data is blitted to the corresponding buffers in the system memory with an asynchronous copy engine of the second GPU. The YUV sub-sample data is blitted from the system memory to buffers of the first GPU, at . The YUV data is blitted to corresponding texture buffers in the second GPU. The Y, U, and V sub-sample data are buffered in three corresponding buffers, and therefore the copy from frame buffer of the second GPU  to the system memory  and the copy from system memory  to the texture buffers of first GPU  are each implemented by sets of three copies. The YUV sub-sample data is converted using a pixel shader in the first GPU  to recreate the RGB frame data, at . The device driver interface on the attached adapter is programmed to render a full screened aligned quad from the corresponding texture buffers holding the YUV data. At , the recreated RGB frame data is then presented on the primary display  by the first GPU . Accordingly, the shaders are utilized to provide YUV compression and decompression.","In one implementation, each buffer of Y, U and V samples is double buffered in the frame buffer of the second GPU  and the system memory . In addition, the Y, U and V samples copied into the first GPU  are double buffered as textures. In another implementation, the Y, U and V sample buffers in the second GPU  and corresponding texture buffers in the first GPU  are each double buffered. The Y, U and V sample buffered in the system memory  may each be triple buffered.","In one implementation, the shim layer  tracks the bandwidth needed for blitting and the efficiency of transfers on the bus to enable the compression or not. In another implementation, the shim layer  enables the YUV compression or not based on the type of application. For example, the shim layer  may enable compression for game application but not for technical applications such as a Computer Aided Drawing (CAD) application.","The foregoing descriptions of specific embodiments of the present technology have been presented for purposes of illustration and description. They are not intended to be exhaustive or to limit the invention to the precise forms disclosed, and obviously many modifications and variations are possible in light of the above teaching. The embodiments were chosen and described in order to best explain the principles of the present technology and its practical application, to thereby enable others skilled in the art to best utilize the present technology and various embodiments with various modifications as are suited to the particular use contemplated. It is intended that the scope of the invention be defined by the claims appended hereto and their equivalents."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Embodiments of the present technology are illustrated by way of example and not by way of limitation, in the figures of the accompanying drawings and in which like reference numerals refer to similar elements and in which:",{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIGS. 5A and 5B"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIGS. 6A-6C"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 10"}]},"DETDESC":[{},{}]}
