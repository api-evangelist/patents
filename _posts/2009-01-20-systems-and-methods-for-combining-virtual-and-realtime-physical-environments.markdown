---
title: Systems and methods for combining virtual and real-time physical environments
abstract: Systems, methods and structures for combining virtual reality and real-time environment by combining captured real-time video data and real-time 3D environment renderings to create a fused, that is, combined environment, including capturing video imagery in RGB or HSV/HSV color coordinate systems and processing it to determine which areas should be made transparent, or have other color modifications made, based on sensed cultural features, electromagnetic spectrum values, and/or sensor line-of-sight, wherein the sensed features can also include electromagnetic radiation characteristics such as color, infra-red, ultra-violet light values, cultural features can include patterns of these characteristics, such as object recognition using edge detection, and whereby the processed image is then overlaid on, and fused into a 3D environment to combine the two data sources into a single scene to thereby create an effect whereby a user can look through predesignated areas or “windows” in the video image to see into a 3D simulated world, and/or see other enhanced or reprocessed features of the captured image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08040361&OS=08040361&RS=08040361
owner: Systems Technology, Inc.
number: 08040361
owner_city: Hawthorne
owner_country: US
publication_date: 20090120
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["REFERENCE TO RELATED APPLICATION","FIELD OF INVENTION","BACKGROUND OF INVENTION","SUMMARY OF INVENTION","DETAILED DESCRIPTION"],"p":["This application claims the benefit of U.S. application for patent Ser. No. 11\/104,379, filed Apr. 11, 2005, which is incorporated by reference herein.","The present invention relates to the field of virtual reality (VR).","Portions of the disclosure of this patent document contain material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office file or records, but otherwise reserves all rights whatsoever.","As the power and speed of computers has grown, so has the ability to provide computer-generated artificial and virtual environments. Such virtual environments have proven popular for training systems, such as for driver training, pilot training and even training in performing delicate medical and surgical procedures. These systems typically involve combining prerecorded or computer generated visual information with a real world environment to provide the perception of a desired environment. For example, a driver's training simulator may include a physical representation of the driver's seat of an automobile with a video or computer generated image of a road and traffic projected on what would be the windshield of the simulator car of a student driver. The image is made to be reactive to the actions of the driver, by changing speeds and perspectives in response to acceleration, braking and steering by the driver. Similarly, sophisticated flight simulators include a physical cockpit and projected flight environments that present real world situations to the pilot via a display.","In some cases, a virtual reality is projected in front of the eyes of a user via a virtual reality helmet, goggles, or other input device, so that the only image seen by the user is the virtual image. In other instances, mirrors and partially reflective materials are used so that a user can view both the real world environment and the virtual environment at the same time.","A disadvantage of prior art virtual reality and simulation systems is difficulty in combining real world and virtual world images in a realistic and unrestricted manner. In some prior art cases, certain views and angles are not available to a user because they require prior calculation of image perspective and cannot be processed in real time. In other instances, the ability to interact with the virtual world with physical objects is limited or unavailable.","The present systems include methods, devices, structures and circuits for combining virtual reality and real-time environment. Embodiments of the systems combine captured real-time video data and real-time 3D environment rendering(s) to create a fused, that is, a combined environment or reality. These systems capture video imagery and process it to determine which areas should be made transparent, or have other color modifications made, based on sensed cultural features and\/or sensor line-of-sight. Sensed features can include electromagnetic radiation characteristics, e.g., visible color, infra-red intensity or ultra-violet intensity. Cultural features can include patterns of these characteristics, such as object recognition using edge detection, depth sensing using stereoscopy or laser range-finding. This processed image is then overlaid on a three-dimensional (3D) environment to combine the data sources into a single scene or image that is then available for viewing by the system's user. This creates an effect by which a user can look through predefined or pre-determined areas, or \u201cwindows\u201d in the video image and then see into a 3D simulated world or environment, and\/or see other enhanced or reprocessed features of the captured image.","Methods of deploying near-field images into the far-field virtual space are also described and included as preferred embodiments. In one preferred embodiment, a depth sensing method, such as with use of a laser range finder, video pixels corresponding to various depths in the environment are placed and rendered in a virtual environment consistent with the sensed depths of the pixels, and virtual objects are then placed between, in front of, or beyond the video-based objects. Alternatively, the video-based and virtual objects could be moved within the virtual environment as a consequence or function of user interaction, such as with a joystick or through voice commands. Additionally, the predetermined area, or portals where the virtual scene is placed can be designated via depth. For example, an actual window could be cut out of a wall, and a background surface could be placed at, e.g., 10 feet or some other distance behind the cut out in the wall. In such an example, the virtual scene would then replace every pixel that lies beyond some threshold, predetermined distance behind the cut out in the wall.","In another aspect, when a physical object of interest is isolated from the surrounding environment, by, for example, framing it with a keying color, sensing its depth, or using object recognition, it can be physically manipulated by the user and commanded to move into the environment and at a chosen or predetermined distance. At a predetermined distance, the isolated video is mounted onto a virtual billboard, which is then deployed in the virtual environment. If the user chooses to physically retrieve the object, the video is removed from the virtual billboard when it reaches the distance where the physical object is actually located, at which point the user proceeds to maneuver and manipulate the physical object in near-space. In this manner, realistic manipulations of real objects can be made at relatively great distances, but without requiring large physical spaces for the system.","These and other embodiments, features, aspects, and advantages of the presently described systems will become better understood with regard to the following description, appended claims and accompanying drawings.","Reference symbols or names are used in the Figures to indicate certain components, aspects or features shown therein. Reference symbols common to more than one Figure indicate like components, aspects or features shown therein.","Described herein are several embodiments of systems that include methods and apparatus for combining virtual reality and real-time environments. In the following description, numerous specific details are set forth to provide a more thorough description of these embodiments. It is apparent, however, to one skilled in the art that the systems need not include, and may be used without these specific details. In other instances, well known features have not been described in detail so as not to obscure the inventive features of the system.","One prior art technique for combining two environments is a movie special effect known as \u201cblue screen\u201d or \u201cgreen screen\u201d technology. In this technique, an actor is filmed in front of a blue screen and can move or react to some imagined scenario. Subsequently, the film may be filtered so that everything blue is removed, leaving only the actor moving about. The actor's image can then be combined with some desired background or environment so that it looks like the actor is actually in some desired location. This technique is often used in filming scenes involving driving. Actors are filmed in a replica of a car in front of a blue screen. Some movement (for example, shaking) is provided to simulate driving over a road surface and the driver might even turn the wheel as if turning the car. In reality, of course, the car does not move at all. Next, the scene of the drivers is combined with footage taken by cameramen in a real car on the road on which the actors are pretending to drive. The result gives the perception that the actors are actually driving a car on the road. This process is also referred to as chroma-key.","Typically, motion picture chroma-key shots are done in several steps over time, making the system inapplicable for real time virtual environments. However, some chroma-key processes are used in real time in certain video and television applications. For example, a television weatherman is typically shot live in front of a chroma-key matte, such as a blue screen or green screen. The weatherman's image (with the matte color filtered out) is combined with an image from another source, such as the weather map or satellite picture, with which the weatherman appears to interact. In reality, the weatherman is watching a monitor with the weather map image on it and uses that to point at portions of the blue screen which would correspond to the weather map. Such an application is very limited and doesn't permit realistic interaction on the part of the human involved with the virtual image.","The present inventive system permits, a user, to see and work with physical objects at close range (near field) and to have these objects transition to virtual images or computer-transformed video as they move to a threshold distance away from the user, and beyond that distance (far field). The system also provides a field of view visual system by using motion cueing systems to account for user position and orientation. The system uses live video capture, real-time video editing, and virtual environment simulation.","System","One preferred embodiment of the inventive system comprises cameras, processors, image generators, position detectors, displays, physical objects, and a physical space.  illustrates one embodiment of the system of the present invention. A user  is equipped with a head mounted display (HMD) . Atop the HMD  is mounted a camera  for receiving the actual physical image  viewed by the user . The camera may alternatively be integrated with the HMD or not, but is mounted at some location where the camera  at least approximately has the same view as the user's eyes. The user  is equipped with a head tracker  which provides 3D spatial information about the location and attitude of the head of the user . The head tracker is used to permit proper perspective and viewing angle of the virtually generated portions of the display image on the HMD .","The user  can interact with physical objects in the environment. In , the user  is shown interacting with a control handle . Some physical objects may be used to represent real world counterparts, such as accelerators, steering mechanisms, firing devices, etc.","The output of the camera  is provided to a conventional image capture device, represented by block  and then to a conventional image processing device or circuit, represented by block . The purpose of the image processor  is to identify all areas of a real video image that should be transmitted through to the HMD  and which areas are to be overlaid with virtual imagery.","Head tracker  is coupled to spatial information algorithm, device or circuit, represented by block  where the location and attitude of the user's head is derived. This information is provided to a conventional 3D simulation algorithm, device or circuit, represented by block  which generates a possible 3D image based on the location of user  and the line of sight of user . Any input from physical devices is provided to conventional physical trigger information algorithm, device or circuit, represented by block  and then to a conventional 3D simulation processor, represented by block . Trigger block  is used to indicate any changes that should be made to the generated virtual image based on manipulation of physical objects by user . The output of 3D simulation block  is provided, along with the output of image processing block , to a conventional image combination algorithm, device or circuit, represented by block . The virtual image is overlaid with the real image via a masking process so that the virtual image is only visible in desired areas of the frame. This combined image is provided to the user via the HMD  and it is this combined image that the user  views.","Environment","One preferred embodiment of the systems is used in a combination physical\/virtual environment. The physical environment may vary from application to application, depending on the desired end use. By way of example, consider the inside of a vehicle, such as a helicopter, truck, boat, etc.  illustrates a partial view of an interior with a combination of physical and virtual regions defined. Referring to , a wall  is shown with a windshield , a door  and window  defined in the wall (virtual world). This might represent the interior of a helicopter, personnel carrier, boat, or some other environment. The virtual spaces - are represented in this embodiment by the application of a specific electromagnetic threshold, such as magenta. In one embodiment, the defined virtual surfaces - are flat surfaces painted the desired electromagnetic spectrum (e.g., color). In another embodiment, the areas - are openings in wall , backed with shallow dishes painted the desired color. In such an embodiment, the user  can actually extend himself and physical objects seemingly beyond the boundaries of the defined environment.","Image Generation","The system of , when used with an environment such as is shown in , provides an environment that is a combination of real and virtual images.  is a flow chart describing how image combination and generation takes place. At step , the camera  receives a video frame. At step  the frame is digitized to yield a frame buffer of digital value pixels. Alternatively the camera could be a digital camera that captures the image as digital pixels. The pixels are stored with attributes including color and intensity. For example, the pixels may be stored as 32 bit values with eight bit red, green, and blue values along with an eight bit alpha value (RGB).","At step  the color of each pixel is compared to a target masking color. In one preferred embodiment, the target value or color is magenta. Magenta is preferred because it is an atypical color in most environments and has relatively high selectability in different light conditions. The goal is to render a frame mask that makes each pixel that matches the target color to be transparent. If the target color is matched by the pixel under review, the pixel is turned transparent at step . If no, the original color of the pixel is maintained at step . This decision process is performed for each pixel in each frame.","At step  the virtual image is generated based on the current state of the environment and other factors described below. At step  the video image (with matching pixels rendered transparent) is overlaid onto the virtual image. The combined image will show the actual video except where the pixels have been turned transparent. At those locations the virtual image will be seen. At step  this combined image is provided to the HMD and the user sees a combination of real and virtual images.",{"@attributes":{"id":"p-0056","num":"0055"},"figref":["FIGS. 4A-4C","FIG. 4A","FIG. 4B","FIG. 4B","FIG. 4C","FIG. 4C"]},"In an alternate embodiment, rather than specify the color range of the pixels that will be made transparent, i.e. the background color, the color range of the pixels that will be preserved will be specified\u2014all other pixels would be rendered transparent and replaced with the virtual environment. For instance, green could be designated as the color that will be preserved. Thus a trainee's flight suit and flight gloves would be displayed as a real-time image that the trainee observes. Interactive hardware that is physically touched, such as a gun, litter, or hoist, that is painted a green would similarly be displayed, as would the trainee's boots if they are sprayed with, for example, a non-permanent coating of green. The rest of the environment could be virtual, consisting mainly of texture maps of the cabin interior and hardware that will not be touched by the viewer.","Training of Color Recognition","One aspect of the system that relates to the use of a target color in an RGB system as a filter for combining images concerns a problem related to accurate tracking of the color in a variety of dynamically changing lighting conditions. The color magenta may not appear to be a color within the threshold range of recognition in different lighting conditions. For example, the magenta background may appear closer to white in extremely bright lighting and closer to black in low light conditions. If the target color and zones are not recognized accurately, the image combination will not look realistic.","Another embodiment of the system implements a camera with a user-controlled exposure setting to address this problem. Many micro cameras only offer auto-exposure, as a cost and space-saving feature, whereby the camera self-adjusts to the sensed light intensity in its field-of-view. This automatically changes the color settings of all viewed objects so as to maximize overall contrast. However, such designs do not allow tight tolerances to be set for the color that is to be filtered in the system, such as, for example, magenta. Using auto-exposure, tolerances would have to be low enough to accommodate for changes in environmental lighting and reflected object brightness, but this could allow unintended colors in the video image to be filtered, or conversely, fail to be filtered when desired. By selecting and fixing the camera exposure level, the color of objects in the video image would remain constant for a given lighting level. In another embodiment and to further ensure that the portal surface color to be filtered remains constant, the portal surface color could be made to emit its own light instead of relying on reflected light.","Yet another solution to target color recognition is to train the system in a variety of lighting conditions so that accurate pixel masking may result. In attempts to produce this, light intensity reaching a magenta panel is varied by changing the distance between a light bulb and the panel. The camera is trained on the magenta panel while in the auto-exposure mode, and for each new distance the RGB components registered by the camera are recorded\u2014in effect generating an RGB map for varying light intensities.  show the resulting profiles of Green and Blue as functions of Red intensity. Any measured value of Red that the camera registers can be checked against the corresponding Green and Blue values that the profiles predict. A match results if the predicted and measured values fall within a predetermined range of each other.","With the adaptive color recognition in place, the camera can be in auto-exposure mode, where the picture gain is automatically increased or lowered, that is, made brighter or darker, as the camera attempts to keep the overall picture brightness constant. This is a feature available in most if not all video cameras. Consequently, the present system is not limited to more expensive cameras that include manual exposure or fixed exposure. Instead, nearly any simple web cam, which can measure as little as 1\u2033 in length, can be used, reducing cost and complexity of the system while increasing its robustness to variability.","Pixel Masking",{"@attributes":{"id":"p-0062","num":"0061"},"figref":["FIG. 7","FIGS. 5 and 6"],"b":["601","602","603","604","605"],"i":"a "},"At step  it is determined if the green value is within the acceptable range for the corresponding red value. If so, then the pixel is considered to be the target color and is made transparent at step . If not, the pixel is left as is at step .","Near-Field to Far-Field Transitions and Vice Versa","One advantage of the present systems and methods is that they allow a user to observe and physically interact with the near-space environment or domain while the simulated far-space domain is seamlessly interwoven into the visual scene. Additionally, these techniques enable a user to physically hold an object, release and send it into the far-space environment, such as a litter lowered from a helicopter cabin toward the simulated water below, perform tasks that affect that object, which is now part of simulated far-space, and retrieve and physically grasp the object once again as it returns to the near-space domain.","Current virtual reality (VR) graphics techniques distort perspective in near-space environments and for that reason they do not provide the capability to effectively combine near- and far-field images in a way to permit effective interaction between these environments with physical as well as simulated objects. Specifically, conventional VR systems have distorted representations of objects that are relatively close to the observer, e.g., closer than arm's length, because they distort perspective at these distances. For the VR user to perform basic manual tasks such as gunnery, the digits of the hands would have to be tracked\u2014not just the fingertips, but also the joints and hands. Where speed and dexterity are required for complex manual tasks, such as removing a gun barrel, it is believed that conventional VR would not be feasible due to masking, sensor lag, and component simulation fidelity issues. Furthermore, with regard to design of conventional VR systems, the far-space environment that is projected onto screens is clearly distinguishable from the near-space environment that includes, for example, cockpit controls, hands, etc., which detracts from realism. It is believed that this delineation between environments can arise from: screen distortion effects, seaming and blank space between screens that are intended to connect continuously, low screen resolution, screen reflection, etc. In contrast the present systems and methods convert both the near and far-space into bitmaps, so that the visual quality of the two environments is much more consistent than in conventional VR technology.","To accomplish an effective transition and realistic presentation of near-field to far-field images, the present systems and methods use images of the actual physical device being used in the simulation. For example, consider when the simulation is a helicopter, and the device to be used in near-field and far-field is a stretcher on a winch. One task for a user of the system is to maneuver the stretcher out of a door of the helicopter and lower it below to a downed pilot or to a person stranded in an otherwise inaccessible location to train for a rescue operation.","In such an example, the stretcher is lowered from the helicopter with a winch that is located and operated within the helicopter cabin. The aircrew user(s) would not make physical contact with the stretcher when the winch is in operation. Rather than build an actual replica of the stretcher and place it outside the cabin, texture maps of the stretcher's image taken at different perspectives, for example, eight perspectives ranging from a direct side view to looking straight down from on top, could be used. These photos or images would initially be taken with a colored backdrop and later processed in accordance with the description herein so that only the pixels belonging to the hardware remained, that is, the backdrop color pixels would have been removed. These eight texture maps would then be assembled into a mesh using conventional techniques, similar to putting together a box. The resulting 3D texture map mesh would provide the user extremely realistic perspectives of the stretcher-winch-line assembly as the stretcher (mesh) is virtually lowered from the cabin to the water below. The winch and cable could be actual hardware, because the aircrew must physically interact with both. The stretcher texture map translation is preferably slaved to the winch's actual rotation in accordance with the description herein and conventional techniques.","To accomplish an effective transition and realistic presentation of near-field to far-field images, the present systems may also use real-time bitmaps of the object(s) that are being deployed into and\/or retrieved from virtual space. In this technique each object to be deployed is identified and isolated by the computer, and the image's bitmap is attached to a virtual billboard. This billboard can then be translated and rotated within the virtual simulated environment, and can be occluded from view by other virtual objects when it is moved behind them. Thus, a person can be placed inside a stretcher and physically lowered a short distance, after which the image of both the stretcher and person could be attached to a virtual billboard. This billboard then reacts virtually to the hoist operator commands, that is, it is lowered and raised while the operator views the real-time, physical movements of the stretcher, e.g., swaying and twisting, and of the person inside, e.g., waving.","The object to be deployed can be identified and isolated by the computer using a variety of methods including: (1) user head position and orientation; (2) object position and orientation; (3) edge detection and object recognition; (4) depth ranging; or (5) framing the object with a keying background color, or the object being completely framed in darkness if using brightness keying.","The near-field\/far-field transition capabilities of the present systems and methods permit a range of training exercises and manipulations that would not be possible in a traditional VR system. With weapons, for example, a user can hold a physical weapon in his hands in the near-field. Use of the trigger activates bullets or other projectiles that would appear only in the far-field.","Another example of the near-field\/far-field transition is given in the following aircraft rescue hoist example.  is an image of a helicopter and shows the location of the hoist hook attach point at . A camera is preferably fixed to the airframe frame at location shown at , and a conventional ultrasound tracker is preferably placed adjacent to the hook, shown at . The concept and user-component spatial relationships that are preferably employed for the hoist simulation are shown in . The fixed camera  provides a stable view of the rescue litter  after it has been physically lowered below the cabin deck . It should be noted that more than one fixed camera can be used to proved a positionally anchored view of the litter. The images of multiple cameras can be tiled to one another and the sub-area of the composite picture selected as a function of the user's head position and attitude. This can enable a very wide field of view of the physical object(s) that will be virtually moved. If the user's HMD camera  was used to isolate the rescue litter, clipping of the litter could occur as the user moved his head relative to the litter . For instance, the HMD camera's field-of-view is given by the angle extending between dashed lines  and . If this image is pasted to the virtual billboard, as the billboard is lowered the operator would expect the fringes of the rescue litter to come into view\u2014however, they would not since they are not captured by the HMD camera. The fixed camera, however, is positioned such that the entire litter would be in view for all instances of operation, because the field of view between  and  capture the full length of the litter. Prior to the lowering rescue litter reaching the level of the cabin deck  the user could or will view the litter from his HMD camera . An ultrasound tracker provides high resolution head position data which is used to determine precisely the head-mounted camera position  relative to the fixed camera's position, shown at . The magenta-colored floor  acts as a backdrop to the litter. Once the litter  has reached a predetermined level above the floor or deck  the cable (not shown) will physically cease paying out, but the electrical commands of the hoist control will command the depth of the virtual litter, composed of real time video from the fixed camera. The fixed camera video pixels associated with the litter and rescue personnel are isolated via Chromakey and pasted onto a transparent virtual billboard, and this billboard moves in the virtual environment in response to the hoist controller's commands and the helicopter's virtual motion. The video from the hoist operator's helmet-mounted camera is processed such that the pixels associated with the litter and rescue personnel are replaced by the fixed camera video. All other video from the helmet-mounted display is preserved and viewed by the operator.","The video associated with the area above the deck area is removed, as shown in , as are the video pixels belonging to the magenta keying color.  is a schematic illustration showing a representation of a rescue litter , a cabin deck  and a flight glove  from a fixed camera view, i.e., a raw video view. All of the area below the cabin deck is discarded, as shown at . The remaining video pixels are those of the litter  and any pixels that are contained within or are overhanging the litter. The isolated litter video, shown in , is then pasted onto a virtual billboard. The user's tracked head position and attitude relative to the fixed camera is used to translate and rotate the user's perspective so that the observed position and orientation of the virtual billboard is consistent with the user's motion. If the user's hands or arms are in view of the HMD camera, as shown at  in , the green pixels associated with the flight suit\/gloves will be preserved but will not be pasted to the virtual billboard. In this way the user views the hands and arms that are over the deck edge in a conformal manner, shown for example in  at , with a background image also shown in .  illustrates the layer hierarchy described above, with the viewer's eye shown at , the green pixels and pixels aft of the cabin deck shown at , the litter placed on the virtual billboard  and the image generated scene shown at . When the hoist commands raises the virtual billboard to the level where the actual litter exists, that is, when the virtual cable length matches the actual cable length, the video will be removed from the virtual billboard, and the user will then view the litter from his\/her own HMD camera .",{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 15","b":["701","702","703","704","702","703","705","706","703","705","704"]},{"@attributes":{"id":"p-0074","num":"0073"},"figref":["FIGS. 16A-16C","FIG. 16A","FIG. 16A","FIG. 16B","FIG. 16B","FIG. 16C"],"b":["800","801","802","803"]},"Hue, Saturation and Brightness Color Coordinate System","The present system and methods may also use the Hue, Saturation and Brightness (HSV) color coordinate system for target recognition purposes. The HSV model, also called HSB, defines a color space in terms of three constituent components as will be described with reference to .","For the purposes of the present systems and methods Hue or \u201cH\u201d specifies the dominant wavelength of the color, except in the range between red and indigo, that is, somewhere between 240 and 360 degrees, where Hue denotes a position along the line of pure purples. The value is roughly analogous to the total power of the spectrum, or the maximum amplitude of the light waveform. However, as may be seen from the equations below that value is actually closer to the power of the greatest spectral component (the statistical mode, not the cumulative power across the distribution).","Similarly, in the present systems and methods Saturation or \u201cS\u201d refers to the \u201cvibrancy\u201d of the color, and its values range from 0-100%, or 0.0 to 1.0. It is also sometimes called the \u201cpurity\u201d by analogy to the colorimetric quantities excitation purity and colorimeric purity. The lower the saturation of a color, the more \u201cgrayness\u201d is present and the more faded the color will appear. The saturation of a color is determined by a combination of light intensity and how much it is distributed across the spectrum of different wavelengths. The purest color is achieved by using just one wavelength at a high intensity, such as in laser light. If the intensity drops, so does the saturation.","In the present system the term Value or \u201cV\u201d refers to the brightness of the color, and this value ranges from 0-100% with 0% representing the minimum value of the chosen color and 100% representing the maximum value of the chosen color.","Given a color in the RGB system defined by (R, G, B) where R, G, and B are between 0.0 and 1.0, with 0.0 being the least amount and 1.0 being the greatest amount of that color, an equivalent (H, S, V) color can be determined by a series of formulas. Let MAX equal the maximum of the (R, G, B) values and MIN equal the minimum of those values. The formula can then be written as",{"@attributes":{"id":"p-0080","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"H","mo":"=","mrow":{"mo":"{","mrow":{"mrow":[{"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":{"mn":"60","mo":"\u00d7","mfrac":{"mrow":[{"mi":["G","B"],"mo":"-"},{"mi":["MAX","MIN"],"mo":"-"}]}},"mo":"+","mn":"0"},"mo":","}},{"mrow":{"mrow":{"mi":["if","MAX"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":"=","mi":"R"}}]},{"mtd":[{"mrow":{"mrow":{"mrow":{"mn":"60","mo":"\u00d7","mfrac":{"mrow":[{"mi":["B","R"],"mo":"-"},{"mi":["MAX","MIN"],"mo":"-"}]}},"mo":"+","mn":"120"},"mo":","}},{"mrow":{"mrow":{"mi":["if","MAX"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":"=","mi":"G"}}]},{"mtd":[{"mrow":{"mrow":{"mrow":{"mn":"60","mo":"\u00d7","mfrac":{"mrow":[{"mi":["R","G"],"mo":"-"},{"mi":["MAX","MIN"],"mo":"-"}]}},"mo":"+","mn":"240"},"mo":","}},{"mrow":{"mrow":{"mi":["if","MAX"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":"=","mi":"B"}}]}]},"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}},"mi":"S"},{"mrow":[{"mfrac":{"mrow":{"mi":["MAX","MIN"],"mo":"-"},"mi":"MAX"},"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}},"mi":"V"},{"mi":"MAX","mo":"."}],"mo":"="}],"mo":"="}}}}}},"The resulting values are in (H, S, V) form, where H varies from 0.0 to 360.0, indicating the angle in degrees around the color circle where the hue is located. The S and V values vary from 0.0 to 1.0, with 0.0 being the least amount and 1.0 being the greatest amount of saturation or value, respectively. As an angular coordinate, H can wrap around from 360 back to 0, so any value of H outside of the 0.0 to 360.0 range can be mapped onto that range by dividing H by 360.0, taking the absolute value and finding the remainder. This type of calculation is also known as modular arithmetic. Thus, \u221230 is equivalent to 330, and 480 is equivalent to 120, for example.","For a given target hue and saturation range, a range of brightness values can be specified that would correspond to the range of lighting conditions that could be expected in the operating environment.","Pixel Masking in the HSV System","Another solution to target color recognition results from training the system in a variety of lighting conditions so that accurate pixel masking may result. In attempts to produce this, light intensity reaching a colored panel is varied by changing the distance between a light bulb and the panel. The camera is trained on the colored panel while in the auto-exposure mode and for each new distance the HSV components registered by the camera are recorded. This in effect generates an HSV map for varying light intensities.  shows an example of HSV thresholds, and if a pixel's HSV values all fall within them the pixel is rendered transparent.",{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIGS. 19-24","FIG. 19","FIG. 20","FIG. 21","FIGS. 22-24","FIGS. 19-24"]},"Depth Ranging","With reference to , an alternative method to using color for virtual portal designation is using the depth of sensed object pixels is described. In this context the term depth refers to the distance between the position of a predetermined object and the position of a predetermined sensor, preferably a camera mounted on or near the head of the user. In this context the term \u201cpixel depth\u201d refers to those pixels associated with the distance of a sensed object from a predetermined sensor, preferably a camera. The parallax in the stereoscopic image can give range information on each pixel, as can laser ranging. Pixels within a given range threshold can be preserved\u2014those outside of the threshold can be made transparent. This approach would eliminate the requirement of a background color, such as magenta, and relatively few to no modifications would have to be made to a cabin or cockpit to accommodate system requirements to create a combined environment.  shows a representation of such an alternate system and process. User  has an HMD  and a head tracker . The user is also shown holding aircraft controls . HMD camera  and cameras  and  are used for sensing depth. Cameras  and  are mounted on the HMD, flanking, left and right camera  as shown in , and\/or mounted in the environment external to the HMD. When mounted in the environment, cameras  and  can be commanded to swivel and translate in response to the user's head movements. Additional cameras can be placed in the environment. The purpose of the cameras is to create a depth map of the user's environment.","In  cameras ,  and , and tracker  provide information to the conventional depth processor circuit , which correlates the image pixels of HMD camera  to depth. In this example the image sensed by HMD camera  is composed of a near-field window sill  and a far-field backdrop . Depth information on the image is sent to the video capture circuit or board , where a check, shown at , is performed on each pixel to determine if its depth lies beyond a predetermined distance. If so, the pixel is rendered transparent in . Signals from control devices  that the user manipulates to interact with the virtual environment, as well as head spatial information, represented at  are sent to the 3-D simulation circuit , which produces a graphical representation of the simulation, shown at . The processed video layer  is overlaid on the simulation layer  in the overlay circuit , either through digital combination or a conventional 3D graphics application programming interface, and the composite image  is sent to the user's HMD  for display.","An alternative application of depth-keying is given in . As in  the image observed by the HMD camera is composed of a near-field object  and a far-field object . The near-field object pixels are preserved based on sensed range, and the far-field object pixels are rendered transparent. The near-field pixels are sorted by depth and proximity in circuit  and pasted onto transparent virtual billboards in . This output  is sent to the 3D graphics API and embedded into the 3D simulation environment , shown in . The output  is sent to the HMD for display to the user.","In  the pixel depths computed from the depth processor  and the planar locations of the pixels on the image are used to position the video pixels and their billboard  appropriately within the virtual scene. In this way one or more objects associated with the physical near-field scene can be virtually situated, via one or more billboards, in front of and behind virtual objects, occluding and being occluded by the virtual objects, shown at . User controls can also be employed to move both the virtual objects and virtual billboards within the scene.",{"@attributes":{"id":"p-0089","num":"0088"},"figref":["FIG. 27","FIG. 25-26","FIGS. 25-26","FIG. 25"],"b":["914","920","916","918","920","922","924","926","872","886","896","928","932","940","928","886"]},{"@attributes":{"id":"p-0090","num":"0089"},"figref":["FIG. 28","FIG. 29","FIG. 29"],"b":["902","900","904","900","904","906","910","900","904","912","912","902","902"]},"Conversely, where near-field objects are of primary interest,  shows the flanking cameras  and  oriented such that their FOV's  and  include objects that are nearly directly in front of the central camera. Objects that lie in region  will be depth-keyed in the image of camera , and objects that lie within region , i.e., those that are not included in all the FOV's ,  and , are designated as far-field and can be keyed out. The camera configuration shown in  would be used when stereoscopic cueing would not be very useful to the viewer, i.e., the user will not be conducting tasks that require near-field depth judgment such as manual tasks, and a monocular view from camera  would suffice. When stereoscopy would be important for near-field operations and far-field objects do not appear, such as in an enclosed room, only two cameras as shown in  would be needed. Objects that lie in region  would be both depth-sensed, that is, available for depth-keying, and displayed stereoscopically via each camera  and  to the left and right eyes, respectively.","With reference to , for operations that involve near-field tasks where stereoscopic cues are important, and require approximate far-field sensing, the camera configuration shown in  would be employed. In  objects that dominate the central portions of cameras  and  and are shared by both cameras, indicated by a high cross-correlation in block  in , will trigger stereoscopic viewing , otherwise monocular viewing via camera  will be triggered at . In  object pixels from camera  are cross-correlated with object pixels from camera . Any object pixels that appear in both flanking cameras  and  FOV's, shown as region  in , and indicated by a high cross-correlation in block  will be explicitly depth-sensed via stereo imaging at block , otherwise in  they will designated as far-field, that is, corresponding to region  in .","Although specific embodiments of the invention have been described, various modifications, alterations, alternative constructions, and equivalents are also encompassed within the scope of the invention. The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense. It will, however, be evident that additions, subtractions, deletions, and other modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication with color drawing(s) will be provided by the Patent Office upon request and payment of the necessary fee.","The foregoing aspects and the attendant advantages of the present invention will become more readily appreciated by reference to the following detailed description, when taken in conjunction with the accompanying drawings, wherein:",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 3","FIG. 1"]},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIGS. 4A-4C"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 9","FIG. 8"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 10","FIG. 8"]},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 11","FIG. 10"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 12","FIG. 8"]},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 13","FIG. 8","FIG. 10"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 14","FIG. 8"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIGS. 16A-16C"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIGS. 19-24"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 25"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 26","FIG. 25"]},{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 27","FIGS. 25-26"]},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 29","FIG. 28"]},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 30"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 32"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":["FIG. 33","FIG. 32"]},{"@attributes":{"id":"p-0042","num":"0041"},"figref":["FIG. 34","FIG. 32"]}]},"DETDESC":[{},{}]}
