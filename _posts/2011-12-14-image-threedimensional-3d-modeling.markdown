---
title: Image three-dimensional (3D) modeling
abstract: Among other things, one or more techniques and/or systems are disclosed for creating a three-dimensional model of an image. Image data, comprise a series of images of a location, such as along a route, can be received. The image data can comprise a first image element location for a first image element, where the first image element location may correspond to a three-dimensional point in the image, such as a location in the image with a depth from a point of observation. The first image element can be segmented into a first façade plane using the first image element location. The first façade plane can be merged with a second façade plane, resulting in a three-dimensional model of the image. The second façade plane can comprise a second image element, where the image data comprises a second image element location for the second image element.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09324184&OS=09324184&RS=09324184
owner: Microsoft Technology Licensing, LLC
number: 09324184
owner_city: Redmond
owner_country: US
publication_date: 20111214
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Digital photography can allow for a sequence of images to be stitched or glued together to provide for a relatively seamless transition from one image to the next. Further, images, such as side-view images, can be collected while traveling along a route, such as a street. Stitching these side-view images together can provide a user experience of travelling along the route, for example.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key factors or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","A side or lateral-view of panoramic imagery may comprise a series of images (e.g., photo frames, video frames, etc.) stitched\/glued together to form a somewhat seamless view of the imagery. This type of planar panorama imagery typically displays many of the stitched together images at a same time. Currently, images stitched together in this manner can be utilized in conjunction with digital mapping services, where, for example, a user may view planar panorama-type images of a street associated with a map they may be viewing.","When moving laterally along planar panorama imagery, however, a user might see different perspectives of objects in the panorama and\/or objects in the images may be altered due to different viewing angles from which the different images were acquired and\/or resulting from imprecisions in image stitching processes, for example. Desired objects may, for example, be obscured from view by other (e.g., foreground) objects due to a line of sight at image capture, and\/or may comprise artifacts (e.g., distortions) that result in a diminished user experience.","Accordingly, one or more techniques and\/or systems are disclosed that may provide a more natural viewing experience, for example, by creating a three dimensional (3D) model of image data comprising imagery and rendering a lateral panorama using the 3D model. In this way, for example, a user may view a desired object from different angles, distortions in stitched together images may be mitigated, and\/or a more natural 3D effect can be applied to imagery. A natural 3D effect may be applied by distinguishing fa\u00e7ade layers in the imagery, and grouping image elements (e.g., pixels) into their appropriate fa\u00e7ade layers. A 3D location of an image element may be used to group the element into an appropriate fa\u00e7ade layer, for example.","In one embodiment of creating a three-dimensional model of an image, image data comprising a first image element location can be received for a first image element in the image data. The first image element location may correspond to a three-dimensional point in the image. Further, the first image element can be segmented into a first fa\u00e7ade plane based at least upon the first image element location. Additionally, the first fa\u00e7ade plane can be merged with a second fa\u00e7ade plane resulting in the three-dimensional model of the image. The second fa\u00e7ade plane can comprise at least a second image element, where the image data comprises a second image element location for the second image element.","To the accomplishment of the foregoing and related ends, the following description and annexed drawings set forth certain illustrative aspects and implementations. These are indicative of but a few of the various ways in which one or more aspects may be employed. Other aspects, advantages and novel features of the disclosure will become apparent from the following detailed description when considered in conjunction with the annexed drawings.","The claimed subject matter is now described with reference to the drawings, wherein like reference numerals are generally used to refer to like elements throughout. In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the claimed subject matter. It may be evident, however, that the claimed subject matter may be practiced without these specific details. In other instances, structures and devices are shown in block diagram form in order to facilitate describing the claimed subject matter.","As provided herein, a method may be devised that provides for creating a global three-dimensional (3D) model using image data, for example, where the image data may comprise a series of lateral images of a location (e.g., imagery), such as a roadside. The 3D model may be used to render a 3D version of the imagery, such as by rendering an object in the imagery from two different points of observation, for example. As an example, respective 3D points for pixels comprised in the imagery can be identified, and the 3D points may be grouped into fa\u00e7ade planes. The fa\u00e7ade planes may be merged together to create a 3D model. The resulting 3D model may be used to render the imagery in a variety of ways, for example, that may provide an enhanced user experience.",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1","b":["100","100","102","104"]},"In one embodiment, the image data may comprise a series of images captured for a location, such as along one or both sides of a street (e.g., captured while traveling along the street). Further, in one embodiment, respective image elements (e.g., pixels) in the image data may comprise a corresponding element location that identifies a 3D point in the image. In one embodiment, the image element location may comprise information identifying a two-dimensional (2D) location (e.g., X, Y coordinate), and a depth of the element (e.g., from a defined point).","For example, the 2D location can comprise a grid coordinate location (e.g., pixel location in columns and rows of image grid), which may be identified using a pre-defined grid overlaid on the image. Further, for example, the depth of the element may comprise a measurement (e.g., estimate) from a point of observation for the image (e.g., image capture location), and\/or from a defined location (e.g., a side of a street comprised by the imagery).","At  in the exemplary method , the first image element is segmented into a first fa\u00e7ade plane based at least upon the first image element location. In one embodiment, a fa\u00e7ade plane may comprise a viewable portion of a face of one or more objects, such as a side of a building (e.g., or group of buildings), a portion of landscape, and\/or a group of objects in a relatively similar plane, etc.","Further, in this example, respective objects in the image may comprise (e.g., or be comprised in) a corresponding fa\u00e7ade plane that is situated at a particular depth. As an illustrative example, a front of a building may comprise a first fa\u00e7ade plane, landscaping in front of the building may comprise a second fa\u00e7ade plane, and mountains behind the building may comprise a third fa\u00e7ade plane. In one embodiment, the element (e.g., pixel) location can help determine into which fa\u00e7ade plane the element can be segmented, for example, based on its 2D location (e.g., falling within the building face) and depth (e.g., corresponding the depth of the building face from the point of image capture).","At  in the exemplary method , the first fa\u00e7ade plane is merged with a second fa\u00e7ade plane, resulting in the three-dimensional model of the image. The second fa\u00e7ade plane comprises at least a second image element, where the image data comprises a second image element location for the second image element. As an example, the imagery may comprise a plurality of identified fa\u00e7ade planes, respectively comprising image elements (e.g., pixels) segmented for (e.g., grouped together in) the fa\u00e7ade plane.","The first and second fa\u00e7ade planes can be merged to create a 3D model of the object for the imagery (e.g., providing different lines of sight for a same object). Additionally, as an example, a plurality of fa\u00e7ade planes (e.g., representing a plurality of object faces) can be merged from a plurality of images, comprised in the image data, to generate a 3D model for the imagery represented by the image data. Having merged the first and second fa\u00e7ade planes, resulting in the 3D model of the image, the exemplary method  ends at .",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 2","b":["200","202","250"]},"In one embodiment, an image element may comprise a pixel. Often, \u201cpixel\u201d is used to describe a unit of an image, for example, where the unit may comprise a smallest element of the image that can be represented and\/or controlled. As one example, a pixel may comprise an addressable screen element of a display device (e.g., screen pixel, sub-pixel), a single point in a raster image, and\/or a single point in a printed picture. Further, as an example, a \u201cpixel\u201d may comprise an \u201caddress\u201d corresponding to coordinates (e.g., X, Y coordinates, row and column coordinates, Euclidean space coordinates, etc.) for the image, and\/or display screen. In one embodiment, the image element may comprise any type of image \u201cunit\u201d that can be represented and\/or controlled.","In the example embodiment , identifying the image element location, at , can comprise determining a 3D point for the element, in the imagery, at . In one embodiment, determining the three-dimensional point can be based at least upon a two-dimensional image location for the image element and an image element depth for the image element. At , the two-dimensional image location can be determined, based at least upon a two-dimensional image element coordinate location (e.g., an \u201caddress\u201d of the \u201cpixel\u201d) in the image.","At , the image element depth (e.g., Z coordinate) can be determined. In one embodiment, the image element depth may be based on detection and ranging data collected at image capture. As one example, a light detection and ranging (LIDAR) device, a radio wave detection and ranging (RADAR) device, a device combining both light and radio wave detection and ranging (e.g., LADAR), and\/or some other detection and ranging device (e.g., ultrasonic) may be used to collect the detection and ranging data at a same time that the image data is captured. That is, for example, when the image is captured (e.g., of a side of a street) the detection and ranging device may also capture depth data for objects comprised within the image.","In one embodiment, the image element depth may be based on stereoscopic estimation using at least two images respectively comprising different views. As one example, a type of \u201ctriangulation\u201d may be used to estimate a Z-axis depth of a pixel in imagery, where the pixel comprises a portion of an object captured by the imagery, and the imagery comprises at least two different views of the portion of the object comprised by the pixel. In this example, a distance between the observation points of the at least two images is known, and a \u201cline-of-sight\u201d angle to the pixel is also known. Using the \u201cknown\u201d information, a depth from a common observation point may be estimated (e.g., merely estimated due to image capture noise).","In one embodiment, the image element depth may be based on surface modeling of a location comprised by the image. As one example, a digital surface model (DSM) (e.g., DSM may also indicate \u201cdense surface mapping,\u201d \u201cdense surface modeling,\u201d and\/or \u201cdigital surface mapping,\u201d which may be used synonymously with digital surface modeling) may comprise a 3D representation of a surface, such as a portion of the Earth. The DSM may indicate structures, terrain, landscaping, objects, depressions and\/or other items on and\/or in a modeled surface (e.g., area of a neighborhood). In one example, a DSM of an area comprised by the imagery may be reviewed, and one or more objects captured by the imagery may be identified in the DSM. In this example, an estimated depth between an identified object and a desired point of observation may be determined using the DSM, where a distance scale for the area comprised by the DSM may be used to determine an estimated distance (e.g., image element depth).","It will be appreciated that a determination of the image element depth is not limited to the embodiments described herein, and that the instant application, including the scope of the appended claims, is not intended to exclude alternate methods for identifying an image element depth. As an example, data comprising known object measurements (e.g., physical distance measurements identified in public and\/or private records) may be used to estimate a depth from a desired observation point to an object.","At  in the example embodiment , identified image elements can be segmented. At , one or more fa\u00e7ade planes may be identified in the image data. In one embodiment, a fa\u00e7ade plane may comprise a representation of a visible portion of an object face that lies in a plane. As an illustrative example,  is illustrates an example embodiment  where one or more portions of one or more techniques described herein may be implemented. As one example, an image may capture a view of objects, such as utilities, buildings and landscape objects, as viewed from a point of image capture.","In the example embodiment , a visible portion of a utility pole face  lies in a foreground plane ; a visible portion of a building face  lies in a mid-ground plane ; and a visible portion of a mountain face  lies in a background plane . In this example, the utility pole face  can serve to define a first fa\u00e7ade plane; the building face  can serve to define a second fa\u00e7ade plane; and the mountain face  can serve to define a third fa\u00e7ade plane. That is, for example, image element location information for image elements (e.g., pixels) that make up the utility pole face  may indicate that these elements are substantially located a particular depth corresponding to the foreground plane  such that the foreground plane  is defined at this depth, or that the utility pole face  is located in the foreground plane . Similarly, image element location information for image elements (e.g., pixels) that make up the building face  may indicate that these elements are substantially located a particular depth corresponding to the mid-ground plane  such that the mid-ground plane  is defined at this depth, or that the building face  is located in the mid-ground plane . Also, image element location information for image elements (e.g., pixels) that make up the mountain face  may indicate that these elements are substantially located a particular depth corresponding to the background plane  such that the background plane  is defined at this depth, or that the mountain face  is located in the background plane . Further, as one example, the first, second and third fa\u00e7ade planes may be comprised in a first image of the image data. In this example, a second image in the image data may comprise a fourth, fifth, and sixth fa\u00e7ade plane, respectively comprising a different visible portion (e.g., face) of the utility pole, building and mountain.","Returning to , at , one or more image elements can respectively be grouped into a corresponding identified fa\u00e7ade plane. In one embodiment, a first image element can be grouped with one or more first fa\u00e7ade image elements, where the first fa\u00e7ade image elements correspond to a first viewable portion of an object face at a first desired depth, for example. Further, a second image element can be grouped with one or more second fa\u00e7ade image elements, where the second fa\u00e7ade image elements correspond to a second viewable portion of an object face at a second desired depth, for example.","In one embodiment, where fa\u00e7ade planes are parallel to a view plane (e.g., a plane comprising a view from a desired observation point), the first image element can be grouped into the first fa\u00e7ade plane that is comprised at a first desired fa\u00e7ade depth, which corresponds to an image element depth for the first image element. As an illustrative example, in  an image may be broken or segmented into three parallel planes , , , which may be parallel to a view plane , as viewed from a desired observation point  (e.g., point of observation for a rendered view of the image).","In this example, the first image element may comprise a pixel having a 2D location (e.g., grid coordinates) that lies within the face of the utility pole face  in the image. Further, if the image element depth of the first image element is comprised within a desired depth range for the first plane , the first image element can be grouped into the first fa\u00e7ade plane, for example. However, for example, if the image element depth of the first image element is comprised within a desired depth range for the second plane , the first image element may not be grouped into the first fa\u00e7ade plane . As an example, from the desired observation point , the utility pole may lie in front of the building. In this example, if the 2D location of the first image element also lies within the building face , and the image element depth of the first image element is comprised within the desired depth range for the second plane , the first image element can be grouped into the second fa\u00e7ade plane, for example, comprising the building face  (e.g., thus illustrating the utility pole in front of the building).","In one embodiment, fa\u00e7ade planes may not be parallel to one another and\/or the view plane. As an example, a side of a building viewed from an observational point in front of the building may comprise a plane having a Z-axis (e.g., depth) dimension. That is, for example, the first fa\u00e7ade plane may lie along a vanishing point or ray from the point of observation, which is not parallel to the view plane. In this embodiment, the first image element may be grouped into the first fa\u00e7ade plane if the first image element comprises a 2D location within the visible face of the object comprised by the first fa\u00e7ade plane, and if the first image element comprises an image element depth within a desired depth range for the first fa\u00e7ade plane (e.g., within a Z-axis range for the first fa\u00e7ade plane).","Returning to , at , one or more first fa\u00e7ade planes can be merged with one or more second fa\u00e7ade planes to produce a 3D model . As one example, a first image can comprise a first fa\u00e7ade plane, comprising a first visible face of an object (e.g.,  of ), and a second image can comprise a fourth fa\u00e7ade plane, comprising a second visible face of the object (e.g., a different view of the same object). In this example, the first fa\u00e7ade plane from the first image can be merged with the fourth fa\u00e7ade plane of the second image to yield a 3D model.","In one embodiment, a first fa\u00e7ade plane can be merged with a second fa\u00e7ade plane in the first image, where the first fa\u00e7ade plane is comprise at a first desired fa\u00e7ade depth and the second fa\u00e7ade plane is comprise at a second desired fa\u00e7ade depth. Further, the first image can subsequently be merged with a second image, where the second image comprises merged fa\u00e7ade planes, resulting in a 3D model . As an example, fa\u00e7ade planes from the first image can be merged to create a first merged image, and fa\u00e7ade planes from the second image can be merged to create a second merged image. In this example, the first merged image can be merged with the second merged image, thereby providing 3D image data of one or more objects in the merged images.","In one embodiment, a 3D point for a first image element location can be combined with a 3D point for a second image element location, resulting in the 3D model of the image. As an example, 3D points for respective image elements (e.g., pixels) from respective images in the image data may be combined to create the 3D model of the image. For example, pixels may be mapped to a global 3D model for the imagery using their respective 3D points. Further, in this example, two pixels that indicate a different view of a same portion of an object (e.g., from two different images) can be linked to the object in the 3D model, thereby allowing for different views of a same object to be rendered (e.g., from different viewpoints).","At  in the example embodiment , the image data can be rendered as a planar panorama image. As an example, a planar panorama image may comprise a lateral view of a location (e.g., a side of a street) made up of a series of relatively consecutive images stitched together to form a relatively seamless image of the location. In one embodiment, the planar panorama image can be rendered using one or more merged fa\u00e7ade planes, where the planar panorama image may comprise a first composite view from a first view point and a second composite view from a second view point.","As an illustrative example,  illustrate example embodiments ,  where one or more portions of one or more techniques described herein may be implemented. In the example embodiment , a first portion  of imagery comprised within image data may comprise a first object  (building), and a second portion  of the imagery may comprise a second object  (utility pole). In this example, a 3D model of the image data may comprise a composite view of the building , where one or more fa\u00e7ade planes comprising a visible face of the building  were merged. Further, the 3D model of the image data may comprise a composite view of the utility pole , where one or more fa\u00e7ade planes comprising a visible face of the utility pole  were merged.","In the example embodiment , a first composite view  from a first view point  comprises a first view of the building  and the utility pole . Further, a second composite view  from a second view point  comprises a second view of the building  and the utility pole . In this example, the first view point  and the second view point  may be disposed at a known distance  from one another. That is, for example, as a user pans along a view of the planar panorama, a different view of the same object(s) may be rendered, thereby providing a three-dimensional viewing experience.","In one embodiment, the image of the planar panorama can be rendered, where a first fa\u00e7ade plane may be rendered at a first panning speed and a second fa\u00e7ade plane may be rendered at a second panning speed. As an illustrative example, with reference to  and , different viewing angles of a same object in the imagery can be a result of parallax, an effect caused by viewing a same object from a different location, thereby providing a different line of sight to the object.","In the example embodiment , in a first composite view  of a street side scene the face of the building  and the face of the utility pole  do no overlap. In a second composite view  of the street side scene the face of the building  and the face of the utility pole  do overlap. In this example , it appears that the utility pole  has moved a greater distance than the building , between the first view point  and the second view point . The apparent disparity in object movement can be a result of parallax, where objects in the foreground appear to move faster than objects in the background when moving along a location.","In this embodiment, when a user pans along the rendered view of the planar panorama, for example, objects in a foreground fa\u00e7ade plane may be panned at a faster speed than objects in a background fa\u00e7ade plane, thereby accounting for or rather giving the impression of a natural parallax effect. As an illustrative example, in the example embodiment , when the planar panorama is rendered the first plane , comprising a view of the utility pole face , may be panned at a faster speed than the second plane , comprising a view of the building face . Further, in this example, the second plane  may be panned at a faster speed than the third plane, comprising a view of the mountain face .","A system may be devised that provides for generating a global three-dimensional (3D) model of image data, for example, where the 3D model may be used to create a 3D user experience of an image comprised by the image data. The 3D model may be used to render an object in resulting imagery from two different points of observation, for example. As an example, a 2D location of a pixel or image element in an image can be identified, and a depth relative to a point of observation can be determined for the pixel. The 2D pixel location and pixel depth may be used to group the pixel into a fa\u00e7ade plane, which can be merged with one or more other fa\u00e7ade planes to create a 3D model.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 5","b":["500","500","502","500","504","504","552","550"]},"As an example, an image element can comprise a portion (e.g., a smallest portion) of the image that may be represented and\/or controlled, such as a pixel (e.g., or sub-pixel). Further, for example, the image data  can comprise one or more images of a location, as well as a pixel location, comprising a 3D point of the pixel, for respective \u201cpixels\u201d in the one or more images. Additionally, as an example, a fa\u00e7ade plane can comprise a viewable portion of a face of one or more objects in an image, such as a side of a building (e.g., or group of buildings), a portion of landscape, and\/or a group of objects in a relatively similar plane. As one example, the segmentation component  can group a pixel into a fa\u00e7ade plane if the pixel's 3D point aligns with the fa\u00e7ade plane (e.g., within a desired range of coordinates in Euclidean space).","In the exemplary system , a merging component  is operably coupled with the segmentation component . The merging component  is configured to merge the first fa\u00e7ade plane with a second fa\u00e7ade plane resulting in the three-dimensional model of the image . The second fa\u00e7ade plane comprises at least a second image element corresponding to a second image element location. As an example, the image data  may comprise a plurality of images of the location (e.g., a series of relatively sequential images along a route at the location\/street), where respective images may comprise one or more fa\u00e7ade planes.","As one example, the merging component  may merge the first fa\u00e7ade plane, from a first image, with the second fa\u00e7ade plane, from a second image. As another example, the first fa\u00e7ade plane may comprise a first view of an object in the image data , and the second fa\u00e7ade plane may comprise a second view of the object. In this example, merging different views of a same object may provide for creation of a 3D model.",{"@attributes":{"id":"p-0053","num":"0052"},"figref":["FIG. 6","FIG. 5","FIG. 5"],"b":["600","600","610","650","610"]},"In one embodiment, the two-dimensional image location for the image element can comprise a coordinate corresponding to a location in the image. As an example, the image element may comprise a \u201cpixel,\u201d a term commonly used to identify a smallest unit of an image (e.g., rendered, displayed, drawn, printed, etc.) that can be represented and\/or controlled. In one example, the \u201cpixel\u201d may comprise a display screen unit, where an \u201caddress\u201d of the pixel comprises grid coordinates (e.g., comprising rows and columns) for the display window. As another example, the 2D image element location may comprise an \u201caddress,\u201d such as a plane coordinate location (e.g., X, Y axes). As one example, the plane comprising the \u201caddress\u201d may comprise a view plane of the image (e.g., a viewable face of the image). In this example, the 2D image element location can correspond to the location on the view plane of the image.","In one embodiment, the image element depth can comprise an estimated distance between the image element (e.g., pixel) and a desired point of observation for the image (e.g., point of image capture, and\/or point of view of the image). As one example, the pixel may be comprised in an object captured by the image. In this example, the image element depth can comprise an estimated distance between the object and the desired point of observation.","In one embodiment, the image element depth may be identified using detection and ranging data collected at image capture. For example, the detection and ranging data may be collected at same time the image is captured, where the detection and ranging data may be collected by a detection and ranging device (e.g., LIDAR, RADAR, LADAR, etc.). Further, in one embodiment, the image element depth may be identified utilizing stereoscopic estimation, which may use at least two images respectively comprising different views. As an example, a distance to a pixel, and\/or object, may be estimated using a type of \u201ctriangulation,\u201d when line of sight angles to the pixel\/object and a distance between the points of image capture for the two images are known.","Additionally, in one embodiment, the image element depth may be identified using dense surface mapping (DSM) of a location comprised by the image. As an example, DSM (e.g., which may also refer to dense surface modeling, digital surface mapping, and\/or digital surface modeling, which can be used synonymously) may provide a 3D model of a surface, such as an area of the Earth comprised by the location captured by the image data . Further, as an example, the DSM can provide estimated distance measurements between points in the surface model. In one example, the estimated distance measurements may be used, such as by the image element location determination component , to identify the image element depth.","In one embodiment, the three-dimensional model of the image data  can comprise first merged fa\u00e7ade planes corresponding to a first depth from two or more images, and can comprise second merged fa\u00e7ade planes corresponding to a second depth from the two or more images. That is, for example, fa\u00e7ade planes  from different images that have a same depth can be merged when creating the 3D model of the image data . Further, in one embodiment, the three-dimensional model of the image data  can comprise a plurality of merged fa\u00e7ade planes from one or more images. As one example, the segmented fa\u00e7ade planes  of a first image may be merged, and the segmented fa\u00e7ade planes  of a second image may be merged. In this example, the merged fa\u00e7ade planes from the first image can be combined with the merged fa\u00e7ade planes of the second image.","Additionally, in one embodiment, the three-dimensional model of the image data  can comprise a plurality of merged three-dimensional points respectively corresponding to an image element location in the image. For example, as described above, respective \u201cpixels\u201d in the one or more images are associated with a 3D point in the image data . As one example, the \u201cpixel\u201d can be associated with an object. In this example, a first pixel may be comprised in a first view of the object, and a second pixel may be comprised in a second view of the object. Further, in this example, the image element locations for the first and second pixels may be merged to create at least a portion of the 3D model of the object.","In the example embodiment , a rendering component  can be configured to render a planar panorama image . The planar panorama image  can comprise a three-dimensional representation of a combination of one or more images. As an example, the planar panorama image  may comprise a lateral view of a location (e.g., a human-scale, side view of a street) made up of a series of relatively consecutive images merged together to form a relatively seamless image of the location (e.g., street).","In one embodiment, the planar panorama image  can comprise an independently rendered view-point for respective one or more image element locations. In this embodiment, for example, the planar panorama image  can be rendered such that it comprises a first view of an object from a first view point and a second view of the object from a second view point. That is, for example, as a user pans along a view of the planar panorama, a different view of the same object may be rendered from a different view angle, thereby providing a three-dimensional viewing experience. Moreover, fa\u00e7ade planes may move a different speeds such that a first fa\u00e7ade plane moves at a first speed and a second fa\u00e7ade plane moves at a second speed during panning (e.g., foreground object moves faster than a background object, etc.).","Still another embodiment involves a computer-readable medium comprising processor-executable instructions configured to implement one or more of the techniques presented herein. An exemplary computer-readable medium that may be devised in these ways is illustrated in , wherein the implementation  comprises a computer-readable medium  (e.g., a CD-R, DVD-R, or a platter of a hard disk drive), on which is encoded computer-readable data . This computer-readable data  in turn comprises a set of computer instructions  configured to operate according to one or more of the principles set forth herein. In one such embodiment , the processor-executable instructions  may be configured to perform a method, such as at least some of the exemplary method  of , for example. In another such embodiment, the processor-executable instructions  may be configured to implement a system, such as at least some of the exemplary system  of , for example. Many such computer-readable media may be devised by those of ordinary skill in the art that are configured to operate in accordance with the techniques presented herein.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.","As used in this application, the terms \u201ccomponent,\u201d \u201cmodule,\u201d \u201csystem,\u201d \u201cinterface,\u201d and the like are generally intended to refer to a computer-related entity, either hardware, a combination of hardware and software, software, or software in execution. For example, a component may be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, a program and\/or a computer. By way of illustration, both an application running on a controller and the controller can be a component. One or more components may reside within a process and\/or thread of execution and a component may be localized on one computer and\/or distributed between two or more computers.","Furthermore, the claimed subject matter may be implemented as a method, apparatus or article of manufacture using standard programming and\/or engineering techniques to produce software, firmware, hardware or any combination thereof to control a computer to implement the disclosed subject matter. The term \u201carticle of manufacture\u201d as used herein is intended to encompass a computer program accessible from any computer-readable device, carrier or media. Of course, those skilled in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.",{"@attributes":{"id":"p-0066","num":"0065"},"figref":["FIG. 8","FIG. 8"]},"Although not required, embodiments are described in the general context of \u201ccomputer readable instructions\u201d being executed by one or more computing devices. Computer readable instructions may be distributed via computer readable media (discussed below). Computer readable instructions may be implemented as program modules, such as functions, objects, Application Programming Interfaces (APIs), data structures, and the like, that perform particular tasks or implement particular abstract data types. Typically, the functionality of the computer readable instructions may be combined or distributed as desired in various environments.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":["FIG. 8","FIG. 8"],"b":["800","812","812","816","818","818","814"]},"In other embodiments, device  may include additional features and\/or functionality. For example, device  may also include additional storage (e.g., removable and\/or non-removable) including, but not limited to, magnetic storage, optical storage, and the like. Such additional storage is illustrated in  by storage . In one embodiment, computer readable instructions to implement one or more embodiments provided herein may be in storage . Storage  may also store other computer readable instructions to implement an operating system, an application program and the like. Computer readable instructions may be loaded in memory  for execution by processing unit , for example.","The term \u201ccomputer readable media\u201d as used herein includes computer storage media. Computer storage media includes volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions or other data. Memory  and storage  are examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, Digital Versatile Disks (DVDs) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by device . Any such computer storage media may be part of device .","Device  may also include communication connection(s)  that allows device  to communicate with other devices. Communication connection(s)  may include, but is not limited to, a modem, a Network Interface Card (NIC), an integrated network interface, a radio frequency transmitter\/receiver, an infrared port, a USB connection or other interfaces for connecting computing device  to other computing devices. Communication connection(s)  may include a wired connection or a wireless connection. Communication connection(s)  may transmit and\/or receive communication media.","The term \u201ccomputer readable media\u201d may include communication media. Communication media typically embodies computer readable instructions or other data in a \u201cmodulated data signal\u201d such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d may include a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.","Device  may include input device(s)  such as keyboard, mouse, pen, voice input device, touch input device, infrared cameras, video input devices, and\/or any other input device. Output device(s)  such as one or more displays, speakers, printers, and\/or any other output device may also be included in device . Input device(s)  and output device(s)  may be connected to device  via a wired connection, wireless connection, or any combination thereof. In one embodiment, an input device or an output device from another computing device may be used as input device(s)  or output device(s)  for computing device .","Components of computing device  may be connected by various interconnects, such as a bus. Such interconnects may include a Peripheral Component Interconnect (PCI), such as PCI Express, a Universal Serial Bus (USB), firewire (IEEE 1394), an optical bus structure, and the like. In another embodiment, components of computing device  may be interconnected by a network. For example, memory  may be comprised of multiple physical memory units located in different physical locations interconnected by a network.","Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network. For example, a computing device  accessible via network  may store computer readable instructions to implement one or more embodiments provided herein. Computing device  may access computing device  and download a part or all of the computer readable instructions for execution. Alternatively, computing device  may download pieces of the computer readable instructions, as needed, or some instructions may be executed at computing device  and some at computing device .","Various operations of embodiments are provided herein. In one embodiment, one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media, which if executed by a computing device, will cause the computing device to perform the operations described. The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. Alternative ordering will be appreciated by one skilled in the art having the benefit of this description. Further, it will be understood that not all operations are necessarily present in each embodiment provided herein.","Moreover, the word \u201cexemplary\u201d is used herein to mean serving as an example, instance or illustration. Any aspect or design described herein as \u201cexemplary\u201d is not necessarily to be construed as advantageous over other aspects or designs. Rather, use of the word exemplary is intended to present concepts in a concrete fashion. As used in this application, the term \u201cor\u201d is intended to mean an inclusive \u201cor\u201d rather than an exclusive \u201cor.\u201d That is, unless specified otherwise, or clear from context, \u201cX employs A or B\u201d is intended to mean any of the natural inclusive permutations. That is, if X employs A; X employs B; or X employs both A and B, then \u201cX employs A or B\u201d is satisfied under any of the foregoing instances. Further, At least one of A and B and\/or the like generally means A or B or both A and B. In addition, the articles \u201ca\u201d and \u201can\u201d as used in this application and the appended claims may generally be construed to mean \u201cone or more\u201d unless specified otherwise or clear from context to be directed to a singular form.","Also, although the disclosure has been shown and described with respect to one or more implementations, equivalent alterations and modifications will occur to others skilled in the art based upon a reading and understanding of this specification and the annexed drawings. The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims. In particular regard to the various functions performed by the above described components (e.g., elements, resources, etc.), the terms used to describe such components are intended to correspond, unless otherwise indicated, to any component which performs the specified function of the described component (e.g., that is functionally equivalent), even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations of the disclosure. In addition, while a particular feature of the disclosure may have been disclosed with respect to only one of several implementations, such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Furthermore, to the extent that the terms \u201cincludes\u201d, \u201chaving\u201d, \u201chas\u201d, \u201cwith\u201d, or variants thereof are used in either the detailed description or the claims, such terms are intended to be inclusive in a manner similar to the term \u201ccomprising.\u201d"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIGS. 4A and 4B"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
