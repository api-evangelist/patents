---
title: System and method to calculate time weight in RDF graphs
abstract: Method to calculate time weight in an RDF graph, including: providing one or more triples of the RDF graph to an inference engine module, the one or more triples comprising a time information; providing a reference time to the inference engine; calculating an elapsed time from the reference time to the time value; and inversely weighting the time information by the elapsed time to provide a calculated time weight. Another embodiment may provide a method to update a time weight of a relation when an RDF graph has been added to or deleted from the relation. Another embodiment may provide a method to update a time weight of a relation represented by an RDF graph, when a new reference timestamp is provided. Another embodiment may provide a system to calculate time weight in an RDF graph.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08606743&OS=08606743&RS=08606743
owner: Avaya Inc.
number: 08606743
owner_city: Basking Ridge
owner_country: US
publication_date: 20110922
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application claims the benefit of U.S. Provisional Patent Application Ser. No. 61\/471,442, filed on Apr. 4, 2011, the content of which is hereby incorporated by reference in its entirety.","1. Field of the Invention","Embodiments of the present invention generally relate to fast methods of updating time weights in a social relation representable by an RDF graph. Additional embodiments relate to updating the saved weight of a relation, and updating a saved time weight of a relation whenever a new reference timestamp is provided.","2. Description of the Related Art","Enterprise communication and collaboration typically involves email, meeting schedule, voice, video, chat, wild, blogs, and various forms of documents, such as design documents and bug reports related to products and service offerings. As different organizations adopt technologies at different paces, these rich sets of digital content often exist in different formats managed by different systems that are often not connected to each other. For example, emails between group members may be stored in special format at local disks or email servers. The product design documents are managed by some special proprietary relational database, whereas the project progress is tracked on a separate group wild site. As a consequence, a lot of valuable information is buried in disparate computers that are not readily accessible.","As the number of media types and the amount of digital contents increase, it can cost significant overhead and a reduction in productivity, as users may have to spend extra effort searching for the relevant information. For instance, when scheduling a project planning meeting between several groups, people typically receive invitations in email that have a subject, time and location, a short description, and some attachments and links. If people need to find out more background information about the participants, the previous history of contacts on this subject, or new products related to this project, people have to do searches using several special applications. As every participant repeats almost identical searches to become informed, the productivity of the enterprise is reduced as the number of participants increases.","There has been active research in how to process large scale Resource Description Framework (\u201cRDF\u201d) databases. The focus of these efforts has been limited to efficient storage and retrieval of large datasets whereas embodiments in accordance with the present invention support general computing and inference over the datasets. Furthermore, the approaches have been based on a homogeneous architecture where a set of computers either use a single protocol or form a fixed topology.","REST composition service based approach does not assume any single protocol or topology.","There has been some work on using RDF database to annotate text. But these systems are special cases of proactive search that we propose. In addition, they do not propose a general architecture to support distributed functions.","REST service architecture in accordance with embodiments of the present invention are also different from the conventional 3-tier web architecture consisting of data, business logic and presentation. In the present architecture, the presentation is not consumed by end users but by agents. Unlike business logic that accesses local data, our logic can access distributed functions through service composition.","In an enterprise, people communicate and collaborate on daily basis. These activities form a social network that is dynamic and implicit with rich relations associated with social contents, e.g. email, IM, co-authorship, etc. This implicit social network can be discovered by inspecting the artifacts of these activities, such as email exchanges and authorships of project documents, and so forth. When calculating these relations, a relation with more recent and frequent activities are valued more than one with infrequent activities in the past, as people in the organizations can take different roles over time.","Thus, there is a need for a knowledge agent based approach for enterprise, derived from search technologies. In particular, there is a need for the knowledge agent to efficiently evaluate a temporal aspect of relations in the social network.","Embodiments in accordance with the present invention provide a method, a system, and a software architecture based on agent and web service technologies to support proactive search to enrich enterprise communication and collaboration. In particular, software agents and REST web services may be combined to deliver relevant information found from RDF databases to the users without interrupting their workflows. The relevant information includes text annotations, implicit social networks, and recommended experts. Service composition may be used to efficiently combine results from distributed functions to support independent and scalable semantic web development. Experimental results indicate the proposed approach is feasible and efficient.","Embodiments in accordance with the present invention may provide a method to calculate time weight in an RDF graph, comprising: providing one or more triples of the RDF graph to an inference engine module, the one or more triples comprising a time information; providing an epoch time to the inference engine; calculating an elapsed time from the epoch time to the time value; and inversely weighting the time information by the elapsed time to provide a calculated time weight.","Embodiments in accordance with the present invention provide a method to update a time weight of a relation when an RDF graph has been added to or deleted from the relation, comprising: providing to an inference engine module the time weight of the relation; providing identification to an inference engine module of the RDF graph comprising timestamp Tn+1; providing an epoch time value to the inference engine; calculating an elapsed time from the epoch time to timestamp Tn+1; and updating the saved time weight by an inversely weighted function of the elapsed time.","Embodiments in accordance with the present invention provide a method to update a time weight of a relation represented by an RDF graph, when a new reference timestamp is provided, comprising: providing to an inference engine module the time weight of the relation at a first reference timestamp; providing a second reference timestamp to the inference engine; calculating an elapsed time from the first reference timestamp to the second reference timestamp; and updating the time weight of the relation by an inversely weighted function of the elapsed time.","Embodiments in accordance with the present invention provide a system to calculate time weight in an RDF graph, comprising: an inference engine module; a first communication interface to the inference engine module, the first communication interface configured to provide one or more triples of the RDF graph to the inference engine module, the one or more triples comprising a time information; a first communication interface to the inference engine module, the first communication interface configured to provide an epoch time to the inference engine, wherein the inference engine module is configured: to calculate an elapsed time from the epoch time to the time value; and to inversely weight the time information by the elapsed time, to provide a calculated time weight.","The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application, the word \u201cmay\u201d is used in a permissive sense (i.e., meaning having the potential to), rather than the mandatory sense (i.e., meaning must). Similarly, the words \u201cinclude\u201d, \u201cincluding\u201d, and \u201cincludes\u201d mean including but not limited to. To facilitate understanding, like reference numerals have been used, where possible, to designate like elements common to the figures. Optional portions of the figures may be illustrated using dashed or dotted lines.","Embodiments of the present invention generally relate to fast methods of updating time weights in a social relation representable by an RDF graph. Additional embodiments relate to updating the saved weight of a relation, and updating a saved time weight of a relation whenever a new reference timestamp is provided.","As used herein in connection with embodiments of the present invention, the term \u201cREST\u201d refers to REpresentational State Transfer web services, as described below in further detail.","As used herein in connection with embodiments of the present invention, the term \u201cRESTful\u201d refers to a web service that is compatible with REST.","As used herein, the term \u201cmodule\u201d refers generally to a logical sequence or association of steps, processes or components. For example, a software module may comprise a set of associated routines or subroutines within a computer program. Alternatively, a module may comprise a substantially self-contained hardware device. A module may also comprise a logical set of processes irrespective of any software or hardware implementation.","An architectural style that underlies the Web is REpresentational State Transfer (\u201cREST\u201d). A web service that is compatible with REST is said to be \u201cRESTful.\u201d REST provides resource management and promotes architectural choices that include: 1) Addressability\u2014each resource can be addressed by Uniform Resource Identifier (\u201cURI\u201d); 2) Connectedness\u2014resources are linked to provide navigations; 3) Uniform Interface\u2014all resources support a subset of a uniform interface between components, namely GET, PUT, DELETE and POST; 4) Statelessness\u2014all requests to a resource contain all of information necessary to process the requests, and the servers do not need to keep any context about the requests; and 5) Layering: intermediate proxies between clients and servers can be used to cache data for efficiency. Because of its relative simplicity, compared to SOAP-based web services, REST web service paradigm is gaining popularity for use in implementing mobile device applications. Researchers have also compared the performance of the same SOAP and REST services implemented on mobile devices. REST was found to be more suitable to mobile devices since REST required less processing time and memory.","As used herein in connection with embodiments of the present invention, the term \u201cresource\u201d refers to a special component that is addressable by URI and supports a uniform interface as defined by REST. Any information that can be named can be a resource: a document or image, a temporal service (e.g. \u201ctoday's weather in Los Angeles\u201d), a collection of other resources, a non-virtual object (e.g. a person), and so on. In other words, any concept that might be the target of an author's hypertext reference fits within the definition of a resource. A resource is a conceptual mapping to a set of entities, not the entity that corresponds to the mapping at any particular point in time.","The semantic web in our approach refers to a set of technologies based on the web architecture and knowledge representation languages including OWL Web Ontology Language and Resource Description Framework (\u201cRDF\u201d). RDF is a graph-structured data model which uses URIs to name nodes and arc types, arriving at subject-predicate-object triples.","N-Triples is a line-based, plain text format for encoding an RDF graph. N-Triples is a simple line-delimited syntax. N-Triples may be used as an RDF syntax for expressing RDF test cases and defining the correspondence between RDF\/XML and the RDF abstract syntax. N-Triples can be extended to have context by use of a syntax called N-Quads. Each triple in an N-Quads document can have an optional context value:","<subject> <predicate> <object> <context>.","As opposed to N-Triples, where each triple has the form:","<subject> <predicate> <object>.","The semantic web technologies offer a set of solutions to address the heterogeneous data problem in enterprises, whereas URI provides a uniform identification mechanism for data in enterprises, and RDF provides a uniform representation language about the relations between those identified data. In addition, HTTP provides a uniform protocol to access the distributed data, and SPARQL provides a declarative way to query those data. Moreover, ontologies offer a way to integrate RDF graphs from different sources. Because semantic web technologies are based on Description Logic (RDF 2004), they also offer a framework to reason and inference about the data.","Despite these advantages, a challenge to adopt semantic web for enterprise is how to transform the raw enterprise data into RDF. It is ideal but unrealistic to force all enterprise systems and applications to expose their data according to a predefined ontology. Instead, organizations should be allowed to evolve their semantic web incrementally and independently. To support this evolutionary path, we adopt the REST web service paradigm as our semantic web infrastructure, because REST is suited for such distributed hypertext systems. Unlike conventional approaches to semantic web that aim to support linking and querying of raw RDF triples (Linked Data), we focus on developing knowledge based web services that can enhance enterprise communications. In particular, we investigate how to develop a scalable and robust REST architecture that can share and compose distributed knowledge web services across organizations.","Proactive search pushes relevant information to users without a user specifically asking for the relevant information. Proactive search is a departure from current interactive search paradigm in several aspects. In interactive searches, a user composes a specific query, enter it into a search box, select results and integrate them into his application manually. Albeit being quite flexible, interactive search has some disadvantages and limitations. Firstly, the interactive search mode usually forces a user to leave his current activity and work on a separate search activity. Secondly, the query does not carry the context from which the search is launched. Thirdly, the results of interactive search depend on the quality and accuracy of user's query. Fourthly, to integrate the search result back into the user's workflow and context, it typically requires user's manual operation.","In proactive search, a user's communication activity is treated as the query to the search engine, thereby providing the necessary context for more accurate results. Instead of asking a user to select the results, proactive search integrates relevant information directly into the communication activity in a nonintrusive way. As a result, the user can focus on his business activities without taking detours to seek for relevant information. For example, an incoming email or an outgoing email can be treated as query to the proactive search engine. The relevant information found about the topic, people or products mentioned in the emails is integrated into the emails as hyperlinks. The disadvantage of proactive search is that the input is limited to current user's activity and context. The second challenge in adopting semantic web technologies is how to determine what is relevant given a context as this is significantly more complex than most queries. In open domain search, these may be very difficult problems. However, as enterprises have more organized and predictable activities and workflows than individual users, we can use those patterns in enterprise data to help tackle these tough problems.","Proactive search can be supported by client-server architecture as interactive search. However, the clients in proactive search assume more responsibility than in interactive search. In proactive search, the clients may be software agents that monitor user's activities and invoke the corresponding knowledge base web services to obtain the right information at the right time.","Overall Architecture","This section presents the overall architecture of a semantic web based system. To support semantic web based proactive search, we provide customized semantic web based functions that may be targeted to different business environments. For example, in a call center, we need functions that classify emails, annotate important concepts in emails, and suggest relevant responses. In a group collaboration application, we need functions that bring up contact history on a subject and show common interest between participants. However, due to the limitation of SPARQL, many of these functions cannot be implemented as SPARQL queries to RDF databases. For this reason, we decide to expose these functions as REST services that may be sharable and reusable across organizations. REST services encourage distributed and independent development of services, which is one of our design goals. Besides connecting different applications, our REST composition approach allows us to distribute a semantic function that is too large for one machine to multiple machines in parallel, and use service composition to aggregate the distributed logic.","On the client side, our software agents may be embedded in user's communication and collaboration applications. These agents monitor user's activities, retrieve relevant information from the REST services, and inject the relevant information into the collaboration environment.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 1","b":["100","100","101","103","104","106","105","104","106","100"]},"By driving the states of agents - with hypertext from the REST services, REST architecture  offers the following degrees of freedom for adopting independent changes. At the server - side, a new service can be deployed without having to reconfigure all the agents -. An existing service can be upgraded without breaking those agents that use the service. On the client side, agents - can acquire different \u201cskills\u201d required for different environments by following the hyperlinks to different REST services. Although in this architecture the agents - do not need to directly communicate with each other, they can still collaborate indirectly by sharing their states through the servers -. Agents - and servers - can also use content negotiation to find the best representation for a given situation.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":["FIG. 2","FIG. 2"],"b":["104","106","201","201","202","202","203","203","204","204","205","205","206"]},"The bottom layer  is the raw RDF triples collected from various sources of enterprise data, including relational databases and web pages. On top of this, it is a layer of the knowledge  derived from the RDF triples\u2014some of those triples may be collected off-line, and some of them may be derived dynamically based on queries. A function layer  takes an input query and produces some outputs based on the derived knowledge layer . The basic REST services layer  exposes these functions as resources, and the composed REST services layer  invokes the basic services  or composed services  at local and remote servers to carry out a task.","Service composition is a process that implements a service by combining outputs from other services. This process can be used to break a large semantic database into small ones and distribute a related function into a set of servers that form a tree structure. The servers in the leaf nodes offer the basic services, while the servers in the interior nodes offer partially composed services. As the result, the server at the root node offers the completely composed services. Because the services may be stateless, a composed service invokes its children services in parallel and merges the results for its parent.",{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 3","b":["300","1","2","3","5","4","6"]},"Notice that this composition architecture is different from the conventional computer cluster architecture which has a fixed entry point and a specified topology. Instead, in our case, each distributed function may have a different entry point and topology of its own. For example,  illustrates two distributed functions with entry points Server  and Server  respectively. Server  is the entry point to the server tree consisting of Servers , , , ,  and Server  is the entry point to the server tree consisting of Servers , , , .","RDF Transformation","This section discusses the knowledge transformation process. As enterprise data often exist in different forms and formats, they have to be transformed into semantic web first. This transformation contains two steps. First the data may be transformed into web resources that have unique URI. Second the metadata may be extracted and transformed into RDF triples, often with the help of public and private ontologies. For structured data, such as relational databases, this transformation is straightforward. In our study, we have transformed a relational database about documents with 160,699 records into 3,182,721 triples following the Dublin Core ontology. The subject is the document URI in all triples. The following are some sample triples with sensitive information replaced by generic strings:",{"@attributes":{"id":"p-0058","num":"0057"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"<uri_1>"]},{"entry":[{},"<http:\/\/purl.org\/dc\/elements\/1.1\/title> \u201cTitle 1\u201d ."]},{"entry":[{},"<uri_1>"]},{"entry":[{},"<http:\/\/purl.org\/dc\/elements\/1.1\/creator> \u201cAuthor 1\u201d ."]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Our experience shows some problems for well-defined data. First, many important relations (i.e., predicates) are not in Dublin Core or other known ontologies. We created and added private ontologies to cover them, but they cannot interoperate easily outside this domain. Second, many data fields, such as author names, may not be properly entered in the database. Many names have variations that make matching and cross-reference from other RDF databases difficult.","Functions and Services","This and the following sections present some functions and services built into an approach and architecture in accordance with embodiments of the present invention. Within our proposed REST architecture, we develop several exemplary functions and services for semantic web based proactive searches, including Entity Annotation, Implicit Social Network, and Exert Finder.","Entity Annotation","Entity annotation function takes an incoming text and produces a set of annotations for the entities in the text based on current knowledge. An annotation is a 4-tuple (phrase, start, length, link) that identifies the phrase being annotated, the starting position and the length (both in characters), and the concept related to the phrase. When clicked, the link will open a web page showing the detail information.","To support this function, we first index the RDF triples on selected predicates. This dramatically reduces the indexing and search space from entire literals to the selected literals. Another technique to save memory and improve efficiency is to avoid creating separate index. Instead, we pre-process RDF triples by tokenizing the literals into phrases so that they would match the tokenized input. The outcome of this process is a many-to-many mapping from indexed phrases to concepts.",{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 4","b":["400","400","401","402","403","404","402","401","405","401","402","404"]},"Process  may be a modification of a left-to-right maximum tokenization process developed for Chinese language processing. The process aims to find the longest token sequence from left to right that matches an indexed phrase in the RDF triples and record the corresponding concept. If more than one concept is found, the server creates a link representing a list of matches. Unlike the traditional tokenization process that covers the entire text, a process in accordance with the present invention skips unmatched tokens.","Process  is exposed as a REST service on a designated resource. There are two ways to invoke the service: HTTP GET and HTTP POST. GET is used for short text, and POST is used for long text. The REST service returns related annotations in the following formats: 1) Java Script Object Notation (\u201cJSON\u201d) for agents in web browsers; 2) HTML for direct rendering in web browsers; 3) HTML tables for embedding and debugging in web browsers; and 4) Prolog terms for efficient service composition.","In a tree of servers, some servers may be configured as parents, some as children, and some servers as both parents and children. For instance, referring to the tree structure of  denoted by solid lines, Server  is a parent, Servers  and  are both parents and children, and Servers  and  are only children. When this function is distributed on a tree of servers like that of , the parent server sends a copy of the text to all its children in parallel, which will return the annotation results or faults. Once the parent server receives all the results within a timeout interval, it merges what it received so far into a coherent annotation and sends it back to its parent or the client if it is the root server. To maintain the longest phrase condition, the merge process removes all covered phrases. In other words, if a server returns an annotation for phrase x and another server returns a phrase that contains x as a substring, then the first annotation is removed from the merged annotation. The merged annotation therefore will contain only longest phrases.","Implicit Social Network","In a business or social enterprise, people communicate and collaborate on a daily basis. These activities form a social network that is dynamic and implicit with rich relations associated with social contents, e.g. email, instant messaging, co-authorship, etc. This implicit social network can be discovered by inspecting the artifacts of these activities, such as email exchanges and authorships of project documents, etc. Because there may be very rich relations between people in an organization, our semantic web approach is well suited for representing and discovering such implicit social networks. Table 1 below sets forth relationships that may be useful to a person in implicit social networks.",{"@attributes":{"id":"p-0071","num":"0070"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Relations in implicit social network:"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Relation","Comment"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Collaborators","Collaborators of this person"]},{"entry":[{},"Followers","People interested in this person"]},{"entry":[{},"Citations","Artifacts that cite this person"]},{"entry":[{},"Products","Artifacts made by this person"]},{"entry":[{},"Expertise","Expertise of this person"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},"To cope with the dynamic nature of the implicit social network and to save memory, these relations may be derived by rules in response to incoming queries. The input of this function is a URI identifying a person, and the output is the relations about the persons found in the knowledge base. Because this information is to be consumed by human users in our system, the current output supports only HTML in which relations may be represented as hypertext.",{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 5","b":["500","500","501","502","503","504","501","505"]},"When calculating these relations, a relation having more recent and frequent activities may be valued more than a relation having less recent and less frequent activities in the past, because people in the organizations can take different roles over time. The activities in a relation therefore may be weighted by an exponential decay function to reflect this time-based relevance value. Assume that a relation R is derived from a set of n activities each with timestamp T, then the time value of this relation, with respect to the current timestamp T, is calculated as listed in equation (1), where C is a normalizing factor and \u03bb is a scaling factor:",{"@attributes":{"id":"p-0075","num":"0074"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mi":"tv","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["T","\u03bb","C","R"],"mo":[",",",",","]}}},{"mi":"C","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msup":{"mi":"\u2147","mrow":{"mo":"-","mrow":{"mi":"\u03bb","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"T","mo":"-","msub":{"mi":["T","i"]}}}}}}}}],"mo":"="},{"mi":"R","mo":"=","mrow":{"mo":["{","}"],"mrow":{"msub":{"mi":["T","i"]},"mo":"|","mrow":{"mn":"1","mo":["\u2264","\u2264"],"mi":["i","n"]}}}}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}}},"Similar to the annotation module , when this function is distributed, the parent server sends a copy of the URI to its children servers in parallel, which return the relations (or faults). The parent server then merges the results using set unions.","Expert Finder","In many enterprise systems, there is a need to find experts with certain skills, so that a problem can be directed to the most qualified persons. In an organizational environment, since we value team work and influence as much as individual skills, a need exists for an Expert Finder process to find experts who not only have required expertise, but also have high reputation and authority.","The expertise of a person can be evaluated based on the products he produces or contributes. For example, if a person designed several web servers, it is reasonable to assume he is an expert in that area. In embodiments in accordance with the present invention, the expertise of a person is represented as a vector where each dimension corresponds to a skill and the value indicates the strength of the skill. To speed up the matching process, a training process is used to compute and save the expertise vectors for each person in the knowledge base. For a person p with an expertise vector e, the relevance of p with respect to a given problem description vector x can be calculated as listed in Equation (2) using the vector space based semantic model as the angle between x and e.\n\nrelevance()=cos()\u2003\u2003(2)\n","The reputation of a person can be evaluated based on how other people evaluate his work. This is calculated as listed in Equation (3) with bounded recursion and loop detection based on the implicit social network as follows, where rating(p) reflects the total evaluation a person has received.\n\nreputation()=rating()+Sum of reputation(followers())\u2003\u2003(3)\n","The authority of a person captures how much power a person has in an organization. It can also be calculated as listed in Equation (4) with bounded recursion and loop detection based on implicit social network as follows, where level(p) corresponds to the power level a person has in an organizational hierarchy.\n\nauthority()=level()+Sum of authority(collaborators())\u2003\u2003(4)\n","The input to the expert finder function is a text describing a problem, and the output is a ranked list of experts. The input text is first converted to a vector to search for persons whose expertise is above a threshold. The candidates in the list then may be re-ranked by averaging the normalized relevance, reputation and authority scores.",{"@attributes":{"id":"p-0083","num":"0082"},"figref":"FIG. 6","b":["600","600","601","602","603","604","601","605"]},"When the Expert Finder process is distributed, a parent server sums up the scores returned from its children servers for the same person. The returned experts then may be merged and re-ranked to pass back up to the parent or agent.","Software Agents and Applications","This and the following sections discuss the agents and applications based on the described functions and services. The proposed architecture, functions, and services were used and applied to several enterprise communication and collaboration systems. Substantially all of the applications use the same REST services, but they differ in how the agents behave. Applications included a Browser Agent, a Call Center Agent, and a Google Wave Agent.","Browser Agent","The browser agent in our study is a Firefox extension that monitors and annotates the web page a user is viewing. A user can activate and deactivate the agent from the browser menus and ask the agent to annotate the current page or restore the original page. The user can also configure the agent to use a different REST service by entering a different URI.  is a screenshot of the interface to our browser agent in front of the Firefox browser window.","Call Center Agent","The software agent in our study assists human agents in call centers by finding relevant information in incoming contacts (e.g. emails) to save human agents from searching for them. Each incoming email was intercepted by the software agent to find and identify the important concepts and recommend experts related to the email using our REST services. The email was then enriched to embed found information as hyperlinks into the original email, and forwarded to the system. When a human agent receives this enriched email, he can click the links to obtain the detailed information.",{"@attributes":{"id":"p-0091","num":"0090"},"figref":"FIG. 8"},"Google Wave Agent","We developed a special software agent, referred here as an iknow wave robot, for Google Wave using Google Robot API to monitor and annotate multi-party group chat in real-time. To enable this agent function, a user just needs to invite the iknow wave robot to the current Wave conversation. When a user clicks a button to finish his chat, the agent will invoke the REST services and annotate the chat text with hyperlinks that point to the relevant concepts and information to bridge the semantic gap in collaboration. These hyperlink annotations may be propagated to all participants of the Wave session in near real-time. Any user can click a link in the chat window to visit it.","Implementation and Experiments","This and following sections describe implementations and experimental results in accordance with embodiments of the present invention. We implemented a prototype REST server system using SWI-Prolog (SWI-Prolog). The HTTP servers were Prolog images (executable program) compiled for different machines. Our current RDF database contains triples from three sources as listed in Table 2 below.",{"@attributes":{"id":"p-0096","num":"0095"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Size of knowledge sources:"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Source","Triples"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Wikipedia (2006\/03\/26)","47,054,407"]},{"entry":[{},"database","3,182,721"]},{"entry":[{},"product","75"]},{"entry":[{},"total","50,237,203"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},"To simulate distributed REST services that contain different knowledge bases, the Wikipedia RDF file was broken up at random into 10 small sets each with up to 5 million triples. This resulted in 12 text files in N-Triples format. These 12 files may be loaded into SWI-Prolog, indexed and converted into binary SWI-Prolog RDF database format for efficient loading. The following table (Table 3) compares the size (KB) of text files with the size (KB) of binary databases. This table shows that for most large files, the binary format has an over 80% size reduction.",{"@attributes":{"id":"p-0098","num":"0097"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"70pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"70pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]},{"entry":[{},"Name","text","binary","ratio"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"70pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"70pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"WP_0","753243","148159","19.67%"]},{"entry":[{},"WP_1","753268","149143","19.80%"]},{"entry":[{},"WP_2","753986","147896","19.62%"]},{"entry":[{},"WP_3","753976","147493","19.56%"]},{"entry":[{},"WP_4","754396","149513","19.82%"]},{"entry":[{},"WP_5","753430","148539","19.72%"]},{"entry":[{},"WP_6","753364","148594","19.72%"]},{"entry":[{},"WP_7","753890","149200","19.79%"]},{"entry":[{},"WP_8","753979","149268","19.80%"]},{"entry":[{},"WP_9","309799","67885","21.91%"]},{"entry":[{},"database","380004","73624","19.37%"]},{"entry":[{},"product","10","7","70.00%"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},"These binary databases were loaded into the memory of different machines according to their capacity. This distributed configuration allows us to recruit different number of non-dedicated machines, ranging from powerful servers to even notebook computers. The smallest system that provides satisfactory performance for the entire 50+ million triples consisted of two Linux machines (3.0 GHz CPU\/4 GB RAM and 1.6 GHz CPU\/4 GB RAM), each with about 25+ million triples.","To test the performance of the distributed servers, we selected 3 Wikipedia binary databases WP{0,1,2}, and distributed them into three server trees in a LAN environment. The first server tree had one root node containing all 15 million triples; the second server tree had one root with 5 million and one child with 10 million triples; the third tree had one root and two children, each with 5 million triples. In all these trees, the root server was a Windows 2003 Server machine with Dual Core (3.0 GHz and 2.99 GHz) and 2 GM RAM and the child servers consisted of two Linux machines mentioned above. To test the performance of these trees, a test text of 1142 characters was sent 10 times to the root server which returns 30 annotations. The average service execution time was recorded using Prolog time\/1 predicate on the root server and summarized in the following table (Table 4) with standard deviations. The execution time includes time for local function, service composition as well as logging.",{"@attributes":{"id":"p-0101","num":"0100"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 4"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Performance of three server trees:"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"147pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Server tree","Avg. Time (second)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"1 node","0.406"]},{"entry":[{},"2 nodes","0.390"]},{"entry":[{},"3 nodes","0.401"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},"Our results showed that a distributed function may outperform its local version when it is distributed over faster machines. When the two Linux machines with more RAM were used, the average service execution time on the root was improved slightly (2 and 3 vs. 1). Also the test showed that parallel distribution of a function to two nodes created only a small overhead compared to distribution to one node (3 vs. 2).","A Fast Process to Calculate Time Weight in RDF Graphs, Usable in Semantic Web-Based Proactive Search for an Enterprise","Resource Description Framework (\u201cRDF\u201d) is a cornerstone of Semantic Web. The basic unit of RDF is a triplet of (subject, predicate, object) that asserts a relation between the subject resource and the object resource. A RDF graph is a set of such triplets where subject and object may be represented as nodes and predicates are edges. When a RDF graph represents an event in the real world, it is often associated with a time point or interval of that event. For example, a RDF graph may represent that John sent an email to Mary on Mar. 30, 2011. Another RDF graph may represent that some people participated in a conference from 9:00 am to 11:00 am on Mar. 30, 2011.","RDF can be used in many applications, one of which is an implicit social network. In implicit social networks, the relations between entities may not be managed explicitly by people but change based on when the relation is evaluated and what interactions may be used to derive the relations. These interactions can be represented as RDF graphs and used to derive social network RDF graphs through an inference engine. For example, given a set of RDF graphs representing the email exchanges between employees in a company, an inference engine can derive the implicit social network RDF graph based on the email exchanges.","Because employees interact with different people at different time, it is important to evaluate the strength of social relation based on the history of the interactions. More recent and more frequent interactions create stronger relations than less recent and less frequent interactions. For instance, if John sent 100 emails to Mary and 50 emails to Bob in last month, then John's relation with Mary is stronger than with Bob based on these interactions. On the other hand, if John sent 1000 emails to Mary last year, and 1000 emails to Bob this year, then John's relation with Bob is stronger than with Mary based on these emails.","Due to the highly dynamic nature of implicit social networks, frequent reevaluation of relation strength is necessary. Whenever a new social interaction, e.g., a new email, is added, the inference engine needs to reevaluate the affected relations. The inference engine also needs to reevaluate a relation depending on a reference time. For example, if John's relation with Mary depends on 1000 emails between them in 2010, the strength of this relation will decrease as time goes by if no new interactions are observed.","The time complexity of such reevaluation is important to the usability of these networks. Assuming an implicit social network of E entities, where the average number of interactions between two entities is A, then the time complexity to reevaluate N changes on entire set of A is O(N*E*E*A). This time complexity translates to O(10^9*N) calculations when E=1000 (large corporation) and A=1000 (average 5 emails per day for 5 years). Reevaluation from scratch every time is therefore not efficient.","What is needed is a method and system that efficiently evaluate the time weight in RDF graphs. When used in social networks, the method should achieve optimal time complexity, e.g., O(N*E*E) for a network of E entities. Embodiments in accordance with the present invention provide such a method and system based on RDF and an exponential decay function that supports efficient incremental updates of time weight in RDF graphs.","Researchers in the field have attempted to introduce temporal information into the RDF framework. The goal of these solutions has been to associate a time domain with a RDF triple to assert the validity of that triple. However, the validity of a triple at a particular time interval or point in time is not the time weight between two people. For instance, the fact that John sent Mary an email on Mar. 30, 2011 does not tell the time weight of their relation, which depends on all social interactions, including emails, between them.","Some RDF models may introduce weight to edges in a RDF graph, but an efficient method of calculating that weight has not been developed in the art. In contrast, embodiments in accordance with the present invention calculate the weight in optimal or near-optimal time.",{"@attributes":{"id":"p-0112","num":"0111"},"figref":"FIG. 9","b":["900","903","903","901","902","901","901","901"]},"Assume that a relation R is derived from a set of n activities each with timestamp T, then the time weight of this relation, with respect to a reference timestamp T, is provided by calculating less time weight as the time difference increases between timestamp Tand timestamp T, i.e., an inverse relationship. For instance, the inverse relationship may be inversely proportional to the time difference, or may be inversely proportional to a function of the time difference (e.g., a logarithmic function), or may be an exponential decay function. In one embodiment in accordance with the present invention, the inverse relationship may be provided according to Equation 5, where C is a normalizing factor and \u03bb is a scaling factor. The factors C and \u03bb may be chosen to keep the weight in the desired range, which is determined by the system.",{"@attributes":{"id":"p-0114","num":"0113"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mo":"\u2062","mrow":{"mrow":[{"mrow":[{"mi":"tw","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["T","\u03bb","C","R"],"mo":[",",",",","]}}},{"mrow":[{"mi":"C","mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msup":{"mi":"\u2147","mrow":{"mo":"-","mrow":{"mi":"\u03bb","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"T","mo":"-","msub":{"mi":["T","i"]}}}}}}}},{"mi":"C","mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":"\u2147","mrow":{"mrow":{"mo":"-","mi":"\u03bb"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"T"}},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msup":{"mi":"\u2147","mrow":{"mi":"\u03bb","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["T","i"]}}}}}],"mo":"="}],"mo":"="},{"mi":"R","mo":"=","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"msub":{"mi":["T","i"]},"mo":"|","mrow":{"msub":{"mi":["T","i"]},"mo":"\u2264","mi":"T"}},{"mn":"1","mo":["\u2264","\u2264"],"mi":["i","n"]}],"mo":","}}}],"mo":[",","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}}]}}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}}},"The calculated time weight is stored for incremental calculations. There may be various mechanisms to represent time weight RDF. Two embodiments of methods to represent time weight RDF are illustrated below.","In a first embodiment in accordance with the present invention, using N-Quad, the time weight may be stored in a file and the file may be linked to a RDF graph by a context URI (the fourth component in an N-Quad-extended RDF triple). The context URI identifies a context file so that it can be accessed, for instance by using HTTP.","The following statement may be added or changed in the context file:","<ex:interact> <ex:time_weight> \u201c{time weight}\u201d.","In the RDF file:","<ex:john> <ex:interact> <ex:mary> <{URI to the context file}>.","In a second embodiment in accordance with the present invention, a RDF reification statement can be used to add the time value as a property to a RDF graph as illustrated below.",{"@attributes":{"id":"p-0122","num":"0121"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"<ex:john> <ex:interact> <ex:mary> ."]},{"entry":[{},"_:xxx rdf:type rdf:Statement ."]},{"entry":[{},"_:xxx rdf:subject <ex:john> ."]},{"entry":[{},"_:xxx rdf:predicate <ex:interact> ."]},{"entry":[{},"_:xxx rdf:object <ex:mary> ."]},{"entry":[{},"_:xxx ex:time_weight \u201c{time value}\u201d."]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Updating the saved weight of a relation proceeds as follows. A saved time weight of a relation should be updated whenever a new input RDF graph has been added to, or deleted from, a relation. If a new RDF graph is at timestamp Tn+1 is added to relation R, then the new time weight of R, with respect to the same reference timestamp, can be calculate based on previous time value by calculating less time weight as the time difference increases between timestamp Tn+1 and timestamp T, i.e., an inverse relationship. For instance, the inverse relationship may be inversely proportional to the time difference, or may be inversely proportional to a function of the time difference (e.g., a logarithmic function), or may be an exponential decay function. In one embodiment in accordance with the present invention, the inverse relationship may be provided according to Equation 6, where is a normalizing factor and A is a scaling factor.\n\n(})=()\u00b1\u2003\u2003(6)\n","A saved time weight of a relation should be updated whenever a new reference timestamp is provided. The reference timestamp, or reference time, is a time moment after the epoch time. Reference time is a time chosen by an application running on the computer system whereas epoch time is a fixed time for all applications running on the system. Assume that tw(Tx, . . . ) is the previous time weight for R at time reference Tx and tv(Ty, . . . ) is the new time weight for R at a different reference timestamp Ty, then the new time weight is calculated incrementally by calculating less time weight as the time difference increases between timestamp Tx and timestamp Ty, i.e., an inverse relationship. For instance, the inverse relationship may be inversely proportional to the time difference, or may be inversely proportional to a function of the time difference (e.g., a logarithmic function), or may be an exponential decay function. In one embodiment in accordance with the present invention, the inverse relationship may be provided according to Equation 7, where C is a normalizing factor and \u03bb is a scaling factor.\n\n()=()\u2003\u2003(7)\n","Embodiments in accordance with the present invention provide an optimal or near-optimal calculation of the time weight because a new time-weight values can be calculated from previous values based on equations (6) and (7) in a minimal number of steps. When adding or removing a new time point, one step is calculated for the newly added or removed time point. Following equation (6), the one step is calculated by adding or subtracting an exponential term, therefore the calculation is performed in an optimal number of steps, and without undue computational complexity. Likewise, when the reference time is shifted, we have to calculate at least one step for the new reference time. Again, equation (7) does exactly one calculation which multiplies the exponential term. Therefore it is also performed in an optimal number of steps, and without undue computational complexity.","While the foregoing is directed to embodiments of the present invention, other and further embodiments of the present invention may be devised without departing from the basic scope thereof. It is understood that various embodiments described herein may be utilized in combination with any other embodiment described, without departing from the scope contained herein. Further, the foregoing description is not intended to be exhaustive or to limit the present invention to the precise form disclosed. Modifications and variations may be possible in light of the above teachings or may be acquired from practice of the present invention.","No element, act, or instruction used in the description of the present application should be construed as critical or essential to the invention unless explicitly described as such. Also, as used herein, the article \u201ca\u201d is intended to include one or more items. Where only one item is intended, the term \u201cone\u201d or similar language is used. Further, the terms \u201cany of\u201d followed by a listing of a plurality of items and\/or a plurality of categories of items, as used herein, are intended to include \u201cany of,\u201d \u201cany combination of,\u201d \u201cany multiple of,\u201d and\/or \u201cany combination of multiples of\u201d the items and\/or the categories of items, individually or in conjunction with other items and\/or other categories of items.","Moreover, the claims should not be read as limited to the described order or elements unless stated to that effect. In addition, use of the term \u201cmeans\u201d in any claim is intended to invoke 35 U.S.C. \u00a7112, \u00b6 6, and any claim without the word \u201cmeans\u201d is not so intended."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["So the manner in which the above recited features of the present invention can be understood in detail, a more particular description of embodiments of the present invention, briefly summarized above, may be had by reference to embodiments, which are illustrated in the appended drawings. It is to be noted, however, the appended drawings illustrate only typical embodiments encompassed within the scope of the present invention, and, therefore, are not to be considered limiting, for the present invention may admit to other equally effective embodiments, wherein:",{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
