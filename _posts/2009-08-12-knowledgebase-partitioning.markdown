---
title: Knowledgebase partitioning
abstract: Some embodiments of knowledgebase partitioning for implementing parallelization in a rule engine have been presented. In one embodiment, a compiler divides a knowledgebase into a set of virtual partitions. The knowledgebase includes a network constructed according to rules added to the knowledgebase. A rule engine may execute the virtual partitions of the partitioned knowledgebase in parallel using threads obtained from a pool of threads.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08645296&OS=08645296&RS=08645296
owner: Red Hat, Inc.
number: 08645296
owner_city: Raleigh
owner_country: US
publication_date: 20090812
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["TECHNICAL FIELD","BACKGROUND","DETAILED DESCRIPTION"],"p":["Embodiments of the present invention relate to artificial intelligence, and more specifically to rule engines.","The development and application of rule engines is one branch of Artificial Intelligence (A.I.), which is a very broad research area that focuses on \u201cmaking computers think like people.\u201d Broadly speaking, a rule engine is a set of one or more software modules running on a computing device (e.g., a server, a personal computer, etc.) that processes information by applying rules to data objects (e.g., facts). A rule is a logical construct for describing the operations, definitions, conditions, and\/or constraints that apply to some predetermined data to achieve a goal. Various types of rule engines have been developed to evaluate and process rules. Conventionally, a compiler compiles a rulebase containing a network constructed according to a set of rules. Then a rule engine evaluates the compiled rulebase against data objects, such as facts. The network may include many different types of nodes, including, for example, object-type nodes, alpha nodes, left-input-adapter nodes, eval nodes, join nodes, not nodes, and terminal nodes, etc.","Typically, facts enter a network at the root node, from which they are propagated to any matching object-type nodes. From a object-type node, a data object is propagated to either an alpha node (if there is a literal constraint), a left-input-adapter node (if the data object is the left most object type for the rule), or a beta node (such as a join node). However, connections between the nodes in the network are synchronous and typically, only one fact is propagated through the network at a time.","However, use cases like event processing, where a huge volume of data is processed, but a relatively low volume of actions are taken as a result of the processing, stress the evaluation algorithm to the most, while keeping the actual rule and\/or query activations low. To support such scenarios, conventional event processing products run specialized algorithms that treat each rule or query on the event stream and\/or cloud individually, allowing high parallelization but limiting the ability of optimizations. These algorithms do not perform well or even do not allow for rules reasoning.","On the other hand, trying to use conventional rule engine algorithms, like RETE, to process events is not feasible, due to the synchronization requirements of such algorithms. Namely, in order to ensure reasoning integrity, the algorithm forces synchronized reasoning, although, the actual matching algorithm accepts some level of parallelism as described by Charles Forgy, Anoop Gupta, and Allen Newel in the paper \u201cHigh-Speed Implementations of Rule-Based Systems.\u201d","In this paper, they discuss the possible gains of Rete parallelization on three levels: matching, conflict resolution, and action steps. In particular, they describe a possible solution for rule parallelization as creating a network composed of multiple partitions, where each partition would include a single rule. Each rule could then be assigned a thread and be processed in parallel. As they note, this solution has a big drawback that is the loss of all the optimizations a Rete network could have, especially node sharing, that in their calculations increase processing cost by a factor of about 1.6.","Described herein are some embodiments of knowledgebase partitioning for implementing parallelization in a rule engine. In one embodiment, a compiler divides a knowledgebase into a set of virtual partitions (or simply referred to as partitions). The knowledgebase includes a network constructed according to rules added to the knowledgebase. A rule engine may execute the virtual partitions of the partitioned knowledgebase in parallel using threads obtained from a pool of threads. Because the rule engine according to some embodiments of the invention processes rules and events in the same rulebase, unlike conventional rule engines that process rules and facts only, the rulebase is thus referred to as a \u201cknowledgebase\u201d in this document to distinguish it from rulebases created by conventional rule engines. Likewise, a rule session of the rule engine according to some embodiments of the invention is referred to as a \u201cknowledge session\u201d hereinafter for similar reason. More details of some embodiments of knowledgebase partitioning are described below.","In the following description, numerous details are set forth. It will be apparent, however, to one skilled in the art, that the present invention may be practiced without these specific details. In some instances, well-known structures and devices are shown in block diagram form, rather than in detail, in order to avoid obscuring the present invention.","Some portions of the detailed descriptions below are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here, and generally, conceived to be a self-consistent sequence of operations leading to a desired result. The operations are those requiring physical manipulations of physical quantities. Usually, though not necessarily, these quantities take the form of electrical or magnetic signals capable of being stored, transferred, combined, compared, and otherwise manipulated. It has proven convenient at times, principally for reasons of common usage, to refer to these signals as bits, values, elements, symbols, characters, terms, numbers, or the like.","It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion, it is appreciated that throughout the description, discussions utilizing terms such as \u201cprocessing\u201d or \u201ccomputing\u201d or \u201ccalculating\u201d or \u201cdetermining\u201d or \u201cdisplaying\u201d or the like, refer to the action and processes of a computer system, or similar electronic computing device, that manipulates and transforms data represented as physical (electronic) quantities within the computer system's registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage, transmission, or display devices.","The present invention also relates to apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer-readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, and magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, or any type of media suitable for storing electronic instructions, and each coupled to a computer system bus.","The algorithms and displays presented herein are not inherently related to any particular computer or other apparatus. Various general-purpose systems may be used with programs in accordance with the teachings herein, or it may prove convenient to construct more specialized apparatus to perform the required operations. The required structure for a variety of these systems will appear from the description below. In addition, the present invention is not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the invention as described herein.",{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 1","FIG. 5"],"b":["510","520"]},"Referring to , processing logic analyzes rules added to a knowledgebase (processing block ). The knowledgebase may include a network (e.g., a Rete network) constructed according to the rules. The network typically includes various types of nodes, which are further discussed in details below. Then processing logic creates virtual partitions in the knowledgebase to increase parallelization points (processing block ). A virtual partition (or simply referred to as a partition hereinafter) broadly refers to a logical dividing between nodes in a network. Thus, a virtual partition may also be viewed as a logical group of one or more nodes in the network. Parallelization points generally refer to points in the network at which parallel processing using multiple threads is possible.","In some embodiments, processing logic creates a partition manager for each virtual partition for task scheduling and synchronization (processing block ). For example, the partition manager may request threads and manage the use of threads to propagate facts through nodes in its respective partition. More details of the partition manager are discussed below. Processing logic may execute at least two of the virtual partitions in parallel using threads (processing block ).","Note that in a scenario where rules are completely disjoint without any possible optimization between them, the rule engine may create one virtual partition for each rule. However, as in the case with many rules, such as business rules, it is frequent that at least some rules share common patterns and constraints, and in this case, the above approach improves over the conventional approach by preserving the network optimizations, such as node sharing, alpha node hashing, etc.","In some embodiments, knowledgebase partitioning is provided as an optional feature in configuration, which can be enabled or disabled by rule engine users. To enable this configuration, the user may use a system property, a configuration file, or an application programming interface (API) call. One example using an API call is shown below:\n\n","Once activated, the feature of knowledgebase partitioning may be divided into two components, namely, a compile time component and a runtime component. The compile time component may be implemented with a compiler and the runtime component may be implemented with a rule engine. Generally speaking, the compile time component partitions a knowledgebase, while the runtime component executes the partitions in the partitioned knowledgebase.",{"@attributes":{"id":"p-0031","num":"0032"},"figref":["FIG. 2","FIG. 5"],"b":"510"},"Referring to , processing logic creates a first virtual partition to contain entry point nodes and object type nodes of a network in a knowledgebase (processing block ). For each rule added to the knowledgebase, processing logic creates a virtual partition for a given rule (processing block ). Then processing logic identifies all nodes associated with the given rule (processing block ).","For each node identified, processing block determines if the node already exists for another rule (processing block ). If the node already exists for another rule, then processing logic allows the virtual partition of the given rule to share this node with the other virtual partition that contains this node (processing block ). Otherwise, processing block adds the node to the virtual partition of the given rule (processing block ). From either processing block  or processing block , processing logic transitions into processing block  to check if there are any more nodes associated with the given rule. If there is at least one more node, then processing logic transitions back to processing block  to repeat the above operations. Otherwise, processing logic checks if there are any more rules added in the knowledgebase (processing block ). If there is at least one more rule, then processing logic transitions back to processing block  to repeat the above operations. Otherwise, the process ends. To further illustrate the above operations, one example is discussed in details below with reference to .","In one example, the following rule is first added to a knowledgebase:",{"@attributes":{"id":"p-0035","num":"0036"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"rule \u201cRule 1: Customer likes cheese\u201d"]},{"entry":[{},"when"]},{"entry":[{},"\u2003Customer( name == \u201cBob\u201d, $likes : likes )"]},{"entry":[{},"\u2003Cheese( price > 10, type == $likes )"]},{"entry":[{},"then"]},{"entry":[{},"\u2003\/\/ do something"]},{"entry":[{},"end"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"A compiler may generate a network in the knowledgebase as illustrated in  for the above rule. In the network , each root node may also be referred to as an entry point. The name of the first entry point  is implicitly called \u201cEntryPoint:MAIN\u201d and each subsequent entry point may receive a name explicitly defined by a rule engine user. Following the entry point  are object type nodes (OTNs)  and . The OTNs may be followed by alpha network (AN), which is a sequence of nodes responsible for evaluating alpha constraints. In , the ANs  and  are each represented as a single node for simplicity, but there can be from zero to many nodes in the AN, following each of the OTNs. Following the ANs is a beta network (BN), which is a sequence of nodes responsible for joining branches in the network  and applying the beta constraints. In , the BN  is represented as a single node for simplicity, but there can be from zero to many BNs preceding each terminal node. Finally, the network  includes a terminal node (TN) for each of the rules in the knowledgebase. For example, TN  is for the rule set forth above.","In some embodiments, the compiler creates a virtual partition that contains all the entry point nodes and the OTNs. This virtual partition may be called \u201cMAIN,\u201d as shown in . Then all the nodes for that rule may be added to a new virtual partition, sequentially numbered for simplicity, unless they already exist for another rule, in which case, they may be shared. Since this is the first rule added into the knowledgebase, there is no sharing and there is one more virtual partition, i.e., Partition , in addition to Partition MAIN in . To determine to which virtual partition each node belongs to, a label may be added to the node with the corresponding virtual partition label.","In some embodiments, the connections between nodes inside the same virtual partition are regular synchronous connection, such as connections  and , but connections between nodes of different partitions are asynchronous, such as connections  and . Furthermore, each of the asynchronous connections may have a queue, which may also be referred to as a built-in queue. Facts reaching an asynchronous connection may be put into the queue temporarily during execution of the partitions. More details of the use of the queue are discussed below.","When a new rule is added to the knowledgebase, the compiler may identify which nodes can be shared and reuse them. New nodes are added as usual and a new partition is created for the new non-shared nodes. For instance, suppose the following rule is added to the knowledgebase:",{"@attributes":{"id":"p-0040","num":"0041"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"rule \u201cRule 2: Order cheese\u201d"]},{"entry":[{},"when"]},{"entry":[{},"\u2003Cheese( price > 10, $cheese : this )"]},{"entry":[{},"\u2003OrderEvent( product == $cheese ) from entry-point \u201cOrders\u201d"]},{"entry":[{},"then"]},{"entry":[{},"\u2003\/\/ do something"]},{"entry":[{},"end"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"In response to the addition of the above rule (i.e., Rule 2), the network  in  is further expanded as illustrated in . In , the network  has three partitions, namely, Partition MAIN, Partition , and a newly created Partition . As shown in , the nodes for the Cheese pattern (i.e., nodes  and ) are reused because these nodes already exist in Partition MAIN and Partition , respectively. A new entry point node \u201cOrders\u201d  and a new OTN \u201cOrderEvent\u201d  are created inside Partition MAIN. The rest of the new nodes associated with Rule 2 (i.e., nodes  and ) are created inside the new partition, Partition .","As previously explained, all connections between different partitions in  are asynchronous. So, the connection  between the AN \u201cprice>10\u201d  and BN \u201cproduct==$cheese\u201d  is an asynchronous connection. Likewise, the connection  between \u201corderEvent\u201d  and \u201cproduct==$cheese\u201d  is also asynchronous.","After dividing the knowledgebase into partitions, a rule engine can execute the partitions using threads. A thread generally refers to an instance of a sequence of code that is operating as a unit, typically on behalf of a single user, transaction, or message. A thread may run on a processing device (e.g., a central processing unit, a multi-core processor, etc.) to perform one or more tasks. Multiple threads can run concurrently on the same processing device. Thus, threads are useful in parallelization.",{"@attributes":{"id":"p-0044","num":"0045"},"figref":["FIG. 4","FIG. 3C","FIG. 5"],"b":"520"},"Referring to , processing logic gets a first thread from a pool of threads to insert a fact into an entry point in a first virtual partition (processing block ). Then processing logic propagates the fact using the first thread until the fact reaches an asynchronous connection between the first virtual partition and a second virtual partition (processing block ). As mentioned above, there may be a queue at each asynchronous connection. Processing logic may put the fact into the queue (processing block ) and release the first thread (processing block ).","In some embodiments, processing logic gets a second thread from the pool of threads (processing block ). Processing logic checks if the second thread is available at processing block . If the second thread is not available, then processing logic returns to processing block . Otherwise, processing logic gets the fact from the queue (processing block ) and uses the second thread to propagate the fact through the asynchronous connection into the second virtual partition, and then through nodes in the second virtual partition (processing block ). When a fact reaches a terminal node associated with a rule in the network, the rule is fully matched. Thus, processing logic may put the rule into an agenda of the rule engine to be fired or activated (processing block ). Firing a rule may cause actions to be performed as a result of the rule being matched. To further illustrate the above method, one example is discussed in details below with reference to the network C in .","As mentioned above, at least some of the virtual partitions are executed in parallel by threads, which may include light-weight threads. Light-weight threads typically refer to user-level threads, which has little amount of context saved with it, compared to other types of threads. In some embodiments, the threads are configured into a pool of worker threads. The size of the pool is configurable by one of several ways, such as using a system property, using a configuration file, or an API call, etc. The following is one example of using the API to configure a thread pool of size five (5):\n\n","Given the previous information of thread pool size and the list of all partitions created at compile time, when a user creates a new knowledge session for the given knowledgebase, the rule engine may internally create a partition manager for each virtual partition and the pool of threads. The partition manager is responsible for task scheduling and synchronization of the virtual partition it is assigned to. The mapping between virtual partitions and partition managers may be one-to-one.","Referring back to the previous example shown in , when a Customer fact is inserted into the knowledgebase through a working memory of the rule engine, it enters the network  with a synchronous operation through the MAIN entry point , propagates to the Customer OTN  with another synchronous operation, and then reaches an asynchronous connection when propagating to the AN \u201cname==\u201cBob\u201d\u201d . At this point, the thread that inserted the fact into the working memory returns to the main application to continue its work, while a new thread is requested by the partition manager of Partition , Partition Manager  (PM). When a worker thread is assigned to PM, the worker thread gets the Customer fact from the built-in queue of the asynchronous connection , propagates it to the AN  and then down the network  as usual, since all connections below the AN  are synchronous. When the propagation is finished, PM releases the worker thread and waits for new propagations.","Similarly, when a Cheese fact is inserted into the network , it may propagate as previously described until it reaches the AN \u201cprice>10\u201d . At this point, there are two propagation paths: one synchronous (i.e., connection ) that will be executed by the same thread that PM is already using, and the other, asynchronous (i.e., connection ) that will be placed on the built-in queue of partition manager of Partition , i.e., PM, for BN \u201cproduct==$cheese\u201d . PM may then request a worker thread and behave in the same way as PM, although for Partition .","The above approach ensures that at any given time, only one worker thread is assigned to each virtual partition. But multiple worker threads might be propagating facts in different virtual partitions. Thus, parallelization may be achieved while preserving other optimizations made to the network . Also, synchronization between the agenda that is firing rules and the network is done to avoid a rule firing while a potential match could be cancelling it.","One of the many improvements of the above approach over the conventional approach is that this new approach not only preserves all optimizations of the network, including node sharing, but also other optimizations that an object oriented network implements, such as alpha node hashing, rules versus data split (which allows knowledgebase sharing among sessions), constraint Just-In-Time compilation (JITing), etc. Also, the above approach makes use of computational resources not available to the original approach, like light-weight threads and in-process light-weight synchronization mechanisms.",{"@attributes":{"id":"p-0053","num":"0056"},"figref":["FIG. 5","FIG. 5"],"b":["500","510","520","501","510","510","501","2","3","3"]},"The partitioned knowledgebase  is then provided to the rule engine , which may execute the partitions in the knowledgebase  as discussed above with reference to . For example, a rule engine user may assert facts into a working memory  of the rule engine . The facts  asserted may be inserted into various entry points of the network of the partitioned knowledgebase . The rule engine  may use threads to propagate the facts  through the network as discussed above.",{"@attributes":{"id":"p-0055","num":"0058"},"figref":["FIG. 6A","FIG. 7"],"b":["7100","7110","7120","7130","7110","7130","7110","7120","7120","700"]},"In some embodiments, the server  includes a rule engine  such as the rule engine  shown in , and a compiler , such as the compiler  shown in . The client machine  may present a GUI  (e.g., a webpage rendered by a browser) to allow users to input rules, events, and\/or facts, which may be sent to the server  to be processed using the compiler  and the rule engine  as discussed above.",{"@attributes":{"id":"p-0057","num":"0060"},"figref":["FIG. 6B","FIG. 7","FIG. 5"],"b":["7200","7150","700","7150","7151","7153","7152","7151","7153","7152","7151","7153"]},{"@attributes":{"id":"p-0058","num":"0061"},"figref":"FIG. 7","b":"700"},"The exemplary computer system  includes a processing device , a main memory  (e.g., read-only memory (ROM), flash memory, dynamic random access memory (DRAM) such as synchronous DRAM (SDRAM), etc.), a static memory  (e.g., flash memory, static random access memory (SRAM), etc.), and a data storage device , which communicate with each other via a bus .","Processing device  represents one or more general-purpose processing devices such as a microprocessor, a central processing unit, or the like. More particularly, the processing device may be complex instruction set computing (CISC) microprocessor, reduced instruction set computing (RISC) microprocessor, very long instruction word (VLIW) microprocessor, or processor implementing other instruction sets, or processors implementing a combination of instruction sets. Processing device  may also be one or more special-purpose processing devices such as an application specific integrated circuit (ASIC), a field programmable gate array (FPGA), a digital signal processor (DSP), network processor, or the like. The processing device  is configured to execute the rule engine with knowledgebase partitioning module  for performing the operations and steps discussed herein.","The computer system  may further include a network interface device . The computer system  also may include a video display unit  (e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)), an alphanumeric input device  (e.g., a keyboard), a cursor control device  (e.g., a mouse), and a signal generation device  (e.g., a speaker).","The data storage device  may include a machine-accessible storage medium  (also known as a computer-readable storage medium) on which is stored one or more sets of instructions (e.g., rule engine with knowledgebase partitioning module ) embodying any one or more of the methodologies or functions described herein. The rule engine with knowledgebase partitioning module  may also reside, completely or at least partially, within the main memory  and\/or within the processing device  during execution thereof by the computer system , the main memory  and the processing device  also constituting machine-accessible storage media. The rule engine with knowledgebase partitioning module  may further be transmitted or received over a network  via the network interface device .","While the machine-accessible storage medium  is shown in an exemplary embodiment to be a single medium, the term \u201ccomputer-readable storage medium\u201d should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and\/or associated caches and servers) that store the one or more sets of instructions. The term \u201ccomputer-readable storage medium\u201d shall also be taken to include any medium that is capable of storing or encoding a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present invention. The term \u201ccomputer-readable storage medium\u201d shall accordingly be taken to include, but not be limited to, solid-state memories, optical and magnetic media, etc.","The module, rule engine with knowledgebase partitioning module , components and other features described herein (for example, in relation to ) can be implemented as discrete hardware components or integrated into the functionalities of hardware components, such as ASICS, FPGAs, DSPs, or similar devices. In addition, the rule engine with knowledgebase partitioning module  can be implemented as firmware or functional circuitries within hardware devices. Further, the rule engine with knowledgebase partitioning module  can be implemented in any combination of hardware devices and software components.","Thus, some embodiments of knowledgebase partitioning have been described. It is to be understood that the above description is intended to be illustrative, and not restrictive. Many other embodiments will be apparent to those of skill in the art upon reading and understanding the above description. The scope of the invention should, therefore, be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The present invention is illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings and in which:",{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3C"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 6B"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
