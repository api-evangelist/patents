---
title: Integrated speech recognition and semantic classification
abstract: A novel system integrates speech recognition and semantic classification, so that acoustic scores in a speech recognizer that accepts spoken utterances may be taken into account when training both language models and semantic classification models. For example, a joint association score may be defined that is indicative of a correspondence of a semantic class and a word sequence for an acoustic signal. The joint association score may incorporate parameters such as weighting parameters for signal-to-class modeling of the acoustic signal, language model parameters and scores, and acoustic model parameters and scores. The parameters may be revised to raise the joint association score of a target word sequence with a target semantic class relative to the joint association score of a competitor word sequence with the target semantic class. The parameters may be designed so that the semantic classification errors in the training data are minimized.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07856351&OS=07856351&RS=07856351
owner: Microsoft Corporation
number: 07856351
owner_city: Redmond
owner_country: US
publication_date: 20070119
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Speech understanding is a growing field that involves enabling a machine to interpret the meaning of spoken language. One aspect of speech understanding is spoken utterance classification, which seeks to apply semantic classification to spoken utterances. Spoken utterance classification typically involves a two-step process. First, a spoken utterance is processed with techniques of automatic speech recognition, using a language model, to determine the sequence of words expressed in the spoken utterance. Second, the sequence of words is processed with techniques of semantic classification, using a classification model, to parse their meaning.","Each of these two steps has the potential to work imperfectly and produce erroneous results. Since the output from the speech recognition process forms the input to the semantic classification process, this means any erroneous results of the speech recognition process will be perpetuated in the semantic classification process, reflecting the old computing aphorism of \u201cgarbage in, garbage out\u201d. The risk of erroneous final output from the semantic classification process is therefore compounded. The potential for error has typically been addressed by training the language model to reduce errors in determining the sequences of words from spoken utterances, and training the classification model to reduce errors in determining the semantic classes of the word sequences.","The discussion above is merely provided for general background information and is not intended to be used as an aid in determining the scope of the claimed subject matter.","A novel system integrates speech recognition and semantic classification, so that the acoustic scores in the speech recognizer that accepts the original spoken utterances may be taken into account when training both language models and semantic classification models, in a variety of embodiments. For example, a joint association score may be defined that is indicative of a correspondence of a semantic class and a word sequence for an acoustic signal. The joint association score may incorporate parameters that are applied to features of the word sequence for signal-to-class modeling of the acoustic signal. It may also incorporate language model parameters and scores, as well as acoustic model parameters and scores. A target word sequence may be identified that has a highest joint association score with a target semantic class. Competitor word sequences may also be identified that each have a highest remaining joint association score with any remaining semantic class. The parameters may then be revised to raise the joint association score of the target word sequence with the target semantic class relative to the joint association score of the competitor word sequence with the target semantic class. The semantic model weighting parameters and language model parameters are designed so that the semantic classification errors in the training data are minimized.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter. The claimed subject matter is not limited to implementations that solve any or all disadvantages noted in the background.",{"@attributes":{"id":"p-0011","num":"0010"},"figref":["FIG. 1","FIGS. 7 and 8","FIG. 2","FIG. 1"],"b":["100","100","200","100","100","100","202","204","206","208","200"]},"Method  begins with step , of defining a joint association score indicative of a correspondence of a semantic class and a word sequence for an acoustic signal, wherein the joint association score incorporates one or more weighting parameters that are applied to one or more features of the word sequence for signal-to-class modeling of the acoustic signal. The signal-to-class modeling may include speech recognition modeling, language modeling, semantic classification modeling, or any or all three of them in combination. The signal-to-class modeling may incorporate model parameters and scores for any of speech recognition modeling, language modeling, semantic classification modeling. Referring to discriminative training system , one or more acoustic signals  may be provided and subjected to an acoustic decoder or preprocessor , to provide a set of n-best word sequences . For example, in the present illustrative embodiment, acoustic decoder or preprocessor  may convert an acoustic signal X() into an acoustic cepstra sequence. The set of n-best word sequences  may be generated, for example, using a lattice of word sequences, which provides a compact representation of n-best word sequences because it can match an acoustic signal with a word sequence by summing up acoustic scores of all paths of the speech lattices that correspond to possible word sequences for the acoustic signal. The n-best word sequences  are used to define the initial weighting parameters of the discriminative acoustic trainer , which render an updated language model  for automatic speech recognition. That is, discriminative acoustic trainer  may use minimum classification error discriminative training to update the acoustic model parameters from baseline acoustic model . In particular, discriminative acoustic trainer  may use weighting parameter update equations that are formed by optimizing a minimum classification error function as discussed in more detail below. Using the update equations, discriminative acoustic trainer  produces updated language model .","The updated language model  and a set of semantic classes  are used initially with a baseline semantic model  to define the weighting parameters of discriminative semantic trainer , which define an updated semantic model . Joint association scorer  uses an acoustic score and a word string probability for a given word sequence relative to an acoustic signal from updated language model , and a class-posterior probability for a given semantic class with respect to a word sequence from updated semantic model . Once a set of joint association scores are obtained, they may be compared with each other by joint association score comparator . These aspects are further described as follows.","For a given acoustic signal , the acoustic scorer or preprocessor  interprets the acoustic signal with reference to the baseline language model  to generate an n-best list \u03c9 () of word sequences W, i.e. \u03c9={W, . . . , W}. For each of the word sequences W, a word string probability P(W) may be calculated based on the weighting parameters of the language model  or . For each of the word sequences W, an acoustic score of the associated acoustic signal P(X|W) can also be obtained, by summing up the acoustic scores of all the paths on an associated speech lattice that yield the word sequence Wfor the given acoustic signal X. At discriminative semantic trainer , the class-posterior probability P(C|W) of a semantic class Cfrom among the semantic classes  matching a word sequence Wmay be modeled with a maximum entropy model, an illustrative example of which is provided as follows, using semantic classification model weighting parameters \u03bb, and lexical features fthat serve as language model weighting parameters:",{"@attributes":{"id":"p-0015","num":"0014"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["C","r"]},{"mi":["W","r"]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},"mo":"=","mfrac":{"mrow":[{"mi":"exp","mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"i"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"\u2062","mrow":{"msub":{"mi":["f","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["C","r"]},{"mi":["W","r"]}],"mo":","}}}}}},{"munder":{"mo":"\u2211","msub":{"mi":["C","r"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"\u2062","mrow":{"msub":{"mi":["f","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["C","r"]},{"mi":["W","r"]}],"mo":","}}}}}}}]}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1"}}}]}}}}},"As an example of the lexical features f, these may for example include n-gram features, such as bigrams, where n=2:",{"@attributes":{"id":"p-0017","num":"0016"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msubsup":{"mi":["f","bigram"],"mrow":{"mi":"c","mo":",","mrow":{"msub":[{"mi":["w","x"]},{"mi":["w","y"]}],"mo":"\u2062"}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["C","r"]},{"mi":["W","r"]}],"mo":","}}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mn":"1","mo":","}},{"mrow":{"mrow":[{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["C","r"]}},{"mrow":{"mrow":{"mi":"c","mo":["\u2062","^","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":["w","x"]}},"mo":"\u2062","msub":{"mi":["w","y"]}},"mo":"\u2208","msub":{"mi":["W","r"]}}],"mo":"="}}]},{"mtd":[{"mrow":{"mn":"0","mo":","}},{"mi":"otherwise"}]}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"2"}}}]}}}}},"A goal of discriminative training system  is to define a semantic classification decision rule \u0108that assigns the best semantic class to any acoustic signal, such as a spoken utterance\u2014that is, a rule that uses weighting parameters trained through one or more revisions to render a reduced classification error or a minimum classification error. Put another way, the goal is to devise a semantic classification decision rule \u0108such that:",{"@attributes":{"id":"p-0019","num":"0018"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mover":{"mi":"C","mo":"^"},"mi":"r"},"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"max","msub":{"mi":["C","r"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["C","r"]},{"mi":["X","r"]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"3"}}}]}}}}},"This can be expanded as a function of word string probability P(W), acoustic signal P(X|W), and class-posterior probability P(C|W), as:",{"@attributes":{"id":"p-0021","num":"0020"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mover":{"mi":"C","mo":"^"},"mi":"r"},"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"max","msub":{"mi":["C","r"]}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"munder":{"mo":"\u2211","msub":{"mi":["W","r"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["C","r"]},{"mi":["W","r"]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}},"mo":",","msub":{"mi":["X","r"]}}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["X","r"]},{"mi":["W","r"]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["W","r"]}}}],"mo":["\u2062","\u2062"]}}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"4"}}}]}}}}},"Method , according to the present illustrative embodiment, incorporates the insight that this equation for the semantic classification decision rule \u0108can be reliably estimated as:",{"@attributes":{"id":"p-0023","num":"0022"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mover":{"mi":"C","mo":"^"},"mi":"r"},"mo":"\u2245","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"max","msub":{"mi":["C","r"]}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"munder":{"mi":"max","mrow":{"msub":{"mi":["W","r"]},"mo":"\u2208","mi":"\u03c9"}},"mo":"\u2062","mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["C","r"]},{"mi":["W","r"]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},{"msup":{"mi":"P","mfrac":{"mn":"1","mi":"L"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["X","r"]},{"mi":["W","r"]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["W","r"]}}}],"mo":["\u2062","\u2062"]}}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"5"}}}]}}}}},"This includes reducing the class-posterior probability to the form P(C|W) rather than P(C|W,X), and weighting the acoustic signal P(X|W) with an exponential term of the inverse of L, a function representing the total semantic classification loss, approximating the semantic classification error rate, exhibited by the language model \u039b() and the semantic model \u039b(). Therefore, the maximum of the product of the three probabilities, as taken in Eq. 5, provides the least semantic classification error rate.","Eq. 5, for the semantic classification decision rule \u0108, can be rewritten in an equivalent logarithmic form, as follows:",{"@attributes":{"id":"p-0026","num":"0025"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mover":{"mi":"C","mo":"^"},"mi":"r"},"mo":"\u2245","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"max","msub":{"mi":["C","r"]}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"munder":{"mi":"max","mrow":{"msub":{"mi":["W","r"]},"mo":"\u2208","mi":"\u03c7"}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mi":"log","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["C","r"]},{"mi":["W","r"]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},{"msup":{"mi":"P","mfrac":{"mn":"1","mi":"L"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["X","r"]},{"mi":["W","r"]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":"|"}}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["W","r"]}}}]}}}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"6"}}}]}}}}},"A joint association score D(C,W;X) may be defined as follows:",{"@attributes":{"id":"p-0028","num":"0027"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["C","r"]},"mo":",","mrow":{"msub":[{"mi":["W","r"]},{"mi":["X","r"]}],"mo":";"}}}},{"mi":"log","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["C","r"]},{"mi":["W","r"]}],"mo":"\u2758"}}},{"msup":{"mi":"P","mfrac":{"mn":"1","mi":"L"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["X","r"]},{"mi":["W","r"]}],"mo":"\u2758"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["W","r"]}}}],"mo":["\u2062","\u2062"]}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"7"}}}]}}}}},"It is this joint association score that is defined by joint association scorer  in discriminative training system , in the illustrative embodiment of . Joint association score D(C,W;X) allows the equation defining the semantic classification decision rule \u0108to be simplified as follows:",{"@attributes":{"id":"p-0030","num":"0029"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mover":{"mi":"C","mo":"^"},"mi":"r"},"mo":"\u2245","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"max","msub":{"mi":["C","r"]}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"munder":{"mi":"max","mrow":{"msub":{"mi":["W","r"]},"mo":"\u2208","mi":"\u03c9"}},"mo":"\u2062","mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["C","r"]},"mo":",","mrow":{"msub":[{"mi":["W","r"]},{"mi":["X","r"]}],"mo":";"}}}}}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"8"}}}]}}}}},"Joint association score D(C,W;X) serves as a class-discriminant function, that rates the joint association of the semantic class Cfrom among the semantic classes  and the word sequence Wfrom among the n-best word sequences  for the racoustic signal, or spoken utterance, X. Joint association score D(C,W;X) therefore incorporates one or more weighting parameters that are applied to one or more features of the word sequence Wfrom at least one of the baseline language model , the updated language model , the baseline semantic model , or the updated semantic model .","Once the joint association scores D(C,W;X) are evaluated by joint association scorer , they may be compared with each other by joint association score comparator , and a particular word sequence from the n-best set \u03c9 of word sequences () may be identified as the target word sequence Wthat has the highest joint association, score with a target semantic class Cthat is the correct semantic class for the racoustic signal, X, as in step  of method  in . The target word sequence Wmay be identified as the word sequence that satisfies the equation:",{"@attributes":{"id":"p-0033","num":"0032"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":["W","r"],"mn":"0"},"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"max","mrow":{"msub":{"mi":["W","r"]},"mo":"\u2208","mi":"\u03c9"}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["C","r"],"mn":"0"},"mo":",","mrow":{"msub":[{"mi":["W","r"]},{"mi":["X","r"]}],"mo":";"}}}}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"9"}}}]}}}}},"The target word sequence Wis therefore the most likely sentence to yield the correct semantic classification for the acoustic signal X, independently of whether or not target word sequence Wwould provide the closest matching transcription, as determined by a system for performing automatic speech recognition in isolation from considerations of semantic classification.","With the target word sequence Wthus identified, one or more additional word sequences from among n-best word sequences  (other than the target word sequence W) may be matched iteratively with semantic classes from among semantic classes  and thus identified as competitor word sequences. This is done by identifying the remaining word sequence that has the highest remaining joint association score with any available semantic class other than the target semantic class C. This may be done iteratively, each time removing the identified competitor word sequences and their matching semantic classes from subsequent consideration, and then finding the remaining word sequence that has the highest joint association with any remaining semantic class. This iterative rule for identifying competitor word sequences can be represented as follows:",{"@attributes":{"id":"p-0036","num":"0035"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["C","r"]},"mo":"=","mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":"max","mrow":{"msub":{"mi":["C","r"]},"mo":"\u2208","msup":{"mi":["C","n"]}}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"munder":{"mi":"max","mrow":{"msub":{"mi":["W","r"]},"mo":"\u2208","msup":{"mi":["\u03c9","n"]}}},"mo":"\u2062","mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["C","r","n"]},"mo":",","mrow":{"msub":[{"mi":["W","r"]},{"mi":["X","r"]}],"mo":";"}}}}}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"10"}}}]}}}}},"In Eq. 10, C=C\\{C, . . . , C} and \u03c9=\u03c9\\{W, . . . , W}, where \\ denotes set-difference. The word sequences from among the n-best word sequences \u03c9 () are identified as matching corresponding classes from among semantic classes C () based on the rule:",{"@attributes":{"id":"p-0038","num":"0037"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":["W","r","n"]},"mo":"=","mrow":{"munder":{"mi":"max","mrow":{"mrow":{"msub":{"mi":["W","r"]},"mo":"\u2208","mrow":{"msup":{"mi":["\u03c9","n"]},"mo":["\u2062","\u2062"],"mi":"\\","msubsup":{"mi":["W","r"],"mn":"1"}}},"mo":[",",","],"mi":"\u2026","msubsup":{"mi":["W","r"],"mrow":{"mi":"n","mo":"-","mn":"1"}}}},"mo":"\u2062","mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["C","r","n"]},"mo":",","mrow":{"msub":[{"mi":["W","r"]},{"mi":["X","r"]}],"mo":";"}}}}}},"mo":"]"}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"Eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"11"}}}]}}}}},"Therefore, each of the n competitor word sequences Whas the joint association score D(C,W;X) with its matching semantic class. Any of a wide variety of number n competitor word sequences may be identified, from 1 up to enough to exhaust the available supply of either word sequences in n-best word sequences \u03c9 () or semantic classes from among semantic classes C ().","Following a minimum classification error framework, in the present illustrative embodiment, a class-specific misclassification function d(X) and a class-specific loss function l(d(X)) may be associated with each acoustic signal X, as part of identifying errors of semantic classification that may be remedied by revising the weighting parameters. The class-specific loss function l(d(X)) is a function of the class-specific misclassification function d(X) that is configured to approximate a binary decision for each acoustic signal Xas to whether the acoustic signal has been correctly classified or not. For example, the class-specific loss function l(d(X)) may be structured as a sigmoid function that approximates 0 when the acoustic signal Xis assigned to the correct semantic class, and that approximates 1 when the acoustic signal Xis assigned to the incorrect semantic class. As an illustrative example, these functions may be defined as follows:",{"@attributes":{"id":"p-0041","num":"0040"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}},{"mrow":{"mo":"-","mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["C","r"],"mn":"0"},"mo":",","mrow":{"msubsup":{"mi":["W","r"],"mn":"0"},"mo":";","msub":{"mi":["X","r"]}}}}}},"mo":"+","msup":{"mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mfrac":{"mn":"1","mi":"N"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":"\u03b7","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["C","r","n"]},"mo":",","mrow":{"msubsup":{"mi":["W","r","n"]},"mo":";","msub":{"mi":["X","r"]}}}}}}}}}}}},"mfrac":{"mn":"1","mi":"\u03b7"}}}],"mo":"="}},{"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},"mn":"12"}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}]},{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["l","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}}}},"mo":"=","mfrac":{"mn":"1","mrow":{"mn":"1","mo":"+","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":[{"mo":"-","mi":"\u03b1"},{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":"+","mi":"\u03b2"}}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"13"}}}]}]}}}},"The parameters \u03b1 and \u03b2 may be selected empirically to improve performance in distinguishing between correct and erroneous classifications, as will be understood by those skilled in the art. \u03b1 is a positive constant that controls the size of the learning window and the learning rate, while \u03b2 is a constant measuring the offset of d(X) from 0. For example, in one experimental set, the loss function was found to be optimized with a value for a of approximately 1.2, and a value for \u03b2 of approximately 20, although these values can be set across a fairly wide range with only a modest amount of loss of performance. It was found in one illustrative embodiment to provide some advantage to set \u03b2 to associate additional loss to samples where the class-specific loss function l(d(X)) was otherwise close to 0.5, that is, indicated to be indeterminate between correct and erroneous classification.","The formula for the class-specific misclassification function d(X) also uses N, which is the number of competitor word sequences, which may be up to one less than the number of word sequences in the n-best word sequences \u03c9 (). It also uses \u03b7 as a smoothing function. d(X) is configured to be very small when the semantic classification of acoustic signal Xis correct, in which case l(d(X)) is close to 0; while d(X) is configured to be very large when the semantic classification of acoustic signal Xis incorrect, in which case l(d(X)) is close to 1. The class-specific loss function l(d(X)) may be summed over r to formulate a total classification loss function L(\u039b,\u039b) that approximates the total semantic classification error rate, as follows:",{"@attributes":{"id":"p-0044","num":"0043"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u039b","W"]},{"mi":["\u039b","\u03bb"]}],"mo":","}}},{"munder":{"mo":"\u2211","mi":"r"},"mo":"\u2062","mrow":{"msub":{"mi":["l","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"14"}}}]}}}}},"The total classification loss function L(\u039b,\u039b) thus defined may be targeted for reduction or minimization, as the criterion for revising the weighting parameters \u039b,\u039b to reduce or optimize the semantic classification error rate. This may therefore serve as the basis for revising one or more of the weighting parameters \u039b,\u039b to raise the joint association score D(C,W;X) of the target word sequence Wwith the target semantic class Crelative to the joint association score of the competitor word sequences Wwith the target semantic class C, as in step  of method . This may be followed by a step  to evaluate whether the language model weighting parameters and\/or the semantic classification weighting parameters are approaching convergence, as at decision node  of method . If they are not yet converging, these steps may be repeated iteratively, while if they are converging, the process may be concluded, as at endpoint  of method .","As one specific illustrative example of how the revision of weighting parameters as in step  works in practice, it may be considered in the context of training a specific set of spoken utterance training data acoustic signals Xand selected target semantic classes Cin the specific context of an Airline Travel Information System (ATIS), in which a selected set of semantic classes C from among semantic classes  are assigned to different word sequences Wfrom among n-best word sequences  formed by automatic speech recognition of training data acoustic signals , and the assignments are iteratively reviewed and their weighting parameters revised for the speech recognition language model and semantic classification model.","As the first step of the training stage, the target word sequences Ware identified from among the best word sequences  for having the highest joint association scores with the target semantic classes C, as in step  of method  of . This also represents going through discriminative training system  to joint association score comparator  for a first iteration. For example, one particular target semantic class Cmay be labeled GRD_SRV(ATL) and may represent the semantic meaning, \u201cWhat is the ground transportation in Atlanta?\u201d The goal of the ATIS system in this case is to train the speech recognition language model and semantic classification model so that, as closely as possible, any spoken utterance by a customer with the semantic meaning of \u201cWhat is the ground transportation in Atlanta?\u201d will be matched to the semantic class GRD_SRV(ATL), despite an expected range of variations not only in voice and accent but also in wording. Since the semantic meaning is the type to be expected from an open-ended query, the semantic classification should be able to reliably distinguish this particular semantic meaning from other, potentially unrelated semantic meanings associated with other spoken utterances, some of which may be rather similar in its wording or other acoustic aspects. Accordingly, one of the spoken utterance training data acoustic signals is provided that is rendered into the word sequence \u201cWhat is the ground transportation in Atlanta?\u201d, and system  identifies this word sequence as the target word sequence Was in step  of method  of . This identification includes joint association scorer  defining a joint association score for the target word sequence Wand the target semantic class C, and joint association score comparator  identifying the joint association score D(C,W;X) for the particular target word sequence Was being the highest joint association score of any of the available word sequences for this particular semantic class.","Then, as in step  of method , system  identifies a competitor word sequence Wfrom among the remaining word sequences \u03c9=\u03c9\\{W, . . . , W}, i.e. those word sequences other than the target word sequence W, such that competitor word sequence Whas the highest joint association score D(C,W;X) with any of the remaining semantic classes other than the target semantic class C, that is, from among the remaining semantic classes C=C\\{C, . . . , C}, which is identified as C. This may continue through further iterations, identifying a subsequent competitor word sequence Wthat has a highest remaining joint association score D(C,W;X) with any of the then remaining semantic classes C=C\\{C, . . . , C}, and so forth, and may continue until there are either no word sequences left or no semantic classes left. The spoken utterance acoustic signal Xmay then be assigned to the semantic class Cthat has the highest joint association score D(C,W;X) with any word sequence Wfrom among the n-best word sequences \u03c9 ().","One result of this process is that the target word sequence Wis the one word sequence from the n-best word sequences \u03c9 () that is most likely to match with the correct, target semantic class C, while the first competitor word sequence Wis the one word sequence most likely to yield an erroneous semantic class. This mechanism of matching the word sequences with semantic classes is therefore effective at matching the closest competitor against a target word sequence for discriminative training, so that the system can be trained foremost to discern the correct word sequence in comparison with its closest competitor, i.e. the word sequence otherwise most likely to be erroneously identified.","The importance of this system for focusing discriminative training on the closest competitor is demonstrated for the present illustrative example, in Table 1 below, which represents experimentally generated joint association scores:",{"@attributes":{"id":"p-0051","num":"0050"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"119pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["C(Class)","W(Corresponding word sequence)","D"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["GRD_SRV(ATL)","what is the ground transportation in",{}]},{"entry":[{},"Atlanta"]},{"entry":["C: GRD_SRV(ATL)","W: what is the transportation","\u221220.04"]},{"entry":[{},"in Atlanta"]},{"entry":["C: FARE(ATL)","W: what is the round trip fare","\u221217.56"]},{"entry":[{},"from Atlanta"]},{"entry":["C: CITY(ATL)","W: what is the transportation","\u221225.46"]},{"entry":[{},"Atlanta"]},{"entry":["C: FLIGHT(ATL)","W: what is the transportation","\u221228.49"]},{"entry":[{},"and Atlanta"]},{"entry":["C: FARE_RT(ATL)","W: what is the round trip fare from the","\u221227.98"]},{"entry":[{},"Atlanta"]},{"entry":["C: AIR_SRV(ATL)","W: what is the transportation","\u221229.09"]},{"entry":[{},"the Atlanta"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"In this example, the joint association score of the target word sequence Wwith the correct, target semantic class C:GRD_SRV(ATL) is D(C,W;X)=\u221220.04. On the other hand, the joint association score of competitor word sequence Wwith the wrong semantic class, FARE(ATL), is D(C,W;X)=\u221217.56, higher than D(C,W;X).","In particular, the competitor word sequence Whappens to include the word \u201cround\u201d, which rhymes with \u201cground\u201d, at approximately the same position in the statement as word \u201cground\u201d in the reference statement for the target class, tending the automatic speech recognition to evaluate these two statements to be closer together than the target word sequence Wis to the reference word sequence, despite Wbeing the word sequence that actually has a matching semantic meaning. This highlights the shortcoming of traditional systems involving two isolated steps of automatic speech recognition and subsequent semantic classification of a word sequence provided to the semantic classifier without the benefit of any additional information about the source acoustic signal, and the capability of embodiments disclosed herein to resolve this issue.","Hence, the classification decision rule \u0108acting on Xinitially yields an erroneous semantic class, as indicated by the acoustic signal Xbeing matched with a different word sequence (W) than the target word sequence (W) as it was previously identified for having the highest joint association scores with the target semantic classes C. These results also provide guidance for how the parameters that went into that classification need to be revised to raise the joint association score of the target word sequence with the target semantic class, and to decrease the joint association score of the competitor word sequences with the target semantic class.","The weighting parameters for both the language model  and the semantic model  may be revised subsequent to the minimum classification error training, by applying a steepest descent optimization technique, in one illustrative embodiment. For the language model , the weighting para meters may be defined as logarithms of probabilities applied to lexical features, such as n-grams, that reduce or minimize the total loss function L(\u039b,\u039b). In particular, the language model (LM) weighting parameters, in the particular example of lexical bigrams, may be defined as p=log(P(w|w)), with a revised weighting parameter of revision iteration t+1 expressed relative to the prior form of the weighting parameter in the form of:",{"@attributes":{"id":"p-0056","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":"p","mrow":[{"msub":[{"mi":["w","x"]},{"mi":["w","y"]}],"mo":"\u2062"},{"mi":"t","mo":"+","mn":"1"}]},"mo":"=","mrow":{"msubsup":{"mi":["p","t"],"mrow":{"msub":[{"mi":["w","x"]},{"mi":["w","y"]}],"mo":"\u2062"}},"mo":"-","mrow":{"msub":{"mi":["\u025b","LM"]},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"r"},"mo":"\u2062","mfrac":{"mrow":[{"mo":"\u2202","mrow":{"msub":{"mi":["l","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}}}}},{"mo":"\u2202","msub":{"mi":"p","mrow":{"msub":[{"mi":["w","x"]},{"mi":["w","y"]}],"mo":"\u2062"}}}]}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"15"}}}]}}}}},"After applying the chain rule to the derivative expression, this equation becomes:",{"@attributes":{"id":"p-0058","num":"0057"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":"p","mrow":[{"msub":[{"mi":["w","x"]},{"mi":["w","y"]}],"mo":"\u2062"},{"mi":"t","mo":"+","mn":"1"}]},"mo":"=","mrow":{"msubsup":{"mi":["p","t"],"mrow":{"msub":[{"mi":["w","x"]},{"mi":["w","y"]}],"mo":"\u2062"}},"mo":"-","mrow":{"msub":{"mi":["\u025b","LM"]},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"r"},"mo":"\u2062","mrow":{"mrow":[{"mrow":[{"msub":{"mi":["l","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["d","r"]}}},{"mo":["[","]"],"mrow":{"mn":"1","mo":"-","mrow":{"msub":{"mi":["l","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["d","r"]}}}}}],"mo":"\u2061"},{"mo":["[","]"],"mfrac":{"mrow":[{"mo":"\u2202","mrow":{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}}},{"mo":"\u2202","msub":{"mi":"p","mrow":{"msub":[{"mi":["w","x"]},{"mi":["w","y"]}],"mo":"\u2062"}}}]}}],"mo":"\u2061"}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"16"}}}]}}}}},"This isolates the partial derivative of the class-specific misclassification function d(X) of EQ. 12, above, with respect to the language model weighting parameters P, which can be resolved as:",{"@attributes":{"id":"p-0060","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":"\u2202","mrow":{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}}},{"mo":"\u2202","msub":{"mi":"p","mrow":{"msub":[{"mi":["w","x"]},{"mi":["w","y"]}],"mo":"\u2062"}}}]},"mo":"=","mrow":{"mrow":[{"mo":"-","mrow":{"mi":"n","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["W","r"],"mn":"0"},"mo":",","mrow":{"msub":[{"mi":["w","x"]},{"mi":["w","y"]}],"mo":"\u2062"}}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"msubsup":{"mi":["H","r","n"]},"mo":"\u2062","mrow":{"mi":"n","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["W","r","n"]},"mo":",","mrow":{"msub":[{"mi":["w","x"]},{"mi":["w","y"]}],"mo":"\u2062"}}}}}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"17"}}}]}}}}},"Here, n(W,ww) denotes the number of times the bigram wwappears in the word sequence Wand n(W,ww) denotes the number of times the bigram wwappears in the word sequence W. EQ. 17 also includes weighting parameters H, that directly incorporate the information from the joint association scores D(C,W;X), as follows:",{"@attributes":{"id":"p-0062","num":"0061"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":["H","r","n"]},"mo":"=","mfrac":{"mrow":[{"mi":"exp","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":"\u03b7","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["C","r","n"]},"mo":",","mrow":{"msubsup":{"mi":["W","r","n"]},"mo":";","msub":{"mi":["X","r"]}}}}}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"m","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":"\u03b7","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["C","r","m"]},"mo":",","mrow":{"msubsup":{"mi":["W","r","m"]},"mo":";","msub":{"mi":["X","r"]}}}}}}}}}]}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"18"}}}]}}}}},"As \u03b7 is increased, and even approaches infinity, the weighting parameters that are associated with the target word sequence Wbut not competitor word sequence Ware increased while those associated with competitor word sequence Wbut not target word sequence Ware decreased. Revisions produced for bigrams common to both the target word sequence Wand the competitor word sequence Wtherefore cancel out, and those corresponding language model weighting parameters are left unchanged. Meanwhile, the language model parameters corresponding to the bigrams that are present in the target word sequence Wbut not in the competitor word sequence Ware increased. In the example of Table 1 above, these bigrams would include \u201cthe transportation\u201d, \u201ctransportation in\u201d, and \u201cin atlanta\u201d. On the other hand, the language model parameters corresponding to the bigrams that are present in the competitor word sequence Wbut not in the target word sequence Ware decreased. In the example of Table 1 above, these bigrams would include \u201cthe round\u201d, \u201cround trip\u201d, \u201ctrip fare\u201d, \u201cfare from\u201d, and \u201cfrom Atlanta\u201d. While the particular example of bigrams are used here, analogous effects could be achieved with any other type of language model weighting coefficient, including n-grams of other n or other lexical features, and either one or any number of different type of language model weighting parameters. One or more weighting parameters may therefore be revised to raise the joint association score of the target word sequence with the target semantic class relative to the joint association score of the competitor word sequence with the target semantic class, according to the present illustrative embodiment.","Analogous functions may be applied to the semantic classification weighting parameters, which may for example correspond to lexical features of the word sequence, in this illustrative embodiment. A revised semantic classifier parameter \u03bbmay be provided with reference to an initial or prior-iteration semantic classifier parameter \u03bbaccording to the equation:",{"@attributes":{"id":"p-0065","num":"0064"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":["\u03bb","k"],"mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":"=","mrow":{"msubsup":{"mi":["\u03bb","k","t"]},"mo":"-","mrow":{"msub":{"mi":["\u025b","\u03bb"]},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"r"},"mo":"\u2062","mfrac":{"mrow":[{"mo":"\u2202","mrow":{"msub":{"mi":["l","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}}}}},{"mo":"\u2202","msub":{"mi":["\u03bb","k"]}}]}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"19"}}}]}}}}},"Applying the chain rule to this equation as well, produces:",{"@attributes":{"id":"p-0067","num":"0066"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":["\u03bb","k"],"mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":"=","mrow":{"msubsup":{"mi":["\u03bb","k","t"]},"mo":"-","mrow":{"msub":{"mi":["\u025b","\u03bb"]},"mo":["\u2062","\u2062"],"mi":"\u03b1","mrow":{"munder":{"mo":"\u2211","mi":"r"},"mo":"\u2062","mrow":{"mrow":{"mrow":[{"msub":{"mi":["l","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}}}},{"mo":["[","]"],"mrow":{"mn":"1","mo":"-","mrow":{"msub":{"mi":["l","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}}}}}}],"mo":"\u2061"},"mo":"\u2062","mfrac":{"mrow":[{"mo":"\u2202","mrow":{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}}},{"mo":"\u2202","msub":{"mi":["\u03bb","k"]}}]}}}}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"20"}}}]}}}}},"Extending the analogy to the revision of the language model weighting parameters, this isolates the partial derivative of the class-specific misclassification function d(X) of EQ. 20, above, with respect to the semantic classification weighting parameters \u03bb, which can be resolved as:",{"@attributes":{"id":"p-0069","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":"\u2202","mrow":{"msub":{"mi":["d","r"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","r"]}}}},{"mo":"\u2202","msub":{"mi":["\u03bb","k"]}}]},"mo":"=","mrow":{"mrow":[{"mi":"\u03c6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["C","r"],"mn":"0"},{"mi":["W","r"],"mn":"0"}],"mo":","}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"msubsup":{"mi":["H","r","n"]},"mo":"\u2062","mrow":{"mi":"\u03c6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["C","r"],"mn":"0"},{"mi":["W","r"],"mn":"0"}],"mo":","}}}}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"21"}}}]}}}}},"Here, \u03c6 incorporates semantic model weighting parameters based on the semantic classes Cand word sequences W, as follows:",{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}},"mo":"\u2062","mrow":{"mrow":{"mi":"\u03c6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["C","r"],"mn":"0"},{"mi":["W","r"],"mn":"0"}],"mo":","}}},"mo":"=","mfrac":{"mrow":[{"mrow":[{"mo":"\u2202","mi":"log"},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["C","r","j"]},{"mi":["W","r","j"]}],"mo":"\u2758"}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mo":"\u2202","msub":{"mi":["\u03bb","k"]}}]}}}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"22"}}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"\u03c6","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["C","r"],"mn":"0"},{"mi":["W","r"],"mn":"0"}],"mo":","}}},{"mrow":[{"msub":{"mi":["f","k"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["C","r","j"]},{"mi":["W","r","j"]}],"mo":"\u2758"}}},{"munder":{"mo":"\u2211","mover":{"mi":"C","mo":"~"}},"mo":"\u2062","mrow":{"mfrac":{"mrow":[{"mo":["(",")"],"mrow":{"mi":"exp","mo":["[","]"],"mrow":{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"\u2062","mrow":{"msub":{"mi":["f","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":{"mi":"C","mo":"~"},"mo":",","msubsup":{"mi":["W","r","j"]}}}}}}}},{"munder":{"mo":"\u2211","mover":{"mi":"C","mo":"~"}},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mi":"exp","mo":["[","]"],"mrow":{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","mrow":{"msub":{"mi":["\u03bb","i"]},"mo":"\u2062","mrow":{"msub":{"mi":["f","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mover":{"mi":"C","mo":"~"},"mo":",","msubsup":{"mi":["W","r","j"]}}}}}}}}}]},"mo":"\u2062","mrow":{"msub":{"mi":["f","k"]},"mo":["(",")"],"mrow":{"mover":{"mi":"C","mo":"~"},"mo":"\u2758","msubsup":{"mi":["W","r","j"]}}}}}],"mo":"-"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"23"}}}]}]}}}},"Again in analogy to the language model weighting parameters, this enables the semantic classification weighting parameters associated with the target semantic class Cbut not with the competitor semantic class Care increased, while those associated with competitor semantic class Cbut not target semantic class Care decreased. Once again, therefore, one or more weighting parameters may be revised to raise the joint association score of the target word sequence with the target semantic class relative to the joint association score of the competitor word sequence with the target semantic class, according to the present illustrative embodiment.","A semantic understanding system may therefore be provided for an application that includes speech recognition and semantic classification systems that have been trained by systems analogous to those described above. Such an application, such as a voice user interface, for example, may be configured to receive a speech input, match the speech input to a semantic class and to a word sequence corresponding to the semantic class, and provide the semantic classes matched to the speech input to an application configured to provide user output that is dependent on the semantic classes matched to the speech input, in an illustrative embodiment. The speech input may be matched to a semantic class and to a corresponding word sequence by applying a semantic classification tool to the speech input, wherein the semantic classification rule selects for the highest joint association score between a semantic class and a word sequence for the speech input. The joint association score may include parameters of a language model and parameters of a semantic classification model that have been iteratively trained to reduce a total loss function indicative of a rate of error in semantically classifying speech inputs due to errors in either the language model or the semantic classification model, in this illustrative embodiment.","The example of an Airline Travel Information System (ATIS) was referred to with reference to the training process, and is one example of an application in which an embodiment of a trained speech recognizer and semantic classifier may illustratively be embodied. That is only one of many possible examples, and any system for voice input and voice user interface, among other potential examples, may also incorporate various embodiments that may be discerned by those skilled in the art based on the disclosure herein.",{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 3","b":["300","300","300"]},"Computing system environment  as depicted in  is only one example of a suitable computing environment for implementing various embodiments, and is not intended to suggest any limitation as to the scope of use or functionality of the claimed subject matter. Neither should the computing environment  be interpreted as having any dependency or requirement relating to any one or combination of components illustrated in the exemplary operating environment .","Embodiments are operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well-known computing systems, environments, and\/or configurations that may be suitable for use with various embodiments include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, telephony systems, distributed computing environments that include any of the above systems or devices, and the like.","Embodiments may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. Some embodiments are designed to be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules are located in both local and remote computer storage media including memory storage devices. As described herein, such executable instructions may be stored on a medium such that they are capable of being read and executed by one or more components of a computing system, thereby configuring the computing system with new capabilities.","With reference to , an exemplary system for implementing some embodiments includes a general-purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit . The system bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way of example and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard , a microphone , and a pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  is operated in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.",{"@attributes":{"id":"p-0087","num":"0086"},"figref":["FIG. 4","FIG. 4"],"b":["400","401","401","402","404","406","408","410"]},"Memory  is implemented as non-volatile electronic memory such as random access memory (RAM) with a battery back-up module (not shown) such that information stored in memory  is not lost when the general power to mobile device  is shut down. A portion of memory  is illustratively allocated as addressable memory for program execution, while another portion of memory  is illustratively used for storage, such as to simulate storage on a disk drive.","Memory  includes an operating system , application programs  as well as an object store . During operation, operating system  is illustratively executed by processor  from memory . Operating system , in one illustrative embodiment, is a WINDOWS\u00ae CE brand operating system commercially available from Microsoft Corporation. Operating system  is illustratively designed for mobile devices, and implements database features that can be utilized by applications  through a set of exposed application programming interfaces and methods. The objects in object store  are maintained by applications  and operating system , at least partially in response to calls to the exposed application programming interfaces and methods.","Communication interface  represents numerous devices and technologies that allow mobile device  to send and receive information. The devices include wired and wireless modems, satellite receivers and broadcast tuners to name a few. Mobile device  can also be directly connected to a computer to exchange data therewith. In such cases, communication interface  can be an infrared transceiver or a serial or parallel communication connection, all of which are capable of transmitting streaming information.","Input\/output components  include a variety of input devices such as a touch-sensitive screen, buttons, rollers, and a microphone as well as a variety of output devices including an audio generator, a vibrating device, and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition, other input\/output devices may be attached to or found with mobile device .","Mobile computing system  also includes network . Mobile computing device  is illustratively in wireless communication with network \u2014which may be the Internet, a wide area network, or a local area network, for example\u2014by sending and receiving electromagnetic signals  of a suitable protocol between communication interface  and wireless interface . Wireless interface  may be a wireless hub or cellular antenna, for example, or any other signal interface. Wireless interface  in turn provides access via network  to a wide array of additional computing resources, illustratively represented by computing resources  and . Naturally, any number of computing devices in any locations may be in communicative connection with network . Computing device  is enabled to make use of executable instructions stored on the media of memory component , such as executable instructions that enable computing device  to implement various functions of discriminative training for integrated speech recognition and semantic classification, in an illustrative embodiment.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims. As a particular example, while the terms \u201ccomputer\u201d, \u201ccomputing device\u201d, or \u201ccomputing system\u201d may herein sometimes be used alone for convenience, it is well understood that each of these could refer to any computing device, computing system, computing environment, mobile device, or other information processing component or context, and is not limited to any individual interpretation. As another particular example, while many embodiments are presented with illustrative elements that are widely familiar at the time of filing the patent application, it is envisioned that many new innovations in computing technology will affect elements of different embodiments, in such aspects as user interfaces, user input methods, computing environments, and computing methods, and that the elements defined by the claims may be embodied according to these and other innovative advances while still remaining consistent with and encompassed by the elements defined by the claims herein."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 4"}]},"DETDESC":[{},{}]}
