---
title: Method for controlling group membership in a distributed multinode data processing system to assure mutually symmetric liveness status indications
abstract: In distributed multinode data processing systems, mechanisms are employed to insure that the nodes are continually informed about the liveness of the other nodes within node groups in the network and across networks. The method of the present invention employs the concept of node stability which it uses to provide indications of membership in a previous stable group as part of messages transmitted in a protocol for forming, maintaining and dissolving node groupings in such a way as to assure that all of the nodes in the group have a consistent indication of liveness status for all of the nodes within any given group of nodes.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07043550&OS=07043550&RS=07043550
owner: International Business Machines Corporation
number: 07043550
owner_city: Armonk
owner_country: US
publication_date: 20020215
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION","1.0 Controlling Group Membership","2.0 Heartbeat Protocols","2.1 Join Protocol","2.2 Death Protocol","2.3 Node Reachability","2.4\u2014Stable\/Unstable AMGs","3.0 Node Event Inconsistency","3.1\u2014Inconsistency Caused by Quick Restart","3.2\u2014Inconsistency Caused by Quick Communication Interruption","4.0 Detection of Bouncing Nodes","4.1 Live Nodes Detect Bouncing Nodes","4.2\u2014Bounced Nodes Detect That They Bounced","5.0 Solution to the Quick Communication Interruption Problem","6.0\u2014Sample Scenarios","6.1\u2014Detection of Bouncing Nodes","(a) Single Bounce","(b) Multiple Bounces","6.2\u2014Quick Communication Interruption Problem","EXAMPLE 1 (IN 3.2)","EXAMPLE 2","Group Merge","Real Group Dissolve","Daemon is Blocked","7.0\u2014Discussion in Terms of Related Drawings"],"p":["The present invention is generally directed to insuring the continuation of consistent group formation events in a distributed topology liveness system, that is, in a multinode data processing system in which node and\/or adapter liveness is communicated throughout the system via heartbeat messages, which are messages that are sent periodically and which indicate node and\/or adapter liveness. More particularly, the present invention is directed to a method for detecting a situation in which a liveness daemon running on one of the nodes has been subject to a rapid restart. Even more particularly, the present invention is directed to a method for determining the existence of such quick restart events and for providing a proper indication thereof to other nodes within the network, with the particular objective of avoiding grouping inconsistencies which are situations in which one node set sees another node set fail in some way without the other node set being aware of the fact that the first node set has also failed. In short, all of the nodes within a node set should have the same view as to the operating status of the other nodes in the node set.","A proper understanding of the present invention is best obtained from an appreciation of the environment in which it is intended to operate. The present invention is employed in multinode data processing systems. These systems include a plurality of nodes each of which incorporates a data processing element which is coupled locally to its own memory system which typically includes both a volatile random access memory and a nonvolatile random access memory. The volatile memory typically comprises an array of semiconductor memory chips. The nonvolatile memory typically comprises a rotating magnetic or optical storage device. The data processing element also typically comprises a central processing unit (CPU). Each node includes one or more data processing elements. The nodes also include adapters which are communications devices which permit messages to be sent from one node to another node or to a plurality of other nodes. Internodal communications typically take place through a switch device which routes transmitted messages to destination nodes within the system.","In order to carry out various data processing functions, the nodes within any given multinode network are organizable into sets of nodes. Nodes and\/or their associated adapters sometimes experience problems, delays or failures. Accordingly, from time to time during the operation of individual nodes, system checks are undertaken to make sure that the nodes are still alive and functioning. This checking is performed via heartbeat message transmissions. Each node in the system is assigned one or more \u201cdownstream\u201d nodes for the purpose of periodically sending a message indicating liveness status. In preferred embodiments, heartbeat signals are only sent to a single other node. However, it is quite easy to instead employ a predefined list of node destinations for receipt of heartbeat signals from any or all of the nodes in the network. These liveness message transmissions are handled by daemon programs running on the various nodes in the system.","Distributed multinode data processing systems of the kind contemplated herein employ heartbeat messaging protocols which are used to control group membership which, of course, shifts over time. It is control of the membership process to which the present invention is directed. This membership process typically includes the establishment of one of the nodes in a group as the so-called Group Leader (GL). The Group Leader acts as a coordinator for nodes coming into (joining) or for nodes exiting the group. Additionally, in the event that there is a problem with the Group Leader, there is preferably also a designated second node which is intended to act as a replacement for the Group Leader in the event that the Group Leader experiences a failure. This second, backup Group Leader is referred to as the Crown Prince. In the context of the present invention, the Group Leader and Crown Prince are employed in the \u201cliveness\u201d (heartbeating) layer. The present invention should not be confused with group membership services which are provided to \u201cend user applications.\u201d In accordance with the present invention, \u201cgroup membership,\u201d as referred to above, refers to the list of members in an Adapter Membership Group which occurs on each network being monitored. On the other hand, \u201cnode reachability\u201d refers to the set of nodes that are considered to be alive, taking all of the adapter membership groups into consideration. In particular, it is noted that the notion of \u201cnode reachability\u201d may include message hops through indirect paths that may cross network boundaries. This set of nodes is supplied from the \u201cliveness layer\u201d to the \u201cgroup communications layer\u201d which runs on top of the \u201cliveness\u201d layer.","More particularly, the present application is concerned with two different scenarios which present potential problems with respect to group membership consistency across the nodes of the system or network. Accordingly, there is provided a method for determination of adapter and node death in a distributed system whereby node events are made consistent, that is, when a first node sees another node as being \u201cdown,\u201d the second other node, if alive, is still able to see the first node as being \u201cdown\u201d within finite amount of time. When a node actually suffers a \u201cpermanent\u201d crash the heartbeat mechanism, together with the associated \u201cjoin\u201d protocol, is able to provide sufficient control and communications amongst the remaining nodes to assure maximum functionality. Accordingly, the present invention does not come into play when nodes crash, since the basic heartbeat mechanism is able to cope with this situation; nonetheless, the present invention becomes important when communication failures and process blockages result in temporary loss of contact amongst a set of distributed peers in the liveness determination subsystem. The present method addresses two possible scenarios which could lead to inconsistent node grouping situations: (1) a node where the liveness daemon is stopped and restarted quickly; and (2) a node whose communications with the rest of the nodes suffers a temporary interruption.","In situations in which the liveness daemon running on one of the nodes is stopped and restarted in a short period of time, certain consistency problems can be engendered. For example, typically it happens that when the liveness daemon restarts, for each local adapter, a message is transmitted which \u201cproclaims\u201d the existence and the willingness of the sending node to become a group leader; it is, in generic terms, a request to know which other nodes are \u201cout there. \u201d These aspects are discussed in more detail below where the nature of the \u201cPROCLAIM\u201d message is considered. However, the other nodes in the group still consider the restarting node (and\/or adapter) as being part of the previous group. Accordingly, group membership is no longer consistent in the sense that there is a lack of symmetry among the various nodes with regards to the \u201cknown\u201d status of the other nodes. When this situation is caused by the \u201cquick\u201d restart of the liveness daemon, it is referred to herein as the \u201cbouncing node\u201d problem or scenario.","Likewise, a problem can occur if a first node, say Node , has a temporary communication problem. If the problem lasts long enough for the other nodes to expel Node  from the group, but not long enough for the local adapter to be declared down, the other nodes can form a new Adapter Membership Group, G, while the adapter at Node  is still considered as being part of the previous group, G (which contains all the adapters). The adapter at Node  then attempts to dissolve the group, since it will have gotten no answer to a liveness (\u201cDEATH\u201d) message that it sent when its old upstream neighbor stopped sending heartbeat signals to it. (For a discussion of a more specific and preferred characterization of the notion of dissolving a group, attention is directed below to Section 2.2). Upon \u201cdissolving\u201d the group, the adapter at Node  reinitializes into a \u201cgroup\u201d with only a single node, which is referred to herein as a singleton group and it resumes operation. Singleton groups are inherently unstable groups since they are typically destined to soon experience a change to inclusion in a larger group. If this all happens before the adapter on Node  is able to form a stable group, then Node  never sees any \u201cnode down\u201d events, where the other nodes see Node  as being \u201cdown,\u201d especially if this is the only adapter group to which Node  belongs. Accordingly, the recognition of this problem brings along with it the notion that some groups are more stable (from time to time) than other groups, and that special handling is required to insure group membership consistency across the network.","In accordance with a preferred embodiment of a first aspect of the present invention, there is provided a method for detecting the quick restart of liveness daemons in a distributed, multinode data processing system in which the nodes communicate liveness indicia in the form of heartbeat messages via adapters coupled to each node. In this method a first message (PROCLAIM) is sent from a first node to other nodes in the network that do not yet belong to the local node's adapter membership group. This message contains some indicia that the sending node has recently experienced an adapter restart. This information, together with locally stored group membership information, is used to determine that a quick restart has actually occurred at the sending node. This situation is handled by expelling the node from the group as a means for insuring correct group membership status.","In accordance with a preferred embodiment of a second aspect of the present invention, there is provided a method for detecting node reachability inconsistencies in the presence of temporary node communication failures or temporary daemon blockage. To accomplish this, an indication of a last stable adapter membership group is maintained at each node. The group join protocol is thus enabled to provide a PREPARE_TO_COMMIT (PTC) message which includes a flag which indicates that the message recipient is considered as belonging to the same stable membership group as the message sender. As used herein, the term \u201cstable\u201d refers to a characteristic for a node or node group which implies that there is only a small likelihood that group membership for that node or node group will change in the near future. In particular, nodes that find themselves isolated as the only members of a group try to join a group as soon as possible. Therefore, such singleton groups are referred to herein as being inherently unstable.","Accordingly, it is an object of the present invention to insure the existence of consistent group membership information across a plurality of nodes in a distributed, multinode data processing system.","It is also an object of the present invention to provide a mechanism to guard against group membership inconsistencies which might arise as the result of the failure and quick restart of a node and\/or one of more of its associated adapters.","It is a further object of the present invention to employ existing group membership control protocols as a mechanism for communicating proper group membership status.","It is yet another object of the present invention to insure proper group membership status in the face of temporary node communication failures.","It is a still further object of the present invention to insure proper group membership status in the face of temporary node daemon blockages.","It is also an object of the present invention to provide an indication that a node is included within a stable group.","It is yet another object of the present invention to provide an indication of stability for the nodes in a data processing network.","It is also an object of the present invention to increase the reliability and availability of distributed data processing systems.","It is also an object of the present invention to expand the capabilities of Topology Services in terms of its utility without significantly altering any of its application programming interfaces (APIs) or its protocols.","Lastly, but not limited hereto, it is an object of the present invention to provide a cooperative relation between the first and second aspects of the present invention (quick restart and failed communications, respectively) to particularly address the problem of assuring consistent node viewpoints with respect to adapter group membership and node reachability.","The recitation herein of a list of desirable objects which are met by various embodiments of the present invention is not meant to imply or suggest that any or all of these objects are present as essential features, either individually or collectively, in the most general embodiment of the present invention or in any of its more specific embodiments.","Adapter and node liveness determination lies at the heart of any highly available distributed cluster data processing system. In order to provide high availability services, a cluster system should be able to determine which nodes, networks, and network adapters in the system are working and be able to accurately determine the group or groups to which they belong. The failure in any such component should be detected as soon as possible and indications of such failure should be passed along to higher level software subsystems for recovery processing by the cluster software and\/or applications running on the cluster.","Determination of node, network, and network adapter liveness is often made through the use of daemon processes running on each node of the distributed system. Daemons run distributed protocols and exchange liveness messages that are forced through different network paths in the data processing system. If no such liveness messages are received within a predetermined time interval, then the sending node or network adapter is assumed to be not working (\u201cdead\u201d) by the other nodes.","Any method of liveness determination can subjected to \u201cfalse down\u201d events, where nodes or network adapters are incorrectly notified as being down or unreachable. Such false events may happen, for example, when temporary communication failures prevent the liveness messages from reaching their destination(s). False \u201cnode down\u201d events may also happen when the liveness determination daemon is prevented from being scheduled because of CPU scheduling, memory contention, excessive interrupts, and other factors. The daemon being stopped is yet another source of false \u201cnode down\u201d notifications.","In the presence of these false events, it is important to provide consistent node reachability notifications: when a node sees the other node as down, the other node\u2014if alive\u2014should see the first as down within a finite, preferably predetermined, time interval. The absence of such consistency may lead to undesirable effects, since software layers above the liveness determination \u201clayer\u201d may be unable to reach an agreement regarding the topology's health (that is, the configuration of nodes within a group together with identifiable paths by which one node may be reached from another), with different nodes having different views of which nodes are reachable.","To explain the mechanisms of the present invention, and how they are employed in Topology Services (a set of system utility programs and defined API calling structures), adapter membership (\u201cheartbeating\u201d) protocols in the subsystem are explained herein in some detail. Topology Services is the layer in the infrastructure which is responsible for detecting the health of adapters, nodes, and networks.","In order to monitor the health and connectivity of the adapters in each network, all adapters in the network attempt to form at least one \u201cAdapter Membership Group\u201d (AMG), which is a group containing all network adapters within the network that can communicate with each other. Adapters in an AMG monitor the \u201cliveness\u201d of each other. When an AMG is formed, all group members receive an \u201cAMG id\u201d (that is, a unique group membership identifier) which identifies the AMG. If, at some point in time an adapter fails, it is expelled from the group, and new adapters that are powered up are invited to join the group. In both cases, a new AMG with a new \u201cAMG id\u201d is formed. Each AMG has one member that is the Group Leader (GL), and all members know who the Group Leader is. Note that a node may belong to several AMGs, one for each of its (network) adapters.","Each AMG has an id, which is included in all protocol messages. The group id includes the GL identification (chosen to be its Internet Protocol (IP) address) and an instance number (chosen to be the time stamp of which indicates when the AMG was formed). Note that the group id is chosen to be the IP address for convenience and that any scheme for assigning a unique and sequentially orderable identifier may be employed.","Each member of an AMG also has an id, which includes the member identification (chosen to be its IP address) and an instance number (chosen to be the time stamp of when its daemon was started or when its adapter was reinitialized).","To determine the set of adapters that are alive in each network, an adapter membership protocol is run in each of the networks. Messages in this protocol are sent using UDP\/IP (\u201cUser Datagram Protocol\u201d\/\u201cInternet Protocol\u201d).","Adapters that are alive form an AMG, where members are organized in a virtual ring topology. To ensure that all group members are alive, each member periodically sends \u201cHEART BEAT\u201d messages to its \u201cdownstream neighbor\u201d and monitors \u201cHEART BEAT\u201d messages from its \u201cupstream neighbor.\u201d Protocols are run when adapters fail or when new adapters become functional. The goal of such protocols is to guarantee that the membership group contains at each moment all (and only) the adapters in the network (but only those belonging to the cluster) that can communicate with each other.","Besides the Group Leader, each group has a \u201cCrown Prince\u201d (backup group leader). The group leader is responsible for coordinating the group protocols, and the Crown Prince is responsible for taking over group leadership if the group leader, or its adapter, fails. Both the choice of Group Leader and Crown Prince, and the position of the adapters in the ring, are determined by a predefined adapter priority rule, which is typically chosen to be the adapters' IP address, hence the desire, as stated above, that its indicia be able to provide a sort into a unique ordering sequence.","A list of all possible adapters in each network is contained in a configuration file that is read by all of the nodes at startup and at reconfiguration time.","In order to attract new members to the group, the Group Leader in each group periodically sends \u201cPROCLAIM\u201d messages to adapters that are in the adapter configuration but do not currently belong to the group. The message is only sent to adapters having a lower IP address than that of the sender. See  and the discussion in Section 7 below.","The \u201cPROCLAIM\u201d messages are ignored by all adapters that are not group leaders. A Group Leader node receiving a \u201cPROCLAIM\u201d message from a higher priority (higher IP address) node responds with a \u201cJOIN\u201d message on behalf of its group. The message contains the membership list of the \u201cjoining group.\u201d","A node GL (Group Leader #) receiving a \u201cJOIN\u201d message from GL (Group Leader #) attempts to form a new group containing the previous members plus all members in the joining group. GL then sends a \u201cPTC\u201d (\u201cPrepare To Commit\u201d) message to all members of the new group, including GL.","Nodes receiving a \u201cPTC\u201d message reply with a \u201cPTC_ACK\u201d message. All nodes from which a \u201cPTC_ACK\u201d message is received are included in the new group. The group leader (GL) sends a \u201cCOMMIT\u201d message, which contains the entire group membership list, to all new group members.","Receiving a \u201cCOMMIT\u201d message marks the transition to the new group, which now contains the old members plus the joining members. After receiving this message, a group member starts sending \u201cHEART BEAT\u201d messages to its (possibly new) downstream neighbor, and starts monitoring \u201cHEART BEAT\u201d messages from its (possibly new) upstream neighbor.","Both \u201cPTC\u201d and \u201cCOMMIT\u201d messages require an acknowledgment to ensure they have been received. If no acknowledgment is received then a finite number of retries is made. Failure to respond to a \u201cPTC\u201d message\u2014after all retries have been exhausted\u2014results in the corresponding adapter not being included in the new group. If a liveness daemon fails to receive a \u201cCOMMIT\u201d message after all retries of the \u201cPTC_ACK\u201d message, then the local adapter gives up the formation of the new group and reinitializes itself into a singleton group. This phenomenon should only occur in the relatively rare case where the Group Leader fails in the short time window between sending the \u201cPTC\u201d message and the \u201cCOMMIT\u201d message.","When the Topology Services daemon is initialized, it forms a singleton adapter group (of which the node is the Group Leader) in each of its adapters. The node then starts sending and receiving \u201cPROCLAIM\u201d messages.","A node or adapter monitors \u201cHEART BEAT\u201d messages coming from its \u201cupstream neighbor\u201d (the adapter in the group that has the next highest IP address among the group members). When no \u201cHEART BEAT\u201d messages are received for some predefined period of time, the \u201cupstream neighbor\u201d is assumed to have failed. A \u201cDEATH\u201d message is then sent to the group leader, requesting that a new group be formed. See  and the discussion in Section 7 below.","Upon receiving a \u201cDEATH\u201d message, the group leader attempts to form a new group containing all adapters in the current group except the adapter that was detected as failed. The group leader sends a \u201cPTC\u201d message to all members of the new group. The protocol then follows the same sequence as that described above for the Join protocol.","After sending a \u201cDEATH\u201d message, the daemon expects to receive a \u201cPTC\u201d message shortly. A number of retries is attempted, but if no \u201cPTC\u201d message is received then the interpretation is that the GL adapter (or its hosting node) died and that the \u201cCrown Prince\u201d adapter also died, and therefore was unable to take over the group leadership. In this case the adapter reinitializes itself into a singleton group and also sends a \u201cDISSOLVE\u201d message, inviting all group members to do the same. This is the mechanism that allows all members of the group to find out about the simultaneous demise of the Group Leader and Crown Prince member nodes.","A node reachability protocol is used to allow computation of the set of nodes that are reachable from the local node (and therefore considered alive). Since not all nodes may be connected to the same network, some nodes may be reachable only through a sequence of multiple network hops. Complete node reachability determinations can only be computed when information about all networks, even those that do not span all nodes, is taken into account.","To compute node reachability, an eventual agreement protocol is used: reachability information at each network is propagated to all networks; when the network topology stops changing, eventually all nodes have consistent information about all networks. Each node is then be able to compute the set of reachable nodes independently and arrive at a consistent result.","Periodically, and until some stopping criteria instruct the daemon to stop doing so, the nodes send the following messages:\n\n","To prevent \u201cpanic\u201d actions of the protocol\u2014such as those caused by the absence of a \u201cCOMMIT\u201d after all \u201cPTC ACKs\u201d or by the simultaneous failure of the Group Leader and the Crown Prince\u2014from causing major node reachability ripples, the concept of \u201cstable\u201d and \u201cunstable\u201d AMGs is now defined. Stable AMGs are those where steady state operations are occurring, while unstable AMGs are those where membership changes are still likely to occur (such as for singleton groups).","At initialization, singleton Adapter Membership Groups start in the unstable state, since it is expected that the adapter will join other peers in bigger groups. The change into a stable group occurs after an inactivity period where membership changes stop occurring. Once a group is stable, it remains stable until the adapter is forced to reinitialize itself because of a \u201cpanic\u201d action.","AMG stability is tied to the Node Connectivity Table and to sending the Node Connectivity Message and the Group Connectivity Message in the following way: to prevent unnecessary node reachability ripples, no information about unstable groups is sent in NCMs and GCMs. This effectively removes unstable groups from the computation of node reachability, and has the desirable effect of eliminating the knowledge of some temporary membership changes from the software layers above.","A liveness daemon which stops (on request or due to a problem) and is then quickly restarted offers a chance for inconsistency to occur. In order to best appreciate this phenomenon, consider the following sequence of events which occurs when a daemon is stopped and then quickly restarted:\n\n","(1) reintegration of the \u201cbouncing\u201d node is seen as occurring too slowly; and","(2) if different networks have very different detection times, it is possible that Node A may be detected as being down and thereafter rejoins one of the groups before being ever detected as down in another network (which has a longer detection time). The net result is that, when node reachability is computed by the other nodes, Node A is never seen as going down at all.","The problem with the scenario in (2) above is that the daemon that restarted starts anew, with no memory of the previous AMG. If other nodes never detect that the node \u201cfailed,\u201d then they cannot take actions to integrate the node into the higher level node group.","Some node event inconsistency problems are possible because of the inherent behavior of the base adapter membership protocols. The following are two examples of scenarios that could lead to inconsistent events.","(1) Node  has a temporary problem in its adapter. The problems lasts long enough for the other nodes to expel Node  from the group, but not long enough for the local adapter to be declared down. While the other nodes form a new AMG G, the adapter at Node  initially considers itself still as part of the previous G (which is assumed in this example to contain all of the adapters). The adapter at Node  then attempts to dissolve the group, since it got no answer to its \u201cDEATH\u201d message that it sent when its old upstream neighbor stopped sending heartbeat messages to it. Upon \u201cdissolving\u201d the group, the adapter at Node  then reinitializes into a singleton unstable group and resumes operation. If the adapter is working again, \u201cPROCLAIM\u201d messages eventually arrive, and the adapter is brought back into the group. If this all happens before the adapter on Node  can form a stable group, then Node  never sees any node down events, whereas the other nodes will have seen Node  as down if this is the only adapter group to which Node  belongs.","(2) This next example is similar to the one above, but this time it is assumed that Node  used to be the Group Leader. During the temporary outage, other adapters in the AMG form group G and expel the adapter at Node . Node  only perceives that it was expelled from the group when the heartbeats from its upstream neighbor stop coming. At some point, Node  declares the upstream neighbor dead and simply sends a \u201cPTC\u201d message to its old group membership. The other nodes, upon seeing the \u201cPTC\u201d message from an adapter with higher priority, immediately respond to the \u201cPTC\u201d message, and a new group G is formed. While the other nodes will have seen Node  failing and then coming back, Node  does not actually see the others failing (except possibly for its old upstream neighbor). Node  is completely oblivious to being expelled from the adapter group.","The detection of \u201cbouncing nodes\u201d (that is, nodes where the liveness daemon exits for any reason and is then restarted within a short period of time) is based on the bouncing nodes and the live nodes finding about the bounce by using normal liveness protocol messages.","One way by which the current nodes in the group can detect bounced members is by receiving \u201cPROCLAIM\u201d messages from them. The \u201cPROCLAIM\u201d message can indeed reveal that the source of the message is a bounced entity by determining that all three of the conditions indicated below exist:\n\n","If a \u201cPROCLAIM\u201d message is received where all three of the conditions listed above are true, then the assessment is that the message came from a group member that bounced. To speed up the detection of the bounce and to allow faster reintegration of the bouncer, the best course of action is to expel it from the group, which can be done by sending a \u201cDEATH\u201d message for the bouncing adapter.","Since the \u201cPROCLAIM\u201d message is likely to reach all group members, then all of them would try to send a \u201cDEATH\u201d message for the bounced adapter, which is wasteful. The alternative is for only the bouncer's downstream neighbor to send the message. Accordingly, such a process is indicated in the pseudo-code provided below:",{"@attributes":{"id":"p-0097","num":"0106"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Handling a \u201cPROCLAIM\u201d message"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"if ( from_group != from my group\u2002id\u2003&&"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"from_IP_address is still part of my group\u2003\u2002&&"]},{"entry":[{},"from_Instance != from the id that is in the group\u2002&&"]},{"entry":[{},"I am the id's downstream neighbor ) {"]},{"entry":[{},"send a \u201cDEATH\u201d message for id"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"One additional method for detecting a bounced daemon includes a step wherein a bounced daemon sends a \u201cJOIN\u201d message even before the Group Leader is notified about the demise of the adapter.","Though \u201cPROCLAIM\u201d messages are usually sent only to adapters which are not currently part of the AMG, implementations of the present protocol may, if desired, use a hardware broadcast to send the message, in which case even members of the AMG may receive the message. In this case, the Group Leader receives a \u201cJOIN\u201d message from an adapter which is still member of the group. This situation can be recognized as a \u201cbounce\u201d by the GL, which then invokes the PTC-COMMIT sequence to expel the adapter from the group.","Normally, a daemon that bounces starts anew with no memory of a previous instance. On the other hand, a bounced daemon that used to be part of a group is likely to receive \u201cHEART BEAT\u201d messages from its old \u201cupstream neighbor.\u201d Such \u201cHEART BEAT\u201d messages tell the bouncing daemon that it bounced quicker than the time it takes to detect a remote adapter as dead.","Again, the goal is to cause the bouncing adapter to be expelled from the previous group as soon as possible. The first thought which occurs as a method for accomplishing this goal is for the daemon that receives such a \u201cHEART BEAT\u201d message to send a \u201cDEATH\u201d message for itself, but this does not work because the bouncing daemon does not know who the Group Leader is, and therefore does not know to whom to send the \u201cDEATH\u201d message. In addition, the Group Leader may have itself been the recipient of the message (that is, the node that bounced). The solution to this problem is for the bouncing daemon to send a new \u201cNOT YOUR NEIGHBOR\u201d message back to the sender of the \u201cHEART BEAT\u201d message. The recipient of this message, being part of the previous group and knowing who the Group Leader is, reacts by sending a \u201cDEATH\u201d message to the Group Leader. Accordingly, such a process is indicated in the pseudo-code provided below:",{"@attributes":{"id":"p-0102","num":"0111"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"Receiving a \u201cHEART BEAT\u201d message which is not to the current group:"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"if ( I am part of a singleton group ) {"]},{"entry":[{},"Reply with \u201cNOT YOUR NEIGHBOR\u201d message"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\u2003}"},{"entry":"Receiving a \u201cNOT YOUR NEIGHBOR\u201d message:"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"if (sender's IP address is part of my group ) {"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Find id corresponding to the IP address"]},{"entry":[{},"Send a \u201cDEATH\u201d message to GL for the id"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"The steps described below are carried out to address the two situations cases described above in section 3.2. The object of performing these steps is to force both sides of a merging group to see roughly the same events prior to the merge.","(1) Each node keeps, for each local adapter, a copy of the last stable AMG to which the local adapter belongs (\u201clast_stable_group\u201d). The rationale for keeping only the stable groups is that only stable groups result in the desire for node reachability to be recomputed.","(2) When sending a \u201cPTC\u201d message, the sender adds an \u201cin_previous_stable_group\u201d flag to the message, according to whether the destination belonged to the last_stable_group AMG\u2014usually the previously formed AMG prior to the new group being committed.","(3) When processing a \u201cPTC\u201d message, a node handles two pieces of information: (1) the \u201cin_previous_stable_group\u201d in the message; and (2) whether the sender of the message belongs to the receiver's \u201clast_stable_group\u201d group. Unless these 2 pieces have the same TRUE\/FALSE value, the \u201cPTC\u201d message is rejected.","The mechanism above withstands both examples in section 3.2 above and is also effective in more normal cases, such as when two AMGs merge in a PROCLAIM-JOIN-PT C sequence. See section 6.2 below.","Some scenarios are presented to depict how the protocols presented herein work to effect their various purposes. In the case of multiple bouncing two of the separate protocols described herein work together in a cooperative fashion to further assure consistent group membership.","After a daemon \u201cbounces,\u201d either of the bouncing detection mechanisms should be activated, in any case resulting in the bouncing \u201cadapter\u201d being removed from the group. Since this happens in all AMGs more or less simultaneously, the node is effectively detected as dead by the others before it can rejoin the AMGs.","This example considers the case wherein there are multiple bouncing adapters, that is, when a number of nodes bounce, while others fail. The concern here is what happens when a bouncing daemon quickly joins in a group with some other adapter whose daemon also bounced. Not being the Group Leader of the group, the daemon does not send any \u201cPROCLAIM\u201d messages. In addition, when multiple nodes bounce it may happen that a bouncing daemon's upstream neighbor has also bounced. Therefore the usual mechanisms are not active in causing the bouncing adapter to be expelled.","This situation is salvaged by the methods of the present invention through the observation that at least one of the bouncing daemons becomes the Group Leader in its new group; the others might become members of this very same group. The Group Leader sends \u201cPROCLAIM\u201d messages periodically, resulting in a \u201cDEATH\u201d message being sent for it. The Group Leader of the original group then attempts to form a new group, but then none of the bouncing daemons should reply to the \u201cPTC\u201d message. This happens because the quick communication interruption mechanism described in section 5.0 above comes into play: the bouncing daemon is still part of the Group Leader's group, while the Group Leader itself is not be part of the bouncing node's (possibly singleton) group. The result is that the \u201cPTC\u201d message is ignored.","In this example, the sender of the \u201cPTC\u201d message has G (Group ) indicated as the last_stable_group. Since the destination (\u201cNode \u201d) does not belong to G, the \u201cin_previous_stable_group\u201d indication in the \u201cPTC\u201d message is set to \u201cFALSE.\u201d Upon receiving the message, Node  first sees the value of in_previous_stable_group: FALSE. It then examines whether the sender belongs to last_stable group. For Node , last_stable_group is G (Group ), and the sender does belong to it (when Node  dissolves the group, it forms a singleton group, but it is an unstable one.) Node  therefore sees that the two pieces of information are inconsistent. Therefore, Node  rejects the message. Node  keeps rejecting the \u201cPTC\u201d message until the stability timer expires (typically after about 10 seconds) and Node  becomes stable. At this point, Node  produces a new last_stable_group indication which does not contain the sender of the \u201cPTC.\u201d Consequently, the next \u201cPTC\u201d is accepted, since the two pieces of information are consistent. When Node  forms a stable singleton group, it sends a node notification saying that all of the other nodes disappeared. And that is the goal: the notification is symmetric to that seen in the other nodes.","In this example, Node  has G (Group ) designated as the last_stable_group. The other nodes all have G (Group ) designated as the last_stable_group. All of the \u201cPTC\u201d messages have TRUE as an indicator for being \u201cin_previous_stable_group,\u201d since all of the recipients belonged to G. On the other hand, the sender of \u201cPTC\u201d (Node ) does not belong to G (the recipients' last_stable_group), so again there is an inconsistency, and the \u201cPTC\u201d is rejected. The same thing happens again until Node  forms a singleton stable group.","Suppose AMG Group  (G) has nodes  and , while Group  (G) has Nodes  and . Node , which is assumed to be G's Group Leader sends a \u201cJOIN\u201d message to Node , which is G's Group Leader. Node  then sends a \u201cPTC\u201d message to Nodes , , and . For Node , the in_previous_stable_group indicator is \u201cFALSE,\u201d since Node  does not belong to G. Node  itself has Node  as not part of the \u201clast_stable_group\u201d (G). The same is true for Node . For Node , the in_previous_stable_group indicator is \u201cTRUE,\u201d since Node  belongs to G. Node 3 itself has Node  as part of the \u201clast stable group\u201d (G). The end result is that all nodes accept the \u201cPTC\u201d, as expected.","If the Group Leader and Crown Prince fail at the same time, the \u201cthird in line\u201d dissolves the group, and all of the adapters in the group form an unstable singleton group. Slowly the remaining members coalesce into a single group. Since the last_stable_group indicators contain the group prior to the dissolve, the \u201cPTC\u201d issued during the coalesce phase are accepted.","This example actually also falls under Example 2 above. If the daemon is blocked for too long and the adapter was expelled from its AMGs, then the node with the blocked daemon eventually forms a singleton stable group for all of its adapters.","The environment in which the present invention is employed is shown in . In particular, there is shown a plurality of nodes  connected in a network via network adapters . Though not specifically shown in the figures herein, the network typically includes a switch which routes messages from source nodes to destination nodes in the network of nodes.  also particularly illustrates the possibility that one of the nodes may experience a failure. This failure could occur with the node itself or within the network adapter through which it is coupled to the other nodes.",{"@attributes":{"id":"p-0118","num":"0127"},"figref":["FIGS. 2A through 2G","FIG. 2A","FIG. 2B","FIG. 2C","FIG. 2D","FIG. 2F","FIG. 2G"],"b":["1","1","2","2","1","1","1","1","1","1","1","1"]},{"@attributes":{"id":"p-0119","num":"0128"},"figref":["FIG. 3A","FIG. 2G","FIG. 3A","FIG. 3B","FIG. 2C","FIG. 3C"]},{"@attributes":{"id":"p-0120","num":"0129"},"figref":["FIGS. 4A","FIG. 4A","FIG. 4B","FIG. 4C"],"b":["4","4","1","5","2"]},"As indicated above, node reachability is maintained across the network by means of a Node Connectivity Table. Such a table, with its concomitant entries, is shown in  for the network and node interconnections shown in . In particular, the NCT shown shows two groups and two networks. Adapter Membership Group A_ includes Node #, Node #, Node # and Node #. In this group, Node # is the Group Leader (GL). Adapter Membership Group B_ includes Node #, Node #, Node # and Node # with Node # being the Group Leader (GL) in AMG B_. This structure is reflected in the Node Connectivity Table that exists at Node #, as shown. In particular, the initial Node Connectivity Table shown if  is included to illustrate the state of the system just prior to a node death. In this death scenario, Node # is assumed to have failed. As shown in , the death of Node # results in the formation of a new group from the remaining nodes. This Adapter Membership Group A_ is formed from Node #, Node # and Node #. The group forming process preferably employs the protocols set forth above with particular reference to . This group formation is communicated to the other nodes in the network via the Group Connectivity Message (GCM), as illustrated in . As each node receives the GCM, it updates its own Node Connectivity Table. A NCT with updated entries for Node # is shown in . In this manner, all of the nodes in the network are made aware of group membership and \u201cliveness\u201d status.",{"@attributes":{"id":"p-0122","num":"0131"},"figref":"FIG. 6","b":["1","1","2"]},"In  it is seen that at time T=0 (seconds being the assumed time unit), termination of the heartbeat daemon on Node # results in missing heartbeats occurring for this node in Networks A and B. However, because of the slower detection process on Network B, the death of Node # is not recognized until T=40. Nonetheless, on Network A, Node # is detected as having \u201cdied\u201d at T=10. At T=20, the heartbeat daemon is restarted on Node # (this is the start of the \u201cquick restart\u201d phenomenon that gives rise to one of the problems considered herein; it should also be noted that the use of the adjective \u201cquick\u201d to describe the restart is meant to be a relative term: it is quick relative to the timing for node and\/or adapter failure detection in a connected network of nodes.) With the \u201crebirth\u201d of Node # in Network A it then joins a group on Network A at T=25. At T=30, Node # is still seen as being alive on Network B which has not as yet determined that is has failed. At T=40, the death of Node # is finally detected at Node #. Node # is now \u201cfree\u201d to join a group on Network B which Node # is capable of processing at T=45. At this point we potentially have Node # as part of a group in Network A and also a member of a different group in Network B. Nowhere in the sequence from T=0 to T=45 shown in  is Node # seen as being completely unreachable in both networks. Therefore, in the time frame considered, Node # is not ever seen as being dead.",{"@attributes":{"id":"p-0124","num":"0133"},"figref":["FIG. 7","FIG. 8"],"b":["1","2","1","1","1","2","1","1","2","1","1","1","1","1","1","1"]},{"@attributes":{"id":"p-0125","num":"0134"},"figref":["FIG. 8","FIG. 7","FIG. 7","FIG. 7"],"b":["1","1","2","1","2","1","1","2","1","1","2","1","1","1","1","1","1"]},{"@attributes":{"id":"p-0126","num":"0135"},"figref":["FIG. 9","FIG. 9"]},{"@attributes":{"id":"p-0127","num":"0136"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0128","num":"0137"},"figref":"FIG. 11","b":["1","1","2","1","1"]},{"@attributes":{"id":"p-0129","num":"0138"},"figref":["FIG. 12","FIG. 11"],"b":"1","ul":{"@attributes":{"id":"ul0007","list-style":"none"},"li":{"@attributes":{"id":"ul0007-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0008","list-style":"none"},"li":["while the address part of the \u201csource Adapter ID\u201d (\u201c1.1.1.1 \u201d) can actually be found in Node #'s AMG, the instance number portions (\u201c7259\u201d in the AMG and \u201c7901\u201d in the message) do not match; and","the Source Group ID in the message does not match the Group ID stored at Node #.\n\nThe indication provided by the inconsistencies above is enough for Node # to determine that Node # bounced.\n"]}}}},{"@attributes":{"id":"p-0130","num":"0141"},"figref":["FIGS. 13A and 13B","FIG. 13A","FIG. 13B"],"ul":{"@attributes":{"id":"ul0009","list-style":"none"},"li":{"@attributes":{"id":"ul0009-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0010","list-style":"none"},"li":["the messages are not intended for the node's current Group ID; and","the current group is singleton.\n\nThe inconsistencies above are interpreted as the recipient node having bounced. In response to the \u201cHEART BEAT\u201d message, it sends a \u201cNOT YOUR NEIGHBOR\u201d response back to the sender, which in turn sends a \u201cDEATH\u201d message to the Group Leader, informing it about the demise of the bounced node.\n"]}}}},{"@attributes":{"id":"p-0131","num":"0144"},"figref":["FIGS. 14A through 14D","FIG. 14A","FIG. 14B","FIG. 14C","FIG. 14D"],"b":["3","2","3","3","2","3","3","3","3","2","3","3"]},{"@attributes":{"id":"p-0132","num":"0145"},"figref":["FIGS. 15A through 15E","FIG. 14","FIG. 15A","FIG. 15B","FIG. 15C","FIG. 15D","FIG. 15E"],"b":["1","1","3","3","1","1","1","3","1","1","3","1","3","1","1","1"]},"While the invention has been described in detail herein in accordance with certain preferred embodiments thereof, many modifications and changes therein may be effected by those skilled in the art. Accordingly, it is intended by the appended claims to cover all such modifications and changes as fall within the true spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The subject matter which is regarded as the invention is particularly pointed out and distinctly claimed in the concluding portion of the specification. The invention, however, both as to organization and method of practice, together with the further objects and advantages thereof, may best be understood by reference to the following description taken in connection with the accompanying drawings in which:",{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 2C"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 2D"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 2E"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 2F"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 2G","FIGS. 2A through 2F"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 3C"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 4A"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 4B"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 4C"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 5B","b":["2","1","3","4","2"]},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 5C"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 6","b":"1"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 11","b":"1"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 13A"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":["FIG. 13B","FIG. 13A"]},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIGS. 14A through 14D","b":"3"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":["FIG. 14B","FIG. 14A"],"b":["2","3"]},{"@attributes":{"id":"p-0051","num":"0050"},"figref":["FIG. 14C","FIGS. 14A and 14B"],"b":"3"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":["FIG. 14D","FIGS. 14A"],"b":["14","14","3"]},{"@attributes":{"id":"p-0053","num":"0052"},"figref":["FIGS. 15A through 15E","FIGS. 14A through 14D"],"b":["1","2","3"]},{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIG. 15B","FIG. 15A"],"b":["1","3","1"]},{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIG. 15C","FIG. 15B","FIG. 115B"],"b":"1"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":["FIG. 15D","FIG. 15D","FIG. 15C"],"b":["3","1"]},{"@attributes":{"id":"p-0057","num":"0056"},"figref":["FIG. 15E","FIG. 15D"],"b":"3"}]},"DETDESC":[{},{}]}
