---
title: System and method for LUN cloning
abstract: A logical unit number (LUN) cloning technique separates data blocks referenced by a writable virtual disk (vdisk) from data blocks referenced only by a backing store to conserve storage resources of a storage system. The LUN cloning technique separates the writable vdisk data blocks from the backing store during periods of reduced processing activity and in a manner that does not interfere with storage service provided by the system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08041888&OS=08041888&RS=08041888
owner: NetApp, Inc.
number: 08041888
owner_city: Sunnyvale
owner_country: US
publication_date: 20040205
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATION","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF AN ILLUSTRATIVE EMBODIMENT"],"p":["The present invention is related to the following U.S. patent application Ser. No. 10\/412,478 titled, Writable Read-Only Snaphots, by Vijayan Rajan and filed on Apr. 11, 2003.","The present invention relates to storage systems and, more specifically, to a technique that conserves storage resources of a storage system.","A storage system is a computer that provides storage service relating to the organization of information on writable persistent storage devices, such as memories, tapes or disks. The storage system is commonly deployed within a network attached storage (NAS) or storage area network (SAN) environment. A SAN is a high-speed network that enables establishment of direct connections between a storage system, such as an application server, and its storage devices. The SAN may thus be viewed as an extension to a storage bus and, as such, an operating system of the storage system enables access to stored information using block-based access protocols over the \u201cextended bus\u201d. In this context, the extended bus is typically embodied as Fibre Channel (FC) or Ethernet media (i.e., network) adapted to operate with block access protocols, such as Small Computer Systems Interface (SCSI) protocol encapsulation over FC or Transmission Control Protocol\/Internet Protocol (TCP\/IP)\/Ethernet.","SCSI is a peripheral input\/output (I\/O) interface with a standard, device independent protocol that allows different peripheral storage devices, such as disks, to attach to the storage system. In SCSI terminology, clients operating in a SAN environment are initiators that initiate requests and commands for data. The storage system is a target configured to respond to the requests issued by the initiators in accordance with a request\/response protocol. The SAN clients typically identify and address the stored information in terms of blocks situated within target disks, or logical units thereof (also referred to within the industry as logical unit numbers or luns).","When used within a NAS environment, the storage system may be embodied as a file server including an operating system that implements a file system to logically organize the information as a hierarchical structure of directories and files on, e.g., the disks. Each \u201con-disk\u201d file may be implemented as a set of data structures, e.g., disk blocks, configured to store information, such as the actual data for the file. The data blocks are typically organized within a volume block number (vbn) space that is maintained by the file system. The file system may also assign each data block in the file a corresponding \u201cfile offset\u201d or file block number (fbn). The file system typically assigns sequences of fbns on a per-file basis, whereas vbns are assigned over a larger volume address space. The file system organizes the data blocks within the vbn space as a \u201clogical volume\u201d; each logical volume may be, although is not necessarily, associated with its own file system. A directory, on the other hand, may be implemented as a specially formatted file in which information about other files and directories are stored.","The file server, or filer, of a NAS system may be further configured to operate according to a client\/server model of information delivery to thereby allow many client systems (clients) to access shared resources, such as files, stored on the filer. In the client\/server model, the client may comprise an application executing on a computer that \u201cconnects\u201d to the filer over a computer network, such as a point-to-point link, shared local area network, wide area network or virtual private network implemented over a public network, such as the Internet. The clients typically communicate with the filer by exchanging discrete frames or packets of data according to pre-defined protocols, such as the TCP\/IP. NAS systems generally utilize file-based access protocols; therefore, each client may request the services of the filer by issuing file system protocol messages (in the form of packets) to the file system over the network.","A common type of file system is a \u201cwrite in-place\u201d file system, an example of which is the conventional Berkeley fast file system. In a write in-place file system, the locations of the data structures, such as index nodes (inodes) and data blocks, on disk are typically fixed. An inode is a data structure used to store information, such as metadata, about a file, whereas the data blocks are structures used to store the actual data for the file. The information contained in an inode may include, e.g., ownership of the file, access permission for the file, size of the file, file type and references to locations on disk of the data blocks for the file. The references to the locations of the file data are provided by pointers, which may further reference indirect blocks that, in turn, reference the data blocks, depending upon the quantity of data in the file. Changes to the inodes and data blocks are made \u201cin-place\u201d in accordance with the write in-place file system. If an update to a file extends the quantity of data for the file, an additional data block is allocated and the appropriate inode is updated to reference that data block.","Another type of file system is a write-anywhere file system that does not overwrite data on disks. If a data block on disk is retrieved (read) from disk into memory and \u201cdirtied\u201d with new data, the data block is stored (written) to a new location on disk to thereby optimize write performance. An example of a write-anywhere file system that is configured to operate on a filer is the Write Anywhere File Layout (WAFL\u2122) file system available from Network Appliance, Inc. of Sunnyvale, Calif. The WAFL file system is implemented within a microkernel as part of the overall protocol stack of the filer and associated disk storage. This microkernel is supplied as part of Network Appliance's Data ONTAP\u2122 storage operating system residing on the filer.","The WAFL file system has the capability to generate a snapshot of its active file system. An \u201cactive file system\u201d is a file system to which data can be both written and read, or, more generally, an active store that responds to both read and write I\/O operations. Further, it should be noted that \u201csnapshot\u201d is a trademark of Network Appliance, Inc. and is used for purposes of this patent to designate a persistent consistency point (CP) image. A persistent consistency point image (PCPI) is a space conservative, point-in-time read-only image of data accessible by name that provides a consistent image of that data at some previous time. More particularly, a PCPI is a point-in-time representation of a storage element, such as an active file system, file or database, stored on a storage device (e.g., on disk) or other persistent memory and having a name or other identifier that distinguishes it from other PCPIs taken at other points in time. A PCPI can also include other information (metadata) about the storage element at the particular point in time for which the image is taken. The terms \u201cPCPI\u201d and \u201csnapshot\u201d may be used interchangeably through out this patent without derogation of Network Appliance's trademark rights.","A file system, such as the WAFL file system, supports multiple snapshots that are generally created on a regular schedule. Each snapshot is a restorable version of the storage element (e.g., the active file system) created at a predetermined point in time and, as noted, is \u201cread-only\u201d accessible and \u201cspace-conservative\u201d. Space conservative denotes that common parts of the storage element in multiple snaphots share the same file system blocks. Only the differences among these various snapshots require extra storage blocks. The multiple snapshots of a storage element are not independent copies, each consuming disk space; therefore, creation of a snapshot on the WAFL file system is instantaneous, since no entity data needs to be copied. Read-only accessibility denotes that a snapshot cannot be modified because it is closely coupled to a single writable image in the active file system. The closely coupled association between a file in the active file system and the same file in a snapshot obviates the use of multiple \u201csame\u201d files. In the example of a WAFL based file system, snapshots are described in TR3002 by David Hitz et al., published by Network Appliance, Inc. and in U.S. Pat. No. 5,819,292 entitled Method for Maintaining Consistent States of a File System and For Creating User-Accessible Read-Only Copies of a File System, by David Hitz et al., each of which is hereby incorporated by reference as though full set forth herein.","Broadly stated, a snapshot is stored on-disk along with the active file system, and is called into a memory of a filer as requested by an operating system. The on-disk organization of the snapshot and the active file system can be understood from the following description of an exemplary file system inode structure  shown in . The inode for an inode file  contains information describing the inode file associated with the active file system. In this exemplary file system inode structure, the inode for the inode file  contains a pointer that references (points to) an inode file indirect block . The inode file indirect block  contains a set of pointers that reference inodes  which, in turn, contain pointers to indirect blocks . The indirect blocks  include pointers to file data blocks A, B and C. Each of the file data blocks (A-C) is capable of storing, e.g., 4 kilobytes (kB) of data. When the file system generates a snapshot of its active file system, a snapshot inode is generated as shown in . The snapshot inode  is, in essence, a duplicate copy of the inode for the inode file  of the file system  that shares common parts, such as inodes and blocks, with the active file system. For example, the exemplary file system structure  includes the inode file indirect blocks , inodes , indirect blocks  and file data blocks A-C as in ","When a user modifies a file data block, the file system writes the new data block to disk and changes the active file system to point to the newly created block.  shows an exemplary inode file system structure  after a file data block has been modified. In this example, file data block C is modified to file data block C\u2032. As a result, the contents of the modified file data block are written to a new location on disk as a function of the exemplary file system. Because of this new location, the indirect block  must be rewritten. Due to this changed indirect block , the inode  must be rewritten. Similarly, the inode file indirect block  and the inode for the inode file  must be rewritten.","Thus, after a file data block has been modified the snapshot inode  contains a pointer to the original inode file indirect block  which, in turn, contains pointers through the inode  and indirect block  to the original file data blocks A, B and C. The newly written indirect block  also includes pointers to unmodified file data blocks A and B. That is, the unmodified data blocks in the file of the active file system are shared with corresponding data blocks in the snapshot file, with only those blocks that have been modified in the active file system being different than those of the snapshot file.","However, the indirect block  further contains a pointer to the modified file data block C\u2032 representing the new arrangement of the active file system. A new inode for the inode file  is established representing the new structure . Note that metadata (not shown) stored in any of the snapshotted blocks (e.g., , , and C) protects these blocks from being recycled or overwritten until they are released from all snapshots. Thus, while the active file system inode for the inode file  points to new blocks , , , A, B and C\u2032, the old blocks ,  and C are retained until the snapshot is fully released.","Snapshots provide a versatile feature that is essential for data recovery operations, such as backup and recovery of storage elements. However, since snapshots are read-only accessible and their contents cannot be modified, their use may be somewhat limited, particularly for operating systems and applications that do not have a notion of a read-only data store (a read-only file system) and that expect to write metadata at any time that the file system is accessible. This limitation may be overcome by using writable read-only snapshot technique as described in U.S. patent application Ser. No. 10\/412,478 entitled Writable Read Only Snapshots, by Vijayan Rajan. A writable, read-only snapshot comprises a read-only \u201cimage\u201d (file) residing in a snapshot and a writable virtual disk (vdisk) residing in the active file system. The writable vdisk is a \u201cshadow\u201d image of the snapshot file and, as such, includes an attribute that specifies the snapshot file to be used as a backing store. According to the technique, a write operation directed to the writable read-only snapshot is \u201ctrapped\u201d such that the data associated with the operation is stored on the shadow, vdisk image in the active file system. In other words rather than directly accessing the read-only snapshot image of a lun, a client accesses the writable vdisk image, which provides a translucent view of the underlying read-only snapshot image. The writable vdisk is a sparse file containing only that data written by the client (e.g., an initiator in a SAN) to the read-only snapshot image subsequent to a snapshot operation to a volume underlying the lun (vdisk).","To the client, the data retrieved from the writable, read-only snapshot is always the latest data written. The client \u201csees\u201d the writable vdisk data first (if it exists) and is served that data, the underlying read-only snapshot image being inaccessible for the range of valid data in the writable vdisk. Read-only data from the underlying snapshot image is delivered to the client when no valid data overlying the range exists in the writable vdisk. The underlying snapshot image is accessible and recoverable via a non-translucent path of directly accessing the snapshot image. By this technique, data integrity of a snapshotted lun or vdisk (as an inviolate backup) is preserved.","The writable read only snapshot technique allows many writable vdisks to be \u201ctied\u201d to a single backing store snapshot file. This backing store file may be quite large consuming substantial storage (disk) space. After a period of time, it may be desirable to delete the backing store and free the disk space it consumes. Yet the backing store may not be deleted as long as it is referenced by at least one writable vdisk, even if that vdisk has totally diverged from the backing store. That is, if the backing store is referenced by a writable vdisk, it cannot be deleted even if every data block in the vdisk has been modified from its original state in the backing store.","Previous solutions to this problem have required taking the writable vdisk offline while the backing store is being released. This is undesirable since it creates downtime visible to client applications served by the filer or storage system and substantially decreases system performance. Other proposed solutions have involved restoring the backing store to the active file system using a technique such as single file snap restore (SFSR) described in U.S. patent application Ser. No. 10\/100,948 entitled System and Method for Restoring a Single File from a Snapshot. Yet this is not a desirable solution because the restored snapshot does not contain the contents of modified data blocks, such as modified data block C\u2032, and rather reflects an older version of the of data blocks, such as old version C. Further any technique involving SFSR would generally render the snapshot inaccessible for the duration of the restore operation.","What is needed is a \u201czero-downtime\u201d technique to delete a snapshot and free its consumed disk space.","The present invention overcomes the disadvantages of the prior art by providing a novel logical unit number (LUN) cloning technique that \u201cseparates\u201d data blocks referenced by a writable virtual disk (vdisk) from data blocks referenced only by a backing store. LUN cloning, in this context, denotes separating the data blocks to create an independent vdisk from the writable vdisk. This, in turn, allows the backing store data blocks to be deallocated, to thereby conserve storage resources of a storage system, such as a multi-protocol storage appliance. Notably, separation of the writable vdisk data blocks is from the backing store occurs without interrupting service of data access requests and preferably occurs during periods of reduced processing activity.","In the illustrative embodiment, a file system invokes a \u201cbackdoor\u201d message handler that loads blocks of the writable vdisk, and its backing store file, from disk into a buffer cache of the storage appliance. The loaded blocks are represented as vdisk and backing store buffer trees that include, among other structures, indirect blocks. A special vdisk loading function of the file system \u201cwalks through\u201d all the indirect blocks of the writable vdisk, searching for invalid values of volume block number (VBN) pointers. While a valid VBN pointer (e.g., a non-zero value) directly references a data block, an invalid VBN pointer represents a \u201chole.\u201d A hole instructs the file system to examine the value of the VBN pointer in a corresponding indirect block of the backing store.","If the VBN pointer of the backing store has a non-zero value, the backdoor message handler loads the \u201cold\u201d data block referenced by the VBN pointer into the buffer cache and marks that block as \u201cdirty,\u201d without changing its data contents. A write allocator of the file system thereafter \u201cwrite allocates\u201d the dirty block by, e.g., choosing a new VBN for a newly allocated block, setting appropriate bits in block allocation structures, placing the chosen VBN into the appropriate indirect block of the writable vdisk, and deleting (freeing) the old data block prior to storing (writing) the new block to disk. This process is repeated for all instances of holes discovered in the indirect blocks of the writable vdisk.","The file system then \u201creleases\u201d an association of the writable vdisk to the backing store by, e.g., deleting a backing store file handle reference from an attributes inode of the writable vdisk. In addition, the file system updates an appropriate entry of a vdisk table of contents (VTOC) structure to remove the backing store file handle reference.","Advantageously, the inventive technique uses backdoor messaging to separate the writable vdisk data blocks from the backing store, thereby obviating the need to take the writable vdisk data blocks offline. Thus the LUN cloning technique results in zero-downtime for the multi-protocol storage appliance and is transparent to client applications served by the storage appliance.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 4","b":"400"},"The multi-protocol storage appliance  is illustratively embodied as a storage system comprising a processor , a memory , a plurality of network adapters ,  and a storage adapter  interconnected by a system bus . The multi-protocol storage appliance  also includes a storage operating system  that provides a virtualization system (and, in particular, a file system) to logically organize the information as a hierarchical structure of named directory, file and virtual disk (vdisk) storage objects on the disks . An example of a multi-protocol storage appliance that may be advantageously used with the present invention is described in co-pending and commonly assigned U.S. patent application Ser. No. 10\/215,917 titled A Multi-Protocol Storage Appliance that Provides Integrated Support for File and Block Access Protocols, by Brian Pawlowski, et al.","Whereas clients of a NAS-based network environment have a storage viewpoint of files within volumes, the clients of a SAN-based network environment have a storage viewpoint of blocks within disks. To that end, the multi-protocol storage appliance  presents (exports) disks to SAN clients through the creation of logical unit numbers (luns) or vdisk objects. A vdisk object (hereinafter \u201cvdisk\u201d) is a special file type that is implemented by the virtualization system and translated into an emulated disk as viewed by the SAN clients. The multi-protocol storage appliance thereafter makes these emulated disks accessible to the SAN clients through controlled exports.","In the illustrative embodiment, the memory  comprises storage locations that are addressable by the processor and adapters for storing software program code. A portion of the memory may be further organized as a \u201cbuffer cache\u201d  for storing data structures associated with the present invention. The processor and adapters may, in turn, comprise processing elements and\/or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system , portions of which are typically resident in memory and executed by the processing elements, functionally organizes the storage appliance by, inter alia, invoking storage operations in support of the storage service implemented by the appliance. It will be apparent to those skilled in the art that other processing and memory means, including various computer readable media, may be used for storing and executing program instructions pertaining to the invention described herein.","The network adapter  couples the storage appliance to a plurality of clients over point-to-point links, wide area networks, virtual private networks implemented over a public network (Internet) or a shared local area network, hereinafter referred to as an illustrative Ethernet network . For this NAS-based network environment, the clients are configured to access information stored on the multi-protocol appliance as files. Therefore, the network adapter  may comprise a network interface card (NIC) having the mechanical, electrical and signaling circuitry needed to connect the appliance to a network switch, such as a conventional Ethernet switch . The clients  communicate with the storage appliance over network  by exchanging discrete frames or packets of data according to pre-defined protocols, such as the Transmission Control Protocol\/Internet Protocol (TCP\/IP).","The clients  may be general-purpose computers configured to execute applications over a variety of operating systems, including the UNIX\u00ae and Microsoft\u00ae Windows\u2122 operating systems. Client systems generally utilize file-based access protocols when accessing information (in the form of files and directories) over a NAS-based network. Therefore, each client  may request the services of the storage appliance  by issuing file access protocol messages (in the form of packets) to the appliance over the network . For example, a client running the Windows operating system may communicate with the storage appliance  using the Common Internet File System (CIFS) protocol over TCP\/IP. On the other hand, a client running the UNIX operating system may communicate with the multi-protocol appliance using either the Network File System (NFS) protocol over TCP\/IP or the Direct Access File System (DAFS) protocol over a virtual interface (VI) transport in accordance with a remote DMA (RDMA) protocol over TCP\/IP. It will be apparent to those skilled in the art that other clients running other types of operating systems may also communicate with the integrated multi-protocol storage appliance using other file access protocols.","The storage network \u201ctarget\u201d adapter  also couples the multi-protocol storage appliance  to clients  that may be further configured to access the stored information as blocks or disks. For this SAN-based network environment, the storage appliance is coupled to an illustrative Fibre Channel (FC) network . FC is a networking standard describing a suite of protocols and media that is primarily found in SAN deployments. The network target adapter  may comprise a FC host bus adapter (HBA) having the mechanical, electrical and signaling circuitry needed to connect the appliance  to a SAN network switch, such as a conventional FC switch . In addition to providing FC access, the FC HBA may offload fiber channel network processing operations for the storage appliance.","The clients  generally utilize block-based access protocols, such as the Small Computer Systems Interface (SCSI) protocol, when accessing information (in the form of blocks, disks or vdisks) over a SAN-based network. SCSI is a peripheral input\/output (I\/O) interface with a standard, device independent protocol that allows different peripheral devices, such as disks , to attach to the storage appliance . In SCSI terminology, clients  operating in a SAN environment are initiators that initiate requests and commands for data. The multi-protocol storage appliance is thus a target configured to respond to the requests issued by the initiators in accordance with a request\/response protocol. The initiators and targets have endpoint addresses that, in accordance with the FC protocol, comprise worldwide names (WWN). A WWN is a unique identifier, e.g., a node name or a port name, consisting of an 8-byte number.","The multi-protocol storage appliance  supports various SCSI-based protocols used in SAN deployments, including SCSI encapsulated over TCP (iSCSI) and SCSI encapsulated over FC (FCP). The initiators (hereinafter clients ) may thus request the services of the target (hereinafter storage appliance ) by issuing iSCSI and FCP messages over the network  to access information stored on the disks. It will be apparent to those skilled in the art that the clients may also request the services of the integrated multi-protocol storage appliance using other block access protocols. By supporting a plurality of block access protocols, the multi-protocol storage appliance provides a unified and coherent access solution to vdisks\/luns in a heterogeneous SAN environment.","The storage adapter  cooperates with the storage operating system  executing on the storage appliance to access information requested by the clients. The information may be stored on the disks  or other similar media adapted to store information. The storage adapter includes I\/O interface circuitry that couples to the disks over an I\/O interconnect arrangement, such as a conventional high-performance, FC serial link topology. The information is retrieved by the storage adapter and, if necessary, processed by the processor  (or the adapter  itself) prior to being forwarded over the system bus  to the network adapters , , where the information is formatted into packets or messages and returned to the clients.","Storage of information on the appliance  is preferably implemented as one or more storage volumes (e.g., VOL1-2 ) that comprise a cluster of physical storage disks , defining an overall logical arrangement of disk space. Each volume may be associated with its own file system and, for purposes herein, volume and file system may be used synonymously. The disks within a volume are typically organized as one or more groups of Redundant Array of Independent (or Inexpensive) Disks (RAID). RAID implementations enhance the reliability\/integrity of data storage through the writing of data \u201cstripes\u201d across a given number of physical disks in the RAID group, and the appropriate storing of redundant information with respect to the striped data. The redundant information enables recovery of data lost when a storage device fails.","Specifically, each volume  is constructed from an array of physical disks  that are organized as RAID groups , , and . The physical disks of each RAID group include those disks configured to store striped data (D) and those configured to store parity (P) for the data, in accordance with an illustrative RAID 4 level configuration.","However, other RAID level configurations (e.g. RAID 5) are also contemplated. In the illustrative embodiment, a minimum of one parity disk and one data disk may be employed. Yet, a typical implementation may include three data and one parity disk per RAID group and at least one RAID group per volume.","To facilitate access to the disks , the storage operating system  implements a write-anywhere file system that cooperates with virtualization modules to provide a function that \u201cvirtualizes\u201d the storage space provided by disks . The file system logically organizes the information as a hierarchical structure of named directory and file objects (hereinafter \u201cdirectories\u201d and \u201cfiles\u201d) on the disks. Each \u201con-disk\u201d file may be implemented as set of disk blocks configured to store information, such as data, whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization modules allow the file system to further logically organize information as a hierarchical structure of named vdisks on the disks, thereby providing an integrated NAS and SAN appliance approach to storage by enabling file-based (NAS) access to the files and directories, while further enabling block-based (SAN) access to the vdisks on a file-based storage platform.","In the illustrative embodiment, the storage operating system is preferably the NetApp\u00ae Data ONTAP\u2122 operating system available from Network Appliance, Inc., Sunnyvale, Calif. that implements a Write Anywhere File Layout (WAFL\u2122) file system. However, it is expressly contemplated that any appropriate storage operating system, including a write in-place file system, may be enhanced for use in accordance with the inventive principles described herein. As such, where the term \u201cWAFL\u201d is employed, it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.","As used herein, the term \u201cstorage operating system\u201d generally refers to the computer-executable code operable on a computer that manages data access and may, in the case of a multi-protocol storage appliance, implement data access semantics, such as the Data ONTAP storage operating system, which is implemented as a microkernel. The storage operating system can also be implemented as an application program operating over a general-purpose operating system, such as UNIX\u00ae or Windows NT\u00ae, or as a general-purpose operating system with configurable functionality, which is configured for storage applications as described herein.","In addition, it will be understood to those skilled in the art that the inventive technique described herein may apply to any type of special-purpose (e.g., storage serving appliance) or general-purpose computer, including a standalone computer or portion thereof, embodied as or including a storage system. Moreover, the teachings of this invention can be adapted to a variety of storage system architectures including, but not limited to, a network-attached storage environment, a storage area network and disk assembly directly-attached to a client or host computer. The term \u201cstorage system\u201d should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems.",{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 5","b":["500","510","512","514","516","518","520","522","524","526","518"]},"An iSCSI driver layer  provides block protocol access over the TCP\/IP network protocol layers, while a FC driver layer  operates with the FC HBA  to receive and transmit block access requests and responses to and from the integrated storage appliance. The FC and iSCSI drivers provide FC-specific and iSCSI-specific access control to the luns (vdisks) and, thus, manage exports of vdisks to either iSCSI or FCP or, alternatively, to both iSCSI and FCP when accessing a single vdisk on the multi-protocol storage appliance. In addition, the storage operating system includes a disk storage layer  that implements a disk storage protocol, such as a RAID protocol, and a disk driver layer  that implements a disk access protocol such as, e.g., a SCSI protocol.","Bridging the disk software layers with the integrated network protocol stack layers is a virtualization system .  is a schematic block diagram of the virtualization system  that is implemented by a file system  interacting with virtualization modules illustratively embodied as, e.g., vdisk module  and SCSI target module . It should be noted that the vdisk module , the file system  and SCSI target module  can be implemented in software, hardware, firmware, or a combination thereof. The vdisk module  is layered on the file system  to enable access by administrative interfaces, such as a streamlined user interface (UI ), in response to a system administrator issuing commands to the multi-protocol storage appliance . In essence, the vdisk module  manages SAN deployments by, among other things, implementing a comprehensive set of vdisk (lun) commands (cmds)  issued through the UI  by a system administrator. These vdisk commands are converted to primitive file system operations (\u201cprimitives \u201d) that interact with the file system  and the SCSI target module  to implement the vdisks.","The SCSI target module , in turn, initiates emulation of a disk or lun by providing a mapping procedure that translates a lun identifier to a vdisk-type file. The SCSI target module is illustratively disposed between the FC and iSCSI drivers ,  and the file system  to thereby provide a translation layer of the virtualization system  between the SAN block (lun) space and the file system space, where luns are represented as vdisks . To that end, the SCSI target module has a set of application programming interfaces (APIs ) that are based on the SCSI protocol and that enable a consistent interface to both the iSCSI and FCP drivers , . By \u201cdisposing\u201d SAN virtualization over the file system , the multi-protocol storage appliance reverses the approach taken by prior systems to thereby provide a single unified storage platform for essentially all storage access protocols.","The file system  is illustratively a message-based system; as such, the SCSI target module  transposes a SCSI request into a message representing an operation directed to the file system. For example, the message generated by the SCSI target module may include a type of operation (e.g., read, write) along with a pathname (e.g., a path descriptor) and a filename (e.g., a special filename) of the vdisk object represented in the file system. Alternatively, the generated message may include an operation type and file handle containing volume\/inode information. The SCSI target module  passes the message into the file system layer  as, e.g., a function call , where the operation is performed.","The file system provides volume management capabilities for use in block-based access to the information stored on the storage devices, such as disks. That is, in addition to providing file system semantics, such as naming of storage objects, the file system  provides functions normally associated with a volume manager. These functions include (i) aggregation of the disks, (ii) aggregation of storage bandwidth of the disks, and (iii) reliability guarantees, such as mirroring and\/or parity (RAID), to thereby present one or more storage objects layered on the file system. A feature of the multi-protocol storage appliance is the simplicity of use associated with these volume management capabilities, particularly when used in SAN deployments.","The file system  illustratively implements the WAFL file system having an on-disk format representation that is block-based using, e.g., 4 kilobyte (kB) blocks and using inodes to describe the files . The file system uses files to store metadata describing the layout of its file system; these metadata files include, among others, an inode file. A file handle, i.e., an identifier that includes an inode number, is used to retrieve an inode from disk. As noted, the WAFL file system also supports multiple snapshots that are generally created on a regular schedule. A description of the structure of the file system, including on-disk inodes, the inode file and snapshots, is provided in U.S. Pat. No. 5,819,292. Notably, snapshots are created on the multi-protocol storage appliance without the need for prior configuration of the underlying storage. This feature of the appliance simplifies the creation and management of data recovery techniques for business continuance compared to previous block-based recovery methods and mechanisms.",{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 7","b":["700","710","750","710","700","712","714","716","718","720","710","730","750","712","750","750"]},"Specifically, the data section  of a regular on-disk inode may include user data or pointers, the latter referencing 4 kB data blocks on disk used to store the user data. Each pointer is preferably a logical volume block number (VBN) to thereby facilitate efficiency among the file system and the disk storage (RAID) layer  when accessing the data on disks. Given the restricted size (128 bytes) of the inode, user data having a size that is less than or equal to 64 bytes is represented, in its entirety, within the data section of that inode. However, if the user data is greater than 64 bytes but less than or equal to 64 kB, then the data section of the inode comprises up to 16 pointers, each of which references a 4 kB block of data on the disk. Moreover, if the size of the data is greater than 64 kilobytes but less than or equal to 64 megabytes (MB), then each pointer in the data section  of the inode references an indirect block that contains 1024 pointers, each of which references a 4 kB data block on disk. Each data block is loaded from disk  into memory  in order to access the data. As noted, the size field  of the metadata section  of the inode refers to the size of the file.","Broadly stated, all inodes of the file system are organized into the inode file. A file system (FS) info block specifies the layout of information in the file system and includes an inode of a file that includes all other inodes of the file system. Each volume has an FS info block that is preferably stored at a fixed location within, e.g., a RAID group of the file system. The inode of the root FS info block may directly reference (point to) blocks of the inode file or may reference indirect blocks of the inode file that, in turn, reference direct blocks of the inode file. Within each direct block of the inode file are embedded inodes, each of which may reference indirect blocks that, in turn, reference data blocks of a file or vdisk.","As noted, a vdisk is a special file type in a volume that derives from a plain (regular) file, but that has associated export controls and operation restrictions that support emulation of a disk. Unlike a file that can be created by a client using, e.g., the NFS or CIFS protocol, a vdisk is created on the multi-protocol storage appliance via, e.g. a user interface (UI) as a special typed file (object). Illustratively, the vdisk is a multi-inode object comprising a special file inode that holds data and at least one associated stream inode that holds attributes. The special file inode functions as a main container for storing data, such as application data, associated with the emulated disk. The stream inode stores attributes that, among others, allow luns and exports to persist over, e.g., reboot operations, while also enabling management of the vdisk as a single disk object in relation to SAN clients.",{"@attributes":{"id":"p-0067","num":"0066"},"figref":["FIG. 8","FIG. 7"],"b":["800","810","840","810","622","818","810","812","813","816","814","810","818","820"]},"In order to access the stream_dir inode , the pointer of xinode field  in lun inode  is modified to reference the inode . The stream_dir inode  comprises a metadata section  that includes a type (stream_dir) field  and an xinode field  that references another on-disk inode structure containing, e.g., access control (such as CIFS permission) information associated with the vdisk. The inode  also includes a data section  containing a pointer  that references a stream directory data block associated with the vdisk, such as stream directory block . The stream directory block  comprises a data section  that includes a plurality of entries, each containing an external representation of a stream inode along with mapping information (i.e., the inode number) for that inode. One of those entries, entry , contains mapping information (e.g., a pointer) that references an attributes (stream) inode .","The attributes inode  comprises a metadata section  that includes a type (stream) field  and a data section  that functions as a persistent store for holding various named attributes associated with the vdisk . Attributes are an implementation mechanism that is internal to the file system and not managed by users. An example of an attribute is a snapshot file handle  or file handle of a \u201cbacking store\u201d file (i.e., the snapshot file). The backing store file handle  includes a snapshot identifier (ID), which is an identifier (pointer) to a snapshot containing the snapshot file, and a file ID, which is an identifier (pointer) to the snapshot file. As described herein, the snapshot file functions as a backing store for the vdisk when the vdisk is used as a writable, read-only snapshot. The vdisk and its associated inodes are further described in U.S. patent application Ser. No. 10\/216,453 titled Storage Virtualization by Layering Vdisks on a File System, by Vijayan Rajan, et al.","While vdisks are self-contained objects containing all data necessary for proper operation and authorization, a vdisk table of contents (VTOC  in ) is provided as a performance enhancement to finding and loading vdisks. The VTOC is not necessary for correct operation and can be reconstructed dynamically by a scan of the vdisks. The VTOC  is a per-volume data structure that is stored in a metadata file and that is used to optimize location determination (i.e. finding) and initialization of persistent vdisks  in a volume . In addition, the VTOC  facilitates resolution of the location of a file within a particular snapshot, i.e., allows efficient resolution of a snapshot file location.","The VTOC  comprises one or more records , wherein each record includes flags  and file entries that can be dynamically recreated from information stored in the encapsulated vdisk storage objects. The file entries include a vdisk filehandle  pertaining to a vdisk in the active file system and a backing store file handle  pertaining to a backing store (snapshot file). If the vdisk is used as a writable, read-only snapshot then the backing store file handle contains a file handle, including snapshot ID of the backing store file; otherwise, for a normal vdisk the backing store file handle contains a zero. The file entries of each record  may also contain directory information comprising a file block number in a parent directory (qtree root) containing an entry for the vdisk, along with an index of directory entries in a parent directory block. The directory entry enables determination of the last component of a path to the snapshot file.","The present invention involves writable, read-only snapshots, each of which comprises a read-only \u201cimage\u201d (file) residing in a snapshot and a writable vdisk residing in the active file system. The writable vdisk is a \u201cshadow\u201d image of the snapshot file image and, as noted, includes an attribute that specifies the snapshot file as a backing store. It should be noted that while there are any vdisks in existence in the active file system specifying a file in a snapshot, the snapshot file is \u201clocked\u201d and cannot be deleted.","In the illustrative embodiment, a write operation directed to the writable read-only snapshot is \u201ctrapped\u201d (directed) to the vdisk in the active file system such that the data associated with the operation is stored on that shadow, vdisk image. In other words rather than directly accessing the read-only snapshot image of a lun, a client accesses the writable vdisk image, which provides a translucent view of the underlying read-only snapshot image. The writable vdisk is a \u201csparse\u201d file containing only that data written by the client (e.g., an initiator in a SAN) to the read-only snapshot image subsequent to a snapshot operation to a volume underlying the lun (vdisk).","Briefly, the sparse vdisk in the active file system is \u201ctranslucent\u201d, i.e., initially the vdisk has a size equal to the size of the snapshot file because there is no data other than the snapshot file data. Since there is no data in the initial instance of the vdisk, the vdisk is completely filled with \u201choles.\u201d On read operations issued by a client to the writable read-only snapshot, the file system searches for the requested block in the vdisk of the active file system. If the block is not found, the corresponding block from the backing snapshot file is accessed and returned. It should be noted that having writable vdisks backed by a snapshot file does not prevent direct access to the snapshot file (for backup or other reasons).","Write operations are only carried out on the sparse vdisk in the active file system, i.e., the vdisk in the active file system stores changes (write data) to the read-only snapshot file. For subsequent read operations directed to the writable read-only snapshot, any modified\/changed (\u201cwritten\u201d) data blocks are returned. Otherwise, the holes in the vdisk result in copies of the read-only data blocks being returned from the associated snapshot file, thereby providing a \u201cspace conservative\u201d storage entity.","For example, assume that a vdisk exists in its original state in the active file system and a snapshot is subsequently taken of the volume underlying that vdisk. Write operations can then be directed to that snapshotted vdisk. To that end, the writable read-only snapshot storage entity may be thought of as comprising two storage space layers: (i) an underlying snapshot layer that is \u201cfrozen in time\u201d and that does not change as long as the snapshot file exists, and (ii) an overlaying vdisk layer of the active file system that does change in time as data is written to that layer.",{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 9","b":["900","920","922","934","930","920","934"]},"The present invention is directed to a LUN cloning technique that \u201cseparates\u201d data blocks referenced by a writable vdisk in the active file system from data blocks referenced only by a backing store, thereby removing dependency of the writable vdisk upon the backing store. By separating the data blocks referenced by the writable vdisk, a user may then delete the backing store and free the disk space consumed by the backing store on the storage appliance.","Broadly stated, a backdoor message handler  of file system  loads blocks of the writable vdisk  and its backing store  from disk  into memory  (buffer cache ) in a manner that does not interfere with storage service provided by the multi-protocol storage appliance. Notably, separation of the writable vdisk data blocks from the backing store occurs during periods of reduced processing activity in the file system, e.g., as part of background task processing on the appliance, so that the present technique may be implemented without any downtime visible to a client application served by the appliance. The loaded blocks are represented as writable vdisk and backing store buffer trees in the buffer cache  of the storage appliance.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIG. 10A","FIG. 10B"],"b":["1000","1050","1002","1052","1010","1012","1060","1062","1020","1022","1024","1026","1070","1072","1074","1076","1030","1080","922","934"]},"In the illustrative embodiment, a special vdisk loading function  of the file system \u201cwalks through\u201d the level 1 buffers (indirect blocks) of the writable vdisk, searching for invalid VBN pointers. Note that a valid VBN pointer has a non-zero value that directly references a data block, whereas an invalid VBN pointer has a zero value that represents a hole. Such a hole instructs the file system to examine the value of the VBN pointer in the corresponding level 1 buffer of the backing store.","For example, the vdisk loading function  examines each field - of the writable vdisk level 1 buffer  and, upon recognizing a hole at field , examines field  of the corresponding backing store level 1 buffer . Note that the corresponding backing store level 1 buffer  (e.g., indirect block  of buffer tree ) has the same file block number (FBN) as the writable vdisk level 1 buffer  (e.g., indirect block  of buffer tree ). If the VBN pointer in field  of the backing store level 1 buffer  has a non-zero value, the loading function  informs the backdoor message handler  to mark an \u201cold\u201d level 0 data block  referenced by the non-zero VBN as \u201cdirty\u201d, a designation normally indicating that the contents have been changed. In this case though the old data block is marked \u201cdirty\u201d without altering the content of the block.","Thereafter, a write allocator  of file system  \u201cwrite allocates\u201d the \u201cdirty\u201d block by, e.g., choosing a new VBN for a newly allocated block containing the unaltered data content, setting appropriate bits in block allocation structures and placing the chosen VBN into the writable vdisk indirect level 1 buffer, effectively filling the hole in the writable vdisk. Note that the block allocation structures include mapping data structures, such as an active map, a space map and a summary map, that are maintained by the file system and used by the write allocator as existing infrastructure for the volume. Block allocation data structures are described in U.S. Patent Application Publication No. US2002\/0083037 A1, titled Instant Snapshot, by Blake Lewis et al. and published on Jun. 27, 2002, which application is hereby incorporated by reference.","The write allocator then deletes the old data block and stores the new block to disk. This process is repeated for all instances of holes discovered in the indirect blocks is of the writable vdisk. Thereafter, the file system \u201creleases\u201d an association of the writable vdisk to the backing store by, e.g., deleting the backing store file handle  from the attributes inode  of the writable vdisk. In addition, the file system updates an appropriate entry of record  in the VTOC  to remove the backing store file handle reference.",{"@attributes":{"id":"p-0085","num":"0084"},"figref":"FIG. 11","b":["1100","1102","922","934","130","427","1104","1040","1040","1106","1108","1118","1110","1112","1118","1114","1040","626","1090","1116","628","620","1118","1120","1106","1122","846","840","672","670","1124"]},"Advantageously, the inventive technique uses backdoor messaging (i.e., the exchange of messages during periods of reduced processing activity in the file system) to separate the writable vdisk data blocks from the backing store, thereby obviating the need to take the writable vdisk data blocks offline. The LUN cloning technique preferably takes place entirely in the background through the use of such backdoor messaging. Thus the LUN cloning technique results in zero-downtime of the multi-protocol storage appliance and is transparent to client applications served by the storage appliance.","Indeed, in some cases, if a client application attempts to write new data to the writable vdisk while lun cloning is running in the background, the write operation will speed up the lun cloning process. New data written to the writable vdisk decreases the number of holes and thereby decreases the number of VBN \u201clookups\u201d that must be made to the backing store. Therefore, write activity may decrease the time it takes to \u201cclone\u201d the lun. Note that a further benefit of the novel cloning technique is that it enhances data read performance in certain situations. By separating the data blocks from the backing store to the writable vdisk, those data blocks may accessed without reference to the backing store. Elimination of this access operation enhances storage service performance.","The aforementioned novel LUN cloning technique may be initiated via user interface with a \u201clun clone\u201d command. The \u201clun clone\u201d command provides a human readable form for a user (system administrator) to specify a particular lun (vdisk) to be cloned, and provides status information to the user. The system administrator may issue the \u201clun clone\u201d command through the UI  of the multi-protocol storage appliance , where it is converted to primitives  that are executed by the file system . To initiate the LUN cloning technique, a \u201cstart\u201d option is used. An example of a \u201clun clone\u201d command including a start option is:\n\n","During the cloning process, the system administrator or user may access status information to determine the progress of the cloning operation by issuing the \u201clun clone\u201d command with a \u201cstatus\u201d option. For example,\n\n","The user may also access status information for all cloning operations by issuing the \u201clun clone\u201d command with a \u201cshow\u201d option. For example,\n\n","Finally, to terminate a LUN cloning operation while cloning is underway, the user may issue the \u201clun clone\u201d command with a \u201cstop\u201d option. For example,\n\n","The foregoing description has been directed to specific embodiments of this invention. It will be apparent, however, that other variations and modifications may be made to the described embodiments, with the attainment of some or all of their advantages. For instance, it is expressly contemplated that the teachings of this invention can is be implemented as software, including a computer-readable medium having program instructions executing on a computer, hardware, firmware, or a combination thereof. Accordingly this description is to be taken only by way of example and not to otherwise limit the scope of the invention. Therefore, it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The above and further advantages of the invention may be better understood by referring to the following description in conjunction with the accompanying drawings in which like reference numerals indicate identical or functionally similar elements:",{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 3","FIG. 2"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 10A"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 10B"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
