---
title: User interface devices for electrophysiology lab diagnostic and therapeutic equipment
abstract: In an electrophysiology (EP) lab, a bedside interface device allows an EP physician to directly control various diagnostic and therapeutic systems, including an electro-anatomic mapping system. The bedside interface device can include a computer with wireless communication capability as well as a touch-responsive display panel and voice recognition. The bedside interface device can also be a hand-graspable wireless remote control device that is configured to detect motions or gestures made with the remote control by the physician, allowing the physician to directly interact with the mapping system. The bedside interface device can also be a motion capture camera configured to determine motion patterns of the physician's arms, legs, trunk, face and the like, which are defined in advance to correspond to commands for the mapping system. The bedside interface device may also include voice recognition capabilities to allow a physician to directly issue verbal commands to the mapping system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09330497&OS=09330497&RS=09330497
owner: St. Jude Medical, Atrial Fibrillation Division, Inc.
number: 09330497
owner_city: St. Paul
owner_country: US
publication_date: 20110812
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","BRIEF SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["a. Field of the Invention","The instant disclosure relates generally to electrophysiology lab integration, and more particularly to user interfaces and devices therefore for electrophysiology lab diagnostic and therapeutic equipment.","b. Background Art","It is known to provide an electrophysiology lab in a medical facility. Such a lab may have use of a wide variety of diagnostic and therapeutic equipment useful in rendering medical service to a patient, such as imaging systems (e.g., fluoroscopy, intracardiac echocardiography, etc.), an electro-anatomic visualization, mapping and navigation system, ablation energy sources (e.g., radio frequency (RF) ablation generator), a recording system (e.g., for ECG, cardiac signals, etc.), a cardiac stimulator and the like. In a typical configuration, as seen by reference to , a procedure room  (i.e., a sterile environment) may have an associated control area or room , which is commonly outfitted with one or more control stations , , . . . that are operated by one or more control technicians. Each control station may include a respective display monitor, keyboard and mouse for use by the technician. Depending on the lab setup, the control station(s) may be across the room, or outside of the procedure room  completely, perhaps configured with a common window to allow the technician(s) to observe the procedure room through the window. These control station(s) allow access to and may be used to control the diagnostic and therapeutic equipment mentioned above.","In conventional practice, an electrophysiology (EP) physician  is scrubbed into a sterile procedure and typically manipulates one or more catheters (not shown) in a sterile drape covered body of the patient . The physician's sterile gloved hands are typically engaged with the catheter handle and shaft next to the patient and he or she is therefore unable to directly make changes himself to any of the EP systems. The procedure room  typically includes one or more monitors (e.g., an integrated multi-display monitor  is shown) arranged so that the physician  can see the monitor  on which is displayed various patient information being produced by the diagnostic and therapeutic equipment mentioned above. In , multiple applications, for example, an electro-anatomic mapping application (e.g., EnSite Velocity\u2122) and an EP signal acquisition and recording application, direct a visual output to a respective display area of monitor . When changes to an application are needed, the physician  verbalizes such commands to the control technicians in the control area\/room  who are working at the various control stations , , . . . . The multiple technicians at multiple control stations use multiple keyboard\/mouse sets to control the multiple applications. The verbal commands between the physician and the technician occur throughout the procedure.","For example, the EP physician  can verbally communicate (i.e., to the control technician\u2014a mapping system operator) the desired view of the map to be displayed, when to collect points, when to separate anatomic locations, and other details of creating and viewing an anatomic map. The EP physician  can also communicate which signal traces to show, the desired amplitude, when to drop a lesion marker, and when to record a segment, to name a few. Where the technician is in a separate room, communication can be facilitated using radio.","While some commands are straightforward, for example, \u201cLAO View\u201d, \u201crecord that\u201d and \u201cstop pacing\u201d, other commands are not as easy to clearly communicate. For example, how much rotation of a model the command \u201crotate a little to the right\u201d means can be different as between the physician and the technician. This type of command therefore involves a question of degree. Also, depending on the physician-technician relationship, other requests related to the mapping system views and setup can be misinterpreted. For example, a request to \u201crotate right\u201d may mean to rotate the model right (i.e., rotate view left) when originating from one physician but can alternatively mean rotate view right (i.e., rotate model left) when coming from another physician. This type of command therefore involves physician-technician agreement as to convention. Furthermore, implementation of requests for event markers, segment recordings, lesion markers and the like can be delayed by the time it takes the technician to hear, understand and act on a physician's command. Ambient discussions and\/or equipment noise in and around the EP lab can increase this delay.","There is therefore a need for improvements in EP lab integration that minimize or eliminate one or more problems are set forth above.","One advantage of the methods and apparatuses described, depicted and claimed herein is that they provide an EP physician with the capability of directly controlling an EP diagnostic or therapeutic system, such as an electro-anatomic mapping system. This capability eliminates the need for the physician to first communicate his\/her wishes to a control technician, who in turn must hear, interpret and act on the physician's command. The improved control paradigm results in reduced times for medical procedures.","A device for allowing a user to control an electro-anatomic mapping system includes an electronic control unit (ECU) and input means, using the ECU, for acquiring a user input with respect to a view of an anatomical model of at least a portion of a body of a patient. The user input is selected from the group comprising a user touch, a user multi-touch, a user gesture, a verbal command, a motion pattern of a user-controlled object, a user motion pattern and a user electroencephalogram. The ECU is configured to communicate the acquired input to the mapping system for further processing.","In an embodiment, the acquired user input can correspond to any of a variety of mapping systems commands, for example only at least one of: (1) creating a map with respect to the view; (2) collecting points with respect to the view; (3) segmenting regions by anatomy with respect to the view; (4) rotating the view; (5) enlarging or reducing a portion of the view; (6) panning the view; (7) selecting one of a plurality of maps for the view; (8) selecting a signal trace; (9) adjusting a signal amplitude; (10) adjusting a sweep speed; (11) recording a segment; (12) placing an event marker; (13) placing a lesion marker with respect to the view; (14) activating a replay feature of a stored, temporally varying physiologic parameter; and (15) activating a replay of a stored video clip.","In an embodiment, the input means includes a touch-responsive display panel coupled to the ECU. The input means also includes user interface logic (executed by the ECU) configured to display a user interface on the touch-responsive display panel. The user interface logic is further configured to allow a user to interact with the touch-responsive panel for acquiring the above-mentioned user input with respect to the anatomical model. The user interface in combination with the touch-panel allows the user to provide input by way of touch, multi-touch, and gesture. In a further embodiment, the device further includes voice recognition logic configured to recognize a set of predefined verbal commands spoken by the user (e.g., the physician). In a still further embodiment, the device includes wireless communications functionality, improving portability of the device within a procedure room or the control room. In a still further embodiment, the user interface logic is configured to present a plurality of application-specific user interfaces associated with a plurality of different diagnostic or therapeutic systems. Through this capability, the user can rapidly switch between application-specific user interfaces (e.g., such as that for an electro-anatomic mapping system, an EP recording system, an ultrasound imaging system, a cardiac stimulator, etc.), while remaining bedside of the patient, and without needing to communicate via a control technician.","In another embodiment, the input means includes a remote control having a handle configured to be grasped by the user. The remote control includes logic configured to acquire the above-mentioned user input. The user input may include user-controlled motion patterns of the remote control, as well as user key-presses on the remote control. The device is also configured to communicate the acquired user input to the mapping system.","In yet another embodiment, the input means includes a motion capture apparatus configured to acquire imaging of movements of the user. The device includes logic configured to identify a motion pattern using the acquired imaging from the motion capture apparatus. The logic is further configured to produce a command, based on the identified motion pattern, and communicate the command to the electro-anatomic mapping system for further processing. The motion capture apparatus provides the capability of receiving input by way of physician gestures (e.g., hand, arm, leg, trunk, facial, etc.). In a further embodiment, the device further includes voice recognition logic configured to identify verbal commands spoken by the user.","Corresponding methods are also presented.","The foregoing and other aspects, features, details, utilities, and advantages of the present disclosure will be apparent from reading the following description and claims, and from reviewing the accompanying drawings.","Referring now to the drawings wherein like reference numerals are used to identify identical or similar components in the various views,  is a diagrammatic overview of an electrophysiology (EP) laboratory in which embodiments of the present invention may be used.  shows a sterile procedure room  where an EP physician  is set to perform one or more diagnostic and\/or therapeutic procedures. It should be understood that the separate control area\/room  of  (not shown in ) may continue to be used in conjunction with the bedside interface device to be described below.  also shows multi-display monitor  as well as a procedure table or bed . While procedure room  may include multiple, individual monitors, monitor  may be a multi-display monitor configured to display a plurality of different input channels in respective display areas on the monitor. In an embodiment, the monitor  may be a commercially available product sold under the trade designation VantageView\u2122 from St. Jude Medical, Inc. of St. Paul, Minn., USA, which can have a 3840\u00d72160 Quad-HD screen resolution with the flexibility to accept up to sixteen (16) digital or analog image inputs while displaying up to eight (8) images on one screen at one time. The procedure table , which may be of conventional construction, is configured to receive a patient (not shown) on whom diagnostic and\/or therapeutic procedure(s) are to be performed.",{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 2","FIG. 1"],"b":["24","24","26","28","24","16","26","16","26"]},"The base interface  is configured to interpret and\/or facilitate directing the input acquired by the bedside interface device  to the appropriate one or more diagnostic and\/or therapeutic systems (e.g., an electro-anatomic mapping system). In an embodiment, base interface  is centralized (as shown), wherein all communications with bedside device  occur through base interface . In a further embodiment, base interface  may be functionally distributed, wherein interface functions are located within each diagnostic or therapeutic system. In a still further embodiment, communications between bedside interface  and certain ones of the diagnostic\/therapeutic systems can be centralized, while communications with other ones of the diagnostic\/therapeutic systems can occur directly (i.e., separately).","The means or apparatus  addresses a number of the shortcomings of the conventional practice as described in the Background. For example, means or apparatus  allows the EP physician  to directly input levels of degree, for example, how much to rotate a view, as opposed to trying to verbally communicate \u201chow much\u201d to a control technician. Further, the use of means or apparatus  avoids the potential confusion that can sometimes occur between the EP physician and the control technician as to convention (i.e., does \u201crotate right\u201d mean rotate the view or the model?). In addition, the use of means or apparatus  reduces or eliminates the inherent time delay between the time when the EP physician verbally issues a command and the time when the command is understood and acted upon by the technician.","With continued reference to , the physician  will typically have access to a plurality of diagnostic and\/or therapeutic systems in order to perform one or more medical procedures. In the illustrative embodiment, the physician  may have access to a first imaging system, such as a fluoroscopic imaging system , a second imaging system, such as an intracardiac ultrasound or echocardiography (ICE) imaging system , an electro-anatomic positioning, mapping, and visualization system , a further positioning system, such as a medical positioning system (magnetic-field based) , a patient data (electrophysiological (EP) data) monitoring and recording system , a cardiac stimulator , an EP data editing\/monitoring system  and an ablation system .  schematically shows a communication mechanism  which facilitates communication between and among the various systems described above. It should be understood, however, that the communications mechanism  may not necessarily function to enable communications between each and every system shown.","The fluoroscopic imaging system  may comprise conventional apparatus known in the art, for example, single plane or bi-plane configurations. A display area  that is shown on monitor  corresponds to the display output of fluoroscopic imaging system .","The intracardiac ultrasound and\/or intracardiac echocardiography (ICE) imaging system  may also comprise conventional apparatus known in the art. For example, in one embodiment, the system  may comprise a commercial system available under the trade designation ViewMate\u2122 Z intracardiac ultrasound system compatible with a ViewFlex\u2122 PLUS intracardiac echocardiography (ICE) catheter, from St. Jude Medical, Inc. of St. Paul, Minn., USA. The system  is configured to provide real-time image guidance and visualization, for example, of the cardiac anatomy. Such high fidelity images can be used to help direct diagnosis or therapy during complex electrophysiology procedures. A display area  that is shown on monitor  corresponds to the display output of the ultrasound imaging system .","The system  is configured to provide many advanced features, such as visualization, mapping, navigation support and positioning (i.e., determine a position and orientation (P&O) of a sensor-equipped medical device, for example, a P&O of a distal tip portion of a catheter). Such functionality can be provided as part of a larger visualization, mapping and navigation system, for example, an ENSITE VELOCITY\u2122 cardiac electro-anatomic mapping system running a version of EnSite NavX\u2122 navigation and visualization technology software commercially available from St. Jude Medical, Inc., of St. Paul, Minn. and as also seen generally by reference to U.S. Pat. No. 7,263,397 entitled \u201cMETHOD AND APPARATUS FOR CATHETER NAVIGATION AND LOCATION AND MAPPING IN THE HEART\u201d to Hauck et al., or U.S. Patent Publication No. 2007\/0060833 A1 to Hauck entitled \u201cMETHOD OF SCALING NAVIGATION SIGNALS TO ACCOUNT FOR IMPEDANCE DRIFT IN TISSUE\u201d, both owned by the common assignee of the present invention, and both hereby incorporated by reference in their entireties as though fully set forth herein. System  can be configured to perform further advanced functions, such as motion compensation and adjustment functions. Motion compensation may include, for example, compensation for respiration-induced patient body movement, as described in copending U.S. patent application Ser. No. 12\/980,515, entitled \u201cDYNAMIC ADAPTIVE RESPIRATION COMPENSATION WITH AUTOMATIC GAIN CONTROL\u201d, which is hereby incorporated by reference in its entirety as though fully set forth herein. System  can be used in connection with or for various medical procedures, for example, EP studies or cardiac ablation procedures.","System  is further configured to generate and display three dimensional (3D) cardiac chamber geometries or models, display activation timing and voltage data to identify arrhythmias, and to generally facilitate guidance of catheter movement in the body of the patient. For example, a display area  that is shown on monitor  corresponds to the display output of system , can be viewed by physician  during a procedure, which can visually communicate information of interest or need to the physician. The display area  in  shows a 3D cardiac model, which, as will be described below in greater detail, may be modified (i.e., rotated, zoomed, etc.) pursuant to commands given directly by physician  via the bedside interface device .","System  is configured to provide positioning information with respect to suitably configured medical devices (i.e., those including a positioning sensor). System  may use, at least in part, a magnetic field based localization technology, comprising conventional apparatus known in the art, for example, as seen by reference to U.S. Pat. No. 7,386,339 entitled \u201cMEDICAL IMAGING AND NAVIGATION SYSTEM\u201d, U.S. Pat. No. 6,233,476 entitled \u201cMEDICAL POSITIONING SYSTEM\u201d, and U.S. Pat. No. 7,197,354 entitled \u201cSYSTEM FOR DETERMINING THE POSITION AND ORIENTATION OF A CATHETER\u201d, all of which are hereby incorporated by reference in their entirety as though fully set forth herein. System  may comprise a gMPS\u2122 medical positioning system commercially offered by MediGuide Ltd. of Haifa, Israel and now owned by St. Jude Medical, Inc. of St. Paul, Minn., USA. System  may alternatively comprise variants, which employ magnetic field generator operation, at least in part, such as a combination magnetic field and current field-based system such as the CARTO\u2122 3 System available from Biosense Webster, and as generally shown with reference to one or more of U.S. Pat. Nos. 6,498,944 entitled \u201cIntrabody Measurement,\u201d 6,788,967 entitled \u201cMedical Diagnosis, Treatment and Imaging Systems,\u201d and 6,690,963 entitled \u201cSystem and Method for Determining the Location and Orientation of an Invasive Medical Instrument,\u201d the entire disclosures of which are incorporated herein by reference as though fully set forth herein.","EP monitoring and recording system  is configured to receive, digitize, display and store electrocardiograms, invasive blood pressure waveforms, marker channels, and ablation data. System  may comprise conventional apparatus known in the art. In one embodiment, system  may comprise a commercially available product sold under the trade designation EP-WorkMate\u2122 from St. Jude Medical, Inc. of St. Paul, Minn., USA. The system  can be configured to record a large number of intracardiac channels, may be further configured with an integrated cardiac stimulator (shown in  as stimulator ), as well as offering storage and retrieval capabilities of an extensive database of patient information. Display areas ,  shown on monitor  correspond to the display output of EP monitoring and recording system .","Cardiac stimulator  is configured to provide electrical stimulation of the heart during EP studies. Stimulator  can be provided in either a stand-alone configuration, or can be integrated with EP monitoring and recording system , as shown in . Stimulator  is configured to allow the user to initiate or terminate tachy-arrhythmias manually or automatically using preprogrammed modes of operation. Stimulator  may comprise conventional apparatus known in the art. In an embodiment, stimulator  can comprise a commercially available cardiac stimulator sold under the trade designation EP-4\u2122 available from St. Jude Medical, Inc. of St. Paul, Minn., USA. The display area  shown on monitor  corresponds to the display output of the cardiac stimulator .","EP data editing\/monitoring system  is configured to allow editing and monitoring of patient data (EP data), as well as charting, analysis, and other functions. System  can be configured for connection to EP data recording system  for real-time patient charting, physiological monitoring, and data analysis during EP studies\/procedures. System  may comprise conventional apparatus known in the art. In an embodiment, system  may comprise a commercially available product sold under the trade designation EP-NurseMate\u2122 available from St. Jude Medical, Inc. of St. Paul, Minn., USA.","To the extent the medical procedure involves tissue ablation (e.g., cardiac tissue ablation), ablation system  can be provided. The ablation system  may be configured with various types of ablation energy sources that can be used in or by a catheter, such as radio-frequency (RF), ultrasound (e.g. acoustic\/ultrasound or HIFU), laser, microwave, cryogenic, chemical, photo-chemical or other energy used (or combinations and\/or hybrids thereof) for performing ablative procedures. RF ablation embodiments may and typically will include other structure(s) not shown in , such as one or more body surface electrodes (skin patches) for application onto the body of a patient (e.g., an RF dispersive indifferent electrode\/patch), an irrigation fluid source (gravity feed or pump), and an RF ablation generator (e.g., such as a commercially available unit sold under the model number IBI-1500T RF Cardiac Ablation Generator, available from St. Jude Medical, Inc.).",{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 3A","FIG. 2","FIG. 3A","FIG. 2"],"b":["26","26","26","60","62","64","62","60","66","68","68","62","60","26","28","26","26","16","10","12","26"],"i":["a","a ","a","a ","a ","a ","a "]},"In the illustrated embodiment, the UI logic  is configured to present a plurality of application-specific user interfaces, each configured to allow a user (e.g., the EP physician ) to interact with a respective one of a plurality of diagnostic and\/or therapeutic systems (and their unique interface or control applications). As shown in , the UI logic  is configured to present on the touch panel surface of computer a plurality of touch-sensitive objects (i.e., \u201cbuttons\u201d, \u201cflattened joystick\u201d, etc), to be described below. In the illustrative embodiment, the UI logic  produces a first, application-selection group of buttons, designated as group , and which are located near the top of the touch panel. Each of the buttons in group  are associated with a respective diagnostic and\/or therapeutic system (and control or interface application therefore). For example, the six buttons labeled \u201cEnSite\u201d, \u201cWorkMate\u201d, \u201cEP4\u201d, \u201cNurseMate\u201d, \u201cMediGuide\u201d, \u201cViewMate\u201d correspond to electro-anatomic mapping system  (for mapping control), EP recording system  (for patient data recording control), stimulator  (for stimulator control), EP data editing and monitoring system  (for charting) and ultrasound imaging system  (for ultrasound control), respectively.","When a user selects one of the buttons in group , the UI logic  configures the screen display of computer with an application-specific user interface tailored for the control of and interface with the particular EP system selected by the user. In , the \u201cEnSite\u201d system is selected, so the UI logic  alters the visual appearance of the \u201cEnSite\u201d button so that it is visually distinguishable from the other, non-selected buttons in group . For example, when selected, the \u201cEnSite\u201d button may appear depressed or otherwise shaded differently than the other, non-selected buttons in group . This always lets the user know what system is selected. The UI logic , in an embodiment, also maintains the application-selection buttons in group  at the top of the screen regardless of the particular application selected by the user. This arrangement allows the user to move from system (application) to system (application) quickly and control each one independently.","With continued reference to , UI logic  presents an application-specific user interface tailored and optimized for control of and interaction with system . This user interface includes a second, common-task group of selectable buttons, designated group , a third, view-mode group of selectable buttons, designated group , a fourth, view-select group of selectable buttons, designated group , a flattened joystick  configured to receive view-manipulation input from the user, a voice recognition control button , and a settings button . Each group will be addressed in turn.","The second group  of buttons includes a listing of common tasks performed by an EP physician when interacting with system . Each of the buttons in group  are associated with a respective task (and resulting action). For example, the five buttons in group  are labeled \u201cZoom In\u201d, \u201cZoom Out\u201d, \u201cAdd Lesion\u201d, \u201cFreeze Point\u201d, and \u201cSave Point\u201d. The \u201cZoom In\u201d and \u201cZoom Out\u201d buttons allow the user to adjust the apparent size of the 3D model displayed on monitor  (i.e., enlarging or reducing the 3D model on the monitor).","For example,  is a view of the monitor  of , showing multiple inset displays for different applications, where the display area (window) shows the EnSite\u2122 display output of a 3D electro-anatomic model at a first magnification level.  is a further view of monitor , showing a zoomed-in view of the same display area (window), now designated , which has an increased magnification level and thus apparent size. This change of course allows the physician to see details in window that may not be easy to see in window .","Referring again to , the \u201cAdd Lesion\u201d button is configured to add a lesion marker to the 3D model. Other commands can be also be executed using the \u201cFreeze Point\u201d and \u201cSave Point\u201d buttons. It should be understood that variations are possible.","Each of the buttons in group  are associated with a respective display mode, which alters the display output of system  to suit the wishes of the physician. For example, the three selectable buttons labeled \u201cDual View\u201d, \u201cRight View\u201d, and \u201cMap View\u201d re-configure the display output of system , as will appear on monitor .","Each of the buttons in group  are associated with a respective viewpoint from which the 3D electro-anatomic model is \u201cviewed\u201d (i.e., as shown in window  on monitor ). Three of the five selectable buttons, namely those labeled \u201cLAO\u201d, \u201cAP\u201d, and \u201cRAO\u201d, allow the user to reconfigure the view point from which the 3D electro-anatomic model is viewed (i.e., left anterior oblique, anterior-posterior, right anterior oblique, respectively). The remaining two buttons, namely those labeled \u201cCenter at Surface\u201d and \u201cCenter at Electrode\u201d allow the user to invoke, respectively, the following functions: (1) center the anatomy shape in the middle of the viewing area; and (2) center the current mapping electrode or electrodes in the middle of the viewing area.","The flattened joystick  is a screen object that allows the user to rotate the 3D model displayed in the window . In addition, as the point of contact (i.e., physician's finger) with the joystick object  moves from the center or neutral position, for example at point , towards the outer perimeter (e.g., through point  to point ), the magnitude of the input action increases. For example, the acceleration of rotation of the model or cursor will increase. While  shows the joystick object  as having three (3) gradations or concentric bands, it should be appreciated that this is for clarity only and not limiting in number. For example, in an embodiment, a relatively larger number of gradations or bands, such as ten (10), may be provided so as to effectively provide for a substantially continuous increase in sensitivity (or magnitude) as the point of contact moves toward the outer radius. In another embodiment, a single gradient may be continuous from the center position, point , to the outer edge of the joystick object , with the centermost portion of the gradient being the brightest in intensity or color and the outermost portion of the gradient being the darkest in intensity or color, for example. In yet another embodiment, a single gradient may be continuous from the center position, point , to the outer edge of the joystick object , with the centermost portion of the gradient being the darkest in intensity or color and the outermost portion of the gradient being brightest in intensity or color, for example.","In a further embodiment, UI logic  can be further configured to present an additional button labeled \u201cFollow Me\u201d (not shown), which, when selected by the user, configures the electro-anatomic mapping system  for \u201cfollow me\u201d control. This style of control is not currently available using a conventional keyboard and mouse interface. For \u201cfollow me\u201d control, UI logic  is configured to receive a rotation input from the user via the touch panel (e.g., joystick ); however, the received input is interpreted by system  as a request to rotate the endocardial surface rendering (the \u201cmap\u201d) while maintaining the mapping catheter still or stationary on the display. In an embodiment, the physician can set the position and orientation of the mapping catheter, where it will remain stationary after the \u201cFollow Me\u201d button is selected.","Another feature of the touch panel computer is that it incorporates, in an embodiment, voice recognition technology. As described above, computer includes microphone  for capturing speech (audio) and voice recognition logic  for analyzing the captured speech to extract or identify spoken commands. The voice recognition feature can be used in combination with the touch panel functionality of computer . The microphone  may comprise conventional apparatus known in the art, and can be a voice recognition optimized microphone particularly adapted for use in speech recognition applications (e.g., an echo-cancelling microphone). Voice recognition logic  may comprise conventional apparatus known in the art. In an embodiment, voice recognition logic  may be a commercially available component, such as software available under the trade designation DRAGON DICTATION\u2122 speech recognition software.","In an embodiment, computer is configured to recognize a defined set of words or phrases adapted to control various functions of the multiple applications that are accessible or controllable by computer . The voice recognition feature can itself be configured to recognize unique words or phrases to selectively enable or disable the voice recognition feature. Alternatively (or in addition to), a button, such as button  in , can be used to enable or disable the voice recognition feature. In this regard, the enable\/disable button can be either a touch-sensitive button (i.e., screen object), or can be hardware button.","Voice recognition logic  is configured to interact with the physician or other user to \u201ctrain\u201d the logic (e.g., having the user speak known words) so as to improve word and\/or phrase recognition. The particulars for each user so trained can be stored in a respective voice (user) profile, stored in memory . For example, in , the currently active voice profile is listed in dashed-line box . In an embodiment, each user can have unique commands, which may also be stored in the respective voice profile. In a further embodiment, the language need not be English, and can be other languages. This flexibility as to language choice enlarges the audience of users who can use the device . The voice recognition feature presents a number of advantages, including the fact that the physician  does not have to remove his\/her hands from the catheter or other medical device being manipulated. In addition, the absence of contact or need to touch computer maintains a sterile condition. The voice recognition feature can also be used either alone or in combination with other technologies.","With continued reference to , UI logic  also presents a \u201cSettings\u201d button . When the \u201cSettings\u201d button  is selected, UI logic  generates another screen display that allows the user to adjust and\/or set\/reset various settings associated with the application currently selected. In an embodiment, the \u201cSettings\u201d button can also allow adjustment of parameters that are more global in nature (i.e., apply to more than one application). For example only, through \u201cSettings\u201d, the physician or another user can edit all of the phrases associated with a particular physician or specify a timeout (i.e., the elapsed amount of time, after which the computer will stop listening (or not) for voice commands). The physician or another user can also edit miscellaneous parameters, such as communication settings and the like.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":["FIG. 3B","FIG. 3A"],"b":["88","26","88"],"i":"a "},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 5","b":["26","38","64","70","90","70","92","94","96","98","64","92","94","96","98"],"i":"a "},{"@attributes":{"id":"p-0063","num":"0062"},"figref":["FIG. 6","FIG. 6"],"b":["26","32","70","100","101","64"],"i":"a "},"It should be understood that variations in UI logic  are possible. For example, certain applications can be linked (in software) so that multiple applications can be controlled with a single command (e.g., the Record command). In another embodiment, UI logic  can be configured to provide additional and\/or substitute functions, such as, without limitation, (1) map creation; (2) collecting points; (3) segmenting regions by anatomy; (4) map view (rotate and zoom); (5) select\/manipulate a number of maps and view each; (6) selection of signal trace display; (7) adjust EP signal amplitude; (8) sweep speed; (9) provide single button (or touch, multi-touch, gesture) for recording a segment, placing an event marker, and\/or placing a lesion marker.","It should be further understood that the screen layouts in the illustrative embodiment are exemplary only and not limiting in nature. The UI logic  can thus implement alternative screen layouts for interaction by the user. For example, while the screen displays in  show an approach that incorporates the top level menu items on every screen, multi-level menus can also be used. For example, the screen layouts can be arranged such that a user descends down a series of screens to further levels of control. To return to upper levels (and to the \u201chome\u201d screen), a \u201cBack\u201d button or the like can be provided. Alternatively, a \u201cHome\u201d button can be provided.","In a still further embodiment, UI logic  can be configured for bi-directional display of information, for example, on the touch-responsive display panel. As one example, the \u201cEnSite\u201d user interface () can be configured so that the EnSite\u2122 model is sent to the computer and displayed on the touch-responsive display panel. The user interface provided by UI logic  can allow the user to drag his or her finger on the panel to rotate the model. The display of the model provides context with respect to the act of dragging. Other information can be displayed as well, such as a waveform. In various embodiments, all or a portion of the items\/windows displayed on monitor  (see, e.g., ) may be displayed or mirrored on the touch-responsive display panel. For example, display area or window  may be displayed on the touch-responsive display panel allowing the physician or other user to directly modify the features of window  at the patient's bedside. Other display areas\/windows, such as windows , , , , and\/or  (see ) may also be displayed and\/or modified on the touch-panel display panel. One further example involves displaying feedback information or messages originating from the various devices or systems back to the touch-responsive display panel. In this regard, the UI logic  can configure any of the user-interfaces to have a message area, which can show informational messages, warning messages or critical error messages for viewing by the user. The message area feature provides a way to immediately alert the physician to such messages, rather than the physician having to watch for messages on multiple displays.",{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 7A","b":["26","26","26","34","26","102","104","28","26"],"i":["b","a","b ","b ","b","b "]},"Since the wand system is contemplated as being used in the sterile procedure room, multiple embodiments are contemplated for avoiding contamination. In this regard, wand system may be configured with a disposable remote control portion , with a reusable remote control portion  that is contained within an enclosure compatible with sterilization procedures, with a reusable remote control portion  adapted to be secured in a sterilization-compatible wrapper, or with a reusable remote control portion  that is encased in a sterile but disposable wrapper.","With continued reference to , remote control portion  may include an optical detector , an electronic processor , a memory , an optional accelerometer  and a wireless transmitter\/receiver . The processor  is configured to execute a control program that is stored in memory , to achieve the functions described below. The optical emitter  is configured to emit a light pattern  that can be detected and recognized by optical detector . For example, the light pattern may be a pair of light sources spaced apart by a predetermined, known distance. The control program in remote  can be configured to assess movement of the light pattern  as detected by detector  (e.g., by assessing a time-based sequence of images captured by detector ). For example, in the exemplary light pattern described above, processor  can be configured to determine the locations of the light sources (in pixel space). In an embodiment, the control program in remote  may only discern the light pattern  itself (e.g., the locations in pixel space) and transmit this information to base interface , which in turn assesses the movement of the detected light pattern in order to arrive at a description of the motion of the remote . In a still further embodiment, various aspects of the processing may be divided between processor  and a processor (not shown) contained in base interface . The processor  communicates with base interface via the wireless transmitter\/receiver , which may be any type of wireless communication method now known or hereafter developed (e.g., such as those technologies or standards branded Bluetooth\u2122, Wi-Fi\u2122, etc.). The processor  is configured to transmit wirelessly to interface the detected keypresses and information concerning the motion of the remote control  (e.g., the information about or derived from the images from the optical detector ). In an embodiment, the motion of remote control  may also be determined, or supplemented by, readings from accelerometer  (which may be single-axis or multi-axis, such as a 3-axis accelerometer). In some instances, rapid motion may be better detected using an accelerometer than using optical methods. In an embodiment, electronic wand system may be similar to (but differing in application, as described herein) a commercially available game controller sold under the trade designation Wii Remote Controller, from Nintendo of America, Inc.","Either the remote  or the base interface (or both, potentially in some division of computing labor) is configured to identify a command applicable to the one of the EP diagnostic\/therapeutic systems, such as electro-anatomic mapping system , based on the detected motion of the remote . Alternatively, the command may be identified based on a key press, or a predetermined motion\/key press combination. Once the remote  and\/or interface identifies the command it is transmitted to the appropriate EP system. In an electro-anatomic mapping system embodiment, the wireless remote control  is configured to allow an EP physician to issues a wide variety of commands, for example only, any of the commands (e.g., 3D model rotation, manipulation, etc.) described above in connection with touch panel computer . By encoding at least some of the control through the wireless remote control  that the EP physician controls, one or more of the shortcomings of conventional EP labs, as described in the Background, can be minimized or eliminated. As with touch panel computer , electronic wand system can reduce procedure times as the EP physician will spend less time playing \u201chot or cold\u201d with the mapping system operator (i.e., the control technician), but instead can set the display to his\/her needs throughout the medical procedure.",{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 7B","b":["26","26","102","115","115"],"i":"c"},{"@attributes":{"id":"p-0072","num":"0071"},"figref":"FIG. 8","b":["26","26","26","26","26","34","26","116","118","120","116","28","116","122","28"],"i":["d","a","b ","c","d ","d ","b","b "]},"The motion capture apparatus includes the capability to detect hand\/arm\/leg\/trunk\/facial motions (e.g., gestures) of the EP physician or other user and translate the detected patterns into a desired command. Apparatus also includes audio capture and processing capability and thus also has the capability to detect speech and translate the same into desired commands. In an embodiment, apparatus is configured to detect and interpret combinations and sequences of gestures and speech into desired commands. The base interface is configured to communicate the commands (e.g., rotation, zoom, pan of a 3D anatomical model) to the appropriate EP diagnostic or therapeutic system (e.g., the electro-anatomic mapping system ). In an embodiment, the motion capture apparatus may comprise commercially available components, for example, the Kinect\u2122 game control system, available from Microsoft, Redmond, Wash., USA. A so-called Kinect\u2122 software development kit (SDK) is available, which includes drivers, rich application programming interfaces (API's), among other things contents, that enables access to the capabilities of the Kinect\u2122 device. In particular, the SDK allows access to raw sensor streams (e.g., depth sensor, color camera sensor, and four-element microphone array), skeletal tracking, advanced audio (i.e., integration with Windows speech recognition) as well as other features.","Since there is no contact contemplated by EP physician  during use of motion capture apparatus , contamination and subsequent sterilization issues are eliminated or reduced. In addition, the lack of contact with apparatus for control purposes allows the EP physician to keep his hands on the catheter or other medical device(s) being manipulated during an EP procedure. By encoding at least some of the control through the motion capture apparatus , with which the EP physician interacts, one or more of the shortcomings of conventional EP labs, as described in the Background, can be minimized or eliminated. As with the previous embodiments, the motion capture apparatus can reduce procedure times.","It should be understood that variations are possible. For example, the motion capture apparatus can be used in concert with sensors and\/or emitters in a sterile glove to assist the apparatus to discriminate commands intended to be directed to one of the EP systems, versus EP physician hand movements that result from his\/her manipulation of the catheter or medical device, versus other movement in the EP lab in general. In another embodiment, the motion capture apparatus may discriminate such commands by being \u201cactivated\u201d by a user when a specific verbal command is issued (e.g., \u201cmotion capture on\u201d) and then \u201cdeactivated\u201d by the user when another specific verbal command is issued (e.g., \u201cmotion capture off\u201d).",{"@attributes":{"id":"p-0076","num":"0075"},"figref":["FIGS. 9-10","FIGS. 9 and 10"],"b":["26","26","16","26","26"],"i":["e ","f","e","f "]},{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 11","b":["26","26","26","34"],"i":["g","g ","g "]},{"@attributes":{"id":"p-0078","num":"0077"},"figref":"FIGS. 12-13","b":["26","124","26","26","126","26","26"],"i":["h ","h ","i ","h","i "]},"It should be understood that variations are possible. For example, in a further embodiment, primary control by the physician in manipulating or interacting with the mapping system may be through use of voice control alone (i.e., a microphone coupled with voice recognition logic), apart from its inclusion with other modes or devices for user interaction described above. In a still further embodiment, the physician can be equipped with headgear that monitors head movements to determine at what location on the screen\/monitor the physician is looking. In effect, such headgear can act as a trackball to move or otherwise manipulate an image (or view of a model) on the monitor in accordance with the physician's head movements. In a yet further embodiment, the physician can be equipped with headgear that monitors head movements and\/or also monitors brainwave patterns (e.g., to record a user electroencephalogram (EEG)). Such monitored data can be analyzed to derive or infer user input or commands for controlling an image (or view of a model), as described above. An EEG-based embodiment may comprise conventional apparatus known in the art, for example, commercially available products respectively sold under the trade designation MindWave\u2122 headset from NeuroSky, Inc., San Jose, Calif., USA, or the Emotiv EPOC\u2122 personal interface neuroheadset from Emotiv, Kwun Tong, Hong Kong. In a still further embodiment, the physician can be equipped with an eye tracking apparatus, wherein monitored eye movements constitute the user input to be interpreted by the system (e.g., the eye movements can be interpreted as a cursor movement or other command).","It should also be appreciated that while the foregoing description pertains to an EP physician manually controlling a catheter through the use of a manually-actuated handle or the like, other configurations are possible, such as robotically-actuated embodiments. For example, a catheter movement controller (not shown) described above may be incorporated into a larger robotic catheter guidance and control system, for example, as seen by reference to U.S. application Ser. No. 12\/751,843 filed Mar. 31, 2010 entitled ROBOTIC CATHETER SYSTEM (published as U.S. patent application publication no. 2010\/0256558), owned by the common assignee of the present invention and hereby incorporated by reference in its entirety as though fully set forth herein. Such a robotic catheter system may be configured to manipulate and maneuver catheters within a lumen or a cavity of a human body, while the bedside interface devices described herein can be used to access and control the EP diagnostic and\/or therapeutic systems. In at least one embodiment, a bedside interface device as described herein may also be used to access and control the robotic catheter system.","In accordance with another embodiment, an article of manufacture includes a computer storage medium having a computer program encoded thereon, where the computer program includes code for acquiring user input based on at least one of a plurality of input modes, such as by touch, multi-touch, gesture, motion pattern, voice recognition and the like, and identifying one or more commands or requests for an EP diagnostic and\/or therapeutic system. Such embodiments may be configured to execute one or more processors, multiple processors that are integrated into a single system or are distributed over and connected together through a communications network, and where the network may be wired or wireless.","It should be understood that while the foregoing description describes various embodiments of a bedside interface device in the context of the practice of electrophysiology, and specifically catheterization, the teachings are not so limited and can be applied to other clinical settings.","It should be understood that the an electronic control unit as described above may include conventional processing apparatus known in the art, capable of executing pre-programmed instructions stored in an associated memory, all performing in accordance with the functionality described herein. It is contemplated that the methods described herein may be programmed, with the resulting software being stored in an associated memory and where so described, may also constitute the means for performing such methods. Implementation of an embodiment of the invention, in software, in view of the foregoing enabling description, would require no more than routine application of programming skills by one of ordinary skill in the art. Such a system may further be of the type having both ROM, RAM, a combination of non-volatile and volatile (modifiable) memory so that the software can be stored and yet allow storage and processing of dynamically produced data and\/or signals.","Although numerous embodiments of this invention have been described above with a certain degree of particularity, those skilled in the art could make numerous alterations to the disclosed embodiments without departing from the spirit or scope of this invention. All directional references (e.g., plus, minus, upper, lower, upward, downward, left, right, leftward, rightward, top, bottom, above, below, vertical, horizontal, clockwise, and counterclockwise) are only used for identification purposes to aid the reader's understanding of the present invention, and do not create limitations, particularly as to the position, orientation, or use of the invention. Joinder references (e.g., attached, coupled, connected, and the like) are to be construed broadly and may include intermediate members between a connection of elements and relative movement between elements. As such, joinder references do not necessarily infer that two elements are directly connected and in fixed relation to each other. It is intended that all matter contained in the above description or shown in the accompanying drawings shall be interpreted as illustrative only and not limiting. Changes in detail or structure may be made without departing from the spirit of the invention as defined in the appended claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 3A","FIG. 2"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 3B","FIG. 3A"]},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 4A","FIG. 2"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 4B","FIG. 4A"]},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 5","FIG. 3A"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 6","FIG. 3A"]},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 7A"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 7B","FIG. 7A"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIGS. 9-10"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIGS. 12-13"}]},"DETDESC":[{},{}]}
