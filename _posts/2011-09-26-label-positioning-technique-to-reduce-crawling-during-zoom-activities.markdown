---
title: Label positioning technique to reduce crawling during zoom activities
abstract: A graphics or image rendering system, such as a map image rendering system, receives image data from an image database in the form of vector data that defines various features of the map, such as roads, boundaries, etc., in addition to text strings or symbols to be displayed with the features to provide, for example, labels for the features. The label positioning technique generally divides the displayed road feature into a number of road segments and then labels each road segment in a sequence based on an ordered set of labels and a position of each road segment.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08237745&OS=08237745&RS=08237745
owner: Google Inc.
number: 08237745
owner_city: Mountain View
owner_country: US
publication_date: 20110926
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF TECHNOLOGY","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The present disclosure relates to image rendering systems, such as electronic map display systems, and more specifically to an image rendering engine that renders a label of a feature of an electronic map in a manner that reduces a crawling motion of the labels while changing zooming levels of the map.","Digital maps are found in and may be displayed by a wide variety of devices, including mobile phones, car navigation systems, hand-held GPS units, computers, and many websites. Although digital maps are easy to view and to use from an end-user's perspective, creating a digital map is a difficult task and can be a time-consuming process. In particular, every digital map begins with storing, in a map database, a set of raw data corresponding to millions of streets and intersections and other features to be displayed as part of a map. The raw map data that is stored in the map database and that is used to generate digital map images is derived from a variety of sources, with each source typically providing different amounts and types of information. This map data must therefore be compiled and stored in the map database before being accessed by map display or map rendering applications and hardware.","There are, of course, different manners of digitally rendering map images (referred to as digital map images) based on map data stored in a map database. One method of rendering a map image is to store map images within the map database as sets of raster or pixilated images made up of numerous pixel data points, with each pixel data point including properties defining how a particular pixel in an image is to be displayed on an electronic display device. While this type of map data is relatively easy to create and store, the map rendering technique using this data typically requires a large amount of storage space for comprehensive digital map images, and it is difficult to manipulate the digital map images as displayed on a display device in very many useful manners.","Another, more flexible methodology of rendering images uses what is traditionally called vector image data. Vector image data is typically used in high-resolution and fast-moving imaging systems, such as those associated with gaming systems, and in particular three-dimensional gaming systems. Generally speaking, vector image data (or vector data) includes data that defines specific image objects or elements (also referred to as primitives) to be displayed as part of an image via an image display device. In the context of a map image, such image elements or primitives may be, for example, individual roads, text labels, areas, text boxes, buildings, points of interest markers, terrain features, bike paths, map or street labels, etc. Each image element may generally be drawn as a set of one or more triangles (of different sizes, shapes, colors, fill patterns, etc.), with each triangle including three vertices interconnected by lines. For any particular image element, the image database may store a set of vertex data points that may be used to generate one or more of the triangles. Generally speaking, each vertex data point includes data pertaining to a two-dimensional or a three-dimensional position of the vertex (in an X, Y or an X, Y, Z coordinate system, for example) and various vertex attributes defining properties of the vertex, such as color properties, fill properties, line width properties for lines emanating from the vertex, etc.","During the image rendering process, the vertices defined for various image elements of an image to be rendered are provided to and are processed in one or more image shaders which operate in conjunction with a graphics processing unit (GPU), such as a graphics card or a rasterizer, to produce a two-dimensional image on a display screen. Generally speaking, an image shader is a set of software instructions used primarily to calculate rendering effects on graphics hardware with a high degree of flexibility. Image shaders are well known in the art and various types of image shaders are available in various application programming interfaces (APIs) provided by, for example, OpenGL and Direct3D, to define special shading functions. Basically, image shaders are simple programs in a high level programming language that describe or determine the traits of either a vertex or a pixel. Vertex shaders, for example, define the traits (e.g., position, texture coordinates, colors, etc.) of a vertex, while pixel or fragment shaders define the traits (color, z-depth and alpha value) of a pixel. A vertex shader is called for each vertex in an image element or primitive so that, for each vertex input into the vertex shader, the vertex shader produces one (updated) vertex output. Each vertex output by the vertex shader is then rendered as a series of pixels onto a block of memory that will eventually be sent to a display screen.","As a more particular example of image shader technology, Direct3D and OpenGL graphic libraries use three basic types of shaders including vertex shaders, geometry shaders, and pixel or fragment shaders. Vertex shaders are run once for each vertex given to the graphics processor. As noted above, the purpose of a vertex shader is to transform a position of a vertex in a virtual space to the two-dimensional coordinate at which it appears on the display screen (as well as a depth value for the z-buffer of the graphics processor). Vertex shaders can manipulate properties such as position, color, and texture coordinates by setting vertex attributes of the vertices, but cannot create new vertices. The output of the vertex shader is provided to the next stage in the processing pipeline, which is either a geometry shader if present or the rasterizer. Geometry shaders can add and remove vertices from a mesh of vertices and can be used to generate image geometry procedurally or to add volumetric detail to existing images that would be too costly to process on a central processing unit (CPU). If geometry shaders are being used, the output is then sent to the rasterizer. Pixel shaders, which are also known as fragment shaders, calculate the color and light properties of individual pixels in an image. The input to this stage comes from the rasterizer, and the fragment shaders operate to fill in the pixel values of the polygons being sent through the graphics pipeline. Fragment shaders are typically used for scene lighting and related effects such as color toning. There is not a one-to-one relationship between calls to the fragment shader and pixels on the screen as fragment shaders are often called many times per pixel because they are called for every image element or object that is in the corresponding space, even if that image object is occluded.","The use of vector graphics can be particularly advantageous in a mobile map system in which image data is sent from a centralized map database via a communications network (such as the Internet, a wireless network, etc.) to one or more mobile or remote devices for display. In particular, vector data, once sent to the receiving device, may be more easily scaled and manipulated (e.g., rotated, etc.) than pixilated raster image data. However, the processing of vector data is typically much more time consuming and processor intensive on the image rendering system that receives the data. Moreover, using vector image data that provides a higher level of detail or information to be displayed in a map leads to a higher amount of vector data or vertices that need to be sent to the map rendering system from the map database that stores this information, which can result in higher bandwidth requirements or downloading time in some cases.","Moreover, in the case of both rasterized map images and vector data generated images, text or other legend symbols, such as those used to label streets, roads and other features of a map, are generally placed on or proximately near to a map object for which the label identifies or provides information. Generally, each time a zoom level of a computerized map is changed or adjusted, labels may be redrawn. The redrawing of labels during zooming may result in a visually distracting crawling motion or effect where the labels are perceived to move along a dimension of its associated map feature.","A computer-implemented method for rendering a map on a display device determines a road feature to display in a viewing window having a first zoom level. The method determines an ordered set of labels to be displayed on the road feature and labels the road feature in the following manner. First, a segment of the road feature is determined for applying a label. Second, a subsequent label of the ordered set of labels is placed at the center of the determined road segment if the length of the road segment fits the length of the subsequent label. Third, a pair of resultant segments is determined if the subsequent label is placed. For each resultant segment produced, the first, second, and third steps are repeated until a stop condition is reached.","In one embodiment, an order in which each pair of resultant segments is placed is based on the label placed on a parent of the pair of resultant segments. When a parent segment of a pair of resultant segments is proximate the first endpoint of the road feature then a resultant segment that is proximate to the first endpoint of the road feature is to be labeled first with a subsequent label while the other resultant segment of the pair is to be labeled with a label subsequent the first label. When a parent segment of a pair of resultant segments is proximate the second endpoint of the road feature then a resultant segment that is proximate to the second endpoint of the road feature is to be labeled first while the other resultant segment of the pair is to be labeled with a label subsequent the first label. A stop condition may be reached when there exists no resultant segments that fit a corresponding sequential label.","According to another embodiment, a computer device includes a communications network interface, a processor, a memory coupled to the processor, a display device coupled to the processor and a number of routines stored in the memory that execute on the processor. A first routine receives a set of map data comprising data defining at least one road feature of a map and an ordered set of labels associated with the road feature. A second routine selects a zoom level, a viewing window position, and a viewing window size for rendering the map with the road feature on the display device. A third routine determines a segment of the road feature to apply a label. A fourth routine places a subsequent label from the ordered set of labels at the center of the determined segment if a length of the determined segment fits a length of the subsequent label. A fifth routine determines a pair of resultant segments produced by placing the label on the determined segment if a label is placed. A sixth routine repeats the third, fourth and fifth routine for each resultant segment produced until a stop condition is reached.","A graphics or image rendering system, such as a map image rendering system, receives image data from an image database in the form of vector data that defines various linear features of the map, such as roads, boundaries, etc., in addition to text strings or symbols to be displayed with the features to provide, for example, labels for the features. A label positioning technique generally divides the displayed linear feature into a number of segments and then labels each segment in a sequence based on an ordered set of labels and a position of each segment.","The label rendering technique positions the text string and\/or symbol labels such that the labels are less likely to move along a dimension of the linear feature (e.g., a road feature) when a map zoom level is changed. Generally, the label movement during zooming is referred to as \u201ccrawling\u201d and can be a distraction for map viewers. The image rendering system also provides a more consistent labeling process that enhances visual properties within a map image.","Referring now to , a map-related imaging system  includes a map database  stored in a server  or in multiple server located at, for example, a central site or at various spaced apart sites, and also includes multiple map client devices , , , and , each of which stores and implements a map rendering device or a map rendering engine. The map client devices - may be connected to the server  via any hardwired or wireless communication network , including for example a hardwired or wireless LAN, MAN or WAN, the Internet, or any combination thereof. The map client devices - may be, for example, mobile phone devices (), computers such a laptop, desktop or other types of computers (, ) or components of other imaging systems such as components of automobile navigation systems (), etc. Moreover, the client devices - may be communicatively connected to the server  via any suitable communication system, such as any publically available or privately owned communication network, including those that use hardwired based communication structure, such as telephone and cable hardware, and\/or wireless communication structure, such as wireless communication networks, including for example, wireless LANs and WANs, satellite and cellular phone communication systems, etc.","The map database  may store any desired types or kinds of map data including raster image map data and vector image map data. The image rendering systems described herein are highly suited for use with vector image data which defines or includes a series of vertices or vertex data points for each of numerous sets of image objects, elements or primitives within an image to be displayed. However, while the image rendering systems may primarily generate images based on vector image data, in some embodiments, a combination of vector image map data and raster image map data may be utilized. For example, for some zoom levels or when a slow client machine is performing image generation, raster backgrounds without any text (e.g., water, parks, roads, etc.) may be rendered with text generated from vector data. Generally speaking, each of the image objects defined by the vector data will have a plurality of vertices associated therewith and these vertices will be used to display a map related image object to a user via one or more of the client devices -.","As will also be understood, each of the client devices - includes an image rendering engine having one or more processors , one or more memories , a display device , and in many cases a rasterizer or graphics card  which are generally programmed and interconnected in known manners to implement or to render graphics (images) on the associated display device . The display device  for any particular client device - may be any type of electronic display device such as a liquid crystal display (LCD), a light emitting diode (LED) display, a plasma display, a cathode ray tube (CRT) display, or any other type of known or suitable electronic display.","Generally, speaking, the map-related imaging system  of  operates such that a user, at one of the client devices -, opens or executes a map application (not shown in ) that operates to communicate with and to obtain map information or map related data from the map database  via the server , and that then displays or renders a map image based on the received map data. The map application may allow the user to view different geographical portions of the map data stored in the map database , to zoom in or zoom out on a particular geographical location, to rotate, spin or change the two-dimensional or three-dimensional viewing angle of the map being displayed, etc. More particularly, when rendering a map image on a display device or a display screen  using the system described below, each of the client devices - downloads map data in the form of vector data from the map database  and processes that vector data using one or more image shaders to render an image on the associated display device .","Referring now to , an image generation or imaging rendering device  associated with or implemented by one of the client devices - is illustrated in more detail. The image rendering system  of  includes two processors and , two memories and , a user interface  and a rasterizer . In this case, the processor , the memory and the rasterizer  are disposed on a separate graphics card (denoted below the horizontal line), although this need not be the case in all embodiments. For example, in other embodiments, a single processor may be used instead. In addition, the image rendering system  includes a network interface , a communications and storage routine  and one or more map applications  having map display logic therein stored on the memory , which may be executed on the processor . Likewise one or more image shaders in the form of, for example, vertex shaders  and fragment shaders  are stored on the memory and are executed on the processor . The memories and may include either or both volatile and non-volatile memory and the routines and shaders are executed on the processors and to provide the functionality described below. The network interface  includes any well known software and\/or hardware components that operate to communicate with, for example, the server  of  via a hardwired or wireless communications network to obtain image data in the form of vector data for use in creating an image display on the user interface or display device . The image rendering device  also includes a data memory , which may be a buffer or volatile memory for example, that stores vector data received from the map database , the vector data including any number of vertex data points and one or more lookup tables as will be described in more detail.","During operation, the map logic of the map application  executes on the processor  to determine the particular image data needed for display to a user via the display device  using, for example, user input, GPS signals, prestored logic or programming, etc. The display or map logic of the application  interacts with the map database , using the communications routine , by communicating with the server  through the network interface  to obtain map data, preferably in the form of vector data or compressed vector data from the map database . This vector data is returned via the network interface  and may be decompressed and stored in the data memory  by the routine . In particular, the data downloaded from the map database  may be a compact, structured, or otherwise optimized version of the ultimate vector data to be used, and the map application  may operate to transform the downloaded vector data into specific vertex data points using the processor . In one embodiment, the image data sent from the server  includes vector data generally defining data for each of a set of vertices associated with a number of different image elements or image objects to be displayed on the screen  and possibly one or more lookup tables. If desired, the lookup tables may be sent in, or may be decoded to be in, or may be generated by the map application  to be in the form of vector texture maps which are known types of data files typically defining a particular texture or color field (pixel values) to be displayed as part of an image created using vector graphics. More particularly, the vector data for each image element or image object may include multiple vertices associated with one or more triangles making up the particular element or object of an image. Each such triangle includes three vertices (defined by vertex data points) and each vertex data point has vertex data associated therewith. In one embodiment, each vertex data point includes vertex location data defining a two-dimensional or a three-dimensional position or location of the vertex in a reference or virtual space, as well as an attribute reference. Each vertex data point may additionally include other information, such as an object type identifier that identifies the type of image object with which the vertex data point is associated. The attribute reference, referred to herein as a style reference or as a feature reference, references or points to a location or a set of locations in one or more of the lookup tables downloaded and stored in the data memory .","Existing map rendering systems may receive map data from a database in the form of vector data or image data and render a map surface including a set of map features on a display based on the vector data or image data. The map rendering systems may label one or more of the map features with text or image labels (e.g., icons) that identify the map features. When a zoom level of a displayed map is changed, the map features may be re-labeled. Existing systems may re-label linear map features (such as roads, boundaries, etc.) in a manner that produces a crawling effect of the labels, where the labels appear to move along a dimension of the linear map feature. This can be highly distracting to map viewers. The labeling technique described herein may be used to reduce the crawling effect of labels during zoom level changes. Generally, the labeling technique for rendering a label along a map object involves iteratively bisecting a road section using labels of an ordered set of labels. A label may be placed in a sequence on the resultant segments of the bisections based and an ordered set of labels and a bisection order. The bisection labeling process may be repeated each time a zoom level is changed.",{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIGS. 3A-3G","FIGS. 3A-3G"]},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 3A","FIG. 3A","FIG. 3A"],"b":["400","400","400","401","402","400","400","400","403","400","401","402","404","406","404","406","404","406","400","404","406","403"]},"The two road segments ,  that result from the bisection may then be labeled at their centers using subsequent labels in the sequence of the ordered set of labels and so on. Generally, each subsequent road segment that is processed may be labeled with the next label in the ordered set. In particular, at this stage of the bisection process, a selection may be made to start with either road segment  or road segment . Because of the orientation of the road segments within the viewing window, road segment  may be referred to herein as a left branch segment (at range 0-90 pixels) that is proximate the first endpoint  of road feature  and road segment  may be referred to herein as a right branch segment (at range 110-200 pixels) that is proximate second endpoint . A default process may select the left branch segment. In this case, the left branch segment  may be labeled with the subsequent label of the set Ta and right branch segment  may be labeled with the subsequent label Tb. Generally, when the end of the ordered set of labels is reached (e.g., selected or placed) the order may start again from the beginning of the ordered set of labels. Thus, because label Tb has been selected for the right branch segment , a subsequent label in the ordered set of labels may start over with the label I.",{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 3B","FIG. 3B"],"b":["404","0","90","45","404","408","410","400","408","404","410"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 3C","FIG. 3D","FIG. 3E","FIG. 3E"],"b":["408","0","30","15","408","408","410","410","75","30","75","65","85","412","414","412","414","410","200","412","412","414","412","400","406","400","110","200","155","406","416","418","416","418","406","416","418","416","30","60","65","85","90","110","120","190"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 3F","FIGS. 3A-3E","FIG. 3G","FIGS. 3A-3E","FIG. 3F","FIG. 3G","FIG. 3E"],"b":["5","35","40","60"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 4A","FIGS. 3A-3G","FIGS. 3A-3G","FIG. 4A","FIG. 4A","FIG. 4A"],"b":["502","504","506","1","2","502","504","506","506","504","508","510","512"]},"An order of labeling each of the resultant pair of road segments is also based on the label of the parent road segment that produced the pair of child road segments. The first child road segment of the pair of child road segments to be labeled may be labeled with the next label of the ordered set of labels after the parent label. The second child road segment of the pair of child road segments may be labeled with the next label of the ordered set of labels after the first child road segment. For example, the road segment corresponding with the left branch  may be labeled with the Ta, which is the subsequent label of the ordered set following the parent label I, while the road segment corresponding with right branch  may be labeled with a label subsequent the left branch road segment, which is Tb.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 4B","FIG. 4A","FIG. 4B"],"b":["520","524","522"]},{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 5","FIG. 2"],"b":["300","300","300","48","30","36","34"]},"A block  may determine a linear map feature to be displayed on a viewing window. The linear map feature may be a road, street, boundary, river segment, etc. The viewing window may be defined by a particular zoom level, wherein the zoom level corresponds to a magnification level of a map portion being displayed. Generally, for a constant viewing window size, as zoom level increases, map area decreases. It should be noted that in some embodiments, changing zoom level may correspond to redrawing a map area with greater or lesser map data or detail, where the map data used in a prior zoom level is different than that for the redrawn level. A zoom level change where additional data (e.g., vector data) is used to redraw a map may be different from a zoom level change where only a scale of the map area is changed (e.g., same level of detail but greater magnification). It should be noted that in some embodiments, each time a zoom level is changed and\/or a viewing window position is changed, the existing labels may be removed and the labeling technique described herein repeated.","The entirety of a linear map feature may not always be displayed entirely within a viewing window. Instead, only a portion less than the entire feature may be shown. In this case, the viewable portion of the linear feature may be limited by a size and a position of the viewing window and the zoom level. In some computer mapping applications, the position of the viewing window, the size of the viewing window, and\/or the zoom level may be changed to reveal some or all of the road feature, depending on the total length or size of street. In one embodiment, a larger portion of a road feature may be processed using the described labeling technique even though the entire road feature is not displayed in a current viewing window. In this case, labels on portions of the road that are not visible are not displayed until or unless the viewing window is changed to show those portions. While the process of  may be applied to any linear map feature, the blocks of  may be discussed with respect to a road feature.","A block  may determine an ordered set of labels for labeling the road feature. The set of labels may be text describing the road feature. Alternatively, the set of labels may include images representing, for example, icons. The set of labels may be generally ordered by a priority of each label. In one embodiment, the first label in the sequence may be the highest priority label, with subsequent labels having less priority. In one embodiment, a first label may be a primary label that may be most frequently applied or placed on a road. Ancillary second, third, etc. labels may have less placement frequency. The ordered set of labels may be determined by a mapping application or retrieved from a table.","At block , a road segment of the road feature may be selected for labeling. In a first iteration of this process, an entire displayed road section may be selected in block . A block  determines or selects a next label of the ordered set of labels. In a first iteration of the method of , the selected label may represent a first, primary label.","At block , a length of the road segment along its center line may be determined along with a length of the selected label. A block  may determine whether the length of the road segment is long enough to accommodate the length of the label. If, at block , the segment is not long enough for the determined label, the segment may be discarded or marked as being processed at block . The process may then continue back at block  for another unprocessed road segment. The block  may search for road segments that are not marked processed. If the length of the road segment at block  is long enough to accommodate the length of the label, block  may place the selected label at the center of the road segment. A block  may determine where to place the selected label by dividing the length of the road segment in half (e.g., bisecting the road) and placing the selected label at the bisection or midpoint. A block  may determine a pair of resultant segments produced by bisecting the determined road segment. The block  may reserve a length of space about the midpoint of the road segment for the label so that the resultant segments do not include the length of the label. This may be performed by subtracting half the length of the label from each of the two segments produced from a bisection of the determined road segment. The resultant segments may represent additional unprocessed segments.","The blocks  and\/or  may include determining an order of the label processing or segmentation such that the labels are placed on the road segments according to the order illustrated in  or B. The process of  continues from the block  to the block  where a next unprocessed road segment may be determined for processing. The process of  may be repeated for every remaining unprocessed segment until a stop condition is reached. In one embodiment, a stop condition may be reached when no remaining segments can fit a corresponding label from the ordered label set. When this labeling process is performed, the overall placement of the labels may be visually perceived to be similar at two or more different zoom levels. Another stop condition may be reached when the number of bisections executed is above a threshold number of bisections. This threshold number may be a fixed number or may be adjusted based on a total length of a road feature that is displayable within a viewing window. In some embodiments, stop conditions may be based on characteristics of the label set. For example, the label set may not be a fixed set, where certain labels may have a fixed number of uses. In the currently described label set, for example, the icon label may only be used a single time before the icon label is removed from the label set. In this embodiment, a stop condition may be encountered when there are no remaining labels in the set.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":["FIG. 6A","FIG. 6B"],"b":["600","602","600","316","604","604","600","604","610","612","614"]},"Of course, the labeling encoding techniques described herein may be altered or varied in any number of manners to provide an image rendering system, such as a map rendering system, with the ability to render individual text characters within a text string along a curved, e.g., multi-segmented line. Throughout this specification, plural instances may implement components, operations, or structures described as a single instance. Although individual operations of one or more methods are illustrated and described as separate operations, one or more of the individual operations may be performed concurrently, and nothing requires that the operations be performed in the order illustrated. Structures and functionality presented as separate components in example configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements fall within the scope of the subject matter herein.","For example, the network  may include but is not limited to any combination of a LAN, a MAN, a WAN, a mobile, a wired or wireless network, a private network, or a virtual private network. Moreover, while only four client devices are illustrated in  to simplify and clarify the description, it is understood that any number of client computers or display devices are supported and can be in communication with the server .","Additionally, certain embodiments are described herein as including logic or a number of components, modules, or mechanisms. Modules may constitute either software modules (e.g., code embodied on a machine-readable medium or in a transmission signal) or hardware modules. A hardware module is tangible unit capable of performing certain operations and may be configured or arranged in a certain manner. In example embodiments, one or more computer systems (e.g., a standalone, client or server computer system) or one or more hardware modules of a computer system (e.g., a processor or a group of processors) may be configured by software (e.g., an application or application portion) as a hardware module that operates to perform certain operations as described herein.","In various embodiments, a hardware module may be implemented mechanically or electronically. For example, a hardware module may comprise dedicated circuitry or logic that is permanently configured (e.g., as a special-purpose processor, such as a field programmable gate array (FPGA) or an application-specific integrated circuit (ASIC)) to perform certain operations. A hardware module may also comprise programmable logic or circuitry (e.g., as encompassed within a general-purpose processor or other programmable processor) that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.","Accordingly, the term hardware should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in a certain manner or to perform certain operations described herein. Considering embodiments in which hardware modules are temporarily configured (e.g., programmed), each of the hardware modules need not be configured or instantiated at any one instance in time. For example, where the hardware modules comprise a general-purpose processor configured using software, the general-purpose processor may be configured as respective different hardware modules at different times. Software may accordingly configure a processor, for example, to constitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.","Hardware and software modules can provide information to, and receive information from, other hardware and\/or software modules. Accordingly, the described hardware modules may be regarded as being communicatively coupled. Where multiple of such hardware or software modules exist contemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) that connect the hardware or software modules. In embodiments in which multiple hardware modules or software are configured or instantiated at different times, communications between such hardware or software modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware or software modules have access. For example, one hardware or software module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further hardware or software module may then, at a later time, access the memory device to retrieve and process the stored output. Hardware and software modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information).","The various operations of example methods described herein may be performed, at least partially, by one or more processors that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations. Whether temporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions. The modules referred to herein may, in some example embodiments, comprise processor-implemented modules.","Similarly, the methods or routines described herein may be at least partially processor-implemented. For example, at least some of the operations of a method may be performed by one or processors or processor-implemented hardware modules. The performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the processor or processors may be located in a single location (e.g., within a home environment, an office environment or as a server farm), while in other embodiments the processors may be distributed across a number of locations.","The one or more processors may also operate to support performance of the relevant operations in a \u201ccloud computing\u201d environment or as a \u201csoftware as a service\u201d (SaaS). For example, at least some of the operations may be performed by a group of computers (as examples of machines including processors), these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., application program interfaces (APIs).)","The performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the one or more processors or processor-implemented modules may be located in a single geographic location (e.g., within a home environment, an office environment, or a server farm). In other example embodiments, the one or more processors or processor-implemented modules may be distributed across a number of geographic locations.","Some portions of this specification are presented in terms of algorithms or symbolic representations of operations on data stored as bits or binary digital signals within a machine memory (e.g., a computer memory). These algorithms or symbolic representations are examples of techniques used by those of ordinary skill in the data processing arts to convey the substance of their work to others skilled in the art. As used herein, an \u201calgorithm\u201d or a \u201croutine\u201d is a self-consistent sequence of operations or similar processing leading to a desired result. In this context, algorithms, routines and operations involve physical manipulation of physical quantities. Typically, but not necessarily, such quantities may take the form of electrical, magnetic, or optical signals capable of being stored, accessed, transferred, combined, compared, or otherwise manipulated by a machine. It is convenient at times, principally for reasons of common usage, to refer to such signals using words such as \u201cdata,\u201d \u201ccontent,\u201d \u201cbits,\u201d \u201cvalues,\u201d \u201celements,\u201d \u201csymbols,\u201d \u201ccharacters,\u201d \u201cterms,\u201d \u201cnumbers,\u201d \u201cnumerals,\u201d or the like. These words, however, are merely convenient labels and are to be associated with appropriate physical quantities.","Unless specifically stated otherwise, discussions herein using words such as \u201cprocessing,\u201d \u201ccomputing,\u201d \u201ccalculating,\u201d \u201cdetermining,\u201d \u201cpresenting,\u201d \u201cdisplaying,\u201d or the like may refer to actions or processes of a machine (e.g., a computer) that manipulates or transforms data represented as physical (e.g., electronic, magnetic, or optical) quantities within one or more memories (e.g., volatile memory, non-volatile memory, or a combination thereof), registers, or other machine components that receive, store, transmit, or display information.","As used herein any reference to \u201cone embodiment\u201d or \u201can embodiment\u201d means that a particular element, feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment. The appearances of the phrase \u201cin one embodiment\u201d in various places in the specification are not necessarily all referring to the same embodiment.","Some embodiments may be described using the expression \u201ccoupled\u201d and \u201cconnected\u201d along with their derivatives. For example, some embodiments may be described using the term \u201ccoupled\u201d to indicate that two or more elements are in direct physical or electrical contact. The term \u201ccoupled,\u201d however, may also mean that two or more elements are not in direct contact with each other, but yet still cooperate or interact with each other. The embodiments are not limited in this context.","As used herein, the terms \u201ccomprises,\u201d \u201ccomprising,\u201d \u201cincludes,\u201d \u201cincluding,\u201d \u201chas,\u201d \u201chaving\u201d or any other variation thereof, are intended to cover a non-exclusive inclusion. For example, a process, method, article, or apparatus that comprises a list of elements is not necessarily limited to only those elements but may include other elements not expressly listed or inherent to such process, method, article, or apparatus. Further, unless expressly stated to the contrary, \u201cor\u201d refers to an inclusive or and not to an exclusive or. For example, a condition A or B is satisfied by any one of the following: A is true (or present) and B is false (or not present), A is false (or not present) and B is true (or present), and both A and B are true (or present).","In addition, use of the \u201ca\u201d or \u201can\u201d are employed to describe elements and components of the embodiments herein. This is done merely for convenience and to give a general sense of the description. This description should be read to include one or at least one and the singular also includes the plural unless it is obvious that it is meant otherwise.","Still further, the figures depict preferred embodiments of a map rendering system for purposes of illustration only. One skilled in the art will readily recognize from the following discussion that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles described herein.","Upon reading this disclosure, those of skill in the art will appreciate still additional alternative structural and functional designs for a system and a process for rendering map or other types of images using the principles disclosed herein. Thus, while particular embodiments and applications have been illustrated and described, it is to be understood that the disclosed embodiments are not limited to the precise construction and components disclosed herein. Various modifications, changes and variations, which will be apparent to those skilled in the art, may be made in the arrangement, operation and details of the method and apparatus disclosed herein without departing from the spirit and scope defined in the appended claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIGS. 3A-3G"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIGS. 4A-4B"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIGS. 6A and 6B"}]},"DETDESC":[{},{}]}
