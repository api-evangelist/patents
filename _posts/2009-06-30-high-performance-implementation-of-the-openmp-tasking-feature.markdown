---
title: High performance implementation of the OpenMP tasking feature
abstract: A method and system for creating and executing tasks within a multithreaded application composed according to the OpenMP application programming interface (API). The method includes generating threads within a parallel region of the application, and setting a counter equal to the quantity of the threads. The method also includes, for each one of the plurality of threads, assigning an implicit task, and executing the implicit task. Further, the method includes, upon encountering a task construct, during execution of the implicit tack, for an explicit asynchronous task generating the explicit asynchronous task, adding the explicit asynchronous task to a first task queue, where the first task queue corresponds to the one of the plurality of threads; and incrementing the counter by one.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08914799&OS=08914799&RS=08914799
owner: Oracle America Inc.
number: 08914799
owner_city: Redwood Shores
owner_country: US
publication_date: 20090630
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["A typical computer system includes hardware and software. The hardware includes at least one processing device that executes instructions defined by the software (i.e., an application). The processing device may be a processor, a micro-core on a multi-core processor, or other such device that can process instructions. Often a computer system may include multiple processing devices that execute the application(s) in parallel. For example, multiple processors and\/or multiple micro-cores may execute in parallel. Parallel execution may shorten the amount of time required to process the instructions of the application. Thus, parallel applications, or applications developed to be executed in parallel, tend to execute faster than applications that execute serially.","One Application Program Interface (API) that may be used to develop parallel applications is OpenMP. The OpenMP API supports multi-platform shared-memory parallel programming in C\/C++ and Fortran on various processor architectures. Further, the OpenMP API includes compiler directives, library routines, and environment variables that influence run-time behavior.","In general, in one aspect, the invention relates to a system including a processor, a physical memory, and a computer usable storage medium having computer readable program code embodied therein. The computer readable program code is adapted to, when executed by the processor, implement a method for generating tasks for a parallel processing application. The method includes generating threads within a parallel region of the application, and setting a counter equal to the quantity of the threads. The method also includes, for each one of the plurality of threads, assigning an implicit task, and executing the implicit task. Further, the method includes, upon encountering a task construct, during execution of the implicit tack, for an explicit asynchronous task: generating the explicit asynchronous task, adding the explicit asynchronous task to a first task queue, where the first task queue corresponds to the one of the plurality of threads, and incrementing the counter by one.","In general, in one aspect, the invention relates to a computer readable storage medium having computer readable program code embodied therein. The computer readable program code is adapted to, when executed by a processor, implement a method for generating a task for a parallel processing application. The method includes: receiving an indication of a requirement to generate the task, and determining that the task is synchronous. The method further includes, in response to determining that the task is synchronous, allocating the task on a stack, and generating the task.","In general, in one aspect, the invention relates to a computer readable storage medium having computer readable program code embodied therein. The computer readable program code is adapted to, when executed by a processor, implement a method for generating a task for a parallel processing application. The method includes receiving an indication of a requirement to generate the task. The method further includes determining that the task is asynchronous, and, in response to determining that the task is asynchronous, allocating the task on a heap, allocating at least one parameter associated with the task on the heap, and generating the task.","Other aspects of the invention will be apparent from the following description and the appended claims.","Specific embodiments of the invention will now be described in detail with reference to the accompanying Figs. Like elements in the various Figs. are denoted by like reference numerals for consistency.","In the following detailed description of embodiments of the invention, numerous specific details are set forth in order to provide a more thorough understanding of the invention. However, it will be apparent to one of ordinary skill in the art that the invention may be practiced without these specific details. In other instances, well-known features have not been described in detail to avoid unnecessarily complicating the description.","Those skilled in the art will appreciate that while the invention is described with respect to OpenMP, the invention is not limited to OpenMP implementations.","In general, embodiments of the invention relate to a method and system for executing a computer application. More specifically, embodiments of the invention relate to a method and system for creating and executing tasks within a multithreaded application composed according to the OpenMP application programming interface (API). An OpenMP application may include tasks, i.e., specific units of executable code along with the corresponding data environment (e.g., parameters for execution) which may be executed by a thread. A task may be generated when a thread encounters a task construct or a parallel construct.",{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 1","FIG. 1"],"b":["110","110","110","110","120","130","130","140","130"]},"Generally, an OpenMP program uses the fork-join model of parallel execution. Specifically, multiple threads of execution perform tasks defined implicitly or explicitly by OpenMP directives included in the program (e.g., the annotated source code () shown in ).  shows a diagram of the execution of an OpenMP program, in accordance with embodiments of the invention.","As shown in , the OpenMP program begins in a first sequential region (), with execution of a single master thread (). The first sequential region () terminates when the master thread () encounters a fork (). As shown, the fork () may create a team of slave threads () that, along with the master thread (), execute in a first parallel region (). In one embodiment, the fork () may represent a parallel construct, meaning an OpenMP directive specifying the parallel execution of multiple threads. Further, such a parallel construct may specify an implicit task associated with each thread. In particular, each implicit task may be tied to a particular thread, meaning the implicit task is always executed by a thread to which it is initially assigned.","In one or more embodiments, any thread within the first parallel region () may encounter a task construct (not shown), meaning an OpenMP directive specifying the creation of a new task. Such tasks are referred to as explicit tasks, and may be defined by the task construct as untied tasks (i.e., not necessarily continued by the thread which first executes the task) or tied tasks (i.e., tasks always continued by the thread which first executes the task). Further, explicit tasks may be defined by the task construct as synchronous (i.e., requiring immediate execution) or asynchronous (i.e., able to be executed at a later time).","Referring again to , after the first parallel region (), the team of threads encounters a join (). The join () represents a barrier, meaning a defined point in the program beyond which no thread in a team may proceed until all threads in the team have reached the barrier and all explicit tasks generated by the team have been completed. After the join (), the master thread () executes alone in a second sequential region (). Thereafter, the OpenMP program may enter one or more parallel regions as required by directives in the source code. For example, as shown in , the OpenMP program may encounter a second fork () to begin a second parallel region (), thereby creating a second team of threads. After encountering a second join (), the master thread () may enter a third sequential region ().",{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 3","FIG. 4A"],"b":["300","300","340","340"]},"Additionally, the OpenMP runtime environment () may also include one or more task queues (e.g., task queue  (), task queue  (), task queue N () shown in ). In one or more embodiments, each task queue may be associated with a particular thread being executed in an OpenMP program. For example, assume that task queue  () is associated with the master thread () shown in , assume that task queue  () is associated with a particular slave thread () in the first parallel region () shown in , and so forth. As shown in , each task queue may include one or more tasks (e.g., task  (), task  (), task X (), task Y (), etc.) queued for an associated thread. In one or more embodiments, each task queue may be processed in a last-in, first-out (LIFO) manner. For example, referring to , assume task queue  () was loaded first with task  (), then with task  (), and finally with task  (). Accordingly, when processing the task queue  () in a LIFO manner, the first task to be dequeued would be task  ().","Of course, one skilled in the art will appreciate that  are merely exemplary illustrations of embodiments of the invention and, as such, as not intended to limit the scope of the invention. For example, while  illustrates two parallel regions, an OpenMP program may include any number of parallel regions, in accordance with requirements of a particular use or application. Further, one skilled in the art will appreciate that the terms used in this description have other recognized names. For example, a master thread may also be referred to as a primary thread, slave threads may be referred to as child or worker threads, and the like.",{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIGS. 4A-4B","FIG. 2","FIGS. 4A-4B"],"b":"270"},"At ST , a team of threads may be generated. For example, referring to , a master thread () executing an OpenMP program may encounter a fork () (e.g., a parallel construct), and may thus create multiple slave threads (). At ST , an implicit task may be assigned to each thread in the team. At ST , a counter (e.g., counter () shown in ) may be set equal to the number of threads in the team.","At ST , a loop to process each thread in the team may be entered. At ST , the thread executes a current task. For example, the thread may execute an implicit task assigned to the thread (at ST ). At ST , a determination is made about whether the thread has encountered a task construct. If it is determined that the thread has not encountered a task construct, then at ST , a determination is made about whether the current task is completed. If it is determined that the current task is not completed, then at ST , the thread continues to execute the current task. However, if it is determined at ST  that the current task is completed, then the process continues at ST  (described below).","Returning to ST , if it is determined that the thread has encountered a task construct, then at ST , an explicit task may be generated. ST  is described in greater detail below with reference to . If the explicit task generated at ST  is an asynchronous task, then at ST , the explicit task may be enqueued on a task queue associated with the current thread (e.g., task queue  () shown in ). However, in one or more embodiments, if the explicit task fails to be enqueued at ST  (e.g., the task queue  () is full), then the explicit task may be converted to a synchronous task (i.e., the explicit task is executed immediately).","At ST , the counter (e.g., counter () shown in ) may be incremented by one. At ST , a determination is made about whether the current task is completed. If it is determined that the current task is completed, then at ST , the counter may be decremented by one. Note that, by incrementing the counter for each task generated (at ST  and ST ), and decrementing the counter for each task completed (at ST ), the counter tracks all tasks that remain to be completed by the entire team of threads.","At ST , a determination is made about whether the counter is equal to zero. If it is determined that the counter is not equal to zero (i.e., all tasks generated for the team of threads have not been completed), then at ST , a determination is made about whether the task queue for the current thread is empty. If it is determined that the task queue is not empty, then at ST , a task is dequeued from the task queue. In one embodiment, the task is dequeued in a LIFO manner. For example, referring to , assuming task  () was the last task added to task queue  (), and thus task  () may be dequeued.","However, if it is determined at ST  that the task queue is empty, then at ST , the current thread may steal a task (if available) from a task queue associated with a different thread. After either ST  or ST , at ST , the current task (i.e., either the task dequeued at ST  or the task stolen at ST ) may be executed.","Returning to ST , if it is determined that the counter is equal to zero (i.e., all tasks generated for the team of threads have been completed), then at ST , the current thread waits (e.g., sleeps or is otherwise suspended) at a barrier. In other words, the current thread has completed all required tasks, and is thus suspended in order to wait for the remaining threads of the team to complete processing. For example, referring to , assume one of the slave threads () waits at the join () for the remaining slave threads () and\/or the master thread () to complete processing. After ST , the processing of the current thread through the loop (entered at ST ) is complete. Of course, one of skill in the art will understand that the loop entered at ST  may represent the parallel processing of some or all of the threads in the team. In other words, the processing of a thread through the loop entered at ST  may occur simultaneously with the processing of other threads through the same loop.","Returning to ST , if the generated explicit task is a synchronous task, then the processing of the thread continues on the flowchart shown in . In other words,  represents a continuation of the flowchart shown in . After ST  (shown in ), at ST , the current task may be suspended. At ST , the generated synchronous task may be executed immediately (i.e., without being queued). At ST , the generated synchronous task may complete executing. At ST , the suspended task may resume executing. After ST , the flowchart continues at ST  (shown in ).","In one or more embodiments, when creating an explicit task ((i.e., ST  shown in ), it may be beneficial to allocate the explicit task on a stack rather than on a heap. For example, allocating an explicit task on a stack may be faster than allocating the explicit task on a heap. However, if an explicit task is allocated on a stack, it may be required that any descendant tasks of the explicit task must finish before the explicit task finishes.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 5","FIG. 5","FIG. 4A","FIG. 5"],"b":"462"},"In one or more embodiments of the invention, one or more of the steps described below may be omitted, repeated, performed in parallel, and\/or performed in a different order. Accordingly, the specific arrangement of steps shown in  should not be construed as limiting the scope of the invention.","At ST , a determination is made about whether a task queue (e.g., task queue  () shown in ) is full. If it is determined that the task queue is not full, then at ST , a determination is made about whether the explicit task is specified as synchronous within the OpenMP program (i.e., by the task construct defining the explicit task). If it is determined that the explicit task is not specified as synchronous within the OpenMP program, then at ST , a determination is made about whether the explicit task is specified as synchronous by a runtime library (e.g., OpenMP runtime library () shown in ). If it is determined that the explicit task is not specified as synchronous by a runtime library, then at ST , the explicit task may be set to have an asynchronous execution mode.","At ST , the explicit task and associated parameters may be allocated on a heap. Such associated parameters may be variables that refer to data on which the task operates. For example, consider the following code segment:",{"@attributes":{"id":"p-0040","num":"0039"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"154pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"int a = 1;"]},{"entry":[{},"#pragma omp task firstprivate(a)"]},{"entry":[{},"{"]},{"entry":[{},"\u2003...;"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"In the above example, the task parameter \u201ca\u201d has the value \u201c1.\u201d At ST , a determination is made about whether the explicit task is descendent from a stack task (i.e., a task allocated on a stack. Generally, a stack is a limited local memory space reserved for a function. In contrast, a heap is a global free memory area which may be allocated and kept valid until it is freed. Typically, memory in the stack may be allocated much faster than memory in the heap, due to the heap's complex allocation\/free algorithm. However, the memory in the stack space is only valid until the function returns.","Returning to , if it is determined at ST  that the explicit task is descendent from a stack task, then at ST , any ancestor stack tasks may be converted to heap tasks. After ST , or if it is determined at ST  that the explicit task is not descendent from a stack task, then at ST , the explicit task is generated.","However, if it is determined at ST  that the task queue is full, or if it is determined at ST  that that the explicit task is specified as synchronous within the OpenMP program, or if it is determined at ST  that the explicit task is specified as synchronous by a runtime library, then at ST , the explicit task may be set to have an synchronous execution mode. Alternatively, if it is determined at ST  that the task queue is not full but previously had been full, the task is specified as synchronous unless the level of the task queue is below a predefined percentage threshold (e.g., 90% full, 80% full, etc.) and\/or a predefined numerical threshold (e.g., 40 queued tasks, 120 queued tasks, etc.).","At ST , a determination is made about whether there is sufficient space on a stack for task parameters required by the explicit task. If it is determined that there is sufficient space on a stack, then at ST , the task parameters are allocated on the stack. However, if it is determined at ST  that there is not sufficient space on a stack, then at ST , the task parameters are allocated on the heap using a fast allocation method. For example, the task parameters may be allocated on the heap using the mt-unsafe fast allocation method, meaning allocating a block from a local free list (i.e., a list of free memory blocks for each thread) and then put it back when it is freed. After either ST  or ST , at ST , the explicit task is allocated on the stack. At ST , the explicit task is generated.","Embodiments of the invention provide improved creation, scheduling, and execution of OpenMP tasks. As described above, in one or more embodiments, tasks may be queued in a last-in, first-out (LIFO) queue associated with each thread, thereby simplifying the load on the compiler on runtime. Further, in one or more embodiments, each task may be allocated on either a stack or a heap according to various criteria, thereby allowing the use of faster stack memory without having to determine beforehand if each task will have unfinished descendant tasks. Additionally, in one or more embodiments, a counter may be configured to track all tasks created within a parallel region of the application, thereby allowing detection of termination of the tasks.","Use of the above-described embodiments of the invention have been shown to improve the performance of programs including OpenMP tasks. For example, Table I of test results (see below) shows substantial improvement in the performance of a quick sort algorithm when using embodiments of the invention.",{"@attributes":{"id":"p-0047","num":"0046"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE I"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Test Results Using Embodiments of the Invention"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"147pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Threads"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1","2","4","8","12","16"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Original","1.000","1.705","3.043","5.330","7.940","8.527"]},{"entry":["Using Task Allocation","1.000","1.977","3.859","7.552","10.741","11.376"]},{"entry":["Using Task and","1.000","1.971","3.884","7.630","11.179","13.778"]},{"entry":"Parameter Allocation"},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]}}]}}},"As shown, when using 16 threads, embodiments of the invention (shown on the bottom row) are 13.8 times faster than a single thread. In contrast, when not using embodiments of the invention (shown on the top row), using 16 threads is only 8.5 times faster than a single thread. The above test results were generated on a Sun\u2122 computer having four SPARC64-VII\u2122 (impl 0x7 ver 0x90 clock 2520 MHz) physical processors. Each processor had four cores, and each core had two virtual processors. The test machine used the Solaris\u2122 operating system developed by Sun Microsystems\u00ae), Inc. located in Santa Clara, Calif.","Embodiments of the invention may be implemented on virtually any type of computer regardless of the platform being used. For example, as shown in , a networked computer system () includes a processor (), associated memory (), a storage device (), and numerous other elements and functionalities typical of today's computers (not shown). The networked computer () may also include input means, such as a keyboard () and a mouse (), and output means, such as a monitor ().","The networked computer system () is connected to a local area network (LAN) or a wide area network via a network interface connection (not shown). Those skilled in the art will appreciate that these input and output means may take other forms. Further, those skilled in the art will appreciate that one or more elements of the aforementioned computer () may be remotely located and connected to the other elements over a network. Further, software instructions to perform embodiments of the invention may be stored on a computer readable storage medium such as a compact disc (CD), a diskette, a tape, or any other physical computer readable storage device.","While the invention has been described with respect to a limited number of embodiments, those skilled in the art, having benefit of this disclosure, will appreciate that other embodiments can be devised which do not depart from the scope of the invention as disclosed herein. Accordingly, the scope of the invention should be limited only by the attached claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF DRAWINGS","p":[{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIGS. 4A-4B"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
