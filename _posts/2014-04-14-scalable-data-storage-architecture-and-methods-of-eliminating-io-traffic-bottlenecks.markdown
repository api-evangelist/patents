---
title: Scalable data storage architecture and methods of eliminating I/O traffic bottlenecks
abstract: A Storage Area Network (SAN) system has host computers, front-end SAN controllers (FE_SAN) connected via a bus or network interconnect to back-end SAN controllers (BE_SAN), and physical disk drives connected via network interconnect to the BE_SANs to provide distributed high performance centrally managed storage. Described are hardware and software architectural solutions designed to eliminate I/O traffic bottlenecks, improve scalability, and reduce the overall cost of SAN systems. In an embodiment, the BE_SAN has firmware to recognize when, in order to support a multidisc volume, such as a RAID volume, it is configured to support, it requires access to a physical disk attached to a second BE_SAN; when such a reference is recognized it passes assess commands to the second BE_SAN. Buffer memory of each FE_SAN is mapped into application memory space to increase access speed, where multiple hosts share an LBA the BE_SAN tracks writes and invalidates the unwritten buffers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09118698&OS=09118698&RS=09118698
owner: 
number: 09118698
owner_city: 
owner_country: 
publication_date: 20140414
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY","DETAILED DESCRIPTION OF THE EMBODIMENTS"],"p":["This application is a continuation-in-part of U.S. patent application Ser. No. 13\/731,854, filed 31 Dec. 2012, which is a continuation-in-part of U.S. patent application Ser. No. 11\/292,838, filed 2 Dec. 2005, now U.S. Pat. No. 8,347,010, the disclosures of which are incorporated herein by reference.","This invention relates generally to the field of storage RAID controllers and Storage Area Network (SAN) systems. As ever-increasing demand for more data hence, for more data storage, SAN and RAID technologies available today have failed to meet performance requirements and with prohibitively high cost made them out of reach for the majority of the small and some medium size businesses.","A majority of small and some medium size businesses and corporations, recognizing the productivity gains resulting from high-performance and high availability computing systems are often unable to acquire suchlike systems due to prohibitively high cost of data storage subsystems. To reduce the cost they often utilize large number of disjoint individual servers where each server is dedicated to one or more specific applications, such as mail servers, accounting application server, etc. This environment, clearly, leads to underutilized available aggregate computing power of all servers since each server is dedicated to specific application and the workload cannot be shared. This approach introduces other issues such as system and network administration, fault tolerance, fragmented data storage, data storage and backup management problems, as well as system complexity, and so forth. Data access and data sharing could be done at different levels such as at block level (shared block storage), multiple hosts accessing the same disk drives or Logical Unit Numbers (LUNs), or at the file level using file systems like Network File System, Common Internet File System, etc.","A Network File System (NFS) is a client\/server application that facilitates viewing and optionally storing and updating files stored on a remote computer (often called a file server). The client system, which may be a work station or cluster node, has to run NFS client and the other computer, file server, needs the NFS server software. Both computers typically must have networking protocol software such as Transmission Control Protocol\/Internet Protocol (TCP\/IP) and networking hardware such as Ethernet, InfiniBand, Myrinet, or other Network Interface Cards (NICs) installed, since the NFS server and NFS client use network protocols to exchange the files data. This approach leads to a bandwidth bottleneck on both client and file server sides, mainly, due to NFS protocol overhead, limited Peripheral Component Interconnect (PCI) bus data rate, and possibly high latency associated with traditional architecture of the data storage subsystem attached to it.","A protocol standard developed by Microsoft, Common Internet File System (CIFS), which allows programs to make requests for files and services located on a remote computer, facilitates the same basic function as previously mentioned NFS. CIFS is typically utilized in Microsoft operating system environments to allow shared access from one computer to files stored on another remote computer within the same network. A CIFS client makes a request to a CIFS server (usually in another computer) for a file access or to pass a message. The server executes requested action and returns a response. CIFS is a public version of the Server Message Bock (SMB) protocol. The file server running CIFS suffers from the same problems as earlier mentioned NFS server because, this is in essence the same or similar hardware and technology.","TCP\/IP protocol overhead together with network latency affects the performance of NFS\/CIFS subsystems by significantly increasing access delays for network-attached disk when compared to locally attached disk. However, locally attached disk performance, usually, is much lower compared to data storage subsystem implementations such as RAID or Storage Area Network (SAN) subsystem. Traditional SAN design and implementation even though in many cases superior to locally attached disk drives, underutilize aggregate data rate potential of all attached disk drives by making use of time division multiplexing over typically small number of I\/O (network) links between servers and the SAN subsystem attached to it.","The present invention is an improvement over the existing data storage architectures by means of allowing parallel execution of the Front-End code on the independent FE_SAN controllers and employing locking mechanism in the Back-End code (executed on the BE_SAN controllers) to enforce data coherency and prevent data corruption.","In a preferred embodiment, the FE_SAN controllers accept all Small Computer System Interface (SCSI) commands, messages, and data for Front-End processing. The resulting output is forwarded to their intended BE_SAN controllers over the interconnecting network. Any network topology is allowed. From the BE_SAN controllers the SCSI commands, data, and messages after Back-End processing are sent to the anticipated SCSI targets (disk drives or memory persistent devices) or other storage systems. Neither SCSI target devices (disk drives) nor initiators (host computers) are aware of the underlying transport or FE_SAN and BE_SAN controllers. Hosts would perceive a \u201cvirtual\u201d disk drive (or drives) attached to the FE_SAN (one or more). FE_SAN controllers could be configured as a controller group to share the load (command and message processing), enhancing the available bandwidth, and improving availability. In such cases, the host would have a notion of a single multi-channel controller attached to it.","A Storage Area Network (SAN) system has host computers, front-end SAN controllers (FE_SAN) connected via a bus or network interconnect to back-end SAN controllers (BE_SAN), and physical disk drives connected via network interconnect to the BE_SANs to provide distributed high performance centrally managed storage. Described are hardware and software architectural solutions designed to eliminate I\/O traffic bottlenecks, improve scalability, and reduce the overall cost of SAN systems. In an embodiment, the BE_SAN has firmware to recognize when, in order to support a multidisc volume, such as a RAID volume, it is configured to support, it requires access to a physical disk attached to a second BE_SAN; when such a reference is recognized it passes assess commands to the second BE_SAN. Further, the BE_SAN has firmware to make use of the physical disk attached to the second BE_SAN as a hot-spare for RAID operations. Buffer memory of each FE_SAN is mapped into application memory space to increase access speed, where multiple hosts share an LBA the BE_SAN tracks writes and invalidates the unwritten buffers.","This invention relates generally to the field of storage RAID controllers and Storage Area Network (SAN) systems. Principally, the invention relates to improving data access speeds by facilitating massively parallel data access by separating Front-End and Back-End software or firmware functionality. Thus, the invention executes appropriate code concurrently on separate Front-End Storage Area Network (FE_SAN) and Back-End Storage Area Network (BE_SAN) controllers connected via high-speed network (switched fabric such as InfiniBand, Serial Attached SCSI (SAS), Fibre Channel (FC), Myrinet, etc.).","A number of different storage interfaces may be supported at the BE_SAN with this invention, including standard devices such as SCSI, SAS, Advanced Technology Attachment (ATA), Serial ATA (SATA), FC, and other similar disk storage as well as PCI Express (PCle), Hyper Transport (HT), etc. interfaces.","The FE_SAN interface card (FE_SAN controller) may be provided in different physical formats, PCI, PCI-X, PCle, SBus, or other interface board formats. Each FE_SAN interface card has firmware (software) that provides SCSI device discovery, error handling and recovery, and some RAID functionality. The back-end may be provided in a number of different physical formats such as in the standard disk drive enclosure format (including 19 inch rack and standalone enclosures), or an integrated circuit that is easily adaptable to the standard interior configuration of a SAN controller.","Each BE_SAN controller has firmware (software) that provides SCSI device discovery, fault management, RAID functionality, and disk locking methods for portions of the physical disk, RDMA capability, and error handling functionality. The firmware on each BE_SAN controller (interface) provides all the functionality necessary to interface target disk storage interface such as SCSI, SAS, ATA, SATA, or FC disk drives or other data storage devices and systems.","In an embodiment, the FE_SAN has firmware that provides an Application Programming Interface (API) and an Application Binary Interface (ABI) to allow a host Operating System (OS) and applications to memory-map a file or part of a file residing in the FE_SAN controller memory to directly access the file data bypassing traditional OS stack and SCSI layers.","As ever-increasing demand for more data hence, for more data storage, SAN and RAID technologies available today have failed to meet performance requirements. However, this enabling scalable technology is specifically designed to extend system capabilities and eliminate I\/O bottlenecks.","To achieve the best performance, every server or cluster node should be connected directly or through a rearrangeably non-blocking switched fabric to the SAN subsystem. The SAN subsystem should be able to support the sum of data rates on all links between the servers (or cluster nodes) and the SAN subsystem without significantly increasing the command queue size (the number of outstanding requests in the queue waiting to be executed). However, the architectural designs (hardware and software) of today's SAN subsystems create bottlenecks in the SAN controllers due to insufficient aggregate bandwidth and limited scalability. Even though Fibre Channel (FC) is designed with the intent to allow similar architectures, the protocol complexity and prohibitively high cost of the FC technology (HBAs and FC switches) effectively prevented significant progress in the field of data storage.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIGS. 1A and 1B","FIG. 2","FIGS. 1A and 1B","FIGS. 1A and 1B"],"b":"2"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 3","FIG. 3A"],"b":["352","354","356","354","358","360"]},"Preferably the present invention is implemented in firmware running over a multitasking pre-emptive Real Time Operating System (RTOS) on hardware platform comprised of one or more embedded Central Processing Units (CPUs), possibly Application-Specific Integrated Circuits (ASIC), or Field-Programmable Gate Arrays (FPGA), a Random Access Memory (RAM), and programmable input\/output (I\/O) interfaces. It is to be appreciated that the various processes and functions described herein may be either part of the hardware, embedded microinstructions running on the hardware, or firmware (embedded software) executed by the RTOS. However, it should be further understood that the present invention might be implemented in various other forms of hardware, software, firmware, or a combination thereof.",{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":["FIGS. 5A and 5B","FIG. 4"]},"The SCSI standard defines the device model and SCSI command set for all SCSI devices (please see ). The SCSI command set is designed to provide efficient peer-to-peer operation and control of SCSI devices (disk drives, tapes, printers, etc.) by an operating system. The SCSI command set provides multiple operating systems concurrent access and control over one or more SCSI devices. However, proper coordination of these activities between the multiple operating systems is critical to avoid data corruption. Commands that assist with coordination between multiple operating systems are described in the SCSI standard.",{"@attributes":{"id":"p-0041","num":"0040"},"figref":["FIG. 6","FIG. 6"]},"Atomic execution for some commands in the system from  is an essential part of the invention since it helps protect data integrity. Atomic execution requires that a certain \u201catomic\u201d command be effectively executed in their entirety without interruption by of other commands or messages, regardless of whether those other commands precede or follow the atomic command. Front-End code, generally, performs command processing and translates Virtual Block Addresses (VBA) to Logical Block Addresses (LBA) and sends them to the appropriate BE_SAN controllers. The Back-End code, then, checks whether the command could be executed immediately or not. The command may have to be queued for later execution, or a BUSY signal could be returned to the Front-End code because the SCSI standard typically does not require a guarantee when and in what order commands are executed, particularly when components of a system are busy. Since, the Front-End and Back-End codes execute asynchronously until a command that requires atomic execution is to be started. For successful execution, all the requirements for successful completion of the atomic command have to be met before the atomic command can begin. In an embodiment, writing to device media is an atomic command.","For example, to ensure correct execution of a WRITE command, before writing data to intended LBAs, those LBAs have to be locked and off limit to other WRITE or READ commands. That means that WRITE command has to be executed atomically. The Front-End code examines the command; and determines that a WRITE command has been received. Then, it sends a LOCK request to the appropriate BE_SAN controllers to acquire a distributed lock. Back-End code responds acknowledging successful locking of the requested LBAs. At this point, it is safe to write to the locked LBAs, LBAs usually spread across multiple disk drives. Upon the successful completion of the WRITE command, the LOCK is removed by the Front-End code. There are some other commands that might require atomic execution; however the same locking mechanism could be used for those commands.","The previously described method does not permit concurrent writes to occur to the same LBAs, and thereby limits write performance to a single file. Indeed, it is not possible to actually write data to a disk media simultaneously from two different and independent sources without having a risk of actually corrupting the data. However, if sufficient buffer space is available at BE-SAN controllers the data that have to be written from number of different hosts (initiators) could be concurrently spooled into pre-allocated independent memory buffers without risking data corruption. To achieve this kind of parallelism on a WRITE request FE_SAN controller requests a LOCK for all LBAs that must be written to. If currently there are no other writers, the LOCK will be granted. However, if there is another writer holding a LOCK, every BE_SAN controller that has to be written to checks if there is sufficient buffer space to satisfy the request. If the request could be satisfied a tagged-LOCK will be issued and the data transfer from the host to the allocated buffer would be initiated. The actual writes to the disks would be done in the order that tagged-LOCKs are granted. If the Front-End code for some reason was unable to acquire a LOCK for all the LBAs that it needs it will release all already-locked LBAs and start the process all over again. In case that default number of attempts is exceeded upper management software layers will resolve the issue by verifying the reason for the unsuccessful locking.","An additional advantage is that the LOCKs are associated with LUNs using specific ID (identifiers). Therefore, Back-End code would not have to search through long LOCK tables to discover whether a command (request) could be executed or not. Another advantage is that only LBAs that are locked out might be inaccessible during that time. All other LBAs remain accessible allowing greater level for parallelism.","The actual implementation may vary due to use of some specific hardware or of the shelf software. However, the principal architecture would not change. It would be relatively straight forward to use of-the-shelf inexpensive but not limited to x86 hardware platforms with embedded RTOS or embedded Linux OS on it for the BE_SAN controllers since great deal of functionality is already available. For the FE_SAN controllers the same platform could be used as for the BE_SAN controllers. However, if InfiniBand or Myrinet network is the interconnect of choice between FE_SAN and BE_SAN controllers it would be natural to use of the shelf or modified PCIe single board computer or Myrinet interface cards to execute Front-End code on them. In alternative embodiments, such as those where communications between FE_SAN and host are over a PCI or other bus, a PCI or other bus single-board computer may be used. For entry-level systems, where the cost is more important than performance, the Front-End code could be embedded in the device driver or in a software module running on the host computer.","In an embodiment, individual storage units such as hard disk drives , , , ,  are coupled to back-end storage controller (BE_SAN) . Further, individual storage units such as hard disk drives , , , ,  are coupled to at least a second back-end storage controller (BE_SAN) . Both BE_SANs ,  are coupled to be accessed through network  from at least one or more FE_SAN controller , . Further, each FE_SAN is coupled to at least one host computer , .",{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 9","b":["954","956","958","954","902","904","906","956","908","910","922","958","924","926","928","930","954","954","912","970","912","970","902","904","906","940","1","10","2","3","4","5","6","954","954","954"]},"In an embodiment, virtual disk , operating with drives , , , on the second BE_SAN , is also configurable as a RAID, in an embodiment a RAID-5 mode virtual disk, and in an alternative embodiment a RAID-6 mode virtual disk.","Some existing RAID controllers, including some operable in RAID-5 or RAID-6 modes, have the ability to reconstruct data onto a replacement hard disk from remaining disks used by a RAID dataset or virtual disk; in these systems, once a failed disk has been replaced with a spare disk, data is reconstructed onto the spare disk. Once data is reconstructed onto the spare disk-which becomes a formerly-spare disk, redundancy is restored and, should a second disk used by the RAID virtual disk fail in turn, data can once again be reconstructed from still-functioning disks (which may include the formerly-spare disk) and provided to host systems.","A \u201chot spare\u201d is a disk that is attached to a RAID controller, and unused by existing configured virtual disks. When a disk containing part of data of a RAID virtual disk fails, the \u201chot spare\u201d may be substituted for the failed disk without requiring that a technician physically replace the failed disk. In some systems this replacement may require operator intervention, and in other systems a hot-spare may be substituted for a failed disk automatically. Once a hot-spare is substituted into a RAID dataset or virtual disk, data may be reconstructed onto the hot-spare\u2014which becomes a formerly-spare disk and redundancy may be restored. Typically, however, the hot-spare must be attached to the same RAID controller as the failed drive.","In an embodiment, a storage system such as the one depicted on the  provides access to the stored data on the storage media via FE_SAN  controller's device driver and SCSI interface. However, the same storage system provides an API and ABI for mapping portion of the FE_SAN controller memory  into application address space. UNIX, Linux, and other Operating Systems provide methods, such as POSIX-compliant mmap( ) system call, (see FIG. \u2014PRIOR ART), that allows an application  to map a file  into its memory . Therefore, there is one-to-one correspondence between a memory address and a word in the file. For example, Linux kernel documentation describes in details how mmap method is used in Linux and how the PCI memory is mapped above the highest system memory.",{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 11"},"In an embodiment on , it is shown how memory buffers B and B corresponding to a file LBAs may be allocated, mapped, and distributed across a number of FE_SAN controllers  and  and BE_SAN controllers , , and . Front-End firmware FW and FW running on FE_SAN controllers  and  allocates and manages memory buffers B and B upon receiving a request from an application A or A or host OS via provided API and ABI to map a file or portion of a file into application address space. In addition, the memory buffers may have one or more MEMORY BUFFERs ATTRIBUTEs (MEM_BUFF_ATTR) that define the system behavior upon a command or control message is received.","Applications A and A running on computers  and  () may send a request to map a file or portion of a file into their respective address spaces; Front-End firmware FW or FW running on FE_SAN  and  controllers sends a request, as needed, to the corresponding Back-End firmware FW, FW, or FW running on BE_SAN , , and  controllers to process the request. The memory buffers B, B, B, B, and B are dynamically allocated and managed upon application accesses a specific memory location (that corresponds to a specific LBA). When applications A and A write data to the memory mapped file, a copy-on-write method is used to automatically copy data from FE_SAN \u201cmmapped\u201d (memory-mapped) buffers B and B to corresponding buffers B, B, and B allocated on the BE_SAN controllers , , and . It is presumed that the application(s) that were granted direct memory access to the FE_SAN  and  controllers' memory buffers assume complete control and responsibility for synchronizing and maintaining the file data coherency. For example, a number of different flags can be used with mmap method. Linux mmap( ) man page gives an example of how mmap( ) is implemented in Linux. However, these memory-mapped buffers physically reside in the FE_SAN controller's memory and are mapped into the address space used by the kernel or application at what ordinarily be a software buffer location.","If an application running on host computer  writes data to mmapped memory buffers, of a file, in the FE_SAN controller , the copy-on-write method copies data automatically to the corresponding buffers B, B, and B on BE_SAN controllers , , and . Because, Back-End firmware FW, FW, or FW maintains information about all LBAs that are cached in \u201cmmapped\u201d memory buffers B and B for all FE_SAN controllers  and ; if data is modified on one FE_SAN controller (say ), based on the MEM_BUFF_ATTR, the corresponding memory addresses may be automatically invalidated on all other FE_SAN controllers (in this example this would be FE_SAN ). As described above, on msync( ) system call a LOCK is placed on the LBAs and corresponding memory buffers B, B, and B that have to be flushed to the media to prevent any data modification until the transaction is complete. Because of the latency related to copy-on-write method between FE_SAN and BE_SAN controllers' memory buffers, the data coherency should be guaranteed and maintained by the applications; for instance, an explicit request such as msync( ) or msync_buff( ) could be used to accomplish data synchronization. Typically, msync( ) call is associated with synchronizing data to the media. However, an msync_buff( ) call synchronizes the data in the memory of the FE_SAN and BE_SAN controllers without immediately writing the data to the media.","In an embodiment on , it is shown how memory buffers B and C may be allocated, mapped, and distributed across a number of FE_SAN  and  and BE_SAN , , and  controllers. However, the main difference between the apparatus depicted on  and  is that all memory buffers B and C on the FE_SAN controllers  and  have their corresponding independent buffers B, B, B, and C, C, C on the BE_SAN controllers , , and . Thus, the method to create corresponding memory buffers on the BE_SAN controllers is the same as described above. However, when synchronizing the data, on msync( ) request, all the buffers containing the file data that need to be synchronized are specified in the request. Also, which buffers are invalidated and which are not, on the FE_SAN and BE_SAN controllers, depends on whether they contain a newly written data and on the attributes found in the MEM_BUFF_ATTR. This behavior is explained in details, in an example, in the following flow chart (). The key benefit to maintaining independent memory buffers on the BE_SAN controllers , , and  is that concurrent copy-on-write, even to the same LBAs, does not alter the coherent view of the file data. When an application wants to commit the newly written data to the memory mapped file  (for example, the new data written by application A to the memory buffers B on the FE_SAN controller  and mirrored to corresponding independent buffers B, B, and B on the BE_SAN controllers , , and ), the file data synchronization is done upon explicit request. Thus, the applications on different host computers  and  may see different data if the buffers , , B, B, B, C, C, and C are not synchronized. This atomic synchronization is done upon receiving an explicit request, for example using msync( ) or msync_buff( ) calls, from an application A or host OS, via provided API and ABL In addition, copy-on-write (data mirroring) protects the data in case that either FE_SAN  or  or BE_SAN controller , , or  fails.","Similarly to the apparatus and method described with reference to , Back-End firmware FW maintains information about all \u201cmmapped\u201d memory buffers B and C on all FE_SAN controllers  and . For example, an application A on the computer  issues msync( ) or msync_buff( ) synchronization request via Front_End firmware FW running on the EF_SAN controller . Upon receiving msync( ) request, the Back-End firmware FW synchronizes the data written by the application A. Thus, subsequent read commands return the data with the latest and up-to-date changes. However, if the data in the memory buffers C is modified, the MEM_BUFF_ATTRs for the buffers of the file define whether the modified data should be invalidated or not. Thus, the actual behavior of the system may be altered by privileged applications and users.","The flow diagram of an embodiment on the  shows a method of the present invention for data synchronization which is done upon an application A request to commit the written data into FE_SAN controller's memory which is mapped into the application's address space. However, the file or portion of the file is mapped via corresponding memory buffers B and C to the applications' A and A address spaces via API and ABI primitives.","The method is entered at step  after application A sends a request to the Front-End firmware FW to synchronize the data that is placed into specified LBAs in the memory buffers B. The Front-End firmware FW calculates on which BE_SAN controllers , , or  the mirrored LBAs reside. Then at step  the Front-End firmware FW sends synchronize (LBAs) requests to all BE_SAN controllers , , or  that own the modified LBAs.","At step , Back-End firmware FW executes lock (LBAs) method placing LOCKs on all memory buffers B and C containing the modified LBAs. At step  the Back-End firmware FW executes sync_lbas (LBAs) which either modifies the pointers to point to the memory locations with up-to-date data or copies to the memory locations that maintain the up-to-date LBAs.","At step  the firmware FW creates a list of all the FE_SAN controllers that keep cached version of the LBAs that are to be synchronized and at step  sends a (multicast) message to those FE_SAN controllers (in this example ) requesting that those LBAs, or the data in the corresponding memory buffers, to be invalidated.","In this example, FE_SAN controller  is the only controller that keeps those LBA in the cache; thus, at the step  the Front-End firmware receives and starts the execution of invalidate_msg (LBAs). At the next step , the Front-End firmware FW determines if those LBAs contain modified data since the last update from the BE_SAN controller. If the answer is No, it proceeds to the step  and invalidates specified LBAs and sends the done_invalid_msg (LBAs) to the requesting BS_SAN controller ().","At the step , Back-End firmware on the BE_SAN controller  invalidates the corresponding buffers from the previous step, updates pointers and proceeds to step .","At step  requesting Back_End firmware FW releases the locks for the successfully invalidated LBAs on the FE_SAN controllers () and the method  ends.","However, if the answer at step  is Yes, then the firmware determines from the MEM_BUFF_ATTR whether it should or should not invalidate those LBAs (or pages). If the answer is Yes, proceeds to the step ; however, if the answer is No then it proceeds to the step  and informs the Back-End firmware about the action taken.","At step  the Back_End firmware FW releases the locks for the LBAs (or pages) specified in the message from the FE_SAN controllers () and the method  ends. Obviously, the actual implementation, based on the actual hardware properties, may be somewhat different than the case presented in the example here but the main idea would stay the same.","Back-End firmware FW, FW, and FW running on BE_SAN controllers , , and  maintain read and write pointers (for each memory buffer) on every FE_SAN controller  and  and BE_SAN controller , , and . Thus, on an application msync( ) request the read pointers for the memory pages specified in msync( ) request for all FE_SAN controllers will point to the memory buffers with the up-to-date data or the latest written data as described above. Thus, all the memory buffers specified in msync( ) request on all other FE_SAN controllers based on the MEM_BUFF_ATTR attributes may be automatically invalidated. The granularity of the memory buffers is one page (or block) which is typically 512 bytes or 4 KB in size, obviously the actual size depends on specific hardware and software implementation. When an application writes into a memory buffer on the FE_SAN controller  then a dirty data flag is set automatically. When application commits the data, the Back-End firmware looks into a list to find all FE_SAN controllers which cached that particular page(s) and sends a multicast message to invalidate that\/those page(s) on all other FE_SAN controllers (in this case FE_SAN controller ).","Thus, in general, if in the meantime another instance of an application has altered the same page(s) (LBAs) on another FE_SAN controller then based on the attributes set in the MEM_BUFF_ATTR it is defined what the firmware on that controller does, either invalidates that page(s) and rereads it\/them from the BE_SAN controller(s) on the next attempt to read from the page or ignores the \u201cinvalidate\u201d message. In either case, the parallel application has to be aware that the data may have been altered on a remote FE_SAN controller.","However, if for some reason an application does not coordinate the I\/O requests but still requires strict data coherency, then the data coherency may be maintained by executing atomic write or read requests which acquire locks and execute atomically as described herein for a standard write command.","In addition, as described above, periodically or on mflush( ) system call, a LOCK is placed on the LBAs and memory buffers that have to be flushed to the media to prevent any data changes while the transaction is in progress. Thus, upon completion of the current \u201cflush data to the media\u201d command, the data on the media is up-to-date.","If an explicit \u201catomic\u201d read or write operation is required, an application executes an atomic read a_read( ) or atomic write a_write( ) call. An atomic read or write request always invokes synchronization primitives using the same method as described above. Thus, a_read( ) request issued by any application running on a  or  computer after a_write( ) always returns the latest data. In this case MEM_BUFF_ATTR cannot alter this atomic execution. The behavior of a_read( ) and a_write( ) commands is the same as if a SCSI read or write command is issued with a Force Unit Access (FUA) or FUA_NV (Force Unit Access Non-Volatile) bit set.","In addition, if an application needs\/wants to create a persistent shared memory, it creates a memory mapped shared file while the coherent view of the data enforcement may be done by simply executing \u201catomic\u201d a_read( ) and a_write( ) commands\/calls. However, the techniques described above may be used to accomplish the same goal if strict data coherency is not required; even write-back-caching without mirroring technique may be used.",{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 15","b":["1576","1578","1572","1574"]},"A file system module ( or ) residing on the FE_SAN controller  or , communicates directly with the FE_SAN controller Front-End firmware FW and FW. In addition, the Front-End firmware FW and FW provides automatic synchronization as described above for all fread( ) fwrite( ), fopen( ), fclose( ), and other calls made to the file system modules  and .","However, if the system cost is the most important concern, then the entire or part of the file system modules  and  could be executed in host computers'  and  memory as separate software modules communicating with FE_SAN controller's firmware via provided API and ABI.","In another embodiment of the present invention illustrated in , in addition to providing support for storage devices such as HDDs and SSDs, , , , and  attached at BE_SAN controller(s)  (and\/or ), it may also support and integrate legacy storage RAID and SAN systems,  and , as well as other storage devices that can be accessed via directly connected links or via SAN Interconnect(s) . Thus, the data stored on the back-end storage devices, , , , and , as well as on legacy storage systems,  and , may be accessed by an application via FE_SAN,  and , and BE_SAN controllers  and . The benefit of using such approach is to leverage the existing storage infrastructure while caching technique and method described in the present invention and supported by FE_SAN,  and , and BE_SAN controllers'  and , hardware and embedded firmware and software to aggregate, reshape, and cache the data to speed up an application access to the cached data. Because of the FE_SAN and BE_SAN controllers' ability to aggregate the cached data and reshape the back-end I\/O traffic, the utilization of the legacy RAID and SAN systems,  and , should be also improved due to receiving mostly optimal I\/O workload. The LUNs presented by the legacy storage systems  and  may be re-exported via BE_SAN and FE_SAN controllers, ,, , and , or aggregated into new LUNs and presented to the host systems  and . Any network topology of interconnecting networks (or fabrics)  and  is allowed. All data access and locking methods as well as FE_SAN controller's memory mapping methods disclosed in this document may be used with the system architecture presented in the . In addition, the system may be used with embedded file systems as described above.","The term firmware as used herein with reference to FE_SAN and BE_SAN firmware includes machine readable instructions executable by a processor and present in a memory associated with that processor, whether the memory is a read-only memory, an erasable (whether ultraviolet or electrically) and rewritable read only memory, or a random-access memory loaded when the system is started. As such, the term firmware may include portions traditionally defined as software as well as portions traditionally defined as firmware.","The foregoing description should not be taken as limiting the scope of the disclosure. It should thus be noted that the matter contained in the above description or shown in the accompanying drawings should be interpreted as illustrative and not in a limiting sense. The following claims are intended to cover generic and specific features described herein, as well as all statements of the scope of the present method and system."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE FIGURES","p":["The foregoing and other objects, features and advantages of the invention will be apparent from the following more comprehensive description of preferred embodiments of the invention, as illustrated in the accompanying drawings in which like reference characters refer to the same parts throughout the different views.",{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIGS. 1A and 1B"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 3","FIG. 3A"]},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIGS. 5A and 5B"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 6","FIG. 3","FIG. 5B","FIG. 6","FIG. 5B"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 7A","FIG. 7B","FIGS. 7A and 7B"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIGS. 8A and 8B","FIG. 7B"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 15","b":["1576","1578","1572","1574"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 16"}]},"DETDESC":[{},{}]}
