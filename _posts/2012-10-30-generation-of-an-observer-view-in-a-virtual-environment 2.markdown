---
title: Generation of an observer view in a virtual environment
abstract: Generation of an observer view in a virtual environment in response to real-time input during a simulation is disclosed. In one embodiment, a device initiates a simulation of a virtual environment. Core view data that identifies a core view in the virtual environment is maintained. The core view is associated with an object in the virtual environment. Core view imagery that depicts a portion of the virtual environment based on the core view data is generated. During the simulation, real-time input that includes first observer view data that identifies a first observer view in the virtual environment is received. The first observer view is unassociated with any object in the virtual environment. First observer view imagery that depicts a portion of the virtual environment based on the first observer view data is generated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09563723&OS=09563723&RS=09563723
owner: Lockheed Martin Corporation
number: 09563723
owner_city: Bethesda
owner_country: US
publication_date: 20121030
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATIONS","TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application claims the benefit of provisional patent application Ser. No. 61\/553,201, filed Oct. 30, 2011, the disclosure of which is hereby incorporated herein by reference in its entirety.","The embodiments relate generally to simulations of virtual environments, and in particular to the generation of an observer view in a virtual environment in response to real-time input during a simulation.","A simulation of a virtual environment typically presents a view of the virtual environment to a participant of the simulation. The view may be from the perspective of an object in the virtual environment that represents the participant, such as an entity that moves about the virtual environment in response to input from the participant, or the view may be from the perspective of an object in the virtual environment with which the participant is associated, such as a car, airplane, or the like.","Sometimes it is desirable to generate an additional view in the virtual environment that is not tied to an object in the virtual environment during a simulation. This may be referred to herein as an observer view. An observer view may provide a participant of the simulation a view of the virtual environment that differs from the view of the virtual environment the participant has by virtue of participating in the virtual environment.","Unfortunately, it may be difficult or impossible to know, prior to the initiation of the simulation, where in the virtual environment such an observer view may be useful. Accordingly, there is a need for flexible mechanisms for initiating observer views in a virtual environment based on real-time input received during a simulation.","Embodiments herein relate to the generation of one or more observer views in a virtual environment during a simulation of the virtual environment. In one embodiment, a device initiates a simulation of a virtual environment. Core view data that identifies a core view in the virtual environment is maintained. The core view is associated with an object in the virtual environment. Core view imagery that depicts a portion of the virtual environment based on the core view data is generated. During the simulation, real-time input that includes first observer view data that identifies a first observer view in the virtual environment is received. The first observer view is unassociated with any object in the virtual environment. First observer view imagery that depicts a portion of the virtual environment based on the first observer view data is generated.","In one embodiment, the core view imagery may be displayed on a display device in a first window, and the first observer view imagery may be concurrently displayed on the display device in a second window.","In one embodiment, a simulation module may expose an application programming interface (API) that is configured to receive data for generating an observer view from an external application that is independent of and external to the simulation module. The input that includes the first observer view data that identifies the first observer view in the virtual environment may be received via the API from the external application.","In one embodiment, the device may receive, during the simulation, a plurality of different real-time inputs, each different real-time input including corresponding observer view data that may identify a corresponding observer view in the virtual environment. Each corresponding observer view may be different from each other corresponding observer view and be different from the core view. A plurality of observer view imagery may be generated, each observer view imagery corresponding to one of the corresponding observer views and depicting a portion of the virtual environment based on the corresponding observer view. The core view imagery may be displayed on a display device in a first window. The plurality of observer view imagery may be concurrently displayed on the display device, wherein each observer view imagery is displayed in a separate window on the display device.","Those skilled in the art will appreciate the scope of the embodiments and realize additional aspects thereof after reading the following description of the embodiments in association with the accompanying drawing figures.","The embodiments set forth below represent the necessary information to enable those skilled in the art to practice the embodiments and illustrate the best mode of practicing the embodiments. Upon reading the following description in light of the accompanying drawings, those skilled in the art will understand the concepts of the embodiments and will recognize applications of these concepts not particularly addressed herein. It should be understood that these concepts and applications fall within the scope of the embodiments and the accompanying claims.",{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 1","b":["10","10","12","14","14","12","12","12","12","16","18","20"]},"The VEM  may be continually updated during the simulation and reflects a current status of the virtual environment. Imagery may be generated, based on the VEM , that depicts portions of the virtual environment and that may be displayed, by way of non-limiting example, on the display device . In some embodiments, the device  may be coupled to a network  to facilitate communications with one or more other devices  that are concurrently involved in the simulation. Such device  may include a simulation module  which may also maintain a VEM  that may reflect the current state of the virtual environment. As the simulation progresses, the device  and the device  may exchange data that facilitates synchronization of the VEM  with the VEM  so that the users  and  of the devices  and , respectively, may perceive substantially identical virtual environments. For example, as the user  manipulates an input device (not illustrated) to move an object in the virtual environment about the virtual environment, positional data identifying such object movements may be provided by the device  to the device  via the network  so the VEM  may be updated to correctly identify a current position of the object in the virtual environment. Accordingly, the simulation module  may render imagery to the user  substantially in real-time that depicts the movements of the object that is being manipulated by the user . Similarly, the user  may also manipulate an object in the virtual environment, and the device  may send positional data identifying such object movements to the device  via the network  so the VEM  may be updated to correctly identify a current position of such object in the virtual environment. While for purposes of illustration only two devices are shown as participating in the simulation, any number of devices may participate in a simulation.","The VEM  may maintain data regarding a plurality of objects ---N (generally, objects ) that are part of the virtual environment. An object  may represent any entity or thing that is part of the virtual environment, such as, by way of non-limiting example, a vehicle, a tree, or an animal. Typically, each human participant in the simulation may be represented as an object in the virtual environment, and may be a particular type of object based on the particular simulation, such as a human object, a car object, or the like.","All or some of the plurality of objects ---N may have associated core view data ---N (generally, core view data ) that may identify a core view in the virtual environment. The core view data  may comprise, for example, information that identifies a portion of the virtual environment, such as a volume in the virtual environment, that may be perceivable by the corresponding object . The core view data  may include, by way of non-limiting example, a location identifier that identifies a location within the virtual environment, a view direction identifier that identifies a view direction in the virtual environment with respect to the location, a horizontal field-of-view (FOV) identifier that identifies a horizontal FOV, and a vertical FOV identifier that identifies a vertical FOV.","The simulation module  may render core view imagery for display on the display device  based on the core view data . For example, assume that the user  is represented in the virtual environment by the object -. The object - may comprise, by way of non-limiting example, a simulated human in the virtual environment. Thus, as the user  manipulates an input device, such as a keyboard, joystick, or other input device, the simulation module  may translate such manipulations into movements of the object - in the virtual environment. The simulation module , based on the core view data -, may render, or may otherwise generate, imagery, referred to herein as core view imagery to distinguish such imagery from observer view imagery discussed below, and may display core view imagery  in a first window  on the display device . The core view imagery  may depict the portion of the virtual environment that is within the core view defined by the core view data -. As the user  manipulates the input device, such manipulations may change the location of the object - in the virtual environment, and\/or the view direction of the object -, and thus, the core view data - may change in accordance with such manipulations. The simulation module  may continually render new imagery based on the updated core view data -, providing the user  a real-time view of the virtual environment that changes in response to manipulations of the object -.","As will be discussed in greater detail herein, the simulation module  may also maintain one or more observer views ---N (generally, observer views ), each of which is identified via corresponding observer view data ---N (generally, observer view data ). An observer view , in contrast to a core view, is unassociated with any object  in the virtual environment. The observer view data  may include information such as, by way of non-limiting example, an observer view location identifier that identifies a location within the virtual environment, a view direction identifier that identifies a view direction in the virtual environment with respect to the location, a horizontal field-of-view identifier that identifies a horizontal field-of-view, and a vertical field-of-view identifier that identifies a vertical field-of-view.","The observer views  may be generated in response to real-time input received during the simulation. Such information may be received, for example, via real-time input entered by a user, such as the user . In some embodiments, the simulation module  may interact with one or more external applications , and may receive real-time input that may include observer view data from such external application . The phrase \u201cexternal application\u201d is used herein to refer to a process that may execute in conjunction with a processor, and is initiated separately from and independent of the simulation module . The external application  may execute on the same device  as the simulation module , or, as will be described in greater detail herein, may execute on a different device that is communicatively coupled to the device .","In one embodiment, the simulation module  may interact with the external application  by exposing to the external application  an application programming interface (API)  that is configured to receive data for generating an observer view . Thus, during runtime, the external application  may interface with the simulation module  via the API  and interact with the simulation module  to generate, define, update, and\/or delete one or more observer views . It should be noted that the embodiments are not limited to the use of an API, and other inter-process communication mechanisms may be used.","The simulation module  may generate observer view imagery that depicts a portion of the virtual environment that is within the observer view  defined by the corresponding observer view data . For example, observer view imagery  associated with the observer view - may be displayed in a second window  on the display device . Observer view imagery  associated with the observer view -N may be displayed in a third window  on the display device . Notably, the core view imagery , observer view imagery , and observer view imagery  may all be concurrently displayed on the display device , such that the user  has three simultaneous, different views of the virtual environment.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 2","FIG. 1","FIG. 1"],"b":["56","56","56","56","32","56","20","56","32","32","1","32","32","32","1","34","1","58","1","56","56","58","1","32","1","32","2","32","5","32","32","58","1"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 2","FIG. 2"],"b":["40","1","40","1","60","62","32","1","32","5","40","1","40","1","32","1","32","5","60","58","1","32","1","58","1","40","1","40","1","40","1","32","56","32","1"]},"A core view  and an observer view  may be defined in any desirable manner, and in some embodiments, the same type of information may be used to define either a core view  or an observer view . In one embodiment, a core view  and an observer view  may be defined via a view frustum.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 3","b":["64","32","32","56","64","66","66","56","58","32","40","32","56"]},"The view frustum  may extend in a view direction , which may be defined by a view direction identifier. The view frustum  may include a near plane , a far plane , and top, right, bottom, and left planes  that define a volume of the virtual environment . The top, right, bottom, and left planes  may be defined, for example, via a horizontal field of view (FOV)  which is identified by a horizontal FOV identifier and a vertical FOV  which is identified by a vertical FOV identifier. The horizontal FOV identifier and the vertical FOV identifier may define the respective FOVs in terms of degrees, for example. For a core view , the view frustum  may be identified in the corresponding core view data . For an observer view , the view frustum  may be identified in the corresponding observer view data .","In one embodiment, in operation, to generate and display core view imagery associated with a core view , the simulation module  accesses the core view data , intersects the VEM  with the view frustum defined by the core view data , and that portion of the virtual environment that, according to the VEM  is within the volume of the view frustum, may be rendered and displayed. In this example, at a particular instance in time, that portion of the virtual environment may include objects -A--F. This process may be repeated many times each second so that changes in the virtual environment may be displayed substantially in real-time as such changes occur. The process may be similar for displaying observer view imagery associated with an observer view .",{"@attributes":{"id":"p-0038","num":"0037"},"figref":["FIG. 4","FIG. 4","FIG. 1","FIG. 4","FIG. 4","FIG. 4","FIG. 4"],"b":["12","16","1000","16","34","1002","32","12","36","34","1004","12","42","1","40","1","1006","40","1","32"]},"As discussed above, the real-time input may come, for example, from the user . In one embodiment, the user  may enter such real-time input into the external application , which may provide the observer view data - to the simulation module  via the API . The device  may generate observer view imagery  that depicts a portion of the virtual environment based on the observer view data - (, step ).","The device  may display the core view imagery  in a first window  and concurrently display the observer view imagery  in the second window . In one embodiment, the first observer view data - may include a location identifier that identifies a location within the virtual environment, a view direction identifier that identifies a view direction in the virtual environment, a horizontal FOV identifier that identifies a horizontal FOV and a vertical FOV identifier that identifies a vertical FOV.","In one embodiment, the user  may provide updated real-time input that alters one or more of the observer view data -. The external application  may provide the updated real-time input via the API  to the simulation module . For example, the updated real-time input may include a different observer view location identifier that differs from the first observer view location identifier provided by the user . In response to receiving the updated observer view location identifier, the device  may alter the observer view - based on the second observer view location identifier to generate an altered observer view -. The device  may generate new observer view imagery  that may depict a second portion of the virtual environment based on the updated observer view data -.",{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 5","b":["10","80","12","22","80","82","82","16","16","46","82","84","80","84","86","86","88","90","92","94"]},"The API  may comprise any one or more function calls, method invocations, or other functional programming modules for defining, modifying and deleting an observer view. In one embodiment, the API  may include the following functions:\n\n","The external application  provides the observer view data  to the API  of the simulation module  via the network . The simulation module , in response to receiving the observer view data , may generate an observer view  based on the observer view data . The simulation module  may then generate observer view imagery  that depicts a portion of the virtual environment based on the observer view data . Notably, the user  now has an additional view of the virtual environment based on real-time input provided by the user  at a device that is communicatively coupled to the device .","The user  may also provide updated real-time information that alters the observer view . For example, the user  may provide real-time input that includes a second observer view location identifier that identifies a second location in the virtual environment that differs from the location identified by the observer view location ID . The observer view data  may be updated with the second observer view location identifier, and any other updated information. The simulation module  then alters the observer view  based on the second observer view location identifier to generate altered observer view imagery . The altered observer view imagery  depicts a second portion of the virtual environment based on the updated observer view data .","The user , or the user , may also provide real-time input to generate one or more additional observer views. For example, the user  may provide real-time input to the external application  that may include observer view data  such as an observer view location identifier that identifies a location within the virtual environment, a view direction identifier that identifies a view direction in the virtual environment, a horizontal FOV identifier that identifies a horizontal FOV, and a vertical FOV identifier that identifies a vertical FOV. The external application  may provide the observer view data  to the API  of the simulation module . The simulation module , in response to receiving the observer view data , generates an observer view  based on the observer view data  received from the external application . The simulation module  may then generate observer view imagery  that depicts a portion of the virtual environment based on the observer view data . Note that the observer view imagery  may depict a different portion of the virtual environment than both the observer view imagery  and the core view imagery . Thus, the simulation module  may receive, during the simulation, a plurality of different real-time inputs, each different real-time input including corresponding observer view data that identifies a corresponding observer view in the virtual environment. Each corresponding observer view may be different from each other corresponding observer view and may be different from the core view. While for purposes of illustration only two observer views are discussed, the user  or user  may be able to establish any number of observer views for presentation on the display device . As the number of observer views increases, the size of the windows, such as the windows , , , and any additional windows, may decrease to fit the additional observer views in the area available on the display device .",{"@attributes":{"id":"p-0047","num":"0108"},"figref":["FIG. 6","FIG. 6","FIG. 5","FIG. 6","FIG. 6","FIG. 6","FIG. 6","FIG. 6","FIG. 6","FIG. 6","FIG. 6","FIG. 6","FIG. 6"],"b":["16","82","16","2000","16","16","16","46","2002","82","80","2004","82","46","2006","82","84","86","16","46","2008","86","16","96","2010","82","86","84","2012","16","86","96","86","2014","16","98","86","2012","2014","82","16","2016","16","96","2018"]},{"@attributes":{"id":"p-0048","num":"0109"},"figref":["FIG. 7","FIG. 7","FIG. 5"],"b":["106","106","16","44","82","106","108","110","112","114","116","118","120","110","112","114"]},"A focal length field  may allow the user to provide a particular focal length of the observer view. A drop down list  may allow the user to select a particular type of observer view imagery from a list of potential types of observer view imagery that could be generated, such as infrared (IR) imagery, non-IR imagery, colored imagery, black and white imagery, and the like. A horizontal FOV field  may allow the user to define a desired horizontal FOV identifier that identifies a desired horizontal FOV. A vertical FOV field  may allow the user to define a desired vertical FOV identifier that identifies a desired vertical FOV. In some embodiments, particular manipulations of an input device by the user, such as the depression of a particular key on a keyboard, may be translated as a movement of the observer view in the virtual environment. A linear step size field  may allow the user to identify, for each such discrete manipulation by the user, an amount of movement of the observer view in the virtual environment. Thus, for purposes of illustration, assume that an \u201cUp Arrow\u201d key on an input keyboard is mapped to a forward linear movement of the observer view in the virtual environment, and a \u201cDown Arrow\u201d key is mapped to a reverse linear movement of the observer view in the virtual environment. In this example, a single depression of the Up Arrow key by the user would translate into a 1 meter forward movement of the observer view in the virtual environment. Thus, repeated depressions of the Up Arrow key would result in successive 1 meter forward movements of the observer view. A single depression of the Down Arrow key by the user would translate into a 1 meter reverse movement of the observer view in the virtual environment. Thus, repeated depressions of the Down Arrow key would result in successive 1 meter reverse movements of the observer view.","An angular step size field  may allow the user to identify, for each such discrete manipulation by the user, an amount of angular movement of the observer view in the virtual environment. Assume further for purposes of illustration that a \u201cLeft Arrow\u201d key is mapped to a left angular movement of the observer view in the virtual environment, and a \u201cRight Arrow\u201d key is mapped to a right angular movement of the observer view in the virtual environment. In this example, a single depression of the Left Arrow key by the user would translate into a 3 degree rotation of the observer view to the left. Repeated depressions of the Left Arrow key would result in successive 3 degree rotations of the observer view to the left. A single depression of the Right Arrow key by the user would translate into a 3 degree rotation of the observer view to the right. Repeated depressions of the Right Arrow key would result in successive 3 degree rotations of the observer view to the right.","While for purposes of illustration particular keys on a keyboard have been discussed as being associated with movements of the observer view within the virtual environment, it will be understood that any keys may be mapped to a desired movement or rotation of the observer view within the virtual environment, and indeed any input device, such as a mouse, a joystick, a wireless motion controller comprising an accelerometer, or the like, may be used to manipulate the observer view within the virtual environment.","As mentioned previously, any number of observer views in the virtual environment may be established.  is a diagram of an observer view management user interface  which may be used by a user to manage multiple observer views that have been established in the virtual environment according to one embodiment. The user interface  may be offered by one or more of the simulation module , the external application , or the external application . A scroll list  may allow the user to scroll through multiple observer views based on observer view names provided, for example, by a user in the observer view name field  () when establishing the respective observer view. Assume for purposes of illustration that the user has selected the observer view identified by a particular observer view name . Upon selection of the particular observer view name , the observer view management user interface  may provide in an observer view data area  attributes of the selected observer view, such as the current position, or location, of the observer view in the virtual environment, the current heading, or direction, of the observer view in the virtual environment, the current horizontal and vertical FOVs, respectively, the linear and angular step sizes, focal length, and the like. The user may edit any one more of these attributes by selecting respective Edit buttons  that are located adjacent to such information.","The user may be able to delete one or more observer views by selecting a particular observer view name identified in the scroll list , and activating a Delete Observer View button . If the observer view management user interface  is being offered by an external application, upon receipt of the selection of the Delete Observer View button , the observer view management user interface  may generate a delete observer view message identifying the respective observer view, and send the delete observer view message to the simulation module . The simulation module  may then delete the respective observer view.","The user may also be able to create one or more observer views by selecting a Create Observer View button . Upon selecting the Create Observer View button , an additional user interface window, such as the user interface  illustrated in  may be provided to the user to allow the user to create a new observer view in the virtual environment.",{"@attributes":{"id":"p-0055","num":"0116"},"figref":"FIG. 9","b":["12","12","12","151","152","154","154","152","151","151","151"]},"The system bus  may be any of several types of bus structures that may further interconnect to a memory bus (with or without a memory controller), a peripheral bus, and\/or a local bus using any of a variety of commercially available bus architectures. The system memory  may include non-volatile memory  (e.g., read only memory (ROM), erasable programmable read only memory (EPROM), electrically erasable programmable read only memory (EEPROM), etc.) and\/or volatile memory  (e.g., random access memory (RAM)). A basic input\/output system (BIOS)  may be stored in the non-volatile memory , and can include the basic routines that help to transfer information between elements within the device . The volatile memory  may also include a high-speed RAM, such as static RAM, for caching data.","The device  may further include a computer-readable storage device , which may comprise, for example, an internal hard disk drive (HDD) (for example, an enhanced integrated drive electronics (EIDE) HDD or serial advanced technology attachment (SATA) HDD), a flash memory, or the like. The computer-readable storage device  and other drives, sometimes referred to as computer-readable or computer-usable media, provide non-volatile storage of data, data structures, computer-executable instructions, and the like. Although for purposes of illustration the description of the computer-readable storage device  above refers to a HDD, it should be appreciated by those skilled in the art that other types of media which are readable by a computer, such as Zip disks, magnetic cassettes, flash memory cards, cartridges, and the like, may also be used in the operating environment, and further, that any such media may contain computer-executable instructions for performing novel functionality as disclosed herein.","A number of modules can be stored in the computer-readable storage device  and in the volatile memory , including an operating system module  and one or more program modules , which may implement the functionality described herein in whole or in part, including, for example, functionality associated with the simulation module , the API , and the external application . It is to be appreciated that the embodiments can be implemented with various commercially available operating system modules  or combinations of operating system modules .","All or a portion of the embodiments may be implemented as a computer program product stored on a non-transitory computer-usable or computer-readable storage medium, such as the computer-readable storage device , which includes complex programming instructions, such as complex computer-readable program code, configured to cause the processor  to carry out the functionality described herein. Thus, the computer-readable program code can comprise software instructions for implementing the functionality of the embodiments described herein when executed on the processor . The processor , in conjunction with the program modules  in the volatile memory , may serve as a control system for the device  that is configured to, or adapted to, implement the functionality described herein.","A user may be able to enter commands and information into the device  through one or more input devices, such as, for example, a keyboard (not illustrated), a pointing device such as a mouse (not illustrated), a touch-sensitive surface (not illustrated), or the like. Other input devices may include a microphone, an infrared (IR) remote control, a joystick, a game pad, a stylus pen, or the like. These and other input devices may be connected to the processor  through an input device interface  that is coupled to the system bus , but can be connected by other interfaces such as a parallel port, an Institute of Electrical and Electronic Engineers (IEEE) 1394 serial port, a Universal Serial Bus (USB) port, an IR interface, and the like.","The device  may also include a communication interface  suitable for communicating with the network . The device  may also include a video port  interfacing with the display device  that provides information to the user .","Those skilled in the art will recognize improvements and modifications to the embodiments. All such improvements and modifications are considered within the scope of the concepts disclosed herein and the claims that follow."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The accompanying drawings incorporated in and forming a part of this specification illustrate several aspects of the embodiments, and together with the description serve to explain the principles of the embodiments.",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
