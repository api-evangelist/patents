---
title: Depth-of-field effects using texture lookup
abstract: A graphical processing unit (GPU) and methods for rendering a three-dimensional (3D) scene generated in a field of view having in-focus and out-of-focus regions on a two-dimensional (2D) screen region of pixels are described. One method includes initially rendering the scene to create color and depth texture maps and creating mip-map layers for the color texture map. The method further comprises subsequently rendering the scene by, for each pixel: creating a mip-map layer selection value as a function of a depth of the pixel from the depth texture map, generating a color value by interpolation using color values from at least one of the mip-map layers chosen according to the mip-map layer selection value, and setting a color of the pixel to the generated color texture.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06975329&OS=06975329&RS=06975329
owner: NVIDIA Corporation
number: 06975329
owner_city: Santa Clara
owner_country: US
publication_date: 20021209
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT","MULTIPLE TEXTURE LOOKUPS","FILTERING DEPTH VALUES","APPLICATION PROGRAMMING INTERFACE"],"p":["1. Field of the Invention","One or more aspects of the present invention generally relate to computer graphics and, more particularly, to rendering three-dimensional (3D) scenes with depth-of-field correction.","2. Description of the Related Art","In general, three-dimensional (3D) computer graphics is the art and science of generating 3D scenes on a two-dimensional (2D) computer screen. In a 3D animation, a sequence of images is displayed, giving the appearance of motion in three-dimensional space. Interactive 3D computer graphics allows a user to change viewpoint or geometry in real-time, thereby requiring the graphics system to create new images. Conventionally, the higher the number of images that are displayed per second, commonly referred to as the frame rate, the smoother the animation will appear but the more processing power and system bandwidth is required. For example, the human eye may detect new images coming onto the screen at frame rates less than 30 frames per second (fps). Several factors may affect the actual frame rate on a computer graphics system, such as the number of pixels on the screen. Generally, higher frame rates are possible with a lower number of pixels, but visually perceptible image quality may suffer.","While the number of pixels and frame rate is important in determining graphics system performance, visual quality of the image generated may be equally important. For example, an image on a 2D screen with a high pixel density may still appear unrealistic if all the objects appearing in the scene are in focus. The unrealistic appearance is due to an incorrect (or lack of) depth-of-field effect of the image. Depth of field generally refers to the distance range (i.e., an in-focus range) from a viewpoint within which objects in an image look sharp. In general, depth-of-field corrections attempt to blur objects or areas of an image that are either closer or farther away from a particular focal point.",{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 1","b":["100","112","114","114","112","100","122","124","126","128"]},"Traditional approaches to performing depth-of-field corrections on a 3D scene typically perturb (i.e., jitter) a viewpoint multiple times, rendering the scene for each perturbation. The rendered scenes for each of the perturbations are accumulated in an accumulation buffer. After all the perturbations, the final scene in the accumulation buffer (possibly after some filtering) may be transferred to a frame buffer. The concept is that, by perturbing the viewpoint, objects that are in focus should remain in focus, while objects that are out of focus should change with each perturbation. The final accumulated values for pixels of out-of-focus objects should be a sum of the value of neighboring pixels offset by a distance determined by the perturbations. Accordingly, out-of-focus objects should appear blurry in the final scene. Another approach to depth of field effects is achieved by randomizing sample locations, as described in U.S. Pat. No. 4,897,806.","However, all these previous approaches conventionally require that the scene be rendered numerous times (or, the rendering to have many more samples per pixel) to achieve acceptable image quality. As a result, the processing time required to achieve acceptable image quality using these approaches may lower the frame rate of the graphics system to an unacceptable level. Therefore, these conventional approaches to depth-of-field corrections may be unsuitable for real time 3D graphics processing systems.","Accordingly, a need exists for improved rendering of 3D scenes with depth-of-field effects, namely, where less processing time is used.","The present invention generally provides methods and apparatus for rendering a three-dimensional (3D) scene generated in a field of view having in-focus and out-of-focus regions.","An aspect of the present invention includes initially rendering the scene to create color and depth texture maps and creating mip-map layers for the color texture map. The final scene is subsequently generated by, for each pixel: creating a mip-map layer selection value as a function of a depth of the pixel from the depth texture map, generating a color value by interpolation using color values from at least one of the mip-map layers chosen according to the mip-map layer selection value, and setting a color of the pixel to the generated color value.","For some embodiments, a filtering function may be applied to filter depth values of the depth texture map. For some embodiments, one or more actions which may be included are: creating a set of texture coordinate tuples, generating a color value for each texture coordinate tuple by interpolation using color values from at least one of the mip-map layers chosen according to the mip-map layer selection value, and generating a color value for the pixel by blending together the color values generated for each texture coordinate tuple.","One or more other aspects of the present invention include providing an application program interface (API) to interface with a graphics processor, receiving the 3D scene by the API from an application program, and passing the 3D scene to the graphics processor from the API, wherein the graphics processor is configured to initially render the scene to create color and depth texture maps and create mip-map layers for the color texture map. The graphics processor is further configured to subsequently generate the final scene by, for each pixel: creating a mip-map layer selection value as a function of a depth of the pixel from the depth texture map, generating a color value by interpolation using color textures from at least one of the created mip-map layers chosen according to the mip-map layer selection value, and setting a color of the pixel to the generated color value.","Still other aspects of the present invention provide graphical processing units (GPUs) for rendering a three-dimensional (3D) scene on a two-dimensional (2D) screen region of pixels. The GPUs may include a graphics processor configured to initially render a scene to create color and depth texture maps, and mip-map layers for the color texture map. The GPUs may also include a shader unit configured to subsequently render the scene by, for each pixel: creating a mip-map layer selection value as a function of a depth of the pixel from the depth texture map, generating a color value by interpolation using color values from at least one of the mip-map layers chosen according to the mip-map layer selection value, and setting a color of the pixel to the generated color value.","Still other aspects of the present invention provide a computer-readable medium containing a program which, when executed by a processor, performs operations for rendering a three-dimensional (3D) scene generated in a field of view having in-focus and out-of-focus regions on a two-dimensional (2D) screen region of pixels. The operations generally include initially rendering the scene to create a color texture map and a depth texture map, and creating mip-map layers for the color texture map, and subsequently rendering the scene by, for each pixel, creating a mip-map layer selection value as a function of a depth of the pixel from the depth texture map, generating a color value by interpolation using color values from at least one of the mip-map layers chosen according to the mip-map layer selection value, and setting a color of the pixel to the generated color value.","Still other aspects of the present invention provide a computer system containing a processor and a storage medium containing a program which, when executed by the processor, performs operations for rendering a three-dimensional (3D) scene generated in a field of view having in-focus and out-of-focus regions on a two-dimensional (2D) screen region of pixels. The operations generally include initially rendering the scene to create a color texture map and a depth texture map, and creating mip-map layers for the color texture map, and subsequently rendering the scene by, for each pixel, creating a mip-map layer selection value as a function of a depth of the pixel from the depth texture map, generating a color value by interpolation using color values from at least one of the mip-map layers chosen according to the mip-map layer selection value, and setting a color of the pixel to the generated color value.","The present invention generally provides for performing depth-of-field corrections when rendering a three-dimensional (3D) scene on a two-dimensional (2D) screen of pixels. Included are capturing depth and color values for each pixel as textures in a first pass of rendering the 3D scene, and subsequently blurring out-of-focus portions of the scene based on captured depth values for the pixels in a second pass of rendering the object. For different embodiments, different operations may be performed in either hardware or software or a combination thereof.",{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 2","b":["200","200","210","220","230","240","210","220","210","230","210","210","230","220","230","240"]},"For some embodiments, the GPU  may be a pipelined GPU, such as the GeForce\u00ae line of pipelined GPUs available from nVidia of Santa Clara, Calif. As illustrated, the GPU  may include at least one texture unit  and at least one shader unit , which may be separate components or may each be part of a graphics processor. In general, the texture unit  may receive texture coordinates for individual pixel fragments, access texture values from texture maps stored in a texture memory , and perform various levels of filtering on the texture values. Texture memory  may be part of a same memory used for the frame buffer , or may be separate memory dedicated to the texture unit . The shader unit  may access texture values from the texture memory , combine the texture values and\/or perform additional filtering operations to generate a final color value for a pixel that is eventually written to the frame buffer .","For some embodiments, the shader unit  may be designed as a programmable engine, for example, that may execute a set of instructions (a \u201cpixel program\u201d), which may be passed through the API . A pixel program may cause the shader unit  to perform the operations described below. Alternatively, the operations described below may be implemented in hardware as part of the texture unit , as another part of the GPU , or external to the GPU .","One advantage to a programmable shader unit  is that an application programmer may be able to implement various functions for generating scenes with different fields of view, which may be suitable for different applications. For example, the field of view  of  illustrates just one fields of view that may be implemented by the shader unit . Other exemplary fields of view are illustrated in .",{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 3A","FIG. 3B","FIG. 3C"],"b":["312","314","312","314","312","314"]},"Different fields of view may be utilized, for example, to draw a viewer's attention to a particular in-focus object. For example, in-focus regions and out of focus regions may be dynamically changed to keep a selected object (e.g., a character in a computer game, an advertiser's product, etc.) in-focus. One or more corrections for depth-of-field correction may be utilized to keep the selected object in focus while blurring the rest of the scene, drawing a viewer's attention to the selected object. Further, as illustrated in , a field of view may have one or more in-focus regions . More than one in-focus region  may be useful, for example, in applications where tracking particular objects at different depths, while maintaining a high level of detail, is important.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 4","b":"400"},"As previously described, operations of the rendering process  may be implemented in software (e.g., as part of the application program ), in hardware (e.g., as part of the GPU ), and\/or as a pixel program executed by the shader unit . Accordingly, while the following description of the rendering process  may refer to the elements of , other hardware and software elements may also be capable of performing the operations of the rendering process .","The rendering process  begins at step . At step , a scene is rendered to create a texture map of colors and a texture map of depth values (i.e., z-values) for each pixel. For example, the scene may be rendered into a color buffer and a depth buffer. The color buffer and depth buffer may then be captured as color and depth texture maps, for example, in texture buffers of texture unit  of .","At step , mip layers (mip maps) are created for the texture map of colors. The mip maps may be created using a filter kernel of any suitable size and shape. As used herein, a filter kernel refers to a shape of an area of a texture map that includes texture values to be included in a filtering function (i.e., a filtering function with a 2\u00d72 filter kernel would typically perform a weighted blending of 4 texture values together). For some embodiments, the different mip maps may be created using a traditional 2\u00d72 filter kernel, blending four texture values from one mip map to create a single texture value on the next layer mip map. Each mip-map layer represents a level of blurriness of the original in-focus scene.","However, for other embodiments, symmetrical filter kernels larger than 2\u00d72 texels may be used to generate the mip layers in an effort to minimize visible discontinuities on the blurred regions. For example, a filter kernel of 4\u00d74, 6\u00d76 or 8\u00d78 may be used to generate the mip-maps. The visible discontinuities arise from low-order (bilinear) interpolation of low-resolution mip-map layers into higher-resolution regions on the final scene. The slope discontinuities caused by the interpolation of texel values can cause visible artifacts (discontinuities), due to magnification of a region on a low-resolution mip layer onto a larger region on the final scene. A larger filter kernel may effectively average a larger area of texels, creating a smoother transition between texels, so the discontinuities aren't as visible on the displayed pixels.","For some embodiments, the mip-map layer selection values may be generated by passing the texture unit  a value corresponding to a desired spacing of texture coordinates (e.g., ds and dt) for the mip map. For example, a one-dimensional spacing of texture coordinates for a given mip layer N may be 2\/S, where S may be the number of pixels along the corresponding dimension on the mip-mapped texture\/screen. For example, this value may be passed to the texture unit , which may generate a mip-map selection value having the specified change in texture coordinates based on the depth texture map. For other embodiments, a texture unit may generate a mip-map layer selection value based on an integer layer number (i.e., the mip layer N).","The operations of steps  and  may be considered a first rendering pass to perform pre-filtering operations. Steps \u2013 correspond to iterative (i.e., looped) operations performed for each pixel during a second rendering pass. Operations of steps \u2013 may be performed on a number of pixels in a parallel manner. The second rendering pass of steps \u2013 may be thought of as a \u201c2\u00bd-D\u201d rendering: a 2D rendering based on depth values (i.e., a third dimension z).","At step , a pixel is chosen. At step  a z-value for the pixel (captured at step ) is returned from the depth texture map for the chosen pixel. At step , a mip layer selection value is generated based on the z-value for the pixel. Any suitable algorithm may be performed to generate the mip layer selection value. In general, the greater the distance the z-value for the chosen pixel is from an in-focus z-value, the higher the mip layer selection value (i.e., the lower the resolution).","However, as illustrated in , various fields of view may be defined by different functions. Accordingly, the mip layer selection value may be determined by similar functions, or by the same functions. As illustrated in , the mip layer selection values may be mapped directly to the z-axis. For example, z-values corresponding to the in-focus region  may correspond to a mip layer selection value of 0, while z-values corresponding to the out-of-focus regions  may increase from zero accordingly. Therefore, creating a mip-map layer selection value as a function of a depth value may be reduced to looking up a value from a lookup table or map.","At step  a texture value for color is looked up based on the mip layer selection value. For example, a texture-coordinate tuple for one or more mip layers may be calculated for the chosen pixel, and passed to the texture unit . (As used herein, a tuple generally refers to an ordered set of values, such as s and t texture coordinates.) For some embodiments, the texture unit  may return a texture value that is a bilinear interpolation of texel values surrounding the texture-coordinate tuple on the selected mip map. Further, if the mip layer selection value is not an integer (i.e., the mip map selection value is between mip layers), the texture value returned from the texture lookup may be a trilinear interpolation of texel values from mip-map layers above and below the mip layer selection value. For example, if the mip layer selection value is 3.9, the texture lookup may return a value that is a trilinear interpolation of texel values from mip layer  and mip layer , which corresponds to the linear interpolation of the bilinear interpolations on each of the surrounding mip-map layers. For some embodiments, bilinear texture values returned from adjacent mip layers may be blended together with a blending function that gives greater weight to a texture value from a mip layer closest to the mip layer selection value.","At step  the pixel color is set to the color texture value returned from the texture lookup. At step , if more pixels remain, steps \u2013 are repeated. If there are no more pixels at step , the routine is terminated at step . For example, the final pixel values may be written to a page buffer to be displayed on a screen. Thus, by varying the level of mip map (or mip maps) used to generate a final color value for each pixel, based on its depth value, out-of-focus objects should appear blurred in the final screen image of the scene.","As previously described, for some embodiments, symmetrical filter kernels larger in size than 2\u00d72 texels may be used to generate the mip layers in an effort to avoid visible discontinuity artifacts. For some embodiments, a similar but even higher quality result may be obtained by blending texture values from multiple texture lookups using a set of texture-coordinate tuples. In other words, while a smaller filter kernel size may be used to generate the mip maps, a larger area of textures may be sampled to generate the final pixel color, effectively compensating for the smaller filter kernel size. An advantage to this approach may be an improvement in image quality (smoothness) of the blurred regions. Using conventional mip-mapping techniques, magnification of low-resolution layers onto large regions of the final scene may lead to discontinuities, even if the layers are pre-blurred with wide filter kernels. Using multiple texture lookups may reduce or eliminate the problem by performing interpolation (filtering) of higher order than conventional bilinear interpolation.",{"@attributes":{"id":"p-0048","num":"0047"},"figref":["FIG. 5","FIG. 5","FIG. 4"],"b":["502","512","402","412","516","514"]},"At step , the texture values returned from the multiple texture lookups are blended using any suitable blending function. At step , the pixel color is set to the blended texture color value. At step , if more pixels remain, steps \u2013 are repeated. If there are no more pixels at step , the routine is terminated at step . Thus, by performing multiple texture lookups, final pixel values may be blended as if a larger kernel size were used to generate the mip maps, which may improve the smoothness of the resulting blurred region.","The set of texture-coordinate tuples may be created from a single texture coordinate. For example, each of the set of texture-coordinate tuples may be offset from the single texture-coordinate tuple by a fixed offset. The set of texture-coordinate tuples may form a symmetrical pattern around the single texture coordinate. For example, as illustrated in , a set of offset texture coordinates  may form a diamond pattern around an initial single texture coordinate . As illustrated by the dashed arrows in , for each texture coordinate  and , surrounding texels  may be blended together to create a texture value returned from a texture lookup.","According to some aspects, the distance between the offset texture coordinates  (i.e., the offset from the initial texture coordinate ) may be chosen according to the mip layer selection value. As illustrated, for different embodiments, the diamond pattern may be rotated around the initial texture coordinate  at different angles (\u0398) of rotation, while maintaining a constant offset. The angle \u0398 may be 0\u00b0 (), 30\u00b0 (), 45\u00b0 (), or an arbitrary angle N (). It should be noted, however, that the exemplary patterns of  are exemplary only, and that the set of texture coordinates may take on various shapes.","Depth values in the depth texture map may be filtered prior to rendering the final scene. For example,  illustrates operations of a method  in which depth values (z-values) are filtered at step . The remaining operations of  are similar to the operations of the method  of , except that the filtered depth values are used at step  to create a mip layer selection value for the chosen pixel. Any suitable filtering function may be used to filter the depth values. For example, as previously described with reference to the mip-map layers, the filtering function applied to the depth values may use a symmetrical kernel, larger in size than 2\u00d72 texels.","Filtering the depth values prior to rendering the final scene may provide for smoother transitions from foreground objects to background objects, by effectively pushing foreground objects into the background and pulling background objects into the foreground. For some embodiments, filtering the depth values may reduce or eliminate a visible artifact that may occur when performing depth-of-field corrections on a scene with an out-of focus foreground object occluding (i.e., blocks the view of) an in-focus background object.","The artifact may be described with reference to , which illustrate an in-focus background object  occluded by an out-of-focus foreground object .  illustrates the objects  and  prior to performing depth-of-field corrections: both objects appear in focus.  illustrates the artifact that occurs if the depth-of-field corrections are not properly performed. As illustrated, the object  has a blurred interior  and blurred edges . However, rather than blurring from the foreground object  to the background object , which is what would be expected, the exterior of edges  bordering the foreground object  appear sharp, while the interior of edges  appear blurred. The artifact results from a sharp transition from looking up a texture value from a lower resolution mip map to looking up a texture value from a higher resolution mip map at the border between object  and .",{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 8C","b":["810","820","824"]},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 9","b":"900"},"The method  may be performed for each pixel, after capturing the depth values. The method begins at step . At step , a pixel is chosen. Steps \u2013 are performed for each neighboring pixel. A neighboring pixel may refer to any pixel surrounding the chosen pixel in a neighboring range. The neighboring range may be chosen, for example, to optimize performance or image quality. In other words, the size of the neighboring range may determine a number of loops through steps \u2013 are required. For some embodiments, the predefined neighboring range may include only pixels immediately surrounding the chosen pixel (i.e., a 3\u00d73 range of pixels). However, for other embodiments, the neighboring range may be larger than 3\u00d73 pixels.","At step , a neighboring pixel is chosen. At step , a depth value (Z) is retrieved for the chosen pixel. At step , a depth value (Z) is retrieved for the neighboring pixel. At step , if the depth value of the neighboring pixel is less than the depth value of chosen pixel (i.e., the neighboring pixel is in the foreground relative to the chosen pixel), the depth value of the neighboring pixel is included when generating a filtered depth value of the chosen pixel. Otherwise, the depth value of the neighboring pixel is not included, correctly preventing background objects from blurring into foreground objects.","At step , if there are more neighboring pixels, steps \u2013 are repeated. Otherwise, a filtered depth value for the chosen pixel is generated at step  using the depth values of the neighboring pixels included at step . If there are more pixels at step , steps \u2013 may be repeated. Otherwise, the method is terminated at step .","Depth values included in the filtering may be simply accumulated at step . Accordingly, the filtering operation of step  may include an operation to normalize the generated filtered depth value. Further, any suitable filtering operation may be performed at step  to generate the filtered depth value.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":["FIG. 10","FIG. 9","FIG. 10"],"b":["1010","1020","1010","1022","1010","1030","1010","1022"]},"Pixels  and  represent exemplary pixels having the filtered depth value, which may depend on the weighting of a filtering function used to filter the foreground neighboring pixels  with the chosen pixel . For example, given a first weighting function, the resultant filtered depth value may be that of pixel . However, as illustrated, the filtered depth value of pixel  may be in-focus, which may result in the undesirable sharp edge artifact illustrated in . Therefore, for some embodiments, a second weighting function may be used giving a greater weight to the depth values of the foreground neighboring pixels , resulting in the filtered depth value of pixel  (i.e., closer to the depth values of the foreground pixels ) which is out-of-focus. For example, the second filtering function may give a weight to depth values of foreground neighboring pixels  that is proportional to twice the difference between the depth value of the chosen pixel  and depth values of the foreground neighboring pixels .","Referring back to , for some embodiments, the application programming interface (API)  may allow an application program  to specify depth-of-field corrections to be performed on an image rendered by the GPU . For example, the application program  may pass a 3D field of view to the API for display by the GPU , as well as various parameters that specify an amount or type of depth-of-field corrections to perform.","For example, the application program  may pass the API , a single quality parameter having a value between 0 and 1, with 0 corresponding to a poorest quality. In other words, if the application program  passes the API  a quality parameter with a value of 0, a final scene may be rendered with no depth-of-field corrections. Alternatively, if the API is called with a value of 1 for the quality parameter, the final scene may be rendered utilizing a combination of the depth-of-field correction methods described above (e.g., multiple texture lookups, z-value filtering, etc.). The application program  may also pass other parameters to the API  to enable\/disable individual depth-of-field correction features, such as pre-filtering the z-buffer, blurring the z-buffer, and generating multiple texture coordinates for multiple texture lookups.","Accordingly, embodiments of the present invention may provide methods and systems for performing depth-of-field corrections when rendering a three-dimensional (3D) image of a scene having in-focus and out-of-focus regions on a two-dimensional (2D) screen region of pixels. The depth-of-field corrections may result in out-of-focus objects appearing blurred relative to in-focus objects, which may add realism and enhance the quality of the rendered scene.","While the foregoing is directed to embodiments of the present invention, other and further embodiments of the invention may be devised without departing from the basic scope thereof, and the scope thereof is determined by the claims that follow. In the claims, the listed order of method steps do not imply any order to the performance of the steps, unless specifically stated in the claim."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["So that the manner in which the above recited features of the present invention can be understood in detail, a more particular description of the invention, briefly summarized above, may be had by reference to embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments of this invention and are therefore not to be considered limiting of its scope, for the invention may admit to other equally effective embodiments.",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIGS. 3A\u20133D"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIGS. 6A\u20136D"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIGS. 8A\u20138C"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 10","FIG. 9"]}]},"DETDESC":[{},{}]}
