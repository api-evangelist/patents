---
title: Video transmission
abstract: Disclosed is a method of transmitting video via a network and a user device and computer program product configured to implement the method. The method comprises transmitting video of one or more users, received from an image capture device, to at least another user device via the network; receiving information about a communication channel between the user device and the other user device and/or about one or more resources of the user device and/or the other user device; selecting characteristics from a plurality of visual user characteristics based on the received information; and controlling the video based on detection of the selected characteristics to track the selected characteristics.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09307191&OS=09307191&RS=09307191
owner: Microsoft Technology Licensing, LLC
number: 09307191
owner_city: Redmond
owner_country: US
publication_date: 20131119
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Conventional communication systems allow the user of a device, such as a personal computer or mobile device to conduct voice or video calls over a packet-based computer network such as the Internet. Such communication systems include voice or video over internet protocol (VoIP) systems. These systems are beneficial to the user as they are often of significantly lower cost than conventional fixed line or mobile cellular networks. This may particularly be the case for long-distance communication. To use a VoIP system, the user installs and executes client software on their device. The client software sets up the VoIP connections as well as providing other functions such as registration and user authentication. In addition to voice communication, the client may also set up connections for other communication media such as instant messaging (\u201cIM\u201d), SMS messaging, file transfer and voicemail.","Recently, internet capabilities and functionality has been integrated into user devices such as games consoles arranged to be connected to a television set of other (e.g. large-screen) display means, television sets themselves (often referred to as a \u201cSmart TV\u201d), set-top boxes arranged to be connected to a television set etc. . . . This includes the integration of client software into a games console, television set, set-top box (or similar) to enable communications over a packet-based computer network such as the Internet. This integration of client software allows a large, high-resolution screen to be utilised for video calling by outputting video signals to a near-end user received from a far-end user. Furthermore, significant processing power can be provided in user devices such as set-top boxes, TVs etc. particularly as the power requirements for a large, mains electricity powered consumer electronics device are less stringent than, for example mobile devices. This can enable a full range of features to be included in the embedded communication client, such as high quality voice and video encoding of video data received from a camera connected to the user device or from other similar image input means.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","There is disclosed a user device comprising a network interface, a video controller and a resource manager. The network interface is configured to transmit video of one or more users, received from an image capture device, to at least another user device via a network. The video controller is configured to select of one or more characteristics from a plurality of visual user characteristics and control the video based on detection of the selected characteristics to track the selected characteristics. The resource manager is configured to receive information about a communication channel between the user device and the other user device and\/or about one or more resources of the user device and\/or the other user device and, based on the received information, to control said selection by the video controller.","Also disclosed is a user device comprising a network interface for connecting to a network and one or more processors. The one or more processors are configured to transmit video of one or more users, received from an image capture device, to another user device via the network. The one or more processors are further configured to select characteristics from a plurality of visual user characteristics and to generate first and second sets of boundary data based on detection of the selected characteristics at first and second times respectively. The one or more processors are further configured to generate transition data based on the first and second sets of boundary data using a dynamic model and to control the video based on the transition data to track the selected characteristics.","Also disclosed are corresponding methods, and respective computer program products comprising executable code configured to each of those methods.","Disclosed herein is a technique whereby, during a video call (1-to-1 or multiparty), a user's location in a room is detected using depth detection means (a depth detector or similar) and video of the call as transmitted to other user(s) is controlled to track the user based on that detection. More specifically, respective visual characteristics of that user (such as body parts of that user) are so detected, and the video is controlled to track selected ones, but not necessarily all, of those characteristics (e.g. body parts). Whether or not a particular visual characteristic is tracked depends on communication channel conditions (e.g. channel bandwidth) and\/or near-end and\/or far-end device resources\u2014for example, (e.g.) fewer characteristics (e.g. body parts) may be tracked for lower channel bandwidth and\/or smaller screen size of the far-end device and more characteristics (e.g. body parts) may be tracked for higher channel bandwidth and\/or larger screen size of the far-end device.",{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 1","FIG. 1"],"b":["100","102","104","108","110","112","114","102","100","104","110","114","106","100","102","102","108","112","106","100","106","104","110","106","104","102","102","104","104","104","106"],"i":["a ","b ","a","b","a","b"]},"The user device  executes an instance of a communication client, provided by a software provider associated with the communication system . The communication client is a software program executed on a local processor in the user device . The client performs the processing required at the user device  in order for the user device  to transmit and receive data over the communication system .","The user device  corresponds to the user device  and executes, on a local processor, a communication client which corresponds to the communication client executed at the user device . The client at the user device  performs the processing required to allow the user  to communicate over the network  in the same way that the client at the user device  performs the processing required to allow the users and to communicate over the network . The user device  corresponds to the user device  and executes, on a local processor, a communication client which corresponds to the communication client executed at the user device . The client at the user device  performs the processing required to allow the user  to communicate over the network  in the same way that the client at the user device  performs the processing required to allow the users , to communicate over the network . The user devices ,  and  are endpoints in the communication system .  shows only four users (, ,  and ) and three user devices ( and ) for clarity, but many more users and user devices may be included in the communication system , and may communicate over the communication system  using respective communication clients executed on the respective user devices.",{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 2","FIG. 2"],"b":["104","206","100","104","202","208","210","224","212","216","218","226","214","220","106","210","202","209","104","208","210","212","214","216","218","220","104","208","210","212","214","216","218","220","104","202","216","224","226","208","212","218","202","220","214","202","104","106","220","220","106","106"]},"The projector  and sensor  constitute a depth detector  for capturing non-visible radiation data in three dimensions (\u201c3D\u201d). In this embodiment, the projector  projects a radiation pattern, forward of the sensor , which is detectable by the sensor ; sensor data from the sensor  is used to build up the 3D image based on distortions in the detected radiation pattern (as explained in more detail below with reference to ). The depth detector  and camera  may be housed together in a single unit external to the user device , possibly having a power supply separate to the user device , connected to the processor  via a suitable interface (e.g. USB or USB-based). An example of such a unit is the Microsoft Kinect Sensor\u2122.",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 2","b":["204","202","204","206","100","204","104","106","220","206","204","206","102","102","206","102","100"]},"With reference to , there will now be described a method of transmitting video over a network.  is a functional diagram of part of the user device .","As shown in , the user device  comprises an encoder  (e.g. an H264 encoder). This may be a hardware encoder to which video data is supplied by the processor  for encoding prior to transmission over the network , a software encoder implemented by software executed on (e.g.) processor , or a combination of both. The user device further comprises controller  (implemented e.g. as part of the software of the client ), the camera , the depth detector  and a depth processor . The controller  comprises a resource manager  and a video signal processor . The camera  captures images of a video of users , (labelled \u201cnear-end video\u201d in ) in a visible spectrum (that is, visible to a human eye) and supplies those captured images to a first input of the video signal processor. The sensor  captures images in a non-visible spectrum (that is, not visible to a human eye) and supplies those captured images to an input of the depth processor . The depth processor  has an output coupled to a second input of the video signal processor. The projector  projects non-visible radiation forward of the sensor  towards users , which is detectable by the sensor . The controller , encoder  and sensor processor  constitute a video processing system .","The resource manager  has first, second and third inputs and an output. The first of resource manager  input is configured to receive information about one or more communication channels between the user device  and one or more other user devices (e.g. , ) of the network . The second input of the resource manager  is configured to receive information about resources of the user device  and\/or one or more other user devices (e.g. , ). The third input of the resource manager is coupled to an output the video signal processor .","The output of the resource manager  is coupled to a third input of the video signal processor . The video signal processor  has an output coupled to an input of the encoder . The encoder is configured to supply encoded video data which may be subject to further processing at the device  (e.g. packetization) before being supplied to the network interface  for transmission over the network  to at least one of the second user device  and the third user device .","The sensor processor  is operable to process sensor data captured by, and received from, the sensor  to detect a respective plurality of visual user characteristics for each user in a field of view of the sensor .","The video signal processor  is configured to select characteristics from a plurality of visual user characteristics based information output from the resource manager  and to control video supplied to the encoder  for encoding based on detection of the selected characteristics by the sensor processor , with the video being so controlled to track the selected characteristics i.e. such that the video tracks the selected characteristics over time, eventually making the detected characteristics visible in the video.","This will now be described with reference to .  show a user  (e.g. , ) in the vicinity of the user device  (which is a shown as a games console in this embodiment). The user device is coupled to the display , the camera , the projector  and the sensor  all of which are external to the user device  in this embodiment. In , video of a first visible region \u2014limited to include upper portions (e.g. head, shoulders etc.) of the user  but not lower portions (e.g. feet, legs etc.) of the user \u2014as captured by camera  is supplied to the encoder  for encoding and ultimately transmission to another user (e.g. , ) over the network . As shown in , a version of the video of region is transmitted to the other user is optionally displayed on display  as well, overlaid on video received from that other user. In , video of a second visible region \u2014which includes both the upper portions of the user  and the lower portions of the user \u2014as captured by camera  is supplied to the encoder  for encoding and ultimately transmission the other user over the network . As shown in , a version of the video of the second region is transmitted to the other user is optionally displayed on display  as well, overlaid on video received from that other user.","The first and second visible region , have a respective size and location determined by the video control module based on data received from resource manager  and data received from sensor processor  as explained below.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 5","FIG. 5","FIG. 5","FIG. 5"],"b":["224","226"]},"This radiation pattern  is projected forward of the sensor  by the projector . The sensor  captures images of the non-visible radiation pattern as projected in its field of view. These images are processed by the sensor processor  in order to calculate depths of users in the field of view of the sensor  (effectively building a three-dimensional representation of the user) thereby allowing the recognition of different users and different respective body parts thereof.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 6B","FIG. 6A","FIG. 6C"],"b":["102","104","216","226","222","102","222","500","224"]},"As illustrated in , the user  thus has a form which acts to distort the projected radiation pattern as detected by sensor  with parts of the radiation pattern projected onto parts of the user further away from the projector being effectively stretched (i.e. in this case, such that dots of the radiation pattern are more separated) relative to parts of the radiation projected onto parts of the user closer to the projector (i.e. in this case, such that dots of the radiation pattern are less separated), with the amount of stretch scaling with separation from the projector, and with parts of the radiation projected onto objects significantly backward of the user being effectively invisible to the sensor . Because the radiation pattern  is systematically inhomogeneous, the distortions thereof by the user's form can be used to discern that form (e.g. to identify the user's head, left hand, right arm, torso etc.) by the sensor processor  processing images of the distorted radiation pattern as captured by sensor . For instance, separation of an area of the user from the sensor could be determined by measuring a separation of the dots of the detected radiation pattern within that area of the user.","Whilst in  the radiation pattern is shown as visible to a human eye, this is purely to aid in understanding and the radiation pattern as projected onto the user  will not be visible to a human eye in reality.","As will now be described with reference to , the images (sensor data) of the non-visible radiation pattern  captured by the sensor  are processed by the sensor processor . This processing comprises skeletal detection processing. Skeletal detection processing is known in the art and is currently implemented, for instance, in the Microsoft Xbox 360\u2122 (sometimes used in conjunction with the Microsoft Kinect Sensor\u2122) the results of which are made available by way of an Application Programming Interface (API) for use by software developers.","The sensor detector  receives sensor data from sensor  and processes it to determine a number of users (e.g. , ) in the field of view of the sensor  and to identify a respective plurality of skeletal points for each user using skeletal detection techniques which are known in the art. Each skeletal point represents an approximate location of the corresponding human joint in the video.","Specifically, in this embodiment, sensor detector  detects twenty respective skeletal points for each user in the field of view of sensor . Each skeletal point corresponds to one of twenty recognized human joints, with each varying in space and time as a user (or users) moves within the sensor 's field of view. The location of these joints at any moment in time is calculated based on the user's (or users' respective) three dimensional form as detected by sensor . A skeletal point also has a tracking state: it can be \u201ctracked\u201d for a clearly visible joint, \u201cinferred\u201d when a joint is not clearly visible but sensor processor  is inferring its location, or \u201cnon-tracked\u201d, for example, for a lower joint in seated-mode tracking (i.e. when it is detected that a user is seated and lower joints are not tracked by the sensor processor ).","Each skeletal points may be provided with a respective confidence value indicate a likelihood of the corresponding joint having been correctly detects. Points with confidence values below a certain threshold may be excluded from processing by the video module .","These twenty skeletal points are illustrated in  with the corresponding human joint shown in table 1.",{"@attributes":{"id":"p-0044","num":"0043"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Skeletal Points"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Name of Skeletal Point:","Corresponding human joint:","Labelled as:"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["AnkleLeft","Left ankle","722b"]},{"entry":["AnkleRight","Right ankle","722a"]},{"entry":["ElbowLeft","Left elbow","706b"]},{"entry":["ElbowRight","Right elbow","706a"]},{"entry":["FootLeft","Left foot","724b"]},{"entry":["FootRight","Right foot","724a"]},{"entry":["HandLeft","Left hand","702b"]},{"entry":["HandRight","Right hand","702a"]},{"entry":["Head","Head","710"]},{"entry":["HipCenter","Centre, between hips","716"]},{"entry":["HipLeft","Left hip","718b"]},{"entry":["HipRight","Right hip","718a"]},{"entry":["KneeLeft","Left knee","720b"]},{"entry":["KneeRight","Right knee.","720a"]},{"entry":["ShoulderCenter","Centre, between shoulders","712"]},{"entry":["ShoulderLeft","Left shoulder","708b"]},{"entry":["ShoulderRight","Right shoulder","708a"]},{"entry":["Spine","Spine","714"]},{"entry":["WristLeft","Left wrist","704b"]},{"entry":["WristRight","Right wrist","704a"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"The skeletal points and the video from camera  are correlated such that the location of a skeletal point as reported by the sensor processor at a particular time corresponds to the location of the corresponding human joint within a frame (image) of the video at that time. The sensor detector  supplies these detected skeletal points as skeletal point data to the video controller  for use thereby. For each frame of video data, the skeletal point data supplied by the sensor processor  comprises locations of skeletal points within that frame e.g. expressed as Cartesian coordinates (x,y) of a coordinate system bounded with respect to a video frame size.","The video controller  receives the detected skeletal points for one or more users (, ) and is configured to determine therefrom a plurality of visual characteristics of that user (or a respective plurality of visual characteristic of those users). In this embodiment, visual user characteristics take the form of human body parts. Body parts are detected by the video controller, each being detected by way of extrapolation from one or more skeletal points provided by the video processor  and corresponding to a region within the corresponding video frame of video from camera  (that is, defined as a region within the afore-mentioned coordinate system).  illustrates detected body parts which have been detected based on the skeletal points of .",{"@attributes":{"id":"p-0047","num":"0046"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Body Parts (Visual Characteristics)"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"140pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"77pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"That characteristic as"]},{"entry":["Name of Body part (visual characteristic)","detected in FIG. 7B:"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["Head","750"]},{"entry":["Shoulders","752"]},{"entry":["Mid-spine (upper portion of a spine)","756"]},{"entry":["Low-pine (lower portion of the spine)","758"]},{"entry":["Whole-spine (the entirety of the spine)","760"]},{"entry":["Hips","762"]},{"entry":["Elbows","754a, 754b"]},{"entry":["Legs","764a, 764b"]},{"entry":["Feet","766a, 766b"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"It should be noted that these visual characteristic are visual in the sense that they represent features of a user's body which can in reality be seen and discerned; however, in this embodiment, they are not \u201cseen\u201d by the video controller (e.g. they are not detected in video data captured by camera ); rather the video controller extrapolates an (approximate) relative location, shape and size of these features within a frame of the video from camera  from the arrangement of the twenty skeletal points as provided by sensor processor  (and not based on e.g. processing of that frame)\u2014for example, by approximating each body part as a rectangle (or similar) having a location and size (and optionally orientation) calculated from detected arrangements of skeletal points germane to that body part.","A method  of controlling video to be transmitted over a network based on detected user characteristics (body parts in this embodiment) will now be described with reference to .  shows a flowchart of the method  on the left and a corresponding visual representation of each method step on the right.","In this embodiment, the method is implemented algorithmically as part of the software of client \u2014specifically by controller .","The method  will be described in the context of a real-time video call conducted using the first user (near-end) device  and the second user (far-end) device  between users thereof.","The method considers the following body regions:\n\n","At the start of the video call the resource manager  of client  of the near-end device  determines a video resolution (to be used for video transmitted to the far-end device ) based the information received thereby. For instance, this information may include information about one or more of:\n\n","The video resolution may be determined, at least in part, by way of negotiation with the far-end device  (e.g. whereby the near-end resource manager  request information about resources of the far-end device therefrom).","During the call, the resource manager  of the near-end device  monitors available resources (e.g. the available bandwidth), takes decisions to increase or decrease the resolution for the video being sent to the far-end device , and communicated those decisions to the video controller . The video resolution may thus vary dynamically thought the call e.g. due to fluctuation channel bandwidth arising, say, from one or both of the neared and far-end devices being connected to the network  via an unreliable wireless e.g. WiFi connection).","Selection of characterises comprises selecting more characteristics if the received information indicates better channel quality and\/or more device resources and selecting fewer characteristics if the received information indicates worse channel quality and\/or more fewer device resources. For instance, for lower bandwidth and\/or smaller screen size a lower resolution is determined by the resource manager  (causing selection a region with fewer body parts); for higher bandwidth and\/or larger screen size, a higher resolution is determined by the resource manager  (causing selection a region with more body parts) than is determined for said lower bandwidth and\/or said smaller screen size.","In this embodiment, video is captured from camera  at a fixed resolution of 1920\u00d71080 pixels. However, as described below, the captured video may be cropped (cropping being to the removal of the outer parts of images of the video) prior to transmission. That is, only selective parts of the captured videos data\u2014as captured from a selected visible region\u2014are supplied to encoder  for encoding and subsequent transmission to the far-end device. Supplying cropped video data thus means supplying less video data to the encoder  as video data outside of the defined region is not supplied to the encoder.","The skeletal point data is received by the video controller from the sensor processor  at step S. In the exemplary illustration on the right-hand side of , skeletal data for collocated users and (both in the field of vision of sensor ) is shown. However, the method  can be applied to received skeletal data for any number of users (one or more) as will be apparent.","At step S, the video controller  selects respective body regions for each user in the field of view of sensor  based on the determined resolution received from the resource manager . Depending on this video resolution, the video controller selects body regions out of the possible plurality of body regions of table 2 as follows:",{"@attributes":{"id":"p-0060","num":"0066"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"154pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Resolution 1920 \u00d7 1080:","Region 4"]},{"entry":[{},"Resolution equal or greater than 1280 \u00d7 1024:","Region 3"]},{"entry":[{},"Resolution equal or greater than 640 \u00d7 480:","Region 2"]},{"entry":[{},"Other resolutions:","Region 1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"Or alternatively as follows:",{"@attributes":{"id":"p-0062","num":"0068"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"154pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Resolution width 1920:","Region 4"]},{"entry":[{},"Resolution width equal or greater than 1280:","Region 3"]},{"entry":[{},"Resolution width equal or greater than 640:","Region 2"]},{"entry":[{},"Other resolutions:","Region 1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"The selection of characterises comprises selecting more characteristics for higher resolutions (better channel conditions and\/or greater device resources) and selecting fewer characteristics for lower resolutions (worse channel conditions and\/or lesser device resources).","For each user , in the field of vision of sensor , the selected region is detected (that is, respective body parts of the selected region are detected) based on the received skeletal data. The video is then cropped (as described below), before being encoded and transmitted, based on the detected body parts (e.g. for region , the video is cropped based on detection of user 's head, user 's head, user 's shoulders, and user 's shoulders but not on any other body parts of those users as only the head and shoulders are included in region ). Thus, not all skeletal points necessarily contribute to the cropping (as some of these may serve only to define body parts that are not part of the selected region). That is, the control of the video may be based on a selection of skeletal points of the plurality of detected skeletal points but not others of the plurality of detected skeletal points.","For instance, in the exemplary depiction on the right hand side of , region  (head, shoulders, mid-spine, elbows) has been selected, and video control is thus based on detected region 2 characteristic for both user (in ) and for user (in ).","As part of the video control (which comprises selectively cropping the video in accordance with the determined resolution in this embodiment), at step S the video controller generates boundary data based detection of the body parts of the selected region. The boundary data defines a rectangular boundary  (cropping rectangle). The cropping rectangle is formed from the union of the respective detected regions for all tracked users.","At step S, video controller  modifies the generated boundary data based on a predetermined aspect ratio (e.g. an aspect ratio of a display of the far-end device , such as 4:3, 16:9 etc.), whereby the cropping rectangle is adjusted to the predetermined aspect ratio. The adjusted cropping rectangle (bounding rectangle) is shown as  on the right hand side of  (and has an aspect ratio of 16:9 in this embodiment).","At step S, the video is cropped based on the adjusted rectangle as explained in more detail below. The cropped video is then scaled to an output resolution (e.g. matching that of the screen of the fared device to which it is being transmitted), before being encoded by encoder .","The method  is performed for each frame of the video data. Because the boundary rectangles track users characteristics\u2014which may move as a user moves about, or separate as e.g. two users move apart\u2014between frames, cropping rectangles move around.","However, rather than simply cropping each frame of video data to the adjusted rectangle determined for that frame (i.e. by supplying only video data from the portion of that frame defined by the adjusted rectangle), transition data is generated based on respective adjusted rectangles calculated for one or more earlier frames and on the adjusted rectangle calculated for a current frame. The transition data is generated based on an elastic spring model.","In embodiments, the elastic spring model may be defined as follows:",{"@attributes":{"id":"p-0072","num":"0078"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"m","mo":"*","mfrac":{"mrow":[{"msup":{"mo":"\u2146","mn":"2"},"mo":"\u2062","mi":"x"},{"mo":"\u2146","msup":{"mi":"t","mn":"2"}}]}},{"mrow":[{"mrow":{"mo":"-","mi":"k"},"mo":"*","mi":"x"},{"mi":"D","mo":"*","mfrac":{"mrow":[{"mo":"\u2146","mi":"x"},{"mo":"\u2146","mi":"t"}]}}],"mo":"-"}],"mo":"="}}},"br":{}},"The cropping rectangles move around according to the elastic spring model, which smoothes transitions between frames and prevents jittery video. It also increases efficiency of encoding for the following reason. Because the elastic model effectively \u2018dampens\u2019 movement of cropping rectangles, it reduces differences between adjacent frames which, as will be apparent, results in more efficient differential encoding.","This will now be described with reference to .  shows a flowchart of a method  of controlling video to be transmitted to the far-end user as time progresses and an exemplary pectoral representation of the method  on the right hand side.","At step S, for a current video frame  in the sequence of video frames of the captured video, the video controller generate a first set and one or more second sets of boundary data (e.g. by calculating bounding rectangles having predetermined aspect ratios as described above) based on detection of the selected visual user characteristics (body parts in this embodiment) at a first time and one or more second times respectively, the first time being a time of the current frame  and the second time(s) being time(s) of previous frames. The boundary data for earlier frames may be generated before the boundary data for later frames (e.g. on a frame-by-frame basis). Therefore S may take place over a period spanning several video frames.","Two exemplary bounding rectangles ,  at a respective first time t1 and second time t2 are shown on the right hand side of . The rectangle for t2 is shown larger than the rectangle for t1 which may be e.g. due to two users (, ) being tracked and those users moving further apart, or due to a change in available resources (e.g. increase in channel bandwidth) causing the resource manager  to increase the determined aspect ratio, resulting in a region with more body parts being selected (e.g. a switch from region  to region ) and the bounding rectangle being automatically adjusted by the video controller  accordingly.","At step S, the video module  generates transition data based on the first and second sets of boundary data using the dynamic model described above. The generated transition data effectively defines a transitional bounding rectangle  (shown in ) representing a transition to the bounding rectangle of the current frame  from bounding rectangles of previous frames.","For example, in accordance with the above elastic spring model, bounding rectangles may be parameterized by one or more points at different locations (one form of boundary data). A point may have a second position (\u201cdesiredPosition\u201d) at time t2 being a parameter of rectangle  and a first position \u201ccurrentPosition\u201d at time t1 being a parameter of rectangle . In this case, the transition data may be generated by updating \u201ccurrentPosition\u201d as follows, with the updated \u201ccurrentPosition\u201d being a parameter of the transitional bounding rectangle :",{"@attributes":{"id":"p-0079","num":"0085"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"velocity = 0"]},{"entry":[{},"previousTime = 0"]},{"entry":[{},"currentPosition = <some_constant_initial_value>"]},{"entry":[{},"UpdatePosition (desiredPosition, time)"]},{"entry":[{},"{"]},{"entry":[{},"x = currentPosition \u2212 desiredPosition;"]},{"entry":[{},"force = \u2212 stiffness * x \u2212 damping * m_velocity;"]},{"entry":[{},"acceleration = force \/ mass;"]},{"entry":[{},"dt = time \u2212 previousTime;"]},{"entry":[{},"velocity += acceleration * dt;"]},{"entry":[{},"currentPosition += velocity * dt;"]},{"entry":[{},"previousTime = time;"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"At step S, the video controller control the video based on the transition data to track the selected characteristics. Here, this involves cropping the video such that only image data of frame  that is within bounding rectangle  is supplied to the encoder  for encoding and transmission to the far-end user  (and not image data of frame  that is outside of bounding rectangle ).","The detector  and sensor processor  are configured such that users separated by more than approximately 1 meter from the detector  are not tracked (that is are outside of the field of vision of the sensor), so they don't affect the cropping rectangle. Thus, for instance, if two users are initially being tracked and one moves out of the field of vision, in accordance with the methods  and , cropping rectangles calculated thereafter will be based only on the remaining user's characteristics and will thus automatically \u2018zoom in\u2019 on the remaining user (with the elastic model ensuring a smooth transition for this zoom).","In embodiments, the video controller  may feed back into the resource manager , such that if the resource manager considers there is enough bandwidth for 1920\u00d71080 video, but there is at least one body to be tracked the video send resolution is switched to 1280\u00d7720 to enable dynamic tracking (as, for video transmitted resolution equal to that at which it is captured, there is no \u2018room for manoeuvre\u2019 as, for each frame, the cropping rectangle effectively encompasses the entirety of that frame\u2014this is true e.g. where up-scaling is not employed).","As indicated, the sensor processor  supplies not only information identifying each detected skeletal point but also identifying which of one or more users in the sensor 's field of vision those skeletal points corresponds to. Thus, the described method  can be implemented with any number of users (as long as the sensor detector  remains capable of distinguishing there between) and, for each characteristic in the selected region, will track those characterises for each region. Thus, the method  adapts automatically as multiple users walk in and out of frame, with the bounding rectangles automatically transitioning in a smooth manner (due to the elastic model) to accommodate new users as they walk into frame (by effectively zooming out to include any body parts of that user for the selected region in the video) and to adjust as users walk out of frame (by effectively zooming in to exclude any regions previously occupied by body parts of that user for the selected region such that only body parts for the selected region of the remaining users are retained in the video).","The methods ,  implemented by the controller  can be implemented during a multiparty call conducted over the network  using e.g. user devices ,  and  (between users , ,  and ), with individual bounding rectangles being determined by the near-end user  (first device) for each far-end device  (second device) and  (third device).","That is, in addition to the above, the resource manager may receive further information about at least one of: a communication channel between the user device  and the third user device , and resources of the further user device (in addition to receiving the information described above such as similar information for the second device ). The resource manager then selects further characteristics selected from the plurality of visual user characteristics (e.g. body parts) based on the received further information for controlling video to be transmitted to the third user device  (in addition to selecting the aforementioned characteristics for controlling video to be transmitted to the second user device ).","The video controller  then controls the video to be transmitted to the third user  device based on detection of the selected further characteristics selected for the third device  (in order to track the selected further characteristics in the third device video), whilst controlling the video to be transmitted to the second user device  based on detection of the characteristics selected for the second device .","The further selection of characteristics for the third user device may be independent and different from the selection of characterises for the second user device. Thus the further video transmitted to the second user device may be different form the video transmitted to the second user device, with the further video transmitted to the third user device tracking more or fewer user characteristics than the video transmitted to the second user device.","The selection of the characteristics for the second user device video is independent from the selection of characteristic for the third user video. Whilst a condition (such as the first user device  being connected to the network  via a slow connection) may cause a similar cropping for both, other conditions (such as one of the second and third devices being connected to the network  via a slow connection, or one of those devices having limited resources) may cause different cropping.","For example, the third user device  may have a small screen (e.g. smartphone screen) and\/or be connected to the network  via a slow connection; in contrast, the second user device  may have a large screen (e.g. be connected to a TV screen) and\/or be connected to the network  via a fast connection. In this case, video transmitted to the second user device may be subject to \u201cregion 4\u201d cropping (see table 2) such that user  receives video showing users and top-to-toe. In contrast, video transmitted to user device  may be subject to \u201cregion \u201d cropping (see table 2) such that user  receives video showing only the respective heads and shoulders of users , ","Whilst in the above, the video to be transmitted over the network is controlled by way of video signal processing, alternatively or additionally the video may be controlled by the video controller  (of controller ) by manipulating the camera itself based detection of the selected features e.g. the manipulation comprising manipulating mechanics of the camera to perform at least one of: a pan operation, a zoom operation and a tilt operation. For e.g. a multi-party call, optical zoom and digital zoom (cropping) may be used in conjunction e.g. with mechanical zoom being used to capture video showing the highest selected number of user characteristics (e.g. mechanical zoom could be used to select a region to be shown to the second user device  in the above example) with cropping of that video being used to control video for users who are to be sent video with fewer user characteristics (e.g. the third device  in the above example).","Further, whilst in the above an elastic model is employed, as an alternative any dynamic model (e.g. based on one or more differential equations in time) could be used to generate the transition data.","Further, whilst in the above selected visual characteristic (legs, arms etc.) are detected based on sensor data supplied by a depth detector which projects a nonvisible radiation pattern forward of a sensor configured to detect that pattern, alternative detections are envisages. For instance, a depth detection could be a time-of-flight based detection in which radiation propagation times are used to measure depth. Alternatively, an array of cameras having different inclinations (such as a plenoptic camera used in conjunction with a 3D image recognition algorithm) or similar) could be used to build up a 3D image (from multiple two-dimensional images), with the visual characteristic being detected form the 3D image.","Generally, any of the functions described herein (e.g. the functional modules shown in  and the functional steps shown in ) can be implemented using software, firmware, hardware (e.g., fixed logic circuitry), or a combination of these implementations. The modules (video processing system , controller , video signal processor , resource manager , encoder  etc.) shown separately in  and the steps shown separately in  may or may not be implemented as separate modules or steps. The terms \u201cmodule,\u201d \u201cfunctionality,\u201d \u201ccomponent\u201d and \u201clogic\u201d as used herein generally represent software, firmware, hardware, or a combination thereof. In the case of a software implementation, the module, functionality, or logic represents program code that performs specified tasks when executed on a processor (e.g. CPU or CPUs). The program code can be stored in one or more computer readable memory devices. The features of the techniques described herein are platform-independent, meaning that the techniques may be implemented on a variety of commercial computing platforms having a variety of processors. For example, the user devices may also include an entity (e.g. software) that causes hardware of the user devices to perform operations, e.g., processors functional blocks, and so on. For example, the user devices may include a computer-readable medium that may be configured to maintain instructions that cause the user devices, and more particularly the operating system and associated hardware of the user devices to perform operations. For example, some or all of the modules of  may be implemented by software of a client application executed on one or more processors. Thus, the instructions function to configure the operating system and associated hardware to perform the operations and in this way result in transformation of the operating system and associated hardware to perform functions. The instructions may be provided by the computer-readable medium to the user devices through a variety of different configurations.","One such configuration of a computer-readable medium is signal bearing medium and thus is configured to transmit the instructions (e.g. as a carrier wave) to the computing device, such as via a network. The computer-readable medium may also be configured as a computer-readable storage medium and thus is not a signal bearing medium. Examples of a computer-readable storage medium include a random-access memory (RAM), read-only memory (ROM), an optical disc, flash memory, hard disk memory, and other memory devices that may us magnetic, optical, and other techniques to store instructions and other data.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF FIGURES","p":["For an understanding of the present subject matter and to show how the same may be carried into effect, reference will be made by way of example to the following drawings in which:",{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIGS. 4A and 4B"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIGS. 6A, 6B and 6C"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 7A"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7B"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
