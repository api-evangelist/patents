---
title: Method for transforming a multithreaded program for general execution
abstract: A technique is disclosed for executing a program designed for multi-threaded operation on a general purpose processor. Original source code for the program is transformed from a multi-threaded structure into a computationally equivalent single-threaded structure. A transform operation modifies the original source code to insert code constructs for serial thread execution. The transform operation also replaces synchronization barrier constructs in the original source code with synchronization barrier code that is configured to facilitate serialization. The transformed source code may then be conventionally compiled and advantageously executed on the general purpose processor.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09367306&OS=09367306&RS=09367306
owner: NVIDIA CORPORATION
number: 09367306
owner_city: Santa Clara
owner_country: US
publication_date: 20110330
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION","System Overview"],"p":["1. Field of the Invention","The invention relates generally to compiler systems and, more specifically, to a method for transforming a multithreaded program for general execution.","2. Description of the Related Art","Certain computer systems include a parallel processing subsystem that may be configured to concurrently execute plural program threads that are instantiated from a common program. Such systems are referred to in the art as having single program multi-data (SPMD) parallelism. CUDA is a programming model known in the art that implements SPMD execution on parallel processing subsystems. An application program written for CUDA may include sequential C language programming statements, and calls to a specialized application programming interface (API) used for configuring and managing parallel execution of program threads. A function within a CUDA application that is destined for concurrent execution on a parallel processing subsystem is referred to as a \u201ckernel\u201d function. An instance of the kernel is referred to as a thread, and a set of concurrently executing threads are organized as a thread block. A set of thread blocks may further be organized into a grid. Each thread is identified by an implicitly defined set of index variables. Each thread may access their instance of the index variables and act independently with respect to other threads based on the index variables. For example, CUDA defines a 3-tuple of index variables for thread position within a block, and a 2-tuple of index variables for thread position within a grid.","Based on a specific set of index variables, a given thread may independently access memory or other system resources with variable latency, leading to certain threads advancing further in execution than other threads. However, certain algorithms require coherent state among different threads at certain synchronization points before processing may advance. To enable proper synchronization among threads, CUDA provides synchronization barriers, whereby if any thread calls a certain synchronization primitive, all threads within a related group of concurrent threads must call the same synchronization primitive before any thread may advance past the synchronization primitive. In this way, related threads at different stages of execution may synchronize their execution stage before advancing.","In certain scenarios a user may wish to execute an existing SPMD application, such as a CUDA application, on a general purpose central processing unit (CPU) rather than on a parallel processing subsystem. Unfortunately, conventional CPUs are typically configured to execute only a limited number of independent concurrent threads, and conventional operating systems that support execution of a larger number of threads typically map each thread to an independent process, requiring burdensome context switches to perform thread synchronization at synchronization barriers. Therefore, directly mapping threads for a CUDA program to a set of equivalent threads in a general purpose processing environment represents an unacceptably inefficient approach to executing a CUDA program on a general purpose CPU.","As the foregoing illustrates, what is needed in the art is a technique for efficiently executing an SPMD application on a general purpose CPU.","One embodiment of the present invention sets forth a method for executing a multi-threaded program on a single-threaded processor core, comprising identifying a kernel function included within the multi-threaded program, enumerating a plurality of barrier synchronization calls within the kernel function, modifying the kernel function by replacing each enumerated barrier synchronization call within the kernel function with a plurality of barrier commands and inserting a plurality of execution control commands into the kernel function, and transferring the modified kernel function to a transformed source file.","Other embodiments of the present invention include, without limitation, a computer-readable storage medium including instructions that, when executed by a processing unit, cause the processing unit to perform the techniques described herein as well as a computing device that includes a processing unit configured to perform the techniques described herein.","One advantage of the disclosed technique is that programs structured for multi-threaded execution on a parallel processing subsystem may be efficiently and advantageously executed instead on a general purpose processor.","In the following description, numerous specific details are set forth to provide a more thorough understanding of the invention. However, it will be apparent to one of skill in the art that the invention may be practiced without one or more of these specific details. In other instances, well-known features have not been described in order to avoid obscuring the invention.",{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 1","FIG. 1"],"b":["100","100","102","104","105","105","106","107","107","108","102","106","105","112","105","113","112","110","103","113","112","110","114","107","116","107","118","120","121","107"]},"In one embodiment, the parallel processing subsystem  incorporates circuitry optimized for graphics and video processing, including, for example, video output circuitry, and constitutes a graphics processing unit (GPU). In another embodiment, the parallel processing subsystem  may be integrated with one or more other system elements, such as the memory bridge , CPU , and I\/O bridge  to form a system on chip (SoC).","It will be appreciated that the system shown herein is illustrative and that variations and modifications are possible. The connection topology, including the number and arrangement of bridges, the number of CPUs , and the number of parallel processing subsystems , may be modified as desired. For instance, in some embodiments, system memory  is connected to CPU  directly rather than through a bridge, and other devices communicate with system memory  via memory bridge  and CPU . In other alternative topologies, parallel processing subsystem  is connected to I\/O bridge  or directly to CPU , rather than to memory bridge . In still other embodiments, I\/O bridge  and memory bridge  might be integrated into a single chip. Certain embodiments may include two or more CPUs  and two or more parallel processing systems . The particular components shown herein are optional; for instance, any number of add-in cards or peripheral devices might be supported. In some embodiments, switch  is eliminated, and network adapter  and add-in cards ,  connect directly to I\/O bridge .",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 2","b":["200","222","242","210","210"]},"The parallel processor compiler  is configured to conventionally compile the parallel application source code  into parallel executable code . The parallel executable code  may be loaded into the parallel processing subsystem  of  for execution. Any synchronization barriers within the parallel executable code  are implemented using native constructs available within the parallel processing subsystem . The compilation flow from parallel application source code  through parallel execution of the at least one kernel function on the parallel processing subsystem  represents a conventional approach to executing a parallel application defined by the parallel application source code .","Embodiments of the present invention define a compilation flow and code transformation techniques that enable the parallel application source code  to efficiently execute as a single thread on the general purpose CPU  rather than plural threads on the parallel processing subsystem . A source code transform module , described in greater detail below, transforms the parallel application source code  to generate transformed source code . Parallelism inherent within the parallel application source code  is serialized for execution on general purpose CPU . Furthermore, synchronization barriers within the parallel application source code  are transformed for serial execution. Serialization transforms applied to the parallel application source code  produces a transformed source code  that is generic, single threaded, and suitable for conventional compilation and execution. The CPU compiler  conventionally compiles the transformed source code  to generate serialized executable code , which is suitable for serial execution on the general purpose CPU .",{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 3A","b":["302","302","302"]},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 3B","b":["302","302","112","102"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 3C","b":["304","1","3","310","312"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 3D","b":["304","304","306","306","310","312","324","320","322","1","2","3","1","3","1","3"]},"The transform partitions execution of kern( ) along boundaries defined by synchronization barriers , , and serializes execution of kern by thread number, given by variable tid. The while-loop  controls which section of program code - is currently executed by for-loop , which controls which thread index variable (or variables) is presented to the program code program code - . The for-loop  is structured as a scalar value (tid) that sequences through each concurrent thread. However, multi-dimensional indices may also be used as a thread identifier in a conventional kernel function. Any technically feasible technique may be used to map the scalar value of tid to a multi-dimensional set of index variables. In one embodiment, tid may be used as an index into a set of arrays, with each array comprising a list of corresponding index variable values. For example, CUDA typically identifies a thread according to a 3-tuple block identifier, and a 2-tuple grid identifier. The value of tid may be used as an index into an array structure that returns an appropriate 3-tuple, and an array structure that returns an appropriate 2-tuple for use within a kernel function.","The transform includes, without limitation, inserting execution sequence variables \u201ccurrent_restart\u201d and \u201cnext_restart,\u201d while-loop , an assignment to current_restart from next_restart, for-loop , switch statement , and synchronization barrier code ,  in place of each synchronization barrier , .","The synchronization barrier code ,  comprises an assignment statement, a barrier goto statement, and a barrier label statement. The assignment statement assigns a value for next_restart that corresponds to a specific case number within the switch statement . The case number is associated with a goto command within the switch statement  that targets the barrier label statement. The barrier label statement marks a location in the transformed source code  where execution should commence after each thread has executed and reached the same barrier code. The barrier goto statement is located before the barrier label statement. The barrier goto statement serves to advance one iteration of the for-loop , which is analogous to transitioning execution to a subsequent thread.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 3E","FIG. 3D","FIG. 1","FIG. 3D"],"b":["306","320","324","320","350","112","102","306","1","2","3","1","350","310","324","1","350","2","350","312","3","350","310","304","1","2","306","330","332"]},{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 4","FIGS. 1-2"],"b":"400"},"The method begins in step , where a transform module, such source code transform module  of , opens a code stream for original source code, such as parallel application source code . In one embodiment, the transform module comprises a software application executing on computer system . The original source code includes a kernel function designed for multi-threaded execution. The original code stream may be opened using any technically feasible technique and from any technically feasible storage medium. For example, the original code stream may be opened from a data file stored within system disk . In step , the transform module opens a transformed code stream, such as transformed source code . In one embodiment, the transformed code stream comprises a file within system disk .","In step , the transform module identifies a kernel function within the original code stream. The kernel function, such as kernel function , is designed for concurrent execution on a parallel processing subsystem. In step , the transform module enumerates all calls to barrier synchronization constructs within the kernel function.","In step , the transform module copies a generic structure of the kernel function to the transformed code stream. The generic structure includes programming code not subject to modification by the transform module. In step , the transform module generates a barrier control loop within the transformed code stream. The barrier control loop comprises the while-loop  of , and related statements. In step , the transform module generates a serialization control loop within the transformed code stream. The serialization control loop comprises the for-loop , the switch statement , and related statements. In step , for each enumerated barrier statement, the transform module generates and configures an instance of synchronization barrier code and writes the code to the transformed code stream. In step , the transform module closes the transformed code stream and closes the original code stream. The method terminates in step .","In sum, a technique for transforming a multi-threaded program into a single-threaded program for execution on a general purpose processor is disclosed. Original source code for the multi-threaded program is transformed to generate transformed source code, which may be compiled for execution on the general purpose processor. The transform modifies the original source code to serialize execution over a specified number of thread instances. The original source code is also partitioned along synchronization barriers, and each call to a synchronization barrier construct is replaced with synchronization barrier code that facilitates serialized execution. The transformed source code includes an outer loop structure that controls execution points related to the synchronization barrier code, and an inner loop that sequentially executes a portion of code for each thread instance. The portion of code is disposed between synchronization barriers. Efficient execution is achieved because overhead related to serialization is limited to basic flow control constructs that guide serial execution.","One advantage of the disclosed technique is that programs structured for multi-threaded execution on a parallel processing subsystem may be efficiently and advantageously executed instead on a general purpose processor.","While the foregoing is directed to embodiments of the invention, other and further embodiments of the invention may be devised without departing from the basic scope thereof. For example, aspects of the present invention may be implemented in hardware or software or in a combination of hardware and software. One embodiment of the invention may be implemented as a program product for use with a computer system. The program(s) of the program product define functions of the embodiments (including the methods described herein) and can be contained on a variety of computer-readable storage media. Illustrative computer-readable storage media include, but are not limited to: (i) non-writable storage media (e.g., read-only memory devices within a computer such as CD-ROM disks readable by a CD-ROM drive, flash memory, ROM chips or any type of solid-state non-volatile semiconductor memory) on which information is permanently stored; and (ii) writable storage media (e.g., floppy disks within a diskette drive or hard-disk drive or any type of solid-state random-access semiconductor memory) on which alterable information is stored. Such computer-readable storage media, when carrying computer-readable instructions that direct the functions of the present invention, are embodiments of the invention.","In view of the foregoing, the scope of the invention is determined by the claims that follow."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["So that the manner in which the above recited features of the invention can be understood in detail, a more particular description of the invention, briefly summarized above, may be had by reference to embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments of this invention and are therefore not to be considered limiting of its scope, for the invention may admit to other equally effective embodiments.",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 3C"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 3D"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 3E"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 4"}]},"DETDESC":[{},{}]}
