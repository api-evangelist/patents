---
title: Fast combinatorial algorithm for the solution of linearly constrained least squares problems
abstract: A fast combinatorial algorithm can significantly reduce the computational burden when solving general equality and inequality constrained least squares problems with large numbers of observation vectors. The combinatorial algorithm provides a mathematically rigorous solution and operates at great speed by reorganizing the calculations to take advantage of the combinatorial nature of the problems to be solved. The combinatorial algorithm exploits the structure that exists in large-scale problems in order to minimize the number of arithmetic operations required to obtain a solution.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07451173&OS=07451173&RS=07451173
owner: Sandia Corporation
number: 07451173
owner_city: Albuquerque
owner_country: US
publication_date: 20040909
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","STATEMENT OF GOVERNMENT INTEREST","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION","General Linearly Constrained Least Squares Problem","Solving the Unconstrained Least Squares Problem for a Single RHS","Equality Constraints by Weighting","Algorithm for Column Partitioning","Variable Equality Constraints by Direct Elimination","Evolving Constraints Active Set Algorithms for Linear Inequality Constraints","Non-negativity Constrained Least Squares: Problem NNLS","Fast Combinatorial NNLS Algorithm","Example of the Use of Active\/Passive Set Algorithms for the Solution of the NNLS Problem","Performance Comparison of NNLS Algorithms","Combined Equality and Inequality Constraints","Comparison with Prior Optimization Methods and Further Improvements","CONCLUSION"],"p":["This application claims the benefit of U.S. Provisional Application No. 60\/501,879, filed Sep. 9, 2003, which is incorporated herein by reference.","This invention was made with Government support under contract no. DE-AC04-94AL85000 awarded by the U.S. Department of Energy to Sandia Corporation. The Government has certain rights in the invention.","The present invention relates to methods to solve least squares problems and, in particular, to a fast combinatorial algorithm for the solution of linearly constrained least squares problems.","Algorithms for multivariate image analysis and other large-scale applications of multivariate curve resolution (MCR) typically employ iterative, linearly constrained alternating least squares (ALS) algorithms in their solution. The efficiency with which the original problem can be solved, therefore, rests heavily on the underlying constrained least squares solver. Efficiency of solution is particularly important with large-scale MCR applications. In this context, the term \u201clarge-scale\u201d can be used to denote problems in which the number of observation vectors is much greater than the number of variables to be estimated from each observation. A typical spectral imaging application, for instance, might require that a complete spectrum be measured at each pixel in a spatial array of a large number (e.g., of order 10) of total pixels in order to estimate the concentrations of a modest number (e.g., of order 10) of chemical components at each pixel. A least squares problem having multiple observation vectors is denoted herein as a multiple right hand side (RHS) problem. See, e.g., P. Kotula and M. Keenan, \u201cAutomated analysis of SEM X-ray spectral images: A powerful new microanalysis tool,\u201d 9, 1 (2003); U.S. Pat. No. 6,584,413 to Keenan and Kotula; and U.S. Pat. No. 6,675,106 to Keenan and Kotula. Descriptions of the current state-of-the-art for solving constrained least squares problems in chemical applications can be found in the literature. See, e.g., C. L. Lawson and R. J. Hanson, , Prentice-Hall, Inc., Englewood Cliffs, N.J. (1974); A. Bjorck, Numerical Methods for Least Squares Problems, SIAM, Philadelphia (1995); R. Bro and S. DeJong, \u201cA fast non-negativity-constrained least squares algorithm,\u201d 11, 393 (1997); and M. Van Benthem et al., \u201cApplication of equality constraints on variables during alternating least squares procedures,\u201d 16, 613 (2002).","Given its fundamental role in solving a variety of linearly constrained optimization problems, much attention has been paid to methods for estimating least squares models subject to non-negativity constraints in particular. The method of non-negativity constrained least squares (NNLS) is an important tool in the chemometrician's toolbox. Often, it is used in ALS algorithms when the variables to be estimated are known to be non-negative (such as fluorescence spectra or chemical concentrations). The simplest way to implement these non-negativity constraints is to solve the corresponding unconstrained least squares problem and then overwrite any negative values with zeros. Unfortunately, this overwriting method does not employ an appropriate least squares criterion so one is left with a solution that has ill-defined mathematical properties. ALS algorithms based on the overwriting method, for instance, do not necessarily lower the cost function on successive iterations and, therefore, are not guaranteed to converge to a least squares minimum. Fortunately, there exist methods that solve the NNLS problems in a mathematically rigorous fashion. These methods impose non-negativity on the solution while minimizing the sum of squared residuals between the data being fit and their estimates in a true least-squares sense.","While rigorous methods ensure true least squares solutions that satisfy the non-negativity constraints, they can be computationally slow and demanding. In addition, rigorous methods are more difficult to program than the simple overwriting method. These are probably the principal reasons many practitioners of techniques that employ NNLS choose not to use rigorous methods. The standard NNLS algorithm originally presented by Lawson and Hanson was designed to solve the NNLS problem for the case of a single vector of observations. See Lawson and Hanson, Chapter 23. When applied in a straightforward manner to large-scale, multiple RHS problems, however, performance of the standard NNLS algorithm is found to be unacceptably slow owing to the need to perform a full pseudoinverse calculation for each observation vector. Bro made a substantial speed improvement to the standard NNLS algorithm for the multiple RHS case. Bro's fundamental realization was that large parts of the pseudoinverse could be computed once but used repeatedly. Specifically, his algorithm precomputes the cross-product matrices that appear in the normal equations formulation of the least squares problem. Bro also observed that, during ALS procedures, solutions tend to change only slightly from iteration to iteration. In an extension to his NNLS algorithm that he characterized as being for \u201cadvanced users,\u201d Bro retained information about the previous iteration's solution and was able to extract further performance improvements in ALS applications that employ NNLS. These innovations led to a substantial performance increase when analyzing large multivariate, multi-way data sets. However, even with Bro's improvements, a need remains for further performance improvements and for a fast algorithm that has general applicability to the solution of linearly constrained, multiple RHS least squares problems.","The present invention is directed to a fast combinatorial algorithm that has improved performance compared to prior algorithms for solving linearly constrained least squares problems. The combinatorial algorithm exploits the structure that exists in large-scale problems in order to minimize the number of arithmetic operations required to obtain a solution. Based on combinatorial reasoning, this algorithm rearranges the calculations typical of standard solution methods. This rearrangement serves to reduce substantially the amount of computation required in a typical MCR problem. The combinatorial algorithm has general applicability for solving least squares problems subject to a wide variety of equality and inequality constraints.","The application of the combinatorial algorithm to the solution of the NNLS problem is described herein in detail. The combinatorial algorithm has been tested by performing calculations that are typical of ALS applications on spectral image data. The spectral image data sets varied widely in size and, in all cases, the combinatorial algorithm outperforms both Bro's simple and advanced-user modes, and it does so without the attendant overhead required in the latter case. Using the overwriting method as a point of reference, the combinatorial algorithm also exhibits much better scaling behavior for problems with very large (e.g., greater than 10) numbers of RHS vectors.","The present invention is directed to a fast combinatorial algorithm for the solution of linearly constrained least squares problems. The fast combinatorial algorithm can be used to solve both least squares problems subject to either equality or inequality constraints.","For solving a multiple right-hand-side least squares problem AX=B, subject to a set of equality constraints EX=F, j=1, 2, . . . , n, the fast combinatorial algorithm comprises: a) computing constant parts of the pseudoinverse of A; b) partitioning the column indices C of B into q column subsets Ccorresponding to q unique constraint matrices E, k=1, 2, . . . , q; c) choosing a column subset  to be a particular subset Ccorresponding to a kunique constraint that has not yet been solved; d) forming a general set of linear equations using the constant parts of the pseudoinverse and the constraint matrices Eand Ffor the column subset ; e) factoring the coefficient matrix of the general set of linear equations to provide an equivalent triangular set of linear equations; f) solving the triangular set of linear equations for the jcolumn vector of X for a j in the column subset ; g) repeating step f) until all column vectors of X corresponding to the column subset  have been solved for; and h) repeating steps c) through g) for a next column subset  until all q column subsets have been solved for, thereby providing a solution matrix {circumflex over (X)}.","For solving a multiple right-hand-side least squares problem AX=B, subject to an inequality constraint CX\u2267D, the fast combinatorial algorithm comprises: a) computing the constant parts of the pseudoinverse of A; b) choosing an initial feasible solution for X and, for each column of X, partitioning the row indices into a passive set , containing the sets of the indices of the variables that are unconstrained, and an active set , containing the sets of the indices of variables that are constrained to equal the boundary value; c) putting the column indices of the initial feasible solution that are not known to be optimal into a column set C, thereby providing a passive set and a complementary active set ; d) solving for Xcombinatorially according to AX=B, subject to X=0, for the variables represented in the current passive set ; e) identifying any infeasible solution vectors in X, putting their column indices in an infeasible set I, and making them feasible by moving variables from an infeasible passive set to a complementary active set ; f) solving for Xcombinatorially according to AX=B, subject to X=0, for the variables represented in the infeasible passive set ; g) repeating steps e) through f) until no infeasible solution vectors are obtained, thereby providing a current feasible solution X; h) identifying any solution vectors Xthat are not optimal and removing their column indices j from the column set C, thereby providing a new column set C representing solution vectors that are not optimal; i) for each column index j in column set C, moving a variable from the active set to the passive set for each non-optimal solution vector x, thereby providing a new passive set and new complementary active set ; and j) repeating steps d) through i) until all solution vectors xare optimal, thereby providing a solution matrix {circumflex over (X)}.","Notation herein is typical of this type of work: scalars are lowercase italics, column vectors are lowercase bold, matrices are uppercase bold, and transposes of vectors (i.e., row vectors) and matrices are indicated by a superscript T. Columns of matrices are depicted using lowercase bold with a dot-index subscript, e.g., column one of matrix A is the column vector a. Rows of matrices are depicted using lowercase bold with an index-dot subscript, e.g., row one of matrix A is the row vector a (note that it is not shown as a transpose). Prepended superscripts represent the ordinal in a series of iterations. Sets of indices are represented with fancy type, e.g.,  and sets containing sets of indices by the bolded fancy type, e.g., . Finally, submatrices are represented as pre- and\/or post-subscripted bolded capital letters. Thus, Xis the submatrix of X comprising the rows whose indices are in the sets  and columns whose indices are in the set C.","The general, linearly constrained linear model for a single vector of observations (or single RHS) can be defined as:\n\nAx=b satisfying the constraints:\n\nCx\u2266d\n\nEx=f\u2003\u2003(1)\n\nThe p-vector x holds the p model parameters that will be estimated during the least squares process, and b is an m-vector of observations. A is an m\u00d7p data matrix that expresses the linear relationship between the p model parameters and the m individual observations. In a spectral imaging application, b might be the m-channel spectrum observed at a single pixel. Then, A is a matrix whose columns represent the pure-component spectra of the p chemical species that constitute the sample and we wish to estimate x, the concentrations of the respective pure components in the given pixel. The linear inequality constraints are described by the entities C and d, and the equality constraints by E and f. These constraints are used to limit possible solutions to those that are physically acceptable. Non-negativity of species concentrations and spectral emission intensities are typical examples of inequality conditions that must be met by solutions that conform to physical reality. A, together with the system of constraints, defines the structure and limits of the data model.\n","Consider first the solution to the least squares problem having a single RHS. According to the least squares model, the vector of unknown model parameters x is typically estimated by finding that value of x, {circumflex over (x)}, that minimizes the sum of the squared differences between the model predictions {circumflex over (b)}=A{circumflex over (x)} and the observations b, namely,",{"@attributes":{"id":"p-0021","num":"0020"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","mrow":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mover":{"mi":"b","mo":"^"},"mi":"i"},{"mi":["b","i"]}],"mo":"-"}},"mn":"2"},"mo":"."}}}},"br":{}},{"@attributes":{"id":"p-0022","num":"0021"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":"x","mo":"^"},"mo":"=","mrow":{"munder":{"mi":["min","x"]},"mo":"\u2062","mrow":{"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["Ax","b"],"mo":"-"}},"mn":["2","2"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["subject","to","constraints","on","x"]}}}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}}},"As will be discussed later, a constrained least squares problem can be recast in terms of a sequence of unconstrained problems, thus the procedure for solving the unconstrained least squares problem is of fundamental importance and will be reviewed briefly. The unconstrained least squares solution to Eq. (1) can be written as:\n\n{circumflex over (x)}=Ab\u2003\u2003(3)\n\nwhere A is the pseudoinverse of A. The pseudoinverse can be computed in a variety of ways. Perhaps the most familiar computation to chemometricians is the normal equations, the first step of which is the formation of the cross-products by left multiplication of Eq. (1) by the transpose of A\n\nAAx=Ab\u2003\u2003(4)\n\nTherefore, a commonly employed algorithm for obtaining the least squares estimate of {circumflex over (x)} in the unconstrained case is to solve the normal equations:\n\nCompute AA\n\nCompute Ab\n\nSolve for x, the set of equations AAx=Ab\u2003\u2003(5)\n\nIt will be important to note that the matrix of coefficients AA only involves the structure of the model, whereas the RHS Ab involves both the model structure and the observations specific to a given experiment. x can then be solved for using various techniques known to those with skill in the art.\n","The fact that AA is known to be a symmetric positive definite matrix allows some additional numerical improvements. AA can be factored using the Cholesky decomposition to yield two triangular systems of equations that are readily solved by standard means. See G. H. Golub and C. F. Van Loan, 3Ed., The Johns Hopkins University Press, Baltimore (1996). The solution algorithm can then be written:\n\nCompute ()\n\nCompute Ab\n\nSolve for x, the triangular sets of equations:\n\nGy=Ab\n\nGx=y\u2003\u2003(6)\n\nAn alternative method of solution makes use of an orthogonal factorization of the data matrix A. The least squares solution algorithm when the \u201cthin\u201d QR factorization is employed can be written\n\nCompute the QR factorization:A=QR\n\nCompute Qb\n\nSolve for x, the triangular set of equations:Rx=Qb\u2003\u2003(7)\n\nIn this formulation, Q is an m\u00d7p matrix having orthonormal columns and R is upper triangular. Not surprisingly, there is a close connection between these two methods. The Cholesky factor G, for instance, is equal to the QR factor R. In the remainder of this description, only the normal equations approach to solving least squares problems will be used. It will be recognized, however, that the combinatorial algorithm of the present invention does not depend on the details of the normal equations method and that other methods for solving unconstrained least squares problems could be used with equal effect. The details of the foregoing methods, as well as several others, are available in the references of Bjorck, Bro, and elsewhere.\n","Solving the Unconstrained Least Squares Problem with Multiple RHS Now, consider the solution to the least squares problem having a multiple RHS, wherein the number of columns of observations is greater than one. In the case of multiple observation vectors bmultiple solution vectors {circumflex over (x)}can be estimated. The general, the linearly constrained least squares model can be conveniently written in matrix terms as:\n\nAX=B satisfying the constraints:\n\nCX\u2267D\n\nEX=F\n\nwhere:\n\nB=[bb. . . b]\n\nX=[xx. . . x], etc.\u2003\u2003(8)\n\nwhere n is the number of RHS vectors. As before, the goal is to minimize the sum of the squared residuals, now\n",{"@attributes":{"id":"p-0026","num":"0025"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mover":{"mi":"b","mo":"^"},"mi":"ij"},{"mi":["b","ij"]}],"mo":"-"}},"mn":"2"}}},"mo":"\u2261","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mover":{"mi":"B","mo":"^"},"mo":"-","mi":"B"}},"mi":"F","mn":"2"}},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0027","num":"0026"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":"X","mo":"^"},"mo":"=","mrow":{"mrow":[{"munder":{"mi":["min","x"]},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["AX","B"],"mo":"-"}},"mi":"F","mn":"2"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","mrow":{"munder":{"mi":"min","msub":{"mi":"x","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mi":"Ax","mrow":{"mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mi":"b","msub":{"mi":["\u2022","j"]}}],"mo":"-"}},"mn":["2","2"]}}}],"mo":"="}},{"mi":["subject","to","the","constraints"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}},"br":[{},{},{},{},{},{}],"in-line-formulae":[{},{},{},{},{},{},{},{},{},{}],"sup":["T","T","T","T"],"sub":["\u00b7j ","\u00b7j","\u00b7j","\u00b7j "]},"Using one data set (data set  in Table I) with p=7, m=1024 and n=16384, Algorithm (10) runs in 12.3 seconds using Matlab Version 7 on a 1 GHz Pentium III-based computer. This algorithm clearly requires excessive computation. However, the speed of the computation can be improved. In particular, it is noted that (1) AA doesn't depend on the column j so its computation can be moved outside of the loop, and (2) that Abis just the jcolumn of AB so the matrix-matrix multiplication can also be performed outside of the loop according to a revised algorithm:\n\nCompute AA\n\nCompute AB\n\nFor j=1:n\n\nSolve for x, the set of equations =()\n\nend\u2003\u2003(11)\n\nThe revised Algorithm (11) runs in 2.3 seconds for the same data set on the same computer. Finally, solving the sets of linear equations requires the factorization of AA, which can also be accomplished outside of the loop. Employing the Cholesky factorization, the revised algorithm becomes:\n\nCompute ()\n\nCompute AB\n\nFor j=1:n\n\nSolve for x, the triangular sets of equations:\n\n=()\n\nGx=y\n\nend\u2003\u2003(11)\n\nThe revised Algorithm (12) requires 1.5 seconds to run. Of course, these efficiencies are well known in the art. Indeed, these efficiencies are built-in to the Matlab function which, including additional loop optimizations, can solve the current problem in 0.8 second.\n","The reason that AA (and its factorization) can be moved outside of the loop is that each individual single RHS problem has exactly the same structure as defined by the data matrix A, as noted previously. Therefore, the common structural factor AA can be computed once and a simpler calculation on each individual column can subsequently be performed to obtain the final solution of an unconstrained problem.","This commonality of structure is the key insight that leads to the \u201ccombinatorial approach\u201d of the present invention to solving constrained least squares problems. In more complex, constrained least squares minimization problems, the problem structures may not be the same for all RHS and, as with active\/passive set algorithms, may even evolve as the solution proceeds. However, if at any point in the algorithm the number of unique problem structures reflected in A and the constraints is much less than the number columns that are to be solved, computational efficiency can be achieved by grouping together and solving all subproblems having like structure. In the following discussion, the combinatorial approach will be developed in several simple examples of equality and inequality constrained least squares problems. Finally, a combinatorial algorithm will be described for the problem NNLS in detail.","A common approach to the solution of equality constrained least squares problems is the method of weighting. Consider the solution to the problem:\n\nAX=B subject to the constraints:\n\nEx=f,j=1:n (13)\n\nwhere the structure of the constraints, namely, E, can vary from column to column of B. The solution for the jsingle column of B is formulated as:\n",{"@attributes":{"id":"p-0032","num":"0031"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"A"}},{"mtd":{"mrow":{"mi":"\u03b3","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["E","j"]}}}}]}},"mo":"\u2062","msub":{"mi":"x","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"b","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}},{"mtd":{"mrow":{"mi":"\u03b3","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"f","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}}]}}],"mo":"="},{"mi":["\u03b3","is","a","large","constant"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}],"mo":";"}},{"mrow":{"mo":["(",")"],"mn":"14"}}]}}}},"br":[{},{},{},{},{},{},{},{},{},{}],"in-line-formulae":[{},{},{},{},{},{},{},{},{},{},{},{},{},{}],"i":["A","A+\u03b3","E","E","x","=A","b","E","f","A","A+\u03b3","E","E","x","A","B","E","f"],"sup":["T","2","T","T","2","T","T","2","T","T","T","T","2","T","T","2","T","T","2","T"],"sub":["j","j","\u00b7j","\u00b7j","j","\u00b7j","j","j ","\u00b7j","j","j","\u00b7j","\u00b7j","j","\u00b7j ","j","j"]},"A closure constraint is an example of an equality constraint. The closure constraint is also known as the \u201csum-to-one\u201d constraint. It may arise, for instance, in describing the relative concentrations of reacting species in a closed chemical system. To apply closure, Eis a p-vector of ones 1and Fis an n-vector of ones 1. Since E is independent of the column index, computations involving E can be moved outside of the loop to yield the analog of Algorithm (12), which requires only a single matrix factorization:\n\nCompute (11)\n\nCompute 11\n\nFor j=1:n\n\nSolve for x, the triangular sets of equations:\n\nGy=h\n\nGx=y\n\nend\u2003\u2003(17)\n","Another example of an equality constraint is the variable equality problem. In the variable equality problem, certain variables are constrained to equal a known value and the set of variables to be constrained may vary from column to column. For the case of simple variable equality, Eis a k\u00d7p selection matrix where k is the number of the p total variables that are to be constrained in the jcolumn. The complete structure of the constraint matrix can be concisely represented by a p\u00d7n matrix V. If the ivariable in the jcolumn of X is to be constrained, v=1, otherwise, v=0. In this representation, EE=diag(v) and Ef={tilde over (f)}. The application of Ein the latter expression transforms the k-vector finto a p-vector {tilde over (f)}where vindexes the locations of the original k variables and the remaining elements are zero. Straightforward application of Algorithm (16) yields:\n\nCompute AA\n\nCompute AB\n\nFor j=1:n\n\nSolve for x, the set of equations:\n\n(())=()+\u03b3\n\nend\u2003\u2003(18)\n","It is at this point that the combinatorial approach of the present invention provides a performance improvement for the solution of large-scale problems. Given p total variables, there are at most 2unique combinations of variables that can be constrained and, in practical applications, there are often many times fewer. If q is the number of unique constraint matrices E(or equivalently, v), where k=1, 2, . . . , q, and q<<n, then multiple RHS problems necessarily share a common constraint structure and the corresponding factorizations can be moved out of the loop. Letting Cbe the set of column indices having the kunique constraint matrix in common, it is noted that\n\n{CC. . . C}={1 2 . . . n}\u2003\u2003(19)\n\nThis partitioning into q unique constraint matrices enables Algorithm (18) to be rewritten as a combinatorial algorithm:\n\nCompute AA\n\nCompute AB\n\nCompute the column partition {CC. . . C}=C\n\nFor all \u03b5C\n\nCompute (())\n\nCompute =()+\u03b3\n\nFor all j\u03b5\n\nSolve for x, the triangular sets of equations:\n\nGy=h\n\nGx=y\n\nend\n\nend\u2003\u2003(20)\n\nwhere V is the representation of the common constraint for the column subset .\n","The combinatorial Algorithm (20) for solving equality constrained least squares problems requires q matrix factorizations as compared to the n factorizations needed by prior Algorithm (18). The remaining need for the optimization of the combinatorial algorithm is for an efficient method for partitioning the column indices into the sets having common constraints.","An algorithm for partitioning a logical matrix into column sets having equal columns is, perhaps, best illustrated by example. In the foregoing discussion, the matrix V was used to indicate which variables were to be equality constrained. Matrix (21) is a 16 RHS example where a small fraction of the variables are to be constrained, as indicated by a one. Note that each column of V can be thought of as the binary representation of a decimal integer. The Vector (22) gives the decimal equivalent of each column, which is obtained by multiplying each column of V by the row vector [4 2 1].",{"@attributes":{"id":"p-0038","num":"0037"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"V","mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"1"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"1"},{"mn":"1"},{"mn":"1"},{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"0"}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"21"}}]}}}},"br":{},"in-line-formulae":[{},{}]},"The code (Vector (22)) can then be sorted and the column indices permuted accordingly. The results are shown in Vectors (23) and (24).\n\nsorted code=[0 0 0 0 1 1 1 2 2 2 3 3 4 4 4 4]\u2003\u2003(23)\n\nindices=[9 10 11 16 3 4 15 7 8 14 2 6 1 5 12 13]\u2003\u2003(24)\n\nSubtracting a circularly shifted copy (Vector (25)) of the sorted code (Vector (23)) from the sorted code provides the shifted difference code (Vector (26))\n\nshifted code=[4 0 0 0 0 1 1 1 2 2 2 3 3 4 4 4]\u2003\u2003(25)\n\nshifted diff=[\u22124 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0]\u2003\u2003(26)\n\nVector (26) has the following properties: (a) the number of non-zero elements is equal to the number of unique vectors contained in the original matrix, (b) the locations of the non-zero elements are pointers to the start of each column index set in the permuted indices vector, and (c) the locations of the non-zero elements are pointers to the binary code for each of the respective column sets. Thus, for this example, there are q=5 unique columns out of n=16 total, and the unique column index sets are\n\nC={9 10 11 16}, C={3 4 15}, C={7 8 14}, C={2 6}, C={1 5 12 13}\u2003\u2003(27)\n\nwith\n",{"@attributes":{"id":"p-0039","num":"0038"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"v","msub":{"mi":"C","mn":"1"}},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}}]}}},{"msub":{"mi":"v","msub":{"mi":"C","mn":"2"}},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"1"}}]}}},{"msub":{"mi":"v","msub":{"mi":"C","mn":"3"}},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"1"}},{"mtd":{"mn":"0"}}]}}},{"msub":{"mi":"v","msub":{"mi":"C","mn":"4"}},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"1"}},{"mtd":{"mn":"1"}}]}}},{"mrow":[{"mi":"and","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":"v","msub":{"mi":"C","mn":"5"}}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"1"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}}]}}],"mo":"="}],"mo":[",",",",",",","]}},{"mrow":{"mo":["(",")"],"mn":"28"}}]}}}}},"A sample Matlab code that implements this column partitioning algorithm, and which accounts for the special case that all columns are equal, is shown below:","function [ColumnSet, StartIndices] = ColumnPartition(V)","[nVars, nRHS] = size(V);","[code, ColumnSet] = sort (2.^(nVars\u22121:-1:0)*V );","if code(1) == code(end)\n\n","else\n\n","end","While Matlab utilizes a version of Quicksort, other sorting algorithms can be used as well. A particularly efficient choice for the case that there are relatively few variables, e.g., 2<n, is based on the counting sort algorithm, which is described in T. H. Cormen et al., 2Ed., MIT Press, Cambridge, Mass. (2001).","As in the previous example, consider a simple variable equality such that the matrix Eis a k\u00d7p selection matrix where k is the number of the p total variables that are to be constrained in the jcolumn. A p\u2212k\u00d7p selection matrix Ucan be defined that selects the variables that are unconstrained in the jcolumn. The matrix P given by:",{"@attributes":{"id":"p-0048","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["P","j"]},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":["U","j"]}}},{"mtd":{"msub":{"mi":["E","j"]}}}]}}}},{"mrow":{"mo":["(",")"],"mn":"29"}}]}}}},"br":{},"sup":"th "},{"@attributes":{"id":"p-0049","num":"0050"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"Ax","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mo":"=","mrow":{"mrow":[{"msup":{"mi":["AP","T"]},"mo":"\u2062","msub":{"mi":"Px","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},{"mrow":[{"mrow":[{"mo":["[","]"],"mrow":{"msubsup":[{"mi":["AU","j","T"]},{"mi":["AE","j","T"]}],"mo":"\u2062"}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"msub":[{"mi":["U","j"]},{"mi":"x","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}],"mo":"\u2062"}}},{"mtd":{"mrow":{"msub":[{"mi":["E","j"]},{"mi":"x","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}],"mo":"\u2062"}}}]}}],"mo":"\u2061"},{"mrow":{"mrow":[{"msubsup":{"mi":["AU","j","T"]},"mo":"\u2062","msub":{"mi":"Ux","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},{"msubsup":{"mi":["AE","j","T"]},"mo":"\u2062","msub":{"mi":"Ex","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}],"mo":"+"},"mo":"=","msub":{"mi":"b","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}],"mo":"="}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"30"}}]}}}},"br":[{},{},{},{},{}],"sub":["\u00b7j","\u00b7j ","j","j","\u00b7j","\u00b7j","j","\u00b7j","j","j","j","\u00b7j","j","\u00b7j","j","j","\u00b7j","ij","ij","j, ","j","v",{"sub2":"j"},"j "],"in-line-formulae":[{},{},{},{}],"i":["AU","U","x","=b","\u2212AE","f","U","A","AU","U","x","=U","A","b","\u2212U","A","AE","f"],"sup":["T","T","T","T","T","T","T","th ","th ","c"]},{"@attributes":{"id":"p-0050","num":"0051"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mmultiscripts":[{"mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"A"}},"msub":[{"mi":["v","j"]},{"mi":["v","j"]}],"none":[{},{}],"mprescripts":{}},{"mi":"x","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"none":[{},{}],"mprescripts":{},"msub":{"mi":["v","j"]}}],"mo":"\u2062"},{"mmultiscripts":{"mrow":[{"mo":["(",")"],"mrow":{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"B"}},{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"none":[{},{}],"mprescripts":{},"msub":{"mi":["v","j"]}},"mo":"-","mrow":{"mmultiscripts":[{"mrow":[{"mo":["(",")"],"mrow":{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"A"}},{"msubsup":{"mi":["v","j","c"]},"mo":"\u2009"}],"none":[{},{}],"mprescripts":{},"msub":{"mi":["v","j"]}},{"mi":"x","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"none":[{},{}],"mprescripts":{},"msubsup":{"mi":["v","j","c"]}}],"mo":"\u2062"}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"33"}}]}}}},"br":[{},{},{},{},{}],"sub":["j ","1 ","2 ","q"],"in-line-formulae":[{},{},{},{},{},{},{},{}],"sup":["T","T"],"img":{"@attributes":{"id":"CUSTOM-CHARACTER-00031","he":"3.13mm","wi":"2.79mm","file":"US07451173-20081111-P00001.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}},{"@attributes":{"id":"p-0051","num":"0052"},"maths":[{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":["compute","G"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mi":"chol","mo":"\u2061","mrow":{"mo":["(",")"],"mmultiscripts":{"mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"A"}},"msub":[{"msub":{"mi":"v"},"mi":"Q"},{"msub":{"mi":["v","Q"]}}],"none":[{},{}],"mprescripts":{}}}}],"mo":"="}}},{"mtd":{"mrow":{"mrow":[{"mi":["compute","H"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mmultiscripts":{"msub":[{"mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"B"}}},{"mi":"v"}],"mi":"Q","none":[{},{}],"mprescripts":{}},"mo":["\u2062","\u2062"],"msub":{"mo":"-","msub":{"mi":"v"}},"mrow":{"msub":[{"mrow":[{"mo":["(",")"],"mrow":{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"A"}},{"msubsup":[{"mi":["v","c"],"mrow":{"mo":"\u2062","mi":"Q"}},{"mi":["v","c"],"mrow":{"mo":"\u2062","mi":"Q"}}],"mo":"\u2062"}]},{"mi":"X","mrow":{"mrow":{"mi":"Q","mo":"\u2062"},"mo":"\u2062","mstyle":{"mtext":{}}}}],"mo":"\u2062"}}],"mo":"="}}}]}}},{"@attributes":{"id":"MATH-US-00011-2","num":"00011.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":["for","all","j","y"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"msup":{"mi":["G","T"]}},"mo":"=","msub":{"mi":"h","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}},{"@attributes":{"id":"MATH-US-00011-3","num":"00011.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":[{"mi":"G","msub":{"msub":{"mi":["v","Q"]}}},{"mi":"x","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}],"mo":"\u2062"},"mo":"=","mi":"y"}}}],"br":[{},{},{}],"in-line-formulae":[{},{},{},{}]},"In  is shown a flow diagram of a generic fast combinatorial algorithm for solution of an equality constrained least squares problem. It is desired to solve the multiple right-hand-side least squares problem AX=B, subject to a set of equality constraints, EX=F, where j=1, 2, . . . , n.","At step , the constant parts of the pseudoinverse of A (e.g., AA and AB, or R and QB) are precomputed.","At step , column indices C of B are partitioned into q subsets corresponding to q unique constraint matrices E(or equivalently, V), where k=1, 2, . . . , q. Cis the set of column indices having the kunique constraint in common, according to Eq. (19). A vector representation, v for a column subset  having the kunique constraint can be obtained using an algorithm for column partitioning (e.g., Eqs. (21)-(28)).","At step , a column subset Q corresponding to a kunique constraint is chosen from the set of column indices, according to \u03b5{CC. . . C}. Therefore,  is chosen to be a particular subset Cthat has not yet been solved.","At step , a general set of linear equations is formed using the constant parts of the pseudoinverse and the constraint matrices Eand Ffor the column subset . The general set of linear equations is dependent on the problem type. For example, to solve the problem of variable equality constraints by direct elimination, the general set of linear equations can comprise the coefficient matrix",{"@attributes":{"id":"p-0057","num":"0058"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"M","mo":["\u2062","\u2062"],"msub":[{"mo":"=","msub":{"mi":["v","Q"]}},{"mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"A"}},"msub":{"mi":["v","Q"]}}]}}},"br":{}},{"@attributes":{"id":"p-0058","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"H","mo":["\u2062","\u2062"],"msub":{"mo":"=","msub":{"mi":["v","Q"]}},"mrow":{"msub":[{"mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"B"}},"mi":"Q"},{"mo":"-","msub":{"mi":["v","Q"]}}],"mo":["\u2062","\u2062"],"mrow":{"msub":[{"mrow":[{"mo":["(",")"],"mrow":{"msup":{"mi":["A","T"]},"mo":"\u2062","mi":"A"}},{"msubsup":[{"mi":["v","Q","C"]},{"mi":["v","Q","C"]}],"mo":"\u2062"}]},{"mi":["X","Q"]}],"mo":"\u2062"}}},"mo":","}}},"br":{}},"At step , the coefficient matrix of the general set of linear equations is factored to provide an equivalent triangular set of linear equations for the column subset . The factorization method used depends on the solution process. For example, if a Cholesky factorization of the general equations is used to provide the triangular set of linear equations, the Cholesky factor is G=chol(M), according to Algorithm (34).","At step , the solution vector xfor the jcolumn of X is solved for using the triangular set of linear equations for the column subset . For example, if the triangular set of linear equations are obtained from a Cholesky factorization, the jcolumn vector of X can be obtained by solving the triangular set of linear equations",{"@attributes":{"id":"p-0061","num":"0062"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"msup":{"mi":["G","T"]},"mo":"\u2062","mi":"y"},{"mrow":{"msub":[{"mi":"h","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mi":"G","msub":{"mi":["v","Q"]}},{"mi":"x","mrow":{"mi":["\u2022","j"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"and"},"mo":"=","mi":"y"}],"mo":"="},"mo":","}}},"br":{}},"At step , if all of the xhave been solved for corresponding to all j columns of X in the column subset , the algorithm proceeds to step . If all of the solution vectors in the column subset  have not been solved for, the algorithm returns to step  and the next column j of X in the column subset  is solved for.","At step , after all j of the columns for the column subset  have been solved for, the algorithm returns to step . At step , another column subset  is chosen, corresponding to another unique constraint k. For relatively large p, if may be advantageous to sort the various sets of column indices and solve them in optimized order, as described in the Further Improvements section below. Steps  through  are then repeated until solution vectors have been obtained for all of the q column subsets  of column indices in C, thereby providing the solution matrix {circumflex over (X)}.","Now consider the solution of the least squares problem with inequality constraints. In general, inequality constraints define a feasible region in multidimensional space. At the least squares minimum, variables either will reside in the interior of the feasible region or will be found on the boundary. The passive set comprises those variables that are interior to the feasible region, and the active set, those on the boundary. If it were known, a priori, the active set for the optimal solution, the solution to the inequality constrained least squares problem could be obtained as the solution to the equality constrained problem with all active variables being constrained to be equal to their boundary values. The active\/passive set strategy for solving inequality constrained least squares problems, then, is to iteratively move variables between the active and passive sets, typically one at a time, in such a way that the cost function steadily decreases and feasibility of the solution is maintained. Since there are a limited number of ways to partition variables between the active and passive sets, convergence of this process is guaranteed.","A generic active\/passive set algorithm for the case that there are n multiple independent RHS vectors is:\n\ninitialize X,  and  to be feasible\n\nfor j=1:n\n\nk=0\n\nwhile xis not optimal\n\n[]=\u0192([])\n\n1\n\nend while\n\nend for\u2003\u2003(35)\n\nThe elements of the active () and passive () sets are subsets that index the rows of X. The variables in X that are indexed in  are those that are solved for in an unconstrained least squares sense.  is merely the complement of  and it contains the indices of variables that are constrained to the boundary value. The challenge resides in selecting variables to assign to one set or the other. The function \u0192 embodies the details of how variables are moved between the active and passive sets, and how the solution vectors are updated from the kiteration to the k+1iteration. This function is problem-dependent but might be a single iteration of Lawson and Hanson's NNLS algorithm, for instance. No matter what the other details of \u0192 are, at some point an unconstrained least squares solution for the passive variables, that is, a subset of x, will be computed. In general, this subset will differ from column to column, and will necessarily differ from iteration to iteration. Since the optimization is performed for each column of X in a serial fashion, this approach can be termed column-serial.\n","As the first step toward grouping, or partitioning, like problems to take advantage of the combinatorial approach, it is observed that the two loops in Algorithm (35) can be interchanged. That is, rather than performing all iterations for each column in series, we can perform the kiteration for all columns corresponding to non-optimal solutions in parallel:\n\ninitialize X,  and  to be feasible\n\ninitialize C={1 2 . . . n}\n\nk=0\n\nwhile C\u2260\u00d8\n\nfor all j\u03b5C\n\n[]=\u0192([])\u2003\u2003(36)\n\nif xis optimal, remove index j from C\n\nend for\n\n1\n\nend while\n\nUsing the combinatorial approach of the present invention, the RHS columns can be partitioned corresponding to like passive sets at the points(s) in the algorithm that the passive variables need to be solved for. Therefore, this approach can be termed column-parallel. Using this column-parallel approach in a fast combinatorial algorithm significantly reduces the computational burden, as described above. Again, the details are problem-dependent.\n","Problem NNLS is used to illustrate this combinatorial, column-parallel approach. A high level description of Lawson and Hanson's approach to Problem NNLS for a single RHS is:\n\ninitialize x,  and  to be feasible\n\nwhile x is not the optimal solution\n\nsolve Ax=b subject to x=0\n\nwhile any x<0\n\nmake x feasible (move variables from \u2192)\n\nsolve Ax=b subject to x=0\n\nend while\n\nif x is not optimal\n\nmove a variable from \u2192\n\nend if\n\nend while\u2003\u2003(37)\n","The standard NNLS Algorithm (37) simply fills in the relevant details for the function \u0192 in Algorithm (35) and is appropriate to the single RHS problem. Note that the solution to the inequality-constrained least squares problem is obtained by solving a sequence of equality-constrained problems. The details of how variables are selected to be moved between sets, optimality conditions, etc., have all been thoroughly described in the references of Lawson and Hanson, Bro, and others. An example will follow that helps to clarify the differences in selecting variables to assign to the active and passive set for the standard NNLS algorithm and the combinatorial algorithm.","The general column-parallel active\/passive set Algorithm (36), the details of NNLS in Algorithm (37), and the efficient way to solve the equality constrained problems with Algorithm (34) or, alternatively, Algorithm (20) can be used to design an efficient fast combinatorial NNLS (FC-NNLS) algorithm for large-scale problems. In the fast combinatorial approach the columns of X whose passive sets are identical are identified, they are collected together, the appropriate pseudoinverses are computed and the passive variables are solved for. At a very basic level, the active\/passive set method solves an inequality-constrained least squares problem as a sequence of equality-constrained problems. The overall FC-NNLS algorithm is responsible for defining that sequence and it does so in two ways. First, the combinatorial NNLS algorithm governs the general flow of problems in the column-serial\/column-parallel sense outlined above. Second, the NNLS algorithm defines the details of each individual problem by choosing the particular passive-set variables that must be estimated at each step in the sequence. The NNLS algorithm, in turn, relies upon an unconstrained least squares solver to estimate the optimal zero-equality-constrained solution for each subproblem. The complete, integrated FC-NNLS algorithm is shown below:\n\ncompute AA\n\ncompute AB\n\ninitialize X, and  to be feasible\n\ninitialize C={1 2 . . . n}\n\nwhile C\u2260\u00d8\n\nsolve AX=B,subject to A=0,given AA,(AB),,\n\nset is infeasible (i.e. any element of <0)}\n\nwhile I\u2260\u00d8\n\nmake Xfeasible (move variables from \u2192)\n\nsolve AX=Bsubject to X=0,given AA,(AB),,\n\nset is infeasible}\n\nend while\n\nfor all j\u03b5C\n\nif xis optimal\n\nremove index j from C\n\nelse\n\nmove a variable from \u2192\n\nend if\n\ne end for\n\nend while\u2003\u2003(38)\n","In  is shown is shown an exemplary flow diagram of the fast combinatorial algorithm for solving a multiple right-hand-side least squares problem AX=B, subject to the non-negativity constraint X\u22670. Of course, the steps of the FC-NNLS algorithm can be generalized to the solve any inequality constrained least squares problem CX\u2267D using the combinatorial approach of the present invention.","At step , the parts of the pseudoinverse of A (e.g., AA and AB, or R and QB) that will remain constant throughout the calculation are precomputed.","At step , an initial feasible solution is chosen for X. The indices of the elements of X that have been initialized to zero are placed in the active set  and the elements of the passive set  are the indices of X that have been initialized to be greater than zero. It is standard to choose the zero-vector or zero-matrix as the initial feasible solution (i.e., a null passive set), and to assume that all variables are actively constrained. This would seem to be a particularly poor choice in the context of curve resolution applications. As pointed out by Gemperline and Cash in their discussion of the \u201cself-modeling curve resolution research hypothesis,\u201d very few actively constrained variables are expected in the fitted model if the constraints accurately and precisely reflect the physical reality of the given experiment. See P. J. Gemperline and E. Cash, \u201cAdvantages of Soft versus Hard Constraints in Self-Modeling Curve Resolution Problems. Alternating Least Squares with Penalty Functions,\u201d 75, 4236 (2003). Therefore, a better choice may be to initially assume that all variables are passively constrained. This assumption is equivalent to obtaining a fully unconstrained least squares solution as the first step in the solution process. There are many methods available to perform the minimization (e.g., normal equations and orthogonal factorization). See A. Bjorck, , SIAM, Philadelphia (1996). The solution can then be made feasible by overwriting all negative values with zero to provide the initial active set variables. The initial passive set variables are then determined, namely, the positive entries in X. This produces the so-called overwriting approximation to NNLS, which has been commonly employed in MCR applications. See J. J. Andrew and T. M. Hancewicz, \u201cRapid Analysis of Raman Image Data Using Two-Way Multivariate Curve Resolution,\u201d 52, 797 (1998). In fact, a column of X is itself the optimal solution in the commonly encountered case that all elements are positive. With this choice of initializer, the rigorous least squares solutions obtained by the combinatorial NNLS algorithm are seen to be refinements of those obtained by the approximate overwriting method. This choice of initializer is similar in spirit to Bro's advanced-user mode in the sense that approximate active and passive sets are given to the NNLS algorithm up front. The current approach has the advantage, however, that the estimates are self-generated, thus avoiding the overhead associated with retaining copies of active and passive sets during ALS procedures.","At step , the column indices of the initial feasible solution that are not known to be optimal are put into a column set C. Note that, particular if a fully unconstrained least squares solution is used to obtain the initial feasible solution, many solution vectors may already be optimized and require no further processing. The non-optimal column indices can be partitioned into a passive set , containing the current passive sets of indices, and the complimentary active set , containing the current active sets of indices.","The remainder of the algorithm has a main loop\/inner loop structure similar to that of the original Lawson and Hanson NNLS algorithm. The main loop serves to generate trial least squares solutions Xgiven the current passive set and to test them for optimality. The inner loop accounts for the fact that the unconstrained least squares solution for a passive-set variable may become infeasible (i.e., contain negative values) at some point and provides a mechanism for moving variables from the passive set to the active set in order to make the solution feasible.","At step , a solution Xis obtained by combinatorially solving the least squares problem AX=B, subject to equality constraint X=0, for variables represented in the current passive set . The combinatorial solution can start with step  of the algorithm shown in , using the passive set of column indices.","After the passive variables are initially solved for, the trial least squares solutions Xare tested for feasibility. A set I can index the infeasible solutions (i.e., those columns of Xhaving an element less than zero).","Therefore, at step , the column indices of the infeasible solution vectors in Xcan be identified and put into the infeasible set I.","At step , if there are no infeasible solutions (i.e., set I is the null set), the algorithm proceeds to step . Otherwise, at step , the infeasible solution vectors in Xare made feasible by moving variables from the infeasible passive set to the complementary active set . That is, and are subsets of the sets and that contain the passive sets and the active sets, respectively, of the indices of the infeasible solutions. A feasible solution can be obtained by selecting a search direction that is guaranteed, by the theory of convex functions, to lower the cost function. The details of the variable selection and movement strategies have been described by Lawson and Hanson, and Bro. The solution can then moved along the search direction until all variables satisfy the constraints. This has the effect of leaving at least one variable on the boundary of the feasible region and a single variable is moved from \u2192.","If m variables are found to be infeasible at a given point in the iteration, it may be highly probable that all m variables will be actively constrained in the final solution. In this situation, computational efficiency can be achieved by moving all of the m variables from \u2192 at one time. This is equivalent to applying the overwriting approximation to the inner loop of the standard NNLS algorithm. Note that the outer loop will make the proper adjustment to make the solution rigorous, in a least squares sense, in the event that one or more of the m variables has been guessed wrong.","At step , after making the infeasible solution vectors feasible, it is necessary to re-estimate the variables corresponding to the infeasible passive set in the column set I. Again, the least squares estimates Xcan be obtained in a combinatorial fashion by solving the least squares problem AX=B, subject to the equality constraint X=0, for the variables represented in the infeasible passive set . The combinatorial solution can start with step  of the algorithm shown in , using the passive set of column indices. At step , the new least squares solutions Xare again tested for feasibility. The combinatorial the inner loop terminates when I has been emptied, thereby providing a current feasible solution X.","After the inner loop is completed and only feasible solutions have been obtained, each solution vector Xis tested for optimality. The optimality criteria have been described by Lawson and Hanson, and Bro","Therefore, at step , the column indices j of the optimal solution vectors are identified and removed from the set C, thereby providing a new column set C representing solution vectors that are not optimal.","At step , the set C is checked for any remaining non-optimal column indices.","At step , for each solution vector xthat is not optimal (i.e., those indices j still remaining in set C), a variable is selected to be moved from the active set to the passive set to provide a new passive set and complementary active set , for the subsequent iteration.","A single iteration of the FC-NNLS algorithm is completed for all of the non-optimal solutions indexed by set C before a subsequent iteration is started on any at step . The algorithm terminates when C becomes empty at step , providing the solution matrix {circumflex over (X)}.","The following example helps to clarify the differences between the serial approach of the standard NNLS algorithm of Lawson and Hanson and parallel approach of the combinatorial algorithm of the present invention. Consider first a NNLS problem with a single RHS. The following data can be modeled, after Eq. (1), as",{"@attributes":{"id":"p-0087","num":"0088"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"b","mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"92"}},{"mtd":{"mn":"74"}},{"mtd":{"mn":"18"}},{"mtd":{"mn":"41"}}]}}},{"mi":"A","mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"95"},{"mn":"89"},{"mn":"82"}]},{"mtd":[{"mn":"23"},{"mn":"76"},{"mn":"44"}]},{"mtd":[{"mn":"61"},{"mn":"46"},{"mn":"62"}]},{"mtd":[{"mn":"49"},{"mn":"2"},{"mn":"79"}]}]}}}],"mo":[",","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},{"mrow":{"mo":["(",")"],"mn":"39"}}]}}}},"br":{}},{"@attributes":{"id":"p-0088","num":"0089"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":"x","mo":"^"},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mo":"-","mn":"0.45"}}},{"mtd":{"mn":"0.71"}},{"mtd":{"mn":"0.68"}}]}}}},{"mrow":{"mo":["(",")"],"mn":"40"}}]}}}},"br":{}},{"@attributes":{"id":"p-0089","num":"0090"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mo":"\u2009","mn":"0"},"mo":"\u2062","mover":{"mi":"x","mo":"^"}},{"mrow":[{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}}]}},{"msup":{"mo":"\u2009","mn":"1"},"mo":"\u2062","mover":{"mi":"x","mo":"^"}}],"mo":"\u21d2"},{"mrow":[{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"0.81"}}]}},"mo":"\u2009"},{"msup":{"mo":"\u2009","mn":"2"},"mo":"\u2062","mover":{"mi":"x","mo":"^"}}],"mo":"\u21d2"},{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0.63"}},{"mtd":{"mn":"0.35"}}]}},"mo":"."}],"mo":"="}],"mo":"="}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"41"}}]}}}},"br":[{},{},{}],"in-line-formulae":[{},{}],"sup":["0","0","1","1","2","2"],"img":[{"@attributes":{"id":"CUSTOM-CHARACTER-00109","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00110","he":"3.13mm","wi":"2.79mm","file":"US07451173-20081111-P00003.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00111","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00004.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00112","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00113","he":"3.13mm","wi":"2.79mm","file":"US07451173-20081111-P00003.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00114","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00004.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00115","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00116","he":"3.13mm","wi":"2.79mm","file":"US07451173-20081111-P00003.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}],"b":["1","2","2","3"]},"The first step when performing a simple direct elimination problem of this type is to partition the matrices into the constrained and unconstrained parts as follows (for the final NNLS solution above)",{"@attributes":{"id":"p-0091","num":"0092"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"b","mo":"=","mrow":{"mi":"Ax","mo":"=","mrow":{"mrow":[{"mrow":{"mo":["[","]"],"msub":{"mi":"a","mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"}}},"mo":"\u2062","msub":{"mi":"x","mn":"1"}},{"mrow":[{"mo":["[","]"],"mrow":{"msub":[{"mi":"a","mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2"}},{"mi":"a","mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"3"}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"x","mn":"2"}}},{"mtd":{"msub":{"mi":"x","mn":"3"}}}]}}],"mo":"\u2061"}],"mo":"+"}}}},{"mrow":{"mo":["(",")"],"mn":"43"}}]}}}}},{"@attributes":{"id":"p-0092","num":"0093"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":["b","_"]},"mo":"=","mrow":{"mrow":[{"mi":"b","mo":"-","mrow":{"mrow":{"mo":["[","]"],"msub":{"mi":"a","mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"}}},"mo":"\u2062","msub":{"mi":"x","mn":"1"}}},{"mrow":[{"mo":["[","]"],"mrow":{"msub":[{"mi":"a","mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2"}},{"mi":"a","mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"3"}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"x","mn":"2"}}},{"mtd":{"msub":{"mi":"x","mn":"3"}}}]}}],"mo":"\u2061"}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"44"}}]}}}},"br":{}},{"@attributes":{"id":"p-0093","num":"0094"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":["b","_"]},"mo":"=","mrow":{"mi":"b","mo":"=","mrow":{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"92"}},{"mtd":{"mn":"74"}},{"mtd":{"mn":"18"}},{"mtd":{"mn":"41"}}]}},{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"89"},{"mn":"82"}]},{"mtd":[{"mn":"76"},{"mn":"44"}]},{"mtd":[{"mn":"46"},{"mn":"62"}]},{"mtd":[{"mn":"2"},{"mn":"79"}]}]}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":"x","mn":"2"}}},{"mtd":{"msub":{"mi":"x","mn":"3"}}}]}}],"mo":"\u2061"}],"mo":"\u2245"}}},{"mrow":[{"mo":"\u2234","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mover":{"mi":"x","mo":"^"},"mn":"2"}}},{"mtd":{"msub":{"mover":{"mi":"x","mo":"^"},"mn":"3"}}}]}}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0.63"}},{"mtd":{"mn":"0.35"}}]}}],"mo":"="}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"45"}}]}}}},"br":[{},{},{},{}],"img":[{"@attributes":{"id":"CUSTOM-CHARACTER-00117","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00118","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00119","he":"3.13mm","wi":"2.79mm","file":"US07451173-20081111-P00003.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00120","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00121","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00122","he":"3.13mm","wi":"2.79mm","file":"US07451173-20081111-P00003.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00123","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}],"in-line-formulae":[{},{},{},{}],"sup":"+"},"Given the nature of NNLS, a constrained problem with multiple RHS cannot be treated in the same way as an identical unconstrained least squares problem. Since the pseudoinverse can vary from column to column, one cannot simply precompute it once and apply it to each column of B as is the case with the unconstrained least squares solution. Consider the data above augmented by additional observations to provide a multiple RHS problem",{"@attributes":{"id":"p-0095","num":"0096"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"B","mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"92"},{"mn":"99"},{"mn":"80"}]},{"mtd":[{"mn":"74"},{"mn":"19"},{"mn":"43"}]},{"mtd":[{"mn":"18"},{"mn":"41"},{"mn":"51"}]},{"mtd":[{"mn":"41"},{"mn":"61"},{"mn":"39"}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"47"}}]}}}},"br":{}},{"@attributes":{"id":"p-0096","num":"0097"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":"x","mo":"^"},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mn":"0.82"},{"mn":"0.30"}]},{"mtd":[{"mn":"0.63"},{"mn":"0"},{"mn":"0.30"}]},{"mtd":[{"mn":"0.35"},{"mn":"0.15"},{"mn":"0.30"}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"48"}}]}}}},"br":[{},{},{}],"in-line-formulae":[{},{}],"img":[{"@attributes":{"id":"CUSTOM-CHARACTER-00124","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00125","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00126","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}],"sub":"j","sup":["th ","th ","th "]},{"@attributes":{"id":"p-0097","num":"0098"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mmultiscripts":{"mover":{"mi":"x","mo":"^"},"mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"},"none":[{},{}],"mprescripts":{},"mn":"0"},"mo":["=","\u2062"],"mi":{},"mrow":{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}}]}},"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"1"},"mmultiscripts":{"mover":{"mi":"x","mo":"^"},"mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"},"none":[{},{}],"mprescripts":{},"mn":"1"}},{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"0.81"}}]}},"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"2"},"mmultiscripts":{"mover":{"mi":"x","mo":"^"},"mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"},"none":[{},{}],"mprescripts":{},"mn":"2"}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0.63"}},{"mtd":{"mn":"0.35"}}]}}],"mo":"="}],"mo":"="}}}},{"mtd":{"mrow":{"mmultiscripts":{"mover":{"mi":"x","mo":"^"},"mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2"},"none":[{},{}],"mprescripts":{},"mn":"0"},"mo":["=","\u2062"],"mi":{},"mrow":{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}}]}},"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"1"},"mmultiscripts":{"mover":{"mi":"x","mo":"^"},"mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2"},"none":[{},{}],"mprescripts":{},"mn":"1"}},{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"0.87"}}]}},"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"2"},"mmultiscripts":{"mover":{"mi":"x","mo":"^"},"mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2"},"none":[{},{}],"mprescripts":{},"mn":"2"}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0.82"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"0.15"}}]}}],"mo":"="}],"mo":"="}}}},{"mtd":{"mrow":{"mmultiscripts":{"mover":{"mi":"x","mo":"^"},"mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"3"},"none":[{},{}],"mprescripts":{},"mn":"0"},"mo":["=","\u2062"],"mi":{},"mrow":{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}}]}},"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"1"},"mmultiscripts":{"mover":{"mi":"x","mo":"^"},"mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"3"},"none":[{},{}],"mprescripts":{},"mn":"1"}},{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0"}},{"mtd":{"mn":"0.78"}}]}},"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"2"},"mmultiscripts":{"mover":{"mi":"x","mo":"^"},"mrow":{"mi":"\u2022","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"3"},"none":[{},{}],"mprescripts":{},"mn":"2"}},{"mrow":[{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"0.36"}},{"mtd":{"mn":"0.52"}}]}},"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"3"},"mmultiscripts":{"mover":{"mi":"x","mo":"^"},"mi":"\u20223","none":[{},{}],"mprescripts":{},"mn":"3"}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0.30"}},{"mtd":{"mn":"0.30"}},{"mtd":{"mn":"0.30"}}]}}],"mo":"="}],"mo":"="}],"mo":"="}}}}]}},{"mrow":{"mo":["(",")"],"mn":"50"}}]}}}},"br":{}},"However, if the progression of passive sets by iteration is examined,",{"@attributes":{"id":"p-0099","num":"0100"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mo":"\u2009","mn":"0"},"mo":"\u2062"},{"mrow":[{"mrow":[{"mo":["{","}"],"mrow":{"mi":["\u2205","\u2205","\u2205"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}},{"msup":{"mo":"\u2009","mn":"1"},"mo":"\u2062"}],"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"1"}},{"mrow":[{"mrow":[{"mo":["{","}"],"mrow":{"mrow":[{"mo":["{","}"],"mn":"3"},{"mo":["{","}"],"mn":"3"},{"mo":["{","}"],"mn":"3"}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}},{"msup":{"mo":"\u2009","mn":"2"},"mo":"\u2062"}],"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"2"}},{"mrow":[{"mrow":[{"mo":["{","}"],"mrow":{"mrow":[{"mo":["{","}"],"mrow":{"mn":["2","3"],"mo":","}},{"mo":["{","}"],"mrow":{"mn":["1","3"],"mo":","}},{"mo":["{","}"],"mrow":{"mn":["2","3"],"mo":","}}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}},{"msup":{"mo":"\u2009","mn":"3"},"mo":"\u2062"}],"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"3"}},{"mo":["{","}"],"mrow":{"mrow":[{"mo":["{","}"],"mrow":{"mn":["2","3"],"mo":","}},{"mo":["{","}"],"mrow":{"mn":["1","3"],"mo":","}},{"mo":["{","}"],"mrow":{"mn":["1","2","3"],"mo":[",",","]}}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}}],"mo":"="}],"mo":"="}],"mo":"="}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"51"}}]}}}},"br":{},"img":{"@attributes":{"id":"CUSTOM-CHARACTER-00127","he":"3.13mm","wi":"3.13mm","file":"US07451173-20081111-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},"sup":["th ","st "]},{"@attributes":{"id":"p-0100","num":"0101"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mo":"\u2009","mn":"0"},"mo":"\u2062","mover":{"mi":"x","mo":"^"}},{"mrow":[{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"}]}]}},{"msup":{"mo":"\u2009","mn":"1"},"mo":"\u2062","mover":{"mi":"x","mo":"^"}}],"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"1"}},{"mrow":[{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0.81"},{"mn":"0.87"},{"mn":"0.78"}]}]}},{"msup":{"mo":"\u2009","mn":"2"},"mo":"\u2062","mover":{"mi":"x","mo":"^"}}],"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"2"}},{"mrow":[{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mn":"0.82"},{"mn":"0"}]},{"mtd":[{"mn":"0.63"},{"mn":"0"},{"mn":"0.36"}]},{"mtd":[{"mn":"0.35"},{"mn":"0.15"},{"mn":"0.52"}]}]}},{"msup":{"mo":"\u2009","mn":"3"},"mo":"\u2062","mover":{"mi":"x","mo":"^"}}],"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"3"}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mn":"0.82"},{"mn":"0.30"}]},{"mtd":[{"mn":"0.63"},{"mn":"0"},{"mn":"0.30"}]},{"mtd":[{"mn":"0.35"},{"mn":"0.15"},{"mn":"0.30"}]}]}}],"mo":"="}],"mo":["=","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"9.7em","height":"9.7ex"}}}]}],"mo":"="}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"52"}}]}}}},"br":{}},"Therefore, the fast combinatorial algorithm of the present invention reorganizes the calculation in two ways. First, problems that share a common passive set are grouped and solved them together; and second, recognizing that the passive sets vary from iteration to iteration, each NNLS iteration is performed for all columns in parallel rather than performing all iterations for each column in series. Of course, as in this example, not all columns will require the same number of iterations to achieve optimality. It is a simple matter, however, to remove a given column from further consideration once it's optimal solution has been obtained.","In the example above, zero vectors were used to initialize the NNLS algorithm. As noted above, a better choice is to initially assume that all variables are passively constrained and then overwrite all negative values with zero. To the extent that the non-rigorous, overwriting method for approximating NNLS provides a reasonable estimate of the true active and passive sets, it provides a good initial feasible solution for rigorous NNLS. Given the foregoing example together with its overwriting solution, the rigorous NNLS solution can be obtained in a single iteration with a total of three pseudoinverse calculations.",{"@attributes":{"id":"p-0103","num":"0104"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mo":"\u2009","mn":"0"},"mo":"\u2062","mover":{"mi":"x","mo":"^"}},{"mrow":[{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mn":"0.92"},{"mn":"0.30"}]},{"mtd":[{"mn":"0.71"},{"mn":"0"},{"mn":"0.30"}]},{"mtd":[{"mn":"0.69"},{"mn":"0.14"},{"mn":"0.30"}]}]}},{"msup":{"mo":"\u2009","mn":"1"},"mo":"\u2062","mover":{"mi":"x","mo":"^"}}],"mo":["\u2062","\u2062"],"mover":{"mo":"\u2192","mn":"1"}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mn":"0.82"},{"mn":"0.30"}]},{"mtd":[{"mn":"0.63"},{"mn":"0"},{"mn":"0.30"}]},{"mtd":[{"mn":"0.35"},{"mn":"0.15"},{"mn":"0.30"}]}]}}],"mo":"="}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"53"}}]}}}},"br":{},"sup":"0"},"The final example shows a modest decrease in the number of pseudoinverses that must be computed, about a factor of two, as compared to the one-column-at-a-time serial approach. The decrease can become more dramatic as the number of RHS becomes much larger. For any p-pure component system, there exist 2possible passive sets. For the three-component example above, there are eight possible  and they are: {1, 2, 3}, {1, 2}, {1, 3}, {2, 3}, {1}, {2}, {3}, and \u00d8. If one is solving a problem where the number of RHS is greater than 2, one can necessarily save computational effort by grouping the columns that have identical passive sets and computing one pseudoinverse per passive set. In fact, if one initializes the NNLS algorithm with the overwriting solution then the number of possible passive sets that need to be considered decreases by one, since the all-positive columns of X are already solved in the NNLS sense. Even if 2\u22121 is a number that is large compared to the number of RHS, the number of unique passive sets is usually smaller than the number of RHS if the constrained linear model is truly appropriate for the data. Thus, the fast combinatorial algorithm can be used to great advantage to solve these problems.","The fast combinatorial algorithm of the present invention can reduce the computational burden for rigorous NNLS solutions for problems with many more RHS than factors. Thus, the performance of the fast combinatorial algorithm was compared against the previous fast algorithms of Bro. To do so, both Bro's fast algorithm (FNNLS) and his alternate, advanced-user routine (FNNLSb) were downloaded from the Internet. See, \u201cNon-negativity constrained least squares regression,\u201d (retrieved on 2004-04-24 from URL:http:\/\/www.models.kvl.dk\/source\/). The solution in Bro's standard FNNLS algorithm is initialized with all zeros. The FNNLSb algorithm, on the other hand, is initialized with estimates of the active and passive sets. In the comparisons, the known true passive and active sets were used to initialize FNNLSb. Times for the overwriting method were obtained by terminating the new fast algorithm after the initialization step. The overwriting method, while it doesn't provide the true least squares solution, should provide the fastest benchmark against which to measure the new algorithm. All programs were coded in Matlab\u00ae Version 7 (MATLAB, Ver. 7, The MathWorks, Inc., Natrick, Mass. 2004) and executed on a 1 GHz Pentium III computer containing 1 GByte of main memory. In all cases, least squares solutions were obtained by the method of normal equations. Cross-product matrices were precomputed for the rigorous NNLS algorithms and the times to do so are included in the totals, since frequently this was the most time-consuming step in the entire calculation. An exemplary Matlab Version 7 code implementation of the complete FC-NNLS algorithm is shown below:",{"@attributes":{"id":"p-0106","num":"0107"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"function [X, P] = fcnnls(A, B)"},{"entry":"% NNLS using normal equations and fast combinatorial strategy"},{"entry":"%"},{"entry":"% I\/O: [X, P] = fcnnls(A, B);"},{"entry":"% \u2003X = fcnnls(A, B);"},{"entry":"%"},{"entry":"% \u2003A is the mObs \u00d7 pVar coefficient matrix"},{"entry":"% \u2003B is the mObs \u00d7 nRHS matrix of observations"},{"entry":"% \u2003X is the pVar \u00d7 nRHS solution matrix"},{"entry":"% \u2003P is the pVar \u00d7 nRHS passive set logical array"},{"entry":"%"},{"entry":"% M. H. Van Benthem and M. R. Keenan"},{"entry":"% Sandia National Laboratories"},{"entry":"% P: set of passive sets, one for each column"},{"entry":"% C: set of column indices for solutions that have not yet converged"},{"entry":"% I: set of column indices for which the current solution is infeasible"},{"entry":"% J: working set of column indices for which current solution is"},{"entry":"optimal"},{"entry":"% Check the input arguments for consistency *********************"},{"entry":"error(nargchk(2,2,nargin))"},{"entry":"[mObs, pVar] = size(A);"},{"entry":"if size(B,1) ~= mObs"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"error(\u2032Incompatible sizes of A and B\u2032)"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"end"},{"entry":"nRHS = size(B,2);"},{"entry":"% Precompute parts of pseudoinverse"},{"entry":"AtA = A\u2032*A; "},{"entry":"AtB = A\u2032*B"},{"entry":"opts.SYM=true;"},{"entry":"opts.POSDEF=true;"},{"entry":"% Obtain initial feasible solution and corresponding passive set"},{"entry":"X = linsolve(AtA, AtB, opts);"},{"entry":"P = X > 0;"},{"entry":"X(~P) = 0;"},{"entry":"C = find(~all(P));"},{"entry":"maxiter = 3*pVar;"},{"entry":"iter = 0;"},{"entry":"W = zeros(pVar, nRHS);"},{"entry":"% Active set algorithm for NNLS main loop **********************"},{"entry":"while ~isempty(C)"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"% Solve for the passive variables (uses subroutine below)"]},{"entry":[{},"X(:,C) = cssls(AtA, AtB(:,C), P(:,C));"]},{"entry":[{},"% Find any infeasible solutions"]},{"entry":[{},"I = C(find(any(X(:, C) < 0)));"]},{"entry":[{},"% Make infeasible solutions feasible"]},{"entry":[{},"while ~isempty(I) && (iter < maxiter)"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"iter = iter + 1;"]},{"entry":[{},"[i,j] = find(P(:,I) & (X(:,I) < 0));"]},{"entry":[{},"linearIdx = sub2ind(size(X), i, I(j)\u2032);"]},{"entry":[{},"P(linearIdx) = 0;"]},{"entry":[{},"X(:, I)= cssls(AtA, AtB(:,I), P(:,I));"]},{"entry":[{},"I = I(find(any(X(:,I) < 0)));"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"end"]},{"entry":[{},"if iter == maxiter"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"error(\u2032Maximum number of inner loop iterations exceeded\u2032);"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"end"]},{"entry":[{},"% Check solutions for optimality"]},{"entry":[{},"W(:,C) = AtB(:,C)\u2212AtA*X(:,C);"]},{"entry":[{},"J = find(all(~P(:,C).*W(:,C) <= 0));"]},{"entry":[{},"C = setdiff(C, C(J));"]},{"entry":[{},"% For non-optimal solutions, add the appropriate variable to P"]},{"entry":[{},"if ~isempty(C)"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[mx, mxidx] = max(~P(:,C).*W(:,C));"]},{"entry":[{},"P(sub2ind([pVar nRHS],mxidx, C)) = 1;"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"end"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"end"},{"entry":"% *********************** Subroutines **********************"},{"entry":"function [X] = cssls(AtA, AtB, P)"},{"entry":"% Solve the set of equations AtB = AtA*X for the passive variables in"},{"entry":"set P"},{"entry":"% using the fast combinatorial approach"},{"entry":"X = zeros(size(AtB));"},{"entry":"opts.SYM=true;"},{"entry":"opts.POSDEF=true;"},{"entry":"if (nargin == 2) | isempty(P) | all(P(:))"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"X = linsolve(AtA, AtB, opts);"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"else"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"[pVar nRHS] = size(P);"]},{"entry":[{},"codedPset = 2.{circumflex over (\u2009)}(pVar\u22121:\u22121:0)*P;"]},{"entry":[{},"[sortedPset, sortedEset] = sort(codedPset);"]},{"entry":[{},"breaks = diff(sortedPset);"]},{"entry":[{},"breakIdx = [0 find(breaks) nRHS];"]},{"entry":[{},"for k = 1:length(breakIdx)\u22121"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"cols2solve = sortedEset(breakIdx(k)+1:breakIdx(k+1));"]},{"entry":[{},"vars = P(: ,sortedEset(breakIdx(k)+1));"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"X(vars,cols2solve)=linsolve(AtA(vars,vars),AtB(vars,cols2solve),opts);"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"end"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"end"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"In  is shown a table that summarizes the performance of each of these algorithms and the new FC-NNLS algorithm on 15 data sets. Most of the data were acquired as Energy Dispersive X-ray (EDS) spectral images. Details regarding several of these data sets are presented in the Kotula reference and that reference provides a general description applicable to all of the EDS data. One data set (data set ) is a visible fluorescence image that has also been described previously. See Keenan et al., \u201cAlgorithms for constrained linear unmixing with application to the hyperspectral analysis of fluorophore mixtures,\u201d , Vol. 4816, S. Shen, ed., 193 (2002). Data sets ,  and  comprise the same actual data; they were analyzed for three different rank estimates, however. In all cases, the coefficient matrices A were obtained as the spectral pure components from previous MCR-ALS analyses, and these were used to estimate the pure-component concentrations at each pixel in the respective spectral images.","The table contains specific information about the data including dimensions (m and n) and estimated rank (p). Also contained in the table are some metrics related to the partitioning of the variables between the active and passive sets. In the final, optimal solutions, the variables that were actively constrained comprised 5% to 20% of the total. On average, more than 50% of the RHS columns contain at least one actively constrained variable. The two most interesting observations, however, are first, that the number of unique passive sets is only a small fraction of the total number of actively constrained RHS vectors and, second, as the number of factors increases, the number of unique passive sets is only a small fraction of total possible (2). It is these two characteristics of real data that make possible the performance improvements obtained using the new FC-NNLS algorithm.","As illustrated in , the FC-NNLS algorithm held a performance advantage in all cases examined. The ratios of the FNNLS solution times to those obtained by the FC-NNLS algorithm ranged from 15 to 126. Analogous ratios for the advanced-user algorithm FNNLSb range from 3 to 27. It should be noted that the latter comparisons are best-case for the performance of FNNLSb. Since the true active and passive sets were used to initialize the algorithm, no solutions were ever found to be infeasible (i.e., the inner loop was never entered), and computed solutions were always optimal. This would not be the case when only approximate active and passive sets are supplied, so the relative performance of FNNLSb in a real-world application would be worse. Perhaps the most telling performance comparison is with the overwriting method. For the smaller data sets, there is essentially no significant difference in the computation times between the approximate overwriting method and the rigorous FC-NNLS algorithm. Predictably, as the number of RHS becomes much larger, the advantages of the combinatorial strategy become more impressive.","In  is shown a graphical presentation of the timing data presented in . In general, computation times increased linearly with problem size, defined here to be the product of the number of factors and the number of RHS. Notably, the computation times obtained using FC-NNLS scale very slowly with problem size, and, in fact, are only marginally slower than the overwriting method for all but the very largest problems (data sets  and ).","In the description of the FC-NNLS algorithm, it was shown that obtaining the solution to the inequality-constrained least squares problem could be reduced to solving a sequence of equality-constrained problems. Clearly, additional equality constraints, such as variable equality, can be accommodated simultaneously. Recall that the equality constraint matrices were represented by a p\u00d7n logical matrix V. The active-set variables at each iteration of the inequality-constrained problem can be represented in a similar manner. Using subscripts A and E to mean active constraints and equality constraints, respectively, the logical matrices can be combined by a boolean or operation:\n\nV=Vv \u2003\u2003(54)\n\nVcan then be used with the combinatorial approach to group like subproblems together and to impose all of the constraints at once.\n","In all of the algorithms presented herein, the normal equations for the fully unconstrained problem were precomputed. This approach was introduced by Bro for the NNLS problem. Bro showed that during the normal equation solution process, the structural parts for problems comprising a subset of variables could be easily derived from the structural part of the full data matrix, namely:\n\n=()\n\n=()\u2003\u2003(55)\n\nThus, formation of the cross-product matrices AA and AB could be moved outside of the loop. Bro did not take the next step of moving the factorization outside of the loop, however, since\n\n()\u2260(())\u2003\u2003(56)\n\nThus, the application of Bro's algorithm to the multiple RHS problem still requires at least n matrix factorizations per iteration. Note, also, that since there may be multiple solutions that need to be computed for each individual RHS, Bro's approach is effective for improving performance even in the single RHS case.\n","At this point, there are several things that can be done to further improve computational performance. First, the factorization can, in fact, be moved outside of the loop since efficient updating procedures are available such that:\n\n()\u2192downdate\u2192()\u2003\u2003(57)\n\nPerformance improvements have been demonstrated using downdating of the QR factorization and, since the Cholesky factor of AA is simply the R matrix from the QR decomposition of A, there is an analogous updating scheme for Cholesky factorization as well. This approach can effectively improve performance in the single RHS case. Second, the combinatorial approach can be used with factorization updating and, finally, the sequence of updating operations can be ordered in an optimal way. Two approaches may be used. Removing the kcolumn from a p-column Cholesky factor requires p-k Givens rotations. Thus, removing the pcolumn requires no work. It is often the case, in practice, that one or two variables will be actively constrained much more frequently than the others. Permuting the order of the variables so the column indices of the more frequently constrained variables are closer to p can reduce the computational burden.\n","Finally, the combinatorial approach reduces the worst case number of factorizations by the ratio of n:2. For relatively large p, however, this can still be a large number of factorizations. Data set  in , for instance, had p=15 with over 1000 unique combinations of passive variables. In cases such as this, additional advantage can be gained by considering the order in which the passive variable groups are solved. That is, by carefully considering the ordering of the groups, it should be possible to move from the solution of one passive variable combination to that of a nearby combination through updating the factorization rather than by a full factorization. To put this in more concrete terms, consider a 3 variable system in which all 2=8 combinations are present. The lexical sorting using binary code gives:",{"@attributes":{"id":"p-0115","num":"0116"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"133pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Decimal","Binary"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"0","000"]},{"entry":[{},"1","001"]},{"entry":[{},"2","010"]},{"entry":[{},"3","011"]},{"entry":[{},"4","100"]},{"entry":[{},"5","101"]},{"entry":[{},"6","110"]},{"entry":[{},"7","111"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}},"br":{}},{"@attributes":{"id":"p-0116","num":"0117"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"133pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Decimal","Gray Code"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"0","000"]},{"entry":[{},"1","001"]},{"entry":[{},"2","011"]},{"entry":[{},"3","010"]},{"entry":[{},"4","110"]},{"entry":[{},"5","111"]},{"entry":[{},"6","101"]},{"entry":[{},"7","100"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}},"br":{}},"Of course the gray code scheme is neither unique, nor necessarily the best for real world problems. The basic idea of sorting the problems by similarity can be implemented in a variety of ways. In practice, it is generally found that actual passive set matrices are relatively dense in the sense that they contain mostly ones. In that case, a monotone gray code may be more efficient. Tree-based algorithms can also be constructed in an efficient manner such that when traversed, successive least squares problems are similar.","The fast combinatorial algorithm of the present invention can significantly reduce the computational burden when solving general equality and inequality constrained least squares problems with large numbers of observation vectors. The combinatorial algorithm provides a mathematically rigorous solution and operates at great speed by reorganizing the calculations to take advantage of the combinatorial nature of the problems to be solved. Using large data sets that are typical of spectral imaging applications and the particular case of non-negativity-constrained least squares, order-of-magnitude speed improvements have been obtained as compared to other recent fast NNLS algorithms. While Algorithm (38), FC-NNLS, was derived by considering Problem NNLS and the solution of unconstrained problems by normal equations, the framework expressed in the algorithm can be used, with very little modification, to solve much more general problems. By leaving the actual solution algorithm unspecified in Algorithm (38), other orthogonal factorization methods can be substituted for the normal equations (after performing the appropriate factorizations up front rather than forming the cross product matrices). The method of weighting can also be used to impose the active constraints, which further enables the possibility imposing non-negativity as a preference. For example, the weighting parameter can be adjusted to impose the constraints as a preference, thus allowing variables indexed by the active set to be slightly infeasible. Factorization updating, etc. can also be used.","The actual algorithms for testing solution feasibility, and for moving variables between the active and passive sets, have also not been fully specified in Algorithm (38). This enables the framework to easily handle other types of constraints with little modification. For instance, implementation of a solution to Problem BVLS (bounded variable least squares), where both upper and lower bounds are present, is a straightforward extension. Only the test for feasibility, and the strategy for moving variables between the active and passive sets needs to be changed to handle this problem. No changes are required to the parts of the algorithm concerned with solving for the passive variables. Likewise, combined non-negativity and variable equality constraints can be imposed in a natural way. These represent two different types of active constraints, but again, there is no change to passive variables solution procedure. The same combinatorial approach is not limited to NNLS, but can be used to great advantage for more general equality and inequality constrained least squares problems.","The present invention has been described as a fast combinatorial algorithm for the solution of linearly constrained least squares problems. It will be understood that the above description is merely illustrative of the applications of the principles of the present invention, the scope of which is to be determined by the claims viewed in light of the specification. Other variants and modifications of the invention will be apparent to those of skill in the art."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The accompanying drawings, which are incorporated in and form part of the specification, illustrate the present invention and, together with the description, describe the invention. In the drawings, like elements are referred to by like numbers.",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 4","FIG. 3"]}]},"DETDESC":[{},{}]}
