---
title: User controlled real object disappearance in a mixed reality display
abstract: The technology causes disappearance of a real object in a field of view of a see-through, mixed reality display device system based on user disappearance criteria. Image data is tracked to the real object in the field of view of the see-through display for implementing an alteration technique on the real object causing its disappearance from the display. A real object may satisfy user disappearance criteria by being associated with subject matter that the user does not wish to see or by not satisfying relevance criteria for a current subject matter of interest to the user. In some embodiments, based on a 3D model of a location of the display device system, an alteration technique may be selected for a real object based on a visibility level associated with the position within the location. Image data for alteration may be prefetched based on a location of the display device system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09255813&OS=09255813&RS=09255813
owner: MICROSOFT TECHNOLOGY LICENSING, LLC
number: 09255813
owner_city: Redmond
owner_country: US
publication_date: 20111014
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Mixed reality, also referred to as augmented reality, is a technology that allows virtual imagery to be mixed with a real world view. A feature of a see-through, mixed or augmented reality display device unlike other display devices is that the images displayed do not monopolize the user's view. When a user looks at a computer screen of a laptop, desktop computer or smartphone, software executing on the processor generates what is viewed on one hundred percent of the computer screen. The user's view is diverted from the real world when looking at the computer screen. With a see-through, mixed reality display device, the user can see through the display and interact with the real world while also seeing images generated by one or more software applications. One may say there is shared control of the display by the executing software and the people and things the user sees, which are not under computer control.","In embodiments described below, a see-through, head mounted mixed reality display device system causes a real world object to disappear from a field of view of the device due to the real world object's relation to a particular user associated subject matter. The subject matter may be identified as that to be avoided in some examples. A real world object may also disappear due to its degree of relevance to a current subject matter of interest to a user.","The technology provides an embodiment of one or more processor readable storage devices having instructions encoded thereon for causing one or more processors to execute a method for causing disappearance of a real object in a see-through display of a see-through, mixed reality display device system. The method comprises receiving metadata identifying one or more real objects in a field of view of the see-through display. For example, the one or more real objects may be identified based on image data captured by one or more physical environment facing cameras attached to the see-through, mixed reality display device system. It is determined whether any of the one or more real objects satisfies user disappearance criteria. Responsive to determining a real object satisfies the user disappearance criteria, the image data is tracked to the real object in the see-through display for causing the disappearance of the real object in the field of view of the see-through display. The content of the image data is based upon an alteration technique assigned to the real object.","The technology provides an embodiment of a see-through, head mounted, mixed reality display device system for causing disappearance of a real object in a field of view of see-through display of the display device system. The system comprises one or more location detection sensors and a memory for storing user disappearance criteria including at least one subject matter item. One or more processors have access to the memory and are communicatively coupled to the one or more location detection sensors for receiving location identifier data for the display device system. The one or more processors identify one or more real objects in the field of view of the see-through display which are related to the at least one subject matter item and within a predetermined visibility distance for a location determined from the location identifier data. At least one image generation unit is communicatively coupled to the one or more processors and optically coupled to the see-through display for tracking image data to the identified one or more real objects in the field of view of the see-through display for causing disappearance of the one or more real objects.","The technology provides an embodiment of a method for causing disappearance of a real object in a field of view of a see-through display of a see-through, mixed reality display device system. The method comprises receiving user input of a physical action identifying subject matter for disappearance including a real object for disappearance which is currently in the field of view of the see-through display. The subject matter for disappearance is stored in user disappearance criteria. Image data is tracked to the real object for disappearance in accordance with an alteration technique. Responsive to identifying a user designated alteration technique to be applied to the real object for disappearance currently in the field of view, selecting the user designated alteration technique as the alteration technique.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.","The technology provides various embodiments for causing disappearance of a real object in a see-through display of a mixed reality display device system. The term object, as in other image processing applications, may refer to a person or thing. For example, edge detection may be applied to a person or a thing in image data as part of a pattern recognition process for identifying what is in the image. As noted above, with a see-through, mixed reality display device, the user can literally see through the display and interact with the real world while also seeing images generated by one or more software applications.","A software application executing on the display device system identifies user disappearance criteria. In some examples, the user disappearance criteria may be based on user input which specifically identifies subject matter items for which a user does not want to see related objects. In some examples, subject matter may be a category or general topic which may be embodied across different types of objects. For example, identified subject matter may be a type of people such as clowns. In another example, the subject matter may be a type of tree. In other examples, a subject matter item may be referring to a specific object or group of objects. For instance, the subject matter may be a specific person, such as one's nine year old brother, or a specific thing, such as a cell tower in a specific location which has ruined a view of scenery. In another example, the subject matter may be a specific person, place, or thing to which different types of objects relate. An example of such a specific thing is a specific restaurant chain. Some examples of related objects are a person when dressed in an employee uniform of the chain, a billboard advertising the chain, a roadside sign for the chain, or a building housing one of the chain restaurants.","In some embodiments, user disappearance criteria may be determined based on a current subject matter of interest to a user. In order to emphasize information for the current subject matter of interest, objects not satisfying a relevance criteria for the current subject matter of interest are made to disappear from the display.","As described further below, image data is positioned in the see-through display over at least a part of a real object which is related to a subject matter item in a field of view of the see-through display. As either the user's head, body or eye gaze may change, or a position of the real object may change or both may change, the overlaid image data tracks the position of the disappearing real object with respect to the field of view of the see-through display.","To make an object disappear, different alteration techniques may be employed. Some techniques can be a simple redaction effect of covering the real object with black image data or obscuring the real object by tracking blurry image data to the object. Other techniques may involve replacing the real object with a virtual different type of object in the display. For example, if a clown is to be blocked, an avatar which tracks the movement of the real life clown may be displayed in the see-through display. In other examples, an erasure technique may be employed. One example of an implementation technique for an erasure technique is displaying image data of what is behind the real object to cover the real object in the see-through display. In another implementation example for an erasure technique, image data is generated by replicating image data of objects which surround the real object. The image data is displayed to cover the real object, effectively blending the real object out of the field of view of the display. In some embodiments, a position of a real object satisfying user disappearance criteria within a predetermined visibility distance of the see-through display is a basis for selecting an alteration technique for the real object.","Particularly, in the use of erasure techniques, some embodiments provide a collision avoidance safety feature by tracking a position or trajectory of the see-through, mixed reality display device and a relative position or trajectory of the \u201cerased\u201d real object and outputs a safety warning if the \u201cerased\u201d real object and the display device fall within a collision distance.",{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 1A","b":["8","2","4","6","2","4","4","4","2","4","50","12","4","2"]},"Head mounted display device , which in this embodiment is in the shape of eyeglasses in a frame , is worn on the head of a user so that the user can see through a display, embodied in this example as a display optical system  for each eye, and thereby have an actual direct view of the space in front of the user. The use of the term \u201cactual direct view\u201d refers to the ability to see real world objects directly with the human eye, rather than seeing created image representations of the objects. For example, looking through glass at a room allows a user to have an actual direct view of the room, while viewing a video of a room on a television is not an actual direct view of the room. Based on the context of executing software, for example, a gaming application, the system can project images of virtual objects, sometimes referred to as virtual images, on the display that are viewable by the person wearing the see-through display device while that person is also viewing real world objects through the display. Therefore, each display optical system  is a see-through display for its respective eye, and the two display optical systems  together may also be referred to as a see-through display.","Frame  provides a support for holding elements of the system in place as well as a conduit for electrical connections. In this embodiment, frame  provides a convenient eyeglass frame as support for the elements of the system discussed further below. In other embodiments, other support structures can be used. An example of such a structure is a visor or goggles. The frame  includes a temple or side arm for resting on each of a user's ears. Temple  is representative of an embodiment of the right temple and includes control circuitry  for the display device . Nose bridge  of the frame includes a microphone  for recording sounds and transmitting audio data to processing unit .","Computing system  may be a computer, a gaming system or console, or a combination of one or more of these. An application may be executing on computing system , or may be executing in the see-through, mixed reality display system .","In this embodiment, computing system  is communicatively coupled to one or more capture devices A and B. In other embodiments, more or less than two capture devices can be used to capture the room or other physical environment of the user. Capture devices A and B may be, for example, cameras that visually monitor one or more users and the surrounding space such that gestures and\/or movements performed by the one or more users, as well as the structure of the surrounding space, may be captured, analyzed, and tracked. A gesture acts as one or more controls or actions within an application and\/or to animate an avatar or on-screen character.","Capture devices A and B may be depth cameras. According to an example embodiment, each capture device A, B may be configured to capture video with depth information including a depth image that may include depth values via any suitable technique including, for example, time-of-flight, structured light, stereo image, or the like. According to one embodiment, the capture device A, B may organize the depth information into \u201cZ layers,\u201d or layers that may be perpendicular to a Z axis extending from the depth camera along its line of sight. The capture device A, B may include an image camera component which may include an IR light component, a three-dimensional (3-D) camera, and an RGB camera that may be used to capture the depth image of a scene. The depth image may include a two-dimensional (2-D) pixel area of the captured scene where each pixel in the 2-D pixel area may represent a length in, for example, centimeters, millimeters, or the like of an object in the captured scene from the camera.","Each capture device, A and B, may also include a microphone (not shown). Computing system  may be connected to an audiovisual device  such as a television, a monitor, a high-definition television (HDTV), or the like that may provide game or application visuals. In some instances, the audiovisual device  may be a three-dimensional display device. In one example, audiovisual device  includes internal speakers. In other embodiments, audiovisual device , a separate stereo or computing system  is connected to external speakers .",{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 1B","b":["8","50","2","5","4","5","6"]},"Furthermore, as in the computing system , applications like a disappearance application (see  below) may execute on a processor of the mobile device  which user actions control and which may display image data by the display optical systems . A display  of the mobile device  may also display data, for example menus, for executing applications and be touch sensitive for accepting user input. The mobile device  also provides a network interface for communicating with other computing devices like computing system  over the Internet  or via another communication network  (e.g. WiFi, Bluetooth, infrared, RFID transmission, WUSB, cellular, 3G, 4G or other wireless communication means) via a wired or wireless communication medium using a wired or wireless communication protocol. A remote network accessible computer system like computing system  may be leveraged for processing power and remote data access by a processing unit  like mobile device . Examples of hardware and software components of a mobile device  such as may be embodied in a smartphone or tablet computing device are described in , and these components can embody the hardware and software components of a processing unit  such as those discussed in the embodiment of . Some other examples of mobile devices  are a smartphone, a laptop, or notebook computer and a netbook computer.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 2A","b":["102","115","115","113","4","5","2","20","20","12","113","113","2"],"i":["a ","b "]},"The data from the camera may be sent to a processor  of the control circuitry , or the processing unit , or both, which may process them but which the unit , may also send to one or more computer systems  over a network  for processing. The processing identifies and maps the user's real world field of view. Additionally, the physical environment facing camera  may also include a light meter for measuring ambient light.","Control circuits  provide various electronics that support the other components of head mounted display device . More details of control circuits  are provided below with respect to . Inside, or mounted to temple , are ear phones , inertial sensors , one or more location sensors , e.g. a GPS transceiver, an infrared (IR) transceiver, optional electrical impulse sensor  for detecting commands via eye movements and temperature sensor . In one embodiment, inertial sensors  include a three axis magnetometer A, three axis gyro B and three axis accelerometer C (See ). The inertial sensors are for sensing position, orientation, and sudden accelerations of head mounted display device . From these movements, head position may also be determined.","Mounted to or inside temple  is an image source or image generation unit . In one embodiment, the image source includes micro display  for projecting images of one or more virtual objects and lens system  for directing images from micro display  into light guide optical element . Lens system  may include one or more lenses. In one embodiment, lens system  includes one or more collimating lenses. In the illustrated example, a reflecting element  of light guide optical element  receives the images directed by the lens system .","There are different image generation technologies that can be used to implement micro display . For example, micro display  can be implemented using a transmissive projection technology where the light source is modulated by optically active material, backlit with white light. Micro display  can also be implemented using a reflective technology for which external light is reflected and modulated by an optically active material. Digital light processing (DLP), liquid crystal on silicon (LCOS) and Mirasol\u00ae display technology from Qualcomm, inc. are all examples of reflective technologies. Additionally, micro display  can be implemented using an emissive technology where light is generated by the display, see for example, a PicoP\u2122 display engine from Microvision, Inc.",{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 2B","b":["14","115","2","14","14","14","2","115"],"i":"r "},"In one embodiment, the display optical system  includes a light guide optical element , opacity filter , see-through lens  and see-through lens . In one embodiment, opacity filter  is behind and aligned with see-through lens , lightguide optical element  is behind and aligned with opacity filter , and see-through lens  is behind and aligned with lightguide optical element . See-through lenses  and  are standard lenses used in eye glasses and can be made to any prescription (including no prescription). In some embodiments, head mounted display device  will include only one see-through lens or no see-through lenses. Opacity filter  filters out natural light (either on a per pixel basis or uniformly) to enhance the contrast of the virtual imagery. Light guide optical element  channels artificial light to the eye. More details of the light guide optical element  and opacity filter  are provided below.","Light guide optical element  transmits light from micro display  to the eye  of the user wearing head mounted display device . Light guide optical element  also allows light from in front of the head mounted display device  to be transmitted through light guide optical element  to eye , as depicted by arrow  representing an optical axis of the display optical system , thereby allowing the user to have an actual direct view of the space in front of head mounted display device  in addition to receiving a virtual image from micro display . Thus, the walls of light guide optical element  are see-through. Light guide optical element  includes a first reflecting surface  (e.g., a mirror or other surface). Light from micro display  passes through lens  and becomes incident on reflecting surface . The reflecting surface  reflects the incident light from the micro display  such that light is trapped inside a planar, substrate comprising light guide optical element  by internal reflection.","After several reflections off the surfaces of the substrate, the trapped light waves reach an array of selectively reflecting surfaces . Note that only one of the five surfaces is labeled  to prevent over-crowding of the drawing. Reflecting surfaces  couple the light waves incident upon those reflecting surfaces out of the substrate into the eye  of the user. In one embodiment, each eye will have its own light guide optical element . When the head mounted display device has two light guide optical elements, each eye can have its own micro display  that can display the same image in both eyes or different images in the two eyes. In another embodiment, there can be one light guide optical element which reflects light into both eyes.","Opacity filter , which is aligned with light guide optical element , selectively blocks natural light, either uniformly or on a per-pixel basis, from passing through light guide optical element . In one embodiment, the opacity filter can be a see-through LCD panel, electro chromic film, or similar device which is capable of serving as an opacity filter. Such a see-through LCD panel can be obtained by removing various layers of substrate, backlight and diffusers from a conventional LCD. The LCD panel can include one or more light-transmissive LCD chips which allow light to pass through the liquid crystal. Such chips are used in LCD projectors, for instance.","Opacity filter  can include a dense grid of pixels, where the light transmissivity of each pixel is individually controllable between minimum and maximum transmissivities. While a transmissivity range of 0-100% is ideal, more limited ranges are also acceptable. In one example, 100% transmissivity represents a perfectly clear lens. An \u201calpha\u201d scale can be defined from 0-100%, where 0% allows no light to pass and 100% allows all light to pass. The value of alpha can be set for each pixel by the opacity filter control unit  described below. A mask of alpha values can be used from a rendering pipeline, after z-buffering with proxies for real-world objects.","When the system renders a scene for the mixed reality display, it takes note of which real-world objects are in front of which virtual objects. In one embodiment, the display and the opacity filter are rendered simultaneously and are calibrated to a user's precise position in space to compensate for angle-offset issues. Eye tracking can be employed to compute the correct image offset at the extremities of the viewing field. If a virtual object is in front of a real-world object, then the opacity is turned on for the coverage area of the virtual object. If the virtual object is (virtually) behind a real-world object, then the opacity is turned off, as well as any color for that pixel, so the user will only see the real-world object for that corresponding area (a pixel or more in size) of real light. Coverage may be on a pixel-by-pixel basis, so the system could handle the case of part of a virtual object being in front of a real-world object, part of the virtual object being behind the real-world object, and part of the virtual object being coincident with the real-world object. The opacity filter assists the image of a virtual object to appear more realistic and represent a full range of colors and intensities. More details of an opacity filter are provided in U.S. patent application Ser. No. 12\/887,426, \u201cOpacity Filter For See-Through Mounted Display,\u201d filed on Sep. 21, 2010, incorporated herein by reference in its entirety.","Head mounted display device  also includes a system for tracking the position of the user's eyes. As will be explained below, the system will track the user's position and orientation so that the system can determine the field of view of the user. However, a human will not perceive everything in front of them. Instead, a user's eyes will be directed at a subset of the environment. Therefore, in one embodiment, the system will include technology for tracking the position of the user's eyes in order to refine the measurement of the field of view of the user. For example, head mounted display device  includes eye tracking assembly  (see ), which will include an eye tracking illumination device A and eye tracking camera B (see ).","In one embodiment, eye tracking illumination source A includes one or more infrared (IR) emitters, which emit IR light toward the eye. Eye tracking camera B includes one or more cameras that sense the reflected IR light. The position of the pupil can be identified by known imaging techniques which detect the reflection of the cornea. For example, see U.S. Pat. No. 7,401,920, entitled \u201cHead Mounted Eye Tracking and Display System\u201d, issued Jul. 22, 2008 to Kranz et al., incorporated herein by reference. Such a technique can locate a position of the center of the eye relative to the tracking camera. Generally, eye tracking involves obtaining an image of the eye and using computer vision techniques to determine the location of the pupil within the eye socket. In one embodiment, it is sufficient to track the location of one eye since the eye usually moves in unison. However, it is possible to track each eye separately. Alternatively, an eye tracking camera may be an alternative form of tracking camera using any motion based image of the eye to detect position, with or without an illumination source.","Another embodiment for tracking eye movements is based on charge tracking. This concept is based on the observation that a retina carries a measurable positive charge and the cornea has a negative charge. Sensors , in some embodiments, are mounted by the user's ears (near earphones ) to detect the electrical potential while the eyes move around and effectively read out what the eyes are doing in real time. (See Control your mobile music with eyeball-activated earphones!, Feb. 19, 2010 [retrieved from the Internet Jul. 12, 2011: http:\/\/www.wirefresh.com\/control-your-mobile-music-with-eyeball-actvated-headphones].) Eye blinks may also be tracked as commands. Other embodiments for tracking eyes movements, such as blinks, which are based on pattern and motion recognition in image data from the eye tracking camera B mounted on the inside of the glasses, can also be used.","In the embodiments above, the specific number of lenses shown are just examples. Other numbers and configurations of lenses operating on the same principles may be used. Additionally,  only show half of the head mounted display device . A full head mounted display device would include another set of see through lenses  and , another opacity filter , another light guide optical element , another micro display , another lens system  physical environment facing camera  (also referred to as outward facing or front facing camera ), eye tracking assembly , earphone , sensor  if present and temperature sensor . Additional details of a head mounted display 2 are illustrated in U.S. patent application Ser. No. 12\/905,952 entitled Fusing Virtual Content Into Real Content, Filed Oct. 15, 2010, fully incorporated herein by reference.",{"@attributes":{"id":"p-0067","num":"0066"},"figref":["FIG. 3A","FIG. 3B","FIG. 3B","FIGS. 1A and 1B"],"b":["2","4","5","2","4","5","4","5","4","5","2","12","50","4","5","136","2"]},"Note that some of the components of  (e.g., outward or physical environment facing camera , eye camera , micro display , opacity filter , eye tracking illumination unit A, earphones , sensors  if present, and temperature sensor  are shown in shadow to indicate that there are at least two of each of those devices, at least one for the left side and at least one for the right side of head mounted display device .  shows the control circuit  in communication with the power management circuit . Control circuit  includes processor , memory controller  in communication with memory  (e.g., D-RAM), camera interface , camera buffer , display driver , display formatter , timing generator , display out interface , and display in interface . In one embodiment, all of components of control circuit  are in communication with each other via dedicated lines of one or more buses. In another embodiment, each of the components of control circuit  are in communication with processor .","Camera interface  provides an interface to the two physical environment facing cameras  and each eye camera  and stores respective images received from the cameras ,  in camera buffer . Display driver  will drive microdisplay . Display formatter  may provide information, about the virtual image being displayed on microdisplay  to one or more processors of one or more computer systems, e.g. , , ,  performing processing for the mixed reality system. The display formatter  can identify to the opacity control unit  transmissivity settings for which pixels of the display optical system . Timing generator  is used to provide timing data for the system. Display out interface  includes a buffer for providing images from physical environment facing cameras  and the eye cameras  to the processing unit , . Display in interface  includes a buffer for receiving images such as a virtual image to be displayed on microdisplay . Display out  and display in  communicate with band interface  which is an interface to processing unit , .","Power management circuit  includes voltage regulator , eye tracking illumination driver , audio DAC and amplifier , microphone preamplifier and audio ADC , temperature sensor interface , electrical impulse controller , and clock generator . Voltage regulator  receives power from processing unit , via band interface  and provides that power to the other components of head mounted display device . Illumination driver  controls, for example via a drive current or voltage, the eye tracking illumination unit A to operate about a predetermined wavelength or within a wavelength range. Audio DAC and amplifier  provides audio data to earphones . Microphone preamplifier and audio ADC  provides an interface for microphone . Temperature sensor interface  is an interface for temperature sensor . Electrical impulse controller  receives data indicating eye movements from the sensor  if implemented by the display device . Power management unit  also provides power and receives data back from three axis magnetometer A, three axis gyro B and three axis accelerometer C. Power management unit  also provides power and receives data back from and sends data to one or more location sensors , which include a GPS transceiver and an IR transceiver in this example.",{"@attributes":{"id":"p-0071","num":"0070"},"figref":["FIG. 3B","FIG. 3B"],"b":["4","5","304","306","304","320","322","324","326","328","330","332","334","336","2","302","232","338","2","302","232","340","342","346","348"]},"In one embodiment, wireless communication component  can include a Wi-Fi enabled communication device, Bluetooth communication device, infrared communication device, cellular, 3G, 4 G communication devices, wireless USB (WUSB) communication device, RFID communication device etc. The wireless communication component  thus allows peer-to-peer data transfers with for example, another display device system , as well as connection to a larger network via a wireless router or cell tower. The USB port can be used to dock the processing unit ,  to another display device system . Additionally, the processing unit , can dock to another computing system  in order to load data or software onto processing unit , , as well as charge processing unit , . In one embodiment, CPU  and GPU  are the main workhorses for determining where, when and how to insert virtual images into the view of the user.","Power management circuit  includes clock generator , analog to digital converter , battery charger , voltage regulator , see-through, near-eye display power source , and temperature sensor interface  in communication with temperature sensor  (located on the wrist band of processing unit ). An alternating current to direct current converter  is connected to a charging jack  for receiving an AC supply and creating a DC supply for the system. Voltage regulator  is in communication with battery  for supplying power to the system. Battery charger  is used to charge battery  (via voltage regulator ) upon receiving power from charging jack . Device power interface  provides power to the display device .","Image data may be applied to a real object identified in a location and which is within a user field of view of a see-through, mixed reality display device system. The location of real objects like people and things in the user's environment are tracked in order to track a virtual object to its intended real object. For image processing purposes, both a person and a thing may be an object, and an object may be a real object, something physically present, or a virtual object in an image displayed by the display device . Software executing on one or more of the hardware components discussed above use the data provided by sensors such as the camera, orientation sensors and the one or more location sensors and network connections to track real and virtual objects in a user's environment.","Typically, virtual objects are displayed in three dimensions so that just as a user interacts with real objects in three dimensions, the user may interact with virtual objects in three dimensions.  discussed next describe embodiments of methods at different levels of detail, exemplary data structures and software components which process image data, user input and user disappearance criteria for causing real objects satisfying user disappearance criteria to disappear from the see-through display.",{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 4","b":["8","456","50","12","456","456","462","459","8","456","8","456","8"],"sub":["1 ","1 "]},"In this embodiment, each of the systems ,  and  are communicatively coupled over one or more networks  to various databases discussed further below such as reference object data sets , image database(s) , and user profile databases .","Some examples of other processor based systems  are other see-through, mixed reality display device systems, other head mounted display systems, servers, mobile devices like mobile phones, smartphones, netbooks, notebooks, and the like and desktop computers. These other processor based systems  communicate with the display device system  to provide data in various multimedia formats like text, audio, image and video data, from one or more of its applications. For example, the data may be video of a friend at a same location at which the user is present, or a social networking page showing messages from others on the user's friend list. As discussed in examples below, such data may also be image disappearance data for a real object the sender does not wish to see. In this way, the recipient user can view a scene altered to obscure the sender's undesired real object from the recipient's field of view.","The display device system  and the other processor based systems  execute a client side version of a push service application which communicates over a communication network  with an information push service application . A user may register an account with the information push service application  which grants the information push service permission to monitor the user's executing applications and data generated and received by them as well as user profile data , and device data for tracking the user's location and device capabilities.","In one embodiment, computing system  includes a user profile database  which may aggregate data from user profile data stored on the different user computer systems ,  of the user. The local copies , of the user profile data may store some of the same user profile data  and may periodically update their local copies with the user profile data  stored by the computer system  in an accessible database over a communication network . User disappearance criteria  may also be stored in user profile data . Subject matter for disappearance data  and a current subject matter of interest data  are both types of user disappearance criteria .",{"@attributes":{"id":"p-0081","num":"0080"},"figref":["FIG. 5A","FIGS. 13A through 13D"],"b":["410","473","412"]},"The disappearance application , associates one or more real object types  with the subject matter identifier  based on the disappearance subject matter the identifier  represents. Similarly, the disappearance application  associates one or more subject matter keywords  with the subject matter identifier  in data record . For example, the disappearance application , may interface with a search application , for identifying synonyms and related words to the subject matter which the search application returns to the disappearance application , . The keywords assist when looking for matches in real object metadata for any real object which satisfies user disappearance criteria. The subject matter identifier  and the keywords  can also identify real object types  related to the subject matter by looking for matches in reference object datasets. User input may identify a real object type  and a subject matter keyword  in some examples too.",{"@attributes":{"id":"p-0083","num":"0082"},"figref":"FIG. 5B","b":["410","473","414","415","417","417","472","474"]},"A user may indicate subject matter a user wishes to disappear, but a user may also indicate a current subject matter of interest. For a current subject matter of interest, one or more real object types of interest for that subject matter are identified. Relevancy criteria is applied to metadata (See ) describing a real object having a type, e.g. road sign, matching at least one of the real object types of interest. A real object of the one or more object types of interest is made to disappear if the real object does not satisfy the relevancy criteria. In some examples, user input may directly identify a current subject matter of interest, and the disappearance application , prompts the user via one or more electronically provided requests to identify one or more real object types for the current subject matter of interest. For example, a user may be interested in Chinese restaurants, and wishes to declutter her view of building signs on a busy street. However, having all real objects, including the street in front of her, disappear may be dangerous, and not helpful in locating a Chinese restaurant.","In some instances, the user has indicated a current subject matter of interest via another executing application , and that application communicates with the disappearance application , for identifying the current subject matter of interest, subject matter keywords, particularly those relevant to the application , and real object types of interest . For example, for a car navigation application example, different types of road signs may be identified as real object types of interest. The executing application selects the alteration indicator  and any applicable replacement object  too.","In some examples, the disappearance application  provides a software interface, for example, an application programming interface (API), which defines data formats for defining real object types , subject matter identifiers , subject matter keywords , and relevancy criteria , e.g. rules for determining relevancy.",{"@attributes":{"id":"p-0087","num":"0086"},"figref":"FIG. 5C","b":["420","473","422","422","414","415","417","416","422","462","428","428","462","428"]},"An example of logic implemented by the disappearance application for the set of default rules is that a real object have the same type as a real object type for current interest, and that the metadata of the real object include at least one of a subject matter keyword  or the subject matter identifier .","As for the example of , the user may select an alteration technique and any applicable replacement object identifier in some instances. In other examples, an executing application , interfacing with the disappearance application , sets the alteration technique indicator  for selecting an alteration technique to be used for a real object type and sets any applicable replacement object .","In some instances, the subject matter item data record ,  may be stored temporarily. An event trigger may prompt an electronically provided request to the user of whether he or she wishes to store the subject matter as disappearance subject matter or subject matter of interest in non-volatile memory for later retrieval. Some examples of event triggers are the object for disappearance is no longer in the field of view, the user has moved out of a location, or data has been received identifying the subject matter as being no longer of interest. A request is electronically provided when it is displayed, played as audio data or a combination of the two. Requests may also be electronically provided for alteration techniques and replacement object preferences. A default alteration technique and any replacement objects for the default technique may be selected if a user does not specify any.","Some other examples of user profile data are the user's expressed preferences, the user's friends' list, the user's preferred activities, a list of the user's reminders, the user's social groups, the user's current location, and other user created content, such as the user's photos, images and recorded videos. In one embodiment, the user-specific information may be obtained from one or more applications and data sources such as the user's social networking sites, address book, schedule data from a calendar application, email data, instant messaging data, user profiles or other sources on the Internet as well as data directly entered by the user.","Each version of the push service application  also stores in user profile data  a tracking history of the user. Some examples of events, people and things tracked in the tracking history are locations visited, transactions, content and real things purchased, and people detected with whom the user has interacted. If electronically identified friends (e.g. social networking friends, contact lists) are registered with the push service application  too, or they make information available through data to the user or publicly through other applications , the push service application  can use this data as well to track a social context for the user. The locations visited are tracked over time, and allow real objects for disappearance and their alteration image data to be stored, prefetched, and ready for download as the user nears a familiar location.","Location identifier data for the mixed reality device system  may be obtained based on one or more detection sensors. In some instances, the computing system  is in communication with detection sensors on the mixed reality device like the one or more transceivers . Data from one or more different types of location detection sensors may be used to obtain location identifier data as discussed in the following examples. Cell tower triangulation may be used based on signals from the processing unit , e.g. when it is embodied as a mobile device , to identify a location of a mixed reality display device system. Global Positioning System (GPS) data may be obtained from a GPS transceiver  of the display device, or as disclosed in , from GPS transceiver  in the processing unit  for identifying a location of a mixed reality device. GPS technology may be used to identify when a user has entered a geofence. A geofence identifier may be used to retrieve images of the area within the geofence, and in some cases a three dimensional model of the area generated from image data.","Locations or spaces of smaller area may also be delineated or fenced by other types of wireless detection sensors such as a wireless universal serial bus (WUSB) transceiver, a Bluetooth transceiver, RFID transceiver or an IR transceiver, e.g. , . Identification data may be exchanged with the computer system  or other computer systems  including other see-through mixed reality display device systems. In other examples, the computing system  communicates with an intermediary detection sensor. An example of such an intermediary detection sensor is a wireless network access point, e.g. WiFi, through which the display system  is communicatively coupled to the computer system . The location of the network access point is stored by the computing system . The physical environment or outward facing cameras  may also be used as location detection sensors, alone or in combination with other sensor data, e.g. GPS coordinate data. The image data may be compared with other images using pattern recognition software to identify a match.","Device data  may include a unique identifier for the computer system , a network address, e.g. an IP address, model number, configuration parameters such as devices installed, the operation system, and what applications are available in the display device system  and are executing in the display system  etc. Particularly for the see-through, mixed reality display device system , the device data may also include data from sensors or determined from the sensors like the orientation sensors , the temperature sensor , the microphone , the electrical impulse sensor  if present, and one or more location detection sensors , , (e.g. a GPS transceiver, an IR transceiver). Image data  is data captured by outward facing cameras  and stored to be analyzed for detecting real objects either locally or remotely by the computing system .","Before discussing applying image data for causing disappearance of a real object in a field of view of the see-through, mixed reality display, a discussion is first presented about components for identifying real objects in a location and a see-through display field of view. Furthermore, positions of virtual objects such as those associated with disappearances are identified and image data generated including them based on the received image data of the real world.","Computing system  may be implemented using one or more computer systems. In this example, the computing system  is communicatively coupled to one or more depth camera(s) A, B in a location for receiving three-dimensional (3D) image data of the location from which real objects and their positions in 3D can be identified.","Image data from outward (from the user's head) facing cameras  can be used to determine the field of view of the see-through display which approximates the field of view of the user. The cameras  are placed at a predetermined offset from the optical axis  of each display optical system . The offset is applied to image data of the cameras  for identifying real objects in the display field of view. Based on the resolution or focal length setting of the cameras , a distance to real objects in the field of view can be determined. In some examples, the outfacing cameras  may be depth cameras as well as video cameras.","As described further below, real objects are identified in image data and their appearance characteristics  are stored, for example in metadata  which are accessible over a communication network  from computer system  or stored locally . Reference object data sets  provide categories of appearance characteristics tied to different types of objects, and these reference object data sets  may be used to recognize objects in image data, and also to select appearance characteristics of virtual objects so they look realistic. A replacement object  identifier may store an identifier of an instantiation of a reference object data set.",{"@attributes":{"id":"p-0100","num":"0099"},"figref":["FIG. 5D","FIGS. 6A through 6E"],"b":["431","432","432","433","432","20","20","2"]},"The position data also includes a visibility level , which is based on the position of the real object from the display device system . The optical axes  of the display optical systems  may be reference points from which the position and trajectory of the real object is determined, and in turn the visibility level. In some embodiments, a visibility level is assigned as a refinement to a predetermined visibility distance for a location. As discussed below, location image tracking software  like Photosynth\u00ae may provide a predetermined visibility distance  for a location detected for the display device system  the user is wearing. For example, a display device being worn at the top of Half Dome at Yosemite may see for one hundred miles. Another display device being worn by a user walking down a busy Seattle business district street during rush hour may have a predetermined visibility distance of about 15 meters or approximately 45 feet.","A number of visibility levels may be defined based on data for the ability of average human sight to distinguish a set of appearance characteristics and movements of one or more body parts. In many embodiments, each visibility level represents at least a distance range between the object and the display device  and a level of recognition. In some examples, the angle of the real object from each optical axis  may be a basis as well as the position of the real object with respect to the display device. The trajectory may also be a basis. (As mentioned below, even if a real object is stationary, a trajectory  may be assigned to it because the display device system  is moving.) Some examples of visibility levels representing distances from farther away to closer are a color recognition level, a joint movement recognition level, and a facial movement recognition level. In some embodiments, real objects outside the predetermined visibility distance are identified with a not visible level.","Location data  for the real object may also be stored if available. This may be GPS data or other location data independent of the field of view of the display device .","Keywords  may be stored for the real object. In some instances, the disappearance application, local or server side, may have identified this real object as satisfying disappearance criteria. For example, the real object is stationary and in a user's work location. The keywords in such an example include the subject matter identifier. Other sources of origin for keywords may be metadata associated with the real object by other users which loaded their images to the location image databases  via the location image tracking application , e.g. Photosynth\u00ae. Furthermore, an application capturing data of a location like a store application  may associate keywords with real objects in its location. Additionally, keywords may be assigned for the real object based on data received by either the local or server side information push service application  from other applications  executed for the user or other users which permit them to be monitored. The disappearance application,  or , may also assign keywords based on user profile data related to the real object.","An appearance characteristic data set  describing the physical features or characteristics of the real object from the identification processing discussed below also is stored in the real object meta data .",{"@attributes":{"id":"p-0106","num":"0105"},"figref":"FIG. 5E","b":["481","481","483","484","485","486","487","488","491","490","474","474","472"]},"In an example of a desk as the type of object, a sub-field of the type of object may be selected as office desk. The size ranges  may range from typical values of 4 to 6 feet in width, 2 to 4 feet in length and 2 to 4 feet in height. Colors available may be brown, silver, black, white, navy blue, beige, or gray. Someone may have a red office desk, so the reference appearance characteristics typically provide commonly found or average parameters. The surfaces  may include a flat surface which the geometric orientation  indicates as horizontal. Vertical surfaces may also be noted from the image data of desk. The surface texture  for the flat surface may be smooth and the patterns available  may indicate wood grain, and vinyl reflective. Types of wood grain patterns may be sub-fields or sub-records to the patterns available  record.",{"@attributes":{"id":"p-0108","num":"0107"},"figref":"FIG. 5F","b":["492","493","494","495","496","497","498","499"]},"As mentioned above, the reference object data sets  also provide input parameters for defining the appearance characteristics of a virtual object of a disappearance. In one embodiment, disappearance display data  may define types of virtual objects and their appearance characteristics for rendering by the microdisplay  of the display device . For example, these reference objects  may be considered templates and parameters for appearance characteristics of virtual objects. For display data , specific data values, e.g. a specific color and size, are selected in an instantiation of a template for generating the actual virtual object to be displayed. For example, a class may be defined for each type of object, and the disappearance application at runtime instantiates a virtual object of the respective class with the parameters for the appearance characteristics of size, type of material, color, pattern, surface texture, shape parameters, and geometric orientations of each surface and the object. The display data  may be implemented in a markup language. For example, Extensible Markup Language (XML) may be used. In another example, a markup language like Virtual Reality Modeling Language (VRML) may be used.","An appearance characteristic data set  for a real object may have fields and subsets defined similarly as for a reference data object set  of the same type but includes actual data values detected or determined based on captured data of the real object. A data value may not be able to be determined for each data field. In some embodiments, the data value assigned is chosen from a selection of available types provided by the reference object data set .","Sound recognition software  may be used to identify nearby users and other real objects in addition to interpreting commands. Facial and pattern recognition software  may also be used to detect and identify users in image data as well as objects in image data. User input software  can receive data identifying physical actions like gestures, particular spoken commands or eye movements for controlling an application. The one or more physical actions may indicate a response or request of a user with respect to a real or virtual object. For example, in , a thumb gesture indicates a real object to be made to disappear. The applications , , and  of the computing system  may also communicate requests and receive data from server side versions of sound recognition software  and facial and pattern recognition software  in identifying users and other objects in a location.","The block diagram of  also represents software components for recognizing physical actions in image data which is discussed further below. Furthermore, the image data plus sensor data available is processed for determining positions of objects, including other users, within a field of view of the see-through, near-eye display device . This embodiment illustrates how the various devices may leverage networked computers to map a three-dimensional model of a user field of view and the surrounding space and the real and virtual objects within the model. An image processing application  executing in a processing unit , communicatively coupled to a display device  can communicate image data  from front facing cameras  over one or more communication networks  to a depth image processing and skeletal tracking application  in a computing system  for processing of image data to determine and track objects, which include both people and things, in three dimensions. In some embodiments, additionally, the image processing application  may perform some processing for mapping and locating objects in a 3D user space locally and may interact with the remote location image tracking application  for receiving distances between objects. Many combinations of shared processing are possible between the applications by leveraging network connectivity.","A depth image processing and skeletal tracking application  detects objects, identifies objects and their locations in the model. The application  may perform its processing based on depth image data from depth cameras like A and B, two-dimensional or depth image data from one or more outward facing cameras , and images obtained from databases . The image databases  may include reference images of objects for use in pattern and facial recognition (e.g. as may be performed by software ). Some of the images in one or more of the databases  may also be accessed via location metadata  associated with objects in the images by a location image tracking application . Some examples of location metadata include GPS metadata, location data for network access points like a WiFi hotspot, location data based on cell tower triangulation, and location data from other types of wireless transceivers.","The location image tracking application  identifies images of the user's location in one or more image database(s)  based on location identifier data received from the processing unit , or other location units (e.g. GPS units) identified as being within a vicinity of the user, or both. Additionally, the image database(s)  may provide images of a location uploaded by users who wish to share their images. The database may be indexed or accessible with location metadata like GPS data, WiFi SSID, cell tower based triangulation data, a WUSB port location, or a position of an infrared transceiver. The location image tracking application  provides distances between objects in an image based on location data to the depth image processing application . In some examples, the location image tracking application  provides a three-dimensional model of a location, and the model may be dynamic based on real-time image data updates provided by cameras in the location. Besides fixed cameras in specified locations, other users's display device systems  and mobile devices can provide such updates. Photosynth\u00ae is an example of such a location image tracking application .","The depth image processing and skeletal tracking application , and the image processing software , may both generate metadata  for real objects identified in image data. For identifying and tracking living things, at least humans anyway, in a field of view or user location, skeletal tracking may be performed.","Outward facing cameras  provide RGB images (or visual images in other formats or color spaces) and depth images in some examples to computing system . If present, capture devices A and B may also send visual images and depth data to computing system  which uses the RGB images and depth images to track a user's or object's movements. For example, the system will track a skeleton of a person using the depth images. There are many methods that can be used to track the skeleton of a person using depth images. One suitable example of tracking a skeleton using depth image is provided in U.S. patent application Ser. No. 12\/603,437, \u201cPose Tracking Pipeline\u201d filed on Oct. 21, 2009, Craig, et al. (hereinafter referred to as the '437 application), incorporated herein by reference in its entirety.","The process of the '437 application includes acquiring a depth image, down sampling the data, removing and\/or smoothing high variance noisy data, identifying and removing the background, and assigning each of the foreground pixels to different parts of the body. Based on those steps, the system will fit a model to the data and create a skeleton. The skeleton will include a set of joints and connections between the joints. Other methods for tracking can also be used. Suitable tracking technologies are also disclosed in the following four U.S. patent applications, all of which are incorporated herein by reference in their entirety: U.S. patent application Ser. No. 12\/475,308, \u201cDevice for Identifying and Tracking Multiple Humans Over Time,\u201d filed on May 29, 2009; U.S. patent application Ser. No. 12\/696,282, \u201cVisual Based Identity Tracking,\u201d filed on Jan. 29, 2010; U.S. patent application Ser. No. 12\/641,788, \u201cMotion Detection Using Depth Images,\u201d filed on Dec. 18, 2009; and U.S. patent application Ser. No. 12\/575,388, \u201cHuman Tracking System,\u201d filed on Oct. 7, 2009.","Skeletal tracking data identifying which joints moved over a period of time and is sent to a gesture recognizer engine  which includes multiple filters  to determine whether a gesture or action was performed by any person or object in the image data. A gesture is a physical action for a user to provide input to the disappearance application. A filter comprises information defining a gesture, action or condition along with parameters, or metadata, for that gesture, or action. For instance, a throw, which comprises motion of one of the hands from behind the rear of the body to past the front of the body, may be implemented as a gesture comprising information representing the movement of one of the hands of the user from behind the rear of the body to past the front of the body, as that movement would be captured by the depth camera. Parameters may then be set for that gesture. Where the gesture is a throw, a parameter may be a threshold velocity that the hand has to reach, a distance the hand travels (either absolute, or relative to the size of the user as a whole), and a confidence rating by the recognizer engine  that the gesture occurred. These parameters for the gesture may vary between applications, between contexts of a single application, or within one context of one application over time.","Inputs to a filter may comprise things such as joint data about a user's joint position, angles formed by the bones that meet at the joint, RGB color data from the scene, and the rate of change of an aspect of the user. Outputs from a filter may comprise things such as the confidence that a given gesture is being made, the speed at which a gesture motion is made, and a time at which a gesture motion is made. In some instances, two-dimensional image data is only available. For example, the front facing cameras  only provide two-dimensional image data. From the device data , the type of front facing camera  can be identified, and the recognizer engine  can plug in two-dimensional filters for its gestures.","More information about recognizer engine  can be found in U.S. patent application Ser. No. 12\/422,661, \u201cGesture Recognizer System Architecture,\u201d filed on Apr. 13, 2009, incorporated herein by reference in its entirety. More information about recognizing gestures can be found in U.S. patent application Ser. No. 12\/391,150, \u201cStandard Gestures,\u201d filed on Feb. 23, 2009; and U.S. patent application Ser. No. 12\/474,655, \u201cGesture Tool\u201d filed on May 29, 2009. both of which are incorporated herein by reference in their entirety.","The image processing software  executing in the display device system  may also have depth image processing capability or a capability for 3D position estimation of an object from stereo images from the outward facing cameras . Additionally, the image processing software  may also include logic for detecting a set of gestures indicating user input. For example, a set of finger or hand gestures may be recognizable. Skeletal tracking may be used but pattern recognition of the finger or hand in image data may also recognize a gesture in the set.","In the discussion below of identifying objects around a user, references to outward facing image data are referring to image data from outward facing cameras . In these embodiments, the field of view of the outward facing cameras  approximates the user field of view as the camera is located at a relatively small offset from the optical axis  of each display optical system , and the offset is taken into account in processing the image data.",{"@attributes":{"id":"p-0123","num":"0122"},"figref":"FIG. 6A","b":["510","136","4","5","12","512","132","132","132","113","476","474","470"]},"In step , the one or more processors executing the facial and pattern recognition software  also identify one or more appearance characteristics of each real object such as type of object, size, surfaces, geometric orientation, shape, color, etc. In step , a three-dimensional (3D) position is determined for each real object in the field of view of the see-through display device. Based on an executing application, the one or more processors in step  identify one or more virtual object 3D positions in the field of view. In other words, where each object is located with respect to the display device , for example with respect to the optical axis  of each display optical system .",{"@attributes":{"id":"p-0125","num":"0124"},"figref":["FIG. 6B","FIG. 17"],"b":["512","520","2","965","5","144","2","8","8"]},"In step , one or more processors, retrieve one or more images of the location from a database (e.g. ), for example via a request to the image tracking software . Local or server based executing versions or both of the facial and pattern recognition software  in step  select one or more images matching image data from the one or more outward facing cameras . In some embodiments, steps  and  may be performed remotely by a more powerful computer, e.g. 12, having access to image databases. Based on location data (e.g. GPS data) in step , the one or more processors determine a relative position of one or more objects in outward facing image data to one or more identified objects in the location, and in step , determines a position of a user from the one or more identified real objects based on the one or more relative positions.","In some embodiments, such as in  with depth cameras A and B capturing depth image data for the living room, a user wearing a see-through, near-eye, mixed reality display may be in a location in which depth image processing software  of a computer system  provides a three-dimensional mapping of objects within a location such as a defined space, e.g. a store.  is a flowchart of an embodiment of a method for generating a three-dimensional model of a location. In step , a computer system with access to depth cameras like system  with capture devices A and B creates a three-dimensional model of a location based on depth images. The depth images may be from multiple perspectives and may be combined based on a common coordinate location, e.g. a store space, and creates a volumetric or three dimensional description of the location. In step , objects are detected in the location. For example, edge detection may be performed on the depth images to distinguish objects, including people, from each other. In step , the computer system  executing depth image processing and skeletal tracking software  and the facial and pattern recognition software  identifies one or more detected objects including their positions in the location, and identifies in step  one or more appearance characteristics of each real object. The objects may be identified with reference images of things and people from user profile data , image databases , and the reference object data sets .","The image processing software  can forward outward facing image data and sensor data to the depth image processing software  and receives back from the computer system  three-dimensional positions and identifications including appearance characteristics. The three-dimensional positions may be with respect to a 3D model coordinate system for the user location. In this way, the disappearance application can determine which real objects are in the field of view and which real objects are not currently in the field of view, but which are in the 3D modeled location.",{"@attributes":{"id":"p-0129","num":"0128"},"figref":["FIG. 6D","FIGS. 6A through 6D"],"b":["540","210","320","322","8","451","450","12","451","542","544","451","546","451","548"]},"For a real object indicated for disappearance by the disappearance application , in the user's see-through display, the image processing application  tracks the position of the real object in the field of view of the display device to a position in each display optical system, and tracks indicated display data , , e.g. a black rectangle for redaction, to cover the real object in each display optical system , and thus in the field of view. The image processing application  of the see-through, mixed reality display device system  will format display data , for causing disappearance of a real object under control of the device side disappearance application to a format which can be processed by the image generation unit , e.g. the microdisplay , and provide instructions to the opacity controller  for the opacity filter , if used. For some alteration techniques like erasure, disappearance display data , comprises image data generated by copying image data surrounding the real object to be erased and covering the erased object with it. In other examples, the disappearance display data , is image data (e.g. from a database ) of what is behind the object to be erased.",{"@attributes":{"id":"p-0131","num":"0130"},"figref":["FIGS. 7 through 12","FIGS. 6A through 6D"]},{"@attributes":{"id":"p-0132","num":"0131"},"figref":"FIG. 7","b":["602","451","451","456","604","456"],"sub":["1","1 "]},"In some examples, the local device disappearance application receives a message from the server side application  identifying which real objects satisfy user disappearance criteria. In yet other applications, the local disappearance application performs keyword searches on the real object meta data received and locally identifies real objects for disappearance.","For any real object identified for disappearance, the disappearance application causes the image processing software  to control the images on the see-through display via the image generation unit  for tracking image data to any identified real object for causing its disappearance in the see-through display. If no real object in the field of view is identified for disappearance, the one or more processors of the display device system  return to other processing in step .",{"@attributes":{"id":"p-0135","num":"0134"},"figref":["FIG. 8","FIG. 8","FIG. 7","FIG. 7"],"b":["604","606","608","612","456","614","606","612"],"sub":"1 "},"Network access to software and image databases which track locations and the real objects therein allow the disappearance application to prefetch any applicable image data for real objects designated for disappearance in such tracked locations, when user entry into such a location satisfies prediction criteria.","In step , the disappearance application checks for identification of any real object satisfying user disappearance criteria, but which is outside the current field of view of the display device but within a predetermined visibility distance for the location of the display device system  of the user. In step , the disappearance application prefetches or causes to be prefetched any applicable disappearance image data for any real object identified in step .","In step , the disappearance application applies or causes to be applied a location prediction method for identifying one or more subsequent locations which satisfy a prediction criteria. In step , the disappearance application determines whether any satisfactory subsequent location has been identified. If not, then in step , processing returns to the field of view checking in step  at a next scheduled check. If a subsequent location satisfying prediction criteria was identified in step , the disappearance application checks for identification of any real object satisfying user disappearance criteria in any identified subsequent location in step , and prefetches, in step , any applicable disappearance image data for any real object identified in step .","As the field of view of the display device includes on what the user's eye are currently focused as well as what the user can peripherally see at the current point of time, the hardware and software components of the display system prioritize keeping the field of view free of real objects satisfying user disappearance criteria, over prefetching of data and location prediction. Thus, in some embodiments, the check (step ) for identifying any real objects for disappearance in the field of view and disappearance processing, if any found (step ), may occur more often and have higher priority for components of the display system  than the prefetching and location prediction steps.","As discussed for , the server side disappearance application  may send a message which the local application checks for identifying real objects satisfying user disappearance criteria in the field of view, within a predetermined visibility distance for a location of the user, or in a subsequent location. In other words, networked resources are leveraged for assistance in identifying real objects for disappearance. The implementation example process of searching metadata for matches to subject matter keywords and identifiers for identifying real objects for disappearance may be performed by the server side application  to offload work from the local device system processors. The local copy may also prefetch by requesting the server side disappearance application  to perform the prefetching, and apply a location prediction method by requesting the server side  to have the location prediction performed and provide the results of any identified subsequent locations.","Additionally, the server side application  may assist with disappearance processing by providing the disappearance image display data  for a particular alteration technique when requested by the local application to save memory space on the display system .","Furthermore, the application copy requesting the prefetching, for example either the local disappearance application or the server side , may prefetch the image data to be stored at another participating computer system in the location of the real object to be made to disappear. In the case of a subsequent location, the requesting application can schedule the other computer system to download the image data at a certain time period before the estimated arrival of the user. The image data for alteration arrives in time for the user's entry into the location. In one example, the local copy application may then download the image data to the display device system  when a connection is made with the other computer system , or in another example, when the user's display device system  is within a distance criteria of a reference point in the location.","As mentioned above, for each location, the location based tracking application  may have assigned a predetermined visibility distance. For a real object for disappearance, a user or application may have selected an alteration technique such as replacing the undesired real object with an avatar overlay which tracks the facial expressions or at least the body movements of the real object, a person in this example. The person to be overlaid with the avatar may be within the predetermined visibility distance, but is more than forty feet away so the user cannot clearly see the facial expressions of the person.","As tracking the facial expressions of a real person with avatar facial expressions is computationally intensive and of little benefit to the user at this distance, another alteration technique or different image data may be applied. For example, image data of the person, or the avatar, with a blurred appearance may be displayed to the user. When the person is within, for example twenty feet, of the user, then the local copy of the disappearance application or the server copy  works with the image processing software , the depth image processing and skeletal tracking application  or both to track the movements of the person, and continuously tracks the person's position in the field of view to a position on the display optical systems  so the image generation unit  tracks image data of the avatar's movements to those of the disappeared person's body. When the person is within ten (10) feet of the user, both facial and body movement tracking of the avatar image data to the person in the see-through display is performed. This example illustrates selection of different alteration techniques for different visibility levels.","Visibility level definitions may be programmed as part of the disappearance application , or stored in accessible memory. Visibility levels may be based on studies of which appearance characteristics and movements are visually recognizable for an average person. Other refinements in visibility levels based on personal characteristics, for example age, may also be incorporated.",{"@attributes":{"id":"p-0146","num":"0145"},"figref":["FIG. 9","FIG. 9"],"b":["606","614","642","642","45","644"],"sub":"61 "},"Appearance characteristics may also be a basis for selecting a visibility level. Some examples of appearance characteristics which may be a basis for determining a visibility level are size and color. A person wearing bright orange at forty feet may have a visibility level indicating more likely to be seen in the user field of view than a person wearing a navy blouse that is 25 feet away.","In step , each real object is prioritized for disappearance based on its identified visibility level in the field of view. Priority increases for a visibility level with closeness to the display device in the field of view. In step , an alteration technique is selected for each real object for disappearance based on a priority of the visibility level for the respective real object. Other bases for selection of an alteration technique may include computation time to implement the alteration technique, memory resources available and the number of real objects to be made to disappear from the current field of view. For example, is a child wearing a see-through mixed reality display device is scared of clowns, and a parade for the local circus is going down the street. Replacing each of five clowns which just came into the field of view of the see-through display with the desired bunny rabbit avatar may not occur quickly enough without exposing the child to at least one clown. A redaction effect may be applied at first, for example a black box displayed over each clown. In step , the selected alteration technique is applied for each real object satisfying disappearance criteria. In the next iteration of the process of , the black box of one or more of the redacted clowns may be replaced with the bunny rabbit avatar overlay.","The prioritizations in the example of  may also be applied to selection of different replacement objects for the disappearance image data for a selected technique. As discussed above, the user may have selected an alteration technique of replacement and has indicated he or she will accept replacement objects generated by other users. Some of the image data associated with these replacement objects may require a lot of memory and have dynamic content so they are more computationally expensive to display. The disappearance application may select from among the available replacement objects based also on the factors of visibility level, implementation time and number of objects to process.","A user may share his alteration image data with another nearby user so the nearby user can experience how the user experiences the real world around them.  is a flowchart of an embodiment of a process for sharing alteration image data between see-through, mixed reality display device systems within a predetermined distance of each other. In step  a first see-through mixed reality display device system identifies a second see-through mixed reality display device system within a predetermined distance. For example, the display device systems may exchange identity tokens via a Bluetooth, WUSB, IR or RFID connection. The type and range of wireless transceiver can be selected to allow connections oly within a predetermined distance. Location data such as GPS or cell triangulation in combination with an application like Bump\u00ae may also be used for identifying devices within a predetermined distance of each other.","In step , the disappearance application of the first device receives an identifier of a real object satisfying disappearance criteria of another user wearing the second mixed reality device, and in step , the first device receives image data from the second device for tracking to the real object for an alteration technique. In step , the disappearance application of the first device displays the image data tracking the real object from the perspective of its field of view of its see-through display.",{"@attributes":{"id":"p-0152","num":"0151"},"figref":"FIG. 11","b":["662","456","5","4","462","420"],"sub":"1 "},"In step , the disappearance application identifies any real object types associated with the current subject matter of interest. When interfacing with an executing application , the application  indicates real object types(s) for the current interest, e.g. via a data item like , to the disappearance application . In the case of a user, the disappearance application may output an audio or visual request for the user to identify real object types of which the user only wishes to see ones relevant to the current interest. The user may enter input identifying such real object types using any of the various input methods discussed above. The disappearance application may also identify real object types based on searches related to the subject matter in online databases and user profile data. Additionally, default real object types may have been stored for common subjects of interests, some examples of which are restaurants and directions.","In step , the disappearance application identifies any real objects in the field of view of the see-through display matching any identified real object type, for example based on matches with an object type in appearance characteristics stored in the metadata for each real object identified as being in the current field of view by the image processing software . In step , the disappearance application determines whether any identified real object does not satisfy relevance criteria for the current subject matter of interest. For example, the disappearance application can apply a keyword search technique to the metadata of any real object identified as having a matching real object type. The search technique returns a relevancy score for each real object. For example, the applied keyword search technique may return a relevancy score based on a Manhattan distance weighted sum for the metadata of the real object. Based on a keyword relevancy score for each real object metadata search, in step , the disappearance application identifies any real object not satisfying relevance criteria for the current subject matter of interest. In step , the disappearance application causes the image generation unit  for example via the image processing software , to track image data to each real object not satisfying relevance criteria for causing its disappearance in the field of view of the see-through display. In the example of the woman looking for the Chinese restaurant in a crowded restaurant district street, removing other building signs declutters her view so she may more quickly find the Chinese restaurant where her friends are waiting.","Of course, although a real object may be made to disappear from the field of view of the see-through display, the real object is still there in the user's environment. To avoid the user walking into people or other objects and being injured, a collision avoidance mechanism may be employed.  is a flowchart of an embodiment of a process for providing a collision warning to a user with respect to a disappeared real object. In step , the disappearance application , determines a position and a trajectory of the mixed reality display device relative to a real object which is disappeared from the see-through display. In step , the disappearance application , determines whether the mixed reality device and the real object are within a collision distance. If the device and the disappeared real object are within a collision distance, then in step , the disappearance application , outputs a safety warning. For example, the disappearance application , displays image data or plays audio data including a safety warning. If the device and the real object are not within a collision distance, processing in step  returns to other tasks such as a task for another application or updating the identification of real objects in the field of view until a next scheduled check.",{"@attributes":{"id":"p-0156","num":"0155"},"figref":["FIGS. 13A","FIG. 13A"],"b":["13","13","13","2","704","704","702","2","702"],"i":["l ","r "]},{"@attributes":{"id":"p-0157","num":"0156"},"figref":["FIG. 13B","FIG. 13C"],"b":["706","702","451","477","113","456","709","451","113","451","456","477","478","110"],"sub":["1 ","1 "]},"Based on the thumb gesture and the audio erase command, the disappearance application sends location data and image data from the cameras  to the location image tracking application  over a network with a request for real-time image data at the location and from the perspective of the see-through display as represented by the image data from cameras  and their predetermined offset from the display optical axes . If the real time image data is available, for example from a display device system  being worn by person , the disappearance application causes the image processing software  to display the image data over the person  for the perspective of the user.  illustrates an example of the see-through display field of view with the obstruction of the person  removed. The edges of the image data may need to be blended with image data of the surrounding space around the person  extracted from the image data from the outward facing cameras . In another example, image data of the surrounding space may be extracted and replicated to generate image data obscuring an object.",{"@attributes":{"id":"p-0159","num":"0158"},"figref":"FIGS. 14A","b":["14","14","14","2","14","712","712","710","704","704"],"i":["l ","r "]},"The user has also selected a replacement object of an avatar which looks like an ordinary person for the location to be overlaid and tracked to any clowns. The disappearance application may select from a number of replacement objects representing ordinary people for the location. Although the clown is within the predetermined visibility distance  for the location, the distance to the clown indicates a visibility level for color detection only in the current location, a busy business district street. The disappearance application causes redaction effect black image data to be applied while the clown is in this visibility level. Avatar data may be prefetched while the trajectory of the clown with respect to the display device is monitored by the disappearance application .  illustrates an example of a redaction alteration technique being applied to the clown. Black image data is tracked to the clown in the see-through display of the device .",{"@attributes":{"id":"p-0161","num":"0160"},"figref":["FIG. 14C","FIG. 14D"],"b":["456","456"],"sub":["1","1"]},{"@attributes":{"id":"p-0162","num":"0161"},"figref":["FIGS. 15A and 15B","FIG. 15A","FIG. 15A"],"b":["462","456","462","456","720","2","732","734","722","728","730","724","726","462","462","2","728"],"sub":["1","1 "]},"Instead of the user having to scan each of the signs to find the correct direction for Route 5 West,  illustrates how the disappearance application can cause the irrelevant signs to disappear. In fact, the irrelevant signs are altered to assist the user in finding the relevant information more quickly. As illustrated in , the road signs for 24 North (), 24 South () and 5 East () are all overlaid in the see-through display with copies of the Route 5 West sign (), all pointing to the left. The user spends less time trying to find the right sign to indicate where to turn. The disappearance application may also receive real object types which the car navigation application  requests never to disappear, even if the user requests it. The stop sign  is an example of such a real object type, and safety is the reason.",{"@attributes":{"id":"p-0164","num":"0163"},"figref":["FIG. 16","FIG. 16","FIGS. 1A and 1B","FIG. 16"],"b":["800","800","801","802","803","806","808","806","801","810","812","808"]},"CPU , memory controller , and various memory devices are interconnected via one or more buses (not shown). The details of the bus that is used in this implementation are not particularly relevant to understanding the subject matter of interest being discussed herein. However, it will be understood that such a bus might include one or more of serial and parallel buses, a memory bus, a peripheral bus, and a processor or local bus, using any of a variety of bus architectures. By way of example, such architectures can include an Industry Standard Architecture (ISA) bus, a Micro Channel Architecture (MCA) bus, an Enhanced ISA (EISA) bus, a Video Electronics Standards Association (VESA) local bus, and a Peripheral Component Interconnects (PCI) bus also known as a Mezzanine bus.","In one implementation, CPU , memory controller , ROM , and RAM  are integrated onto a common module . In this implementation, ROM  is configured as a flash ROM that is connected to memory controller  via a PCI bus and a ROM bus (neither of which are shown). RAM  is configured as multiple Double Data Rate Synchronous Dynamic RAM (DDR SDRAM) modules that are independently controlled by memory controller  via separate buses (not shown). Hard disk drive  and portable media drive  are shown connected to the memory controller  via the PCI bus and an AT Attachment (ATA) bus . However, in other implementations, dedicated data bus structures of different types can also be applied in the alternative.","A graphics processing unit  and a video encoder  form a video processing pipeline for high speed and high resolution (e.g., High Definition) graphics processing. Data are carried from graphics processing unit (GPU)  to video encoder  via a digital video bus (not shown). Lightweight messages generated by the system applications (e.g., pop ups) are displayed by using a GPU  interrupt to schedule code to render popup into an overlay. The amount of memory used for an overlay depends on the overlay area size and the overlay preferably scales with screen resolution. Where a full user interface is used by the concurrent system application, it is preferable to use a resolution independent of application resolution. A scaler may be used to set this resolution such that the need to change frequency and cause a TV resync is eliminated.","An audio processing unit  and an audio codec (coder\/decoder)  form a corresponding audio processing pipeline for multi-channel audio processing of various digital audio formats. Audio data are carried between audio processing unit  and audio codec  via a communication link (not shown). The video and audio processing pipelines output data to an A\/V (audio\/video) port  for transmission to a television or other display. In the illustrated implementation, video and audio processing components - are mounted on module .",{"@attributes":{"id":"p-0169","num":"0168"},"figref":"FIG. 16","b":["814","830","832","830","801","802","804","1","804","4","832"]},"In the implementation depicted in  console  includes a controller support subassembly  for supporting four controllers ()-(). The controller support subassembly  includes any hardware and software components needed to support wired and wireless operation with an external control device, such as for example, a media and game controller. A front panel I\/O subassembly  supports the multiple functionalities of power button , the eject button , as well as any LEDs (light emitting diodes) or other indicators exposed on the outer surface of console . Subassemblies  and  are in communication with module  via one or more cable assemblies . In other implementations, console  can include additional controller subassemblies. The illustrated implementation also shows an optical I\/O interface  that is configured to send and receive signals that can be communicated to module .","MUs () and () are illustrated as being connectable to MU ports \u201cA\u201d () and \u201cB\u201d () respectively. Additional MUs (e.g., MUs ()-()) are illustrated as being connectable to controllers () and (), i.e., two MUs for each controller. Controllers () and () can also be configured to receive MUs (not shown). Each MU  offers additional storage on which games, game parameters, and other data may be stored. In some implementations, the other data can include any of a digital game component, an executable gaming application, an instruction set for expanding a gaming application, and a media file. When inserted into console  or a controller, MU  can be accessed by memory controller . A system power supply module  provides power to the components of gaming system . A fan  cools the circuitry within console . A microcontroller unit  is also provided.","An application  comprising machine instructions is stored on hard disk drive . When console  is powered on, various portions of application  are loaded into RAM , and\/or caches  and , for execution on CPU , wherein application  is one such example. Various applications can be stored on hard disk drive  for execution on CPU .","Gaming and media system  may be operated as a standalone system by simply connecting the system to monitor  (), a television, a video projector, or other display device. In this standalone mode, gaming and media system  enables one or more players to play games, or enjoy digital media, e.g., by watching movies, or listening to music. However, with the integration of broadband connectivity made available through network interface , gaming and media system  may further be operated as a participant in a larger network gaming community.","As discussed above, the processing unit  may be embodied in a mobile device .  is a block diagram of an exemplary mobile device  which may operate in embodiments of the technology. Exemplary electronic circuitry of a typical mobile phone is depicted. The phone  includes one or more microprocessors , and memory  (e.g., non-volatile memory such as ROM and volatile memory such as RAM) which stores processor-readable code which is executed by one or more processors of the control processor  to implement the functionality described herein.","Mobile device  may include, for example, processors , memory  including applications and non-volatile storage. The processor  can implement communications, as well as any number of applications, including the interaction applications discussed herein. Memory  can be any variety of memory storage devices types, including non-volatile and volatile memory. A device operating system handles the different operations of the mobile device  and may contain user interfaces for operations, such as placing and receiving phone calls, text messaging, checking voicemail, and the like. The applications  can be any assortment of programs, such as a camera application for photos and\/or videos, an address book, a calendar application, a media player, an internet browser, games, other multimedia applications, an alarm application, other third party applications like a disappearance application and image processing software for processing image data to and from the display device  discussed herein, and the like. The non-volatile storage component  in memory  contains data such as web caches, music, photos, contact data, scheduling data, and other files.","The processor  also communicates with RF transmit\/receive circuitry  which in turn is coupled to an antenna , with an infrared transmitted\/receiver , with any additional communication channels  like Wi-Fi, WUSB, RFID, infrared or Bluetooth, and with a movement\/orientation sensor  such as an accelerometer. Accelerometers have been incorporated into mobile devices to enable such applications as intelligent user interfaces that let users input commands through gestures, indoor GPS functionality which calculates the movement and direction of the device after contact is broken with a GPS satellite, and to detect the orientation of the device and automatically change the display from portrait to landscape when the phone is rotated. An accelerometer can be provided, e.g., by a micro-electromechanical system (MEMS) which is a tiny mechanical device (of micrometer dimensions) built onto a semiconductor chip. Acceleration direction, as well as orientation, vibration and shock can be sensed. The processor  further communicates with a ringer\/vibrator , a user interface keypad\/screen, biometric sensor system , a speaker , a microphone , a camera , a light sensor  and a temperature sensor .","The processor  controls transmission and reception of wireless signals. During a transmission mode, the processor  provides a voice signal from microphone , or other data signal, to the RF transmit\/receive circuitry . The transmit\/receive circuitry  transmits the signal to a remote station (e.g., a fixed station, operator, other cellular phones, etc.) for communication through the antenna . The ringer\/vibrator  is used to signal an incoming call, text message, calendar reminder, alarm clock reminder, or other notification to the user. During a receiving mode, the transmit\/receive circuitry  receives a voice or other data signal from a remote station through the antenna . A received voice signal is provided to the speaker  while other received data signals are also processed appropriately.","Additionally, a physical connector  can be used to connect the mobile device  to an external power source, such as an AC adapter or powered docking station. The physical connector  can also be used as a data connection to a computing device. The data connection allows for operations such as synchronizing mobile device data with the computing data on another device.","A GPS receiver  utilizing satellite-based radio navigation to relay the position of the user applications is enabled for such service.","The example computer systems illustrated in the figures include examples of computer readable storage devices. Computer readable storage devices are also processor readable storage devices. Such media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage devices includes, but is not limited to, RAM, ROM, EEPROM, cache, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, memory sticks or cards, magnetic cassettes, magnetic tape, a media drive, a hard disk, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can accessed by a computer.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1B"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5C"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 5D"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 5E"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 5F"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 6B"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 6C"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 6D"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIGS. 13A","b":["13","13","13"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIGS. 14A","b":["14","14","14"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIGS. 15A and 15B"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 17"}]},"DETDESC":[{},{}]}
