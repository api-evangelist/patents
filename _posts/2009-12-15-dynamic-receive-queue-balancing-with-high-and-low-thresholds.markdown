---
title: Dynamic receive queue balancing with high and low thresholds
abstract: A method according to one embodiment includes the operations of assigning a network application to at least one first core processing unit, from among a plurality of core processing unit. The method of this embodiment also includes the operations of assigning a first receive queue to said first core processing unit, wherein the first receive queue is configured to receive packet flow associated with the network application; defining a high threshold for the first receive queue; and monitoring the packet flow in the first receive queue and comparing a packet flow level in the first receive queue to the high threshold; wherein if the packet flow level exceeds the threshold based on the comparing, generating a queue status message indicating that the packet flow level in the first queue has exceeded the queue high threshold.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08346999&OS=08346999&RS=08346999
owner: Intel Corporation
number: 08346999
owner_city: Santa Clara
owner_country: US
publication_date: 20091215
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD","BACKGROUND","DETAILED DESCRIPTION"],"p":["The disclosure relates to packet load balancing and, more particularly, to dynamic receive queue balancing.","As network speeds increase, it becomes necessary to scale packet processing across multiple processors in a system. For receive processing, a feature called RSS (Receive Side Scaling) can distribute incoming packets across multiple processors in a system. RSS is a Microsoft\u00ae Windows\u00ae operating system (\u201cOS\u201d) technology that enables receive-processing to scale with the number of available computer processors by allowing the network load from a network controller to be balanced across multiple processors. RSS is described in \u201cScalable Networking: Eliminating the Receive Processing Bottleneck\u2014Introducing RSS\u201d, WinHEC (Windows Hardware Engineering Conference) 2004, Apr. 14, 2004 (hereinafter \u201cthe WinHEC Apr. 14, 2004 white paper\u201d). It is also described in the Scalable Network Pack of the Network Driver Interface Specification (NDIS). NDIS describes a Microsoft Windows device driver that enables a single network controller, such as a NIC (network interface connection), to support multiple network protocols, or that enables multiple network controllers to support multiple network protocols. The current version of NDIS is NDIS 6.2, and is available from Microsoft\u00ae Corporation of Redmond, Wash. The subsequent version of NDIS, known as NDIS 6.2, available from Microsoft Corporation, is currently known as the \u201cScalable Networking Pack\u201d for Windows Server 2003. With the RSS feature, the OS distributes the processing load for network traffic across multiple processors, cores, or hardware threads by maintaining an indirection table in the network device that maps flows to the processing unit.","On the network device side of packet flow processing, so-called \u201capplication targeted routing\u201d (ATR) can be used to assign a network application to a specific MAC receive queue and processor core. Once the queue\/core pair is assigned, ATR logic (residing in the MAC) can be used to track TCP\/IP and\/or UDP packet flows and post packets to the correct queue\/core pair. However, an overloaded CPU or excessive packet offloading (for example, security offloading using IPSec or LinkSec protocols) may cause the receive queue to become overloaded. Conventional implementations of ATR do not account for load conditions on the MAC queues, and thus, an overloaded queue may result in packet loss and\/or performance degradation.","Although the following Detailed Description will proceed with reference being made to illustrative embodiments, many alternatives, modifications, and variations thereof will be apparent to those skilled in the art.","RSS and ATR techniques may be used to take advantage of multiple cores to more efficiently process packet flows, however these techniques may suffer from decreased performance due to a failure to react to packet flow at the receive queues. For example, a network MAC may include ATR logic to load incoming packets to a particular MAC queue based an application-specific filter properties. However, as a result of not being emptied by the application side and\/or burst traffic on the network adaptor side, packets may be dropped as queues become overloaded. Generally, this disclosure describes systems (and methods) of setting thresholds for receive queues, and monitoring the packet flow in the queues with respect to the thresholds. As queue capacity exceeds a threshold, a messaging system is provided herein that notifies an operating system of an impending queue overload condition. In response to this message, the operating system may assign the application (associated with the troubled queue) to a new core, and the application may also be reassigned to a new receive queue. Moving a packet flow away from an overloaded queue may reduce or eliminate dropped packets in the network adapter and\/or network adapter packet flow inefficiencies.","While the receive queues can be located at the network adapter or defined within host system memory (or elsewhere), there are a number of advantages of monitoring and reacting to queue conditions at the network adaptor. For instance, the network adaptor may generally reference the receive queues more frequently than the host, and therefore be able to detect and react to a queue overload condition more quickly than, for example, a software module running in host memory. Further, the network adapter is generally better suited to monitor the queue levels during the frequent intervals when packets are being added, rather than the relatively infrequent intervals when the receive queues are emptied. Because the network adaptor is already servicing the queues more frequently, it may be less overhead for the network adaptor to add queue monitoring to its existing service schedule than to increase the queue service schedule of the host system.","System Architecture",{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 1","b":["100","102","104","102","102","106","108","106","106","108","113","112","110","1","1","112","104"]},"One or more applications can be assigned to each CPU core. For example, as depicted in , Application A, A and A can be assigned to CPU A, meaning that CPU A has the primary responsibility for executing instructions and exchanging commands and data related to these applications. Similarly, Application B and B can be assigned to CPU B and Application n can be assigned to CPU n. It should be noted at the outset that at least one application running in system memory  may include a \u201cnetwork application,\u201d meaning that such an application involve receiving and\/or sending packets from\/to the network adaptor . Of course, other system applications, including non-network applications, may be running in system memory .","Application load balancer code  is configured to balance the plurality of applications residing in system memory  across the plurality of available CPU cores. Specifically, application load balancer  is configured to assign an application to a particular core for processing. Assignment of an application to a core may occur, for example, upon system initialization and may also be performed dynamically during operation of the system  and\/or as one or more applications become invoked in memory . Application load balancer  may be configured to monitor the processing usage (e.g., memory access, computation cycles in use, cache usage, network traffic, etc.) and\/or available processing capacity of each core to determine available processing capacity on a core-by-core basis. Application load balancer code  may then distribute applications across the cores in an effort to balance the load across all of the available cores in the system.","Network adapter  may comprise a network interface card (NIC)  that generally includes media access control (MAC) circuitry  and physical interface (PHY) circuitry . MAC circuitry  may be configured to assemble data to be transmitted into packets, that include destination and source addresses along with network control information and error detection hash values. PHY circuitry  may include encoding and decoding circuitry (not shown) to encode and decode data packets. NIC  may be coupled to a medium to receive one or more packet flows, as indicated by packet flow . NIC  may also include a plurality of receive queues, labeled Queue A, Queue B, . . . , Queue n. Receive queues Queue A, Queue B, . . . , Queue n are configured to reference packets associated with a particular application received by the NIC  (via incoming packet flow ).","MAC circuitry  may include queue control and queue assignment circuitry  configured to assign each receive queue to a particular packet flow associated with a network application in the host system. Circuitry  is also configured to move packet flow data into an assigned queue, either directly or after additional processing (e.g., after security processing using IPSec or LinkSec protocols). In operation, device drivers  may exchange commands and data with circuitry  to control the certain operations of circuitry . For example, device drivers  can control circuitry  to assign Queue A to the packet flows for Applications A, A and A (which are assigned to CPU A by application load balancer ). Similarly, Queue B . . . Queue n can be assigned to respective packet flows for respective applications, as indicated in .","In addition, circuitry  is configured to establish at least one threshold for at least one receive queue. For example, a ceiling threshold may be established for at least one queue, defining a point above which the queue is in danger of an overload condition. In operation, device drivers  may be configured to communicate with circuitry  (via communication path ) to establish queue thresholds. A threshold, as used herein, is a percentage (or range) of overall queue packet flow level capacity. A ceiling (high) threshold is used herein to provide an alert mechanism that queue capacity is about to be exceeded, which may result in packet loss and\/or inefficient packet throughput. A high threshold may be defined so that, if exceeded, the system has time to react and change the queue for the packet flow before the queue becomes overloaded. Of course, a ceiling threshold may be provided for each receive queue, and in other embodiments a ceiling and a floor threshold may be generated for each queue, as will be explained in greater detail below.","Circuitry  may be further configured to monitor one more queues to determine the packet flow level in a queue and compare the packet flow level to a high threshold defined for that queue. If the high threshold is exceeded, this may be indicative of an impending queue overload. A queue overload can be a result of, for example, bandwidth reduction of the corresponding CPU core (e.g., the core is too busy to empty the queue efficiently), too many network applications are assigned to one queue resulting in too many packet flows on the queue, or burst traffic (e.g., as may result from security offloading by the MAC) overruns the queue. If packet flow in a queue exceeds the high threshold, circuitry  is further configured to generate a queue status message indicative of the status of the queue. In this case, the queue status message may include information of an impending queue overload condition. Circuitry  may also be configured to forward, via communication path , the queue status message to the operating system  (and, more specifically, to application load balancer ).","The queue status message may cause the application load balancer  to migrate one or more applications to another core, which will, in turn, move the packet flow from one queue to another queue. Thus, for example, as depicted in  and assuming that the packet flow of Queue A has been identified as exceeding a threshold, circuitry  may provide a message to load balancer , which, in turn may reassign Application A to CPU B, indicated by migration path . Device drivers  may control circuitry  (via communication path ) to reassign packet flows to a new queue. As a result, circuitry  may reassign packet flows associated with Application A to Queue B, as indicated by migration path .",{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 2","FIG. 2","FIG. 1","FIG. 2","FIG. 1"],"b":["200","100","106","116","118"]},"As shown in , Queue A includes packet flows A and A associated with applications A and A, respectively. Queue B includes packet flow B associated with application B, and Queue n includes packet flow n associated with application n. Each queue may have a configurable low threshold (or floor threshold) level A, B, . . . , and . The low threshold value may be defined, for example, based on expected packet flow for the applications assigned to the particular queue, and\/or the low threshold value may be set within a certain range, e.g., 10-20% of the maximum queue capacity and\/or other user-programmable values or ranges. When the capacity of a queue falls below the low threshold, this may be an indication that the particular queue has additional capacity available to accept addition packet flows (via a network application assignment to that queue). Each queue may have a configurable high threshold (or ceiling threshold) level A, B, . . . , and . When the packet traffic level of a queue rises above the high threshold of the queue, the traffic level of the queue may be approaching a queue overload condition.","As an example,  depicts the traffic level in Queue A surpassing high threshold A A, indicating an excess of traffic on Queue A.  also depicts the traffic in queue B falling below low threshold B B, indicating capacity for additional traffic on queue B. Queue status message  may be generated indicating, at least, that the high threshold A in Queue A is exceeded. Such a message  may be forwarded to the application load balancer . In response thereto, the application load balancer  may cause a migration of one or more of the applications away from the current core assignment by assigning one or more applications to at least one other core. Thus, in this example, Application A, A and\/or A may be moved from CPU A to another core.","While the forgoing represents one example of a queue status message  indicative of an exceeded high threshold in one or more queues, the message  of this embodiment may also include additional information regarding the capacity of other queues. In the example of , the packet flow in Queue B (as a result of packet flow related to Application B) is below the low threshold B, while the high threshold A of Queue A is exceeded. Message  may be generated to indicate that the packet flow in Queue A has exceeded the high threshold A and the packet flow in Queue B is below the low threshold B. Such a message  may be forwarded to the application load balancer . In response thereto, the application load balancer  may cause a migration of one or more of the applications away from the current core assignment to a different core. Thus, in this example, Application A to be moved away from CPU A and assigned to CPU B, since Queue B (assigned to CPU B a priori, as discussed above) is indicated as having available capacity. This is depicted graphically in  as migration path  where Application A is re-assigned to CPU B as Application B. Of course, packet flows associated with Application B may now be assigned to Queue B, as indicated by migration path . Thus, queue status message  may include information related to the capacity of one or more queues relative to the high threshold and\/or the low threshold assigned to that queue.",{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 3","FIG. 3","FIG. 1","FIG. 3","FIG. 1"],"b":["300","120","100","106","108","116","118","2","120","302","304","306","308","310"]},"Circuitry  is configured to assign at least one application packet flow to a respective receive queue. To that end, device driver  (as shown in ) may be used to exchange commands and data with circuitry  (including circuitry ) to assign particular queues to particular packet flows. An application (not shown in this Figure) may already be assigned to a particular CPU core, as described above. Thus, once a packet flow associated with the application is assigned to a receive queue by circuitry , a queue\/core pair may be created for packets associated with an application. The queue\/core pair related to an application packet flow may be used to populate an indirection table , as will be described in greater detail below.","In addition, circuitry , either directly or in response to commands from device drivers , may generate a queue high and\/or low threshold table . Table  may include floor (low) and\/or ceiling (high) threshold values for at least one receive queue that has been assigned an application packet flow by circuitry . In one embodiment, as described with reference to , each receive queue is assigned a low threshold and a high threshold. The low threshold is a value below which is indicative of extra available packet flow capacity on the queue, and the high threshold is a value above which is indicative of an impending queue overload condition. Each receive queue may be assigned unique low\/high threshold values, or, alternatively, all of the receive queues may be assigned the same low and high threshold values.","Hash function circuitry  may include packet filters operable to identify a packet flow with a particular application. In operation, circuitry  may parse an incoming packet flow to obtain flow identity (\u201cID\u201d) information. Normally, a packet flow includes one or more fields that permit identification. For example, the packet flow ID for a TCP packet may include a sequence of source IP address, destination IP address, source port number, and destination port number, L2\/L4 data, etc., any of which can be utilized by circuitry  to ID the packet flow. Of course, circuitry  may be configured to ID other packet protocols, e.g., using UDP packet information.","Circuitry  may be configured to generate a hash value hrelated to the packet flow ID. The hash result hmay correspond to one of entries in the indirection table . Based on the hash result h, indirection table  returns the identity information of a core and receive queue associated with the incoming packet flow. Once the appropriate queue is known (using table ), packets associated with that application may be buffered into the appropriate receive queue and\/or forwarded to other processing circuitry, for example, security offloading circuitry (not shown).  depicts one embodiment of indirection table  consistent with the present disclosure. Each row of indirection table  may represent the association of a queue, a CPU or core, and a packet flow , as represented by a hash index (h). As stated, circuitry  is configured to receive an incoming packet flow, parse the packet flow to determine the packet flow identifier and look up the hash index assigned to the packet flow. The hash index (h) may be used to reference a row in the indirection table , to determine the receive queue and core assigned to that packet flow.","Queue threshold monitoring circuitry  is configured to monitor at least one receive queue and compare the capacity in the receive queue to the threshold values established in table  for that queue. For example, if both high and low thresholds are established for a given queue in table , circuitry  may compare the current queue traffic level to the low and high thresholds to determine whether the queue is currently overloaded (high threshold exceeded) or has available capacity (below the low threshold). If any of the thresholds are exceeded, queue threshold monitoring circuitry  may generate status message . Message  may include information to identify the queue that has exceeded a high threshold and the queue that has current capacity below the low threshold. As described above, such a message may be interpreted by the host system OS to reassign at least one application away from an overloaded queue, and possibly toward a queue that has available capacity. Circuitry  may be configured to communicate the queue status message to the OS , using, for example an in-band message protocol or an out-of-band (OOB) message protocol.","In order to prevent \u201cfalse alarms\u201d of an overloaded queue, circuitry  may include a time-out mechanism (e.g., timer circuitry, latch circuitry, debounce circuitry, etc., not shown) that is permitted to lapse before any message is sent to the OS. Thus, for example, in a \u201cbursty\u201d environment, the high threshold may momentarily be exceeded (with remaining capacity in the queue), but may settle below the threshold once the packets are timely moved out of the queue. Before circuitry  generates a message, circuitry  may wait until a predetermined time-out period has lapsed. The time out period may be selected, for example, based on the overall traffic flow of the queue and\/or other criteria which may be evident to one of ordinary skill in the art.","Exemplary Methodology",{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 5","FIG. 1"],"b":["500","104","510","520","525","530","540","550"]},"If the timeout period is complete , a flag may be set , indicating an impending queue overload condition, or, more generally, that the status of the queue has changed. If the flag is set , a queue status message may be generated indicating a change in queue level status . More specifically, in this example, the queue status message indicates an impending queue overload condition. The message can be forwarded to an operating system to enable, for example, a change in the associated packet flow\/core assignment. In this embodiment, the queue threshold status message may be communicated to the OS using, for example, a bus signal, an interrupt, a mailbox, semaphore, and\/or passive polling of registers by the host system.","Where a plurality of queues are monitored and both a high and low threshold are defined for each queue, operations of this embodiment may also include determining if a queue capacity is below the floor threshold , and if so, determining if the queue capacity has remained below the low threshold for longer than a timeout period . If the timeout period is complete , a flag may be set , indicating available capacity on a queue or that the status of the queue has changed. Operations may also include determining if both flags ,  are set, and if so, a message is generated indicating a change in queue level status . Here, both flags being set mean that one queue indicates an overload condition, while another queue indicates available capacity. The message can be forwarded to an operating system to enable, for example, a change in the associated packet flow\/core assignment away from the queue that is overloaded and into the queue that has capacity.","If the high threshold flag for one queue is set , but the floor threshold flag  remains unset for any queue, operations may further include determining if the low threshold in at least one queue can be raised . Other queues may have available capacity, but each of the other queues may have exceeded their respective low thresholds. Thus, raising the floor of other queues may enable identification of a queue that has additional capacity to receive additional packet flows. If the low threshold for at least one queue can be raised , operations  and  may be performed until the low threshold flag is set . Thus, the floor of at least one queue may be raised incrementally until the packet flow for that queue drops below the low threshold. If the floor threshold of at least one queue cannot be raised (), a message may be generated indicating an impending queue overload condition, or that the status of the queue has changed . In this case, the message may include information related to an exceeded threshold for a queue, but does not include information regarding another queue having available capacity. The message can be forwarded to an operating system to enable, for example, a change in the packet flow\/core assignment.","In this embodiment, the low threshold may be used to determine the queue that has the most available capacity (among the plurality of queues). Generating a queue status message having queue capacity information in terms of both high and low thresholds may enable the OS to migrate an application so that packets are routed to the best receive queue, by migrating an application to the core that matches a queue having the most available additional capacity. Also, if a queue status message has been forwarded to the hosts system and the queue status changes by packet flow falling below the high threshold, another queue status message may be forwarded to the host system to update the host on the status of the queue.","The operations described herein can reduce or eliminate \u201cguesswork\u201d on the part of the OS in terms of where to best move an application and which receive queue is best suited to begin receiving packet flows for an application. These operations can reduce or eliminate repetitive attempts to assign an application to a queue (which may result in \u201cqueue thrashing\u201d) which may otherwise occur if the OS does not have additional queue capacity information. For example, in the embodiment of , while both Queue B and Queue n are depicted as having additional capacity, Queue B may be a better choice to receive the packet flows from Application A, since the capacity on Queue B is greater than Queue n and the capacity on Queue B is below the low threshold B, while the capacity on Queue n is above the low threshold ","In this embodiment, the network adaptor may not affirmatively forward the queue status message to the host system, but may publish the queue level status message for the host system to periodically poll. In one embodiment, the network adaptor may publish the queue level status message in MAC registers, and make these registers available to the host system, for example, using the network adaptor device driver. In another embodiment, the network adaptor may write the queue level status message in a re-locatable data structure resident in host system memory, and the host system may communicate the location of this data structure to the network adaptor via the network adaptor device driver.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":["FIG. 6","FIG. 1","FIG. 5"],"b":["600","113","110","102"]},"Operations according to this embodiment may include receiving a queue status message from a MAC . As described above, the queue status message may include information regarding a receive queue capacity or level of traffic referenced by the receive queue. The message may indicate a change in the condition in one or more receive queues, for example, (1) a high threshold indicator has been activated (2) a high threshold indicator has been deactivated, (3) a low threshold indicator has been activated, and (4) a low threshold indicator has been deactivated. If the queue threshold status message indicates that one or more queues are detecting traffic levels below the low thresholds and none of the high thresholds have been crossed, then no action may be taken. If the queue status message indicates that one or more receive queue high threshold flags have been set (indicative of an impending queue overload condition), operations of this embodiment may include determining if another core is available to assume the packet traffic of the overloaded queue . If a core is available, an application may be moved from an overloaded queue\/core pair to the available core , thus creating a new application\/core pairing. Operations according to this embodiment may also include sending a message to the MAC indicating the new packet flow\/core assignment .","While the foregoing has assumed that a network application may be moved from one core to a different core, it is also contemplated herein that non-network applications may be moved away from one core to another core. For example, an application load balancer, in response to the queue status message from the network adapter, may determine that an overloaded queue is a result of an overloaded core. Instead of migrating network applications away from that core, the application load balancer may instead identify non-network applications associated with the core and move one or more of these non-network applications to another core.","In an embodiment where the status message indicates that a receive queue is overburdened with traffic, but no other receive queue currently has capacity available to accept traffic, the application load balancer may adjust the low threshold level settings of one or more receive queues, as described above with reference to . Once a receive queue is found to have adequate space to begin receiving packet flow from an application, the application may be moved to that core.","If an application is moved to another core, a message may be sent to the MAC circuitry indicating a new application\/core assignment . When the MAC receives the message of the new application\/core assignment, there may be a window of time where portions of packet flows to be processed by the migrated application may still be remaining in the receive queue associated with the former core of the migrated application. The MAC may therefore modify a queue handler of the former queue to forward existing packets to the new queue. Alternatively, packets in the old queue may be discarded (for example, by freeing the memory allocated to the received packets) and the MAC may send a request to the link partner requesting retransmission of the dropped packets. from the transmitting application.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":["FIG. 7","FIG. 1","FIG. 6"],"b":["700","104"]},"A new application\/core pair assignment may be received by a network adapter . As described above, a host system (e.g., device drivers) may control MAC circuitry to establish a new application packet flow\/queue assignment. The network adaptor may then determine whether or not this packet flow is currently being serviced by the network adaptor by checking if the packet flow identifier has been registered by the network adaptor. For example, the network adapter may check if the packet flow is associated with a hash index in a hash index table . If the packet flow is currently registered with the network adaptor, meaning the packet flow is not new , the network adaptor may then use the hash index to look up an entry in an indirection table . If the packet flow is not currently registered with the network adaptor, meaning a new packet flow , a new hash index may be generated to associate the new queue assignment with the packet flow . For example, the hash index may add a new entry to the first available row of a MAC indirection table . Whether the indirection table entry previously existed, or has been created to accommodate a new packet flow, the queue and core associated with the packet flow may be written to a corresponding entry in the indirection table .","While the foregoing is prided as exemplary system architectures and methodologies, modifications to the present disclosure are possible. For example, operating system  may manage system resources and control tasks that are run on system . For example, OS  may be implemented using Microsoft Windows, HP-UX, Linux, or UNIX, although other operating systems may be used. When a Microsoft Windows operating system is used, the ndis.sys driver may be utilized at least by device driver  and an intermediate driver (not shown). For example, the ndis.sys driver may be utilized to define application programming interfaces (APIs) that can be used for transferring packets between layers. In one embodiment, OS  shown in  may be replaced by a virtual machine which may provide a layer of abstraction for underlying hardware to various operating systems running on one or more processors.","Operating system  may implement one or more protocol stacks (not shown). A protocol stack may execute one or more programs to process packets. An example of a protocol stack is a TCP\/IP (Transport Control Protocol\/Internet Protocol) protocol stack comprising one or more programs for handling (e.g., processing or generating) packets to transmit and\/or receive over a network. A protocol stack may alternatively be comprised on a dedicated sub-system such as, for example, a TCP offload engine.","Other modifications are possible. For example, memory  and\/or memory associated with the network adaptor  (not shown) may comprise one or more of the following types of memory: semiconductor firmware memory, programmable memory, non-volatile memory, read only memory, electrically programmable memory, random access memory, flash memory, magnetic disk memory, and\/or optical disk memory. Either additionally or alternatively, memory  and\/or memory associated with the network adaptor  (not shown) may comprise other and\/or later-developed types of computer-readable memory.","Embodiments of the methods described herein may be implemented in a system that includes one or more storage mediums having stored thereon, individually or in combination, instructions that when executed by one or more processors perform the methods. Here, the processor may include, for example, a system CPU (e.g., core processor of ) and\/or programmable circuitry such as the MAC circuitry. Thus, it is intended that operations according to the methods described herein may be distributed across a plurality of physical devices, such as processing structures at several different physical locations. Of course, the operations described herein as attributable to the host system and the network adapter could be performed by a storage medium, on one or the other, having instructions that when executed by one or more processors perform the methods. Also, it is intended that the method operations may be performed individually or in a subcombination, as would be understood by one skilled in the art. Thus, not all of the operations of each of the flow charts need to be performed, and the present disclosure expressly intends that all subcombinations of such operations are enabled as would be understood by one of ordinary skill in the art.","The storage medium may include any type of tangible medium, for example, any type of disk including floppy disks, optical disks, compact disk read-only memories (CD-ROMs), compact disk rewritables (CD-RWs), and magneto-optical disks, semiconductor devices such as read-only memories (ROMs), random access memories (RAMs) such as dynamic and static RAMs, erasable programmable read-only memories (EPROMs), electrically erasable programmable read-only memories (EEPROMs), flash memories, magnetic or optical cards, or any type of media suitable for storing electronic instructions.","The Ethernet communications protocol, described herein, may be capable permitting communication using a Transmission Control Protocol\/Internet Protocol (TCP\/IP). The Ethernet protocol may comply or be compatible with the Ethernet standard published by the Institute of Electrical and Electronics Engineers (IEEE) titled \u201cIEEE 802.3 Standard\u201d, published in March, 2002 and\/or later versions of this standard.","As used herein, a \u201cPHY\u201d may be defined as an object and\/or circuitry used to interface to one or more devices, and such object and\/or circuitry may be defined by one or more of the communication protocols set forth herein. The PHY may comprise a physical PHY comprising transceiver circuitry to interface to the applicable communication link. The PHY may alternately and\/or additionally comprise a virtual PHY to interface to another virtual PHY or to a physical PHY. PHY circuitry  may comply or be compatible with, the aforementioned IEEE 802.3 Ethernet communications protocol, which may include, for example, 100BASE-TX, 100BASE-T, 10 GBASE-T, 10 GBASE-KR, 10 GBASE-KX4\/XAUI, 40 GbE and or 100 GbE compliant PHY circuitry, and\/or PHY circuitry that is compliant with an after-developed communications protocol.","\u201cCircuitry\u201d, as used in any embodiment herein, may comprise, for example, singly or in any combination, hardwired circuitry, programmable circuitry, state machine circuitry, and\/or firmware that stores instructions executed by programmable circuitry.","The terms and expressions which have been employed herein are used as terms of description and not of limitation, and there is no intention, in the use of such terms and expressions, of excluding any equivalents of the features shown and described (or portions thereof), and it is recognized that various modifications are possible within the scope of the claims. Accordingly, the claims are intended to cover all such equivalents."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Features and advantages of embodiments of the claimed subject matter will become apparent as the following Detailed Description proceeds, and upon reference to the Drawings, wherein like numerals depict like parts, and in which:",{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
