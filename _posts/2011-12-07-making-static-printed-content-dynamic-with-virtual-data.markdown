---
title: Making static printed content dynamic with virtual data
abstract: The technology provides embodiments for making static printed content being viewed through a see-through, mixed reality display device system more dynamic with display of virtual data. A printed content item, for example a book or magazine, is identified from image data captured by cameras on the display device, and user selection of a printed content selection within the printed content item is identified based on physical action user input, for example eye gaze or a gesture. A task in relation to the printed content selection can also be determined based on physical action user input. Virtual data for the printed content selection is displayed in accordance with the task. Additionally, virtual data can be linked to a work embodied in a printed content item. Furthermore, a virtual version of the printed material may be displayed at a more comfortable reading position and with improved visibility of the content.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09182815&OS=09182815&RS=09182815
owner: Microsoft Technology Licensing, LLC
number: 09182815
owner_city: Redmond
owner_country: US
publication_date: 20111207
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Static printed material may be thought of as a form of read only memory which requires no power and stores its data in a form visible to the human eye. Vellum texts over a thousand years old survive to this day. Additionally, the data is presented generally in a format comfortable for human eye reading which presents printed text against a contrasting physical background of white or another contrasting colored paper. The physical nature of the printed material allows a user to physically sift through its data for \u201csomething of interest,\u201d for example, by flipping through its pages and looking at illustrations or catchy titles in a magazine. One can hold one's place in a physical book, and flip back a clump of pages, which the user's brain knows is about the right section, to reread a section and look back at the place held by a finger too. Of course, physical books, periodicals and paper have their disadvantages too due to the permanent setting of information on their pages.","Mixed reality is a technology that allows virtual imagery to be mixed with a real world view. A see-through, head mounted, mixed reality display device may be worn by a user to view the mixed imagery of real objects and virtual objects displayed in the user's field of view. Such a head mounted display (HMD) device can update, and in some instances, restore the data embodied in the static printed material. In other words, the physical book, magazine or other embodiment of static printed material becomes a form of memory which is dynamic in the sense that what appears on a printed sheet of paper, a printed card or other printed medium can change.","The technology provides an embodiment of a method for making static printed content dynamic with virtual data using a see-through, near eye, mixed reality display device. The method comprises identifying a printed content item in a field of view of a see-through, near eye, mixed reality display device and identifying user selection of a printed content selection within the printed content item based on physical action user input. A task a user is requesting is determined for the printed content selection based on physical action user input and is performed. Virtual data related to the printed content selection is displayed in accordance with the requested task.","The technology provides an embodiment of a system for a see-through, near-eye, mixed reality display device system for making static printed material dynamic. The system comprises a respective see-through display for each eye positioned by a support structure. An example of a support structure is a frame. At least one outward facing camera is positioned on the support structure for capturing image data in a field of view of the respective see-through displays. One or more software controlled processors are communicatively coupled to the at least one outward facing camera for receiving image data and to at least one image generation unit optically coupled to the respective see-through displays.","The one or more software controlled processors identify user selection of a printed content selection based on physical action user input and image data. For example a page of a book may be in view of the at least one outward facing camera. A physical action user input is an action performed by a user using a body part and captured by a natural user interface (NUI). The physical action provides data or commands which direct the operation of an application. Some examples of physical actions are eye gaze and a gesture.","The one or more software controlled processors are communicatively coupled to a search engine having access to datastores including content, layout and virtual data for works and printed content items embodying the works. The one or more software controlled processors identify a printed content item including the printed content selection and a work including a medium independent version of the printed content selection based on formulating one or more queries based on the image data. The one or more queries are sent to the search engine.","The one or more software controlled processors cause the at least one communicatively coupled image generation unit to display virtual data associated with the printed content selection or the medium independent version of the printed content selection by each optically coupled respective see-through display.","The technology provides an embodiment of one or more processor readable storage devices having instructions encoded thereon which instructions cause one or more processors to execute a method for improving readability of static printed material with virtual data using a see-through, near eye, mixed reality display device system. The method comprises identifying printed material in a field of view of the see-through, near eye, mixed reality display device system based on image data captured by one or more outward facing cameras of the display device system and determining whether readability criteria is satisfied for the printed material positioned in the field of view. Responsive to readability criteria not being satisfied, a virtual version of the printed material is displayed in the field of view which satisfies readability criteria. Additionally, action is taken responsive to physical action user input with respect to either or both of the virtual version of the printed material or the printed material if the printed material is still within a field of view of the one or more outward facing cameras.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.","The technology provides various embodiments for making static printed content dynamic with virtual data seen through and displayed by a see-through, near-eye, mixed reality display device system. The see-through display device system identifies a real book, magazine, newspaper or other real printed material in a user field of view. A book, magazine, newspaper, a card or a separate sheet of paper are all examples of a printed content item which object recognition software can identify from image data captured by front facing cameras positioned on the display device system to capture objects in the field of view of the display device which approximates the user field of view when looking through the display device.","In some instances, eye gaze data identifies where a user is focusing in the field of view, and can thus identify at which portion of the printed content item a user is looking. A gaze duration on a portion of the printed material can identify the portion as a printed content selection. Gaze duration is an example of a physical action of a user using a body part. A gesture performed by a user body part such as a hand or finger and captured in image data is also an example of physical action user input. A blink or blinking sequence of an eye can also be a gesture. A pointing or particular movement gesture by a hand, finger or other body part can also indicate a printed content selection like a word, sentence, paragraph or photograph. A user generated sound command such as a voice command may also be considered an example of a physical action indicating user input. Sound based actions typically accompany other physical actions like a gesture and eye gaze.","Once the user selects a picture or text, different tasks or applications can be executed with respect to the content selection like augmentation with interactive games and holograms, replacement with updated content, and annotation using virtual data, either three dimensional, two dimensional or both. Readability can also be improved by generating and displaying a virtual version of at least a part of the printed content item.",{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 1A","b":["8","2","4","6","2","115","14","14"]},"The use of the term \u201cactual direct view\u201d refers to the ability to see real world objects directly with the human eye, rather than seeing created image representations of the objects. For example, looking through glass at a room allows a user to have an actual direct view of the room, while viewing a video of a room on a television is not an actual direct view of the room. Each display optical system  is also referred to as a see-through display, and the two display optical systems  together may also be referred to as a see-through display.","Frame  provides a support structure for holding elements of the system in place as well as a conduit for electrical connections. In this embodiment, frame  provides a convenient eyeglass frame as support for the elements of the system discussed further below. The frame  includes a nose bridge portion  with a microphone  for recording sounds and transmitting audio data in this embodiment. A temple or side arm  of the frame rests on each of a user's ears. In this example, the right temple includes control circuitry  for the display device .","As illustrated in , an image generation unit  is included on each temple  in this embodiment as well. Also, not shown in this view, but illustrated in  are outward facing cameras  for recording digital images and videos and transmitting the visual recordings to the control circuitry  which may in turn send the captured image data to the processing unit  which may also send the data to one or more computer systems  over a network .","The processing unit  may take various embodiments. In some embodiments, processing unit  is a separate unit which may be worn on the user's body, e.g. a wrist, or be a separate device like the illustrated mobile device  as illustrated in . The processing unit  may communicate wired or wirelessly (e.g., WiFi, Bluetooth, infrared, RFID transmission, wireless Universal Serial Bus (WUSB), cellular, 3G, 4G or other wireless communication means) over a communication network  to one or more computing systems  whether located nearby or at a remote location. In other embodiments, the functionality of the processing unit  may be integrated in software and hardware components of the display device  as in .","A remote, network accessible computer system  may be leveraged for processing power and remote data access. An application may be executing on computing system  which interacts with or performs processing for display system , or may be executing on one or more processors in the see-through, mixed reality display system . An example of hardware components of a computing system  is shown in .",{"@attributes":{"id":"p-0052","num":"0051"},"figref":["FIG. 1B","FIG. 2A"],"b":["8","50","136","2","137","50","12"]},{"@attributes":{"id":"p-0053","num":"0052"},"figref":["FIG. 1C","FIG. 17"],"b":["4","4","7","4","4"]},{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIG. 2A","FIG. 2B"],"b":["102","115","2","115","113","113","14","14","113","142","14"]},"In many embodiments, the two cameras  provide overlapping image data from which depth information for objects in the scene may be determined based on stereopsis. In some examples, the cameras may also be depth sensitive cameras which transmit and detect infrared light from which depth data may be determined The processing identifies and maps the user's real world field of view. Some examples of depth sensing technologies that may be included on the head mounted display device  without limitation, are SONAR, LIDAR, Structured Light, and\/or Time of Flight.","Control circuits  provide various electronics that support the other components of head mounted display device . In this example, the right temple includes control circuitry  for the display device  which includes a processing unit , a memory  accessible to the processing unit  for storing processor readable instructions and data, a wireless interface  communicatively coupled to the processing unit , and a power supply  providing power for the components of the control circuitry  and the other components of the display  like the cameras , the microphone  and the sensor units discussed below. The processing unit  may comprise one or more processors including a central processing unit (CPU) and a graphics processing unit (GPU).","Inside, or mounted to temple , are ear phones , inertial sensors , one or more location or proximity sensors , some examples of which are a GPS transceiver, an infrared (IR) transceiver, or a radio frequency transceiver for processing RFID data. Optional electrical impulse sensor  detects commands via eye movements. In one embodiment, inertial sensors  include a three axis magnetometer A, three axis gyro B and three axis accelerometer C. The inertial sensors are for sensing position, orientation, and sudden accelerations of head mounted display device . From these movements, head position may also be determined In this embodiment, each of the devices using an analog signal in its operation like the sensor devices , , , and  as well as the microphone  and an IR illuminator A discussed below, include control circuitry which interfaces with the digital processing unit  and memory  and which produces and converts analog signals for its respective device.","Mounted to or inside temple  is an image source or image generation unit  which produces visible light representing images. In one embodiment, the image source includes micro display  for projecting images of one or more virtual objects and coupling optics lens system  for directing images from micro display  to reflecting surface or element . The microdisplay  may be implemented in various technologies including transmissive projection technology, micro organic light emitting diode (OLED) technology, or a reflective technology like digital light processing (DLP), liquid crystal on silicon (LCOS) and Mirasol\u00ae display technology from Qualcomm, Inc. The reflecting surface  directs the light from the micro display  into a lightguide optical element , which directs the light representing the image into the user's eye. Image data of a virtual object may be registered to a real object meaning the virtual object tracks its position to a position of the real object seen through the see-through display device  when the real object is in the field of view of the see-through displays .","In some embodiments, one or more printed content selections being tracked for augmentation may be printed with one or more markers to improve detection of a content selection. Markers may also include metadata describing the content selection. For example, a photograph in a magazine may be printed with IR retroreflective markers or RFID tags which include the identifiers for the people in the photograph, as well as the place, date and time of day at which it was taken. Additionally, an identifier of one or more printed or electronic versions of a work in which it has been printed may be included. An IR or RFID unit  may detect the marker and send the data it contains to the control circuitry .",{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 2B","b":["14","115","2","14","14","14","2","115"],"i":"r "},"In the illustrated embodiment, the display optical system  is an integrated eye tracking and display system. The system includes a light guide optical element , opacity filter , and optional see-through lens  and see-through lens . The opacity filter  for enhancing contrast of virtual imagery is behind and aligned with optional see-through lens , lightguide optical element  for projecting image data from the microdisplay  is behind and aligned with opacity filter , and optional see-through lens  is behind and aligned with lightguide optical element . More details of the light guide optical element  and opacity filter  are provided below.","Light guide optical element  transmits light from micro display  to the eye  of the user wearing head mounted, display device . Light guide optical element  also allows light from in front of the head mounted, display device  to be transmitted through light guide optical element  to eye , as depicted by arrow  representing an optical axis of the display optical system , thereby allowing the user to have an actual direct view of the space in front of head mounted, display device  in addition to receiving a virtual image from micro display . Thus, the walls of light guide optical element  are see-through. Light guide optical element  includes a first reflecting surface  (e.g., a mirror or other surface). Light from micro display  passes through lens  and becomes incident on reflecting surface . The reflecting surface  reflects the incident light from the micro display  such that light is trapped inside a waveguide, a planar waveguide in this embodiment. A representative reflecting element  represents the one or more optical elements like mirrors, gratings, and other optical elements which direct visible light representing an image from the planar waveguide towards the user eye .","Infrared illumination and reflections, also traverse the planar waveguide  for an eye tracking system  for tracking the position of the user's eyes. The position of the user's eyes and image data of the eye in general may be used for applications such as gaze detection, blink command detection and gathering biometric information indicating a personal state of being for the user. The eye tracking system  comprises an eye tracking illumination source A and an eye tracking IR sensor B positioned between lens  and temple  in this example. In one embodiment, the eye tracking illumination source A may include one or more infrared (IR) emitters such as an infrared light emitting diode (LED) or a laser (e.g. VCSEL) emitting about a predetermined IR wavelength or a range of wavelengths. In some embodiments, the eye tracking sensor B may be an IR camera or an IR position sensitive detector (PSD) for tracking glint positions.","The use of a planar waveguide as a light guide optical element  in this embodiment allows flexibility in the placement of entry and exit optical couplings to and from the waveguide's optical path for the image generation unit , the illumination source A and the IR sensor B. In this embodiment, a wavelength selective filter  passes through visible spectrum light from the reflecting surface  and directs the infrared wavelength illumination from the eye tracking illumination source A into the planar waveguide  through wavelength selective filter  passes through the visible illumination from the micro display  and the IR illumination from source A in the optical path heading in the direction of the nose bridge . Reflective element  in this example is also representative of one or more optical elements which implement bidirectional infrared filtering which directs IR illumination towards the eye , preferably centered about the optical axis  and receives IR reflections from the user eye . Besides gratings and such mentioned above, one or more hot mirrors may be used to implement the infrared filtering. In this example, the IR sensor B is also optically coupled to the wavelength selective filter  which directs only infrared radiation from the waveguide including infrared reflections of the user eye , preferably including reflections captured about the optical axis , out of the waveguide  to the IR sensor B.","In other embodiments, the eye tracking unit optics are not integrated with the display optics. For more examples of eye tracking systems for HMD devices, see U.S. Pat. No. 7,401,920, entitled \u201cHead Mounted Eye Tracking and Display System\u201d, issued Jul. 22, 2008 to Kranz et al., see U.S. patent application Ser. No. 13\/221,739, Lewis et al., entitled \u201cGaze Detection in a See-Through, Near-Eye, Mixed Reality Display,\u201d filed Aug. 30, 2011, and see U.S. patent application Ser. No. 13\/245,700, Bohn, entitled \u201cIntegrated Eye Tracking and Display System,\u201d filed Sep. 26, 2011, all of which are incorporated herein by reference.","Another embodiment for tracking the direction of the eyes is based on charge tracking. This concept is based on the observation that a retina carries a measurable positive charge and the cornea has a negative charge. Sensors , in some embodiments, are mounted by the user's ears (near earphones ) to detect the electrical potential while the eyes move around and effectively read out what the eyes are doing in real time. (See -!, Feb. 19, 2010, http:\/\/www.wirefresh.com\/control-your-mobile-music-with-eyeball-actvated-headphones, which is hereby incorporated by reference.) Eye blinks may be tracked as commands Other embodiments for tracking eyes movements such as blinks which are based on pattern and motion recognition in image data from the small eye tracking camera B mounted on the inside of the glasses, can also be used. The eye tracking camera B sends buffers of image data to the memory  under control of the control circuitry .","Opacity filter , which is aligned with light guide optical element , selectively blocks natural light from passing through light guide optical element  for enhancing contrast of virtual imagery. When the system renders a scene for the mixed reality display, it takes note of which real-world objects are in front of which virtual objects and vice versa. If a virtual object is in front of a real-world object, then the opacity is turned on for the coverage area of the virtual object. If the virtual object is (virtually) behind a real-world object, then the opacity is turned off, as well as any color for that display area, so the user will only see the real-world object for that corresponding area of real light. The opacity filter assists the image of a virtual object to appear more realistic and represent a full range of colors and intensities. In this embodiment, electrical control circuitry for the opacity filter, not shown, receives instructions from the control circuitry  via electrical connections routed through the frame.","Again,  only show half of the head mounted display device . A full head mounted display device would include another set of optional see-through lenses  and , another opacity filter , another light guide optical element , another micro display , another lens system  physical environment facing camera  (also referred to as outward facing or front facing camera ), eye tracking assembly , earphones , and sensors  if present. Additional details of a head mounted display  are illustrated in U.S. patent application Ser. No. 12\/905,952 entitled Fusing Virtual Content Into Real Content, Filed Oct. 15, 2010, fully incorporated herein by reference.",{"@attributes":{"id":"p-0069","num":"0068"},"figref":["FIG. 3","FIG. 3"],"b":["8","12","54","54","191","190","191","192","193","194","195","196","191","113","192","191","200","50"]},"Virtual data engine  processes virtual objects and registers the position and orientation of virtual objects in relation to one or more coordinate systems. Additionally, the virtual data engine  performs the translation, rotation, scaling and perspective operations using standard image processing methods to make the virtual object appear realistic. A virtual object position may be registered or dependent on a position of a corresponding real object. The virtual data engine  determines the position of image data of a virtual object in display coordinates for each display optical system . The virtual data engine  may also determine the position of virtual objects in various maps of a real-world environment stored in a memory unit of the display device system  or of the computing system . One map may be the field of view of the display device with respect to one or more reference points for approximating the locations of the user's eyes. For example, the optical axes of the see-through display optical systems  may be used as such reference points. In other examples, the real-world environment map may be independent of the display device, e.g. a 3D map or model of a location (e.g. store, coffee shop, museum).","One or more processors of the computing system , or the display device system  or both also execute the object recognition engine  to identify real objects in image data captured by the environment facing cameras . As in other image processing applications, a person can be a type of object. For example, the object recognition engine  may implement pattern recognition based on structure data  to detect particular objects including a human. The object recognition engine  may also include facial recognition software which is used to detect the face of a particular person.","Structure data  may include structural information about targets and\/or objects to be tracked. For example, a skeletal model of a human may be stored to help recognize body parts. In another example, structure data  may include structural information regarding one or more inanimate objects in order to help recognize the one or more inanimate objects. The structure data  may store structural information as image data or use image data as references for pattern recognition. The image data may also be used for facial recognition. As printed material typically includes text, the structure data  may include one or more image datastores including images of numbers, symbols (e.g. mathematical symbols), letters and characters from alphabets used by different languages. Additionally, structure data  may include handwriting samples of the user for identification. Based on the image data, the dynamic printed material application  can convert the image data to a computer standardized data format for text with a smaller memory footprint. Some examples of computer standardized text data formats are Unicode based on the Universal Character Set (UCS) and the American Standard Code for Information Interchange (ASCII) format. The text data can then be searched against databases for identification of the content including the text or for related information about the content of the text.","Upon detection of one or more objects by the object recognition engine , image and audio processing engine  may report to operating system  an identification of each object detected and a corresponding position and\/or orientation which the operating system  passes along to an application like dynamic printed material application .","The sound recognition engine  processes audio received via microphone .","The outward facing cameras  in conjunction with the gesture recognition engine  implements a natural user interface (NUI) in embodiments of the display device system . Blink commands or gaze duration data identified by the eye tracking software  are also examples of physical action user input. Voice commands may also supplement other recognized physical actions such as gestures and eye gaze.","The gesture recognition engine  can identify actions performed by a user indicating a control or command to an executing application. The action may be performed by a body part of a user, e.g. a hand or finger typically in reading applications, but also an eye blink sequence of an eye can be gestures. In one embodiment, the gesture recognition engine  includes a collection of gesture filters, each comprising information concerning a gesture that may be performed by at least a part of a skeletal model. The gesture recognition engine  compares a skeletal model and movements associated with it derived from the captured image data to the gesture filters in a gesture library to identify when a user (as represented by the skeletal model) has performed one or more gestures. In some examples, a camera, in particular a depth camera in the real environment separate from the display device  in communication with the display device system  or a computing system  may detect the gesture and forward a notification to the system , . In other examples, the gesture may be performed in view of the cameras  by a body part such as the user's hand or one or more fingers.","In some examples, matching of image data to image models of a user's hand or finger during gesture training sessions may be used rather than skeletal tracking for recognizing gestures.","More information about the detection and tracking of objects can be found in U.S. patent application Ser. No. 12\/641,788, \u201cMotion Detection Using Depth Images,\u201d filed on Dec. 18, 2009; and U.S. patent application Ser. No. 12\/475,308, \u201cDevice for Identifying and Tracking Multiple Humans over Time,\u201d both of which are incorporated herein by reference in their entirety. More information about the gesture recognition engine  can be found in U.S. patent application Ser. No. 12\/422,661, \u201cGesture Recognizer System Architecture,\u201d filed on Apr. 13, 2009, incorporated herein by reference in its entirety. More information about recognizing gestures can be found in U.S. patent application Ser. No. 12\/391,150, \u201cStandard Gestures,\u201d filed on Feb. 23, 2009; and U.S. patent application Ser. No. 12\/474,655, \u201cGesture Tool,\u201d filed on May 29, 2009, both of which are incorporated by reference herein in their entirety.","The computing environment  also stores data in image and audio data buffer(s) . The buffers provide memory for receiving image data captured from the outward facing cameras , image data from an eye tracking camera of an eye tracking assembly  if used, buffers for holding image data of virtual objects to be displayed by the image generation units , and buffers for audio data such as voice commands from the user via microphone  and instructions to be sent to the user via earphones .","Device data  may include a unique identifier for the computer system , a network address, e.g. an IP address, model number, configuration parameters such as devices installed, identification of the operation system, and what applications are available in the display device system  and are executing in the display system  etc. Particularly for the see-through, mixed reality display device system , the device data may also include data from sensors or determined from the sensors like the orientation sensors , the temperature sensor , the microphone , the electrical impulse sensor  if present, and the location and proximity transceivers .","In this embodiment, the display device system  and other processor based systems  used by the user execute a client side version of a push service application which communicates over a communication network  with an information push service engine . The information push service engine  is cloud based in this embodiment. A cloud based engine is one or more software applications which execute on and store data by one or more networked computer systems. The engine is not tied to a particular location. Some examples of cloud based software are social networking sites and web-based email sites like Yahoo!\u00ae and Hotmail\u00ae. A user may register an account with the information push service engine  which grants the information push service permission to monitor the user's executing applications and data generated and received by them as well as user profile data , and device data  for tracking the user's location and device capabilities. Based on the user profile data aggregated from the user's systems , , the data received and sent by the executing applications on systems ,  used by the user, and location and other sensor data stored in device data , , the information push service  can determine a physical context, a social context, a personal context or a combination of contexts for the user.","The local copies of the user profile data , may store some of the same user profile data  and may periodically update their local copies with the user profile data stored by the computer system  in an accessible database  over a communication network . Some examples of user profile data  are the user's expressed preferences, the user's friends' list, the user's preferred activities, the user's favorites, some examples of which are, favorite color, favorite foods, favorite books, favorite author, etc., a list of the user's reminders, the user's social groups, the user's current location, and other user created content, such as the user's photos, images and recorded videos. In one embodiment, the user-specific information may be obtained from one or more data sources or applications such as the information push service , a user's social networking sites, contacts or address book, schedule data from a calendar application, email data, instant messaging data, user profiles or other sources on the Internet as well as data directly entered by the user. As discussed below, a state of being may be derived from eye data and be updated and stored in the user profile data  both locally and by the remote push service application . In this embodiment, a network accessible state of being lookup table  links identified eye data with a state of being as a reference for deriving the state of being.","Trust levels may be determined by user profile data  which identifies people known to the user, for example as social networking friends and family members sharing the same gaming service, which may be subdivided into different groups based on trust levels. Additionally, the user may explicitly identify trust levels in their user profile data  using a client side push service application . In one embodiment, the cloud based information push service engine  aggregates data from user profile data stored on the different user computer systems ,  of the user.","Each version of the push service application  also stores in user profile data  a tracking history of the user. Some examples of events, people and things tracked in the tracking history are locations visited, transactions, content and real things purchased, and people detected with whom the user has interacted. If electronically identified friends (e.g. social networking friends) are registered with the push service application  too, or they make information available to the user or publicly through other applications , the push service application  can use this data as well to track the content and social context of the user.","As discussed further below, the dynamic printed material application  may access one or more search engines  for accessing information for identifying a printed content selection and a printed content item including it as well as related virtual data . Examples of resources which may be searched for identification and pertinent virtual data are illustrated as publisher databases  and printed content related resources  indexed for Internet searching. For example, a general purpose search engine like Bing\u00ae or Google\u00ae may be accessed as well as a search engine for the Library of Congress, university libraries or publisher databases made available to the public or on a subscription basis as may be identified in user profile data. Publishers may have pointers to virtual content in their databases  as publishers may have a business model for encouraging virtual content to be developed for their print material. Additionally, entities not associated with the publishers or who wish to maintain their own data resources may wish to make virtual content available through their own websites which are Internet indexed resources. From searching on information derived from image data of the printed content selection and the printed content item containing it, data fields in metadata  for a printed content selection can be filled with values.  discussed below provides an example of a printed content selection metadata record.","One advantage of the technology is the ability to update previously published material which was printed without any plan for virtual augmentation. As discussed below, a user may be requested to view printed version identifying data on the printed content, for example a title page of a book or newspaper or a table of contents of a magazine. Other examples of version identifying data are standardized identifiers, an example of which is the International Standard Book Number (ISBN) for books. The ISBN number on the book identifies data such as a language group, publisher, title and edition or variation of the book. For periodicals, the International Standard Serial Number (ISSN) identifies the title of a periodical and a Serial Item and Contribution Identifier (SICI) is a standard used to identify specific volumes, articles or other identifiable parts of the periodical. For example, the ISSN may identify a periodical, for example the Journal of Head Mounted Displays and a SICI identifies an article by bibliographic items, some examples of which are title, volume and number, date of publication, beginning and ending pages and content format, e.g. TX for printed text. Other content formats may indicate web publication and audiovisual formats. The image data from the outward facing cameras or text converted from image data of the viewed identifying data is sent to one or more search engines .","Discussed below are methods for improving readability of printed material by creating a virtual version of the printed material to satisfy readability criteria  which may be stored as rules for execution of a rule engine of the dynamic printed material application . Some examples of readability criteria are comfortable reading position criteria and visibility criteria. Some examples of comfortable reading position criteria are an angular position criteria from a respective reference point of a see through display for each eye included in the see-through, mixed reality display device system, a depth distance from the see through display for each eye, and an orientation of content data in the field of view of the display device. An example of a reference point is an optical axis  of the display typically located about the center of the display and which approximates an alignment with the user's pupil when the user is looking straight ahead at an object. If printed material is too off to one side, that can result in neck strain. Depth distance criteria can indicate reading material is too close or too far away. Orientation of content data can indicate the text or pictures are upside down or off to one side which is not ideal for viewing. Examples of visibility criteria can be the size of the text or pictorial content in the field of view. If too small or too large, the size of the content can be adjusted to a comfortable level.","The criteria may be based on the user's actual eyesight if a prescription has been uploaded, eyesight typical for a user's age or based on average eyesight characteristics for humans.","Once the printed version of the work the user is looking at is identified and the printed content selection is located within it, the dynamic printed material application  can query one or more search engines  to search for virtual content  for the printed content selection based on the printed content item including it. In some embodiments, the virtual content is associated with a work or a work version including the content selection independent of the medium expressing the content. For example, paper or other printable material is an example of a medium. Another medium expressing a work is an electronic display or audio recording.","In some instances, the virtual data  is data specifically generated for appearing in relation to the content selection as laid out in a specific printed version, for example on a particular page of a book or other subdivision of printed material. For example, a publisher may create virtual content for updating a recent version of a textbook with an explanation to be displayed over a page with out-of-date information indicating that there are nine (9) planets and listing the planets. The up-to-date explanation may be an image specifically formatted to overlay the entire page and explains that there are now only eight (8) planets rather than nine (9) and why Pluto does not qualify as a planet. In another example, a publisher, who has the layout of a book stored in its databases can supply interactive games and other content for a book at predetermined positions on a page and for specific pages.","In other examples, virtual content  is tied to a medium independent work or work version. For example, a professor may store her notes she has made at different points in her printed copy of a textbook to be available for any version of the textbook independent of medium. In other words, the content of the textbook is a work. Current, previous and future versions of the textbook are versions of the work. The dynamic printed material application  links each note to a subdivision of the work in a medium independent organization of the work. For example, a note may be linked to a phrase in a particular paragraph which can be identified by executing software instructions for text matching. A paragraph is a medium independent subdivision while a page is dependent on the particular printing or electronic layout. A paperback copy of a textbook with smaller print is a different printed work version from a hardback copy of the textbook in larger print although they contain the exact same version of the textbook content. The professor may allow her virtual notes to be available for storage or streaming, at her discretion, to students who take her class or past students by granting permission and access to them.",{"@attributes":{"id":"p-0092","num":"0091"},"figref":"FIG. 4A","b":["210","212","214","216","218","220","222","220","222","216","210","218","212","212","214"]},"For example, the poem \u201cBeowulf\u201d is a work. The original old English form of the poem is a work version, as would be a version which has substituted modern English terms for some of the words. Another example of a version would be a French translation. Another example would be the original old English poem footnoted with comments. A printed version identifier  may identify a printed version of the poem on one or more sheets of vellum maintained in a library. This printed version would also have the work version identifier for the original old English form and the work identifier for Beowulf associated with it. A different printed content item version identifier  identifies an anthology of English literature which has printed the version of Beowulf footnoted with comments beginning on its page . This different printed version has a different printed content item version identifier  and work version identifier than the original old English form of the poem, but has the same work identifier. For content within the anthology version of the poem selected by a user, the position data of the printed content selection is in terms of page . In this instance, likely, the work version position data  and the work position data  indicate the same stanza.",{"@attributes":{"id":"p-0094","num":"0093"},"figref":"FIG. 4B","b":["211","213","215","212","216","220","211","213","215","80","16","24","212","215"]},"Publishers may provide access to their datastores of copyrighted works for identification purposes and as a reference for the layout of the work, work version or printed version for developers of virtual content. By being able to access the layout of the works, particular work versions and particular printed content item versions, developers can create virtual content  for medium independent and medium dependent versions of a work. As illustrated, the databases ,  and  and the virtual content  may cross-reference each other.","For works not subject to copyright, datastores under control of libraries, particularly those with large collections like the Library of Congress, other national libraries, universities, and large public libraries, and book compilation websites like Google Books\u00ae and sites maintained by universities may be searched for copies of a work, a work version or a printed content version for layouts to which to reference position data , , .","Embodiments of methods for the technology and example implementation processes for some of the steps of the methods are presented in figures below. For illustrative purposes, the method embodiments below are described in the context of the system embodiments described above. However, the method embodiments are not limited to operating in the system embodiments described above and may be implemented in other system embodiments.",{"@attributes":{"id":"p-0098","num":"0097"},"figref":"FIG. 5","b":["192","202","190","302","202","304"]},"In step , the dynamic printed material application  determines a task for the printed content selection based on physical action user input, and in step , performs the task. Virtual data related to the printed content selection is displayed in accordance with the task in step .","Some examples of tasks are an interactive task which displays and updates interactive virtual content, e.g. games, responsive to user input, a Snippet\u00ae tool which allows a user to select printed content and send it to another user via a messaging application like email, instant messaging or Short Message Service (SMS), an annotation application, a language translation application, a search task, a bring up-to-data application, a definition application, a follow-me application which creates a virtual version of printed content which the user can manipulate in his or her field of view while no longer looking at the actual printed content, a readability application for improving visibility of content and comfort during reading, and a redaction and refresh application which generates an unmarked version of printed content. For example, a marked up version of printed content may include underlining, scribbles, and notes in the margin which make the content almost unreadable. For example, a user having only an abridged version of a work may define a task which fills in the content deleted. Another example is a restore application in which a user identifies missing pages, and they are displayed. As mentioned above, a user can define tasks as well.",{"@attributes":{"id":"p-0101","num":"0100"},"figref":"FIG. 6","b":["312","202","2","200"]},"In step , a query is formulated based on the one or more version identifying sections and sent in step  to a search engine for a printed content item version identifier. The dynamic printed material application  receives a printed content item version identifier in step . Optionally, in step , responsive to verifying the identity of the printed content item, the dynamic printed material application  receives a medium independent work identifier and any applicable medium independent work version identifier. The dynamic application  may also receive a work identifier and work version identifier by using the printed content item version identifier  as an index into publisher databases  or Internet indexed resources .",{"@attributes":{"id":"p-0103","num":"0102"},"figref":"FIG. 7A","b":["196","322","202","324","120","326","202"]},{"@attributes":{"id":"p-0104","num":"0103"},"figref":"FIG. 7B","b":["332","202","334","120","336","202"]},{"@attributes":{"id":"p-0105","num":"0104"},"figref":"FIG. 7C","b":["342","202","344","120","346","202","348","202"]},{"@attributes":{"id":"p-0106","num":"0105"},"figref":"FIG. 8A","b":["202","352"]},"The dynamic printed material application  in step  receives user input selecting define gesture, and in step , receives user input selecting a task or subtask from the menu. The outward facing cameras  in step  capture image data of a gesture performed by the user of which the dynamic printed material application  is notified and in step , the dynamic printed material application  associates the gesture as a request for the task or sub-task selected in the menu.","Some printed material like books and periodicals may be printed with a layout including designated spots for virtual content. For example, next to a photograph with a marker with metadata identifying the photograph and related virtual content or data may be a space of predetermined dimensions where the related virtual data fits. The space may also have a marker, e.g. an RFID tag or an IR marker, identifying the virtual content to display there. However, even for content pre-printed for augmentation by virtual data, a user may activate a task such as a search task and receive data for which the page has not been preformatted. The software executing in the computing environment  on the display device system , the remote computer system  or both determines where to place the virtual data. A user may also designate placement through physical action. For example, a user may gaze at virtual data for a duration and then gaze at a blank spot on a sheet or a page. In another example, a user may point to a virtual object with a finger and drag the finger to another spot on the sheet or page.",{"@attributes":{"id":"p-0109","num":"0108"},"figref":"FIG. 8B","b":["202","353","202","202","355","357","202","202","359"]},"If the interline position is not suitable, in step , the dynamic application  determines whether the virtual data content fits any margin positions and still satisfies visibility criteria. If one or more satisfactory margin positions are available, the dynamic application  selects a satisfactory margin position closest to the printed content selection in step . If a satisfactory margin position is not available, the dynamic printed material application  formats the virtual content into one or more sections having the layout characteristics of the current section in step  and in step , displays the one or more sections with the formatted virtual content after the current section in the layout of the printed material. An example of a current section is a page. Layout characteristics for a page as a section include typical page layout settings. Some examples of such settings are margins, page number placement, interline spacing, spacing around pictures, font and font size. Some examples of the layout of the printed material may be a newspaper, a book, a magazine, or a greeting card. In the example of printed material as a book, the one or more sections formatted with the virtual content may be made to appear as additional pages of the book.","In the example of , the virtual data is formatted to appear within the perimeter of the physical printed material. In other examples, a floating position may also be a position option. For example, a margin space may appear to be extended to include a picture linked to a content selection for which annotations already take up the nearest margin space. In another example, a floating explanatory paragraph may appear to pop up perpendicularly out of the page in an interline space near a concept it explains. In the embodiment of  below, a virtual version of the printed content selection may be assigned a floating position linked to a user field of view rather than the printed material itself",{"@attributes":{"id":"p-0112","num":"0111"},"figref":"FIGS. 9A through 15"},{"@attributes":{"id":"p-0113","num":"0112"},"figref":"FIG. 9A","b":["202","362"]},"In one example, the dynamic printed material application  identifies out-of-date sections in a printed content item by sending a search query including a printed content version identifier to a search engine which requests identification of out-of-date sections. Responsive to the search results identifying out-of-date sections, the dynamic application  receives the metadata with the position data in the layout of the printed content item.","A data access identifier like a Uniform Resource Locator (URL) may be included in the metadata for up-to-date virtual data which replaces the out-of-date section. The metadata may be in a standardized format using a markup language like Extensible Markup Language (XML) for interfacing with applications through an application programming interface (API). In the example above, the dynamic printed material application  requests the up-to-date virtual data based on the printed content version identifier and the position data.","When such a data access identifier is received by the publisher's database manager, virtual up-to-date data for substitution of the out-of-date material is sent to the sender of the data access identifier. In step , the dynamic printed material application  retrieves virtual data with up-to-date information for the printed content selection, for example from the publisher's databases , and in step  displays the virtual data with up-to-date information in a position related to the position of the printed content selection. For example, a substitution or floating position can be designated for the up-to-date virtual data. In other examples, the dynamic printed material application  may search Internet indexed resources  for virtual data related to the printed content item as well, and retrieve up-to-data content for the printed content selection from an independent developer of virtual content.",{"@attributes":{"id":"p-0117","num":"0116"},"figref":"FIG. 10A","b":["380","384","382","381","381"],"sub":["1 ","1","1 ","2 "]},{"@attributes":{"id":"p-0118","num":"0117"},"figref":["FIG. 10B","FIG. 10A"],"b":["380","384","384","382","372","386","386","113","387","385","388","381","381"],"sub":["1 ","2 ","1 ","2","1 ","2 "]},"As the see-through, mixed reality display device system is capable of producing virtual objects in three dimensions (3D), the picture of the lion, and the calculator  lie flush on the page as if printed on the page. The 3D hologram  appears to come out of the page.","Another task which readers of printed content can now have performed with the display system  is one or more embodiments of a search task. Publishers and other indexers of content may supplement their stored layouts with a keyword associated with one or more printed content selections which may be used for searching for information related to the printed content selection. As discussed for , printed material may be printed with markers, e.g. invisible RFID or IR tags or a subset of visual data, which identify different printed content selections. An example of a visible marker is a subset of image data which acts as a signature for the image data. For example, as the printed content item is identified, image data sub-elements along intersecting diagonal lines may identify a picture. The marker data is a quick reference which may cut down on processing time in identifying the printed content selection in a layout. One or more keywords may be stored and updated for each marker.",{"@attributes":{"id":"p-0121","num":"0120"},"figref":"FIG. 9B","b":["368","202"]},"In step , the dynamic printed material application  may request and receive at least one keyword associated with the printed content selection from one or more printed content related datastores and in step , formulates a search query based on the at least one keyword received. Some examples of printed content related datastores are the printed content related Internet indexed resources  and the publishers databases . In step , the dynamic printed material application  sends the search query to a search engine, and in step , displays search results in the field of view of the see-through display. See for example, virtual data  of a virtual piece of paper with text search results  at an off-page position along the closest margin to the printed content selection of the male explorer image data which is the basis of the search.",{"@attributes":{"id":"p-0123","num":"0122"},"figref":"FIGS. 10A","b":["383","389","390","391"]},{"@attributes":{"id":"p-0124","num":"0123"},"figref":"FIG. 9C","b":["369","144","144","210"]},"In another example, a visual marker may be stored as structure data , and the object recognition software  identifies the visual marker from image data captured by the cameras  and notifies the processing unit  of a marker identifier which the processing unit may send to a publisher database  or printed content related Internet indexed resources  for one or more associated keywords.","A subdivision as discussed above may be a paragraph or a picture or a stanza. Gaze data allows the system to pinpoint within the subdivision where the user is focused, for example on what word or words within a paragraph or at what object in a photo the user is looking. Keywords can be assigned at a very detailed level. Gaze duration, gaze with blinking, and gestures with finger pointing and voice data for refinement or clarification may be used to select content within a subdivision.","In step , the dynamic printed material application  selects a search task for the printed content selection based on physical action user input data. As discussed above, selection of the printed content item and a search request may be indicated at the same time. In step , the dynamic printed material application  requests and receives at least one keyword associated with the marker from one or more datastores, e.g.  or , and in step  generates a search query based on the at least one keyword associated with the marker. The search query is sent in step  to a search engine, and in step , the dynamic application  causes the display of the search results in the field of view of the see-through display.",{"@attributes":{"id":"p-0128","num":"0127"},"figref":["FIG. 10B","FIG. 10B"],"b":["383","389","390","391","210","202"]},{"@attributes":{"id":"p-0129","num":"0128"},"figref":"FIG. 11A","b":["202","392","394","120"]},"The dynamic printed material application , in step , generates annotation data based on user entry with the virtual key entry input device and, in step , displays the annotation data at a position indicated by physical action user input. In step , the annotation data is stored and linked to the annotation data with a medium independent data version, e.g. a work or work version, of the content in the printed content selection. By storing annotations with a medium independent version, the user can recall the annotations regardless of the particular print layout of the work. Optionally, in step , the annotation data can be linked with the printed content selection in the printed content item as well as this is a version a user is typically using at the current time.","In other examples, instead of a virtual key entry input device, annotation data may be entered via the processing unit, e.g. a mobile device  like in . Also as indicated in the example of , handwritten notes can be made into and displayed as annotation data.",{"@attributes":{"id":"p-0132","num":"0131"},"figref":"FIG. 11B","b":["402","202","212","220","211","213","215","404","406","202","408"]},{"@attributes":{"id":"p-0133","num":"0132"},"figref":"FIGS. 12A","b":["12","12","12"]},{"@attributes":{"id":"p-0134","num":"0133"},"figref":"FIG. 12A","b":["2","113","113","202","704","704","480","410","482","412","414"],"i":["l ","r ","l ","r "],"sub":"1 "},"The dynamic printed material application  identifies the letters in the handwriting based on user handwriting samples stored as structure data  and formats the selected handwritten content as text in a computer standard text data format, and stores the content selection and a printed content selection metadata record . The handwritten content selection may also be marked as an annotation and linked to a list of annotations.",{"@attributes":{"id":"p-0136","num":"0135"},"figref":"FIG. 12B","b":["494","494","113","15","481","483"],"i":["l ","r "]},{"@attributes":{"id":"p-0137","num":"0136"},"figref":["FIG. 12C","FIG. 12A"],"b":["113","113","482","462"],"i":["l ","r "]},{"@attributes":{"id":"p-0138","num":"0137"},"figref":["FIG. 12D","FIG. 12A"],"b":["2","113","113","482","202","412","412","202","412"],"i":["l ","r"],"sub":["2 ","2 ","2 "]},"Besides providing additional information or interactive content to static printed material, a virtual version can be made of the static printed content material to improve visibility of the content due to changing appearance characteristics of the content or position of the content in the display device field of view.",{"@attributes":{"id":"p-0140","num":"0139"},"figref":"FIGS. 13A and 13B"},{"@attributes":{"id":"p-0141","num":"0140"},"figref":"FIG. 13A","b":["432","202"]},"In step , the dynamic printed material application  determines whether the position of the printed material in the field of view satisfies comfort criteria. Some examples of comfort criteria are angle of the printed material, e.g. a book or magazine, and angle of the text determined with respect to reference points of the display device, for example, the optical axes  of the see-through displays. Whether the angle of the printed material is within a comfortable reading zone may also be determined based on estimated gaze vectors and head position data derived from the orientation sensing unit . For example, a reference head position may be a zero degree head position meaning looking straight ahead without looking up or down or left or right at an angle. From this reference zero degree head position, a reading comfortable zone or comfort criteria may indicate a head position not more than  degrees from the zero degree position. A gaze estimation vector of more than  degrees in any direction from an optical axis  may be used as a threshold indicating comfort criteria is no longer satisfied.","The angle or orientation of text determination can identify whether the text is upside down or at another uncomfortable angle. A depth distance may also be determined based on stereopsis applied to the image data of the capture devices  or depth data obtained when the cameras  are implemented with depth sensing capability. If the printed material is outside a depth comfort range, e.g. 1 to 3 feet, generation of a virtual version may be triggered. A user's arm length, which may be available in user profile data , may be a guide for determining a depth comfort range.","If the position satisfies reading position criteria, in step , the dynamic printed material application  returns to step  for a next scheduled check. If the reading position criteria is not satisfied, the dynamic printed material application  determines a new position for satisfying comfortable reading position criteria in step . For example, a new position orients the book to within comfortable angle and distance boundaries of a reading comfort zone. In step , the dynamic printed material application  generates image data of a virtual version of the printed material. The virtual version may be generated based on image data of the printed material. Additionally, the dynamic printed material application  may generate the virtual version based on an electronic version of the printed material accessible by the dynamic printed material application . For example, a newspaper publisher may make an electronic version accessible with a mobile tag printed on copies of its newspapers. The electronic version may have the complete text of the printed material, e.g. the entire day's newspaper, as well as its layout information. In step , the dynamic printed material application  causes the image generation units  to display the virtual version in the see-through display at the new position and in step  returns to step  at a next scheduled check.",{"@attributes":{"id":"p-0145","num":"0144"},"figref":["FIG. 13B","FIGS. 13A and 13B","FIG. 13A"],"b":["442","202","446","452","202","432"]},"If the visibility criteria is not satisfied, then in step , the display printed material application  generates a virtual version with a changed size of the content data to satisfy visibility criteria, and in step  causes display of the virtual version of the content data with the changed size in the see-through display. In step , the dynamic printed material application  returns to step  at a next scheduled check.",{"@attributes":{"id":"p-0147","num":"0146"},"figref":["FIG. 14","FIG. 1B"],"b":["489","484","8","2","490","202","489","484","489"]},"The grandmother  likes to see her grandson's expression when he sees the display of the virtual data such as the jack-o-lantern pumpkins , , , and his reaction to the events and dialogue in the story. However, her grandson is only learning his letters and can't read such a book yet. The grandmother selects a reader role for herself from a menu and assigns her grandson a participant role. To interest children in reading, their direction is focused on the book by displaying virtual data when they look at the book. Display device systems  can detect other devices within a predetermined distance. For example, the display device systems may exchange identity tokens via a Bluetooth, WUSB, IR or RFID connection. The type and range of location proximity transceivers  can be selected to allow connections only within a predetermined distance. Location data such as from a GPS transceiver  or cell triangulation based on wireless transceiver signals  in combination with an application like Bump\u00ae may also be used for identifying devices within a predetermined distance of each other.","The virtual data is activated by eye gaze on a certain portion of the content of the story by the reader. The story is a work, and the content is on page  of the printed content item which is the book . The grandmother holds the book on her lap, and the reading angle does not satisfy the comfort criteria. However, the book is a stage from which virtual data emanates when a participant like the grandson  views it in the field of view of his display device system  and the eye gaze of the reader activates virtual content or data.","The process of  detects the uncomfortable reading position for the grandmother. As discussed further below in , her looking up from the book repeatedly may also trigger a follow-me task. A virtual version of the printed page is projected into the grandmother's eyes so that the virtual page, and any adjusted size of font or contrast for the virtual page, appears at a reading comfortable zone in her display. Her gaze duration on the content triggering the virtual pumpkins in the virtual page also triggers the virtual pumpkins , and to appear for her grandson participant to appear to be floating out of the physical page when he looks at the book. In her display, the grandmother sees virtual pumpkins , and from her perspective. Also due to the process example of , the virtual page is placed so she can comfortably read it. Another criteria for placement is not blocking a participant in the field of view as space allows. Appearance layout changes discussed below can also be used to accommodate keeping participants in view.","If the physical book is still within the field of view of the grandmother's display device, the dynamic application  takes action responsive to the grandmother's physical action with respect to the physical book, e.g. turning a page, as well as with respect to the virtual version page currently displayed.",{"@attributes":{"id":"p-0152","num":"0151"},"figref":"FIG. 15","b":["502","202","2"]},"In step , the dynamic printed material application  determines whether an electronic version of the printed content item including printed material the user is viewing is available. If so, the dynamic application  can load at least a section of the printed content item into memory to be available as the user reads furthers and advances in the content embodied in the printed material. If an electronic version is available, the dynamic application  in step  displays image data including content of a printed content selection at a position in the field of view independent of the position of the printed content item. Particularly, in an example where the user moving his or her head position back and forth has triggered the follow me task, the dynamic application  selects a section of the printed content item the user has been focusing on as the printed content selection if gaze duration data or gesture or voice data has not affirmatively indicated a content selection. In step  display of the image data responsive to physical action input of the user with respect to the virtual version is updated. If the printed content item is still within the field of view, the dynamic application  responds to both physical action user input with respect to the printed item version and the virtual version.","Physical action user input may include gestures interacting with the virtual object such as pushing a virtual page around in the field of view, and pinning it to a \u201cposition\u201d outside the field of view which is tracked by the dynamic application . If the user moves her head to look at the \u201cposition,\u201d the application  causes the virtual page to reappear. For example, a cook following a recipe may use hand motions to push a recipe in and out of view as she performs the different steps of the recipe.","Optionally, in step , the dynamic application updates display of the image data responsive to layout change requests of the user. An example of a layout change request is to make the virtual content appear on a transparent sheet of paper and increase the spacing between the text so the user can see through the virtual content. Voice commands may request transparency. Or the user, may increase separation between portions of the text to create a see-through window in which to look at what he or she is working on. A stretching gesture with both hands pulling a virtual page in opposite directions may adjust the spacing. A mechanic working under a car may have a transparent version of a manual page with a window he has inserted by a gesture of a flat hand pushing down a section of the page, and then pushing up another section of the page so he can see in between. Another example of a gesture could also be two flat hands pushing the page portions, e.g. paragraphs, apart.","In the case where an electronic version of the printed content item is now available, in step , the dynamic application  stores image data of the section of printed content selection currently in the field of view, and in step  outputs instructions, e.g. audio or projected by the display device, requesting the user to look at one or more sections of the printed content item including content to be available in the current reading session. For example, before going under the car and starting work, the mechanic may look at the pages in the manual for the car pertinent to his repair. The dynamic application  in steps  and  captures and stores image data of each of the one or more sections while the respective section is in the field of view of the outward facing cameras and then proceeds to step . In one sense, the dynamic application  makes a photocopy of the pages looked at by the user.","Tasks performed for a physical content selection can also be performed for the virtual version. Resulting virtual data from any task performance can be stored and viewed later when again looking at the physical content selection as well.",{"@attributes":{"id":"p-0158","num":"0157"},"figref":["FIG. 16","FIG. 3","FIG. 16","FIG. 16","FIG. 16"],"b":["12","54","800","800","802","800","804","804","805","807","806","800","800","808","810"]},"Device  may also contain communications connection(s)  such as one or more network interfaces and transceivers that allow the device to communicate with other devices. Device  may also have input device(s)  such as keyboard, mouse, pen, voice input device, touch input device, etc. Output device(s)  such as a display, speakers, printer, etc. may also be included. All these devices are well known in the art and need not be discussed at length here.","As discussed above, the processing unit  may be embodied in a mobile device .  is a block diagram of an exemplary mobile device  which may operate in embodiments of the technology. Exemplary electronic circuitry of a typical mobile phone is depicted. The phone  includes one or more microprocessors , and memory  (e.g., non-volatile memory such as ROM and volatile memory such as RAM) which stores processor-readable code which is executed by one or more processors of the control processor  to implement the functionality described herein.","Mobile device  may include, for example, processors , memory  including applications and non-volatile storage. The processor  can implement communications, as well as any number of applications, including the applications discussed herein. Memory  can be any variety of memory storage media types, including non-volatile and volatile memory. A device operating system handles the different operations of the mobile device  and may contain user interfaces for operations, such as placing and receiving phone calls, text messaging, checking voicemail, and the like. The applications  can be any assortment of programs, such as a camera application for photos and\/or videos, an address book, a calendar application, a media player, an internet browser, games, other multimedia applications, an alarm application, other third party applications like a skin application and image processing software for processing image data to and from the display device  discussed herein, and the like. The non-volatile storage component  in memory  contains data such as web caches, music, photos, contact data, scheduling data, and other files.","The processor  also communicates with RF transmit\/receive circuitry  which in turn is coupled to an antenna , with an infrared transmitted\/receiver , with any additional communication channels  like Wi-Fi, WUSB, RFID, infrared or Bluetooth, and with a movement\/orientation sensor  such as an accelerometer. Accelerometers have been incorporated into mobile devices to enable such applications as intelligent user interfaces that let users input commands through gestures, indoor GPS functionality which calculates the movement and direction of the device after contact is broken with a GPS satellite, and to detect the orientation of the device and automatically change the display from portrait to landscape when the phone is rotated. An accelerometer can be provided, e.g., by a micro-electromechanical system (MEMS) which is a tiny mechanical device (of micrometer dimensions) built onto a semiconductor chip. Acceleration direction, as well as orientation, vibration and shock can be sensed. The processor  further communicates with a ringer\/vibrator , a user interface keypad\/screen, biometric sensor system , a speaker , a microphone , a camera , a light sensor  and a temperature sensor .","The processor  controls transmission and reception of wireless signals. During a transmission mode, the processor  provides a voice signal from microphone , or other data signal, to the RF transmit\/receive circuitry . The transmit\/receive circuitry  transmits the signal to a remote station (e.g., a fixed station, operator, other cellular phones, etc.) for communication through the antenna . The ringer\/vibrator  is used to signal an incoming call, text message, calendar reminder, alarm clock reminder, or other notification to the user. During a receiving mode, the transmit\/receive circuitry  receives a voice or other data signal from a remote station through the antenna . A received voice signal is provided to the speaker  while other received data signals are also processed appropriately.","Additionally, a physical connector  can be used to connect the mobile device  to an external power source, such as an AC adapter or powered docking station. The physical connector  can also be used as a data connection to a computing device. The data connection allows for operations such as synchronizing mobile device data with the computing data on another device.","A GPS receiver  utilizing satellite-based radio navigation to relay the position of the user applications is enabled for such service.","The example computer systems illustrated in the figures include examples of computer readable storage devices. Computer readable storage devices are also processor readable storage device. Such devices may include volatile and nonvolatile, removable and non-removable memory devices implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Some examples of processor or computer readable storage devices are RAM, ROM, EEPROM, cache, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, memory sticks or cards, magnetic cassettes, magnetic tape, a media drive, a hard disk, magnetic disk storage or other magnetic storage devices, or any other device which can be used to store the desired information and which can be accessed by a computer.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1B"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1C"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 4A"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 4B"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 7A"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 7B"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 7C"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 8A"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 8B"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 9A"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 9B"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 9C"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 10A"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 10B"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 11A"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 11B"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 12A"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 12B"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 12C"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 12D","FIG. 12A"]},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 13A"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 13B"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 17"}]},"DETDESC":[{},{}]}
