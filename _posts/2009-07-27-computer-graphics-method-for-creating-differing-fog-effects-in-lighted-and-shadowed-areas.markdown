---
title: Computer graphics method for creating differing fog effects in lighted and shadowed areas
abstract: A method for providing fog or fading due to atmospheric effects during computer animation of an image. The method includes determining a lighting value for a scene's or an image's pixels based on the positions of the pixels relative to a light source. The method includes, for each of the pixels, setting a fade in start distance for each pixel measured from a camera location so as to define when fog is added, e.g., when to fade a pixel's color by blending this color with a fog color. The fade in start distances are set based on each pixel's lighting value, whereby pixels with smaller lighting values or shadowed pixels begin to fade first while highlighted pixels fade with greater distances. The method includes adding fog to the image starting at the fade in start distances for each of the pixels and then rendering the image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08411089&OS=08411089&RS=08411089
owner: Disney Enterprises, Inc.
number: 08411089
owner_city: Burbank
owner_country: US
publication_date: 20090727
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY OF THE DESCRIPTION","DETAILED DESCRIPTION OF EMBODIMENTS"],"p":["1. Field of the Invention","The present invention relates, in general, to computer graphics such as graphics systems, hardware, and software used for image generation including interactive and\/or frequently changing\/update image generation for video games and other applications, and, more particularly, to computer graphics systems and methods for use in interactive video games and other applications that provide improved fog effects.","2. Relevant Background","Recently, the video game industry has been enjoying tremendous growth with increasing demand for video games with visually appealing, captivating, and artistic animation. Video games are widely played using video game systems and a monitor such as television and using a computer and its hardware and monitor such as to play multi-player online or Web-based games. More recently, video games and similar applications have been provided in many more setting such as in vehicles and on handheld\/portable electronic devices such as cellphones, handheld gaming devices, personal digital assistants. It is expected that the demand for video games with desirable animation will only continue to grow in the coming years.","Computer graphics systems are used in many video games and other animation applications to render animation in a responsive or interactive manner. Graphics system designers generally attempt to provide realism or some artistic quality in their graphics systems by closely modeling a three dimensional (3D) virtual world based on the real world. For example, graphics systems allow game designers to place 3D objects such as mountains, houses, characters, animals, and so on in a scene that is to be animated during game play. Interfaces also allow the game designers and animators to select colors for each of these 3D objects and even for each pixel within the animated object. Further, the animator may select where to place a light source, such as the Sun, within the animated scene and select a location of a camera (or eye-coordinate) to set a point of view for the scene. As a result, the animator is able to create a scene with carefully colored\/painted objects that are highlighted in a desired manner with lighting and shadows, and the scene is viewed from a location chosen by the game designer (or that changes as the player moves a character and with them a camera or eye-coordinate).","In the real world, object become more difficult to see with distance due to atmospheric effects, and, if these effects are not taken into account, computer-generated images have a distinctive, clear quality that is unrealistic. To address this difference between the real world and animated worlds, graphics systems are used to change clarity of the images due to atmospheric effects such as fog, smog, mist, smoke, pollution and the like, with all of these effects generally lumped together and called \u201cfog\u201d or a \u201cfog effect\u201d in computer graphics. Adding fog to a 3D scene can enhance realism, provide ambiance, and obscure artifacts sometimes caused when distant geometry comes into view.","Various mechanisms have been used in graphics systems to simulate fog or provide a fog effect. Essentially, fog is implemented by blending the color of objects in a scene with a chosen fog color based on the depth of an object in a scene or its distance from the viewpoint or camera. As objects grow more distant, their original color blends more and more with the artist\/designer-selected fog color. This blending of the two colors creates the illusion (typically, on a pixel by pixel basis) that the object is being increasingly obscured by fog or other atmospheric effects present in the scene or simply based on distance as is the case in the real world. For example, gray may be chosen for the fog color and a pixel within an object may have a yellow color. As the object's pixel becomes more and more distant from the camera or viewpoint (e.g., location of a video game character) the amount or percentage of gray is increased until the pixel is rendered using only gray, which causes the pixel to fade out of view in the animation.","Hence, the range-based fog effect method determines how far the viewer is from the object (e.g., determines a z value or distance from an eye or camera coordinate to an object) and, depending on this z value, blends the object to a certain degree with the color of the fog (e.g., averages the pixel's color with the predefined fog color). Thus, objects appear to fade away in fog the farther away they are from the viewer. The game designer or programmer also may specify, such as via a game interface or application programming interface (API), how much visibility drops off with distance, such as may be used to indicate a thickness or density of fog, smoke, or other atmospheric effect. The visibility drop off or fade value may be referred to as the \u201cfog scale\u201d herein and may be defined in a linear, an exponential, an exponential-squared, or other manner. While the fog scale can be used to define the effect as the viewer's distance changes, the fog itself typically has a uniform density or is homogeneous throughout the animated scene. However, four values generally are used to create existing fog effects, i.e., pixel color, fog color, a z value (or distance from camera\/viewpoint), and a fog scale or blending factor.","The blending effect may be carried out in different ways such as using pixel fog (or table fog) or vertex fog. The blending effect under the pixel fog model is calculated on a per-pixel basis in the graphics system's device driver. The vertex fog model makes calculations in the pipeline when transformation and lighting calculations are performed. The vertex fog model calculates the blending for each vertex in a polygon and then interpolates the effect across the face of the polygon. One extension is layered fog. Layered fog utilizes a heterogeneous approach to fog density. Fog is created in layers, with each layer having its own density. This is a more costly processing approach, though, since calculations must be made for each layer, and these calculations must be blended together for a final result. This effect can most easily be pictured if the layers are arranged by decreasing density along the y axis, such that the fog is most dense near the ground. For example, a swamp where heavy fog is present on the ground and the fog dissipates the further up it moves. One important distinction is that fog can either be applied on a per pixel basis or on a per vertex basis and then interpolated to decide the values for each pixel. In each of these methodologies, the fog effect or amount of blending\/fading is typically tied to the z value or distance from the viewpoint\/camera, and pixels at a particular distance from the camera (or with a like z value) are generally treated identically throughout the animated image with application of an identical blending factor (or fog scale). For example, two trees on opposite sides of a mountain that are the same distance from a viewpoint\/camera in a scene would have their pixels blended with a fog color in an identical manner.","Traditionally, in games and other applications using computer-generated images or computer graphics, fog is introduced at a set distance from a camera or viewpoint with all aspects of the animation and\/or generated image affected uniformly by the fog effect. Fog is used to more accurately introduce depth to an animated scene with more distant objects being less clear than nearer objects. It was recognized that fading or applying a fog effect solely on distance does not provide a true or accurate representation of what actually happens. In the real world, objects that are not as well lighted or are in shadow tend to have their details lost first or more quickly while the well lighted objects and their details (or highlights) tend to remain visible. For example, a faraway building may cast a relatively large shadow, and the portion of the building that is in the sunlight (or well lighted) likely will have many details or highlights readily visible while objects or details in the cast shadow may not be visible all or at least are more difficult to see (e.g., are effected more by fog or atmospheric effects).","It was determined that it may be desirable to provide a computer graphics system and fog effect method that better models this real world effect, and it was also recognized that there are artistic merits to such a shadow-first fog effect (or highlighted-last fog effect). A fog effect method and computer graphics system that implements such a method are provided that allows fog to be introduced so as to allow details to be lost first in shadowed areas or on shadowed objects (or areas with a lower \u201clight value\u201d) while the details in lighted areas or highlights of non-shadowed objects are lost second\/later and\/or more slowly. This may be considered an artistic technique as it may differ somewhat from physical accuracy but provides a painting or illustrating technique that provides a way to capture a particular physical effect. In some embodiments, the shadow-first fog effect is applied uniformly to the animation (e.g., to replicate fading of an image with distance such as by use of blue or other background color for a fog color), while in other embodiments, the shadow-first fog effect is used to introduce a more localized fog effect as may occur with the addition of a cloud, ground fog, smoke, or the like.","Briefly, in some embodiments, a fog effect method is provided that includes inserting or adding a fade in (or fog effect) start point and a fade in end point for pixels based on their light value, and this fade in start point is nearer to the viewpoint (eye location-coordinate) or camera position than the fade in start point for a fully lit pixel (e.g., a pixel with a light value of 1 (or normal to a light source) may have a fade in start point of 10 distance units from the viewpoint while a pixel with a light value of 0 (or fully shadowed) may have a fade in start point of 5 distance units from the viewpoint). The fog effect may be applied using any of a number of conventional techniques starting at these differing start points and being completed at the same or, more typically, differing fade in end points. In one case, the fog effect is applied linearly between start and end points while in other embodiments other techniques or fog scales are used such as exponential, exponential squared, or the like. In some embodiments, the fog effect method also includes modifying the light or lighting value to account for shadowing by an object positioned between the light source and the surface or object containing a particular pixel. In such cases, for example, a pixel may have a lighting value of 0.8 (on a 0 to 1 scale) but a determination may be made that an object is blocking the light from the source that would result in the pixel having its lighting value reduced to zero (or to a value between 0 and the original value of 0.8) prior to performing the fog effect calculation.","The fog effect method may involve the following steps: finding a lighting value for each pixel, determining a distance from a camera for the pixel, determining a fade in start and a fade in end distance for the pixel based on its lighting value, retrieving a color of the pixel without fog and a fog color, performing a fog effect calculation to determine the contribution of each of the two colors for the fog to achieve a fog effect using the fade in range determined for the pixel, and then mixing or blending the two color contributions to determine what color to use in rendering the pixel. Such a process is performed for each pixel, and then animation or rendering of the image may proceed with the pixel color information from the fog effect method.","More particularly, a method is provided for creating fog or a fog effect during computer animation of an image. The method may be performed by a variety of software, firmware, and hardware components that may be generally thought of as a graphics system with one or more processors (such as CPUs, GPUs, and the like) that run or use fog effect modules or similar devices to perform the desired functions or steps of the method. The method may include determining a lighting value with a fog effect module for each pixel of an image that is to be animated, and the lighting values may be provided to the fog effect module by another component\/software of the graphics system and is determined based on the location\/orientation of the pixels relative to a light source placed in the image or scene by an animator. The method may further include, for each of the pixels, setting a fade in start distance that is measured from a camera or eye-coordinate location. The fade in start distance defines when fog is to be added (e.g., when to start to fade a pixel's color by blending this color with a fog color).","Significantly, the fade in start distances are selected or set based on the lighting values associated with each of the pixels, whereby pixels with smaller lighting values (e.g., shadowed pixels) begin to fade first while highlighted pixel and details of the image associated with such pixels fade second and\/or more slowly. The method may then include adding fog to the image with the fog effect module (e.g., a pixel shader including such a module) starting at the fade in start distances for each of the pixels and then rendering the image using the fog-added or fog-faded pixels.","The setting of the fade in distances may include retrieving from memory a fade in start distance for pixels associated with a minimum lighting value and a fade in start distance for pixels associated with a maximum lighting value, and then the determined lighting value for each pixel is compared to these values (e.g., a pixel with a lighting value midway between the minimum and maximum may be assigned a fade in start distance that is midway between the two retrieved fade in start distances when a linear calculation\/comparison is performed). The fade in start distance for pixels with a minimum lighting value may be less than the fade in start distance for pixels with a maximum lighting value (e.g., about one third to one half of the maximum lighting value fade in start distance or the like). The method may also include retrieving from memory fade in end distances for pixels with minimum and maximum lighting values, and then setting a fade in end distance based on these values and a comparison of a pixel's lighting value to the minimum and maximum values (e.g., in a linear comparison or calculation implementation, a lighting value for a pixel that is seventy-five percent of the maximum (assuming the minimum value is zero) may be assigned a fade in end distance that is seventy-five percent of the way or range between the end distances of the minimum and maximum lighting values).","The adding of the fog to the image may include blending a pixel color with a fog color based on the pixel's distance from the camera relative to the fade in start and end distances set for the pixel based on its lighting value, with the blending being performed using linear, exponential, exponential squared, or another fog adding technique. In some embodiments, the determining of the lighting value may include determining whether an object is blocking the light source, and, if so, a pixel's lighting value may be modified to account for the amount of light blocked by the object (e.g., if the pixel's lighting value is 0.8 on a 0 to 1.0 scale without the blocking object but 0.5 with the object between the light source and the pixel location the lighting value may be changed or set to 0.5 prior to setting the fade in start distance at which to begin blending the pixel color with the fog color).","Briefly, embodiments of fog effect methods (or animation with such fog effects) and graphics systems for generating animation with improved or shadow-first fog effects are described in the following paragraphs. The fog effect methods and graphics systems are adapted to introduce fog such that it accounts for a pixel's location in a scene relative to one or more light sources (e.g., what is its light value) and, in some cases, such that it accounts for shadowing by objects. The fog effect methods allow an animator to set a first distance from a camera or viewpoint at which they wish a fog effect to be started for portions of the scene that are shadowed and a second distance (that is greater than the first) from the camera or viewpoint at which they wish the fog effect to be started for portions of the scene that are fully lighted (e.g., for surfaces normal to the light source). When the fog effect is then applied, details are retained in lighted or highlighted objects as compared with details in shadowed objects (e.g., a tree on a shadowed hillside may more quickly fade from view as compared to a rock on a sunny location even though the tree and rock are the same distance from the camera or viewpoint used during animation).","Prior to turning to the new fogging method with its differential treatment of pixels based on their lighting values, it may be useful to generally describe the problem context and prior fogging techniques.  illustrate a relatively simple computer-generated image  as may be provided or rendered by operation of a computer such as via rendering software and\/or by a graphics systems using hardware\/software such as video graphics cards and\/or graphics processing units (GPUs) with fog effect hardware\/software (e.g., a fog effect module and supporting components such as distance or z value determination devices). The image  includes a number of 3D objects including a field  with a hill or mountain  protruding upward. A pair of foreground objects is provided on the hill  in the form of a circular cross-section column  extending upward from the hill  and in the form of square cross-section column .","As shown in , the animator or scene designer has positioned a camera or viewpoint (eye-coordinate)  at a foreground position, and the two columns ,  as measured by lines ,  are approximately the same distance from the camera . In other words, the z value or distance from the camera is considered equal for the following discussion for the two columns (with d=d). The image  has also been provided with a light source  (shown as the Sun for simplicity's sake but often the source is not visible in an animated image and, similarly, the camera  is not shown in a displayed graphics image). The light source  provides light  that highlights or lights the objects in the image , and this lighting  and positioning of light source  also produces shadowing.","As shown, again in a rather simplified manner to ease explanation, the hill  may be thought of as having a fully lighted or highlighted side  and a shadowed or non-lighted side  that are divided roughly with line . In practice, there would likely be many gradations of lighting across the hill , but, for the purposes of this discussion, it is useful to discuss lighting more basically with the understanding that assignment of lighting values may be much more complex and may be implemented in any of various well-known ways presently used or developed later for computer animation.","Briefly, a lighting value may be assigned to each pixel in an image  based on the orientation and location of the pixel relative to one or more light sources . For example, a pixel may be determined to be normal or on a surface normal to the source , and this pixel may have a highest lighting value indicating it is fully lighted or highlighted such as, but not limited to, a lighting value of 1.0 on a 0 to 1.0 lighting value scale. Pixels on surfaces that are not normal to the light sources rays  will have a lighting value indicative of the degree of normalcy to the rays  ranging from 0 up to (but less than) 1.0 in this exemplary lighting value range.","Turning to , the highlighted side  of the hill  may be considered or determined (by the graphics system that produced the image ) to have a maximum lighting value (such as 1.0). In contrast, the shaded or shadowed hillside  may be considered or determined to have a lighting value that is less such as 40 to 60 percent of the maximum or the like (such as 0.5). Now, foreground object or column  includes detail components\/objects , , and a highlighted or lighted side  separated from a shadowed or non-highlighted side . Detail  is on the lighted side  while detail  is on the shadowed side . The column  is positioned on the lighted side  of the hill , and, hence, its lighted side  and detail  may have pixels with maximum lighting values (such as 1.0), but the shadowed side  and detail  may have pixels with a less than maximum values (such as about that of the shadowed hillside  or somewhat higher\/brighter values such as 0.6 with a range or gradation typically occurring for the pixels on the side  from the line  to the farthest points on the column  from the line  and light source ). The column  in contrast is positioned in a shadow on side . Hence, the column  may have a side  with detail  that have lighting values similar to that of the hillside  pixels (such as 0.5) while the side  with detail  and separated from lighted side  by line  (for simplicity sake in this drawing) would have pixels with lower or darker lighting values than the hillside  and those of column side  (so, a value of 0.2 may be appropriate such that detail  is faded or hazy in image  even prior to application of a fog effect).",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 3","b":["300","304","310","320","330"],"sub":["1","2"]},"In use of such a fog effect technique or process in a graphics system, the graphics systems would process each pixel, prior to rendering a scene at a current time, to determine its current distance from the camera or viewpoint . If the distance is less than the first set distance, d, less than the fade in start distance, the pixel is in the region  (within a radius, dfrom the camera) relative to the camera , and no fog is present or no effect is applied such that the graphics system determines and uses the pixel color (i.e., 100 percent pixel color with no blending). If the distance from the camera or z value is greater than the fade in end, d, or falls within the solid fog color region , the graphics system retrieves the fog color and uses this color for rendering the pixel (i.e., 100 percent fog color with no blending). However, when the graphics system determines that the distance from the camera or z value is between the first and second distances, dand d(or between fade in start and end points within the pixel color\/fog color blend region ), the graphics system uses the distance from the camera or z value to perform a fog effect calculation and mixes the contribution determined for the pixel color and for the fog color.","Regardless of the fog effect technique used, the blending is performed solely based upon the distance from the camera or z value. In other words, the blending or fog effect may be linear, exponential, exponential squared, or another blending method, but, in each case, the graphics system performs the blending determination based on the z value or distance and not based on the pixel's lighting value. For example, if the fog scale or blending factor is determined in a linear manner or with a linear blend and the distance from the camera is midway between points  and  (or half way between fade in start and end), the graphics system may determine that each of the colors should contribute 50 percent to the blended color used for the pixel (e.g., the pixel color may be red and the fog color may be gray, and the blend would be 50 percent red and 50 percent gray at rendering of the pixel).",{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 4","FIG. 3","FIGS. 1 and 2"],"b":["110","114","430","114","116","430","118","116","118","130","140","410","130","114","420","140","118","410","420","134","144","136","146","132","142","133","143"]},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 5","b":["500","500"]},"As shown, a camera or viewpoint position  is provided or defined for a scene to be animated, and distance from the camera (or a z value) is determined along a line between the camera  and a pixel of an object positioned within the scene. In use, an interface or API may be provided to an animator, game designer, or the like to allow them to set or define four distance values, dto d, that are used to determine how to introduce fog into the scene, and, significantly, these values are based upon lighting values.","Specifically, a first distance, d, may be defined as shown at  along the distance from camera line  to define when an object (or pixel within such object) that is highlighted or normal to a light source or otherwise has a maximum or highest lighting value (e.g., 1.0 on a 0 to 1.0 scale or the like) should begin to have the fog effect applied (i.e., a fade in start value for maximum lighting values such as values of 1.0) during rendering of animated image by a graphics system. A second distance, d, may then be defined manually (such as by the designer moving a slide bar or entering a blending zone length) or automatically (such as by the fog effect module or API assuming a fixed blending zone as measured between distances dand dfor highlighted pixels). During animation, the graphics system may determine a distance for a pixel that falls between the fade in start and fade in stop distances, dand d, for fully lit pixels and determine that it has a lighting value of 1.0. In such a case, the pixel is in the blending or fog effect zone  in which the pixel is rendered using a blend of pixel color and fog color such as a linear, exponential, exponential-squared, or other method of blending based on the location of the pixel distance between first and second distances.","Likewise, the animator or other user of the fog effect method  may also set a third camera distance, d, that corresponds to a fade in start point or distance as shown at  for pixels determined to have a minimum lighting value (e.g., 0 on a 0 to 1.0 scale or the like). The third distance, d, is typically set within the graphics system (or its API or other software) to have a value that is equal to or, more preferably, at least some offset amount less than the first distance, d, such that fade in starts earlier (or at a smaller distance) for pixels with a minimum lighting value than pixels with a maximum lighting value (e.g., an offset of at least 0.5 to 2 distance units or the like). Hence, if a user indicates that the fade in start for full lighting, d, is equal to 30 distance units than the maximum setting of the fade in start for no lighting or full shadow, d, would be set programmatically (in many cases) to be 30 distance units or less some starting minimum offset value (e.g., to start it at 28 distance units or the like). A fourth distance setting, d, as shown at  is also set either manually within an API or the like by the user or by automatically\/by default (e.g., a present blending zone length may be defined within the software) based upon the setting of the third distance, d. The fourth distance, d, defines where fade ends for pixels with minimum lighting values such that the pixel would be colored using the fog color (or no pixel color contribution in the blending).","Hence, as shown with graph , the fog effect method involves determining\/retrieving a distance from the camera for a pixel and also determining\/retrieving a lighting value for that pixel. Then, using this information, the fog calculation may be performed to determine the contribution of the color of the pixel and the color of the fog, and blending may be performed using a conventional technique such as linear blending or the like. Roughly, the fog effect graph  shows that there are three zones including a no fog zone  in which a pixel regardless of its lighting value will not experience a fog or fade effect, and this zone  is for camera distances or z values of less than the fade in start or third distance, d, set for lighting values of less than the minimum or typically a value of zero, and in this case, the pixel color would be used in rendering an animated image by the graphics system. Similarly, there is a solid fog color zone  used for all pixels that have a camera distance or z value that is greater than the second distance, d, or fade in end point set for pixels having a maximum or full lighting value (such as 1.0 on a 0 to 1.0 scale). In this zone , the pixel would be drawn or rendered using only the fog color (or 100 percent fog color contribution).","An intermediate zone  is provided between the fade in start point or distance, d, with minimum lighting values and the fade in end point or distance, d, for pixels with maximum lighting values. In this zone, the fog effect method involves determining not only the distance from the camera or z value for a pixel but also determining the pixel's lighting value as this second value will determine, at least in part, how the fog effect is applied. For example, a pixel may have a z value or viewpoint distance that is midway between the fade in start point, d, for minimum or no light and the fade in end point, d, for minimum or no light. If the pixel has a lighting value, however, that is a maximum value (such as 1.0), the z value or distance may be at or before the start point, d, for fade in for maximum light values, and, in such cases, no or little fog effect would occur. In contrast, if the pixel has a lighting value that is less than the maximum value, fog will be introduced. Particularly, the lighting value may be a minimum value (e.g., 0), and, in a linear blending factor application, the contribution of the pixel color and of the fog color may be set at 50 percent each.","In this manner, a pixel with a lighting value of less than the maximum may begin to experience some level of fading whenever its z value or viewpoint\/camera distance is between the fade in start point for minimum lighting values and the fade in start point for maximum lighting values. For example, the graph  also shows a fade in start point  and end point  for pixels with a midpoint lighting value (such as 0.5 on a 0 to 1.0 scale), and fog would be applied at a distance less than for pixels with full lighting but after those pixels with minimum lighting values. The location of the start points (and end points) for fade in based on a lighting value between a minimum and maximum may be placed linearly between the start points (and end points) for fade in based on minimum or maximum (e.g., between distance dand d(and dand d) in a linear manner or some other placement technique may be used (and such placement or setting may be performed manually by an artist\/animator or automatically based on a calculation comparing the lighting value to the minimum and\/or maximum). For example, a fifty-percent or midpoint lighting value may be placed midway between the two fade in start points\/distances, dand d, but this is not required.","Briefly, as distances for pixels from a camera are increased, the pixels with lower lighting values (or that receive less light from a source) will experience more of the fog effect as compared to pixels with higher lighting value such that lower lighting or shadowed pixels may fade from view while highlighted or well-lit pixels remain (e.g., highlighted details remain while shadowed details fade from the image or become fully the fog color). As graph  shows, for example, a pixel with minimum lighting will be rendered fully with the fog color at the fade in end point, d, whereas a pixel with maximum lighting will remain at least partially visible or colored at least in part with the no-fog pixel color up to fade in end point, d, which is greater than point, d.","Application of this new fog effect method may become clearer with a working example with reference to the animation of . As shown in , the fog effect method  shown in  has been applied to this relatively simple computer-generated image . As shown, the pixels on two sides ,  of the hill , which are assumed to be at the same camera distance or to have the same z value, are rendered differently or with differing fog effects , . Specifically, the pixels with a higher lighting value on side  have a greater contribution from their pixel color than the pixels with at least some shadowing in the side  that have a smaller lighting value. In other words, the fog effect has been applied earlier to the pixels of side  or to a greater degree as compared to pixels of side  even though these pixels have equal z values or distances from the camera.","Likewise, a similar process happens on the two foreground objects , , which are assumed to have substantially equal z values or viewpoint distances. As shown, the fog effect  is applied equally to the pixels in side  of column or object  as to the hill side , and this may cause both of these areas of the image  to become fully faded or drawn fully using the fog color. This may be expected in the real world, too, because although the pixels of side  are closer to the camera than pixels in side , these pixels of side  have a smaller lighting value. As a result, the detail  may be hidden or fade out of view in image . Note, however, the pixels of side  receive a differing fog effect  (or the contribution of the pixel color is greater than that of pixels in side ) because their lighting values were greater than those of pixels in side . Hence, detail  may remain visible longer than detail  even though these are equally spaced apart from the camera. This same fog effect  may be produced in the pixels of side  of column\/object  as these pixels may have a similar lighting value and are similarly spaced from the camera such that detail  is faded in a manner that is similar to the fading of detail . Further, the pixels of side  may have a fog effect or fading  applied that is less (or allows a greater contribution of the pixel color) than on side  such that detail  remains clear longer than detail , which provides a more desirable artistic effect than prior fog effect techniques.",{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 7","b":["700","700","704","700","710","700","710","720","726","720","726"]},"At , animation or computer generation of an image is initiated using the fog effect module and its defined parameters including those the defined fade in start points and how blending is performed. In practice, the image generation may occur during play of a video game in a game system or similar applications. At , the method  includes determining a distance from the camera or z value for each pixel in a scene that is to be animated. At , for each pixel, the color of the pixel when no fog is present\/applied is determined or retrieved from memory. At , the method  includes determining a lighting value for each pixel. Lighting values are typically determined by graphics systems during animation or image generation, and these values may be retrieved and used for applying the fog effect (e.g., a separate light determination module or routine may not be required to practice the method ).","In some cases, the lighting value may be modified to account for one or more objects that may be at least partially blocking light from a source (or casting a shadow upon a pixel). This process may include determining for a pixel a lighting value for the pixel based on its position\/location relative to a light source (e.g., is the pixel normal to the light source and if not, what degree from normal is the pixel that causes a reduction in its lighting value). Then the object-blocking process may further include determining whether any object in the scene blocks the pixel's light or is between the light source and the pixel. If not, the previously determined lighting value may be used in the fog effect calculation. If there is one or more object, a second lighting value accounting for the object is determined in , and typically, the second lighting value would be between less than the maximum value (although this is possible if the blocking object is transparent such as a window) such as in the range of 0 to 0.9 on a scale of 0 to 1.0 depending on a light blocking characteristic of the object. The first and second values may be combined (such as by taking the minimum of the two values) to provide a new or modified lighting value (e.g., if lighting value is 1.0 without blockage and 0.8 with blockage the modified value would be 0.8 whereas if the non-blocked value is 0.5 and with blockage value is 0.8 the new value would be the same as the original value of 0.5 and so on). Briefly, step  determines how much in light the pixel is and provides a lighting value for each pixel within a predefined lighting value range (such as, but not limited to, 0 to 1 or the like).","At , the method  includes retrieving a fog color set for use in the animation. The fog color may be uniformly set for an image or may be more localized such as when layered fog is used, when localized clouds, fog, smoke, or other atmospheric elements are included in a scene. The fog color often is chosen to be a background color but may be nearly any color chosen by an artist including gray, white, blue, black, and so on. At , with a fog effect module run by a processor, the contributions of the pixel color and fog color are determined. As discussed above with reference to , this typically will involve using the lighting value to determine where the distance or z value falls within a fog\/pixel color range (where the pixel's distance from the camera falls between a fade in start and end point, which will vary based on lighting value). For example, if a linear contribution is used and the lighting value is 0.5 then step  will involve determining where the distance from camera falls between the lighting value 0.5 fade in start and the fade in end points. It may fall nearer the fade in start point (such as 25 percent across the range) such that the contribution of the pixel color is chosen to be 75 percent and the fog color is 25 percent.","In step , the method  continues with blending the two color contributions to determine the fog effect. Again, this may depend upon the particular fog effect utilized and other parameters may be used to determine how to blend the pixel and fog colors. At step , the method  includes rendering the image or animated scene (or computer graphics) utilizing the fog effect pixel colors. At , the method  may end or the method  may continue at  such as when the viewpoint\/camera is moved such as when a player moves a character. The method  would then continue at  to determine at the present or current location of the camera\/viewpoint the distance from the camera for each pixel and then repeating steps -.",{"@attributes":{"id":"p-0053","num":"0052"},"figref":["FIG. 8","FIG. 8"],"b":["810","810","810","820","822","810","820","824","810","828","824","826"]},"The CPU  may also be used to run software\/hardware to provide a light value generator  that functions to determine or provide lighting values for each pixel in a built scene (or via triangle information for the scene's objects that provide vertex information including lighting values). The CPU  may also run software\/hardware to provide a distance (or z value) calculator or mechanism  that performs during operation of system  to determine for each pixel a distance from a camera or an eye-coordinate at a particular point in time. The system  may include or have access to memory  that is used to, at least temporarily, store pixel color data , which may be set by the user of system  via I\/O  and\/or API . The memory  may also be used to store fog effect method parameters including fade in light start and end points (e.g., for pixels with lighting values at a maximum value)  and fade in shadow start and end points (e.g., for pixels with minimum lighting values) .","The system  further includes a GPU  that is used to render the image , and, to this end, the GPU  may include a pixel shader  that uses the fog effect module  to provide fog or to determine a fog effect for each pixel. To this end, as discussed above, the fog effect module  may be fed as input the pixel color data , the fade in start and end points , , the lighting values from generator , and the distances from the camera for each pixel from distance calculator . The fog effect generator  may then function to determine for a pixel with a particular lighting value where its distance falls between the fade in start and end points associated with that particular lighting value, and from this determination, a contribution of the pixel color and the fog color is determined. This contribution may then be used by the fog effect module  (or pixel shader ) in determining a final (or fog-effected) color for each pixel. The blended or fog-effected color is then used by the GPU  in rendering the animated image with the fog effect that may be displayed on the monitor  as shown at .",{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 9","b":["900","910","912","914","916"]},"In the system , main processor  receives inputs from handheld controllers  and\/or other input devices via graphics and audio processor . Main processor  interactively responds to user inputs, and executes a video game or other program supplied, for example, by external storage media  via a mass storage access device  such as an optical disk drive or the like. As one example, in the context of video game play, main processor  can perform collision detection and animation processing in addition to a variety of interactive and control functions.","In this example, main processor  generates 3D graphics and audio commands and sends them to graphics and audio processor . The graphics and audio processor  processes these commands to generate visual images or computer animation with fog effects on display  and sound on speakers R, L or other suitable sound-generating devices. Example system  includes a video encoder  that receives image signals from graphics and audio processor  and converts the image signals into analog and\/or digital video signals suitable for display on a standard display device such as a computer monitor or home color television set .","System  also includes an audio codec (compressor\/decompressor)  that compresses and decompresses digitized audio signals and may also convert between digital and analog audio signaling formats as needed. Audio codec  can receive audio inputs via a buffer  and provide them to graphics and audio processor  for processing (e.g., mixing with other audio signals the processor generates and\/or receives via a streaming audio output of mass storage access device ). Graphics and audio processor  may store audio related information in an audio memory  that is available for audio tasks. Graphics and audio processor  provides the resulting audio output signals to audio codec  for decompression and conversion to analog signals (e.g., via buffer amplifiers L, R) so they can be reproduced by speakers L, R.","Graphics and audio processor  has the ability to communicate with various additional devices that may be present within system . For example, a parallel digital bus  may be used to communicate with mass storage access device  and\/or other components. A serial peripheral bus  may communicate with a variety of peripheral or other devices including, for example, a programmable read-only memory and\/or real time clock , a modem  or other networking interface (which may in turn connect system  to a telecommunications network  such as the Internet or other digital network from\/to which program instructions and\/or data can be downloaded or uploaded), and flash memory . A further external serial bus  may be used to communicate with additional expansion memory  (e.g., a memory card) or other devices. Connectors may be used to connect various devices to busses , , .","Although the invention has been described and illustrated with a certain degree of particularity, it is understood that the present disclosure has been made only by way of example, and that numerous changes in the combination and arrangement of parts can be resorted to by those skilled in the art without departing from the spirit and scope of the invention, as hereinafter claimed. In the above discussion, it should be understood that the particular method or technique for achieving a fog effect, using the differing fade in start and end points that are based on lighting values, are not limiting to the invention and a variety of known techniques may be used to achieve the shadow-first fog effect. For example, a range-based method may be used (with the modifications described herein) with the fog scale or blending value (how much visibility drops off based on where a distance falls between a fade in start point\/distance and a fade in end point\/distance) may be defined using linear, exponential, exponential-squared or other techniques. For example, the blending methods taught in U.S. Pat. Nos. 6,580,430 and 7,245,301, both of which are incorporated herein by reference, may be used to practice the shadow-first fog effect method.","In other words, an animator or artist may typically specify how fog visibility drops off with distance beginning with a fade in start point\/distance (which they may also set such as for minimum and maximum lighting values), and conventional equations such the well-known linear formula, the exponential formula, and\/or the exponential squared formula (or others) may be used, and these may be used with or without modification and individually or together (such as using one for pixels with a lighting value over a particular amount (such as greater than 0.5 on a 0 to 1.0 scale use linear drop off) and another for pixels with a lighting value under a particular amount (such as less than 0.5 use exponential or exponential square to cause faster drop off for more shaded or less highlighted pixels and the details they represent).","In another embodiment, a fog effect method is provided that also functions to control rendering of shadowed pixels to assure that these pixels do not appear brighter than pixels with higher lighting values. For example, if a fog color chosen to be light blue, a shadow may be cast by an object on a darker surface such as one that is colored dark red. If no mechanism is provided, the shadowed area may appear \u201cbrighter\u201d than nearby but more brightly lit surfaces or pixels because the dark red may seem darker than the light blue. To control this potentially unwanted effect, the fog effect method (or a module implementing a fog effect method) may include comparing out of shadow pixels to in shadow pixels (such as by Max(Out, In)), and, for example, a maximum in each of the red, green, and blue channels may be determined to assure that the out of shadow pixels are rendered brighter than in shadow pixels. The above-described fog effect technique was created in part to try to simulate both artistic and physical effects, but the technique may not actually be how an artist may achieve that effect or how the similar effect happens in the real world. As a result, problems may be encountered like the one regarding light blue and dark red (or other color combinations) as mentioned in this paragraph. Artistically, it is typically undesirable for something in shadow to be brighter than the midtones of something not in shadow, and the modification described in this paragraph may be used to try to address this situation in the above-described fog effect technique(s) while other implementations may utilized other methods to overcome or address this potential problem."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 4","FIG. 1","FIG. 3"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 6","FIG. 1","FIG. 5"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
