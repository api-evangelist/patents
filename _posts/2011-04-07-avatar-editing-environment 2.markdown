---
title: Avatar editing environment
abstract: An avatar editing environment is disclosed that allows users to create custom avatars for use in online games and other applications. Starting with a blank face the user can add, rescale and position different elements (e.g., eyes, nose, mouth) on the blank face. The user can also change the shape of the avatar's face, the avatar's skin color and the color of all the elements. In some implementations, touch input and gestures can be used to manually edit the avatar. Various controls can be used to create the avatar, such as controls for resizing, rotating, positioning, etc. The user can choose between manual and automatic avatar creation. The avatar editing environment can be part of a framework that is available to applications. One or more elements of the avatar can be animated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09576400&OS=09576400&RS=09576400
owner: Apple Inc.
number: 09576400
owner_city: Cupertino
owner_country: US
publication_date: 20110407
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","Overview of Avatar Editing Environment","Exemplary Category Picker","Exemplary Avatar Element Picker","Exemplary Avatar Color Picker","Exemplary Manual Editing of Avatar Elements","Exemplary Alternative Avatar Element Picker","Exemplary Manual Editing with Editing Regions","Exemplary Avatar Animation","Exemplary Software Architecture","Exemplary Avatar Editing Process","Exemplary Mobile Device Architecture","Exemplary Operating Environment"],"p":["This application claims the benefit of U.S. Provisional Application Ser. No. 61\/321,840, filed on Apr. 7, 2010, under 35 U.S.C. \u00a7119(e). The disclosure of the prior application is considered part of and is incorporated by reference in the disclosure of this application.","This disclosure relates generally to avatar creation for computer systems and other devices.","An avatar is representation of a user or their alter ego. An avatar is often in the form of a three-dimensional (3D) model used in computer games or a two-dimensional (2D) icon or picture used on Internet forums, social networks and other communities. Avatars can also be used in video games, including online interactive gaming environments.","Avatars in video games are the player's physical representation in the game world. Online games often provide means for creating varied and sophisticated avatars. In some online games, players can construct a customized avatar by selecting from a number of preset facial structures, hairstyles, skin tones, clothes, accessories, etc. (collectively referred to as \u201celements\u201d). Once the preset elements are selected, there is no facility for allowing users to manually adjust the elements (e.g., resize or position elements).","An avatar editing environment is disclosed that allows users to create custom avatars for use in online games and other applications. Starting with a blank face the user can add, rescale and position different elements on the blank face, including but not limited to different eyes, ears, mouth (including teeth and smile), nose, eyebrows, hair, beard, moustache, glasses, earrings, hats, and other elements that are associated with physical characteristics of humans and fashion. The user can also change the shape of the avatar's face, the avatar's skin color and the color of all the elements.","In some implementations, touch input and gestures can be used to edit the avatar. Various controls can be used to create the avatar, such as controls for resizing, rotating, positioning, etc. The user can choose between manual and automatic avatar creation. In some implementations, the avatar editing environment can be part of a framework that is available to applications, such as address books, text messaging, chat sessions, e-mail, games or any other applications. In some implementations, one or more elements of the avatar can be animated. For example, the avatar's eyes can be animated to track an object in a user interface or to indicate direction. In some implementations avatar data can be stored on a network so that the avatar can be used in online applications or downloaded to a variety of user devices at different user locations.","In some implementations, a computer implemented method includes: presenting an avatar editing environment on a display of a device; displaying a three-dimensional avatar model in the avatar editing environment; receiving first input selecting an avatar element category; receiving a second input selecting an avatar element from the avatar category; rendering the selected avatar element on the three-dimensional (3D) avatar model; and receiving third input for manually editing the avatar element.","Some embodiments include one or more application programming interfaces (APIs) in an environment with calling program code interacting with other program code being called through the one or more interfaces. Various function calls, messages, or other types of invocations, which further may include various kinds of parameters, can be transferred via the APIs between the calling program and the code being called. In addition, an API may provide the calling program code the ability to use data types or classes defined in the API and implemented in the called program code.","At least certain embodiments include an environment with a calling software component interacting with a called software component through an API. A method for operating through an API in this environment includes transferring one or more function calls, messages, and other types of invocations or parameters via the API.","The details of one or more implementations of an avatar editing environment are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the avatar editing environment will become apparent from the description, the drawings, and the claims.","Like reference symbols in the various drawings indicate like elements.",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIGS. 1A-1E","b":["100","100"]},"Upon invocation of the avatar editing environment, a user interface  for the editing environment can be presented on a display  of device . Display  can be a touch sensitive display or surface responsive to touch input and gestures. Although a mobile device is shown, device  can be a desktop computer, a handheld computer, a personal digital assistant, a cellular telephone, an electronic tablet, a network appliance, a camera, a smart phone, an enhanced general packet radio service (EGPRS) mobile phone, a network base station, a media player, a navigation device, an email device, a game console, or a combination of any two or more of these devices.","Referring to , in some implementations avatar  can be displayed on user interface . Avatar  can be a 2D or 3D avatar model. Avatar  can also be full body avatar. When the editing environment is invoked or the user is creating a new avatar, a default avatar can be displayed on user interface . In some implementations, the default avatar can have a blank face for receiving elements selected by the user in the editing environment. In other implementations, a default avatar having predefined elements can be displayed. The default avatar can be automatically created based on user preferences specified in a dialog. For example, when the user first invokes a game environment on device , the user can be presented with a number of predefined avatars and the user can select one as a default avatar. In other implementations, a default avatar can be automatically created on-the-fly based on user preferences for physical characteristics, such as gender, age, hair color, eye color, etc. Starting with a blank face the user can add different elements on the blank face, including but not limited to different eyes, ears, mouth (including teeth and smile), nose, eyebrows, hair, beard, moustache, glasses, earrings, hats, and other elements that are associated with physical characteristics of humans and fashion. The user can also change the shape of the avatar's face, the avatar's skin color and the color of all the elements.","In the example shown, the user selected an \u201cEyes\u201d category from category picker . Category picker  can be a bar with icons representing element categories. Text and other visual indicators of categories can also be used. The selected category can be the category having an icon in center position  of category picker . The icon in center position  can be highlighted or otherwise visually enhanced to indicate its selected status. In some implementations, the user can make a swipe gesture on the bar to the left or right to move a different icon into center position . In response to the swipe gesture, category picker  can be animated so that the category icons move like a wheel on a slot machine. Friction can also be simulated so that acceleration of the wheel can be controlled. For example, a faster gesture results in an increased acceleration of the icons passing through center position .","In the example shown, the \u201cEyes\u201d category is currently occupying middle position  and is therefore highlighted to indicate its selected status. An element picker represented by icon  was used to select eyes element . Upon its selection, the eyes element  were added to the face of avatar , which was originally blank. The operation of the element picker is further described in reference to .","Display  presents control region . Control region  can include text describing the currently selected category. For example, the currently selected \u201cEyes\u201d category is indicated by the text \u201cEyes\u201d in control region . Control region  can also include one or more controls (e.g., virtual buttons) for exiting the avatar editing environment. In the example shown, a first virtual button can cancel (e.g., exit) the avatar editing environment without saving changes to avatar  (e.g., \u201cCancel\u201d button). Another button can be used to exit the avatar editing environment and save changes to avatar  (e.g., \u201cDone\u201d button).","A color picker represented by icon  can be selected to allow the user to select a color for the selected category. For example, if the \u201cEyes\u201d category is selected, the color picker can be used to select a color for all the eyes elements that can be selected using element picker . The operation of the color picker will be further described in reference to .","Referring to , the user has selected a \u201cNose\u201d category using category picker . The selection is indicated by an icon of a nose occupying center position  of category picker . Control region  indicates that the category \u201cNose\u201d has been selected by displaying the text \u201cNose.\u201d The user selected nose element  from the element picker. Nose element  was automatically added to the face of avatar . The selected element category can be used to determine a default region on the avatar face to add the selected element from the category. In this case, the \u201cNose\u201d category selection determined the location of nose element  to be the center of the avatar face.","Referring to , the user has selected a \u201cHair\u201d category using category picker . The selection is indicated by an icon of hair occupying center position  of category picker . Control region  indicates that the element category \u201cHair\u201d has been selected by displaying the text \u201cHair.\u201d The user selected hair element  from the element picker. Hair element  was automatically added to the avatar face. The \u201cHair\u201d category selection determined the location of hair element  to be on top of the avatar head.","Referring to , the user has selected a \u201cMouth\u201d category using category picker . The selection is indicated by an image of a mouth occupying center position  of category picker . Control region  indicates that the element category \u201cMouth\u201d has been selected by displaying the text \u201cMouth.\u201d The user selected mouth element  from the element picker. Mouth element  was automatically added to the avatar face. The \u201cMouth\u201d category selection determined the location of mouth element  to be below nose element  on the avatar face.","Referring to , the user has selected a \u201cHat\u201d category using category picker . The selection is indicated by an image of a hat occupying center position  of category picker . Control region  indicates that the category \u201cHat\u201d has been selected by displaying the text \u201cHat.\u201d The user selected a hat element  from the element picker. Hat element  was automatically added to the avatar head. The \u201cHat\u201d category selection determined the location of hat element  to be on top of hair element .","In , the user selected eyes, nose, hair, mouth and a hat for avatar . In some implementations, the elements can be 2D textures, which are rendered onto a 3D model of the avatar head. In some implementations, the elements can be 3D objects that are rendered onto the 3D model of the avatar head. For example, a graphics engine can create \u201ceye sockets\u201d in the 3D model and insert 3D \u201ceye\u201d objects into the sockets. The 3D \u201ceye\u201d objects can be animated to create the illusion that the avatar is looking in a particular direction or tracking objects, as described in reference to .","In some implementations, when adding both hair and hat elements to an avatar, the hair element can be modified so that when the hat element is added to the avatar the hair appears to be covered while still maintaining the selected hair style. For example, \u201cspikey\u201d hair with a baseball cap could result in hair sticking out through the top of the cap. To avoid this issue, the hair element is cut into two parts. The editing environment can determine if a hat and hair combination would result in hair sticking out through the top of the hat, and in those cases, only the bottom half of the hair is rendered on the avatar model.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 2A","b":["116","200","200"]},"In some implementations, one cell (e.g., top left corner cell in ) can display avatar  without the element (e.g., without a hat) for comparison with other cells. Another cell (e.g., center cell) can display avatar  with the currently selected element (e.g., the currently selected hat). In some implementations, each cell can contain a snapshot image of a 3D avatar model to capture appropriate lighting and shadows to provide a 3D effect. In other implementations, each cell can contain a 2D image of the avatar (e.g., front facing view). The user can select an element by touching or otherwise interacting with the cell displaying the desired element. In this example, the user has touched the third cell in the middle row grid view , where the avatar is shown wearing a \u201cCanada\u201d ski hat. The selected cell can be visually enhanced (e.g., highlighted, colored) to indicate its selected status. Touching the \u201cDone\u201d button can return the user to user interface  of the avatar editing environment.","The elements can be displayed in grid view  in a variety of ways. Avatars can be displayed in cells based on element type. For example, holiday hats can be displayed in the same row or column of grid view . In , holiday hats for Thanksgiving, Christmas and Independence Day are displayed in the third row of grid view . Grid view  can be paginated so that a swipe gesture or other gesture can be used to display a new page with a new grid view. In some implementations, grid view  can be updated with new elements in response to a motion gesture. For example, when the user shakes device  or other motion gesture, a new grid view can be displayed with different elements from the currently selected category. An accelerometer or other motion sensor onboard device  can be used to detect motion. The new elements displayed in cells of the new grid view can be randomly selected based on other elements of avatar . For example, eye color, hair color and skin color can be used to select fashion elements having an appropriate color or color scheme, so that the resulting avatar  is color coordinated. The selection of colors can be based on known color heuristics.",{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 2B","b":["114","204","204","200","204","204","204"]},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIGS. 3A-3C","b":"104"},"In some implementations, the user can apply touch input and gestures to resize, rotate and position the elements of avatar . In the example shown, the user resized eyes element  by touching eyes element  at points indicated by arrows and making a pinching gesture in the direction of the arrows.  shows the result of the pinching gesture. In this example, eyes element  had a distance of d between the eyes before pinching () and a distance of d between the eyes after pinching (), where d>d. For elements that have symmetry (e.g., eyes, ears), the symmetry can be maintained without user intervention during manual editing to assist the user in editing. In devices without a touch sensitive display, mouse controls can be used to resize, rotate and position elements on avatar .","Referring to , the user can zoom on any of the elements of avatar  in user interface  for more refined manual editing. In some implementations, the element is automatically zoomed in response to a user touching the element. Multiple taps can change the zoom level increments where one zoom or magnification level change occurs for each tap. Alternatively, a reverse pinching gesture (spreading apart two fingers) can be used zoom elements of avatar . In other implementations, the user can zoom on a particular element using zoom button . Other zoom controls can also be used including, for example, a magnifying glass tool.",{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIGS. 4A and 4B","FIG. 4B"],"b":["400","406","400","402","404","406","400","408","408","408","408"],"i":["a","b "]},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIGS. 5A-5C"},"Referring to , editing regions , contain the left and right eyes of the avatar. The user can move, rotate or resize the eyes anywhere within their respective editing regions , . In this example, the editing regions , are circular regions. Editing regions , , however, can be any closed polygon, including but not limited to circles, squares, rectangles and triangles.  illustrates a rectangular editing region for the nose of the avatar. The user can move, rotate, or resize the nose anywhere within the editing region .  illustrates a rectangular editing region for the mouth of the avatar. The user can move, rotate or resize the mouth anywhere within the editing region ",{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIGS. 6A-6B"},"In some implementations, animations for elements can be selected and previewed in the avatar editing environment. In some implementations, the user can select (e.g., select from a menu) a particular animation for a particular element. In other implementations, the use can set the animations to trigger in response to various trigger events. Some examples of trigger events can be user actions or context. In an email or text messaging application, if the user is waiting for a response from another user, their avatar can be animated to appear to be waiting or sleeping. For example, the avatar's eyes can be closed and the chest animated to contract and expand to simulate slow, deep breathing associated with sleeping. With a full body avatar, the avatar can be animated to tap its foot (perhaps with its arms crossed as well) simulate waiting or impatience.","Referring to , the eyes of avatar  can be animated to track a cursor  in a user interface . In user interface , avatar  is looking down at cursor . In interface , avatar  is looking up and to the right at cursor . The eyes of avatar  can be animated independently of each other and other elements, such as eyebrows. In some implementations, an avatar animation engine (e.g., ) can register with an operation system (e.g., OS 708) to receive trigger events or position information, such as cursor coordinates. In some implementations, a line-of-sight vector from the eyes to the cursor can be computed in display coordinates using vector algebra. The line of sight vector can then be used by animation engine  to animate the eyes of avatar  to create the illusion that avatar  is tracking cursor  with its eyes as cursor  moves about user interface .","Avatar animations can be used in variety of applications, including but not limited to address books, chat sessions, video conferencing, email, games or any other application that can support an animated avatar. In an address book application, when a user receives an avatar with a video card (Vcard) from another individual, the avatar can \u201ccome alive\u201d and follow the movement of a cursor with its eyes, head and\/or body when the Vcard is opened. In a video chat environment, each party can be represented by an avatar rather than a digital image. Each party can use the avatar to track the other party's movement by controlling their respective avatar's eyes, head and body to follow the other party's avatar in a video chat room. In some implementations, an avatar viewing angle can mimic camera position.",{"@attributes":{"id":"p-0053","num":"0052"},"figref":["FIG. 7A","FIGS. 1-6","FIGS. 6A and 6B","FIGS. 1-5"],"b":["700","702","704","706","708","710","704","712","714","712","712","706","708","714","714","706","708"]},"Services layer  can provide various graphics, animations and UI services to support animation engine , avatar editing environment  and applications in applications layer . In some implementations, services layer  includes touch model  for interpreting and mapping raw touch data from a touch sensitive device to touch events (e.g., gestures, rotations), which can be accessed by applications and by avatar editing environment  using call conventions defined in a touch model API. Services layer  can also include communications software stacks for wireless communications.","OS layer  can be a complete operating system (e.g., MAC OS) or a kernel (e.g., UNIX kernel). Hardware layer  includes hardware necessary to perform the tasks described in reference to , including but not limited to: processors or processing cores (including application and communication baseband processors), dedicated signal\/image processors, ASICs, graphics processors (e.g., GNUs), memory and storage devices, communication ports and devices, peripherals, etc.","Software stack  can be included on a mobile device capable of executing software applications. An API specification describing call conventions for accessing API functions can be used by application developers to incorporate avatar editing and animation in applications.","One or more Application Programming Interfaces (APIs) may be used in some embodiments. An API is an interface implemented by a program code component or hardware component (hereinafter \u201cAPI-implementing component\u201d) that allows a different program code component or hardware component (hereinafter \u201cAPI-calling component\u201d) to access and use one or more functions, methods, procedures, data structures, classes, and\/or other services provided by the API-implementing component. An API can define one or more parameters that are passed between the API-calling component and the API-implementing component.","An API allows a developer of an API-calling component (which may be a third party developer) to leverage specified features provided by an API-implementing component. There may be one API-calling component or there may be more than one such component. An API can be a source code interface that a computer system or program library provides in order to support requests for services from an application. An operating system (OS) can have multiple APIs to allow applications running on the OS to call one or more of those APIs, and a service (such as a program library) can have multiple APIs to allow an application that uses the service to call one or more of those APIs. An API can be specified in terms of a programming language that can be interpreted or compiled when an application is built.","In some embodiments, the API-implementing component may provide more than one API, each providing a different view of or with different aspects that access different aspects of the functionality implemented by the API-implementing component. For example, one API of an API-implementing component can provide a first set of functions and can be exposed to third party developers, and another API of the API-implementing component can be hidden (not exposed) and provide a subset of the first set of functions and also provide another set of functions, such as testing or debugging functions which are not in the first set of functions. In other embodiments, the API-implementing component may itself call one or more other components via an underlying API and thus be both an API-calling component and an API-implementing component.","An API defines the language and parameters that API-calling components use when accessing and using specified features of the API-implementing component. For example, an API-calling component accesses the specified features of the API-implementing component through one or more API calls or invocations (embodied for example by function or method calls) exposed by the API and passes data and control information using parameters via the API calls or invocations. The API-implementing component may return a value through the API in response to an API call from an API-calling component. While the API defines the syntax and result of an API call (e.g., how to invoke the API call and what the API call does), the API may not reveal how the API call accomplishes the function specified by the API call. Various API calls are transferred via the one or more application programming interfaces between the calling (API-calling component) and an API-implementing component. Transferring the API calls may include issuing, initiating, invoking, calling, receiving, returning, or responding to the function calls or messages; in other words, transferring can describe actions by either of the API-calling component or the API-implementing component. The function calls or other invocations of the API may send or receive one or more parameters through a parameter list or other structure. A parameter can be a constant, key, data structure, object, object class, variable, data type, pointer, array, list or a pointer to a function or method or another way to reference a data or other item to be passed via the API.","Furthermore, data types or classes may be provided by the API and implemented by the API-implementing component. Thus, the API-calling component may declare variables, use pointers to, use or instantiate constant values of such types or classes by using definitions provided in the API.","Generally, an API can be used to access a service or data provided by the API-implementing component or to initiate performance of an operation or computation provided by the API-implementing component. By way of example, the API-implementing component and the API-calling component may each be any one of an operating system, a library, a device driver, an API, an application program, or other module (it should be understood that the API-implementing component and the API-calling component may be the same or different type of module from each other). API-implementing components may in some cases be embodied at least in part in firmware, microcode, or other hardware logic. In some embodiments, an API may allow a client program to use the services provided by a Software Development Kit (SDK) library. In other embodiments an application or other client program may use an API provided by an Application Framework. In these embodiments, the application or client program may incorporate calls to functions or methods provided by the SDK and provided by the API, or use data types or objects defined in the SDK and provided by the API. An Application Framework may, in these embodiments, provides a main event loop for a program that responds to various events defined by the Framework. The API allows the application to specify the events and the responses to the events using the Application Framework. In some implementations, an API call can report to an application the capabilities or state of a hardware device, including those related to aspects such as input capabilities and state, output capabilities and state, processing capability, power state, storage capacity and state, communications capability, etc., and the API may be implemented in part by firmware, microcode, or other low level logic that executes in part on the hardware component.","The API-calling component may be a local component (i.e., on the same data processing system as the API-implementing component) or a remote component (i.e., on a different data processing system from the API-implementing component) that communicates with the API-implementing component through the API over a network. It should be understood that an API-implementing component may also act as an API-calling component (i.e., it may make API calls to an API exposed by a different API-implementing component) and an API-calling component may also act as an API-implementing component by implementing an API that is exposed to a different API-calling component.","The API may allow multiple API-calling components written in different programming languages to communicate with the API-implementing component (thus the API may include features for translating calls and returns between the API-implementing component and the API-calling component); however the API may be implemented in terms of a specific programming language. An API-calling component can, in one embedment, call APIs from different providers such as a set of APIs from an OS provider and another set of APIs from a plug-in provider and another set of APIs from another provider (e.g. the provider of a software library) or creator of the another set of APIs.",{"@attributes":{"id":"p-0065","num":"0064"},"figref":["FIG. 7B","FIG. 7B"],"b":["720","722","724","724","726","724","726","724","722","724","722","724","726"]},"It will be appreciated that the API-implementing component  may include additional functions, methods, classes, data structures, and\/or other features that are not specified through the API  and are not available to the API-calling component . It should be understood that the API-calling component  may be on the same system as the API-implementing component  or may be located remotely and accesses the API-implementing component  using the API  over a network. While  illustrates a single API-calling component  interacting with the API , it should be understood that other API-calling components, which may be written in different languages (or the same language) than the API-calling component , may use the API .","The API-implementing component , the API , and the API-calling component  may be stored in a machine-readable medium, which includes any mechanism for storing information in a form readable by a machine (e.g., a computer or other data processing system). For example, a machine-readable medium includes magnetic disks, optical disks, random access memory; read only memory, flash memory devices, etc.","In  (\u201cSoftware Stack\u201d ), an exemplary embodiment, applications can make calls to Service A  or Service B  using several Service APIs (Service API A and Service API B) and to Operating System (OS)  using several OS APIs. Service A  and service B  can make calls to OS  using several OS APIs.","Note that the Service B  has two APIs, one of which (Service B API A ) receives calls from and returns values to Application A  and the other (Service B API B ) receives calls from and returns values to Application B . Service A  (which can be, for example, a software library) makes calls to and receives returned values from OS API A , and Service B  (which can be, for example, a software library) makes calls to and receives returned values from both OS API A  and OS API B . Application B  makes calls to and receives returned values from OS API B .",{"@attributes":{"id":"p-0070","num":"0069"},"figref":["FIG. 8","FIGS. 1-7"],"b":["800","800"]},"In some implementations, process  can begin by presenting an avatar editing environment on a display of a device () and displaying an avatar model in the environment (). The avatar model can be a 2D or 3D model. The display can be a touch sensitive display. The avatar model can be displayed with a blank face or a default avatar with predefined elements can be displayed based on information previously gathered from the user.","First input is received selecting an avatar element category (). In some implementations, a category picker can be used such as the category picker described in reference to .","Second input is received selecting an element from the selected element category (). In some implementations, an element picker can be used such as the element picker described in reference to  and .","After an element is selected, the element can be automatically rendered on-the-fly on the avatar model (). In some implementations, elements can be 2D textures that are rendered on a 3D avatar model.","A third input is received for manually editing an element of the avatar (). The third input can be a touch input or gesture focused on the element to be edited. Manual editing can include resizing, rotating and positioning the element. Manual editing can be restricted to editing regions. Manual editing can include zooming or magnifying an element for more refined editing.",{"@attributes":{"id":"p-0076","num":"0075"},"figref":["FIG. 9","FIGS. 1-8"],"b":["902","904","906","902","904","906"]},"Sensors, devices, and subsystems can be coupled to peripherals interface  to facilitate multiple functionalities. For example, motion sensor , light sensor , and proximity sensor  can be coupled to peripherals interface  to facilitate orientation, lighting, and proximity functions of the mobile device. Location processor  (e.g., GPS receiver) can be connected to peripherals interface  to provide geopositioning. Electronic magnetometer  (e.g., an integrated circuit chip) can also be connected to peripherals interface  to provide data that can be used to determine the direction of magnetic North. Thus, electronic magnetometer  can be used as an electronic compass. Accelerometer  can also be connected to peripherals interface  to provide data that can be used to determine change of speed and direction of movement of the mobile device.","Camera subsystem  and an optical sensor , e.g., a charged coupled device (CCD) or a complementary metal-oxide semiconductor (CMOS) optical sensor, can be utilized to facilitate camera functions, such as recording photographs and video clips.","Communication functions can be facilitated through one or more wireless communication subsystems , which can include radio frequency receivers and transmitters and\/or optical (e.g., infrared) receivers and transmitters. The specific design and implementation of the communication subsystem  can depend on the communication network(s) over which a mobile device is intended to operate. For example, a mobile device can include communication subsystems  designed to operate over a GSM network, a GPRS network, an EDGE network, a Wi-Fi or WiMax network, and a Bluetooth network. In particular, the wireless communication subsystems  can include hosting protocols such that the mobile device can be configured as a base station for other wireless devices.","Audio subsystem  can be coupled to a speaker  and a microphone  to facilitate voice-enabled functions, such as voice recognition, voice replication, digital recording, and telephony functions.","I\/O subsystem  can include touch screen controller  and\/or other input controller(s) . Touch-screen controller  can be coupled to a touch screen  or pad. Touch screen  and touch screen controller  can, for example, detect contact and movement or break thereof using any of a plurality of touch sensitivity technologies, including but not limited to capacitive, resistive, infrared, and surface acoustic wave technologies, as well as other proximity sensor arrays or other elements for determining one or more points of contact with touch screen .","Other input controller(s)  can be coupled to other input\/control devices , such as one or more buttons, rocker switches, thumb-wheel, infrared port, USB port, and\/or a pointer device such as a stylus. The one or more buttons (not shown) can include an up\/down button for volume control of speaker  and\/or microphone .","In one implementation, a pressing of the button for a first duration may disengage a lock of the touch screen ; and a pressing of the button for a second duration that is longer than the first duration may turn power to the device on or off. The user may be able to customize a functionality of one or more of the buttons. The touch screen  can, for example, also be used to implement virtual or soft buttons and\/or a keyboard.","In some implementations, the device can present recorded audio and\/or video files, such as MP3, AAC, and MPEG files. In some implementations, the device can include the functionality of an MP3 player, such as an iPod\u2122. The device may, therefore, include a pin connector that is compatible with the iPod. Other input\/output and control devices can also be used.","Memory interface  can be coupled to memory . Memory  can include high-speed random access memory and\/or non-volatile memory, such as one or more magnetic disk storage devices, one or more optical storage devices, and\/or flash memory (e.g., NAND, NOR). Memory  can store operating system , such as Darwin, RTXC, LINUX, UNIX, OS X, WINDOWS, or an embedded operating system such as VxWorks. Operating system  may include instructions for handling basic system services and for performing hardware dependent tasks. In some implementations, operating system  can include a kernel (e.g., UNIX kernel).","Memory  may also store communication instructions  to facilitate communicating with one or more additional devices, one or more computers and\/or one or more servers. Memory  may include graphical user interface instructions  to facilitate graphic user interface processing; sensor processing instructions  to facilitate sensor-related processing and functions; phone instructions  to facilitate phone-related processes and functions; electronic messaging instructions  to facilitate electronic-messaging related processes and functions; web browsing instructions  to facilitate web browsing-related processes and functions; media processing instructions  to facilitate media processing-related processes and functions; GPS\/Navigation instructions  to facilitate GPS and navigation-related processes and instructions; and camera instructions  to facilitate camera-related processes and functions. The memory  may also store other software instructions (not shown), such as security instructions, web video instructions to facilitate web video-related processes and functions, and\/or web shopping instructions to facilitate web shopping-related processes and functions. In some implementations, the media processing instructions  are divided into audio processing instructions and video processing instructions to facilitate audio processing-related processes and functions and video processing-related processes and functions, respectively. An activation record and International Mobile Equipment Identity (IMEI) or similar hardware identifier can also be stored in memory .","Memory  can include instructions for avatar editing environment  and avatar animation engine . Memory  can be a local cache for avatar data  that results from the avatar editing process.","Each of the above identified instructions and applications can correspond to a set of instructions for performing one or more functions described above. These instructions need not be implemented as separate software programs, procedures, or modules. Memory  can include additional instructions or fewer instructions. Furthermore, various functions of the mobile device may be implemented in hardware and\/or in software, including in one or more signal processing and\/or application specific integrated circuits.",{"@attributes":{"id":"p-0089","num":"0088"},"figref":["FIG. 10","FIGS. 1-8"],"b":["1002","1002","1010","1012","1014","1016","1018","1014"],"i":["a ","b "]},"In some implementations, both voice and data communications can be established over wireless network  and the access device . For example, mobile device can place and receive phone calls (e.g., using voice over Internet Protocol (VoIP) protocols), send and receive e-mail messages (e.g., using Post Office Protocol 3 (POP3)), and retrieve electronic documents and\/or streams, such as web pages, photographs, and videos, over wireless network , gateway , and wide area network  (e.g., using Transmission Control Protocol\/Internet Protocol (TCP\/IP) or User Datagram Protocol (UDP)). Likewise, in some implementations, the mobile device can place and receive phone calls, send and receive e-mail messages, and retrieve electronic documents over the access device  and the wide area network . In some implementations, device or can be physically connected to the access device  using one or more cables and the access device  can be a personal computer. In this configuration, device or can be referred to as a \u201ctethered\u201d device.","Devices and can also establish communications by other means. For example, wireless device can communicate with other wireless devices, e.g., other devices or , cell phones, etc., over the wireless network . Likewise, devices and can establish peer-to-peer communications , e.g., a personal area network, by use of one or more communication subsystems, such as the Bluetooth\u2122 communication devices. Other communication protocols and topologies can also be implemented.","Device or can communicate with a variety of services over the one or more wired and\/or wireless networks. In some implementations, services can include mobile device services , social network services , and game center services .","Mobile device services  can provide a variety of services for device or , including but not limited to mail services, text messaging, chat sessions, videoconferencing, Internet services, location based services (e.g., map services), sync services, remote storage , downloading services, etc. Remote storage  can be used to store avatar data, which can be used on multiple devices of the user or shared by multiple users. In some implementations, an avatar editing environment can be provided by one or more of the services , , , which can be accessed by a user of device or through, for example, web pages served by one or more servers operated by the services , , .","In some implementations, social networking services  can provide a social networking website, where a user of device or can set up a personal network and invite friends to contribute and share content, including avatars and avatar related items. A user can use their custom avatar made with an avatar editing environment in place of a digital photo to protect their privacy.","In some implementations, game center services  can provide an online gaming environment, where users of device or can participate in online interactive games with their avatars created using the avatar editing environment described in reference to . In some implementations, avatars and\/or elements created by an avatar editing environment can be shared among users or sold to players of online games. For example, an avatar store can be provided by game center services  for users to buy or exchange avatars and avatar related items (e.g., clothes, accessories).","Device or can also access other data and content over the one or more wired and\/or wireless networks. For example, content publishers, such as news sites, Rally Simple Syndication (RSS) feeds, web sites, blogs, social networking sites, developer networks, etc., can be accessed by device or . Such access can be provided by invocation of a web browsing function or application (e.g., a browser) in response to a user touching, for example, a Web object.","Described embodiments may include touch I\/O device  that can receive touch input for interacting with computing system  () via wired or wireless communication channel . Touch I\/O device  may be used to provide user input to computing system  in lieu of or in combination with other input devices such as a keyboard, mouse, etc. One or more touch I\/O devices  may be used for providing user input to computing system . Touch I\/O device  may be an integral part of computing system  (e.g., touch screen on a laptop) or may be separate from computing system .","Touch I\/O device  may include a touch sensitive panel which is wholly or partially transparent, semitransparent, non-transparent, opaque or any combination thereof. Touch I\/O device  may be embodied as a touch screen, touch pad, a touch screen functioning as a touch pad (e.g., a touch screen replacing the touchpad of a laptop), a touch screen or touchpad combined or incorporated with any other input device (e.g., a touch screen or touchpad disposed on a keyboard) or any multi-dimensional object having a touch sensitive surface for receiving touch input.","In one example, touch I\/O device  embodied as a touch screen may include a transparent and\/or semitransparent touch sensitive panel partially or wholly positioned over at least a portion of a display. According to this embodiment, touch I\/O device  functions to display graphical data transmitted from computing system  (and\/or another source) and also functions to receive user input. In other embodiments, touch I\/O device  may be embodied as an integrated touch screen where touch sensitive components\/devices are integral with display components\/devices. In still other embodiments, a touch screen may be used as a supplemental or additional display screen for displaying supplemental or the same graphical data as a primary display and to receive touch input.","Touch I\/O device  may be configured to detect the location of one or more touches or near touches on device  based on capacitive, resistive, optical, acoustic, inductive, mechanical, chemical measurements, or any phenomena that can be measured with respect to the occurrences of the one or more touches or near touches in proximity to device . Software, hardware, firmware or any combination thereof may be used to process the measurements of the detected touches to identify and track one or more gestures. A gesture may correspond to stationary or non-stationary, single or multiple, touches or near touches on touch I\/O device . A gesture may be performed by moving one or more fingers or other objects in a particular manner on touch I\/O device  such as tapping, pressing, rocking, scrubbing, twisting, changing orientation, pressing with varying pressure and the like at essentially the same time, contiguously, or consecutively. A gesture may be characterized by, but is not limited to a pinching, sliding, swiping, rotating, flexing, dragging, or tapping motion between or with any other finger or fingers. A single gesture may be performed with one or more hands, by one or more users, or any combination thereof.","Computing system  may drive a display with graphical data to display a graphical user interface (GUI). The GUI may be configured to receive touch input via touch I\/O device . Embodied as a touch screen, touch I\/O device  may display the GUI. Alternatively, the GUI may be displayed on a display separate from touch I\/O device . The GUI may include graphical elements displayed at particular locations within the interface. Graphical elements may include but are not limited to a variety of displayed virtual input devices including virtual scroll wheels, a virtual keyboard, virtual knobs, virtual buttons, any virtual UI, and the like. A user may perform gestures at one or more particular locations on touch I\/O device  which may be associated with the graphical elements of the GUI. In other embodiments, the user may perform gestures at one or more locations that are independent of the locations of graphical elements of the GUI. Gestures performed on touch I\/O device  may directly or indirectly manipulate, control, modify, move, actuate, initiate or generally affect graphical elements such as cursors, icons, media files, lists, text, all or portions of images, or the like within the GUI. For instance, in the case of a touch screen, a user may directly interact with a graphical element by performing a gesture over the graphical element on the touch screen. Alternatively, a touch pad generally provides indirect interaction. Gestures may also affect non-displayed GUI elements (e.g., causing user interfaces to appear) or may affect other actions within computing system  (e.g., affect a state or mode of a GUI, application, or operating system). Gestures may or may not be performed on touch I\/O device  in conjunction with a displayed cursor. For instance, in the case in which gestures are performed on a touchpad, a cursor (or pointer) may be displayed on a display screen or touch screen and the cursor may be controlled via touch input on the touchpad to interact with graphical objects on the display screen. In other embodiments in which gestures are performed directly on a touch screen, a user may interact directly with objects on the touch screen, with or without a cursor or pointer being displayed on the touch screen.","Feedback may be provided to the user via communication channel  in response to or based on the touch or near touches on touch I\/O device . Feedback may be transmitted optically, mechanically, electrically, olfactory, acoustically, or the like or any combination thereof and in a variable or non-variable manner.","The features described can be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them. The features can be implemented in a computer program product tangibly embodied in an information carrier, e.g., in a machine-readable storage device, for execution by a programmable processor; and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output. Alternatively or addition, the program instructions can be encoded on a propagated signal that is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information fro transmission to suitable receiver apparatus for execution by a programmable processor.","Attention is now directed towards embodiments of a system architecture that may be embodied within any portable or non-portable device including but not limited to a communication device (e.g. mobile phone, smart phone), a multi-media device (e.g., MP3 player, TV, radio), a portable or handheld computer (e.g., tablet, netbook, laptop), a desktop computer, an All-In-One desktop, a peripheral device, or any other system or device adaptable to the inclusion of system architecture , including combinations of two or more of these types of devices.  is a block diagram of one embodiment of system  that generally includes one or more computer-readable mediums , processing system , Input\/Output (I\/O) subsystem , radio frequency (RF) circuitry  and audio circuitry . These components may be coupled by one or more communication buses or signal lines .","It should be apparent that the architecture shown in  is only one example architecture of system , and that system  could have more or fewer components than shown, or a different configuration of components. The various components shown in  can be implemented in hardware, software, firmware or any combination thereof, including one or more signal processing and\/or application specific integrated circuits.","RF circuitry  is used to send and receive information over a wireless link or network to one or more other devices and includes well-known circuitry for performing this function. RF circuitry  and audio circuitry  are coupled to processing system  via peripherals interface . Interface  includes various known components for establishing and maintaining communication between peripherals and processing system . Audio circuitry  is coupled to audio speaker  and microphone  and includes known circuitry for processing voice signals received from interface  to enable a user to communicate in real-time with other users. In some embodiments, audio circuitry  includes a headphone jack (not shown).","Peripherals interface  couples the input and output peripherals of the system to processor  and computer-readable medium . One or more processors  communicate with one or more computer-readable mediums  via controller . Computer-readable medium  can be any device or medium that can store code and\/or data for use by one or more processors . Medium  can include a memory hierarchy, including but not limited to cache, main memory and secondary memory. The memory hierarchy can be implemented using any combination of RAM (e.g., SRAM, DRAM, DDRAM), ROM, FLASH, magnetic and\/or optical storage devices, such as disk drives, magnetic tape, CDs (compact disks) and DVDs (digital video discs). Medium  may also include a transmission medium for carrying information-bearing signals indicative of computer instructions or data (with or without a carrier wave upon which the signals are modulated). For example, the transmission medium may include a communications network, including but not limited to the Internet (also referred to as the World Wide Web), intranet(s), Local Area Networks (LANs), Wide Local Area Networks (WLANs), Storage Area Networks (SANs), Metropolitan Area Networks (MAN) and the like.","One or more processors  run various software components stored in medium  to perform various functions for system . In some embodiments, the software components include operating system , communication module (or set of instructions) , touch processing module (or set of instructions) , graphics module (or set of instructions) , one or more applications (or set of instructions) , and avatar editing module . Each of these modules and above noted applications correspond to a set of instructions for performing one or more functions described above and the methods described in this application (e.g., the computer-implemented methods and other information processing methods described herein). These modules (i.e., sets of instructions) need not be implemented as separate software programs, procedures or modules, and thus various subsets of these modules may be combined or otherwise rearranged in various embodiments. In some embodiments, medium  may store a subset of the modules and data structures identified above. Furthermore, medium  may store additional modules and data structures not described above.","Operating system  includes various procedures, sets of instructions, software components and\/or drivers for controlling and managing general system tasks (e.g., memory management, storage device control, power management, etc.) and facilitates communication between various hardware and software components.","Communication module  facilitates communication with other devices over one or more external ports  or via RF circuitry  and includes various software components for handling data received from RF circuitry  and\/or external port .","Graphics module  includes various known software components for rendering, animating and displaying graphical objects on a display surface. In embodiments in which touch I\/O device  is a touch sensitive display (e.g., touch screen), graphics module  includes components for rendering, displaying, and animating objects on the touch sensitive display.","One or more applications  can include any applications installed on system , including without limitation, a browser, address book, contact list, email, instant messaging, word processing, keyboard emulation, widgets, JAVA-enabled applications, encryption, digital rights management, voice recognition, voice replication, location determination capability (such as that provided by the global positioning system (GPS)), a music player, etc.","Touch processing module  includes various software components for performing various tasks associated with touch I\/O device  including but not limited to receiving and processing touch input received from I\/O device  via touch I\/O device controller .","System  may further include avatar editing module  for performing the method\/functions as described herein in connection with . Avatar editing module  may at least function to provide the avatar editing environment described with respect to . Avatar editing module  may also interact with other elements of system  to provide the avatar editing functions. Avatar editing module  may be embodied as hardware, software, firmware, or any combination thereof. Although module  is shown to reside within medium , all or portions of module  may be embodied within other components within system  or may be wholly embodied as a separate component within system .","I\/O subsystem  is coupled to touch I\/O device  and one or more other I\/O devices  for controlling or performing various functions. Touch I\/O device  communicates with processing system  via touch I\/O device controller , which includes various components for processing user touch input (e.g., scanning hardware). One or more other input controllers  receives\/sends electrical signals from\/to other I\/O devices . Other I\/O devices  may include physical buttons, dials, slider switches, sticks, keyboards, touch pads, additional display screens, or any combination thereof.","If embodied as a touch screen, touch I\/O device  displays visual output to the user in a GUI. The visual output may include text, graphics, video, and any combination thereof. Some or all of the visual output may correspond to user-interface objects. Touch I\/O device  forms a touch-sensitive surface that accepts touch input from the user. Touch I\/O device  and touch screen controller  (along with any associated modules and\/or sets of instructions in medium ) detects and tracks touches or near touches (and any movement or release of the touch) on touch I\/O device  and converts the detected touch input into interaction with graphical objects, such as one or more user-interface objects. In the case in which device  is embodied as a touch screen, the user can directly interact with graphical objects that are displayed on the touch screen. Alternatively, in the case in which device  is embodied as a touch device other than a touch screen (e.g., a touch pad), the user may indirectly interact with graphical objects that are displayed on a separate display screen embodied as I\/O device .","Touch I\/O device  may be analogous to the multi-touch sensitive surface described in the following U.S. Pat. No. 6,323,846 (Westerman et al.), U.S. Pat. No. 6,570,557 (Westerman et al.), and\/or U.S. Pat. No. 6,677,932 (Westerman), and\/or U.S. Patent Publication 1202\/0015024A1, each of which is hereby incorporated by reference.","Embodiments in which touch I\/O device  is a touch screen, the touch screen may use LCD (liquid crystal display) technology, LPD (light emitting polymer display) technology, OLED (organic LED), or OEL (organic electro luminescence), although other display technologies may be used in other embodiments.","Feedback may be provided by touch I\/O device  based on the user's touch input as well as a state or states of what is being displayed and\/or of the computing system. Feedback may be transmitted optically (e.g., light signal or displayed image), mechanically (e.g., haptic feedback, touch feedback, force feedback, or the like), electrically (e.g., electrical stimulation), olfactory, acoustically (e.g., beep or the like), or the like or any combination thereof and in a variable or non-variable manner.","System  also includes power system  for powering the various hardware components and may include a power management system, one or more power sources, a recharging system, a power failure detection circuit, a power converter or inverter, a power status indicator and any other components typically associated with the generation, management and distribution of power in portable devices.","In some embodiments, peripherals interface , one or more processors , and memory controller  may be implemented on a single chip, such as processing system . In some other embodiments, they may be implemented on separate chips.","The described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from, and to transmit data and instructions to, a data storage system, at least one input device, and at least one output device. A computer program is a set of instructions that can be used, directly or indirectly, in a computer to perform a certain activity or bring about a certain result. A computer program can be written in any form of programming language (e.g., Objective-C, Java), including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.","Suitable processors for the execution of a program of instructions include, by way of example, both general and special purpose microprocessors, and the sole processor or one of multiple processors or cores, of any kind of computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally, a computer will also include, or be operatively coupled to communicate with, one or more mass storage devices for storing data files; such devices include magnetic disks, such as internal hard disks and removable disks; magneto-optical disks; and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, ASICs (application-specific integrated circuits).","To provide for interaction with a user, the features can be implemented on a computer having a display device such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.","The features can be implemented in a computer system that includes a back-end component, such as a data server, or that includes a middleware component, such as an application server or an Internet server, or that includes a front-end component, such as a client computer having a graphical user interface or an Internet browser, or any combination of them. The components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include, e.g., a LAN, a WAN, and the computers and networks forming the Internet.","The computer system can include clients and servers. A client and server are generally remote from each other and typically interact through a network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.","One or more features or steps of the disclosed embodiments can be implemented using an API. An API can define on or more parameters that are passed between a calling application and other software code (e.g., an operating system, library routine, function) that provides a service, that provides data, or that performs an operation or a computation.","The API can be implemented as one or more calls in program code that send or receive one or more parameters through a parameter list or other structure based on a call convention defined in an API specification document. A parameter can be a constant, a key, a data structure, an object, an object class, a variable, a data type, a pointer, an array, a list, or another call. API calls and parameters can be implemented in any programming language. The programming language can define the vocabulary and calling convention that a programmer will employ to access functions supporting the API.","In some implementations, an API call can report to an application the capabilities of a device running the application, such as input capability, output capability, processing capability, power capability, communications capability, etc.","A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made. For example, elements of one or more implementations may be combined, deleted, modified, or supplemented to form further implementations. As yet another example, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, or steps may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other implementations are within the scope of the following claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIGS. 1A-1E"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIGS. 3A-3C"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIGS. 4A and 4B"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIGS. 5A-5C"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIGS. 6A-6B"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIGS. 7A-7C","FIGS. 1-6"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 9","FIGS. 1-8"]},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 10","FIGS. 1-9"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 12"}]},"DETDESC":[{},{}]}
