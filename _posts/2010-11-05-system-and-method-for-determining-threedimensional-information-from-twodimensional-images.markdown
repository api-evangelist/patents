---
title: System and method for determining three-dimensional information from two-dimensional images
abstract: A system and method of determining three-dimensional data for an object by performing optical flow analysis to calculate surface profile analysis for a group of monocular component images that vary wavelength distribution, and determining three-dimensional data relating to the object from the component images and the surface profile analysis. Stereo image features are obtained from a single monocular polychromatic raw image or multiple monocular grayscale images having known spectral imaging collection data. Apparatus are described which allow three-dimensional data for an object to be determined from one or more two-dimensional images of the object.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08149268&OS=08149268&RS=08149268
owner: The United States of America as represented by the Secretary of the Army
number: 08149268
owner_city: Washington
owner_country: US
publication_date: 20101105
---

{"@attributes":{"id":"description"},"GOVINT":[{},{}],"heading":["GOVERNMENT INTEREST","CROSS REFERENCE TO RELATED APPLICATIONS","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["The invention described herein may be manufactured, used, and licensed by or for the United States Government.","This application is related to U.S. application Ser. No. 12\/940,204 entitled \u201cSYSTEM AND METHOD FOR DETERMINING THREE-DIMENSIONAL INFORMATION FROM PHOTOEMISSION INTENSITY DATA,\u201d invented by Ronald Meyers and David Rosen, filed on even date herewith and hereby incorporated by reference, and U.S. application Ser. No. 12\/940,228 entitled \u201cSYSTEM AND METHOD FOR MEASURING DEPOLARIZATION,\u201d invented by David Rosen and Ronald Meyers, filed on even date herewith and hereby incorporated by reference.","The present invention relates to apparatus and methods for determination of three-dimensional data, in particular to determining three-dimensional data from one or more two-dimensional images.","Generally speaking, binocular stereo imaging requires at least two viewpoints, which may need to be spatially distant from one another for large distances between a viewer and an object.","Photographs taken by satellite can be obtained over the interne, revealing a great deal of information in a two-dimensional format. However, obtaining three-dimensional information is useful in such areas as agricultural forecasting, environmental monitoring, forensics, intelligence gathering, object detection (including detection of camouflaged objects using three-dimensional data), target acquisition, remote mapping, and the like.","The unaided eye partially uses photometric stereo methods of depth profiling, in addition to binocular stereo methods, to acquire depth perception. Realistic presentation of images as perceived by the unaided eye is needed in visualization software and training material, which therefore should include photometric stereo information in addition to binocular stereo information. Photometry relates to the measurement of light, in terms of its perceived brightness to the human eye, as distinguished from radiometry, which is the science of measurement of radiant energy (including light) in terms of absolute power. In photometry, the radiant power at each wavelength is weighted by a luminosity function (a.k.a. visual sensitivity function) that models human brightness sensitivity. Photometric stereo, as used herein, is a technique in computer vision for estimating the surface normals of objects by observing that object under different lighting conditions. A surface depth profile value can be calculated when using stereo methods.","Methods relating to the determination of depth and\/or surface depth profile from the optical field are described in J. R. A. Torre\u00e3o, \u201cNatural Photometric Stereo,\u201d Anois do IX Sibgrabi, 95-102 (October 1996) and J. R. A. Torre\u00e3o and J. L. Fernandes, \u201cMatching photometric stereo images\u201d, J. Opt. Soc. Am., A15(12), 2966-2975, (1998), both of which are hereby incorporated by reference.","The article entitled \u201cNatural Photometric Stereo\u201d discusses the human brain's ability to infer shape from the binocular fusion of some kinds of monocular images. Photometric stereo (PS) images have been observed which are monocular images obtained under different illuminations, that produce a vivid impression of depth, when viewed under a stereoscope. According to the article, the same is true of pairs of images obtained in different spectral bands. The \u201cNatural Photometric Stereo\u201d discusses employing an optical-flow based photometric stereo algorithm; a type of \u201ccolour\u201d separated images, which have been so produced as to emulate the kinds of records generated by the photosensitive cells in the human retina, to obtain depth estimates from them. The \u201cNatural Photometric Stereo\u201d article speculates on the possibility that a process similar to PS could work on the human visual system. A natural photometric stereo process is postulated in the \u201cNatural Photometric Stereo\u201d article, invoking some physical and biological arguments, along with experimental results, in support thereof.","In the article entitled \u201cMatching photometric stereo images,\u201d a process of shape estimation is introduced through the matching of photometric-stereo images, which are monocular images obtained under different illuminations. According to the \u201cMatching photometric stereo images\u201d article, if the illumination directions are not far apart, and if the imaged surface is smooth, so that a linear approximation to the reflectance map is applicable, the disparities produced by the matching process can be related to the depth function of the imaged surface through a differential equation whose approximate solution is can be found. The \u201cMatching photometric stereo images\u201d article presents a closed-form expression for surface depth, depending only on the coefficients of the linear-reflectance-map function. If those coefficients are not available, a simple iterative scheme still allows the recovery of depth, up to an overall scale factor.","Various articles have been written on the spatial arrangement of objects and depth perception. In the publication by B. K. P. Horn and B. G. Schunk, \u201cDetermining Optical Flow\u201d, MIT Artificial Intelligence Laboratory, 572, 0-27 (1980)(Horn and Schunk article), hereby incorporated by reference, there is a description of a method for finding optical flow, which is defined in the Horn and Schunk article as:\n\n","According to the Horn and Schunk article, in general optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented in the Horn and Schunk article which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm used in Horn and Schunk article reportedly can handle image sequences that are quantized rather coarsely in space and time and is, reportedly, insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.","In light of the above, there exists a need to obtain three-dimensional information about an object from existing two-dimensional images, such as photographic prints, without the need to return to a given viewpoint to obtain further information.","A method of determining three-dimensional data for an object by performing optical flow analysis to calculate surface profile analysis for a group of monocular component images that vary wavelength distribution, and determining three-dimensional data relating to the object from the component images and the surface profile analysis. Stereo image features are obtained from a single monocular polychromatic raw image or multiple monocular grayscale images having known spectral imaging collection data. The surface depth profile (also termed herein surface height profile) of the object relative to an image plane or illumination vectors for the object provides three-dimensional object data in a raw image. The raw monocular image is input by scanning a photograph, receiving a video signal, or receiving the image from an imaging device. The method optionally includes color correction, or adjusting the wavelength distribution of the image using a correction factor to refine stereo image quality. The correction factor is optionally correlated with the identity of the imaging device, or a recording medium. Image formation data (identity of imaging device, ambient conditions during imaging, and the like) are provided with the image, and may include a correction factor or allow one to be determined. Color correction may also use the known properties of object(s) within the image. The three-dimensional data is used in an improved method of object recognition by providing a stereo image of the object.","An apparatus for determining a three-dimensional data relating to an object from an image of the object includes an image input (such as an optical scanner). An image decomposer decomposes the image into a plurality of component images (for example, an algorithm or plurality of optical filters), an optical flow calculator operating on a computer determines optical flow data from the component images, and a three-dimensional data calculator algorithm operating on the computer calculates the three-dimensional data from the optical flow data. The apparatus optionally includes an image corrector, such as an optical filter modifying the image input, or a software correction using an algorithm. The correction factor used may be retrieved from a memory device (such as a database) based on image formation data. The apparatus may further comprise an output device, such as a visual display or a data communications device.","The present invention has utility as a method and apparatus for extracting three-dimensional data from one or more two-dimensional images.","Monocular images, such as previously recorded photographs, are analyzed using methods according to the present invention, allowing three-dimensional information to be extracted from an image even if it was recorded at a previous time with no intention of recording any three-dimensional data. The information extracted does not have to be real-time, as in some computer-vision applications of photometry, and need not be used for immediate evaluation of three-dimensional shape.","For example, stereo images derived from a monocular color photograph, such as a picture postcard, could be used with this invention for stereo image analysis and stereo map making. A picture postcard is generally not intended as a stereo map of the landscape, but the photometric techniques described herein allow extraction of a three-dimensional image useful for map making and finds applications in constructing changes in an area as a function of time.","Applications of embodiments of the present invention include agricultural forecasting, environmental monitoring, forensics, intelligence gathering, object detection (including detection of camouflaged objects using three-dimensional data), target acquisition, remote mapping, and the like. For example, archived two-dimensional color images, made with a monocular camera and other equipment not intended for map making, can be used to determine three-dimensional information about terrain. Photometric stereo approaches can be used at large object distances, such as those distances above which binocular stereo can be used with conventional equipment.","Images may include photographs (including postcards), television images, infrared spectral images, ultraviolet spectral images, x-ray spectral images, microwave spectral images, and other images recorded with no optimization for three-dimensional information. The image is analyzed for three-dimensional information and illumination information valuable for reasons not foreseen by the individuals originally recording the image.","Image formation associated with an image may be used in an inventive method but is not required. Such image formation data associated with an image illustratively includes imaging device data (sensitivity parameters for the film and\/or imaging device (such as a camera) used to make the image, identity of imaging device, type of film, and the like), imaging condition data (time of image, such as ambient conditions at the time the image was made, solar elevation or other lighting conditions, relative solar alignment, direction of view, distance to objects imaged, altitude (of imaging device and\/or objects imaged), meteorological conditions such as the presence of cloud or precipitation time of day, and the like), color correction factors, or other data associated with the image.","A surface depth profile is a useful calculated value using stereo methods. The surface depth profile is determined by equations that are presented here. Two different ways that the atmosphere impacts photometric stereo are shown: by providing illumination for passive photometric stereo imaging, and by attenuating the reflected EMR differently at different wavelengths. Effective angles of illumination for two different wavelengths (400 nm and 700 nm) are calculated from published experimental data. The utility of photometric stereo is evaluated at different ground albedos and directions of the sun for the atmospheric conditions of the published data. It is noted that the albedo of an object is a measure of how strongly the object reflects light (reflectivity) from light sources such as the Sun; defined as the ratio of total-reflected to incident electromagnetic radiation; ranging from 0 (dark) to 1 (bright). The size of the error caused by wavelength dependent scattering is estimated and the validity of the equations present are also discussed for attenuation that is strongly dependent on wavelength.","According to the present invention, the surface-height profile and illumination information are determined from a two-dimensional color image, such as film images or digital camera images made with a single view (monocular) color camera. Other image data inputs for the present invention include multiple monocular images collected that vary collection wavelengths or time of collection, thereby extending the present invention to analysis of grayscale, infrared, ultraviolet, and video imagery to glean three-dimensional features from monocular sources. In a color image, if the object was illuminated from several sources of light at different angles, the wavelength, distribution of radiances on the object surface depends both on the surface depth profile of the object and on angles of illuminating radiation at the time of the image recording. Photometric techniques are used to determine the surface depth profile of the object and angles of illuminating radiation at the time of the image recording.","Photometric methods are used in the present invention to extract stereo image information from previously recorded monocular color images, including images that were recorded with no intention of ever using them to obtain stereo imaging. The variations of height of the surface of an object relative to a background plane are also optionally calculated. This variation is defined as the height above the background plane in the image field. The variation in height, referred to synonymously as the surface depth profile of the object, is three-dimensional information, synonymously referred to as stereo information, relating to the object. Representative background planes used for height variations include a monocular image taken from an aerial perspective, for which the background plane is a generally planar ground surface and often an orthogonal plane relative to the monocular camera, or a monocular image taken from the ground, in which case the background plane is often a vertically aligned surface.","It is appreciated that while a background plane is most easily extracted as a planar surface, modeling a background plane as a contoured surface optionally is used herein to refine stereo image data extracted. The use of a contoured surface as a background plane is well suited for instances where a monocular image background has at least three points amenable to topographical indexing.","The determination of stereo information from a two-dimensional image is referred to herein as photometric imaging. Although a single view, synonymously monocular, camera is not designed to optimize the extraction of stereo image information, stereo image information is extracted from color images by mathematical processing according to the present invention.","According to the present invention, optical filters or other wavelength discriminating equipment are used to decompose a single color image into multiple component wavelength discriminated images, with the component images varying from one component image to another in intensity distribution as a function of wavelength.","Optionally, a color correction is made to the raw monocular image or a component image to account for non-uniform wavelength sensitivity of the monocular imaging device.","In a preferred embodiment, when a band pass optical filter is used for image decomposition to form component images, each of the component images obtained from the original raw monocular image corresponds to wavelength distribution associated with the passed bandwidth of the optical filter. These component images, derived from decomposition of the original raw monocular color image, are used to calculate an optical flow field. An optical flow field is defined as a field in which each point of the optical flow field is assigned a two-dimensional vector representing the optical flow. The surface depth profile is then extracted from the optical flow data. A three-dimensional image is then constructed using both the mean of the series of component images and the surface depth profile. Image decomposition is also optionally performed with successive monochromatic wavelength sources such as individual laser lines to yield monochromatic component images. With reference data as to time of raw image collection, weather conditions, and solar spectrum, further refinements in three-dimensional image extraction and in particular optical field data are obtained from monochromatic component images.","In addition to, or instead of the surface depth profile, the illumination vector(s) of an object at the time of the image recordation are also calculated from the raw color monocular image. The illumination field is optionally calculated from the three-dimensional image itself, or from the two-dimensional vector mapping defining the optical field. An illumination field is defined herein as the field where a three-dimensional vector is assigned to each point of the field, representing the direction of radiation that is illuminating the object at that point. An intensity distribution in terms of the angles describing the illumination is represented for each of the component images derived from the original color monocular image. If narrow band optical filters are used, the illumination field at each point of the image corresponds to the direction that electromagnetic radiation is propagating at that wavelength of the image.","In examples of the present invention, photometric stereo analysis is applied to an image that has already been recorded using conventional monocular color imaging. The inventive method is readily applied to archival monocular color photographs, even if the equipment used to record the photograph was not designed for stereo imaging and even if imaging conditions were not recorded for the raw color image. The term photograph includes photographic prints, negatives, slides, digital analogs thereof, infrared images, ultraviolet images, and the like.","The three-dimensional image information assists in the identification of a photographed object. The illumination distribution of the surface of the object can characterize the time of day in an outdoor photography, or the indoor lighting for an indoor photograph. Therefore, a method according to the present invention can extract historical or forensic information from a color photograph, and finds representative applications as diverse as refining agricultural crop growth estimations, estimating ice sheet thickness, and discerning camouflaged image objects in addition to those applications detailed above.","The decomposition of the color image into component images is achieved in several ways, depending on the precision and accuracy required for stereo image reconstruction. For instance, a photograph is reimaged with a digital imaging device, and a brightness distribution at different wavelengths calculated and assigned based on the type of film used. This facilitates compensating for wavelength bias in the original imaging system film of the pixel array. In instances when one or more objects depicted in a raw image has a known wavelength distribution, optionally the known wavelength distribution object is used to compensate for wavelength bias in the original imaging system film of pixel array. An example of an object with a known wavelength distribution is leaves on a tree that has a known optical absorption spectrum.",{"@attributes":{"id":"p-0040","num":"0041"},"figref":["FIG. 1","FIG. 1"],"b":["10","10"]},"It is appreciated that a set of component images may be received as image data with the component images varying in collection wavelength distributions. For instance, when the color and infrared monocular images or color filtered image sets are collected, these images are readily used as input image data and obviate the need for wavelength image decomposition. Optical flow is then determined using image set groups selected from the component images, in an analogous method to that used for the component images formed from decomposition of a single image.","Image data for three-dimensional shape analysis of the present invention is applied to archived color photographs made with single view film cameras, images from a database of digital images, and analysis of video information (for example, color television recordings, and other color video recordings made with a single view video camera). Video stills may be analyzed in a similar manner to other images. Time-dependent stereo information can be obtained from video recordings including color movies, color TV, or any other information, series of color images that vary as a function of time, and independent of classical stereographic alignment of the images to afford depth perception.","The present invention is also amenable to determination of the geometric angular distribution of illumination from archived color images made with single view digital or film cameras, color television recordings, and other color video images made with a single view video camera. Time-dependent illumination distributions can be determined from video recordings that vary as a function of time.","It is appreciated that according to the present invention, archived grayscale images recorded with several single view cameras are also operative in situations where the multiple monocular grayscale images which were collected on a media that had different color sensitivities are used as image data input. Each grayscale image collecting camera corresponds to a different distribution of wavelengths, which is known if the type of film or type of camera is known, even if the individual images are black and white. For example a distant object can be photographed by two viewers close to each other, one grayscale camera with sensitivity peaked at yellow wavelengths and the other camera having a sensitivity peaked at green wavelengths are equated to component images as image input per . A plurality of images is obtained, each representing a different wavelength distribution of the object.","The present invention is also operative with archived grayscale images recorded while the wavelength of the source is varying with time are used as image data input. Each moment of time corresponds to a different distribution of illumination wavelengths, which is known if the type of film or type of camera is known, even if the individual images are black and white. An example would be an image made at the same time of day but on days with different atmospheric conditions, so that the color of illumination changes. Archived geosynchronous satellite images are representative of such a source of images.","Methods according to the present invention also are operative with images containing an object with a known BRDF (Bidirectional Reflectance Distribution Function) as an image data input with the BRDF object then being used as a reference to extract information such as stereo image information of other objects in the image, illuminating wavelength information, camera sensitivity information, and atmospheric information.","If necessary, the input color image data  is digitized at step .","The wavelength distribution of the image is optionally adjusted using a color correction factor at step . The correction factor may, for example, be included with or determined from image formation data relating to the formation of the image. The correction factor may be input as part of the color image data in step , or determined from other input data. For example, an identification of the imaging device used to form the image may be used to retrieve (e.g. from a memory device) or otherwise generate the correction factor. The correction factor also is optionally generated from the uncorrected wavelength distribution of objects within the image that have a known wavelength distribution such as leaves, a known substance such as a rock, mineral or ice. Color correction is optional and used as a three-dimensional output image refinement.","The color image is decomposed into multiple component images, the component images distinguished from one another by different wavelengths being used to form the component images at step . The result of the image decomposition is a plurality of two-dimensional component images, each component image corresponding to a different wavelength distribution of the original image. The component images are each independently constructed from a wavelength band or a monochromatic wavelength.","The image decomposition of step  is achieved in several ways. A digital image may be separated into a plurality of narrow band color images using digital processing. A photograph may be decomposed using narrow band optical filters, wavelength discriminating photo detectors or monochromatic wavelengths. It is appreciated that step  is optionally performed in concert with the digitization step  or the input stage .","The optical flow (O.F.) for groups (such as pairs) of component images, synonymously referred to as sub-images, is then calculated at step . In examples such as where a large sequence of component images exists, a three-dimensional vector is optionally determined for each point in the image field.","The surface depth profile for each point within the image field is calculated at step  from the optical field (O.F.) analysis at step . The stereo (three-dimensional) image information, including information from two-dimensional image decompositions and surface depth profile, is calculated at step . The stereo information determined at step  is optionally output as a single or series of stereo images .","The illumination vectors, namely the three-dimensional vectors describing intensity and direction of flux at every wavelength and every point on the surface of the stereo image , are optionally calculated at step  and used to generate illumination distribution direction information as an output at step . The process of steps  and  are optional image refinements to improve resolution and are typically performed if the stereo images  are deemed to provide less information than desired.",{"@attributes":{"id":"p-0054","num":"0055"},"figref":"FIG. 2","b":["40","42","44","46","48","50"]},"The image input  is a scanner, other imaging device such as a digital camera used to image a printed photograph of an object, a camera, a data link to a camera used to directly image an object, a database of images, or a video camera. The image input optionally includes an additional data input, such as a keyboard, wireless receiver, or other data input, through which image formation data associated with an image is received. Image formation data illustratively includes imaging device data, imaging condition data, or other data associated with the image.","The color corrector  is optionally used to act to correct the color balance of the input image. This is achieved by an automated process, for example an algorithm, or by manual balancing of pixels. The color corrector  optionally is placed before the image input, to act as a filter. It is appreciated that the color corrector  may be omitted, or combined with the image input , for example as a software correction to spectral response, which is applied by electronic circuitry within the image input.","The image correction input  operates in communication with a database of color correction factors, from which a correction factor linked to the identity of the imaging device used to provide the image can be retrieved. The image correction input  is optional and provides output refinement as desired.","The image decomposer  is implemented entirely in software, for example using image processing software. The image decomposer  is optionally combined with the image input , for example allowing input of component images with different spectral responses, for example by filtering the image before input. In this case, the image decomposer  includes a number of color filters used in association with an image input  such as a scanner. It is appreciated that an image may be scanned at different wavelength ranges, or with a polychromatic band pass set of wavelengths or a monochromatic wavelength.","An optical flow calculator  is a conventional algorithm operating on a computer.","Optical Flow Calculations","Any appropriate method can be used to calculate optical flow, including that described in B. K. P. Horn and B. G. Schunk, \u201cDetermining Optical Flow,\u201d 572, 0-27 (1980), hereby incorporated by reference. Any appropriate method can be used to calculate surface depth profile from the optical field, including the methods described in J. R. A. Torre\u00e3o and J. L. Fernandes, \u201cMatching photometric stereo images\u201d, 15(12), 2966-2975, (1998) and J. R. A. Torre\u00e3o, \u201cNatural Photometric Stereo,\u201d 95-102 (October 1996), both of which are hereby incorporated by reference.","Further discussion of optical flow calculations are given below. A three-dimensional data image calculator  is an algorithm operating on a computer with the data extracted from a monocular view, using a sequence of two-dimensional images from which optical flow is calculated. The sequence of images can be generated from a single original image, by decomposing the image into multiple component images of different wavelength distributions, or the sequence of two-dimensional images is also readily formed by imaging an object under different conditions, as discussed in more detail below.","As used in the following claims, optical flow is defined as the apparent velocity of a localized region on the image, and can be calculated from two (or more) component images of different wavelength. These component images can be formed from the decomposition of a single color image, a single black-and-white image, or from multiple images obtained under different illumination conditions.","Optical flow data at a coordinate, and an image at a particular wavelength, are defined by a pair of equations for the two components:",{"@attributes":{"id":"p-0064","num":"0065"},"maths":[{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["D","x"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y","\u03be"],"mo":[",",","]}}},"mo":"=","mfrac":{"mi":"dx","mrow":{"mi":["d","\u03be"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}},{"@attributes":{"id":"MATH-US-00001-2","num":"00001.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["D","y"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y","\u03be"],"mo":[",",","]}}},"mo":"=","mfrac":{"mi":"dy","mrow":{"mi":["d","\u03be"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}}],"br":{},"sub":["x ","y "]},"The optical flow at each image point x and y can be calculated using two consecutive images that can be described as brightness value matrices. The brightness element is defined so that its brightness is constant in wavelength. The rate that brightness changes with respect to wavelength is zero, which means",{"@attributes":{"id":"p-0066","num":"0067"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":[{"mi":"dE","mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"\u2192"}}},{"mi":["d","\u03be"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},"mo":"=","mn":"0"}}},"br":{},"sub":["x","y "]},"The chain rule of differentiation applies to the rate of changing brightness. A continuity condition can be derived from the chain rule and expressed by:",{"@attributes":{"id":"p-0068","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mfrac":[{"mrow":[{"mo":"\u2202","mi":"E"},{"mo":"\u2202","mi":"x"}]},{"mi":"dx","mrow":{"mi":["d","\u03be"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}],"mo":"\u2062"},{"mfrac":[{"mrow":[{"mo":"\u2202","mi":"E"},{"mo":"\u2202","mi":"y"}]},{"mi":"dy","mrow":{"mi":["d","\u03be"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}],"mo":"\u2062"}],"mo":["+","+"],"mfrac":{"mrow":[{"mo":"\u2202","mi":"E"},{"mo":["\u2202","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"\u03be"}]}},"mo":"=","mn":"0"}}},"br":{}},{"@attributes":{"id":"p-0069","num":"0070"},"maths":[{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["E","x"]},"mo":"=","mfrac":{"mrow":[{"mo":"\u2202","mi":"E"},{"mo":"\u2202","mi":"x"}]}},{"msub":{"mi":["E","y"]},"mo":"=","mfrac":{"mrow":[{"mo":"\u2202","mi":"E"},{"mo":"\u2202","mi":"y"}]}}],"mo":[",","\u2062",","],"mstyle":{"mtext":{}},"mi":"and"}}},{"@attributes":{"id":"MATH-US-00004-2","num":"00004.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["E","\u03be"]},"mo":"=","mrow":{"mfrac":{"mrow":[{"mo":"\u2202","mi":"E"},{"mo":"\u2202","mi":"\u03be"}]},"mo":"."}}}}]},"The condition of continuity is a constraint necessary to specify a unique solution to the surface depth. However, condition of continuity is insufficient to calculate a unique solution to the optical flow. The perceived direction of optical flow will always be in the direction parallel to the gradient of brightness. If the optical flow component that is parallel to the brightness gradient in the xy-plane is designated as {right arrow over (D)}, where\n\n,\n\nthen the continuity condition can be shown to be equivalent to\n",{"@attributes":{"id":"p-0071","num":"0072"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mover":{"mi":"D","mo":"\u2192"},"mi":"xy"},"mo":"=","mrow":{"mrow":{"mo":"-","mfrac":{"msub":{"mi":["E","\u03be"]},"msqrt":{"mrow":{"msubsup":[{"mi":["E","x"],"mn":"2"},{"mi":["E","y"],"mn":"2"}],"mo":"+"}}}},"mo":"\u2062","msub":{"mover":{"mi":"n","mo":"^"},"mi":"xy"}}}}},"br":{},"sub":"xy "},"In a photometric stereo method, at least two images are used to calculate the surface depth value, designated z, of a perceived three-dimensional object. The slopes of the surface depth on the perceived object are then defined by p and q where",{"@attributes":{"id":"p-0073","num":"0074"},"maths":[{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"p","mo":"=","mrow":{"mrow":[{"mo":"-","mfrac":{"mrow":[{"mo":"\u2202","mi":"z"},{"mo":"\u2202","mi":"x"}]}},{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"}],"mo":["\u2062","\u2062"],"mi":"and"}}}},{"@attributes":{"id":"MATH-US-00006-2","num":"00006.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"q","mo":"=","mrow":{"mo":"-","mrow":{"mfrac":{"mrow":[{"mo":"\u2202","mi":"z"},{"mo":"\u2202","mi":"y"}]},"mo":"."}}}}}]},"The components p and q are proportional to |D{circumflex over (n)}+D{circumflex over (n)}|, which is the magnitude of the optical flow. The component of optical flow, {right arrow over (D)}, that is perpendicular to the brightness gradient, {right arrow over (E)}, cannot be determined without another constraint. Constraints are necessary to determine a unique optical flow, which is necessary to calculate a unique surface depth function. Both analytical and numerical algorithms are available for calculating the optical flow.","For example, methods of calculating optical flow were developed by Horn and Schunk. See, Horn & Schunk, \u201cDetermining Optical Flow,\u201d MIT Artificial Intelligence Laboratory Memo, 572, 0-27 (1980). The constraint used in this example was the smoothness constraint, which limits the motion of the image in a way that the image can be visualized as sliding on the surface of the object being imaged.","The equations for optical flow can solved using numerical methods, for example described below. To digitize E, D, and D, integer values are assigned to x, y and \u03be, so x\u2192j; y\u2192i; and \u03be\u2192k where i, j, k=0, . . . , N. The local partial derivatives of {right arrow over (E)} are averaged for a cube of adjacent values of i, j, and k by",{"@attributes":{"id":"p-0077","num":"0078"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"msub":{"mover":{"mi":["E","_"]},"mi":"x"},"mo":["=","\u2062"],"mi":{},"mrow":{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mrow":{"mo":"{","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mi":["i","k"],"mo":[",",","],"mrow":{"mi":"j","mo":"+","mn":"1"}}},{"mi":["E","ijk"]}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"+","mn":"1"}],"mo":[",",","],"mi":"k"}},{"mi":"E","mrow":{"mrow":{"mi":"i","mo":"+","mn":"1"},"mo":[",",","],"mi":["j","k"]}}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mi":"i","mo":[",",","],"mrow":[{"mi":"j","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}]}},{"mi":"E","mrow":{"mi":["i","j"],"mo":[",",","],"mrow":{"mi":"k","mo":"+","mn":"1"}}}],"mo":"-"}}],"mo":["+","+","+"]}}}}}},{"mtd":{"mrow":{"mi":{},"mo":["\u2062","}"],"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}],"mo":[",",","]}},{"mi":"E","mrow":{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}],"mo":[",",","],"mi":"j"}}],"mo":"-"}}}}},{"mtd":{"mrow":{"msub":{"mover":{"mi":["E","_"]},"mi":"v"},"mo":["=","\u2062"],"mi":{},"mrow":{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mrow":{"mo":"{","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mrow":{"mi":"i","mo":"+","mn":"1"},"mo":[",",","],"mi":["j","k"]}},{"mi":["E","ijk"]}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"+","mn":"1"}],"mo":[",",","],"mi":"k"}},{"mi":"E","mrow":{"mi":["i","k"],"mo":[",",","],"mrow":{"mi":"j","mo":"+","mn":"1"}}}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}],"mo":[",",","],"mi":"j"}},{"mi":"E","mrow":{"mi":["i","j"],"mo":[",",","],"mrow":{"mi":"k","mo":"+","mn":"1"}}}],"mo":"-"}}],"mo":["+","+","+"]}}}}}},{"mtd":{"mrow":{"mi":{},"mo":["\u2062","}"],"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}],"mo":[",",","]}},{"mi":"E","mrow":{"mi":["i","j"],"mo":[",",","],"mrow":{"mi":"k","mo":"+","mn":"1"}}}],"mo":"-"}}}}},{"mtd":{"mrow":{"msub":{"mover":{"mi":["E","_"]},"mi":"\u03be"},"mo":["=","\u2062"],"mi":{},"mrow":{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mrow":{"mo":"{","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mi":["i","j"],"mo":[",",","],"mrow":{"mi":"k","mo":"+","mn":"1"}}},{"mi":["E","ijk"]}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}],"mo":[",",","],"mi":"j"}},{"mi":"E","mrow":{"mrow":{"mi":"i","mo":"+","mn":"1"},"mo":[",",","],"mi":["j","k"]}}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mi":"i","mo":[",",","],"mrow":[{"mi":"j","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}]}},{"mi":"E","mrow":{"mi":["i","k"],"mo":[",",","],"mrow":{"mi":"j","mo":"+","mn":"1"}}}],"mo":"-"}}],"mo":["+","+","+"]}}}}}},{"mtd":{"mrow":{"mi":{},"mo":["\u2062","}"],"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"E","mrow":{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}],"mo":[",",","]}},{"mi":"E","mrow":{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"+","mn":"1"}],"mo":[",",","],"mi":"k"}}],"mo":"-"}}}}}]}}},"br":{},"sub":["x","y ","\u03be","x","y","\u03be"]},"The optical flow, Dand D, can be calculated from the averaged derivatives by an iterative algorithm, for example one described by Horn and Schunk. Below, the terms Dand Dare reassigned as u and v to match the nomenclature of that article. If",{"@attributes":{"id":"p-0079","num":"0080"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mover":{"mi":["u","_"]},"mi":"ijk","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"=","mrow":{"mrow":[{"mfrac":{"mn":["1","6"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"msubsup":[{"mi":"u","mrow":[{"mrow":{"mi":"i","mo":"-","mn":"1"},"mo":[",",","],"mi":["j","k"]},{"mo":["(",")"],"mi":"n"}]},{"mi":"u","mrow":[{"mi":["i","k"],"mo":[",",","],"mrow":{"mi":"j","mo":"+","mn":"1"}},{"mo":["(",")"],"mi":"n"}]},{"mi":"u","mrow":[{"mrow":{"mi":"i","mo":"+","mn":"1"},"mo":[",",","],"mi":["j","k"]},{"mo":["(",")"],"mi":"n"}]},{"mi":"u","mrow":[{"mrow":{"mi":"i","mo":"+","mn":"1"},"mo":[",",","],"mi":["j","k"]},{"mo":["(",")"],"mi":"n"}]}],"mo":["+","+","+"]}}},{"mfrac":{"mn":["1","12"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"msubsup":[{"mi":"u","mrow":[{"mrow":[{"mi":"i","mo":"-","mn":"1"},{"mi":"k","mo":"-","mn":"1"}],"mo":[",",","],"mi":"k"},{"mo":["(",")"],"mi":"n"}]},{"mi":"u","mrow":[{"mrow":[{"mi":"i","mo":"-","mn":"1"},{"mi":"j","mo":"+","mn":"1"}],"mo":[",",","],"mi":"k"},{"mo":["(",")"],"mi":"n"}]},{"mi":"u","mrow":[{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"+","mn":"1"}],"mo":[",",","],"mi":"k"},{"mo":["(",")"],"mi":"n"}]},{"mi":"u","mrow":[{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"-","mn":"1"}],"mo":[",",","],"mi":"k"},{"mo":["(",")"],"mi":"n"}]}],"mo":["+","+","+"]}}}],"mo":"+"}}}},"br":[{},{},{},{}],"in-line-formulae":[{},{},{},{}],"i":["u","=\u016b","\u2212\u0112","[\u0112","\u016b","+\u0112","\u016b","+\u0112","+\u0112","+\u0112","v","= ","\u2212\u0112","[\u0112","\u016b","+\u0112","\u016b","+\u0112","+\u0112","+\u0112"],"sup":["(n+1)","(n)","(n)","(n)","2","2","2","(n+1)","(n)","(n)","(n)","2","2","2","(n) ","(n) ","2 ","2","2","2","2","2","2","2","2","2","2"],"sub":["x","x","y","\u03be","x","y","y","x","y","\u03be","x","y","x","y","x","y","\u03be","x","y","x","y"],"o":"v"},"The values of uand vdo not depend on uand v, but do depend on their averaged local values \u016band calculated by the following weighted averaging formulas:",{"@attributes":{"id":"p-0081","num":"0082"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mover":{"mi":["\u03c5","_"]},"mi":"ijk","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"=","mrow":{"mrow":[{"mfrac":{"mn":["1","6"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"msubsup":[{"mi":"\u03c5","mrow":[{"mrow":{"mi":"i","mo":"-","mn":"1"},"mo":[",",","],"mi":["j","k"]},{"mo":",","mrow":{"mo":["(",")"],"mi":"n"}}]},{"mi":"\u03c5","mrow":[{"mi":["i","k"],"mo":[",",","],"mrow":{"mi":"j","mo":"+","mn":"1"}},{"mo":["(",")"],"mi":"n"}]},{"mi":"\u03c5","mrow":[{"mrow":{"mi":"i","mo":"+","mn":"1"},"mo":[",",","],"mi":["j","k"]},{"mo":["(",")"],"mi":"n"}]},{"mi":"\u03c5","mrow":[{"mrow":{"mi":"i","mo":"+","mn":"1"},"mo":[",",","],"mi":["j","k"]},{"mo":["(",")"],"mi":"n"}]}],"mo":["+","+","+"]}}},{"mfrac":{"mn":["1","12"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"msubsup":[{"mi":"\u03c5","mrow":[{"mrow":[{"mi":"i","mo":"-","mn":"1"},{"mi":"j","mo":"-","mn":"1"}],"mo":[",",","],"mi":"k"},{"mo":["(",")"],"mi":"n"}]},{"mi":"\u03c5","mrow":[{"mrow":[{"mi":"i","mo":"-","mn":"1"},{"mi":"j","mo":"+","mn":"1"}],"mo":[",",","],"mi":"k"},{"mo":["(",")"],"mi":"n"}]},{"mi":"\u03c5","mrow":[{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"-","mn":"1"}],"mo":[",",","],"mi":"k"},{"mo":["(",")"],"mi":"n"}]}],"mo":["+","+"]}}}],"mo":"+"}}}}},"If Eis, for each k, an Nby Nmatrix (i.e., i=0, 1, 2, 3; Nand j=0, 1, 2, 3; . . . , N), then the optical flow at each k is an Nby Nmatrix.","The initial values (n=0) of the optical flow components can be chosen as zero, \u016b= =0, although a better initial value may facilitate convergence. The constants \u2159 and 1\/12 were chosen to optimize convergence using a cube of local values determined by the two images at different wavelengths designated by integers k and k+1.","Optical flow techniques can be used to determine three-dimensional data from two-dimensional image data, including archived images.",{"@attributes":{"id":"p-0085","num":"0086"},"figref":"FIG. 3","b":["62","60","62","64","66","48","50","62","24","68","68","66","68"]},"Atmospheric Effects and Other Approaches","A preferred embodiment of the present invention allows three-dimensional data to be determined from a single view direction, an approach which may be termed photometric stereo imaging. In other embodiments, several wavelengths at different illumination angles may be collected.","Photometric stereo imaging typically uses natural sources of light, such as sunlight or starlight, which are usually multi-wavelength. Electromagnetic radiation (EMR) that is scattered from the atmosphere provides different angular distributions for different wavelengths, which can be used to acquire stereo information (i.e., three-dimensional data). A wavelength distribution of directions characterizes EMR sources for photometric stereo imaging.","Both the angular distribution of intensity at different wavelengths and the atmospheric attenuation have a dependence on atmospheric conditions that may impact a photometric imaging application.","The unaided eye partially uses photometric stereo methods of depth profiling, in addition to binocular stereo methods, to acquire depth perception. Realistic presentation of images as perceived by the unaided eye is needed in visualization software and other applications. A surface depth profile can be determined using the optical flow approach discussed herein.","The atmosphere modifies illumination of an object and directly attenuates the reflected EMR differently at different wavelengths. Effective angles of illumination for two different wavelengths (400 nm and 700 nm) can be calculated from published experimental data.","The utility of photometric stereo was evaluated at different ground albedos and directions of the sun for the atmospheric conditions of the published data. The size of the error caused by wavelength dependent scattering is estimated and the validity of the equations present is also discussed for attenuation that is strongly dependent on wavelength.","Illumination angle distributions that change with wavelength have effective viewing angles that can be calculated as functions of wavelength. An example using radiance profiles from atmospherically scattered EMR (at wavelengths 400 nm and 700 nm) is shown sufficient to calculate effective viewing directions. Although the example shown below is for a specific set of atmospheric conditions, variations in angle distributions and the resulting photometric stereo effects likewise vary with other atmospheric conditions.","A method of determining surface depth profile is now described, assuming that the images come from two wavelengths illuminating the three-dimensional object in two different directions. For example, images may correspond to illumination at different times of day. The angular and wavelength distribution in the atmosphere allows EMR (electromagnetic radiation) scattered in the atmosphere to provide illumination that can be used in photometric imaging.","The surface depth profile of a material in the atmosphere can be calculated using optical flow from the brightness values of two flat images acquired through the atmosphere.","The surface depth profile is extracted from optical quantities other than the angle of the point of view. The surface depth is also optionally extracted from the differential polarization. Polarimetric stereo imaging can also extract stereo information from an object. However, most natural sources of EMR are unpolarized and the degree of polarization after emission is de minimus.","Natural EMR usually has a broad wavelength spectrum. Furthermore, small increments in wavelength are easier to distinguish than small increments in polarization angle. Therefore, photometric stereo imaging is often superior to polarimetric imaging when the polarization background or random noise is large.","The smoothness condition and the continuity condition are together sufficient for specifying the solution to the surface depth profile. The constraint of smoothness can be visualized as the image sliding over the surface of the object being imaged. The constraint on smoothness is mathematically described as the object from one wavelength to the other being transformed by a local rotation by a small angle followed by a translation in a direction perpendicular to the axis of the image, which is mathematically expressed in the smoothness transformation,\n\n\u0394\n\nwhere\n\n\u0394\n\n{right arrow over (\u03b8)}=:\n\nand\n\n\n","{right arrow over (R)} is the position of a point on the three-dimensional object being image, the unit vector {circumflex over (n)}is a displacement of the two-dimensional image perpendicular to the image plane, and both V and Dare correction factors that do not directly affect the orthographic projection (i.e., the parameters that determine the surface depth profile).of optical flow. The z-axis component of the coordinate system origin relative to the image plane determines both Dand V, although Dand V are not part of the surface depth profile. To be a true rotation, the magnitudes of the two intensities should be equal and can be expressed as\n\n|{right arrow over (R)}|\u2248|{right arrow over (R)}+\u0394{right arrow over (R)}|\n\nwhich is only valid if |{right arrow over (\u03b8)}| is small (i.e., |{right arrow over (\u03b8)}|<<1). The equation for the surface of the object can be mathematically described using a function S such that:\n\n()=0\n","The normal to the surface of the object is then the gradient of S (i.e. {right arrow over (\u2207)}S), where",{"@attributes":{"id":"p-0100","num":"0101"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mover":{"mo":["\u2207","\u2192"]},"mo":"\u2062","mi":"S"},{"mrow":[{"mfrac":{"mrow":[{"mo":"\u2202","mi":"S"},{"mo":"\u2202","mi":"x"}]},"mo":"\u2062","msub":{"mover":{"mi":"n","mo":"^"},"mi":"x"}},{"mfrac":{"mrow":[{"mo":"\u2202","mi":"S"},{"mo":"\u2202","mi":"y"}]},"mo":"\u2062","msub":{"mover":{"mi":"n","mo":"^"},"mi":"y"}},{"mfrac":{"mrow":[{"mo":"\u2202","mi":"S"},{"mo":"\u2202","mi":"z"}]},"mo":"\u2062","msub":{"mover":{"mi":"n","mo":"^"},"mi":"z"}}],"mo":["+","+"]}],"mo":"="}}}},"The unit vector in the direction of the gradient of S is designated by the normal vector, {circumflex over (n)},",{"@attributes":{"id":"p-0102","num":"0103"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mover":{"mi":"n","mo":"^"},"mi":"G"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},{"mfrac":{"mn":"1","msqrt":{"mrow":{"msup":[{"mi":"p","mn":"2"},{"mi":"q","mn":"2"}],"mo":["+","+"],"mn":"1"}}},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mrow":{"mo":"-","mi":"p"},"mo":"\u2062","msub":{"mover":{"mi":"n","mo":"^"},"mi":"x"}},{"mi":"q","mo":"\u2062","msub":{"mover":{"mi":"n","mo":"^"},"mi":"y"}},{"mn":"1","mo":"\u2062","msub":{"mover":{"mi":"n","mo":"^"},"mi":"z"}}],"mo":["-","+"]}}}],"mo":"="}}},"br":{}},{"@attributes":{"id":"p-0103","num":"0104"},"maths":[{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"p","mo":"=","mrow":{"mo":"-","mfrac":{"mrow":[{"mo":"\u2202","mi":"z"},{"mo":"\u2202","mi":"x"}]}}}}},{"@attributes":{"id":"MATH-US-00012-2","num":"00012.2"},"math":{"@attributes":{"overflow":"scroll"},"mi":"and"}},{"@attributes":{"id":"MATH-US-00012-3","num":"00012.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"q","mo":"=","mrow":{"mo":"-","mfrac":{"mrow":[{"mo":"\u2202","mi":"z"},{"mo":"\u2202","mi":"y"}]}}}}}]},"The smoothness condition, where the image is sliding over the surface of an object as a function of wavelength, is mathematically described by the optical flow being orthogonal to the unit vector normal to a surface of the object. A smoothness equation that mathematically describes this condition is\n\n\u0394{right arrow over ()}()\u00b7\u00f1()=0\n","An analytic solution to the surface depth profile was derived using the smoothness constraint. The equations for finding the surface depth with the constraint for values of (x{circumflex over (n)}+y{circumflex over (n)})\u22600{circumflex over (n)}is",{"@attributes":{"id":"p-0106","num":"0107"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"z","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}},"mo":"=","mfrac":{"mrow":[{"mrow":[{"msub":{"mi":["D","x"]},"mo":"\u2062","mi":"x"},{"msub":{"mi":["D","y"]},"mo":"\u2062","mi":"y"}],"mo":"+"},{"mi":["Bx","Ay"],"mo":"-"}]}}}},"br":{},"sub":["x","y","G"]},{"@attributes":{"id":"p-0107","num":"0108"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":["\u0394","z"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":"=","mfrac":{"mrow":[{"msub":[{"mi":["D","x"]},{"mi":["D","y"]}],"mo":"+"},{"mi":["B","A"],"mo":"-"}]}}}}},"If V is unknown, the analytical solution for \u0394z requires as input data A and B. The analytical solution is also sensitive to computational noise and is especially difficult to calculate for 0\u2248Bx\u2212Ay.","The equations presented up to the smoothness equation are sufficient to calculate the surface depth profile, \u0394z, if {right arrow over (\u03b8)} known. There are four scalar unknowns (A, B, C, V) and only three linear equations defined by the transformation so that the position of a point on the object is not completely specified by the two conditions (continuity and smoothness) described so far. The actual z-axis component of a the position point on the object is given by z where\n\n()=()\n\nwhere zis an arbitrary real number. The constant zdoes not affect the stereo image shape, which is characterized by {right arrow over (R)}\u2212z{circumflex over (n)}where\n\n\n\nrather than {right arrow over (R)} that contains both object shape and object position information.\n","The arbitrary constant zdoes specify the values of V, D, and {right arrow over (\u03b8)}. Therefore, V is a function of z. However, zcan be chosen so that if z=z(i.e., \u0394z=0), then V=0, which when substituted into the smoothness equation determines the z-component of optical flow as a function of the other components. Therefore, if zis chosen so that V V(z)=0, and \u0394z<<z; then\n\n\n","When V and Dare substituted into the smoothness transformation, a cross product formula results as expressed by\n\n\u0394\n\nfor V(z)=0, and \u0394z<<z.\n","If |{right arrow over (\u03b8)}<<1, the cross product formula represents a rotation which isn't followed by a translation.","The value of zwhere V=0 characterizes the distance between viewer and object. The vector {right arrow over (\u03b8)} at this zcharacterizes the illumination directions on the object. There are three scalar unknowns and three linear equations equivalent to the cross product shown for this choice of z. If these equations are not singular, then there is a unique solution to {right arrow over (\u03b8)}.","The illumination directions are characterized by the rotation vector {right arrow over (\u03b8)}, which is determined by the equation of motion for the image, where the {circumflex over (n)}component is now known. The vector {right arrow over (\u03b8)} that determines effective illumination angles for each wavelength can be calculated after the surface depth profile is specified. The cross product can be expanded out in vector for,\n\n+()=()\u00d7()\n","This equation has three unknowns (A, B, C) and three independent equations. The solution to {right arrow over (\u03b8)} is given by:",{"@attributes":{"id":"p-0116","num":"0117"},"maths":[{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"A","mo":"=","mrow":{"mfrac":{"mn":"1","mrow":{"mi":"y","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":"x","mn":"2"},{"mi":"z","mn":"2"}],"mo":"-"}}}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"msub":{"mi":["D","x"]},"mo":"\u2062","mi":"xy"},{"msub":{"mi":["D","y"]},"mo":"\u2062","msup":{"mi":"y","mn":"2"}},{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["D","x"]},"mo":"\u2062","mi":"p"},{"msub":{"mi":["D","y"]},"mo":"\u2062","mi":"q"}],"mo":"+"}},"mo":"\u2062","mi":"xy"}],"mo":["+","+"]}}}}}},{"@attributes":{"id":"MATH-US-00015-2","num":"00015.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"B","mo":"=","mrow":{"mfrac":{"mn":"1","mrow":{"mi":"y","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":"x","mn":"2"},{"mi":"z","mn":"2"}],"mo":"-"}}}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"msub":{"mi":["D","x"]},"mo":"\u2062","msup":{"mi":"x","mn":"2"}},{"msub":{"mi":["D","y"]},"mo":"\u2062","mi":"xy"},{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["D","x"]},"mo":"\u2062","mi":"p"},{"msub":{"mi":["D","y"]},"mo":"\u2062","mi":"q"}],"mo":"+"}},"mo":"\u2062","mi":"yz"}],"mo":["+","+"]}}}}}},{"@attributes":{"id":"MATH-US-00015-3","num":"00015.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"C","mo":"=","mrow":{"mfrac":{"mrow":[{"mo":"-","mi":"z"},{"mi":"y","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":"x","mn":"2"},{"mi":"z","mn":"2"}],"mo":"-"}}}]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"msub":{"mi":["D","x"]},"mo":"\u2062","mi":"y"},{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["D","x"]},"mo":"\u2062","mi":"p"},{"msub":{"mi":["D","y"]},"mo":"\u2062","mi":"q"}],"mo":"+"}},"mo":"\u2062","mi":"x"}],"mo":"+"}}}}}}]},"A unique solution to {right arrow over (\u03b8)} can be found if and only if |x|\u2260|z| and |y|\u22600.","The equation for \u0394z for (x{circumflex over (n)}+y{circumflex over (n)})\u22600{circumflex over (n)}discussed previously is singular for Bx\u2212Ay=0, so that the function \u0394z does not have to be smoothly differentiable at Bx\u2212Ay=0. However, the limit as Bx\u2192Ay can be found by substituting A, B, and C by their solution equations in Bx\u2212Ay=0. Substitution of the equations for A, B, C, and (Dp+Dq) into Bx\u2212Ay=0 results in either E=D=D=0 or x=y. In either case,",{"@attributes":{"id":"p-0119","num":"0120"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"munder":{"mi":"lim","mrow":{"mi":["Bx","Ay"],"mo":"->"}},"mo":"\u2062","mrow":{"mi":["\u0394","z"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mo":"=","mfrac":{"mrow":[{"msub":[{"mi":["D","x"]},{"mi":["D","y"]}],"mo":"+"},{"mi":["B","A"],"mo":"-"}]}}}}},"A special case of this limit is when E=D=D=0 and A\u2260B, under which the surface is flat (i.e., lim\u0394z=0) at the point designated by (x{circumflex over (n)}+y{circumflex over (n)}).","A, B, and C characterize the directions of illumination. The mathematical method of analyzing photometric stereo is capable of extracting both the surface depth profile and the effective angles of illumination, but not the distance between object and viewer.","Finite difference equations for \u0394z have been developed as part of the present invention from the differential equations described so far and are less sensitive to computational noise than conventional differential equations. The condition of smoothness implies that after digitization there will not be large differences in the change of objects as a function of light wavelength or that a normalization procedure has eliminated large differences.","Digitizing these continuum equations results in finite difference equations with series that converge to the analytical quantities that relate to stereo imaging. The following iterative equation converges to the analytical solution of surface depth profile, \u0394z. The iteration of step n>0 is calculated using the surface profile series that can be written as:",{"@attributes":{"id":"p-0124","num":"0125"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msup":{"mi":"z","mrow":{"mo":["(",")"],"mi":"n"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}}},{"mrow":[{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msup":{"mi":"z","mrow":{"mo":["(",")"],"mn":"0"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}}},{"mo":["[","]"],"mrow":{"mrow":[{"mrow":[{"msub":{"mi":["D","x"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}},{"msup":{"mover":{"mi":["p","_"]},"mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"1"}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}}],"mo":"\u2062"},{"mrow":[{"msub":{"mi":["D","y"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}},{"msup":{"mover":{"mi":["q","_"]},"mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"1"}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}},{"mo":["(",")"],"mfrac":{"mrow":[{"mrow":[{"mrow":{"msub":{"mi":["D","x"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}},"mo":"\u2062","mi":"x"},{"mrow":{"msub":{"mi":["D","y"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}},"mo":"\u2062","mi":"y"}],"mo":"+"},{"msup":[{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["D","x"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}}},"mn":"2"},{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["D","y"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}}},"mn":"2"}],"mo":"+"}]}}],"mo":["\u2062","\u2062"]}],"mo":"-"}}],"mo":"+"}],"mo":"="}}},"br":{},"sub":["x","y","z"],"sup":"0"},{"@attributes":{"id":"p-0125","num":"0126"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msup":{"mi":"z","mrow":{"mo":["(",")"],"mn":"0"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}}},{"mo":["[","]"],"mrow":{"mrow":[{"mover":{"mi":["E","_"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}},{"mrow":{"msub":{"mover":{"mi":["E","_"]},"mi":"\u03be"},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}},"mo":"\u2062","mfrac":{"mrow":[{"mrow":[{"msub":{"mi":["D","x"]},"mo":"\u2062","mi":"x"},{"msub":{"mi":["D","y"]},"mo":"\u2062","mi":"y"}],"mo":"+"},{"msup":[{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["D","x"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}}},"mn":"2"},{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["D","y"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"s","mo":"->"}}}},"mn":"2"}],"mo":"+"}]}}],"mo":"-"}}],"mo":"="}}}},"The zterm can be evaluated only if the local averages of p and q are calculated. The local average derivatives of the surface depth (i.e., p and q) can be calculated at each step, n, of the iteration by",{"@attributes":{"id":"p-0127","num":"0128"},"maths":[{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msup":{"mover":{"mi":["p","_"]},"mrow":{"mo":["(",")"],"mi":"n"}},"mo":"=","mrow":{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mrow":{"mo":["{",")"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":"z","mrow":[{"mi":["i","k"],"mo":[",",","],"mrow":{"mi":"j","mo":"+","mn":"1"}},{"mo":["(",")"],"mi":"n"}]},{"mi":["z","ijk"],"mrow":{"mo":["(",")"],"mi":"n"}}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msubsup":[{"mi":"z","mrow":[{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"+","mn":"1"}],"mo":[",",","],"mi":"k"},{"mo":["(",")"],"mi":"n"}]},{"mi":"z","mrow":[{"mrow":{"mi":"i","mo":"+","mn":"1"},"mo":[",",","],"mi":["j","k"]},{"mo":["(",")"],"mi":"n"}]}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msubsup":[{"mi":"z","mrow":[{"mi":"i","mo":[",",","],"mrow":[{"mi":"j","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}]},{"mo":["(",")"],"mi":"n"}]},{"mi":"z","mrow":[{"mi":["i","j"],"mo":[",",","],"mrow":{"mi":"k","mo":"+","mn":"1"}},{"mo":["(",")"],"mi":"n"}]}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msubsup":[{"mi":"z","mrow":[{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}],"mo":[",",","]},{"mo":["(",")"],"mi":"n"}]},{"mi":"z","mrow":[{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}],"mo":[",",","],"mi":"j"},{"mo":["(",")"],"mi":"n"}]}],"mo":"-"}}],"mo":["+","+","+"]}}}},"mo":"}"}}},{"@attributes":{"id":"MATH-US-00019-2","num":"00019.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msup":{"mover":{"mi":["q","_"]},"mrow":{"mo":["(",")"],"mi":"n"}},"mo":"=","mrow":{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":"z","mrow":[{"mrow":{"mi":"i","mo":"+","mn":"1"},"mo":[",",","],"mi":["j","k"]},{"mo":["(",")"],"mi":"n"}]},{"mi":["z","ijk"],"mrow":{"mo":["(",")"],"mi":"n"}}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msubsup":[{"mi":"z","mrow":[{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"+","mn":"1"}],"mo":[",",","],"mi":"k"},{"mo":["(",")"],"mi":"n"}]},{"mi":"z","mrow":[{"mi":["i","k"],"mo":[",",","],"mrow":{"mi":"j","mo":"+","mn":"1"}},{"mo":["(",")"],"mi":"n"}]}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msubsup":[{"mi":"z","mrow":[{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}],"mo":[",",","],"mi":"j"},{"mo":["(",")"],"mi":"n"}]},{"mi":"z","mrow":[{"mi":["i","j"],"mo":[",",","],"mrow":{"mi":"k","mo":"+","mn":"1"}},{"mo":["(",")"],"mi":"n"}]}],"mo":"-"}},{"mo":["(",")"],"mrow":{"msubsup":[{"mi":"z","mrow":[{"mrow":[{"mi":"i","mo":"+","mn":"1"},{"mi":"j","mo":"+","mn":"1"},{"mi":"k","mo":"+","mn":"1"}],"mo":[",",","]},{"mo":["(",")"],"mi":"n"}]},{"mi":"z","mrow":[{"mi":["i","j"],"mo":[",",","],"mrow":{"mi":"k","mo":"+","mn":"1"}},{"mo":["(",")"],"mi":"n"}]}],"mo":"-"}}],"mo":["+","+","+"]}}}}}}],"br":{},"o":["p","q"],"sup":["(n) ","0"],"sub":"m"},"The illumination directions are characterized by the rotation vector {right arrow over (\u03b8)}, which is determined by the equation of motion for the image, where the {circumflex over (n)}component is now known. Once x, y, and z, D, and Dare known, then solutions to A, B, and C can be calculated from the cross product equation.","Atmospheric Scattering Effects","Atmospheric calculations can be performed to evaluate photometric stereo imaging under different conditions. Atmospheric conditions affect visualization of stereo images illuminated by the sun or other atmospheric influenced sources. Illumination of an object by two wavelengths at different angles, but viewed from a single direction, can be used to generate three-dimensional stereo of the object using surface depth profiles. Photometric stereo is useful in passive techniques because of the angular distribution of atmospherically scattered radiation. The radiance of electromagnetic radiation (EMR) scattered by atmospheric particles (i.e., the sky radiance) does not have a uniform spectrum. The spectrum changes with the direction of the sun relative to the zenith and the direction of view relative to the zenith. This changing spectrum causes some of the three-dimensional perspective seen. However, one of the most important factors influencing illumination is the angle of the sun that varies during the day.","The effective angle is shown to provide the stereo information can be obtained from two wavelengths in the example shown in . An object (tank ) is imaged under two conditions; with the sun on the horizon and at zenith. Imaging occurs at two wavelengths, 400 nm and 700 nm. In other examples, other wavelengths or sun positions can be used.","The radiance of EMR scattered from the atmosphere is designated L(x, A, t) where xis cos \u03b8, where \u03b8 is the polar angle of the view relative to the zenith, A is the albedo of the earth's surface, and tis the time either when the sun is at the zenith (t=0), or the time (t=1) that the sun is on the horizon. The radiance scattered from the particles in the atmosphere as a function of angle of the point of view relative to the zenith is published in Driscoll and Vaughan, ",{"@attributes":{"id":"p-0132","num":"0133"},"figref":["FIG. 4A","FIG. 4B"]},"An image of an object can be obtained for each wavelength. In this example, the viewer in the analysis is facing the object from which the surface depth profile will be determined. In the cases where the sun on the horizon, the viewer is facing opposite from the sun (i.e., antisolar). The directions of illumination at each wavelength, \u03be, occur at the peaks of L(x, A, t, \u03be) where the EMR is strongest. The peaks of L(x, A, t) (i.e., angle of illumination) are determined by A and t.","The different cases provide different types of stereo information. When the sun is at the zenith (t=1), for both A=0 and A=0.8, the antisolar maximum Loccurs at cos \u03b8=0 for wavelength 700 nm and at cos \u03b8=0.8 at wavelength 400 nm. Furthermore, the intensity at 400 nm and 700 nm are almost equal. Therefore, the effective angles from which the object are illuminated is \u03b8=90\u00b0 at 700 nm and \u03b8=37\u00b0 at wavelength 400 nm. The difference between angles is 53\u00b0. The angle between the two effective illumination points is 53\u00b0. When the sun is on the horizon (t=0), there is more than one peak for each xand A. Since the viewer is looking in the antisolar direction, there are broad peaks at \u03b8=0\u00b0 and 37\u00b0 occur for wavelength 400 nm and two peaks, \u03b8=0\u00b0 and 90\u00b0 occur at 400 nm. Since the \u03b8=0\u00b0 occurs at both wavelengths, the stereo perspective is mainly provided at \u03b8=37\u00b0 at 400 nm and 90\u00b0 for 700 nm. Although these are the same angles as in the sun at zenith case, the peak at 400 nm is now broader. Therefore, the surface depth profile will not be as clearly resolved as for the zenith sun case. For the case where the ground albedo is 0.8 case and the sun is on the horizon (i.e., t=1), there is no clearly defined peak radiance for wavelength 400 nm. The radiance at wavelength 700 nm has a well defined peak at \u03b8=0\u00b0, but there is no definite illumination direction for 400 nm. Therefore, the analysis suggests that the photometric stereo effect won't be effective under this condition.","The atmosphere also attenuates the EMR that is reflected off the object due to scattering and absorption. The attenuation from both scattering and absorption is called extinction. If the extinction does not change from with wavelength, then the images won't be substantially different from the zero extinction case. The surface depth profile can be calculated by the methods shown and will be the same as without extinction except for random noise. However, extinction is shown to add an error to the surface depth profile if the attenuation varies strongly with wavelength. At long distances, the extinction from scattering is strong enough. The error increases with the wavelength derivative of the extinction. If the error is large enough, the iterative methods described may not converge and the surface depth profile will be undefined.","Wavelength dependent extinction is shown to cause an error in the surface depth profile if the variation with wavelength is large. This can be easily seen for the special case of a very large difference in extinction coefficients where the intensity at one wavelength is attenuated to zero, and the intensity of the other wavelength is still large. Then there is no second direction of illumination and the stereo effect is not significant. In this case the surface depth profile cannot be calculated accurately. Even in a less extreme example, there is an error caused by a changing wavelength dependence that can be calculated. Wavelength dependent extinction varies with atmospheric conditions. Atmospheric extinction and scattering can be used under some atmospheric conditions to estimate the distance to a far (\u22481000 m) object, although extinction can't be used to estimate surface depth profile if the features do not extend a comparable distance (\u22481000 m). Formulas for the error are presented herein.","Atmospheric Extinction Effects","Extinction-caused error is estimated and limits of validity for photometric stereo are analyzed in this section. The surface depth analysis is insensitive to extinction that is totally independent of wavelength. Factors that aren't \u03be, x or y-dependent will not impact the calculation. Therefore, these \u03be, x or y-independent factors are normalized here to one. Under the conditions of this analysis, the intensity at a viewing point {right arrow over (R)}of a brightness element at point {right arrow over (R)} is:\n\n\u2032(,\u03be)\u2248exp(\u2212\u03bc||)(,\u03be)\n\n,\n\n,\n\nwhere E is the brightness at a brightness element, {right arrow over (R)} is the position of the brightness element, E\u2032 is the intensity at position {right arrow over (R)}, \u03bc is the extinction coefficient, and \u03be is the wavelength. See also Y. Yu and Jitendra Malik, \u201cRecovering photometric properties of architectural objects from photographs,\u201d Proc. 25Ann. Conf. on Computer Graphics and Interactive Techniques, 207-217 (1998), hereby incorporated by reference.\n","The optical flow without extinction is | |, where | | is the component of optical flow in an image plane (i.e., an xy plane):",{"@attributes":{"id":"p-0139","num":"0140"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"msub":{"mover":{"mi":"D","mo":"->"},"mi":"xy"}},{"mo":["\uf603","\uf604"],"mfrac":{"msub":{"mi":["E","\u03be"]},"msqrt":{"mrow":{"msubsup":[{"mi":["E","x"],"mn":"2"},{"mi":["E","y"],"mn":"2"}],"mo":"+"}}}}],"mo":"="}}}},"However, the optical flow will have an error caused by wavelength-dependent extinction. The optical flow with wavelength-dependent extinction is defined as:",{"@attributes":{"id":"p-0141","num":"0142"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mover":{"msubsup":{"mi":["D","xy","\u2032"]},"mrow":{"mo":"->","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}},{"mo":["\uf603","\uf604"],"mfrac":{"msubsup":{"mi":["E","\u03be","\u2032"]},"msqrt":{"mrow":{"msubsup":[{"mi":["E","x","\u20322"]},{"mi":["E","y","\u20322"]}],"mo":"+"}}}}],"mo":"="}}},"br":{}},{"@attributes":{"id":"p-0142","num":"0143"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mi":["E","\u03be","\u2032"]},"mo":"=","mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"exp","mo":["(",")"],"mrow":{"mrow":[{"mo":"-","mi":"\u03bc"},{"mo":["\uf603","\uf604"],"mrow":{"mover":[{"msup":{"mi":["R","\u2032"]},"mo":"->"},{"mi":"R","mo":"->"}],"mo":"-"}}],"mo":"\u2062"}},{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y","z","\u03be"],"mo":[",",",",","]}}}],"mo":"\u2062"}}}}}},"br":{}},{"@attributes":{"id":"p-0143","num":"0144"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mi":["E","\u03be","\u2032"]},"mo":"=","mrow":{"mrow":[{"mo":["[","]"],"mrow":{"mrow":[{"mrow":[{"mo":"-","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mover":[{"msup":{"mi":["R","\u2032"]},"mo":"->"},{"mi":"R","mo":"->"}],"mo":"-"}}},{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"\u03bc"}}],"mo":["\u2062","\u2062"],"mi":"E"},{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"E"}],"mo":"+"}},{"mi":"exp","mo":["(",")"],"mrow":{"mrow":[{"mo":"-","mi":"\u03bc"},{"mo":["\uf603","\uf604"],"mrow":{"mover":[{"msup":{"mi":["R","\u2032"]},"mo":"->"},{"mi":"R","mo":"->"}],"mo":"-"}}],"mo":"\u2062"}}],"mo":"\u2062"}}}}},"Substitution Eand E\u2032 into the definition of f\u2032 results in:",{"@attributes":{"id":"p-0145","num":"0146"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mover":{"msup":{"mi":["D","\u2032"]},"mo":"->"}}},{"mrow":{"mrow":[{"mo":"+","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mover":[{"msup":{"mi":["R","\u2032"]},"mo":"->"},{"mi":"R","mo":"->"}],"mo":"-"}}},{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"\u03bc"}}],"mo":["\u2062","\u2062"],"mfrac":{"mi":"E","msqrt":{"mrow":{"msubsup":[{"mi":["E","x"],"mn":"2"},{"mi":["E","y"],"mn":"2"}],"mo":"+"}}}},"mo":"-","mfrac":{"msub":{"mi":["E","\u03be"]},"msqrt":{"mrow":{"msubsup":[{"mi":["E","x"],"mn":"2"},{"mi":["E","y"],"mn":"2"}],"mo":"+"}}}}],"mo":"="}}},"br":{}},{"@attributes":{"id":"p-0146","num":"0147"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mover":{"msup":{"mi":["D","\u2032"]},"mo":"->"}}},{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mi":"\u0394","mo":"\u2062","mover":{"mi":"R","mo":"->"}}},{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mover":[{"msup":{"mi":["R","\u2032"]},"mo":"->"},{"mi":"R","mo":"->"}],"mo":"-"}},{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"\u03bc"}}],"mo":["\u2062","\u2062"],"mfrac":{"mi":"E","msqrt":{"mrow":{"msubsup":[{"mi":["E","x"],"mn":"2"},{"mi":["E","y"],"mn":"2"}],"mo":"+"}}}}],"mo":"+"}],"mo":"="}}}},"The direction of the flow is still the direction of the gradient of E, and so remains unchanged. Therefore, the error introduced into the optical flow by the extinction is the extinction error, \u0394f",{"@attributes":{"id":"p-0148","num":"0149"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"\u0394","mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mover":{"mi":"D","mo":"->"}}}},{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mover":[{"msup":{"mi":["R","\u2032"]},"mo":"->"},{"mi":"R","mo":"->"}],"mo":"-"}},{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"\u03bc"}}],"mo":["\u2062","\u2062"],"mfrac":{"mi":"E","msqrt":{"mrow":{"msubsup":[{"mi":["E","x"],"mn":"2"},{"mi":["E","y"],"mn":"2"}],"mo":"+"}}}}],"mo":"="}}}},"If the extinction coefficient, \u03bc, is independent of wavelength, \u03be, then the error is zero (\u0394|\u0394{right arrow over (R)}|=0). However, the larger the change in extinction,",{"@attributes":{"id":"p-0150","num":"0151"},"maths":{"@attributes":{"id":"MATH-US-00027","num":"00027"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"\u03bc"}},"mo":","}}},"br":{}},"The equation for extinction corrected optical flow magnitude is",{"@attributes":{"id":"p-0152","num":"0153"},"maths":{"@attributes":{"id":"MATH-US-00028","num":"00028"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mover":{"msup":{"mi":["D","\u2032"]},"mo":"->"}}},{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mi":"\u0394","mo":"\u2062","mover":{"mi":"R","mo":"->"}}},{"mo":["{","}"],"mrow":{"mn":"1","mo":"+","mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mover":[{"msup":{"mi":["R","\u2032"]},"mo":"->"},{"mi":"R","mo":"->"}],"mo":"-"}},{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"\u03bc"}}],"mo":["\u2062","\u2062"],"mfrac":{"mi":"E","msub":{"mi":["E","\u03be"]}}}}}],"mo":"\u2062"}],"mo":"="}}}},"The x and y components of flow are proportional to the optical flow magnitude so that the optical flow modified by extinction are D\u2032 and D\u2032 as given by two equations, which are",{"@attributes":{"id":"p-0154","num":"0155"},"maths":[{"@attributes":{"id":"MATH-US-00029","num":"00029"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mi":["D","x","\u2032"]},"mo":"=","mrow":{"msub":{"mi":["D","x"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mn":"1","mo":"+","mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mover":[{"msup":{"mi":["R","\u2032"]},"mo":"->"},{"mi":"R","mo":"->"}],"mo":"-"}},{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"\u03bc"}}],"mo":["\u2062","\u2062"],"mfrac":{"mi":"E","msub":{"mi":["E","\u03be"]}}}}}}}}},{"@attributes":{"id":"MATH-US-00029-2","num":"00029.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mi":["D","y","\u2032"]},"mo":"=","mrow":{"msub":{"mi":["D","y"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mn":"1","mo":"+","mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mover":{"msup":{"mi":["R","\u2032"]},"mo":"->"},"mo":"-","msub":{"mover":{"mi":"R","mo":"->"},"mi":"T"}}},{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"\u03bc"}}],"mo":["\u2062","\u2062"],"mfrac":{"mi":"E","msub":{"mi":["E","\u03be"]}}}}}}}}}]},"The surface depth profile modified by extinction, z\u2032, is defined by",{"@attributes":{"id":"p-0156","num":"0157"},"maths":{"@attributes":{"id":"MATH-US-00030","num":"00030"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msup":{"mi":["z","\u2032"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}},"mo":"=","mfrac":{"mrow":[{"mrow":[{"msubsup":{"mi":["D","x","\u2032"]},"mo":"\u2062","mi":"x"},{"msubsup":{"mi":["D","y","\u2032"]},"mo":"\u2062","mi":"y"}],"mo":"+"},{"mi":["Bx","Ay"],"mo":"-"}]}}}},"br":{}},{"@attributes":{"id":"p-0157","num":"0158"},"maths":{"@attributes":{"id":"MATH-US-00031","num":"00031"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msup":{"mi":["z","\u2032"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},{"mrow":[{"mi":"z","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},{"mo":["{","}"],"mrow":{"mn":"1","mo":"+","mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mover":{"mi":"R","mo":"->"},"mo":"-","msub":{"mover":{"mi":"R","mo":"->"},"mi":"T"}}},{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"\u03bc"}}],"mo":["\u2062","\u2062"],"mfrac":{"mi":"E","msub":{"mi":["E","\u03be"]}}}}}],"mo":"\u2062"}],"mo":"="}}}},"Therefore, the error z\u2032\u2212z is",{"@attributes":{"id":"p-0159","num":"0160"},"maths":{"@attributes":{"id":"MATH-US-00032","num":"00032"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msup":{"mi":["z","\u2032"]},"mo":"-","mi":"z"},{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mover":[{"msup":{"mi":["R","\u2032"]},"mo":"->"},{"mi":"R","mo":"->"}],"mo":"-"}},{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"\u03bc"}}],"mo":["\u2062","\u2062"],"mfrac":{"mi":"E","msub":{"mi":["E","\u03be"]}}}],"mo":"="}}}},"Therefore, error is surface depth is proportional to",{"@attributes":{"id":"p-0161","num":"0162"},"maths":{"@attributes":{"id":"MATH-US-00033","num":"00033"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"mover":[{"msup":{"mi":["R","\u2032"]},"mo":"->"},{"mi":"R","mo":"->"}],"mo":"-"}},{"mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03be"}},"mo":"\u2062","mi":"\u03bc"}},"mo":"."}],"mo":"\u2062"}}},"br":{}},"The smoothness constraint no longer applies if the error is large. This can be shown using the equation of motion, where the constant zis chosen where V has been minimized (i.e., coordinates where V=0),\n\n\u0394\n\nwhich is equivalent to a rotation of the illumination direction around the {right arrow over (\u03b8)} axis by approximately |{right arrow over (\u03b8)}| rad for small angles (|{right arrow over (\u03b8)}|<<1 rad). However, the analytical solution that is also correct for large angles the transformation is a translation, not a rotation, that is tangent to a circle around |{right arrow over (\u03b8)}| where the translation distance \u0394y is\n\n\u0394| tan \u03c6\n\nwhere \u03c6 is the angle of rotation about {right arrow over (\u03b8)}. This translation is equivalent to a rotation only if \u03c6 is small (|\u03c6|<<1). If \u0394|\u0394{right arrow over (R)}|=0 is too large (1\u2266\u0394|\u0394{right arrow over (R)}|\u22662\u03c0, then the angle \u03c6>1. The cross product cannot then represent a true rotation because the transformation is no longer equivalent to a rotation. Therefore, photometric stereo image conditions are not valid when the extinction varies greatly with wavelength.\n\nOther Uses\n","A visualization analysis using atmospheric scattering can be used to extract stereo information at distances longer than possible with narrow view binocular stereo imaging. The impact of the atmosphere on photometric stereo images occurs both by providing or not providing the illumination in passive imaging, and by extinction where each wavelength attenuates differently.","The angular distribution of wavelength sometimes does not significantly change with wavelength such as under dense fog conditions. An estimate of where extinction is significant is made using the common observation that extremely distant objects (\u22481000 meters (m) under clear conditions) have a bluish tint caused by differential attenuation and atmospheric scattering. The atmosphere scatters blue light (wavelength\u2248500 nanometers (nm)) more than red light (wavelength\u2248600 nm), so that with distance the image grows weaker due to extinction while the scattered path radiance increases. This effect, called aerial perspective, may cause a significant error when the distance is large enough (here \u22481000 m) and the wavelengths extend over a wide range (here \u2248500-600 nm). A correction for aerial perspective can be made to eliminate this error.","However, the correction for aerial perspective can also be used to measure the distance to an object, which is equivalent to z. Therefore, photometric imaging with atmospheric corrections for extinction could provide distance (z) and shape (e.g., surface depth profile) of the object being viewed.","Photometric stereo imaging only requires a sequence of two-dimensional images at different wavelengths. These images can be component images from decomposition of a color image into component images of different wavelength ranges, or may be images taken under different illumination conditions. Digital imagers can easily be modified to record such data. Mathematical algorithms for extract surface depth profiles and effective angles of illumination can be programmed into image processing software.","Natural light sources, especially atmospherically scattered light, have angular distributions of wavelength. Natural light is generally only weakly polarized, so that photometric stereo is particularly valuable where polarimetric stereo methods are not useful.","For the sun on the horizon, the angular distribution of EMR varies with wavelength as often seen by the unaided eye at sunset or sunrise. The two wavelengths are scattered from the atmosphere at distinguishable view angles, which is a condition for photometric stereo imaging. The stereo effect is less effective with the sun at zenith (high noon) because the angular distribution varies with wavelength than when the sun is on the horizon.","The effect of a nonzero albedo was to decrease the effectiveness of the photometric stereo still further. The loss in photometric stereo effect is caused by radiation diffusely reflected from the ground into the atmosphere which scatters some of the EMR back to the ground with an even broader angular distribution than radiation had before ground reflection. Therefore, a high albedo therefore creates an angular distribution that varies less with wavelength than a low albedo.","A disparity-based algorithm, such as described herein for calculation of surface depth profiles, can also be used in other situations where stereo information is needed but only one direction viewpoint is available.","The effect of dispersive (wavelength dependent) extinction is expressed in equations presented here. The error caused by dispersive extinction is proportional to the rate of change,",{"@attributes":{"id":"p-0172","num":"0173"},"maths":{"@attributes":{"id":"MATH-US-00034","num":"00034"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":[{"mo":"\u2146","mi":"\u03bc"},{"mo":"\u2146","mi":"\u03be"}]},"mo":","}}},"br":{}},"Photometric stereo imaging can be used to obtain a stereo image from a single view using images, or component images decomposed from a single color image, of different wavelengths. Surface depth functions are extractable from a single view through the atmosphere using EMR scattered from atmospheric particles in a natural atmosphere. The solution to the example equations used for finding the surface depth use both an equation of continuity constraint and a smoothness constraint. A smoothness constraint can be visualized as the image is sliding on the surface of the object.","The atmosphere affects performance of photometric stereo both by providing natural illumination for passive stereo viewing and by attenuating EMR at different wavelengths. Natural illumination from atmospheric scattering provides the illumination for one approach to passive photometric stereo imaging, using images under different illumination conditions, because the angular distribution of EMR varies with wavelength. Effective angles can be calculated from published data on downwelling radiance for different times of day. Extinction effects are potentially significant if there is a large change in object brightness as a function of wavelength. Very large changes caused by extinction may invalidate the smoothness condition used in the calculation of surface depth.","Atmospheric effects on stereo images may further be included in visualization software for accurate representation of imaging.","Photometric stereo imaging can be used to extract surface depth profiles and illumination directions. The distance between the viewer and an object may be determined using any suitable technique. The calculation of optical flow from a sequence of two-dimensional images, generated from a single color image, or other images representing different wavelength ranges, allows extraction of stereo information.","Photometric techniques can further be used in real time to guide automatic and robotic devices, both under natural lighting and in cases where the surroundings are illuminated with a controlled distribution of angles and wavelengths.","As used herein the terminology signal processing circuitry includes a computer, processor, microprocessor, multiprocessor, controller, mainframe, or a plurality of computers, processors, microprocessors, multiprocessors, controller, or mainframes or equivalents thereof.","As used herein, the terminology \u201cobject\u201d may include a thing, a person, animate or inanimate subject, a plurality of objects, the ground, ground covering (e.g., grass), a localized piece or pieces of the environment, surface, physical entity (or entities) or anything that has photoemission.","The invention is not restricted to the illustrative examples described above. Examples are not intended as limitations on the scope of the invention. Methods, apparatus, compositions, and the like described herein are exemplary and not intended as limitations on the scope of the invention. Changes therein and other uses will occur to those skilled in the art. The scope of the invention is defined by the scope of the claims. Patents, patent applications, or publications mentioned in this specification are incorporated herein by reference to the same extent as if each individual document was specifically and individually indicated to be incorporated by reference."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0016","num":"0017"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0017","num":"0018"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0018","num":"0019"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0019","num":"0020"},"figref":"FIGS. 4A and 4B"},{"@attributes":{"id":"p-0020","num":"0021"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0021","num":"0022"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
