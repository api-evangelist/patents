---
title: Recording a communication pattern and replaying messages in a parallel computing system
abstract: A parallel computer system includes a plurality of compute nodes. Each of the compute nodes includes at least one processor, at least one memory, and a direct memory address engine coupled to the at least one processor and the at least one memory. The system also includes a network interconnecting the plurality of compute nodes. The network operates a global message-passing application for performing communications across the network. Local instances of the global message-passing application operate at each of the compute nodes to carry out local processing operations independent of processing operations carried out at another one of the compute nodes. The direct memory address engines are configured to interact with the local instances of the global message-passing application via injection FIFO metadata describing an injection FIFO in a corresponding one of the memories. The local instances of the global message passing application are configured to record, in the injection FIFO in the corresponding one of the memories, message descriptors associated with messages of an arbitrary communication pattern in an iteration of an executing application program. The local instances of the global message passing application are configured to replay the message descriptors during a subsequent iteration of the executing application program.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08407376&OS=08407376&RS=08407376
owner: International Business Machines Corporation
number: 08407376
owner_city: Armonk
owner_country: US
publication_date: 20090710
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS"],"p":["The present invention generally relates to the electrical, electronic, and computer arts, and, more particularly, to parallel computing systems.","In parallel computing, many calculations are carried out simultaneously, taking advantage of the fact that large problems can often be divided into smaller ones, which can then be solved concurrently. Direct memory address (DMA) is employed in many parallel computing systems. In some cases, parallel computing systems include a plurality of compute nodes interconnected by a network. Communication between the different compute nodes is a significant issue.","Principles of the present invention provide techniques for recording a communication pattern and replaying messages in a parallel computing system.","In an exemplary embodiment, according to one aspect of the invention, a parallel computer system includes a plurality of compute nodes. Each of the compute nodes includes at least one processor, at least one memory, and a direct memory address engine coupled to the at least one processor and the at least one memory. The system also includes a network interconnecting the plurality of compute nodes. The network operates a global message-passing application for performing communications across the network. Local instances of the global message-passing application operate at each of the compute nodes to carry out local processing operations independent of processing operations carried out at another one of the compute nodes. The direct memory address engines are configured to interact with the local instances of the global message-passing application via injection FIFO metadata describing an injection FIFO in a corresponding one of the memories. The local instances of the global message passing application are configured to record, in the injection FIFO in the corresponding one of the memories, message descriptors associated with messages of an arbitrary communication pattern in an iteration of an executing application program. The local instances of the global message passing application are configured to replay the message descriptors during a subsequent iteration of the executing application program.","In another aspect, a method is provided, including the steps of providing a parallel computer system of the kind described; recording, with local instances of the global message passing application, in the injection FIFO in the corresponding one of the memories, message descriptors associated with messages of an arbitrary communication pattern in an iteration of an executing application program; and replaying the message descriptors, with the local instances of the global message passing application, during a subsequent iteration of the executing application program.","One or more embodiments of the invention or elements thereof can be implemented in the form of a computer product including a computer readable storage medium with computer usable program code for performing, or facilitating performance of, some or all of the method steps indicated. Furthermore, one or more embodiments of the invention or elements thereof can be implemented in the form of means for carrying out, or facilitating carrying out, one or more of the method steps described herein; the means can include hardware module(s) or a combination of hardware and software modules implementing the specific techniques set forth herein, and the software can be stored in a computer-readable storage medium (or multiple such media).","As used herein, \u201cfacilitating\u201d an action includes performing the action, making the action easier, helping to carry the action out, or causing the action to be performed. Thus, by way of example and not limitation, software instructions might facilitate an action carried out by one or more processors.","One or more embodiments of the invention may offer one or more of the following technical benefits:","improved programmer productivity (no need to manually set up lists of FIFOs);","performance enhancement through record and replay of messages.","These and other features, aspects and advantages of the invention will become apparent from the following detailed description of illustrative embodiments thereof, which is to be read in connection with the accompanying drawings.","One or more embodiments of the invention are applicable to parallel computing systems. Non-limiting examples of such systems, used solely for purposes of illustration and not limitation, are the Blue Gene\/P and Blue Gene\/Q machines from International Business Machines Corporation of Armonk, N.Y., USA; refer also to US Patent Publications 2009\/0006808 \u201cUltrascalable Petaflop Parallel Supercomputer\u201d and 2009\/0006296 \u201cDMA Engine For Repeating Communication Patterns,\u201d the complete disclosures of both of which are expressly incorporated herein by reference in their entirety for all purposes. The complete disclosure of US Patent Publication 2009\/0003344, entitled \u201cAsynchronous Broadcast For Ordered Delivery Between Compute Nodes In A Parallel Computing System Where Packet Header Space Is Limited,\u201d is also expressly incorporated herein by reference in its entirety for all purposes.","Parallel Computer System with Direct Memory Address (DMA) Engines",{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 1","FIG. 1","FIG. 1"],"b":["100","100","102","1","102","2","102","108","102","1","102","100"],"i":["n","n"]},"Compute node or ASIC () may function as both a compute node and an input\/output (I\/O) node in the parallel computer system . Compute node () includes a plurality of processors or processor cores, (), . . . (), but preferably four. Each of the processor cores  can include, form example, a \u201cdouble\u201d floating point unit, which may in turn include two coupled standard floating point units. This arrangement gives a peak performance of four floating point operations per processor core per clock cycle.","Besides the embedded processor cores , and floating point cores (not shown in ), each node  of the parallel computer system  includes a DMA, or DMA engine  (DMA and DMA engine are used interchangeably herein), and a memory  such as, for example, an embedded dynamic random access memory (DRAM). DRAM  includes injection FIFOs  and reception FIFOs , and can be controlled, for example, by an integrated external DDR2 (double data rate synchronous dynamic random access memory interface) memory controller (not shown in ) and DMA engine . DMA engine  includes processor interface , DMA logic , memory interface , DMA network interface , injection counters , injection FIFO metadata , reception counters , reception FIFO metadata  and status and control registers . The injection FIFO metadata  describes where in memory  the injection FIFOs  are located and the current head and tail of the FIFOs. The reception FIFO metadata  describes where in memory the reception FIFOs  are located, and the current head and tail of the FIFOs. Particularly in a system-on-a-chip implementation, the amount of logic area devoted to the DMA engine may be quite limited, and thus the number of counters may be relatively small. Effective sharing of counters between multiple messages may thus be desirable.","DMA engine  directly controls transfer of long messages, which long messages are typically preceded by short protocol messages deposited into reception FIFOs on a receiving node (for example, a reception FIFO  in memory  of compute node ()). Through these protocol messages, the sender, source or origin compute nodes, and the receiver, target or destination compute nodes agree on which injection counter () and reception counter () identifications to use for message passing, and what the base offsets are for the messages being processed. Long message transfer may be initiated by a core processor on the sender node by placing a \u201cput\u201d message descriptor into an injection FIFO  (in memory ), writing the injection counter base and value via writes via the DMA engine's memory interface , and appropriately modifying the injection FIFO metadata  for the injection FIFO containing that message. This includes advancing a tail pointer indicating the \u201clast\u201d message descriptor in the injection FIFO via a \u201cwrite\u201d to the DMA processor interface . DMA logic  reads the injection FIFO metadata , and recognizes which injection FIFOs have messages to be sent.","The DMA logic causes the DMA memory interface  to read the descriptor in an injection FIFO  (in memory ). The put message descriptor includes the injection () and reception counter () identifications to be used, the message length, the initial injection and reception offsets of the message, the destination node and other network routing information. The DMA engine  begins fetching the message and assembling it into packets to be \u201cput\u201d on to the network . Each packet contains an offset from the reception counter  where the data from this packet is to be stored, and a count of how many bytes in this packet should be written. DMA engine  is responsible for updating this information correctly for each packet, and puts the packets into the DMA network interface  (when space is available), at which time the packet enters the network and is routed to the destination compute node (for example, compute node(n)).","After DMA engine  puts the message in the DMA network interface , it decrements the specified injection counter  by the number of bytes in the packet. Upon reaching the destination, the packet is put into the DMA network interface at that compute node (e.g., (), and the target node's DMA engine \u201crecognizes\u201d that the packet is there. The DMA engine at the receiver or target compute node reads the reception counter identification, offset and count from the received packet, looks up the reception counter base address, writes the appropriate number of bytes starting at the base plus packet offset, and then decrements the counter value by the bytes.","If a remote get operation is used, instead of the processor on the sender node injecting a descriptor into the Injection FIFO , the receiver node sends a short get message (which contains a put descriptor) to the sender compute node (e.g., ()), and the DMA logic at the sender compute node puts this descriptor into the Injection FIFO and advances that FIFO's data appropriately. To share a byte counter, the base address of the shared counter must be set to a value smaller than the base address of any message to be using that counter. The initial value of the counter is set to zero. The initial offset in a message descriptor is the message's starting address minus this base offset. The particular processor increments the counter value by the current message length, and in accordance with the novel operation, the processor need only know the current message length, but not the lengths of the other messages using this counter, nor the number of bytes that have already been received. The reader should note that the byte counter can be shared between messages even if the messages come from different source (sender) nodes.","Network  preferably displays a 10 gigabit Ethernet functionality, providing all the network link cut-through routing buffers and routing control block that allows any two nodes to communicate with low latency. The four (or \u201cp\u201d) processor cores embedded in ASIC (node ()) as shown may be utilized for message handling and computation operations. Virtual cut-through torus routing may be supported in a hardware block, which is integrated into the compute nodes  to allow for the elimination of the network adapter, typically required in conventional parallel computer system operation. Preferably, a virtual channel routing network is supported with two (2) dynamic and two (2) deterministic channels.","The same compute node ASIC construction  can also be used as an I\/O node, which is associated with a subset of the compute nodes (e.g., 16, 32, 64, or 128 compute nodes), for handling fileserver communication and I\/O operations. In some embodiments, the only difference between an I\/O compute node and a computation compute node is that an I\/O node enables and uses an external network interface, such as the 10 Gigabit Ethernet. While the compute nodes may have the integrated 10 gigabit Ethernet (they share a common ASIC), for purposes of discussion, the 10 gigabit Ethernet interface is enabled at the I\/O nodes only.","The network  of interconnected compute nodes  effectively operates a global message-passing application for performing communications across the network, in that each of the compute nodes  includes one or more individual processors  with memories which run local instances of the global message-passing application. The nodes  may be connected by multiple networks; for example, torus network , a collective network (not shown), and a global asynchronous network (not shown), as known from US Patent Publication 2009\/0006296.","In , the ejection FIFO Metadata  of the DMA engine  of node () of a parallel computer system  (as seen in ) is illustrated as two pieces of injection FIFO metadata , , where each piece can describe an injection FIFO set aside for the local instance of the message-passing application operating at the compute node comprising same. The injection FIFO metadata, , , accommodates the DMA engine operation, and therefore, the global message-passing network operation. Injection FIFO metadata  describes the injection FIFO  of memory , but injection FIFO metadata  as shown in  has not been configured in view of the fact that there is no injection FIFO associated with it seen in memory  (the way that the presence of injection FIFO  can be attributed to injection FIFO metadata ). That is, injection FIFO metadata  has not been configured by the message-passing application with an injection FIFO, such as injection FIFO  associated with injection FIFO metadata , as mentioned.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 3","FIG. 2","FIG. 3"],"b":["310","210","310","311","327","210","311","327","315","311","327"]},"The injection FIFOs  are circular buffers within the application memory , and define a start address  and an end address  of a buffer. The injection FIFOs may be thought of as a producer-consumer queue with the communication software application (for example, the global message-passing application) acting as the producer, and the DMA network interface acting as the consumer. The producer queue injection FIFO further includes a producer address  (producer queue address), and the consumer address  (consumer queue address). In operation, a communication software application injects a message by incrementing the producer address . When space in the network is available, the DMA engine  fetches the message descriptor at the consumer address , and injects the corresponding message into the network (via DMA network interface ). The DMA engine  then increments the consumer address .","The DMA engine  preferably provides one hundred twenty eight injection FIFO descriptors, and, therefore, up to one hundred twenty eight active injection FIFOs. With respect to multicore node operation, this novel feature allows for each core  to have its own injection FIFO, and in some cases, multiple injection FIFOs. Such operation improves performance of a large message by splitting the large message across multiple injection FIFOs. A \u201cnetwork_resources\u201d bitmask , shown in  within injection FIFO metadata , specifies the network resources available to messages in the injection FIFO. Network resources such as \u201cnetwork_resources\u201d bitmask  include operation in view of network priorities. The resources include network buffers and network links, where splitting network resources across multiple injection FIFOs allows a communication (message-passing) software application to better control use of network resources.","Injection FIFO metadata  may further include an \u201cis_empty\u201d bit , which allows a communication application to efficiently determine if all the message descriptors in the FIFO have been sent. Put another way, in order to determine if there is work to be done by the DMA engine, the \u201cis_empty\u201d bit  represents whether there are additional, or any, message descriptors to be sent. The injection FIFO metadata  may further include an \u201cis_full\u201d bit , which is used to allow the communication application to efficiently determine if there is room in the descriptor for injecting additional message descriptors. That is, the \u201cis_full\u201d bit  is used by the local instance of the message passing application to determine whether there are more message descriptors (load) to be operated upon (the producer of the injection FIFO). A \u201cwas_threshold_crossed\u201d bit  records if the free space in the injection FIFO was ever below the threshold value . \u201cWas_threshold_crossed\u201d bit  can be cleared using the \u201cclear_threshold_crossed\u201d bit , and a \u201cthreshold interrupt\u201d bit  supports determining whether crossing the threshold also causes an interrupt for the processor cores  of a compute node .","In the injection FIFO metadata , an enable bit  is included for determining whether the injection FIFO metadata is available for application use. If the enable bit  is not set, the descriptor is ignored by the DMA engine. If priority bit  is set, the descriptor is served by the DMA engine more frequently than descriptors without this bit set. \u201cService_quantum\u201d value  is included for determining how many message payload bytes should be sent from this injection FIFO metadata by the DMA engine, assuming room in the network is available, before serving another injection FIFO metadata. The \u201cis_active\u201d bit  is used by the application to determine if the descriptor is active.","In some applications, there are multiple communication patterns that may be active during different parts of the application. Each such pattern may be described by the message descriptors within an injection FIFO. If the number of such patterns is greater than the number of injection FIFOs supported by the injection FIFO metadata, he DMA can be reprogrammed so that injection FIFO metadata can be switched with very low overhead from one communication pattern to another. The DMA engine  only serves the injection FIFO metadata if it is active; the injection FIFO metadata is activated by the communication application (for example, the global message-passing application) using the activate bit . The communication application de-activates the descriptor using the de-activate bit . In this case, the application uses the \u201cis_empty\u201d bit  to see if injection FIFO metadata is finished as to its current injection FIFO and is available for a new injection FIFO. If the \u201cis_empty\u201d bit is set, the application may de-activate the injection FIFO using deactivate bit . This deactivation ensures that the DMA engine does not act on inconsistent information in the injection FIFO metadata while it is being reprogrammed by the application from one injection FIFO to another. To reprogram the injection FIFO metadata, the application then writes in the new injection FIFO metadata including the start, end, producer and consumer addresses. The application then re-activates the injection FIFO metadata using the bit . In this way, only the injection FIFO metadata is reprogrammed to point to a different injection FIFO in memory ; the message descriptors in these different injection FIFOs need not be reprogrammed.","Message Passing Interface and \u201cMultisend\u201d","Parallel computer applications often use message passing to communicate between processors (or between multi-processor nodes). Message passing utilities such as the Message Passing Interface (MPI) support two types of communication: point-to-point and collective. In point-to-point messaging a processor sends a message to another processor that is ready to receive it. In a collective communication operation, however, many processors participate together in the communication operation. Examples of collective operations are broadcast, barrier, all-to-all, and the like.","Each collective communication operation should preferably to be optimized to maximize performance. The known methodologies implement the collective communication operations through separate calls and in a separate software stack. Most typical implementations are specific to hardware or part of specific languages or runtimes. Such implementation methods result in high development and maintenance overheads. In addition, in known methodologies, this type of implementation is repeated for every new version of a parallel computer. For different parallel computers or different versions of a parallel computer, several different parallel programming paradigms need to be supported and each of them may define its own collective primitives. Each of these typically requires separate implementations and optimized runtimes.","US Patent Publication 2009\/0006810, entitled \u201cMechanism To Support Generic Collective Communication Across A Variety Of Programming Models,\u201d the complete disclosure of which is expressly incorporated herein by reference in its entirety for all purposes, describes a system and method to modularize and support a plurality of functionalities and sub-functionalities in providing a collective framework that can be used across different parallel computers that use different message passing programming languages or models. Modules or functionalities of the collective framework may include: 1) programming language semantics, which handles sub-functionalities such as collective API (application programming interface) and synchronization modes; 2) data transfer functionality; 3) collective operations and mechanisms functionality, which may include sub-functionalities such as phase-by-phase traversal of virtual and physical topologies, and network optimizations; 4) specific optimization functionalities such as pipelining, phase independence, and multi-color routes.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 4"},"A schedule , for instance, handles a functionality of collective operations and mechanisms. A schedule  includes a set of steps in the collective mechanism that executes a collective operation. Collective operation may be a call in MPI, such as \u201cbroadcast,\u201d \u201callreduce,\u201d and other calls. Collective mechanism describes how the collective is done; for example, via a spanning tree broadcast mechanism. A schedule  traverses graphs and topologies of nodes in the parallel computer. The schedule may be generated for each node in the collective operation. The schedule may depend on the rank of the node and the rank of the root of the collective. Briefly, ranks identify processes participating in the message passing operation.","A schedule  may split a collective operation into phases. For example, a broadcast can be done through a spanning tree schedule where in each phase a message is sent from one node to the next level of nodes in the spanning tree. In each phase, a schedule  lists sources that will send a message to a processor and a list of tasks that need to be performed in that phase. Examples of tasks may be sending messages or performing arithmetic operations. A schedule may be implemented as a C++ class, which may be called by an executor. An executor  then performs the list of tasks specified by the schedule .","An executor  may handle functionalities for specific optimizations such as pipelining, phase independence and multi-color routes, but not limited to only those optimizations. An executor  may query a schedule on the list of tasks and execute the list of tasks returned by the schedule. An executor  may also handle pipelining and specific optimizations for each collective operation. Typically, each collective operation is assigned one executor.","A multisend interface  provides an interface to \u201cmultisend.\u201d The collective framework may call the multisend interface . The interface may be implemented on a specific architecture, for example, the BlueGene\/P \u201cmultisend\u201d implementation . For example, in the Blue Gene\/P product there may be provided a \u201cmultisend\u201d implementation to allow the collective framework to be available on Blue Gene\/P. It is to be emphasized that references to specific products such as Blue Gene\/P are for purposes of example and illustration, and not limitation.","A \u201cmultisend\u201d is a message passing backbone of a collective framework that provides data transfer functionalities. \u201cMultisend\u201d functionality allows sending many messages at the same time, each message or a group of messages identified by a connection identifier. \u201cMultisend\u201d provides connection functionalities for a distinct stream of data. It is typically a one to many or even many to many message passing event. \u201cMultisend\u201d functionality also allows an application to multiplex data on this connection identifier.","In some instances, \u201cmultisend\u201d provides point-to-point message passing capabilities with many messages being sent. In such cases, the data is only sent between a pair of processors. The receiving processor needing to process the message processes the message and chooses to forward it only on a different \u201cmultisend\u201d call. Thus, in some instances of \u201cmultisend\u201d functionality, data is not forwarded automatically. In addition, connections are managed externally and no communicators are required. That is, in some instances, \u201cmultisend\u201d is independent of communicators, as it may not require a group or communicator to be set up before the \u201cmultisend\u201d is called. A call to \u201cmultisend\u201d lists all the processors that need to participate.","Such implementation of a \u201cmultisend\u201d functionality provides a good match for massive parallel computer system (e.g., BlueGene\/P) hardware or other architectures with slow cores because the message posting and startup overheads are amortized across many messages. Multisend interface  allows a code implementing collective operations to be device independent. Each \u201cmultisend\u201d may be a phase of a given collective operation.","A processor may multicast a message to many other destinations in a \u201cmultisend\u201d call. For example, a processor of rank  can multicast data to processors of ranks , , ,  in one call. The data from the processor of rank  is then received by processors of ranks , , ,  in their respective local buffers on completion of the \u201cmultisend\u201d call. On the Blue Gene\/P torus network described above and in US Patent Publication 2009\/0006296, a \u201cmultisend\u201d can be used to send a deposit bit to destinations along a dimension of a torus. For example, a processor can make a deposit bit \u201cmultisend\u201d call to all its X neighbors on the torus. No predetermined groups are needed for the \u201cmultisend\u201d call. A processor can issue \u201cmultisends\u201d to any destinations.","A \u201cmultisend,\u201d in some instances, communicates to multiple nodes via a single message request. A send mechanism may involve sending data originating from at least one node to one or more destinations, in which the data from different sources is distinguished at destinations using a tuple (a sequence or ordered list of finite length) that includes at least a stream or connection identifier. The tuple may include <source, destination, connection identifier>. The multisend interface at the destination passes the data using this tuple to the schedule and\/or executor waiting for that data.","When several processors issue \u201cmultisend\u201d calls to the same destination processor, that destination processor would typically need to manage these different streams of data. In one embodiment, each node keeps a connection list and if the different sources send data on different connections, the data can be distinguished from one and another using the connection list and identifiers. Some instances use a connection manager component to choose the connections for each \u201cmultisend\u201d call. A connection manager in one embodiment controls a connection identifier for each \u201cmultisend\u201d operation. For example, many classes of a collective operation with the same schedule and executor can overlap with each other with different connection identifiers generated by the connection manager. A connection manager may maintain a list of available connections between a pair of processors and hand each \u201cmultisend\u201d call an available connection. When all connections are in use, a \u201cmultisend\u201d call can wait for a connection to become available. Connections are identified by connection identifiers in some cases.","A language adaptor  interfaces the collective framework to a programming language. For example, a language adaptor such as for a message passing interface (MPI) has a communicator component . Briefly, an MPI communicator is an object with a number of attributes and rules that govern its creation, use, and destruction. The communicator determines the scope and the \u201ccommunication universe\u201d in which a point-to-point or collective operation is to operate. Each communicator contains a group of valid participants and the source and destination of a message is identified by process rank within that group. Communicators are dynamic, that is, they can be created and destroyed during program execution. A language adaptor may also define an API , which is suitable for that programming language. The API for Unified Parallel C (UPC) collectives is likely to be different from MPI collectives, for example.","Each application or programming language may implement a collective API  to invoke or call collective operation functions. A user application for example implemented in that application programming language then may make the appropriate function calls for the collective operations. Collective operations may be then performed via an API adaptor  using its internal components such as an MPI communicator  for MPI in addition to the components in the collective framework, such as schedules, executors, \u201cmultisends,\u201d and connection managers.","Once all components have been implemented, only the language adaptor typically needs to be different for each programming language. Thus, all components except for the language adaptor may be reused across different processors on multiple parallel computers, even on those that use different programming languages or models. Thus, some instances abstract collective communication into components, and is general and applicable to most parallel computer hardware and most programming languages, providing a \u201cprogram once and use at many scenarios\u201d paradigm.","The common paradigm for collective operations in one or more instances supports various semantics, mechanisms, optimizations, and data transfer in effecting collective operations. In a collective communication operation, a group of processors (either from a predetermined communicator or on the fly) participate to carry out a global operation where each of them either receives data or contributes to that operation.","Semantics","The different programming languages have their own semantics with regard to collectives. Non-limiting illustrative examples follow of semantics that may be supported in the framework.","1. Synchrony Vs. Asynchrony:","In a synchronous collective operation all processors have to reach the collective before any data movement happens on the network. For example, all processors need to make the collective API or function call before any data movement happens on the network. Synchronous collectives also ensure that all processors are participating in one or more collective operations that can be determined locally. In an asynchronous collective operation, there are no such restrictions and processors can start sending data as soon as they reach the collective operation. With asynchronous collective operations several collectives can be happening simultaneously at the same time.","2. Blocking Vs. Non-Blocking:","With blocking collectives the processor blocks on the collective and hence can process the messages for the collective as soon as they come in. In case of a reduce operation the processor can also perform arithmetic. With a non-blocking collective the processor initiates the collective and periodically polls the messaging software to make progress on the collective. This allows the processor to compute while the collective is on the network in progress.","3. Groups Vs. on-the-Fly:","A collective operation may be performed on a pre-negotiated or pre-established group of nodes on the parallel computer network, for instance, like a communicator in MPI which is established beforehand. In some programming paradigms a collective operation can happen on the fly where an incoming message carries enough information to build the group for that collective operation on arrival.","4. Collective API:","Each programming language may choose to define its own collective Application Programmer Interface.","Collective Mechanism","A collective mechanism is a set of steps to perform a specific collective operation. It usually involves a group of processors interacting with each other in a specific order forming a virtual-topology. The virtual topology can then be aligned to the physical-topology of the communication network inter-connecting the processors. A schedule functionality shown in  may provide this functionality.","Non-limiting illustrative examples follow of collective operations that may be supported in the framework.","1. Broadcast:","A broadcast is defined by a root sending a message to a group of processors. The semantics of the broadcast are defined by an individual programming language. A language adaptor interfaces the semantics that are specific to the individual programming language to the generic collective framework.","2. Barrier:","In a barrier operation a processor leaves the barrier only after all the participating processors have come to the barrier.","3. Reduce:","In a reduce operation several nodes contribute data to be globally summed. The output of the reduction is returned on the root.","4. All-Reduce:","An all-reduce is similar to the reduce operation, but the output is returned on all the nodes. In some cases, an all-reduce can be designed as a reduce followed by a broadcast.","5. All-to-All:","Each node sends a different message to every other node.","Specific Optimizations","For each collective operation there may be many specific optimizations that are exclusive to that operation. Examples of optimizations that may be supported include but are not limited to pipelining, multi-color collectives, and phase independence.","Pipelining optimization can be applied to broadcast, reduce and all-reduce collectives. Here the collective is performed in small chunks and moved to intermediate nodes while the starting nodes work on the next chunk.","In a multi-color collective, data moves along multiple edge-disjoint routes of a dense interconnection network. A collective operation can be parallelized across the different independent routes. Multi color collective may be supported by the framework. An example of a multi-color collective may include, but is not limited to, multi color rectangular broadcast where packets can go along X+, Y+, Z, X\u2212 directions in a no-blocking manner with packets along Y+, Z+, X+, Y\u2212. Each of these is represented by a color. Packets along one color do not block the other colors.","Phase-independence guarantees that some collective operation phases are independent. That is, the tasks in one phase are not dependent on the tasks in another phase. An executor component  shown in  may provide the functionalities of optimization.","In a broadcast for example, the same data is sent in all phases. Each non-root node (for example, child or intermediate node of the broadcast spanning tree) receives the data once and sends data many times. Hence the data transfer operations in the different phases are independent of each other and can be done simultaneously.","Data Transfer Support","A collective framework may be network device independent. It may function with an external support for message passing for processors to communicate with each other in a collective operation. The data transfer in one embodiment may be through a published multisend interface. For instance, the collective framework may be built on top of a lower level messaging library which provides the \u201cmultisend\u201d call implementation.",{"@attributes":{"id":"p-0087","num":"0086"},"figref":["FIGS. 5A-5C","FIG. 5A","FIG. 5B","FIG. 5C","FIG. 5A","FIG. 5B","FIG. 5C","FIG. 5A","FIG. 5B","FIG. 5C","FIG. 5A","FIG. 5B","FIG. 5C","FIG. 5A","FIG. 5B","FIG. 25C","FIG. 5A","FIG. 5B","FIG. 5C","FIG. 5A","FIG. 5B","FIG. 5C","FIG. 5A","FIG. 5B","FIG. 5C"],"b":["2002","2004","2006","2008","2010","2012","2014","2016","0","0","2002","0","4","2010","1","2","2006","2","1","2004","4","2010","0","0","2002","1","6","2014","2","5","2012","1","2004","0","1","2","0","2002","2","2006","0","1","0","2002","2","3","2008","3","2008","0","1","2","2","2006","5","2012","0","1","2","4","2010","6","2014","0","1","4","2010","2","7","2016","7","2016","0","1","2","6","2014"]},"The above schedule is initially created or agreed upon among the processors involved in the collective operation. A schedule on each participating processor has a list of tasks specific to the respective processor's rank and the root of the collective. A schedule class may be written by a programmer who is aware of the collective operation and the topology constraints.","The executor executes the schedule and performs the sending and receiving of data as directed by the schedule. The executor calls \u201cmultisend\u201d to do the actual data movement. For example, on the root there may be three calls to \u201cmultisend\u201d to send the data shown in . Depending on how many broadcasts are in flight the connection manager generates a unique connection identifier so that the data from the different broadcasts can be distinguished. For example, a broadcast on communicator (a group of processes in MPI) could use the communicator identifier as the connection identifier so that many broadcasts of different collectives can occur together.",{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 6","b":["3002","3004","3002","3004","3008","3006","3006","3002","3004","3002","3008","3006","3012","3010","3008","3010"]},"A common programming paradigm for performing collective operations on parallel computers may be sued, by way of example and not limitation, computers such as BlueGene\/P. Examples of techniques that can be used to implement the common paradigm include but are not limited to following methods. Any other mechanism may be used to extend the collective framework of components. In a new parallel computer, a new \u201cmultisend\u201d call may be implemented. Once that is done all the supported programming languages, i.e. for which adaptors have been designed can be supported on this platform. To support a new collective mechanism or optimization for a specific architecture a new schedule may be developed to take advantage of the new features in the architecture. To extend the framework to a new programming language, a new language adaptor may be developed. This would allow the collective calls in the programming language to use the general interfaces in the collective framework.","Deep Computing Messaging Framework","The Deep Computing Messaging Framework (DCMF) is a message passing runtime designed for the Blue Gene\/P machine and similar architectures. DCMF supports several programming paradigms such as the Message Passing Interface (MPI), Aggregate Remote Memory Copy Interface (ARMCI), Charm++, and others. This support is made possible as DCMF provides an application programming interface (API) with active messages and non-blocking collectives. The DCMF runtime can be extended to other architectures through the development of architecture specific implementations of interface classes. The production DCMF runtime on Blue Gene\/P takes advantage of the direct memory access (DMA) hardware to offload message passing work and achieve good overlap of computation and communication. The skilled artisan will be familiar with DCMF from \u201cThe Deep Computing Messaging Framework: Generalized Scalable Message Passing on the Blue Gene\/P Supercomputer,\u201d by Sameer Kumar et al., presented at the 2008 International Conference on Supercomputing held Jun. 7-12, 2008 on the Island of Kos, Greece, and archived in the proceedings thereof, ISBN 978-1-60558-158-3. The complete contents of the aforesaid Kumar et al. paper are expressly incorporated by reference herein in their entirety for all purposes.","The hierarchical structure of the DCMF runtime is presented in . The majority of applications  are expected to use MPI  or other common middlewares supported by the stack (e.g. ARMCI , Converse\/Charm++ , or others ). The DCMF stack builds upon and co-exists with Lower Level Network APIs . Note that, for example, applications can simultaneously use MPI, ARMCI or Charm++, DCMF API and low-level network APIs to minimize overheads in latency critical regions, as illustrated by arrows , . The DCMF API  serves as the primary interface for higher level messaging systems (or custom applications). This API defines a minimal set of functions to initialize, query, configure and utilize the communication hardware . To ensure message progress, the DCMF API defines four possible thread levels that coincide with their MPI equivalents. Mutual exclusion is ensured for all critical sections within DCMF when the configuration requires it. The runtime system can also be configured to enable interrupts on packet arrival and have a dedicated lightweight communication thread make progress on messaging. Current MPICH implementation  on Blue Gene\/P takes advantage of interrupt support to be fully compliant with the MPI progress semantics.","The DCMF API exposes three basic types of message passing operations: two-sided point-to-point send (DCMF Send), one-sided put (DCMF Put) and get (DCMF Get), and multi-send. All three have non-blocking semantics to facilitate overlapping of computation and communication. The user is notified of completion of communication events through callback functions.","Note global arrays . Internal components  are discussed below.",{"@attributes":{"id":"p-0096","num":"0095"},"figref":"FIG. 8"},"Multiple messaging contexts: To prepare a message transfer, the processes create messaging contexts. For two-sided communication, the context is created via the DCMF Send register( ) call in block . This call takes the reception callback (the cb recv  in ) as an argument and associates it with a new context stored in the opaque persistent protocol object. The context can then be used for initiating subsequent send operations. Multiple contexts can co-exist and be used simultaneously in the same application.","Active message model: Two-sided send operations can be initiated through the DCMF Send call as in block , which takes the context and the message info argument. The latter can deliver meta-data information along with the payload to enable message matching at the receiver side. When the receiver is processing the first packet of the message, the DCMF runtime invokes the reception callback associated with the context and passes it the meta-data (the info argument) and the size of the message as parameters. The callback must return a buffer where the incoming message needs to be stored.","Multisend protocol: DCMF provides a novel multisend protocol. In a multisend, many point-to-point messages to different destinations can be sent through a single operation, thus amortizing the software startup overheads. This call is of importance for low frequency core architectures. Multisend also enables network hardware specific optimizations for groups of messages (e.g., depositing packets along straight lines on the torus network). DCMF provides two flavors of the multisend protocol: multicast and many-to-many. Many-to-many allows different messages to be sent to different processes through a single API call. Multicast is a special case of many-to-many where the same message is delivered to all target processes.","Collective protocols: The API defines function prototypes for optimized non-blocking collective operations including broadcast, reduce\/allreduce, barrier and all-to-all, most of which are implemented via multisend.","Receiver-side activity is shown in blocks , .","With reference to , DCMF has a component based design  with four abstract base components (manifested as C++ classes): device , , ; protocol , , ; sysdep  and \u201cmessager\u201d . An abstract device component manages hardware and software resources associated with a corresponding network device. The abstract device layer in DCMF is important to support architectures which have several different communication networks. It also aids porting the system across various architectures. An abstract device class handles DMA resources.","The \u201cmessager\u201d object encapsulates devices used by a particular manifestation of DCMF. Each DCMF implementation can have its own \u201cmessager\u201d implementation that manages only those devices which exist on the particular architecture. The \u201cmessager\u201d initializes the devices and provides a unified advance method for the higher layers to ensure message progress.","Portability of DCMF is enhanced by the sysdep component , which provides an abstract interface for all kernel dependent services. These services include thread-management, intra-node synchronization, personality information lookup (e.g., local rank and global torus mapping), etc.","Messaging mechanisms (like eager or rendezvous two-sided send) are implemented as protocol classes. The base protocol class provides the generic registration framework for managing multiple messaging contexts. A protocol object typically creates a few device dependent and interrelated message objects, and then posts them in the proper sequence to the appropriate device queues. Devices signal the protocol object through callbacks when posted messages complete. These callbacks may trigger creation and posting of more messages until the required data transfers have been completed.","The abstract device and protocol layers facilitate building protocol classes that use more than one device to implement complex messaging mechanisms. The component based design of DMCF results in high extendability: novel mechanisms and optimizations can easily be explored by adding new protocol objects on top of existing device and message classes.","A fully functional generic DCMF runtime is available via the sockets device, socket send protocol, and the PutOverSend and GetOverSend protocols. These components implement the core functionality of the DCMF runtime and provide a base for porting DCMF to other platforms. A new DCMF runtime implementation optimized to a particular network architecture simply needs to implement a network device and send protocol. The GetOverSend and PutOverSend protocols implement the DCMF Get and DCMF Put APIs through the DCMF Send interface. This allows the implementor to optimize a new DCMF runtime implementation in stages without dropping functionality during the bringup process. The GetOverSend and PutOverSend are intended to ease portability to new architectures and not provide optimal performance for any specific platform.","The sockets version of the DCMF runtime allows portability to generic platforms such as Linux, Mac OS X, Z\/OS, or other architectures. The reason to port to the sockets platform is twofold. The first is that it is a test environment for the DCMF developers to build and test protocols and mechanisms in a portable, low cost, highly available environment. Second, users of the DCMF API who do not have ready access to Blue Gene hardware can test their application code on a non-Blue Gene environment such as workstation Linux. The sockets version of the code currently only scales to a small number (32 process ranks) of nodes, but it can be used as a proof of concept for the portability of the platform and to test the APIs.","Record and Replay","Thus, with reference again to , a parallel computer system, such as the Blue Gene\/P machine, may use the DMA unit  to offload communication from the processing cores . In the Blue Gene\/Q machine, similar functionality to DMA units  is provided in so-called messaging units (MU). References herein to DMA units are to be understood as also referring to messaging units or any other units with similar functionality.","Each message passing operation injects a descriptor to an injection FIFO  in the memory  (the DMA engine  has injection FIFO metadata  (e.g., head and tail pointers) which points to the descriptor in memory ). The descriptor is then processed by the hardware of DMA . There is significant processing overhead associated with injecting descriptors and managing state for each message passing operation. The descriptors may, for example, tell the DMA engines  where to send data, set forth the buffer of the source, set forth the intent of the message (for example, is it to be deposited on a destination buffer or FIFO or is it supposed to be a \u201cremote get\u201d operation where the message is targeted to a totally new destination), and the like. A descriptor thus typically includes a great deal of information with respect to what a message should do.","By way of summary, to avoid burdening the processors  with having to manage communication, the DMA  is employed. The DMA unit sends data in messages. The DMA unit typically packetizes the data and pulls the appropriate data out of the memory . The descriptor has pointers as to where the message starts in memory  and how long the message is. The DMA engine  pulls out the data one packet at a time. A packet could be, for example, up to 512 bytes. The DMA engine puts the packets into the network , the packets flow through the network , and the packets arrive at the destination target node (for example, they could be sent from node () to destination target node ()). Information in the packet header tells the DMA  in the receiving node (for example, node ()) what to do with the packet; for example, where to put it in the memory  of the receiving node. A two kilobyte message could be broken up, for example, into four 512 byte packets. Thus, when sharing data among nodes  in a parallel computing system, there is significant overhead associated with using the descriptors to pass the messages.","In a first level of optimization, the DMA  offloads work from processors  with respect to injecting data and pulling data out of the network . In a second level of optimization, since even the overhead of programming the DMA can be significant for some applications on very large machines, enhancement is obtained by programming the DMA via descriptors. Thus, message passing has been enhanced, and even optimized, via the special-purpose DMA hardware, and one or more embodiments herein address programming overhead.","Very frequently, scientific applications are iterative in nature and the messages sent by the DMA in each iteration are repetitive in nature; that is, the same number of bytes are sent to the same recipients, starting in the same locations in memory on every iteration. The data in the buffers, which is sent, is different in each iteration, but often the descriptor would be identical. For example, consider a case where one megabyte of data is sent between the same points in every iteration; it starts in the same place and is to be placed in the same location. If one were to analogize to packets, it could be said that the headers were the same in each case, and only the payloads were changing. US Patent Publication 2009\/0006296 shows an optimization wherein, if such a pattern is present, it is possible to set up a FIFO which contains a plurality of descriptors. The FIFO only needs to be built once. Then, to \u201ckick off\u201d all the messages, all that is required is to change the pointers (metadata ) to the FIFO  as to where the FIFO starts, where the first descriptor is, and where the last one is. FIFO  is simply a location in main memory . Thus, with very low overhead, just by changing the pointers, one can very efficiently send the repetitive descriptors with low software overhead. In essence, the message descriptor is in the FIFO \u201csomewhere\u201d in memory , and the DMA has pointers to the FIFO, where the first descriptor is, how many descriptors there are, and where the last one is. Thus, it is built the first time and is now present in the FIFO; all that it is necessary to do the second time is to change the pointers that tell the DMA where the messages start and stop; that is, where to point to in the FIFO to send the next message(s).","By way of review, in some applications, the communication pattern is persistent, that is, the same messages from the same buffers are sent in each iteration. For each of these messages a descriptor has to be injected into the injection FIFOs . Some applications can have several phases of computation and communication, which may overlap. Each of these phases may require different injection FIFOs. Injection FIFOs  are scarce resources.","Thus, aspects of US Patent Publication 2009\/0006296 take advantage of an application where a list of DMA descriptors is built and then all that is required to optimize communication is to change the head and tail pointers.","One or more embodiments of the invention address an arbitrary online communication pattern (any application) and provide software to record the communications pattern. The pattern can be recorded, for example, by using interface calls such as the above-mentioned Deep Computing Messaging Framework (DCMF) interface calls to record the communication pattern. DCMF is, as noted, a lightweight communication framework for petascale supercomputing. It resides in the Blue Gene P messaging software stack between the systems programming interface (which interfaces with the network hardware) and the library portability layer, which in turn interfaces with the application layer. It is intended as a non-limiting example of a high-level application communication framework for handling interface calls from an application, which may be intercepted and saved as described herein. Whenever the application initiates one of the interface calls, the FIFOs that the DMA uses are saved and at a later point, when it is desired to restart the persistent communication pattern, simply call \u201creplay\u201d and the DMA head and tail pointers are set so that the communication pattern is replayed.","It is to be emphasized that DCMF is a non-limiting example; in other embodiments, alternatives could be employed; for example, the well-known message passing interface (MPI) standard. In some instances, use can be made of persistent communications within MPI; for example, the MPI_Start and MPI_Send_init commands.","In one or more approaches not employing one or more embodiments of the invention, a user would have to write very low level software to set up the FIFOs and change the pointers, requiring a good deal of work. Aspects of the invention include techniques to record a communications pattern including software (for example, the aforementioned message passing application) which provides needed infrastructure to save the descriptors. The \u201cRecord\u201d feature can be turned on and off, and later, if one wishes to resend a similar message, all that is required is to employ the \u201creplay\u201d command. All the descriptors are saved in an appropriate buffer in memory , and the \u201creplay\u201d command deals with the low-level hardware complexities of setting the pointers to the correct location, thus drastically reducing the amount of work required. To initiate record, in some instances, explicitly put \u201crecord begin\u201d and \u201crecord end\u201d statements in the executing application program (e.g., scientific or financial program). In other instances, the DCMF_Multicast and DCMF_Manytomany API calls automatically record the communication patterns so no modification to the executing application program is required. Thus, depending on which high-level application communication framework is employed, it may or may not be required to modify the executing application program to start and end recording. On the other hand, the \u201creplay\u201d command will typically require an explicit call in the executing application program. In some instances, the MPI API call \u201cMPI_Send_init\u201d functions as a \u201crecord\u201d call while the MPI API call \u201cMPI_Start\u201d functions as a \u201creplay\u201d call.","Thus, in one or more embodiments, persistent applications can be enhanced and even optimized by the use of a record-replay feature. During the \u201crecord\u201d phase, all messages sent via the \u201cmultisend\u201d in the DCMF API calls (such as DCMF_Multicast and DCMF_Manytomany) inject descriptors on one injection FIFO  and the DMA head and DMA tail pointers are saved. Successive application (e.g., scientific or financial application) iterations call \u201creplay,\u201d to replay the saved communication operation. The replay results in the DMA head and tail pointers being set to the start of the list of saved descriptors.","\u201cMultisend\u201d is an interface call, described above and in US Patent Publication 2009\/0006810. Examples of what \u201cmultisend\u201d allows one to do are as follows. Suppose a node wants to broadcast the same data to a number of different nodes. The \u201cmultisend\u201d command could specify the different nodes. In another case, it might be desired to send different data to a number of different nodes (the aforementioned \u201cMany-to-Many\u201d would be appropriate in this case). Aspects of the invention apply to any collection of messages that it is desired to send, including both point-to-point (unique sender sending to unique receiver) and collective communication. Thus, references to \u201cmultisend\u201d and \u201cMany-to-Many\u201d are exemplary and non-limiting in nature.","A persistent ID identifies each saved communication context. A large cumulative buffer for all application contexts is allocated at application start up. One (for example) injection FIFO is located in main memory. Injection FIFO metadata on the DMA points to this buffer. When the record call is made, the communication pattern is recorded in a section of the large cumulative buffer. After the record phase, the next time the application sends the same set of messages, the replay call will set the head and tail pointers in the DMA to point to the section where that pattern was recorded. This allows a large number of communication patterns to be sent with just one hardware injection FIFO, as described below with respect to .","Some embodiments employ a software queue to queue up record and replay operations and allow only one persistent context to be active at a time, thus utilizing only one or two injection FIFOs. Hence, a very large number of contexts can be scheduled to be replayed one after another, allowing the application programmer flexibility to save several different overlapping phases as separate contexts, rather than being limited to the number of injection FIFOs.","The record-replay feature can also be used to optimize collective communication operations. Scientific applications use collective communication calls (for example MPI_Bcast and MPI_Allreduce commands as known to the skilled artisan) to optimize data movement on a large number of processors. On the Blue Gene\/P and Blue Gene\/Q machines, the DMA and Messaging units offload communication from the processor cores. On the DMA devices, communication patterns can be recorded and then replayed with very low overhead by setting the head and tail pointers in the DMA devices with extremely low overheads.","Recalling , a collective communication operation is a series of phases which have to be executed in strict order. For example, in a \u201creduce\u201d operation, data is summed at the leaves of a spanning tree butterfly, from where the data is sent to the intermediates nodes in the tree and finally to the root level nodes. At each level, each node has many child nodes and parent nodes in the butterfly. For short \u201callreduce\u201d operations, using message passing for each of these communication operations has associated overhead.","Applications often repeat the collective operation. For short operations, it is possible to copy source buffers to persistent temporary buffers so that the collective operation is made persistent. At each level of the spanning tree, the processor waits for counters to reach zero and then initiates the send to its parent nodes in the butterfly. Each of these operations can be initiated via a direct-put to minimize processing of the messages at the cores . Once all the data has been completed the processor initiates the next set of replay messages by changing pointers on the DMA devices.","To enable record\/replay, the application first records the communication pattern by calling a \u201cmultisend\u201d call. The persistent \u201cmultisend\u201d allocates a large buffer with finite pre-allocated space for each persistent index and sets up a DMA FIFO to have start and end point to this larger cumulative buffer. During the record phase, the application injects descriptors into a part of this cumulative buffer. During replay, the FIFO head and tail pointers point to the section of the cumulative buffer determined by the persistent ID.  illustrates this record-replay operation.","In , note the three communication patterns  (represented by single-hatching),  (represented by double-hatching), and  (represented by stippling). An application, such as a scientific or financial application has, in this example, the three communication patterns , ,  that it wished to record, as indicated at the top of . Each pattern , ,  may be provided with its own persistent ID. Each pattern is recorded in large cumulative buffer , as seen at the bottom of . When it is desired to replay a given communication pattern (in this case, pattern ), the head and tail pointers in injection FIFO metadata  are set to point to start  and end  locations of pattern  within cumulative injection FIFO . When the head and tail pointers are set as shown, the DMA engine  will replay pattern  by pulling the descriptors of pattern  from memory  in accordance with the pointers in metadata . Thus, one or more embodiments of the invention advantageously permit recording many communication patterns in one (or in alternative embodiments, two, or a small number) of injection FIFOs  (previous techniques have been limited in the number of communication patterns that can be replayed by the number of available injection FIFOs).","Often, applications have multiple different communication patterns, and each of the communication patterns need to be individually replayed independent of the other communication patterns. Behind the scenes, the system software (local instance of global message passing application) simply records everything into a single large buffer ; however, it will appear to the application that the different application communication patterns , ,  are recorded separately and are each identified by a different persistent identifier. When a replay is conducted, the DMA head and tail pointers are set to the section of the large buffer  which corresponds to that particular communication pattern (e.g., point to pattern  as seen in ). In this scheme, the amount of communication patterns that can be recorded and replayed is arbitrarily large (say, thousands), while the number of injection FIFOs which must be used in the DMA is quite small (say, one or two). While the number of injection FIFOs and the number of parallel communication channels in a DMA is limited, in this approach, it is not limited by the number of communication patterns an application has.","One or more embodiments of the invention provide a mechanism to record a communication pattern and then replay it at a very low overhead, as well as a mechanism to extend \u201cmultisend\u201d calls in the DCMF API to be efficiently recorded and then replayed at a later time, and\/or a mechanism to record and replay several hundred communication patterns by using only one hardware injection FIFO.","Given the description thus far, it will be appreciated that, in general terms, a parallel computer system, according to an aspect of the invention, includes a plurality of compute nodes . Each of the compute nodes includes at least one processor , at least one memory, , and a direct memory address engine  coupled to the at least one processor and the at least one memory. The system also includes a network  interconnecting the plurality of compute nodes. The network operates a global message-passing application for performing communications across the network. In some instances, the global message-passing application is provided as a library which can be explicitly included in the application it is desired to run (e.g., financial, scientific). The application it is desired to run, in such cases, will make MPI or DCMF library calls to communicate between the different nodes . The libraries may reside as binary code on a disc or other persistent storage (for example, file system ) and the application it is desired to execute links to those libraries. Local instances of the global message-passing application are loaded from the file system  through the network  and eventually end up in the main memory  of each node . The network  typically does not itself have an instance running on it. In some non-limiting exemplary cases, such as the aforementioned Blue Gene, communication between nodes  involves multiple networks . In some instances, a torus network is used for application data messaging, while the collective network and\/or Internet are used for file input\/output from system .","Local instances of the global message-passing application operate at each of the compute nodes  to carry out local processing operations independent of processing operations carried out at another one of the compute nodes. The direct memory address engines  are configured to interact with the local instances of the global message-passing application via injection FIFO metadata  describing an injection FIFO in a corresponding one of the memories . The local instances of the global message passing application are configured to record, in the injection FIFO  in the corresponding one of the memories , message descriptors associated with messages of an arbitrary communication pattern in an iteration of an executing application program. Note that, as used herein, including the claims, an iteration simply means an occasion to record or replay a pattern, and does not necessarily require that the executing application is implementing some kind of iterative solution technique. The local instances of the global message passing application are configured to replay the message descriptors during a subsequent iteration of the executing application program.","The message descriptors can include, for example, a destination, a source, and a message intent. The direct memory address engines  can be configured, for example, to assemble the messages in packet form by obtaining data from the memory  in accordance with the message descriptors. A given one of the local instances of the global message passing application may sometimes record the message descriptors in response to a record command in the executing application program. In other cases, a given one of the local instances of the global message passing application can be configured to automatically record the message descriptors by intercepting persistent application program interface calls of the executing application program. A given one of the local instances of the global message passing application may replay the message descriptors in response to a replay command in the executing application program.","The injection FIFO metadata may include head and tail pointers to the message descriptors in the injection FIFO, as shown in , and the local instances of the global message passing application may be further configured to replay the message descriptors by resetting the head and tail pointers (i.e., to point to the desired communication pattern , ,  in buffer .","The message descriptors may specify point-to-point and\/or collective communications.","As described with respect to , the message descriptors may, in some cases, be first message descriptors , in which case the arbitrary communication pattern is a first arbitrary communication pattern represented by a first persistent identifier. The local instances of the global message passing application may be further configured to record, in the injection FIFO  in the corresponding one of the memories , second message descriptors  associated with messages of a second arbitrary communication pattern, represented by a second persistent identifier, in another iteration of the executing application program. Furthermore, the local instances of the global message passing application can be still further configured to replay the second message descriptors  during another subsequent iteration of the executing application program, by resetting the injection FIFO metadata (i.e., make head and tail pointers point to  instead of ).","In at least some embodiments of the invention, the number of arbitrary communication patterns stored in the FIFO, each represented by its own persistent identifier and available for replay, may be quite large. For example, Fast Fourier Transform (FFT) is an efficient algorithm to compute the discrete Fourier transform. In a three dimensional Fast Fourier Transform (3D-FFT) operation, Fast Fourier Transforms must be performed along the X, Y and Z dimensions. The 3D-FFT operation is often used in many scientific applications, such as in molecular dynamics (Particle Mesh Ewald computation), computational quantum chemistry and distributed Navier-Stokes. Such applications often have a pair of forward backward 3D FFT operations with four different communication phases. In at least some embodiments of the invention, these four communication phases can each be recorded with a different persistent identifier and then replayed on the same injection FIFO. In some instances, there may be at least one thousand communication patterns stored in the FIFO, each represented by its own persistent identifier and available for replay.","With reference to , an exemplary method  begins at step . In step , provide a system of the kind described. In step , record, with the local instances of the global message passing application, in the injection FIFO  in the corresponding one of the memories , message descriptors associated with messages of an arbitrary communication pattern in an iteration of an executing application program. Recording may be automatic or responsive to a command, as described elsewhere herein. In step , replay the message descriptors, with the local instances of the global message passing application, during a subsequent iteration of the executing application program. Processing continues at step .","In some cases, an additional step includes receiving a record command from the executing application program, as per the parenthetical in step , wherein a given one of the local instances of the global message passing application records the message descriptors in step  in response to the record command in the executing application program.","Typically, as per step , a replay command is received from the executing application program, wherein a given one of the local instances of the global message passing application replays the message descriptors in step  in response to the replay command in the executing application program.","As noted, in some cases, a given one of the local instances of the global message passing application records the message descriptors in step  automatically by intercepting persistent application program interface calls of the executing application program, as also indicated by the parenthetical in step .","In some instances, the message descriptors comprise first message descriptors, and the arbitrary communication pattern is a first arbitrary communication pattern represented by a first persistent identifier. An additional step may include recording, with a given one of the local instances of the global message passing application, in the injection FIFO in the corresponding one of the memories, second message descriptors associated with messages of a second arbitrary communication pattern, represented by a second persistent identifier, in another iteration of the executing application program, as in step . Again, such recording may be automatic or responsive to an explicit command. Another additional step may include replaying, with the given one of the local instances of the global message passing application, the second message descriptors during another subsequent iteration of the executing application program, by resetting the injection FIFO metadata, as in step . Step  may typically be in response to a command, as in step .","Exemplary Article of Manufacture Details","As will be appreciated by one skilled in the art, aspects of the present invention may be embodied as a system, method or computer program product. Aspects of the present invention preferably combine software (including firmware, resident software, micro-code, etc.) and hardware aspects. Furthermore, aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon.","A media interface, such as a diskette or CD-ROM drive, can be provided on file system  to interface with media.","Computer software including instructions or code for performing pertinent methodologies of the invention, as described herein, may be stored, for example, in file system  and, when ready to be utilized, loaded in part or in whole (for example, into memory ) and implemented by processors .","Suitable Input\/output or I\/O devices (including but not limited to keyboards, displays, pointing devices, and the like) can be coupled to the system.","As noted, aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium(s) having computer readable program code embodied thereon. Any combination of one or more computer readable medium(s) may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the computer readable storage medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing. In the context of this document, a computer readable storage medium may be any tangible medium that can contain, or store a program for use by or in connection with an instruction execution system, apparatus, or device.","A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms, including, but not limited to, electro-magnetic, optical, or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or transport a program for use by or in connection with an instruction execution system, apparatus, or device.","Program code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, RF, etc., or any suitable combination of the foregoing.","Computer program code for carrying out operations for aspects of the present invention may be written in any suitable programming languages.","Computer program instructions may also be stored in a computer readable medium that can direct a system  or the like to function in a particular manner, such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function\/act specified in the flowchart and\/or block diagram block or blocks.","The computer program instructions may also be loaded onto system  or the like to cause a series of operational steps to be performed to produce a computer implemented process such that the instructions which execute on the system  or the like provide processes for implementing the functions\/acts specified in the flowchart and\/or block diagram block or blocks.","It should be noted that any of the methods described herein can include an additional step of providing a system comprising distinct software modules embodied on a computer readable storage medium. Method steps can then be carried out or facilitated using the distinct software modules and\/or sub-modules of the system, as described above, executing on one or more hardware processors . Further, a computer program product can include a computer-readable storage medium with code adapted to be implemented to carry out or facilitate one or more method steps described herein, including the provision of the system with the distinct software modules and\/or sub-modules.","In any case, it should be understood that the components illustrated herein may be implemented in various forms. Given the teachings of the invention provided herein, one of ordinary skill in the related art will be able to contemplate other implementations of the components of the invention.","The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein, the singular forms \u201ca\u201d, \u201can\u201d and \u201cthe\u201d are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms \u201ccomprises\u201d and\/or \u201ccomprising,\u201d when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and\/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and\/or groups thereof.","The corresponding structures, materials, acts, and equivalents of all means or step plus function elements in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIGS. 5A-5C"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
