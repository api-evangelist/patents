---
title: System for reconstruction of symbols in a sequence
abstract: A method of reconstructing a damaged sequence of symbols where some symbols are missing is provided in which statistical parameters of the sequence are used with confidence windowing techniques to quickly and efficiently reconstruct the damaged sequence to its original form. Confidence windowing techniques are provided that are equivalent to generalized hidden semi-Markov models but which are more easily used to determine the most likely missing symbol at a given point in the damaged sequence being reconstructed. The method can be used to reconstruct communications consisting of speech, music, digital transmission symbols and others having a bounded symbol set which can be described by statistical behaviors in the symbol stream.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07024360&OS=07024360&RS=07024360
owner: Rensselaer Polytechnic Institute
number: 07024360
owner_city: Troy
owner_country: US
publication_date: 20030317
---

{"@attributes":{"id":"description"},"GOVINT":[{},{}],"heading":["STATEMENT OF POSSIBLE GOVERNMENT INTEREST","FIELD AND BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DESCRIPTION OF THE PREFERRED EMBODIMENTS","The Reconstruction Process","Reconstruction Models","Use of the GHSMM to Reconstruct Speech","Theory of Operation of the GHSMM","Practical Reconstruction Using a GHSMM","EXAMPLE"],"p":["This invention was developed in part with funding under U.S. Air Force project no. F30602-00-1-0527, and the United States government may have an interest in this invention.","The present invention relates generally to the field of symbol reconstruction and in particular to a new and useful method for accurately reconstructing missing portions of a sequence of symbols. The invention is particularly useful for reconstructing portions of oral conversations which are made unintelligible to a listener by surrounding ambient noise, and sudden, loud \u201cstaccato\u201d noise, among other noise sources.","Most forms of communication rely upon transmission of groups of discrete elements arranged in a manner which is understood by both the transmitting and a receiving party. Accurate communication between the transmitter and a receiver depends on the message formed by the groups of discrete elements, or symbols, being transmitted uncorrupted and intact between the two parties.","Corrupted streams of symbols or discrete elements which comprise a communications system having a known structure and probabilities can sometimes still accurately convey a message to a person capable of reconstructing the stream without the corruption. That is, the communication system has known characteristics, or language parameters. For example, when a non-native speaker of a language attempts to say something to a native speaker, the native speaker can often determine the meaning even though the message is not spoken the same way as the native speaker would because the native speaker can apply known language parameters. Similarly, when two persons communicating in the same language over a telephone have their conversation interrupted by noises, their knowledge of the language parameters of their speech sometimes allows them to \u201cfill in\u201d or reconstruct missing sections of the conversation and understand the intended message despite the corrupting noises.","Confidence windowing is the basis for many known reconstruction methods and employs the probability of unknown phonemes conditioned on its relationship with other symbols in the same communication stream. Confidence windows are discussed in greater detail in Christopher W. Scoville, . Master's Thesis, RPI, Troy, N.Y. 1998.","However, many times when a communication is corrupted, or damaged, by external noises, the message cannot be easily ascertained, even when both parties know the general language of communication or when other symbols in the communication are known. For example, if the outdoor performance of a symphony playing a new composition for an audience is corrupted by external noises like wind, traffic, etc., the audience will not likely be able to accurately determine what specific notes should have been heard. And, as well, when a communication is transmitted for reception by a large group of receivers, like a group of attendees at a seminar, some of the receivers of a corrupted portion of the communication may be able to reconstruct the intended message, while others cannot. This is due in part to a lack of knowledge of language rules that can be applied to the communication in these instances.","Many types of communication require accurate transmission and reception of uncorrupted messages. Computer voice recognition, for example, relies upon accurate speech communications from a person using the voice recognition. External interference with the transmission of a voice command to a computer can corrupt the command and result in no action or the wrong action being taken because the voice recognition capability cannot accurately reconstruct the command. That is, reconstruction is different from recognition in that it is a further step beyond recognition.","There are many other instances where it is advantageous to be able to reconstruct a corrupted message quickly and accurately. Speech reconstruction in particular is of great interest and has a wide range of applications, including interaction or communication with a computerized entity, law enforcement interception of communications relating to illegal activities, and assistance to persons with deficient hearing.","Different prior methods for recognizing sequences of symbols, such as speech recognition, are found in the prior art. As shown by its prevalence of use in modeling speech for recognition, hidden Markov models (HMMs) are a preferred modeling tool for this application.","Several patents disclose word recognition using hidden Markov models (HMM), including U.S. Pat. No. 5,608,840, which discloses a method and apparatus for pattern recognition using a hidden Markov model. HMMs are developed from signal samples for use in the recognition system. The HMM equations are weighted to reflect different state transition probabilities.","U.S. Pat. No. 5,794,198 teaches a speech recognition technique which reduces the necessary number of HMM parameters by tying similar parameters of distributions in each dimension across different HMMs or states.","Other patents disclosing speech recognition using HMMs include U.S. Pat. No. 5,822,731, U.S. Pat. No. 5,903,865 and U.S. Pat. No. 5,937,384. However, none of these three or the other patents teaching speech recognition disclose speech reconstruction. If a portion of received speech is not recognizable, the prior systems cannot determine the missing speech.","The ability to extrapolate and accurately replace missing pieces from a stream of symbols is what distinguishes reconstruction from recognition. Recognition assumes perfect or near-perfect communications, with no missing pieces. Recognition is effectively a conversion of a complete, uncorrupted communication from one media to another, such as voice to computer text. Reconstruction may include recognition for determining surrounding states, but is a further step beyond recognition. Reconstruction is a process of determining missing pieces of a communication and replacing those missing pieces with the correct piece, or symbol in the communication.","Hidden Markov models have been used by researchers in many speech processing applications such as automatic speech recognition, speaker verification, and language identification. An HMM is a doubly stochastic process where the underlying stochastic process for the model, usually described by a stochastic finite-state automaton, is not directly observable. The underlying stochastic process is only observed through a sequence of observed symbols, hence the term \u201chidden\u201d Markov model.","A characteristic of the HMM is that the probability of time spent in a particular state, called \u201cstate occupancy\u201d, is geometrically distributed. The geometric distribution, however, becomes a serious limitation and results in inaccurate modeling when the HMMs are used for phoneme recognition, which is essential to speech recognition.","The output of an HMM for each discrete time depends on the observation probability distribution of the current state. A discrete observation hidden Markov model, where the number of possible observation symbols is finite, can be completely described by a) the transition probably matrix describing the probability of transition between states of the finite-state automata, b) the observation probability matrix describing the probability distribution of the observation symbols given the current state, and c) the probability of being in a particular state at zero time.","Thus, the HMM output signal for each clock period depends on the observation probability distribution for the current state. With each clock pulse, a state transition is made depending on the state transition probability matrix. If transitions to the same state are allowed, then the state occupancy duration for a particular state is a random variable with a geometric probability distribution.","A semi-Markov model (SMM) is a more general class of Markov chains in which the state occupancy can be explicitly modeled by an arbitrary probability mass distribution. Semi-Markov models avoid the unrealistic implicit modeling of the state occupancy by replacing the underlying strictly Markov chain with a semi-Markov chain to explicitly model the state occupancy. As a result, semi-Markov chains do not necessarily satisfy the Markov property. While the knowledge of the current state is sufficient to determine the future states in a Markov chain, in a semi-Markov chain the future is also dependent on the past up to the last state change. Since the state occupancy durations are explicitly modeled, transition to the same state is not allowed. Although the semi-Markov model does not satisfy the strict Markov property, it retains enough of the main properties of the Markov chains.","Thus, there are drawbacks to using both HMMs and SMMs when reconstructing sequences of symbols, such as phonemes in a spoken communication.","A modification of the hidden Markov model, called a hidden semi-Markov model (HSMM) provides increased modeling accuracy over both SMMs and HMMs. The complete formulation of the HSMM and its training algorithms allow the HSMM to be used for any application currently modeled by an HMM by making appropriate modifications. Algorithms such as forward-backward procedure, Baum-Welch reestimation formula and Viterbi Algorithm can all be modified for use with an HSMM.","It should be noted that hidden semi-Markov models are different from hidden Markov models. HSMMs add a computational layer of complexity over HMMs which can increase the time to solve the equations and provide results.","Techniques have been developed at Rennselaer Polytechnic Institute to decrease the computation load while maintaining the desirable modeling characteristics of HSMMs. See, N. Ratnayake, \u201cPhoneme Recognition Using a New Version of the Hidden Markov Model\u201d. PhD Thesis, RPI, Troy, N.Y. 1992. Although these techniques are useful, further simplification while maintaining the accuracy of the HSMM is needed to improve it as a symbol sequence reconstruction method.","A method and system for reconstructing sequences of symbols using language parameters and a statistical assessment of the effects of known symbols on unknown symbols, are needed to improve symbol sequence reconstruction accuracy.","It is an object of the present invention to provide a new method and system using statistical analysis to reconstruct a sequence of symbols that is missing parts of the sequence.","It is a further object of the invention to provide a method of approximating confidence windowing techniques using hidden semi-Markov models and generalized hidden semi-Markov models to reconstruct a symbol sequence having missing symbols in a communications system having known parameters.","Yet another object of the invention is to provide a method for speech reconstruction by statistical analysis of the known speech in a stream of speech being reconstructed, without excessive memory or computational requirements for operation.","A further object of the invention is to provide a speech reconstruction method and system which can regularly produce greater than 80% accurate results.","Accordingly, a system and method of reconstructing a damaged sequence of symbols where some symbols are missing are provided in which statistical parameters of the sequence are used to approximate confidence windowing techniques to quickly and efficiently reconstruct the damaged sequence to its original form. Confidence windowing is a higher order calculation using the probabilities related to inter-symbol influence of known symbols in a stream of symbols upon the unknown symbols in the same stream.","In a first embodiment, a hidden semi-Markov model (HSMM) is used to approximate the confidence windowing technique. HSMMs are used to reconstructing a missing symbol in a stream of speech by considering the immediately prior and post states and state duration.","In a second embodiment, a generalized hidden semi-Markov model (GHSMM) is provided, which not only introduces greater accuracy through language parameters, but also by equivalently representing confidence windowing in a lower order equation. A GHSMM uses the same information as an HSMM, and further considers all states in a symbol stream. The method of reconstruction using the GHSMM can be operated on two separate time scales\u2014one time scale relates to prior statistical knowledge about language symbols and the other time scale relates to the relationship between known and unknown symbols within the same stream. The dual time scales make the topology of the system non-stationary.","The reconstruction method is done by first inputting a symbol stream into the system, pre-processing the damaged symbol stream to extract language parameters, applying a generalized HSMM equivalent to confidence windowing to determine the most likely candidates for replacing missing or damaged symbols, post-processing the symbol stream to replace damaged symbols and reconstruct the symbol stream, and outputting the reconstructed stream.","Alternatively, an HSMM is applied to approximate confidence windowing for determining likely candidates to replacing missing symbols. The application of the HSMM is modified by applying a non-commutative product operation and an algorithmic addition to the Viterbi Algorithm in order to maximize the probability of finding the correct replacement symbol for a damaged symbol in the stream.","The invention has been shown to produce repaired symbol streams having greater than 80% accuracy. The accuracy of the system and method make the invention especially useful for evaluating and repairing communications where one party has a speech impediment or a bad connection exists which introduce error and damage to the communication symbol stream. The invention is especially useful for law enforcement and protection, and emergency situations where receiving and understanding as complete a possible symbol stream from a communicating party is essential.","The various features of novelty which characterize the invention are pointed out with particularity in the claims annexed to and forming a part of this disclosure. For a better understanding of the invention, its operating advantages and specific objects attained by its uses, reference is made to the accompanying drawings and descriptive matter in which a preferred embodiment of the invention is illustrated.","As used herein, language symbol is intended to mean a discrete element, printed or verbal, such as a phoneme or group of phonemes, a letter or a number, which can be represented in a model and is part of a known system for communicating, such as a spoken or written language, or a number system.","The term acoustic symbol is used to mean an element comprising a single distinct sound. A phoneme or language symbol may comprise one or more acoustic symbols. For example, the language symbol could be a word, like \u201cinvention\u201d which comprises several phonemes and even more acoustic symbols; or, the language symbol could be a single phoneme, such as \u201ceh\u201d or \u201cum\u201d, which comprise only a few acoustic symbols. Still further, the acoustic symbol could be a single tone or note or sound, such as in a musical composition.","Unless specified otherwise herein, \u201csymbol\u201d is intended to mean a language symbol.","The application of the method and system described herein assumes that the damaged sequence of symbols is part of a language that has a structure with parameters that can be modeled.","And, as used herein, the term language is used to refer to spoken and written languages such as English, Spanish, or Japanese, as well as to other communications systems having structure and parameters which can be identified and quantified, such as music or numeric systems.","The invention herein provides a new and unique method and system for reconstructing damaged communications formed by sequences of symbols made with a communications language having known parameters. Reconstruction is based on the probability of occurrence of a missing symbol given the overall probability of occurrence and transition probabilities of surrounding symbols. The system includes detection of missing symbols, preprocessing, identification of symbols, determination of the most probable missing symbols, and reconstruction of the overall speech segment with the insertion of the symbol determined to be the best candidate for intelligible communication in the language of the sequence using confidence windowing and hidden semi-Markov modeling, among other techniques, as described in greater detail below.","Referring now to the drawings, in which like reference numerals are used to refer to the same or similar elements,  illustrate the general problem of reconstruction of a sequence of symbols in which a stream or sequence  of language symbols  transmitted from a source to a receptor are damaged in transit so that during a period \u0394t some symbols  in the sequence  are unknown or missing.","The symbol sequence  can also be viewed in terms of a signal  having an amplitude , for each symbol in the sequence which is either above or below a threshold value . When the amplitude  is above the threshold , then the symbol  is known and can be understood by the receiving party. But, when the amplitude is below the threshold , such as between times tand t, the symbol is unknown and must be reconstructed to be understood.","The period \u0394t, which is the time t\u2212t, when the unknown symbols are transmitted must be reconstructed using inter-symbol influences. The number of missing or unknown symbols is estimated based on the language of the communication and the size of \u0394t. The inter-symbol influences can be modeled statistically to predict which symbols of the language should be used to replace the missing symbols and complete the sequence .","The method herein provides a statistical approach to reconstructing the symbol stream 10 based on the known inter-symbol influences of the communication language. Statistical probabilities for the presence of the missing symbols in the stream are generated and the most likely candidates are selected to reconstruct the sequence . The candidates are considered in the context of the surrounding known symbols  using symmetric and asymmetric windowing around the unknown symbols ",{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 1C","b":["80","90","80","15","22","24","90","15","15","22","24"],"i":"a "},"A simplified form of the reconstruction process is illustrated in  in terms of speech reconstruction. First, a speech signal is input  into the system, followed by preprocessing and feature extraction  of the speech signal. A Markov-based model incorporating the features of the input speech signal is applied  to damaged speech symbols in the signal sequence as they are received. The Markov-based model approximates confidence windowing to determine the missing or damaged symbols . The speech signal is reconstructed  using the recovered symbols determined  by the Markov-based model. The reconstructed speech is then output  by the system.","Confidence windowing forms the basis for reconstruction involving statistical analysis of unknown symbols conditioned or jointly formed on known symbols. The unknown symbol u is a symbol that equips each state in an HMM, HSMM, and GHSMM. The probability of observing u in a state, b(u), is the same for every state.\n\n()=1()\u03b5\u2003\u2003(1)\n\nwhere N is the number of states in the Markov model. The matrix B also contains the observation probabilities for all symbols in the vocabulary being used. In equation (1), C=0.5 or C=1.0, depending on whether or not the unknown symbol is factored into the stochastic definition of B that requires the probability of each row Bto sum to one.\n","Confidence windowing represents the situation in which perfect information about all other states and durations is available. Thus, application of a confidence window to a sequence having missing symbols will result in the best reconstruction of the sequence. An exact model of a confidence window is a high order equation which is complex and time-consuming to solve. The Markov-based models discussed herein approximate confidence windows to different degrees using lower order equations.",{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIGS. 4A\u20134D","b":["200","210","220","200","10","15","15","210","220","10","200","210","220"],"i":"a "},"When a confidence window  is used to evaluate a symbol stream  to suggest replacements for missing symbols the observation pane  is positioned over an unknown symbol and the symbols in the surrounding transition panes  are used to suggest the correct replacement symbol . As might be expected, and can be shown by experimental data, the larger the confidence window  used to evaluate the stream , the greater the accuracy of the resulting reconstruction using suggested replacement symbols .","Confidence windowing used alone, however, considers only the probability of occurrence for unknown phonemes or symbols conditioned or jointly considered on all known phonemes or symbols to reconstruct a symbol stream. That is, individual acoustic vectors or duration are not considered by confidence windowing.","Hidden Markov models are the least accurate approximation of confidence windows, since they account for only the prior and post states surrounding a missing symbol to determine and reconstruct the missing symbol. Hidden semi-Markov models are more accurate because they include the state duration information, or inter-symbol influence, for the prior, post, and missing states. Generalized hidden semi-Markov models are the most accurate approximation, and are substantially equivalent, to confidence windows. A GHSMM considers all prior and post states in the symbol stream, as well as the duration in each state.","As a simplified example, in a sentence of several words having missing letters, an HMM considers only a missing letter and the surrounding two known letters in a given word to reconstruct the word. A HSMM still considers only the surrounding two letters in the work, but also includes the inter-symbol influence of the known letters on the missing letter. A GHSMM considers the entire sentence and the inter-symbol influence of each letter on the others.","As seen in , in accordance with the invention a GHSMM is alternatively applied  to damaged speech symbols. The GHSMM is an HSMM that also accounts for inter-symbol influences using a second time scale, and thereby has two regeneration times. The speech signal is reconstructed  based on the results of the HSMM application  or the GHSMM application , and then the reconstructed speech signal is output  for use. The application of the GHSMM  exclusively to the speech signal will provide the most accurate results, and significantly more accurate than application of the HSMM  alone.","The preprocessing and feature extraction  involves sampling, filtering noise from the input signal and windowing the signal. Windowing is a process of dividing the signal into discrete windows, so that the signal can be considered as discrete symbols, as discussed further below. Feature extraction can be done for a number of parameters, such as PARCOR coefficients, but LPC cepstrum are preferred.","In preprocessing and feature extraction , a corpus is used to produce language parameters to create the HSMM or GHSMM. The language parameters remain fixed during reconstruction. Preprocessing and feature extraction  is done for each reconstruction, and then matched by the GHSMM or HSMM to the language parameters taken from the corpus. The corpus contains the information for the A, B and D matrices, discussed further below, which are determined in the preprocessing step  of . The GHSMM in turn contains the A, B, D and \u03a0 matrices of a corresponding HSMM, which are generated from a corpus. However, the GHSMM will only revert to the HSMM in special circumstances.","It should be noted that application of the GHSMM has also been shown to be equivalent to confidence windowing with language parameters and approaches the same accuracy. Important language parameters for which values must be obtained are observation probabilities of symbols in the language, transition probabilities from one symbol to another, and the duration of each symbol.","Further, it should be apparent that the language parameters may be speaker-dependent in some instances, such as when a spoken language like English or Spanish or Chinese is being reconstructed. Each of these languages has dialects which differ from each other and are further subject to pronunciation differences between speakers who use the language. Thus, some basic information common to all speakers of a language can be obtained from a text of the language, but other information requires a sample of the speaker whose speech will be reconstructed. Obviously, in cases where the language sequence sources are uniform for a language, either a training text or a sample of one of the sources using the language can be used for feature extraction. This data is used to form the corpus.","An important feature of the invention, however, is that a GHSMM used to evaluate and reconstruct the damaged symbols in a stream for a given speaker or communicator can be generated from the symbol stream itself. That is, the speaker is characterized, the A, B, D and \u03a0 matrices are formed, and the GHSMM is applied . Once the language parameters for a given speaker, or group of speakers as the case may be, are generated, the GHSMM can be developed for the speaker. The hidden semi-Markov models used herein are discrete observation GHSMMs, in which the number of possible observation symbols is finite.",{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 3"},"A={a} is the state probability distribution matrix.","B={b(k)} is the observational probability distribution matrix.","N is the number of states in the model.","D={d(\u03c4), d(\u03c4), . . . , d(\u03c4)} is the state occupancy distribution vector wherein d(\u03c4) is the probability of staying the state i for \u03c4 time units.","V={V, V, . . . , V} is the set of each state's possible observation symbols.","M is the number of distinct observation symbols per state (size of vector quantization codebook), and wherein \u03a0={\u03a0} is the initial state distribution vector, so that the HSMM of  is described in notation as \u03bb={A, B, \u03a0, D}.","As shown in , the HSMM has five states , with vectors  indicating possible state changes. The observational probability matrix B associated with each state  is shown adjacent the state .","The general problem of reconstruction, as set forth above, on a symbol by symbol basis consists of reclaiming the missing symbols from a stream  of known symbols  using statistical techniques, such as an HSMM of . The HSMM needs to take into account inter-symbol influences between states . That is, there must be correlation between states  and the possible disparity of effect between the observation probabilities b(o) and state durations d(\u03c4) in probability evaluation. The effect of known symbols on unknown symbols must be accounted for in order to improve the accuracy of the system. Therefore, inter-symbol influences must be included in the product terms of the equations embodying the HSMM in order to account for known observations in reconstruction and also develop balancing relationships between correlated b(o) and d(\u03c4) so as to include observation duration for reconstruction of symbols in a sequence.","Referring again to , the application of a GHSMM  captures inter-symbol influences as can be shown by experimental results using the confidence windowing method, discussed in greater detail below. The GHSMM  operates on multiple time scales, such as the two scales for language symbols , and acoustic symbols ,  as illustrated in , to capture the inter-symbol influences. The inter-symbol influence is both a screening, or decision-making, of the data set (vocabulary, or symbols of the language) to include only those symbol stream segments in the data set that at least partially match the symbols  around a missing symbol That is, use of the GHSMM  permits exclusion from consideration of transitions to states  which are not possible based on the language parameters and surrounding known symbols  when applying the HSMM to reduce the number of possible states  in the HSMM equations.","The pre-processing  can be accomplished through the following steps. First, known symbols  in a stream  are grouped into a first group of states and unknown symbols are placed in a second group of states. In some cases, a non-commutative probability product operator can be placed in the HSMM B (observation probability) matrix to support time order in HSMM Viterbi Algorithm and evaluation products such as equation (1) described below. The operator is a function of time order in a confidence window.","A minor change is then applied to the Viterbi Algorithm to retrieve the missing symbols. The change is a simple maximization-based step, which will be appreciated by one versed in statistics. The reconstruction is then done by the resulting generalized model, which leverages terms of equation (1), to generate a probability of reconstruction P(R):",{"@attributes":{"id":"p-0088","num":"0087"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"R"}},{"mrow":[{"msub":{"mi":["d","k"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","k"]}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"\u03b8","mo":"=","mn":"1"},"msub":{"mi":["\u03c4","k"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"msub":{"mi":["b","k"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["o","\u03b8"]}}},{"msub":{"mi":["d","u"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["t","u"]}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"\u03d5","mo":"-","msub":{"mi":"\u03c4","mrow":{"mi":"k","mo":"+","mn":"1"}}},"mi":"T"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":["b","u"]},{"mi":["o","\u03d5"]}],"mo":"\u2062"}}],"mo":["\u2062","\u2062","\u2062"],"msub":{"mi":["a","ku"]}}}],"mo":"\u2062"}],"mo":"\u221d"}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{}},{"@attributes":{"id":"p-0089","num":"0088"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"msub":{"mi":["\u03b1","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}},{"mrow":{"munderover":{"mrow":[{"mo":"\u2211","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"\u03c4","mo":"=","mn":"1"}],"mi":"t"},"mo":"\u2062","mrow":{"munderover":{"mrow":{"mo":"\u2211","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"munder":{"mrow":[{"mi":"i","mo":"=","mn":"1"},{"mi":["i","j"],"mo":"\u2260"}]},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"msub":{"mi":"\u03b1","mrow":{"mi":["t","\u03c4"],"mo":"-"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}},{"msub":{"mi":["d","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}},{"munderover":{"mo":"\u220f","mrow":[{"mi":"\u03b8","mo":"=","mn":"0"},{"mi":"\u03c4","mo":"-","mn":"1"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["b","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"o","mrow":{"mi":["t","\u03b8"],"mo":"-"}}}}}],"mo":["\u2062","\u2062","\u2062"],"msub":{"mi":["a","ij"]}}}},"mo":"+"}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"msub":{"mi":["\u03a0","j"]},"mo":["\u2062","\u2062"],"mrow":[{"msub":{"mi":["d","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mo":"\u220f","mrow":{"msub":{"mi":["b","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"o","mrow":{"mi":["t","\u03b8"],"mo":"-"}}}}}]}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mrow":{"mrow":[{"mi":"when","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1"},{"mi":["N","and"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mn":"1"}],"mo":["\u2264","\u2264","\u2264","\u2264"],"mi":["j","t","T"]}}}}]},"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}},"mrow":{"mi":"and","mo":","}}},{"mrow":{"mo":["(",")"],"mn":"3"}}]},{"mtd":[{"mrow":{"mrow":[{"mrow":[{"msub":{"mi":["\u03b2","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}},{"munderover":{"mrow":[{"mo":"\u2211","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"\u03c4","mo":"=","mn":"1"},{"mi":["T","\u03c4"],"mo":"-"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munderover":{"mrow":{"mo":"\u2211","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"munder":{"mrow":[{"mi":"j","mo":"=","mn":"1"},{"mi":"j","mo":"\u2260","mn":"1"}]},"mi":"N"},"mo":"\u2062","mrow":{"msub":{"mi":["a","ij"]},"mo":["\u2062","\u2062","\u2062"],"mrow":[{"msub":{"mi":["d","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}},{"msub":{"mi":"\u03b2","mrow":{"mi":"\u03c4","mo":"+","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}},{"munderover":{"mo":"\u220f","mrow":{"mi":"\u03b8","mo":"=","mn":"0"},"mi":"\u03c4"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["b","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"o","mrow":{"mi":["t","\u03b8"],"mo":"+"}}}}}]}}}],"mo":"="},{"mrow":[{"mi":"when","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1"},{"mi":["N","and"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mn":"1"}],"mo":["\u2264","\u2264","\u2264","\u2264"],"mi":["j","t","T"]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":{}},{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}}]}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}]}}}},"In equation (2) the form for transitioning known to unknown symbols is apparent by the separation of the k (known) probability product term from the u (unknown) probability product term by the transition probability from known to unknown, a.","For a further understanding of the application of the pre-processing , GHSMM  and reconstruction\/post-processing  steps, the steps are illustrated using the equivalent confidence windows  shown in  and operations shown in . The example illustrated is applicable to a system having a small vocabulary with few symbols, each of which has as a different duration. As noted above, the HSMM and GHSMM models are solvable equivalents to the confidence windows.",{"@attributes":{"id":"p-0092","num":"0091"},"figref":"FIGS. 5A\u20135C","b":["200","200","210","220"]},"The symbol pre-processing step  using these confidence windows  is illustrated in . The pre-processing step  is done in preparation for a generalized HMM\/HSMM that transitions known symbols to unknown symbols. In each case shown in , the amount of known information is strictly greater than the unknown information\u2014there is only one observation pane  per window . Pre-processing  is done by applying simply circular rotations with the confidence windows  as seen in the drawings.","As can be seen, in , the observation pane  is rotated left from the left-most position to the right-most position, while in  the observation pane  rotates from the middle position to the left-most position. In , the positions of the transition panes  and observation pane  are reversed, and in , there is no change.","The post-processing reconstruction  is done by applying the opposite operations to those of . That is, a circular rotation right and reversal operations are applied.",{"@attributes":{"id":"p-0096","num":"0095"},"figref":"FIG. 7","b":["150","150","152","154","150","152","150","60"]},{"@attributes":{"id":"p-0097","num":"0096"},"figref":["FIG. 8","FIG. 8"],"b":["100","100","40","220","200"]},"The table below shows the results of evaluating the generalized HSMM of  and using confidence windowing to evaluate the same generalized HSMM. As can be seen, the two methods provide identical results for reconstructing the same vocabularies:",{"@attributes":{"id":"p-0099","num":"0098"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"77pt","align":"left"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},{},"Generalized HSMM","Confidence Window"]},{"entry":[{},"Word","Symbol, P(R)","Symbol P(R)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"?AC","B, P(R) = 1.0","B, P(R) = 1.0"]},{"entry":[{},"?BC","A, P(R) = 0.667","A, P(R) = 0.667"]},{"entry":[{},"?CC","A, P(R) = 1.0","A, P(R) = 1.0"]},{"entry":[{},"A?C","B, P(R) = 0.667","B, P(R) = 0.667"]},{"entry":[{},"B?C","A, P(R) = 0.5","A, P(R) = 0.5"]},{"entry":[{},"AB?","C, P(R) = 1.0","C, P(R) = 1.0"]},{"entry":[{},"AC?","C, P(R) = 1.0","C, P(R) = 1.0"]},{"entry":[{},"BA?","C, P(R) = 1.0","C, P(R) = 1.0"]},{"entry":[{},"BB?","C, P(R) = 1.0","C, P(R) = 1.0"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}}},"br":{}},"In order to achieve these results, it was discovered that two changes to the algorithms used to solve the HSMM are needed. The first change is the use of a non-commutative multiplication operator in the products of equation (2), above. A non-commutative multiplication operator is one in which A*B\u2260B*A. That is, the particular order in which the multiplications of the b(o) terms are performed in equation (2) is important to achieving accurate reconstruction.","One manner of accomplishing the non-commutative behavior is to stipulate the position (time-shift) location where a given probability will be non-zero. In other words, the probability must appear in a certain time slot. For example, in equation (2), \u03bband \u03bbmust appear in a certain time order, while \u03bb may appear in any order of any product in equation (2). Thus:\n\n\u03bb=\u03bb if [\u03b8\u22674, 1, 0]\u2003\u2003(5)\n\n\u03bb=\u03bb if [\u03b8\u22673, 1, 0]\u2003\u2003(6)\n","The terms \u03bband \u03bbare examples of the use of a non-commutative operator in the B (observation probability) matrix. This operator can also be applied to the transition probabilities aof the A (transition probability) matrix. The reconstruction matrix R shown below combines the B and A matrices:",{"@attributes":{"id":"p-0103","num":"0102"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"R","mo":"=","mrow":{"mo":["\uf603","\uf604"],"mtable":{"mtr":[{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1.0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"\u03bb"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0.67"},{"mn":"0.33"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"1.0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0.67"},{"mn":"0.33"}]},{"mtd":[{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0.5"},{"mn":"0.5"},{"mn":"0"}]},{"mtd":[{"msub":{"mi":["\u03bb","a"]}},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1.0"}]},{"mtd":[{"mi":"\u03bb"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1.0"}]},{"mtd":[{"mi":"\u03bb"},{"msub":{"mi":["\u03bb","b"]}},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1.0"}]},{"mtd":[{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1.0"}]},{"mtd":[{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"\u03bb"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]}]}}}}}},"The second change that is made is to apply an algorithmic addition to the Viterbi Algorithm, which is a procedure for finding the most likely state sequence in an HMM\/HSMM. A maximization step, performed across the first eight columns of the reconstruction matrix R is done so that the maximum \u03bb is selected that is not the symbol \u201c?\u201d (unknown). The first eight columns of the reconstruction matrix R are labeled as:\n\n\u03bd={\u201c\u2003\u2003(7)\n\nwhich is the codebook, or legal symbols, for the model of FIG . The character \u201cR\u201d represents the repeat character, which indicates that the preceding one of the other legal symbols is repeated. In a more compact form, the change to the Viterbi Algorithm can be expressed as:\n",{"@attributes":{"id":"p-0105","num":"0104"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["S","R"]},"mo":"=","mrow":{"munder":{"mi":["max","J"]},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mi":"B","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["i","T"],"mo":"*"},"mo":",","mi":"j"}}}}}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}},"br":{},"sub":["R","R ","T "]},"Using a GHSMM with these changes, or the equivalent confidence window technique, permits rapid analysis and reconstruction of a symbol stream with high accuracy. The GHSMM and confidence windowing incorporate observation probabilities and transition probabilities between symbols, as well as duration of symbols. These characteristics and others are modeled as language parameters that can be used to reconstruct a stream of symbols in the corresponding language, when the symbol stream includes damaged symbols.","The GHSMM however, has advantages over the HSMM that make the operation of GSHMM equivalent to confidence widowing. The HSMM is defined by the notation \u03bb={A, B, \u03a0, D}, which incorporates two major differences from the HMM. First, the diagonal elements aof the transition matrix A are all zero, indicating that states persist due to another factor. The matrix D, which contains the duration probabilities d(\u03c4) for each state, allows states to persist for multiple time steps by providing arbitrary state duration distributions. These durations are effectively enforced regeneration times for the HSMM chain. Using these distributions, transitions aare made only when a statistically appropriate time has elapsed for that state.","Because transition decisions are made only at the boundaries of phonemes, the symbol output during backtracking is simply the most likely symbol for a given state. Thus the unknown symbol of equation (1) or any spurious known symbols are all overwritten by the same symbol for the duration of the phoneme. The HSMM thus has a low pass effect on incorrect and unknown symbols in a given state, and has a regeneration time on the order of a phoneme. The HSMM Viterbi formulation is thus superior to the HMM at Time Scale :",{"@attributes":{"id":"p-0109","num":"0108"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"p","mn":"1"},"mo":"=","mrow":{"munder":{"mi":"max","mrow":{"mi":"\u03c4","mo":"\u2264","mrow":{"mi":"t","mo":"-","mn":"1"}}},"mo":"\u2062","mrow":{"munder":{"mi":"max","munder":{"mrow":[{"mn":"1","mo":["\u2264","\u2264"],"mi":["i","N"]},{"mi":["i","j"],"mo":"\u2260"}]}},"mo":"\u2062","mrow":{"msub":{"mi":["a","ij"]},"mo":["\u2062","\u2062","\u2062"],"mrow":[{"msub":{"mi":"\u03b4","mrow":{"mi":["t","\u03c4"],"mo":"-"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}},{"msub":{"mi":["d","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}},{"munderover":{"mo":"\u220f","mrow":[{"mi":"\u03b8","mo":"=","mn":"0"},{"mi":"\u03c4","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msub":{"mi":["b","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"o","mrow":{"mi":["t","\u03b8"],"mo":"-"}}}}}]}}}},{"msub":{"mi":"P","mn":"2"},"mo":"=","mrow":{"msub":{"mi":["\u03c0","j"]},"mo":["\u2062","\u2062"],"mrow":[{"msub":{"mi":["d","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}},{"munderover":{"mo":"\u220f","mrow":[{"mi":"\u03b8","mo":"=","mn":"0"},{"mi":"\u03c4","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msub":{"mi":["b","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"o","mrow":{"mi":["t","\u03b8"],"mo":"-"}}}}}]}},{"mrow":[{"msub":{"mi":["\u03b4","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}},{"mi":"max","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"p","mn":"1"},{"mi":"p","mn":"2"}],"mo":","}}}],"mo":"="},{"mrow":[{"msub":{"mi":["\u03a8","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mo":["{","}"],"mrow":{"mi":["j","t"],"mo":","}}},{"mrow":{"msub":[{"mi":"p","mn":"2"},{"mi":"p","mn":"1"}],"mo":"\u2265"}}]},{"mtd":[{"mrow":{"munder":{"mrow":[{"mrow":{"mi":["arg","max"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["i","\u03c4"],"mo":","}]},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["a","ij"]},"mo":["\u2062","\u2062","\u2062"],"mrow":[{"msub":{"mi":"\u03b4","mrow":{"mi":["t","\u03c4"],"mo":"-"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}},{"msub":{"mi":["d","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}},{"munderover":{"mo":"\u220f","mrow":[{"mi":"\u03b8","mo":"=","mn":"0"},{"mi":"\u03c4","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msub":{"mi":["b","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":"o","mrow":{"mi":["t","\u03b8"],"mo":"-"}}}}}]}}}},{"mi":"otherwise"}]}]}}],"mo":"="}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"1.7em","height":"1.7ex"}}},{"mtext":{}},{"mtext":{}},{"mtext":{}}]}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]}}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}}},"The HSMM still suffers from the effects of the two-sided Markov property in time scale . Because transition probabilities are conditioned only on the prior state, difficulties arise in the presence of large amounts of unknown symbols having statistics such as equation (1). An HSMM not trained for a specific symbol sequence loses important information in this case. If multiple successive phonemes are unknown, the HSMM must then rely only the underlying chain transition probabilities augmented with duration probabilities for making decisions during Viterbi decoding. As damage may occur to only beyond one phoneme or beyond phoneme boundaries, the first order conditioning present in the 2-sided Markov property may cause reconstruction errors.","The duration probability of equation (9) can help mitigate the effects of the two-sided Markov property somewhat, but only if damage is strictly confined to a phoneme. Additionally, if the durations of two phonemes are very similar or identical, duration information is then confounded; hence the use of the confidence window method as a basis for reconstruction. Because of these difficulties, an HSMM not trained for a specific symbol sequence will lose effectiveness at time scale  if a large amount of damage is present. In any event, the HSMM is greatly superior to the HMM at time scale .","The use of the GHSMM, however, overcomes the difficulties associated with the 2-sided Markov property and large amounts of damage by incorporating a non-stationary A matrix that does not require Baum-Welch EM training for a specific sequence. In this matrix, each element ais augmented with additional memory that is capable of extending the regeneration time of the chain to T the length of any given sequence:\n\n() }\n\n()(})\u2003\u2003(10)\n\nThe new transition probabilities aare thus a function of time and of the random variables Wcontained in set w.\n","The probability Pr(a|t) is obtained in a straightforward manner from the regeneration time of the entire utterance, T. For simplicity, this probability is modeled with a uniform distribution, which in discrete time is a function of discrete variable \u03b5.",{"@attributes":{"id":"p-0114","num":"0113"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["a","ij"]},"mo":"|","mi":"t"}}},{"mrow":[{"mfrac":{"mrow":[{"mrow":[{"mi":"u","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"msub":{"mi":"t","msub":{"mi":["a","ij"]}},"mo":"-","mi":"\u025b"}}},{"mi":"u","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"msub":{"mi":"t","msub":{"mi":["a","ij"]}},"mo":["+","+"],"mi":"\u025b","mn":"1"}}}],"mo":"-"},{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"t","msub":{"mi":["a","ij"]}},"mo":["+","+"],"mi":"\u025b","mn":"1"}},{"mo":["(",")"],"mrow":{"msub":{"mi":"t","msub":{"mi":["a","ij"]}},"mo":"-","mi":"\u025b"}}],"mo":"-"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"3.1em","height":"3.1ex"}}},"mn":"0"},{"mi":"int","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mi":"T","mn":"2"}}}],"mo":["\u2264","\u2264"],"mi":"\u025b"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}},"br":{},"sub":["aij ","ij "]},"The probability of configuration of state transitions \u03c9 as given by a Markov Random Field (MRF) (see Kemeny et al., Springer-Verlag, New York, 1976 and Meyn, S. P., and Tweedie, R. L., Springer-Verlag, London, Great Britain, 1993) is designated by the following equation:\n\n(})\u2003\u2003(12)\n\nThe set wcontains integers Z indicating the transition sequences S that the stationary transition ahas participated in. These transition sequences can be of any length, and the integer random variables Wrepresent the location of transition sequences S when arranged in a canonical (sorted) order:\n\n\n\n=arg() if occurs in \u2003\u2003(13)\n\nwhere S \u03b5 S, S\u2261 the set of possible transition sequences S={S, S, . . . , S} and n=number of transitions in a given transition sequence.\n","The key to the second, longer regeneration time of the GHSMM is the following. The random variable sets ware used in a Markov Random Field applied to the GHSMM Viterbi lattice after the first Viterbi pass. The first Viterbi pass produces an additional, generalized observation sequence O,={w, w, . . . , W} and O,={w, . . . , w, w} where n is the number of state transitions determined from the first Viterbi pass.","These observation sequences Orepresent memory over a regeneration time of T, the length of the observation O. Because the win Oare observations of entire transition sequences, they describe a longer regeneration time. Up until the point of generating the {0} (null) set, the time order intersections\n\n\u2003\u2003(14)\n\nyield the identities the transition sequences that could have produced O. A simple cellular automaton calculates a w*\n\n()\u2003\u2003(15)\n\nwhere g( . . . )can include an intersection, expectation, or make w* a small list of W\u03b5Z.\n","The result w* is then used in a MRF on the Viterbi GHSMM trellis. This MRF is interleaved directly into a second Viterbi pass. After the first Viterbi pass, the time variable t can be thought of as a generalized spatial variable since the length of observation sequence O is known and fixed. Because of this, the trellis dimensions are then fixed.","Additionally, the transition time probability equation (11) ensures that each state in the trellis can be considered distinct since its likelihood is dependent upon time t, which has been changed to the generalized spatial variable. Thus, we have a situation where equation (10) can be applied and the trellis is similar to lattice (see Smyth, Padhraic, Elsevier, Science B. V., Pattern Recognition Letters 18(11\u201313), November 1997, pp. 1261\u20131268).","With the GHSMM trellis viewed as a lattice, the probability of transition sequence \u03c9 on the lattice can be given by a MRF. This MRF calculates dislocation energy or potential energy of a transition sequence \u03c9 containing awhose ware some distance from w*:",{"@attributes":{"id":"p-0121","num":"0120"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03c9","mo":"=","mrow":{"mo":["{","}"],"mrow":{"msub":[{"mi":"s","mrow":{"msub":[{"mi":"i","mn":"1"},{"mi":"i","mn":"2"}],"mo":"\u2062"}},{"mi":["s","ij"]},{"mi":"s","mrow":{"msub":[{"mi":"i","mrow":{"mi":"n","mo":"-","mn":"1"}},{"mi":["i","n"]}],"mo":"\u2062"}}],"mo":[",","\u2062",",",",","\u2062",","],"mi":["\u2026","\u2026"],"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}]}}}}},"mo":"=","mfrac":{"msup":{"mi":"\u2147","mrow":{"mo":"-","mrow":{"munder":{"mo":"\u2211","mi":"T"},"mo":"\u2062","mrow":{"munder":{"mi":["min","k"]},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":"w","mo":"*"},"mo":"-","mrow":{"msub":{"mi":["w","ij"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}}}}}}},"mi":"Z"}}},{"mrow":{"mo":["(",")"],"mn":"15"}}]}}}},"br":{}},{"@attributes":{"id":"p-0122","num":"0121"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"Z","mo":"=","mrow":{"munder":{"mo":"\u2211","mi":"\u03c9"},"mo":"\u2062","mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c9"}}}}},{"mrow":{"mo":["(",")"],"mn":"16"}}]}}}},"br":{}},{"@attributes":{"id":"p-0123","num":"0122"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c9"}},"mo":["=","\u2062"],"mi":{},"mfrac":{"msup":{"mi":"\u2147","mrow":{"mo":"-","mrow":{"munder":{"mo":"\u2211","mi":"T"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":["min","k"]},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":"w","mo":"*"},"mo":"-","mrow":{"msub":{"mi":["w","ij"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}}}}}}},"mi":"Z"}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mfrac":[{"msup":{"mi":"\u2147","mrow":{"mo":["-","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":["min","k"]},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":"w","mo":"*"},"mo":"-","mrow":{"msub":{"mi":"w","mrow":{"msub":[{"mi":"s","mn":"1"},{"mi":"s","mn":"2"}],"mo":"\u2062"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}}}}}},"mroot":{"mi":["Z","T"]}},{"msup":{"mi":"\u2147","mrow":{"mo":["-","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":["min","k"]},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":"w","mo":"*"},"mo":"-","mrow":{"msub":{"mi":"w","mrow":{"msub":[{"mi":"s","mn":"2"},{"mi":"s","mn":"3"}],"mo":"\u2062"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}}}}}},"mroot":{"mi":["Z","T"]}}],"mo":["\u2062","\u2062"],"mi":"\u2026"}}}},{"mtd":{"mrow":{"mi":{},"mo":"\u2062","mfrac":{"msup":{"mi":"\u2147","mrow":{"mo":["-","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":["min","k"]},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":"w","mo":"*"},"mo":"-","mrow":{"msub":{"mi":"w","mrow":{"msub":[{"mi":"s","mrow":{"mi":"n","mo":"-","mn":"1"}},{"mi":["s","n"]}],"mo":"\u2062"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}}}}}},"mroot":{"mi":["Z","T"]}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"17"}}]}}}},"br":{},"sub":"ij "},{"@attributes":{"id":"p-0124","num":"0123"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03c9","mo":"=","mrow":{"mo":["{","}"],"mrow":{"msub":[{"mi":"s","mrow":{"msub":[{"mi":"i","mn":"1"},{"mi":"i","mn":"2"}],"mo":"\u2062"}},{"mi":["s","ij"]},{"mi":"s","mrow":{"msub":[{"mi":"i","mrow":{"mi":"n","mo":"-","mn":"1"}},{"mi":["i","n"]}],"mo":"\u2062"}}],"mo":[",","\u2062",",",",","\u2062",","],"mi":["\u2026","\u2026"],"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}]}}}}},"mo":"=","mfrac":{"msup":{"mi":"\u2147","mrow":{"mrow":[{"mo":"-","mrow":{"mi":"\u03be","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mfrac":{"mrow":{"mn":"2","mo":"\u2062","mi":"\u025b"},"mi":"T"}}}}},{"munder":{"mo":"\u2211","mi":"T"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":["min","k"]},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":"w","mo":"*"},"mo":"-","mrow":{"msub":{"mi":["w","ij"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}}}}}],"mo":"\u2062"}},"mi":"Z"}}},{"mrow":{"mo":["(",")"],"mn":"18"}}]}}}},"br":{}},"As the uncertainty in transition time e is increases, the regeneration time T becomes less significant, and the GHSMM reverts back to the HSMM when \u03b5=T\/2. With \u03b5<T\/2, the GHSMM performs its own EM at run time by establishing the MRF on the trellis after the first Viterbi pass and before the second. The observation sequence O produced by the second Viterbi pass is taken as the reconstructed utterance.","The effect of the non-stationary matrix \u00c0 is the non-stationary topology shown in . \u00c0 is of size O(NT) where Tis the longest utterance regeneration time. As O(T)=O(N=number of utterances), this memory is not excessive.  illustrates the operation of equation (10) or GHSMM in which the topology changes. The GHSMM begins with the fully connected states and possible transition vectors illustrated at the left. As the GHSMM executes and uses known information to eliminate and simplify the equations, the topology changes to the semi-Bakis form, in which only possible transition vectors between states remain.","In cases where w* is a small list, this topology results in an average of about 2.5 Viterbi passes being required for each reconstruction. In this case, the observation sequence O maximizing equation (9) is taken as the reconstructed utterance.","The following examples of the application of the invention to a damaged symbol stream are provided for further illustration of the invention.","Reconstruction using the model of  with GHSMM  was tested. Three types of reconstruction simulations using the GHSMM on a 200 word vocabulary were performed. Words in the vocabulary V were spoken by a synthesized voice.","First, words were divided into the confidence windows of , the appropriate phoneme damaged with noise, and the confidence window reconstructed. This was performed for increasingly populous subsets of V. Then, words themselves were damaged by noise placed at random locations for random durations. This was also performed for increasingly populous subsets V. Finally, all 200 words in V were randomly damaged and reconstructed while varying \u03b5 to examine the effect of uncertainty in transition and utterance regeneration times.","In all simulations, a reconstruction is considered correct if it contains all necessary phonemes in time order. If it does not, it is incorrect. The ratio of correct reconstructions to reconstruction attempts provides the overall reconstruction rate, P(R).",{"@attributes":{"id":"p-0132","num":"0131"},"figref":["FIGS. 10A\u2013D","FIG. 10A","FIG. 10B","FIG. 10C","FIG. 10D","FIG. 2B"]},"The reconstruction rates of  show the close approximation of the GHSMM to the confidence windows of , respectively. As clearly illustrated by , the GHSMM (curve ) is much superior in reconstruction rate P(R) and less susceptible to vocabulary size than either the HSMM (curve ) used alone or the HMM (curve ).  shows the reconstruction rate P(R) of the GSHMM , HSMM , and HMM  for a 3-pane confidence window  as shown in  versus increasing size subsets of vocabulary V.  shows the reconstruction rate P(R) of the GSHMM , HSMM , and HMM  for a 4-pane confidence window , with a transition pane to the left as shown in  versus increasing size subsets of vocabulary V.  shows the reconstruction rate P(R) of the GSHMM , HSMM , and HMM  for a 4-pane confidence window  with a transition pane to the right as shown in  versus increasing size subsets of vocabulary V.  shows the reconstruction rate P(R) of the GSHMM , HSMM , and HMM  for a 5-pane confidence window  as shown in  versus increasing size subsets of vocabulary V.  shows the reconstruction rate P(R) of the GSHMM , HSMM , and HMM  for a 6-pane confidence window  as shown in  versus increasing size subsets of vocabulary V.","The graphs of  illustrate how, except for the 4-pane, left transition window, application of the GHSMM to repair damaged communications made with any size vocabulary yields nearly the same result as the application of a confidence window. In the case of the 4-pane window noted, an increasing vocabulary size slowly decreases the effectiveness of the GHSMM. However, the effectiveness of HSMM and HMM model applications to the same vocabularies and damaged communication also fall off at about the same rate, so that the GHSMM is still significantly more effective overall.","As will be appreciated from the graphs of , the application of the GHSMM model to the damaged symbol stream significantly improves over the use of the HSMM model alone and far outpaces the use of hidden Markov models. And, as the size of the confidence window modeled by the GHSMM increases, the accuracy of the GHSMM nearly approximates the confidence window technique exactly, excepting the model of the 4-pane window of . Thus, the practical application of the GHSMM algorithm to the damaged symbol stream as an equivalent model of confidence windowing provides nearly the same accuracy, but in a small fraction of the time and effort that manual application of the confidence windows would require. Further, the GHSMM considers acoustic vectors, time orthography and spectral content of the speech signal, while confidence windowing does not consider any of these.","The GHSMM can be embodied on a computer, and processing damaged symbol streams by application of the algorithm does not test the limits of most computers used for this task. That is, the system of the invention created on a computer is neither memory nor processor intensive while achieving very high accuracy results. And, Baum-Welch Expectation Maximization type training is not needed to make the system work properly. Use of the GHSMM with its multiple time scale capability permits application of the system to any utterance length or complexity\u2014from acoustic symbols to language symbols of varying sizes.","While specific embodiments of the invention have been shown and described in detail to illustrate the application of the principles of the invention, it will be understood that the invention may be embodied otherwise without departing from such principles."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["In the drawings:",{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":["FIG. 1B","FIG. 1A"]},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 1C"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIGS. 4A\u20134E"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIGS. 5A\u20135C"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIGS. 6A\u20136D","FIGS. 5A\u20135C"]},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIGS. 10A\u201310D"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIGS. 11A\u201311E"}]},"DETDESC":[{},{}]}
