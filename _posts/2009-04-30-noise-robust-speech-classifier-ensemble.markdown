---
title: Noise robust speech classifier ensemble
abstract: Embodiments for implementing a speech recognition system that includes a speech classifier ensemble are disclosed. In accordance with one embodiment, the speech recognition system includes a classifier ensemble to convert feature vectors that represent a speech vector into log probability sets. The classifier ensemble includes a plurality of classifiers. The speech recognition system includes a decoder ensemble to transform the log probability sets into output symbol sequences. The speech recognition system further includes a query component to retrieve one or more speech utterances from a speech database using the output symbol sequences.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08412525&OS=08412525&RS=08412525
owner: Microsoft Corporation
number: 08412525
owner_city: Redmond
owner_country: US
publication_date: 20090430
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The current generation of speech recognition software generally requires a large amount of processing power and memory footprint. Such speech recognition software may be ill suited for implementation on small portable electronic devices with constrained memory and processing resources. Moreover, current speech recognition software may be susceptible to background noise and interference. Accordingly, the implementation of such speech recognition software on portable electronic devices may result in degradation of speech recognition accuracy, which leads to speech recognition errors and inefficiency. Furthermore, all currently known speech recognition systems are bounded in accuracy by the underlying technology.","This Summary is provided to introduce a selection of concepts in a simplified form that is further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","Described herein are embodiments of various technologies for speech recognition using a classifier ensemble for noise robust speech recognition on portable electronic devices. However, the ensemble may also be implemented on a device with more computing resources, such as a server. The various embodiments may include a voice activity detector (VAD) that detects speech input, a noise compensated feature pipeline that transforms the speech into feature vectors, and a speech classifier ensemble that converts the feature vectors into recognizable symbol sequences, so that speech recognition based on the symbol sequences may be performed. The various embodiments may be implemented on small portable electronic devices with constrained memory and processing capabilities, as well as other computing devices.","In at least one embodiment, a speech recognition system includes a classifier ensemble to convert feature vectors that represent a speech vector into log probability sets. The classifier ensemble includes a plurality of classifiers. The speech recognition system also includes a decoder ensemble to transform the log probability sets into output symbol sequences. The speech recognition system further includes a query component to retrieve one or more speech utterances from a speech database using the output symbol sequences.","Other embodiments will become more apparent from the following detailed description when taken in conjunction with the accompanying drawings.","This disclosure is directed to embodiments that enable the transformation of speech data in an input audio signal into a recognizable symbol sequence that may be further processed. For example, but not as a limitation, the processing may include storing the symbol sequence that represents the speech data in a memory. Alternatively, the symbol sequence that represents the speech data may be matched to a pre-stored symbol sequence. Accordingly, the embodiments may enable the retrieval of pre-stored speech data that match the input speech data. The transformation of the speech data into symbol sequences may include the use of a voice activity detector (VAD) that detects speech input, a noise compensated feature pipeline that transforms the speech into feature vectors, and a speech classifier ensemble that converts the feature vectors into recognizable symbol sequences. The embodiments described herein may enable the implementation of a noise-robust speech ensemble on various computing devices, including small portable electronic devices with constrained memory and processing capabilities. Various examples of noise robust speech recognition in accordance with the embodiments are described below with reference to .","Exemplary Scheme",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 1","b":["100","100","102","102","104","106","106"]},"The memory  may store program instructions. The program instructions, or modules, may include routines, programs, objects, components, and data structures that cause components of speech recognition-capable device  to perform particular tasks or implement particular abstract data types. The selected program instructions may include instructions for a voice activity detector (VAD) , a noise compensated feature pipeline , a classification component , an indexing component , a speech database , and a query component .","The voice activity detector  may receive an input audio signal  via an input device  (e.g., microphone). In various embodiments, the input audio signal  may include a speech utterance by a speaker . The voice activity detector  may isolate the voiced portion (e.g., speech portion) from the unvoiced portion of the input audio signal . Unvoiced portion of the input audio signal  may be easily corrupted by any kind noise (e.g., background noise, microphone artifacts, system interference, etc.). Accordingly, the isolation of the voiced portion from the unvoiced portion may improve retrieval accuracy under noisy conditions. Otherwise the system may try to match parts of the query to random symbols corresponding to the unvoiced parts.","The voice activity detector  may pass the voiced portion of the input audio signal  to the noise compensated feature pipeline . The noise compensated feature pipeline  may transform the voiced portion of the input audio signal  into feature vectors, i.e., a representative form of the voiced portion.","Subsequently, the noise compensated feature pipeline  may pass the feature vectors to the classification component . The classifier ensemble may translate the feature vectors into symbols that represent the original voiced portion. The symbols that represent the voiced portion may be indexed by the indexing component  and stored in the speech database  along with the original speech data (e.g., electronic recordings of speech utterances). The speech data may also be stored in a data storage that is separate from the speech database  provided that the indexes stored in the speech database  may be used to access the speech data. It will be appreciated that the speech data may be stored electronically in various audio file formats (e.g., WAV, AIFF, and the like). However, in other instances, symbol sequences corresponding to the speech data may be stored alongside of the speech data itself (instead of separately in an index). For example the symbol sequences may be stored as metadata in a WAV file, AIFF file, etc.","Alternatively, the symbols that represent the voiced portion may be compared by the query component  to one or more symbols that are already stored in the speech database . In such a scenario, the query component  may match the representative symbols to the pre-stored symbols. In this way, the exemplary computing environment  may enable the retrieval and manipulation of speech data that is represented by the matching symbols pre-stored in the speech database  (e.g., retrieval of the speech data, modification of the speech data, deletion of the speech data, organization of the speech data, presentation of the speech data, and\/or the like).","Input Sound Signal Frames",{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 2","FIG. 1"],"b":["200","202","202","120","108","110","202","200","200","200","200"]},"Following digitization, the frame capture component  may capture a frame  of the digitized input sound signal  such that the frame  includes a predetermined number of samples. In at least one embodiment, the frame  may include approximately  samples. Subsequently, the frame capture component  may capture a frame  of the digitized input sound signal  that includes the same number of samples as the frame . However, the frame  may be shifted forward in time from the frame  at a predetermined frame interval . In at least one embodiment, the frame interval may be approximately 80 pulse-code modulation (PCM) samples, which correspond to approximately 10 milliseconds (ms)or an operational frame rate of 100 fps. Further, the frame capture component  may capture a frame  of the digitized input sound signal  that has the same length as the frame  and is also shifted from frame  by a frame interval  that is identical to the frame interval  Accordingly, in this fashion, the frame capture component  may capture a plurality of shifted frames of the same duration from the digitized input sound signal . The frames may be further provided to the voice activity detector  and\/or the noise compensated feature pipeline .","It will be appreciated that while some digitalization frequency, sampling rate, frame length, and frame interval values have been described above in various embodiments, the feature pipeline may digitize the input sound source and capture frames using other such values in other embodiments.","Noise Compensated Feature Pipeline",{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 3","FIG. 1"],"b":["110","110"]},"In at least one embodiment, the noise compensated feature pipeline  may include an input\/output component , a pre-emphasis filter component , a windows function component , a Fast Fourier Transform (FFT) component , a power spectrum component , a Mel filter component , a noise suppression component , a logarithmic component , a Discrete Cosine Transform component , a matrix component , and a feature selection component . As described below with respect to , the various components may be used to implement a noise compensated feature pipeline.",{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 4","FIG. 4","FIG. 1"],"b":["400","110"]},"At block , the noise compensated feature pipeline  may receive one or more frames of a digitized speech via the input\/output module , such as the frames described in .","At block , the noise compensated feature pipeline  may use the pre-emphasis component  apply a pre-emphasis filter to the speech data in each frame of the digitized speech. In various embodiments, as described above, each of the one or more frames may include a predetermined number of PCM samples, and each frame may be shifted forward in time from a previous frame by a predetermined time interval. For example, each of the frames may include 256 PCM samples, which correspond to 32 milliseconds. Moreover, each of the fames may be shifted forwarded in time by an 80 PCM sample, or 10 milliseconds.","The pre-emphasis component  may be configured to remove the \u201ctilt\u201d towards the lower frequencies of each frame. Accordingly, the frequencies in each frame may be \u201cnormalized\u201d so that the high frequency bands in each frame may become more noise-robust. In some embodiments, the pre-emphasis filter may be implemented according to the following pseudo code:",{"@attributes":{"id":"p-0041","num":"0040"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"196pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"for (i = FrameLength \u2212 1; i > 0; ++i)"]},{"entry":[{},"\u2003\u2003Frame[i] = Frame[i] \u2212 PreEmphCoeff * Frame[i \u2212 1];"]},{"entry":[{},"Frame[0] = Frame[0] * (1 \u2212 PreEmphCoeff);"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}},"br":{}},{"@attributes":{"id":"p-0042","num":"0041"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"BlockContinuousPreEmphasis\u2003(int\u2003FrameLength,\u2003int\u2003Nstep,"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"float[ ] Frame, ref float xlast, float PreEmphCoeff)"]},{"entry":[{},"savelast = Frame[Nstep \u2212 1];"]},{"entry":[{},"int n;"]},{"entry":[{},"for (i = FrameLength \u2212 1; i > 0; \u2212\u2212i)"]},{"entry":[{},"\u2003\u2003Frame[i] \u2212= PreEmphCoeff * Frame[i \u2212 1];"]},{"entry":[{},"x[0] \u2212= PreEmphCoeff * xlast;"]},{"entry":[{},"xlast = savelast;"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"With respect to the above pseudo code, it will be appreciated that theoretically speaking, Frame[0] may be generally computed as Frame[0]=Frame[0]\u2212PreEmphCoeff*Frame[\u22121] where Frame[\u22121] is the last sample of the previous frame\u2014as in the block continuous version. However, in the non-block continuous version, a simple approximation may be used: Frame[\u22121]=Frame[0]. In at least one embodiment, the value of PreEmphCoeff may be set to approximately 0.97 (this value was determined empirically). However, it will be appreciated that the PreEmphCoeff may be set to other values, provided that that the values are sufficient to remove the \u201ctilt\u201d toward the lower frequencies in each frame.","At block , the noise compensated feature pipeline  may use the window function component  to apply a window function to the speech data in each of the frames. In various embodiments, the window function may be a Hamming window function, a Hanning window function, a Blackman window function, or the like. The window function may serve to control the frequency resolution of a FFT spectrum for each frame that is described below. For example, the feature pipeline component  may apply a Hamming window over the 256 PCM samples of a frame using the following pseudo code:",{"@attributes":{"id":"p-0045","num":"0044"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"for (i = 0; i < FrameLength; ++i)"},{"entry":"\u2003\u2003\u2003\u2003Frame[i] = Frame [i] * (0.54 \u2212 0.46 * Cos(2.0f * (float)PI * i \/"},{"entry":"\u2003\u2003(FrameLength \u2212 1))"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}},"br":{}},"At block , the noise compensated feature pipeline  may use the FFT component  to apply a FFT to each frame of speech data. The FFT may convert the time domain signals of the speech data into frequency domain signals. Since similar speech utterances have more variation in the time domain, but less variation in the frequency domain, the FFT conversion may enable phonemes or other transient characteristics of speech in similar speech utterances to be compared and matched. In various embodiments, a radix-4 Discrete Fourier Transform (DFT) algorithm may be applied to the frame of speech data. In at least one embodiment, the application of DFT to each frame may be represented by the following equation, where x(n) is the input signal, X(k) is the DFT, and N is the frame size:\n\n()=\u03a3()0\u22661 \u2003\u2003(1)\n","At block , the noise compensated feature pipeline  may use the power spectrum component  to compute the output power spectrum of the real and imaginary components of the DFT. In other words, the power spectrum component  may compute the squared magnitude of the DFT, as illustrated in the following equation:\n\n|()|()*() \u2003\u2003(2)\n\nWhere * indicates the complex conjugate. This power spectrum computation may reduce by half the number of PCM samples in each frame. For example, in the instance where a frame includes 256 PCM samples, the power spectrum computation may reduce the number of PCM samples into 128 samples.\n","At block , the noise compensated feature pipeline  may use the Mel-filter component  to perform Mel-filtering on the output power spectrum. In other words, the output power spectrum may be warped according to a Mel-frequency scale, or a similar scale, to model human frequency and pitch resolution capability. In various embodiments, the output power spectrum may be warped using a Mel-scaled triangular filter bank having a predetermined number of bands. Accordingly, the linear-to-Mel frequency transformation using the Mel-scaled triangular filter bank may represented by the following formula, where M is the number of bands in the filter bank, with m ranging from 0 to M\u22121, and f is in hertz (Hz):",{"@attributes":{"id":"p-0049","num":"0048"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"Mel","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"f"}},{"mn":"1127","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"ln","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","mfrac":{"mi":"f","mn":"700"}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}}},"Additionally, H(k) may represent the weight given to the kenergy spectrum bin contributing to the moutput band:",{"@attributes":{"id":"p-0051","num":"0050"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["H","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mn":"0","mo":","}},{"mrow":{"mi":"k","mo":["<",">"],"mrow":[{"mrow":{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"-","mn":"1"}}},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["or","k"]},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"+","mn":"1"}}}]}}]},{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mrow":{"mi":"k","mo":"-","mrow":{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"-","mn":"1"}}}},"mo":")"},{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"-","mn":"1"}}}],"mo":"-"}]},"mo":","}},{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"-","mn":"1"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}}],"mo":["\u2264","\u2264"],"mi":"k"}}]},{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":["(",")"],"mrow":{"mrow":{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"+","mn":"1"}}},"mo":"-","mi":"k"}},{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"+","mn":"1"}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}}],"mo":"-"}]},"mo":","}},{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"m","mo":"+","mn":"1"}}}],"mo":["\u2264","\u2264"],"mi":"k"}}]}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{},"sub":"m"},{"@attributes":{"id":"p-0052","num":"0051"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}},{"msup":{"mi":"Mel","mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":[{"mrow":[{"mi":"k","mo":"+","mn":"1"},{"mi":"K","mo":"+","mn":"1"}]},{"mi":"Fs","mn":"2"}],"mo":"\u00b7"}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}}},"As shown, a Meltransform is the inverse of Mel, and Fs represents the sampling frequency (e.g., 8000 Hz). Accordingly, H(k) may be normalized so that the sum of H(k) is equal to one for each k, as follows:",{"@attributes":{"id":"p-0054","num":"0053"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":["H","m","\u2032"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}},"mo":"=","mfrac":{"mrow":[{"msub":{"mi":["H","m"]},"mo":"\u2062","mi":"k"},{"msub":[{"mi":["\u03a3","k"]},{"mi":["H","m"]}],"mo":["\u2062","\u2062"],"mi":"k"}]}}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}},"br":{},"b":"110"},"At block , the noise compensated feature pipeline  may use the noise suppression component  to provide noise suppression to the Mel filter coefficients. In various embodiments, the noise suppression component  may apply a noise compensation algorithm to separate the speech portion and the noise portion of the speech data, as included in the Mel filter coefficients. The application of at least one embodiment of a noise compensation algorithm is illustrated in . It will be appreciated that in other embodiments, noise compensation algorithms other than the algorithm described in relation to , may be applied to provide noise suppression to the Mel filter coefficients.","At block , the noise compensated feature pipeline  may use the logarithmic component  to obtain logarithms of the noise-suppressed Mel filter coefficients. This logarithmic transformation of the noise-suppressed Mel filter coefficients may serve to account for human sensitivity and\/or perception to the amplitude of sound data in each frame.","At block , the noise compensated feature pipeline  may use the DCT component  to apply a first 1-dimensional (1-D) discrete cosine transform (DCT)(Type II) to the noise suppressed Mel filter coefficients. The purpose of this operation is to de-correlate across the frequencies, that is, along a frequency dimension, and pack the information from the Mel filter coefficients into a smaller set of coefficients. Accordingly, the operation may enhance space\/time efficiency by generating less data for subsequent processing. For example, the noise compensated feature pipeline  may generate 11 output coefficients from an input of 15 Mel filter coefficients. However, it will be appreciated that in other embodiments, other coefficient sets having different numbers of output coefficients may be generated as long as the number of output coefficients is less than the number of input Mel filter coefficients.","At block , the matrix component  may populate the sets of output coefficients of each frame in a set of frames into a matrix. In at least one embodiment, the matrix may include a frequency dimension and a time dimension. The set of frames may include a frame and a number of frames that immediately precedes the frame. For example, when the noise compensated feature pipeline  has generated 11 output coefficients for a frame, the matrix component  may use the 11 output coefficients of the frame, as well as the 11 output coefficients from each of the immediately preceding 8 frames to form an 11\u00d79 matrix, as shown in .",{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 5","b":["500","500","320","502","504","506","5","500","508","7","510","6"]},"Return to , at block , the noise compensated feature pipeline  may use the DCT component  to apply a second 1-D DCT (Type II) to the coefficients of the matrix obtained at block  to generate 2-D DCT coefficients. In various embodiments, the second 1-D DCT may be applied along the time direction to de-correlate each coefficient value of the matrix over the same number of frames as in block  (e.g., 9 frames).","At block , the noise compensated feature pipeline  may use a feature selection mask of the feature selection component  to extract the feature vector from the de-correlated matrix of the block . For example, the feature selection mask may be applied to a de-correlated 11\u00d79 matrix. In such an example, the feature selection mask may provide a selection of 30 coefficient outputs that may have the highest variance, as show below, where the mask may refer to the reference index of the 11\u00d79 matrix illustrated in :",{"@attributes":{"id":"p-0062","num":"0061"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"FeatureSelectionMask ="]},{"entry":[{},"{"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"147pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"1, 2, 3, 4, 5, 6, 7, 8, 9, 10,"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"154pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"11, 12, 13, 14, 15, 16, 17, 18, 19,"]},{"entry":[{},"22, 23, 24, 25, 26, 27,"]},{"entry":[{},"33, 34,"]},{"entry":[{},"44,"]},{"entry":[{},"55,"]},{"entry":[{},"66"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"};"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},"The feature selection mask described above was obtained based on the heuristic of maximum energy. In other words, the output coefficients of the feature selection mask were selected because they empirically carry the most energy. However, in other embodiments, feature selection masks with other heuristics (e.g., maximum entropy) may be used to select the feature vector for each frame, as long as the resultant feature vector is amplitude invariant.",{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 6","b":"314"},"In at least one embodiment, the noise suppression component  may include an input\/output component , a smoothing component , an energy threshold component , a frequency bin comparison component , and a gain component . As described below with respect to , the various components may be used to apply a noise-compensation algorithm to a plurality of Mel filter coefficients.",{"@attributes":{"id":"p-0066","num":"0065"},"figref":["FIG. 7","FIG. 4","FIG. 7"],"b":["700","700","414","400"]},"At block , the noise suppression component  may use an input\/output component  to receive a set of input Mel filter coefficients for each frame. For example, the noise suppression component  may receive a set of 15 Mel coefficients. The input Mel filter coefficients may define a Mel spectrum in a 2-dimensional space that includes a time domain and a frequency domain.","At block , the Mel spectrum defined by the input Mel filter coefficients may be smoothed by the smoothing component  of the noise suppression component . The smoothing of the Mel spectrum may suppress noise spikes that may bias the noise suppression component . In various embodiments, given that S(t, m) is the m-th Mel filter coefficient at time t, the smooth component  may first smooth the Mel spectrum defined by the Mel filter coefficients in the frequency direction, as follows:",{"@attributes":{"id":"p-0069","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":["S","\u2032"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","m"],"mo":","}}},{"mfrac":{"mn":"1","mrow":{"mrow":{"mn":"2","mo":"\u2062","msup":{"mi":["L","\u2032"]}},"mo":"+","mn":"1"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"k","mo":"=","mrow":{"mi":"k","mo":"-","msup":{"mi":["L","\u2032"]}}},{"mi":"k","mo":"+","msup":{"mi":["L","\u2032"]}}]},"mo":"\u2062","mrow":{"mi":"s","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","k"],"mo":","}}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":[{},{},{},{}],"in-line-formulae":[{},{},{},{}],"i":["S","t,m","S","t\u2212","m","S","t,m","S","t,m","S","t,m"],"sub":"log"},"At block , the noise suppression component  may use an energy threshold component  to derive a set of energy thresholds based on the log-smoothed spectrum of a plurality of frames. In various embodiments, the threshold component  may first compute a frame energy of each frame from its log-smoothed spectrum, as follows:\n\n()=\u03a3\u2033() \u2003\u2003(10)\n","Moreover, the threshold component  may track the highest and the lowest frame energies in a predetermined period of time t. In at least one embodiment, t may be set at 2 seconds, although a may be set to other values in other embodiments. Based on the ratio of the highest and the lowest frame energies in the predetermined period of time t, the threshold component  may derive a set of energy thresholds. As further described below, the set of energy thresholds may enable the noise suppression component  to derive a noise log-spectrum for each frame from the log smoothed spectrum of the frame.","At block , the log-smoothed spectrum may be divided into a plurality of frequency bins, or components, by which each bin may be represented by m. The division of the log-smoothed spectrum may be performed by the frequency bin comparison component . The portion of the log-smoothed spectrum included in each frequency bin may be compared to the set of energy thresholds. Based on the comparison, the noise log-spectrum may be unchanged, updated, or reset.","In various embodiments, the portions of the noise log-spectrum in each of the frequency bins may be reset within the log smoothed spectrum, S(t,m) m), if the energy of the current frame, E(t), is very low. For bins with energy levels in the medium range, the update of the noise log-spectrum in each bin may be made on a per-bin basis based on a simple hang-over scheme. For bins with high energy levels, the noise suppression component  may maintain the current noise log-spectrum portion.","Accordingly, the noise log-spectrum may be represented as a mean and a variance, N(t,m) and N(t,m), respectively. In the reset case, N(t,m) may be set to S(t,m) and the variance may be computed in the next frame by (S\u2033(t,m)\u2212N(t,m)). To update the mean\/variance of the noise log-spectrum, a smoothing filter as follows may be used:\n\n()=\u03b2(1,)+(1\u2212\u03b2)() \u2003\u2003(11)\n\n()=\u03b2(1,)+(1\u2212\u03b2)() \u2003\u2003(12)\n\nwhereby the parameters \u03b2and \u03b2are adaptive. Further, bumps in the mean\/variance may be suppressed across the frequency bins.\n","In other words, if a current log smoothed spectrum portion in a frequency bin is high enough from a corresponding noise log-spectrum portion, the noise suppression component  may decide that the log smoothed spectrum portion is indicative of speech. However, any portion of the log smoothed spectrum in a frequency bin that is under the noise log-spectrum may become a new candidate for the noise log-spectrum (the noise spectrum is always the lowest energy candidate).","At block , the noise suppression component  may use the gain component  to compute gain factors to be applied to the Mel filter coefficients. The gain factors may facilitate the suppression of the noise component of each frame, as represented by the noise log-spectrum, from the speech component of each frame, as represented by the log-smoothed spectrum. In various embodiments, the noise suppression component  may be applied on a frequency bin-by-frequency bin basis. The gain factor may be calculated based on the signal-to-noise ratio between the speech and the noise included in each frame.","Accordingly, the signal-to-noise ratio (SNR) at each frequency bin may be computed as:",{"@attributes":{"id":"p-0078","num":"0077"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":["S","N"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","m"],"mo":","}}}},"mo":"=","mfrac":{"mrow":[{"msup":{"mi":["s","\u2033"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","m"],"mo":"\u00b7"}}},{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["N","mean"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","m"],"mo":","}}}}}]}}},{"mrow":{"mo":["(",")"],"mn":"13"}}]}}}}},"Subsequently, the gain component  may convert the obtained SNRs to gain factors that may be smoothed and multiplied with the input Mel spectrum that includes the Mel filter coefficients. In various embodiments, the SNRs may be converted to gain factors as follows:",{"@attributes":{"id":"p-0080","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"G","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","m"],"mo":","}}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"msqrt":{"mrow":{"mn":"1","mo":"-","mfrac":{"mn":"1","mrow":{"mi":["S","N"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","m"],"mo":","}}}}}}},"mo":","}},{"mrow":{"mrow":{"mi":["S","N"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","m"],"mo":","}}}},"mo":">","mn":"0"}}]},{"mtd":[{"mrow":{"msub":{"mi":["G","min"]},"mo":","}},{"mi":"otherwise"}]}]}}],"mo":"=="}},{"mrow":{"mo":["(",")"],"mn":"14"}}]}}}}},"In other words, a low SNR indicates a higher need for noise suppression. Conversely, a high SNR indicates a lesser need for noise suppression. Moreover, the computed gain factors may be smooth with low-pass filtering, as follows:\n\n\u2032()=\u03b3\u2032(1\u22121,)+(1\u2212\u03b3)() \u2003\u2003(15)\n","At block , the smoothed gain factors may be applied to the input Mel spectrum that includes the Mel filter coefficients, as follows:\n\n()=\u2032()() \u2003\u2003(16)\n\nVoice Activity Detector\n",{"@attributes":{"id":"p-0083","num":"0082"},"figref":"FIG. 8","b":"108"},"In at least one embodiment, the voice activity detector  may include a transformation component , a log power capture component , a smoothing component , a noise floor component , a threshold component , a spectrum entropy , a maximum amplitude component , a buffer component , and a classifier component . As described below with respect to , the various components may be used to isolate the voiced portions (e.g., speech portions) from the unvoiced portions of input audio signal.",{"@attributes":{"id":"p-0085","num":"0084"},"figref":["FIG. 9","FIG. 9","FIG. 1"],"b":["900","108","108"]},"The role of voice activity detector  is to isolate the voiced portions (e.g., speech portions) from the unvoiced portions of input audio signal. Unvoiced portions of the input audio signal may easily corrupted by any kind noise (e.g., background noise, microphone artifacts, system interference, etc.). Accordingly, the isolation of the voiced portions from the unvoiced portions may improve retrieval accuracy under noisy conditions. Moreover, the isolation of the voiced portions may help to reduce the computational complexity and memory space need to process the speech in the input audio signal (e.g., speech pattern matching and\/or recognition). For example, it has been estimated that 20% of normal human speech is actually unvoiced (e.g., pauses between words, phrases, sentences, etc.). Thus, reducing such unvoiced portions may reduce the resources needed for speech processing.","In operation, voice activity detector  may use several factors to isolate the voiced portions from the unvoiced portions of the input audio signal. In various embodiments, the voice activity detector  may monitor the power of the input audio signal, as present in a frame of the input audio signal, and compare the power with one or more thresholds to determine if the frame contains a voiced portion or an unvoiced portion of the input audio signal.","In addition, the voice activity detector  may analyze the spectrum entropy of the input audio signal in each frame to detect voiced portions of the input audio signal. For example, voiced portions of the audio signal generally have high amounts of entropy, while noise portions of the audio signal generally have low amounts of entropy, or variability.","Furthermore, the voice activity detector  may also analyze the maximum amplitude of the input audio signal in each frame to improve voiced portion detection and isolation. In some embodiments, the maximum amplitude of the input audio signal are analyzed on a subframe basis, that is, by dividing each frame into a plurality of subframes and performing the maximum amplitude analysis on the subframes. In this way, maximum amplitudes may be computed multiple times per frame to prevent the VAD algorithm form misidentifying medium energy and\/or high-energy noise spikes as voiced portions of the input audio signal.","To increase the reliability of voiced portion identification, the VAD algorithm may further use delayed analysis. In delayed analysis, the VAD algorithm may perform its analysis on the audio data of a frame in conjunction with audio data in N future frames. In at least one embodiment, N may be equal to 16 future frames. In other words, for the current processed frame at time t, the VAD algorithm may perform its analysis for the frame at time (t\u2212N), i.e., the analysis is delayed by N frames.","Accordingly, process  may be initiated at block , at which the voice activity detector  may take an input audio signal, as captured in a plurality of frames, and convert the audio signal in each frame into a Fast Fourier Transform (FFT) power spectrum. This transformation may be accomplished by the transformation component  of the voice activity detector . In various embodiments, the voice activity detector  may perform the conversion of the audio signal in each frame in the same manner as described in blocks - of . In at least some embodiments, the transformation component  may include components that are substantially similar to the components - of the noise compensated feature pipeline . In alternative embodiments, transformation component  may share the components - with the noise compensated feature pipeline .","At block , the voice activity detector  may use the log power capture component  to compute a frame log power for each frame. In various embodiments, given a power spectrum |X(t,k)|, the frame log power may be computed at each of the t-th frames:\n\n()=log(\u03a3()|) \u2003\u2003(17)\n\nin which Iand Iare the respective indices of the start and end frequencies for the power computation.\n","At block , the voice activity detector  may use the smoothing component  to smooth the log power for each frame using the following low-pass filtering:\n\n()=\u03b1()+(1\u2212\u03b1)(1) \u2003\u2003(18)\n","At block , the voice activity detector  may use the noise floor component  to define a noise floor based on the lowest input power in a plurality of preceding seconds, corresponding to t\u2212N frames:\n\n()=min((),(1)) \u2003\u2003(19)\n","The noise floor may be updated with the last input power in the plurality of preceding intervals (e.g., seconds) that correspond to T frames:\n\n()=min((\u2032)) \u2003\u2003(20)\n","At block , the voice activity detector  may use the smoothing component  to smooth the noise floor using the following low-pass filter:\n\n()=\u03b1()+(1\u2212\u03b1)(1) \u2003\u2003(21)\n","At block , the voice activity detector  may use the threshold component  to derive two thresholds, which include an activity threshold, Ta(t), and a pause threshold, Tp(t), from the smoothed noise floor:\n\n()=(1)+(max((),)\u2212) \u2003\u2003(22)\n\n()=(1)+(max((),)\u2212) \u2003\u2003(23)\n\nin which Ga\/Gp represents the offsets to the smoothed noise floor.\n","At block , the voice activity detector  may use the spectrum entropy component  to determine an additive noise level to the power spectrum to calculate a spectrum entropy for each frame, as follows:\n\n()=2\u2003\u2003(24)\n\nin which Erepresents the boosting factor. The total power of the spectrum and noise may be represented as:\n\n()=\u03a3(|()|()) \u2003\u2003(25)\n","Accordingly, the spectrum entropy may be calculated as follows:",{"@attributes":{"id":"p-0100","num":"0099"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mn":"1","mo":"-","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","msub":{"mi":["F","L"]}},"msub":{"mi":["F","H"]}},"mo":"\u2062","mrow":{"mrow":[{"mi":"Ep","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}},{"msub":{"mi":"log","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"Ep","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}}}],"mo":"\u2062"}},{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["F","L"]},{"mi":["F","H"]}],"mo":"-"}}}]}}],"mo":"="},{"mrow":{"mi":"Ep","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}},"mo":"=","mfrac":{"mrow":[{"msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mi":"X","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","k"],"mo":","}}}},"mn":"2"},"mo":"+","mrow":{"mi":"En","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"mover":{"mi":["E","_"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}]}}],"mo":[",","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"26"}}]}}}},"br":{}},"At block , the voice activity detector  may use the maximum amplitude component  to compute the maximum amplitude on a subframe basis. The maximum amplitude may distinguish noise spikes from the onset of speech. The maximum amplitude of the subframes may be defined as:\n\n()=maxlog()|, 0 . . . 1 \u2003\u2003(27)\n\nin which M represents the number of the subframes per frame.\n","At block , the voice activity detector  may use the buffer component  to store the smoothed log power, the spectrum entropy, and the maximum amplitude in a buffer for delayed analysis, as follows:\n\n(),(1), . . . ,()]\u2003\u2003(28)\n\n(),(1), . . . ,()]\u2003\u2003(29)\n\n(0), . . . ,(1),(1,0) . . . ,(0), . . . ,(1),]\u2003\u2003(30)\n","In addition, the buffer component  may create another buffer for a power L(t) mapped from the smoothed log power:",{"@attributes":{"id":"p-0104","num":"0103"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["L","buf"]},"mo":"=","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","N"],"mo":"-"}}},{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","N"],"mo":["-","+"],"mn":"1"}}},{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":[",",",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}}},{"mi":["in","which"],"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mtext":":"}]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"31"}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mn":"2","mo":","}},{"mrow":{"mrow":[{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mover":{"mi":"P","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"mi":"Ta","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"\u2265"}}]},{"mtd":[{"mrow":{"mn":"1","mo":","}},{"mrow":{"mrow":[{"mi":["else","if"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"mover":{"mi":"P","mo":"~"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"mi":"Tp","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}],"mo":"\u2265"}}]},{"mtd":[{"mrow":{"mn":"0","mo":","}},{"mi":"otherwise"}]}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"32"}}]}]}}}},"At block , the voice activity detector  may use the classification component  to classify the input audio signal into voiced portions and unvoiced portions based on the smoothed log power, the spectrum entropy, the power mapped from the smooth logged power, and the maximum amplitude stored in the buffers using delayed analysis. In this way, the voice activity detector  may prepare the input audio signal for further processing by the noise compensated feature pipeline . In at least one embodiment, the VAD algorithm may perform the classification using the following pseudo code:",{"@attributes":{"id":"p-0106","num":"0105"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"State checkStateTransitionFromUnvoiced( )"},{"entry":"\u2003{"},{"entry":"\u2003\u2003state = Unvoiced"},{"entry":"\u2003\u2003count = # of entries in Lwhose value is zero"},{"entry":"\u2003\u2003\/\/ find the index where the next voiced would start"},{"entry":"\u2003\u2003for(int i=N; i>0; i\u2212\u2212)"},{"entry":"\u2003\u2003\u2003if(L[i]<2){"},{"entry":"\u2003\u2003\u2003\u2003indexNextActivityStart = i+1"},{"entry":"\u2003\u2003\u2003\u2003break"},{"entry":"\u2003\u2003\u2003}"},{"entry":"\u2003\u2003\/\/ find the index where the current voiced would end"},{"entry":"\u2003\u2003for(int i=1; i<=N; i++)"},{"entry":"\u2003\u2003\u2003if(L[i]==0){"},{"entry":"\u2003\u2003\u2003\u2003indexCurrActivityEnd = i\u22121"},{"entry":"\u2003\u2003\u2003\u2003break"},{"entry":"\u2003\u2003\u2003}"},{"entry":"\u2003\u2003if(L[0]==2){ \/\/ high power frame"},{"entry":"\u2003\u2003\u2003if(count==0) {"},{"entry":"\u2003\u2003\u2003\u2003state=Voiced \/\/ no low level frames observed"},{"entry":"\u2003\u2003\u2003} else if (indexCurrActivityEnd >= N\/4){"},{"entry":"\u2003\u2003\u2003\u2003\/\/ current voiced seems to continue reasonably long"},{"entry":"\u2003\u2003\u2003\u2003\/\/ find max and its position from near future"},{"entry":"\u2003\u2003\u2003\u2003sampleNearSampleMax = max of Afrom 0 to 4*M"},{"entry":"\u2003\u2003\u2003\u2003indexNearSampleMax = arg max of Afrom 0 to 4*M"},{"entry":"\u2003\u2003\u2003\u2003\/\/ find position amplitude drop toward future"},{"entry":"\u2003\u2003\u2003\u2003indexSampleDropFar = index where energy drop"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u201cA[i]<sampleNearMax\u22122.0\u201d happens from"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003indexNearSampleMax+1 to the end of"},{"entry":"buffer"},{"entry":"\u2003\u2003\u2003\u2003\/\/ find position amplitude drop toward past"},{"entry":"\u2003\u2003\u2003\u2003indexSampleDropNear = index where energy drop"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\u201cA[i]<sampleNearMax\u22122.0\u201d happens from"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003indexNearSampleMax\u22121 to the beginning"},{"entry":"\u2003\u2003\u2003\u2003if (indexSampleDropFar \u2212 indexNearSampleMax >= 5*M ||"},{"entry":"\u2003\u2003\u2003\u2003\u2002indexNearSampleMax \u2212 indexSampleDropNear >= 2*M){"},{"entry":"\u2003\u2003\u2003\u2003\u2003\/\/ it doesn't look like spiky noise"},{"entry":"\u2003\u2003\u2003\u2003\u2003State=Voiced"},{"entry":"\u2003\u2003\u2003\u2003} else if (indexSampleDropFar \u2212 indexNearSampleMax >="},{"entry":"3*M){"},{"entry":"\u2003\u2003\u2003\u2003\u2003\/\/ high power region seems reasonably long"},{"entry":"\u2003\u2003\u2003\u2003\u2003\/\/ find max smoothed power over next few frames"},{"entry":"\u2003\u2003\u2003\u2003\u2003powerSmoothedNearMax = max of P~from first"},{"entry":"4 frms"},{"entry":"\u2003\u2003\u2003\u2003\u2003\/\/ find max smoothed power\/entropy from far"},{"entry":"frames"},{"entry":"\u2003\u2003\u2003\u2003\u2003powerSmoothedFarMax = max of P~from last 2"},{"entry":"frms"},{"entry":"\u2003\u2003\u2003\u2003\u2003entropySpectrumFarMax = max of Efrom last 2"},{"entry":"frms"},{"entry":"\u2003\u2003\u2003\u2003\u2003if (powerSmoothedFaxMax >="},{"entry":"powerSmoothedNearMax\u22127 &&"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2002entropySpectrumFaxMax >= 0.1f){"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\/\/ power of far frame is not reduced much,"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003\/\/ and its entropy is relatively high"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003state=Voiced"},{"entry":"\u2003\u2003\u2003\u2003\u2003}"},{"entry":"\u2003\u2003\u2003\u2003}"},{"entry":"\u2003\u2003\u2003} else if (indexNextActivityStart <= N*3\/4) {"},{"entry":"\u2003\u2003\u2003\u2003\/\/ next voiced frame would start soon"},{"entry":"\u2003\u2003\u2003\u2003\/\/ compute average entropy from current active frames"},{"entry":"\u2003\u2003\u2003\u2003entropyCurrAvg = Average of Efrom 0 to"},{"entry":"indexCurrActiveEnd"},{"entry":"\u2003\u2003\u2003\u2003\/\/ compute average entropy from future active frames"},{"entry":"\u2003\u2003\u2003\u2003entropyNextAvg = Average of Efrom"},{"entry":"indexNextActivityStart"},{"entry":"\u2003\u2003\u2003\u2003\u2003\u2003to the end"},{"entry":"\u2003\u2003\u2003\u2003if (entropyCurrAvg >= 0.2f || entropyNextAvg >= 0.2f){"},{"entry":"\u2003\u2003\u2003\u2003\u2003\/\/ high entropy is observed"},{"entry":"\u2003\u2003\u2003\u2003\u2003state=Voiced"},{"entry":"\u2003\u2003\u2003\u2003}"},{"entry":"\u2003\u2003\u2003}"},{"entry":"\u2003\u2003} else if(L[0]==1){ \/\/ mid power"},{"entry":"\u2003\u2003\u2003if (count==0 && indexNextActivityStart <= N\/4){"},{"entry":"\u2003\u2003\u2003\u2003\/\/ no low power frame & next high power frame coming"},{"entry":"soon"},{"entry":"\u2003\u2003\u2003\u2003state=Voiced"},{"entry":"\u2003\u2003\u2003}"},{"entry":"\u2003\u2003}"},{"entry":"\u2003\u2003if(state==Voiced)"},{"entry":"\u2003\u2003\u2003countActivityTail=\u22121 \/\/ disable grace period"},{"entry":"\u2003\u2003return state;"},{"entry":"\u2003}"},{"entry":"\u2003State CheckStateTransitionFromVoiced( )"},{"entry":"\u2003{"},{"entry":"\u2003\u2003state = Voiced"},{"entry":"\u2003\u2003If(L[0]==0){ \/\/ low power"},{"entry":"\u2003\u2003\u2003if (countActivityTail == 0){ \/\/ grace period ends"},{"entry":"\u2003\u2003\u2003\u2003state = Unvoiced"},{"entry":"\u2003\u2003\u2003} else if(countActivityTail < 0){ \/\/ start grace period"},{"entry":"\u2003\u2003\u2003\u2003countActivityTail=N"},{"entry":"\u2003\u2003\u2003}"},{"entry":"\u2003\u2003\u2003countActivityTail\u2212\u2212"},{"entry":"\u2003\u2003}else{"},{"entry":"\u2003\u2003\u2003countActivityTail=\u22121 \/\/ disable grace period"},{"entry":"\u2003\u2003}"},{"entry":"\u2003\u2003return state;"},{"entry":"\u2003}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}},"br":{}},{"@attributes":{"id":"p-0107","num":"0106"},"figref":["FIG. 10","FIG. 1"],"b":["112","112","1002","1004","1006"]},"The classifier ensemble  may convert feature vectors into corresponding log probability vectors. For example, the noise suppression feature pipeline  () may generate a plurality of feature vectors  (e.g., feature vectors ()-() from a plurality corresponding frames .","As shown, the classification component  may apply a plurality of classifiers (e.g., classifiers A, B, and C) of a classifier ensemble  to each feature vector (e.g., feature vector ()). As further described below, the classifier ensemble  may include various types of classifiers, such as, but not limited to, Hidden Markov Model (HMM) classifiers, Gaussian Mixture Model (GMM) classifiers, Multi-level Preceptron (MLP) classifiers, or a combination of one or more different types of classifiers. In some embodiments, the classifier ensemble  may additionally include supervised classifiers and\/or unsupervised classifiers. In various embodiments, each classifier of the classifier ensemble  may transform the feature vector from each of the frames  into a plurality of log probability sets.","For example, the classification component  may apply the classifier ensemble  that includes the classifiers A, B, and C sequentially to the feature vector sets ()-(). Accordingly, classifier A may produce log probability sets A from the feature vector sets ()-(). Likewise, classifier B may produce log probability set B from the same feature vector sets ()-(), and classifier C may produce a set of log probability vectors C from the same feature vector sets ()-().","The decoder ensemble  may include decoders that correspond to the classifiers in the classifier ensemble . The decoders may transform the log probability sets that are outputted by the classifiers into corresponding sets of output symbol sequences, in which each output symbol sequence is a form of representation for the speech that is originally embodied in the frames . In various embodiments, each of the decoders may be a Viterbi decoder that takes in the log probability sets from each classifier to produce the output symbol sequence. For example, the decoder A may produce output symbol sequence A from the log probability set A. Likewise, the decoder B may produce output symbol sequence B from the log probability set B, and the decoder C may produce output symbol sequence C from the log probability set C.","In some embodiments, the classification component  may feed the output symbol sequences produced from the frames  (e.g., output symbol sequences A-C) to a block fusion component . At the block fusion component , the symbols produced by each of the decoders A, B, and C, may be combined together to form fusion symbol sequence . The fusion symbol sequence  may be indexed by the index component  shown in , and may then be stored in the speech database . Alternatively, as further described below, the query component  shown in  may compare the produced fusion symbol sequence  to one or more symbol sequences that are pre-stored in the speech database  to generate one or more possible matches.","In other embodiments, the classification component  may use the index component  to index the symbol sequences produced from the frames  (e.g., output symbol sequences A-C) for storage in the speech database . Alternatively, as further described below, the query component  shown in  may compare the produced symbol sequences to one or more symbol sequences that are pre-stored in the speech database  to generate one or more possible matches.","Classifier Ensemble",{"@attributes":{"id":"p-0114","num":"0113"},"figref":["FIG. 11A","FIG. 11A"],"b":["1002","1008","1002","1010","1","1102","1106","1102","1104","1106"]},"Accordingly, for each feature vector that correspond to a frame of a voiced portion, the classifier A may provide the probabilities that the utterance represented by the feature vector fall into each of the classes -. As shown in the example, the classifier A may classify the utterance represented by feature vector () as having a 95% probability of being in the \u201ca\u201d class, or class , a 4.5% probability of being in the \u201co\u201d class, or class , and 0.005% probability of being in the stop class, or class . In other words, the classifier A may \u201cthink\u201d that the utterance as represented by the feature vector (), that is, the sound uttered by a speaker and captured by the feature vector (), has a high probability (95%) of being an \u201ca\u201d class sound, a lesser probability (4.5%) of being an \u201co\u201d class sound, and very little probability (0.005%) of being a stop class sound.","It will be appreciated that while the feature vector classification illustrated above is described with respect to probabilities, the classifiers are actually implemented with log probabilities (i.e., the log of the probability values) instead of probabilities in various embodiments. In this way the decoders that correspond to the classifiers, as further described below, may then perform additions rather than multiplications on the log probabilities whilst computing the maximum likelihood symbol sequences.","Accordingly, the log probabilities of the utterance represented by the feature vector () falling into each of a plurality of classification classes (e.g., classes , , and ) may be referred to as a log probability set, (e.g., log probability set ).","Further in this example, having classified the feature vector () into the log probability set , the classifier A may move to a next feature vector (), at which the classifier A may classify the utterance of the speaker, as represented by a feature vector (), into log probability set . Subsequently, the classifier (A) may move to a next feature vector (), at which the utterance of the speaker as represented by the feature vector () may be classified into log probability set . In this way, the classifier A may form log probability sets, such as log probability sets A (), from the log probability set , the log probability set , and the log probability set .","In other words, each of the classifiers A-C in the classifier ensemble  may receive a feature vector of each frame and outputs a vector of log probabilities of each of its output classes. For example, given an input feature vector: P(Y|X) where X is the input feature vector and Y is the output class label, all of the classifiers A-C may generate P(Y|X) for all Y in its symbol alphabet at each frame. As further described below, the generated P(Y|X) may be eventually consumed by the decoders and decoded into maximum likelihood paths of state transitions.","It will be appreciated that  illustrates an exemplary embodiment where a particular exemplary classifier (e.g., classifier A) may classify feature vector (e.g., feature vector ()) into output classes -. However, each classifier of the classifier ensemble  () may classify each feature vector (e.g., feature vector ()) into any number of output classes. For example, classifier A () may classify a feature vector into 10 output classes, in which a log probability that the feature vector falls into each output class may be generated. In other examples, classifier A may classify a feature vector into 24 classes, or 39 classes, or any plurality of output classes.","Moreover, in some embodiments, each classifier in the classifier ensemble  may classify a feature vector (e.g., feature vector ()) into identical numbers of output classes as the other classifiers in the ensemble. For example, returning to , each of the classifiers A-C may classify feature vector () into 10 output classes. However, in other embodiments, at least one classifier in the classifier ensemble  may classify a feature vector into a different number of classes, as in one or more other classifiers in ensemble.","For example, but not as a limitation, the classifier A may classify the feature vector () into 10 classes, the classifier B may also classify the feature vector () into 10 classes, while the classifier C may classify the feature vector () into 24 classes. In another example, but not as a limitation, classifier A may classify the feature vector () into 10 classes, the classifier B may classify the feature vector () into 24 classes, while the classifier C may classify the feature vector () into 39 classes. Moreover, the output classes of each classifier may not have any relationship (semantic, spatial, etc.) to the output classes of the other classifiers in the classifier ensemble . Indeed, the classifier ensemble  may benefit most in terms of classification power when the output classes of the different classifiers are well separated, or orthogonal.","Additionally, as described above, while the classifiers in the classifier ensemble  may be of the same type in some embodiments (e.g., all GMM classifiers), the classifier ensemble  may include at least one classifier that is of a different type than one or more other classifiers. For example, but not as a limitation, the classifier A and the classifier B may be GMM classifiers, while the classifier C may a HMM classifier. In another example, but not as a limitation, the classifier A may be a GMM classifier, the classifier B may be a HMM classifier, while the classifier C may be a MLP classifier.","It will be appreciated that in the various embodiments described above, the classifiers in the classifier ensemble  do not engage in cross talk, that is, each classifier may work independently to classify a set of feature vectors without input from the other classifiers.","Thus, a classifier ensemble that includes classifiers that classify feature vectors into different number of classes, at least two classifiers of different types, and\/or both supervised and unsupervised classifiers may enable different classifiers to contribute different aspects to the conversion of speech into symbol sequences. Accordingly, such a classifier ensemble may provide greater robustness to noise and better ability to recognize and utilize idiosyncratic speaking patterns (intra-speaker variations) to improve retrieval accuracy, as well as provide more symbols for each voiced portion to be classified.","Supervised GMM Training","As shown in , the classifier ensemble  may include supervised GMM classifiers. The supervised GMM classifiers may produce log probability sets, such as log probability sets A, based on phonemes or unsupervised classes present in a feature vector, such as feature vector ().","In various embodiments, the supervised GMM classifiers may be trained based on phoneme labeled speech training data, such as a TIMIT corpus to produce supervised classifiers, or unlabeled training data to produce unsupervised classifiers. The TIMIT corpus is a corpus of phonemically and lexically transcribed speech of American English speakers of both sexes and different dialects. The TIMIT corpus was commissioned by the Texas Instruments (TI) Corporation of Dallas, Tex. and the Massachusetts Institute of Technology (MIT) located in Cambridge, Mass., thereby providing the name TIMIT.","Accordingly, in one example, a GMM classifier may include 10 folded phoneme classes. These phoneme classes are formed by folding together groups of related and confusable phonemes, such as nasals (e.g., \u201cm\u201d, \u201cn\u201d, \u201cng\u201d), closures, stops, vowel groups, etc.","In various embodiments, a discriminant analysis algorithm (e.g. LDA) may be implemented to arrive at these \u201cfolds\u201d. Phonemes may be grouped into different classes or \u201cfolded\u201d, until the intra-class spread is minimized and the inter-class distance is maximized. For example, as described above, the various \u201ca\u201d-based phonemes, such as \u201caa\u201d, \u201cae\u201d, \u201cah\u201d, \u201cah\u201d, \u201cao\u201d, \u201caw\u201d, \u201cax\u201d, \u201cay\u201d, may be grouped into a single \u201ca\u201d class. Likewise, the \u201co\u201d-based phonemes, such as \u201cow\u201d and \u201coy\u201d, may be grouped into a single \u201co\u201d class. Accordingly, the formation of phoneme classes through folding may minimize the length of the output symbol sequences that the index component index  needs to store and retrieve, as well as improve robustness to slight variations in speaking style, intonation, etc. In at least one embodiment, a Fisher discriminant algorithm may be implemented for the analysis.","When the discriminant analysis algorithm has performed the folding of the phoneme classes, a Hidden Markov Model Toolkit (HTK), as developed by the Microsoft Corporation of Redmond, Wash., may be implemented to train class Hidden Markov Models (HMMs). Each HMM may consist of a plurality of states (e.g., 3 states) with state transition probabilities, as well as GMMs of a feature vector. First, each HMM may be trained independently for each class. Subsequently, independent training may be followed by embedded training where the continuity between other classes is learned from training spoken utterances. In this way, the GMM at the center state (e.g., a second state for a 3-state case) of the HMM may be extracted as the GMM data for a GMM classifier.","Unsupervised GMM Training","As further shown in , the classifier ensemble  may include unsupervised GMM classifiers. Like the supervised GMM classifiers, the unsupervised GMM classifiers may produce log probability sets, such as log probability sets A, based on phonemes present in a feature vector, such as the feature vector ().","In various embodiments, the unsupervised GMM classifiers may be trained by the application of an expectation-maximization (EM) algorithm and an agglomerative clustering strategy to estimate the number of clusters that best represent the data. First, a GMM model may be extracted from data vectors using the EM algorithm. The extraction may output a GMM file with a single class, and N Gaussians in the single class mixture. The GMM file may be obtained in a bottom-up merging fashion by minimizing the minimum description length (MDL) cost function.","The clustering algorithm may be started by initializing with a set of cluster parameters and a user selected number of clusters. Further, the cluster means may be generated by selecting appropriate number of samples from the training data, and all the cluster covariances may be set to be equal to the covariance of the complete data set. After this initialization, the algorithm may enter into a loop in which clusters are combined (or eliminated when empty) until only one cluster remains.","The number of subclasses may be estimated using Rissanen's minimum description length (MDL) criterion. This criterion attempts to determine the number of subclasses which best describes the training data. The approximate maximum likelihood estimates of the mean and covariance of the subclasses are computed using the expectation maximization (EM) algorithm. The output class sizes for the unsupervised classifiers may be based on the local mimina of the derivative of the MDL cost function.","Next, the sub-class Gaussians may be divided into separate classes to produce N classes, each initially with a single Gaussian. Due to the tension inherent in MDL, which favors merging clusters on the one hand (this minimizes the bits needed to describe the model) and minimizing Euclidean distance of a point in feature class to the attracting class on the other hand, optimality may be achieved in a single criterion.","Once the initial unsupervised classifier is trained, the trained unsupervised classifier may be used to partition the training set into N subsets for the N classes. Subsequently, the other target configurations of N-class GMM classifiers may be re-trained on this partitioned training data, e.g., N classes with 1, 2, . . . , Gaussians per class.","The final choice of the classifier may be experimentally determined with real world test data. Additionally, the classifiers that work best in a classifier ensemble may be classifiers that compensate or enhance the weaknesses or strengths of other supervised and unsupervised classifiers in the ensemble.","Optimized Diagonal GMM Classification Algorithm","In some embodiments, the classifiers in a classifier ensemble, such as classifier ensemble , may be optimized to run on portable computing devices. Such portable computing devices may include, but are not limited to, a smart phone, a personal digital assistant (PDA), a digital camera, and the like. Typically, these devices may be constrained in terms of both memory and CPU power. Thus, it may be advantageous to optimize a diagonal GMM classification algorithm that is configured to run on such devices as a part of a classifier ensemble.","For example, a classic diagonal GMM classifier implementation, shown in pseudo-code, may be as follows:",{"@attributes":{"id":"p-0140","num":"0139"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"Class_LogLikelihood_Vector"},{"entry":"ClassifyFeatureVectorToOutputClasses(In_Feature_Vector fv)"},{"entry":"{"},{"entry":"\u2003\u2003Class_LogLikelihood_Vector cv; \/\/ return from the classifier"},{"entry":"\u2003\u2003\/\/ Compute the log likelihood for each class (i.e. GMM)"},{"entry":"\u2003\u2003for each Output Class C in the classifier do"},{"entry":"\u2003\u2003{"},{"entry":"\u2003\u2003\u2003\/\/ Compute log likelihood for each subclass (i.e. individual"},{"entry":"Gaussian model)"},{"entry":"\u2003\u2003\u2003for each SubClass SubC do"},{"entry":"\u2003\u2003\u2003\u2003SubC.LogLikelihood = SubC.GConst + \u00bd (fv \u2212 SubC.Mean)."},{"entry":"SubC.Diagonal_Inverse_Covariance_Matrix; \/\/ this is a dot product,"},{"entry":"result is scalar"},{"entry":"\u2003\u2003\u2002Max Sub LogLikelihood = Max over all (SubC.LogLikelihoods);"},{"entry":"\u2003\u2003\u2003\/\/ Sum the weighted subclass log likelihoods"},{"entry":"\u2003\u2003\u2003for each SubClass SubC do"},{"entry":"\u2003\u2003\u2003\u2003Sum += Exp(SubC.LogLikelihood \u2212 Max Sub LogLikelihood) *"},{"entry":"SubC.GaussianWeight;"},{"entry":"\u2003\u2003\u2003cv[index of class C] = Log(Sum) + Max Sub LogLikelihood;"},{"entry":"\u2003\u2003}done"},{"entry":"\u2003\u2003return cv;"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"In various embodiment, the pseudo-code may be transformed as shown below:",{"@attributes":{"id":"p-0142","num":"0141"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"Class_LogLikelihood_Vector"},{"entry":"OptimizedClassifyFeatureVectorToOutputClasses(In_Feature_Vector fv)"},{"entry":"{"},{"entry":"\u2003\u2003Class_LogLikelihood_Vector cv; \/\/return from the classifier"},{"entry":"\u2003\u2003\/\/ Compute the log likelihood for each class (i.e. GMM)"},{"entry":"\u2003\u2003for each Output Class C in the classifier do"},{"entry":"\u2003\u2003{"},{"entry":"\u2003\u2003\u2003\/\/ Compute log likelihood for each subclass (i.e. individual"},{"entry":"Gaussian model)"},{"entry":"\u2003\u2003\u2003for each SubClass SubC do"},{"entry":"\u2003\u2003\u2003\u2003SubC.LogLikelihood = SubC.GConst \u2212 (fv . SubC.Theta \u2212"},{"entry":"\u2003\u2003\u2003\u2003SubC.Delta);"},{"entry":"\/\/ OPTIMIZATION 1"},{"entry":"\u2003\u2003\u2002Max Sub LogLikelihood = Max over all (SubC.LogLikelihoods);"},{"entry":"\u2003\u2003\u2003\/\/ Sum the weighted subclass log likelihoods"},{"entry":"\u2003\u2003\u2003for each SubClass SubC do"},{"entry":"\u2003\u2003\u2003\u2003Sum += Exp(SubC.LogLikelihood \u2212 Max Sub LogLikelihood +"},{"entry":"SubC.LogGaussianWeight); \/\/ OPTIMIZATION 2"},{"entry":"\u2003\u2003\u2003cv[index of class C] = Log(Sum) + Max Sub LogLikelihood;"},{"entry":"\u2003\u2003}done"},{"entry":"\u2003\u2003return cv;"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"As shown, \u201cOPTIMIZATION 1\u201d of the transformed pseudo-code may speed up the diagonal GMM classification algorithm. \u201cOPTIMIZATION 1\u201d may be used to pre-compute the vectors Theta and Delta for each sub-class SubC (i.e., Gaussian of the class mixture) in the following manner, thereby effectively folding in two processing intensive multiply operations into the inner loop:",{"@attributes":{"id":"p-0144","num":"0143"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"SubC.Theta = SquareRoot(0.5 *"]},{"entry":[{},"SubC.Diagonal_Inverse_Covariance_Matrix);"]},{"entry":[{},"\u2003\u2003\u2003SubC.Delta = SubC.Theta * SubC.Mean;"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Furthermore, OPTIMIZATION 2 may pre-compute the logs of the sub-class in order to save one multiply operation in the inner loop.","Decoder Ensemble","As shown in , the output log probability sets of the each classifier (e.g., classifier ) in a classifier ensemble  may be processed by a corresponding decoder (e.g., decoder A) of a decoder ensemble . Such processing may produce a final output symbol sequence (e.g., output symbol sequence A). In various embodiments, the decoder that corresponds to each classifier may be a Viterbi decoder.","In other words, given the output of N classifiers by the classifier ensemble, a Viterbi decoder (without pruning) may be used to process the output of each classifier to produce the final output symbol sequence for that classifier. Moreover, the N decoders may be executed in parallel as independent Viterbi instances without any cross-talk among the N decoders. The N decoders in the decoder ensemble may produce N independent symbol sequences.","In various embodiments, the underlying state class of each classifier is assumed to be a fully connected graph in which any output symbol is allowed to transition into any other one. Using log likelihoods, the recursive formulation for each decoder (e.g., decoder A) may be as shown below, in which \u03c8represents the max log probability of the MLP output vector Oand being in phoneme j at time t:\n\n\u03c8=max{\u03c8(1)+log()}+log(())+\u2003\u2003(33)\n","In some embodiments, the language model likelihoods may be replaced by a single symbol insertion penalty (SIP in (33) above, which may be empirically optimized), as the language model likelihoods may be optimized for each classifier in an ad-hoc manner. However, in other embodiments, a symbol language model may be incorporated for each classifier by estimating the transition probabilities from the training data for those classifiers (e.g., by using na\u00efve Bayes and counting at each frame). Thus, the transition probability matrix may be used instead of the single ad-hoc SIP values. The simpler SIP formulation may be mainly used as a space\/time optimization, but also this simplification serves us well when the number of output classes is small enough such that the transition probabilities are nearly uniform.","While training the classifier ensemble in some embodiments, a \u201cgarbage state\u201d may be optionally defined for some classifiers, which symbolizes speech, i.e., priors that do not fall into any of the symbol categories for that classifier. Additionally, a special \u201cwildcard\u201d category may be defined for the posteriors (i.e., GMM classifiers' output log probabilities), to capture a situation in which there is no clear winning symbol probability.","Accordingly, in at least one embodiment, posterior wild cards may be implemented for the decoders. In various embodiments, a decoder may take the M symbol probabilities for the iclassifier, take the mean and max of M symbol probabilities, and then insert a M+1wildcard probability (and re-normalize), which may be calculated as follows:\n\n(posterior Wild Card)=(1.0\u2212(MaxProb\u2212MeanProb))*(1.0\u2212MaxProb)\/Wildcard Strength \u2003\u2003(34)\n","The Viterbi algorithm of each decoder may then decode log probability sets outputted by a corresponding classifier using this expanded vector of symbol probabilities. The posterior wildcard and the prior wildcard, i.e., the garbage state, may be considered to be in the same cluster for convenience, and signify speech for which there is no clearly discernable class label, e.g., mumbling. Accordingly, in some embodiments, the index component  may index the combined wildcard cluster along with all the other symbol sequences.",{"@attributes":{"id":"p-0153","num":"0152"},"figref":"FIG. 11B","b":["1004","1002","1102","1106","1002","1010","1","1102","1104","1106","1002","1010","1","1010","1"]},"Further in this example, having classified feature vector () into the log probability set , the classifier A may move to a next feature vector (), where the utterance of the speaker as represented by the feature vector () may be classified into a log probability set . Subsequently, the classifier (A) may to a next feature vector (), where the utterance of the speaker as represented by the feature vector () may be classified into a log probability set . In this way, the classifier A may form log probability sets, such as log probability sets A (), from the log probability set , the log probability set , and the log probability set .","The exemplary decoder  may use the Viterbi algorithm described above to convert the log probability set , the log probability set , and the log probability set , as produced by the classifier, into an output symbol sequence. In the example shown, the decoder  may use the Viterbi algorithm to analyze all of the possible \u201cpaths\u201d that lead from each class in probability set , through each output class in the probability set , to each class in the probability set . For the purpose of illustration, paths - are shown, although it will be appreciated that many more paths are actually present. By using the Viterbi algorithm described above, the exemplary decoder  may determine that path  is the most likely path, as the log probabilities traversed by the path  (illustrated as normal probabilities 95%, 75%, 95% for clarity, respectively) show the least amount of variability when compared to 95%, 24.5%, 95% of path , and 95%, 0.005%, 95% of path . In other words, the maximum likelihood path computed via equation (33) above gets maximized whilst traversing path .","Moreover, for the purpose of illustration, given that the \u201ca\u201d output class may be represented by a symbol \u201cA\u201d, the \u201co\u201d class may be represented by a symbol \u201cB\u201d, and the stop class may be represented by a symbol \u201cC\u201d, the exemplary decoder  may derive an output symbol sequence \u201cA, A, A\u201d from the frames . In other words, the exemplary decoder  may transform the speech utterance encapsulated in the frames  into a representative output symbol sequence \u201cA, A, A\u201d. However, in some actual embodiments of implementation, the exemplary decoder  may for purposes of indexing, simply index a symbol: \u201cA\u201d and add side information that its duration was 3 frames, instead of indexing 3 \u201cA\u201d symbols, each of duration 1 frame, such implementation leads to a more compact representation.","Block Fusion Component","As shown in , the classifier ensemble  may include various classifiers. These classifiers may be of heterogeneous types (e.g., MLPs combined with GMMS, which are also combined with HMMs). Moreover, the classifiers may be trained in various styles (e.g., supervised, unsupervised). Further, the classifiers may have different number of output classes. For example, but not as a limitation, the classifier ensemble  may include a classifier that outputs into 10 classes, a classifier that outputs into 24 classes, and a classifier that outputs into 39 classes. Additionally, the classifiers may generate output symbol sequences that include symbols of different durations.","As shown in , the block fusion component  may use block fusion to combine the output symbol sequences from these heterogeneous classifiers into a single fusion symbol sequence, such as fusion symbol sequence . In contrast, conventional symbol-to-symbol fusion approaches to combining outputs of a plurality of classifiers require that the plurality of classifiers to be homogenous. In other words, the plurality of the classifiers must have the same number of output classes, the same output alphabet (i.e., use the same symbols), and that the symbols of the output symbol sequences must have the same duration.","The block fusion component  may use a block-based fused approach to combine a plurality of output symbol sequences into a fusion symbol sequence. The operation of block fusion component may be further described in relation to .",{"@attributes":{"id":"p-0160","num":"0159"},"figref":"FIG. 12","b":["1006","1202","1204","1006","1006","1002","1004"],"sub":["1","2","3","m","1","2, S","3","n","1","2","3","m ","1","2","3","m "]},"Thus, given that P(K), P(K), P(K), and P(K) are the initial apriori probabilities of different message blocks M, the block-based fused approach may be described as below:",{"@attributes":{"id":"p-0162","num":"0161"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"1. Set the apriori probability vector p_0 to the set of initial apriori"},{"entry":"probabilities."},{"entry":"2. Perform the following iteration:"},{"entry":"\u2003\u2003j = 0 \/\/ iteration count"},{"entry":"\u2003\u2003For classifier k = 1:N {"},{"entry":"\u2003\u2003\u2003Calculate aposteriori probability vector q_j from observation S_k"},{"entry":"\u2003\u2003\u2003and apriori vector p_j"},{"entry":"\u2003\u2003\u2003\u2002Set apriori vector p_(j+1) = q_j"},{"entry":"\u2003\u2003\u2003\u2002j = j+1"},{"entry":"\u2003\u2003if(convergecriteria is not met)"},{"entry":"\u2003\u2003\u2003repeat loop"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}},"br":{},"sub":["1","2","3","m","1","1","1","2","1","2","3","1","3","m","1","m","1","2","3","m","1"],"b":["2006","1206","1206","1206","1208"]},"At iteration , the block fusion component  may perform a series of similar operations by which P(K|S) may be performed given S, P(K|S) may be performed given S, P(K|S) may be performed given S, and P(K|S) may be performed given S. In this way, the plurality of iterations may be further performed in this manner using the remainder output symbol sequences of of S, S, S, . . . Suntil convergence is reached. Accordingly, at the point convergence, the block fusion component  may provide the fusion symbol sequence .","Accordingly, the exemplary block fusion component  may enable the block based fusion of any set of heterogeneous classifiers in the classifier ensemble  to generate a fusion symbol sequence. The complexity of such iterative decoding may scale linearly with the number of classifiers in the ensemble.","Indexing Component","As shown in , the indexing component  may index the symbol sequences generated by the classification component  for storage in the speech database . The symbol sequences indexed by the indexing component  may include both output symbol sequences generated by the decoder ensemble , and fusion symbol sequences, such as fusion symbol sequence , as generated by the block fusion component . In some embodiments, the indexing component  may generate an index identifier for each symbol sequence. For example, but not as a limitation, given a fusion symbol sequence \u201cA, B, C\u201d, the indexing component  may assign an index identifier \u201cF\u201d to the fusion symbol sequence, and then store the fusion symbol sequence in the speech database .","In other embodiments, the indexing component  may generate an index identifier for a plurality of symbol sequences that correspond to a particular speech utterance. For example, but not as a limitation, the indexing component  may group together output symbol sequences A-C, as they are symbol sequences generated by a plurality of classifiers for a common set of feature vectors ()-() of a particular speech utterance. The indexing component  may assign a common index identifier \u201cU\u201d for the group of output symbol sequences A-C, prior to storing the set of symbol sequences in the speech database .","Query Component","As shown in , the query component  may work cooperatively with the index component  to query a query symbol sequence against one or more output symbol sequences that are pre-stored in the speech database . The query component  may then generate a list of one or more pre-stored output symbol sequences in the speech database  that match the query symbol sequence. The speech data, that is, utterances that correspond to the generated list of pre-stored output symbol sequences, may then be retrieved and displayed for manipulation (e.g., retrieval of the speech data, modification of the speech data, deletion of the speech data, organization of the speech data, presentation of the speech data, and\/or the like). In various embodiments, the generated list may be display based on the order of relevance, dates of recording, and\/or other criteria.","In various embodiments, the query component  may use a scoring algorithm that implements a union over N independent query time matching paths to retrieve the best speech data that matches a query symbol sequence. This algorithm is illustrated in .",{"@attributes":{"id":"p-0169","num":"0168"},"figref":"FIG. 13","b":["1304","1306","1308","1302","1302","1302","1306","1310","1302","1312","1314","1316"]},"Subsequently, an index component  may perform a concatenation operation that associates all the symbol sequences (paths) - and assigns an index identifier. The result of this concatenation operation may be represented by the notation , which is \u201cU={{(A, B, C, D, E).(F, G, H, I, J) . . . (V,W,X,Y,Z)}\u201d, in which \u201cU\u201d may be stored in the index identifier. The indexed symbol sequences and the speech utterance  may in the speech database  (). It will be appreciated that the classifier ensemble  and the decoder ensemble  may be any of a plurality of classifiers and decoders, respectively. Accordingly, the classifiers and the decoders shown are exemplary rather than limiting. Moreover, the speech utterance  may be one of any plurality of speech utterances that are captured and indexed by the classifier ensemble , the decoder ensemble , and the index component .","At a later retrieval time , the user  may attempt to search for the speech utterance . Accordingly, the user  may input a query speech utterance  to the classifier ensemble  and the decoder ensemble . The speech utterance  may state \u201cGrocery List.\u201d Accordingly, the ensembles  and  may generate query symbol sequences for the speech utterance . For example, the classifier A and the decoder A may generate a symbol sequence  (e.g., A, B, C,). Likewise, the classifier B and the decoder B may generate a symbol sequence  (e.g., F, K, H). Additionally, the classifier C and the decoder C may generate a symbol sequence  (e.g., V, W, X).","Subsequently, an index component  may perform a union operation that associates all the symbol sequences (paths) - of the query. The result of this union operation may be represented by the notation , which is \u201cQ={{(A, B, C,).(F, K, H) . . . (V,W,X)}\u201d.","The query component  may perform scoring operations  that would attempt to match the query to one or more symbol sequences that are stored in the speech database . In various embodiments, the querying component  may compute a match score for each query path associated with each index identifier, and add the scores together to determine whether one or more symbol sequences stored in the speech database  match the query symbol sequence. For example, the querying component  may compute the match scores between the query symbol sequence and stored symbol sequence  as follows:\n\nScore(10)=Score()+Score()+ . . . +Score(null sequence) \u2003\u2003(36)\n","Thus, provided that the result of \u201cScore(Q, U)\u201d is sufficiently high, that is, higher than other the scores computed for the query symbol sequence and other indexed symbol sequences stored in the speech database , speech utterance  may be retrieved from the speech data  as the most likely match to query speech utterance . The speech utterance  may be presented to the user  for further manipulation. In some embodiments, one or more speech utterances associated with other symbol sequences stored speech database  may also be retrieved and presented, as they may have match scores that are lower than the speech utterance . For example, any stored speech utterances that includes the word \u201clist\u201d may be computed as having a lower match score, but would be retrieved as possible matches if their score exceed a predetermined threshold.","Therefore, as can be observed, the total score of the query with respect to any given speech utterance in the database, e.g., U, is the sum or union over all its N independent partial matches with the speech utterance, e.g., U.","In at least some embodiments, the match score computation performed by the query component  may be based on the use of n-gram models, a type of probabilistic modeling for predicting items in a sequence. Moreover, it will be appreciated the n-gram models may also be used to determine a query symbol sequence and a fusion symbol sequence, such as fusion symbol sequence  (), in various other embodiments.","Returning to , it will be appreciated that the classification component  may enable the pluggable addition of classifiers to the classifier ensemble . Thus, the classifiers A-C are merely illustrative rather than limiting. Accordingly, the addition of classifiers having classes that complement the classes of other classifiers may add new or independent information that the existing classifiers do not have the ability to access, thereby increasing the overall accuracy of the classification component . Thus, overall speech recognition accuracy may also be increased. Thus, it will be appreciated that a classifier ensemble that enables the pluggable addition of different classifiers, when implemented on computing platforms of ever increasing power (e.g., expanded memory and improved CPU capability), may provide even more dramatic increases in speech recognition accuracy.",{"@attributes":{"id":"p-0178","num":"0177"},"figref":["FIG. 14","FIG. 14"],"b":"1400"},"At block , the classification component  () may receive a plurality of feature vectors that represent a received speech utterance. In various embodiments, the classification component  may receive the one or more feature vectors from a component that converts input audio signals into feature vectors, such as the noise compensated feature pipeline . The feature vectors may be derived from a plurality of frames of an input audio signal.","At block , the classification component  may convert the plurality of feature vectors into one or more symbol sequences. In various embodiments, the classification component  may perform the conversion of the plurality of feature vectors into a plurality of output symbol sequences using a classifier ensemble, such as the classifier ensemble  (), and a decoder ensemble, such as the decoder ensemble . In some embodiments, the classification component  may use a block fusion component  to combine the one or more output symbol sequences into a single fusion output symbol sequence.","At decision block , the speech recognition-capable device  may determine whether the one or more symbol sequences should be stored in data storage. If the speech recognition-capable device  determines that the one or more symbol sequences are to be stored in the data storage (\u201cyes\u201d at decision block ), the process  may proceed to .","At block , the speech recognition-capable device  may index and store the one or more symbol sequences that represent a received speech utterance into the speech database . In various embodiments, the one or more symbol sequences may be stored if a user has commanded the speech recognition-capable device  to store the received speech utterance (e.g., activate the speech recognition-capable device  to store the received speech).","However, if the speech recognition-capable device  determines that one or more symbol sequences are not to be stored in the speech database  (\u201cno\u201d at decision block ), the process  may proceed to block .","At block , the speech recognition-capable device  may use a query component  to execute a query using one or more symbol sequences from the classification component . The query may be executed against one or more symbol sequences that are previously stored on the speech database . In various embodiments, the query may be execute using a n-gram model based scoring algorithm that compares the one or more query symbol sequences against the pre-stored symbol sequences to find one or more relevant pre-stored symbol sequences. Accordingly, the query component  may retrieve pre-stored speech utterances that correspond to the one or more relevant pre-stored symbol sequences.","In some embodiments, the query component  may implement parallel queries using different symbol sequences derived from the same speech utterance. For example, the query component  may implement queries using the output symbol sequences that are directly outputted by the decoder ensemble , as well as the fusion symbol sequences outputted by the block fusion component . Thus, the query component  may retrieve pre-stored speech utterances that correspond to the one or more relevant pre-stored symbol sequences, as determined based on any of the different symbol sequences. Alternatively, the query component  may retrieve pre-stored speech utterances that correspond to the one or more relevant pre-stored symbol sequences, as determined based only on the output symbol sequences, or only on the fusion output sequences.","At block , the speech recognition-capable device  may present the one or more retrieved speech utterances for manipulation. For example, the retrieved speech data may be presented or played back, modified, deleted, reorganized, and\/or the like.",{"@attributes":{"id":"p-0187","num":"0186"},"figref":["FIG. 15","FIG. 15","FIG. 15"],"b":["1500","1404","1400"]},"At block , the classifier ensemble  () of the classification component  may produce a plurality of log probability sets from the plurality of feature vectors that represent a speech utterance. It will be appreciated that the classifier ensemble  may include different types of classifiers (e.g., GMM, HMM, HLP). Moreover, the classifier ensemble  may include supervised and unsupervised classifiers. Each of the classifiers may produce a log probability set that covers different classification classes, durations, and\/or the like.","At block , the decoder ensemble  of the classification component  may convert the plurality of log probability sets into a plurality of output symbol sequences. In various embodiments, each of the decoders in the decoder ensemble  and each of the classifiers in the classifier ensemble  may share a one-to-one correspondence.","At decision block , the classification component  may determine whether the block fusion component  should combine the plurality of output symbol sequences into a single fusion symbol sequence. In at least one embodiment, the classification component  may make this determination based on a preference setting of the speech recognition-capable device . If the classification component  determines that the block fusion component  should perform its task, (\u201cyes\u201d at decision block ), the process  may proceed to block .","At block , the block fusion component  may combine the plurality of output symbol sequences into a fusion symbol sequence based on an apriori probability algorithm. However, if the classification component  determines that no block fusion is necessary, (\u201cno\u201d at decision block ), the process  may proceed to block .","At block , the classification component may output one or more symbol sequences, either the output symbol sequences from block , or the fusion input symbol sequence from block , for storage or querying. However, it will be appreciated that in some instances, the classification component  may output both the output symbol sequences from block  and the fusion input symbol sequence from block  for storage or querying.","Exemplary Computing Environment",{"@attributes":{"id":"p-0193","num":"0192"},"figref":["FIG. 16","FIG. 1","FIG. 16"],"b":["1600","102","1600","1600","1600"]},"In a very basic configuration, computing system  typically includes at least one processing unit  and system memory . Depending on the exact configuration and type of computing device, system memory  may be volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.) or some combination of the two. System memory  typically includes an operating system , one or more program modules , and may include program data . The operating system  includes a component-based framework  that supports components (including properties and events), objects, inheritance, polymorphism, reflection, and provides an object-oriented component-based application programming interface (API), such as, but by no means limited to, that of the .NET\u2122 Framework manufactured by the Microsoft Corporation, Redmond, Wash. The device  is of a very basic configuration demarcated by a dashed line . Again, a terminal may have fewer components but will interact with a computing device that may have such a basic configuration.","Computing system  may have additional features or functionality. For example, computing system  may also include additional data storage devices (removable and\/or non-removable) such as, for example, magnetic disks, optical disks, or tape. Such additional storage is illustrated in  by removable storage  and non-removable storage . Computer storage media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data. System memory , removable storage  and non-removable storage  are all examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by Computing system . Any such computer storage media may be part of device . Computing system  may also have input device(s)  such as keyboard, mouse, pen, voice input device, touch input device, etc. Output device(s)  such as a display, speakers, printer, etc. may also be included. These devices are well known in the art and are not discussed at length here.","Computing system  may also contain communication connections  that allow the device to communicate with other computing devices , such as over a network. These networks may include wired networks as well as wireless networks. Communication connections  are some examples of communication media. Communication media may typically be embodied by computer readable instructions, data structures, program modules, etc.","It is appreciated that the illustrated computing system  is only one example of a suitable device and is not intended to suggest any limitation as to the scope of use or functionality of the various embodiments described. Other well-known computing devices, systems, environments and\/or configurations that may be suitable for use with the embodiments include, but are not limited to personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-base systems, set top boxes, game consoles, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and\/or the like.","The use of a voice activity detector and a noise compensated feature pipeline may reduce the amount of audio data that a speech recognition device needs to analyze to perform matching between different stored speech utterances. The various embodiments may be implemented on small portable electronic devices with constrained memory and processing capabilities, as well as other computing devices.","Moreover, the implementation of classifier ensemble for speech recognition enables the addition of classifiers having classes that complement the classes of other classifiers. Accordingly, the additional classifiers may add new or independent information that the other classifiers do not have the ability to access. Thus, overall speech recognition accuracy may be increased. Moreover, a classifier ensemble that enables the pluggable addition of different classifiers, when implemented on computing platforms of ever increasing power (e.g., expanded memory and improved CPU capability), may provide even more dramatic increases in speech recognition accuracy.","Conclusion","In closing, although the various embodiments have been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended representations is not necessarily limited to the specific features or acts described. Rather, the specific features and acts are disclosed as exemplary forms of implementing the claimed subject matter."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The use of the same reference number in different figures indicates similar or identical items.",{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 11A"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 11B"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 16"}]},"DETDESC":[{},{}]}
