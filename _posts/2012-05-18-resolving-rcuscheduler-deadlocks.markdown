---
title: Resolving RCU-scheduler deadlocks
abstract: A technique for resolving deadlocks between an RCU subsystem and an operating system scheduler. An RCU reader manipulates a counter when entering and exiting an RCU read-side critical section. At the entry, the counter is incremented. At the exit, the counter is manipulated differently depending on the counter value. A first counter manipulation path is taken when the counter indicates a task-context RCU reader is exiting an outermost RCU read-side critical section. This path includes condition-based processing that may result in invocation of the operating system scheduler. The first path further includes a deadlock protection operation that manipulates the counter to prevent an intervening RCU reader from taking the same path. The second manipulation path is taken when the counter value indicates a task-context RCU reader is exiting a non-outermost RCU read-side critical section, or an RCU reader is nested within the first path. This path bypasses the condition-based processing.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09003420&OS=09003420&RS=09003420
owner: International Business Machines Corporation
number: 09003420
owner_city: Armonk
owner_country: US
publication_date: 20120518
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION OF EXAMPLE EMBODIMENTS"],"p":["1. Field","The present disclosure relates to computer systems and methods in which data resources are shared among data consumers while preserving data integrity and consistency relative to each consumer. More particularly, the disclosure concerns an implementation of a mutual exclusion mechanism known as \u201cread-copy update\u201d (also known as \u201cRCU\u201d) in an operating system kernel environment wherein RCU uses the operating system scheduler and the scheduler uses RCU.","2. Description of the Prior Art","By way of background, read-copy update (also known as \u201cRCU\u201d) is a mutual exclusion technique that permits shared data to be accessed for reading without the use of locks, writes to shared memory, memory barriers, atomic instructions, or other computationally expensive synchronization mechanisms, while still permitting the data to be updated (modify, delete, insert, etc.) concurrently. The technique is well suited to both uniprocessor and multiprocessor computing environments wherein the number of read operations (readers) accessing a shared data set is large in comparison to the number of update operations (updaters), and wherein the overhead cost of employing other mutual exclusion techniques (such as locks) for each read operation would be high. By way of example, a network routing table that is updated at most once every few minutes but searched many thousands of times per second is a case where read-side lock acquisition would be quite burdensome.","The read-copy update technique implements data updates in two phases. In the first (initial update) phase, the actual data update is carried out in a manner that temporarily preserves two views of the data being updated. One view is the old (pre-update) data state that is maintained for the benefit of read operations that may have been referencing the data concurrently with the update. The other view is the new (post-update) data state that is seen by operations that access the data following the update. In the second (deferred update) phase, the old data state is removed following a \u201cgrace period\u201d that is long enough to ensure that the first group of read operations will no longer maintain references to the pre-update data. The second-phase update operation typically comprises freeing a stale data element to reclaim its memory. In certain RCU implementations, the second-phase update operation may comprise something else, such as changing an operational state according to the first-phase update.",{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIGS. 1A-1D"},"It is assumed that the data element list of  is traversed (without locking) by multiple readers and occasionally updated by updaters that delete, insert or modify data elements in the list. In , the data element B is being referenced by a reader r, as shown by the vertical arrow below the data element. In , an updater u wishes to update the linked list by modifying data element B. Instead of simply updating this data element without regard to the fact that r is referencing it (which might crash r), u preserves B while generating an updated version thereof (shown in  as data element B\u2032) and inserting it into the linked list. This is done by u acquiring an appropriate lock (to exclude other updaters), allocating new memory for B\u2032, copying the contents of B to B\u2032, modifying B\u2032 as needed, updating the pointer from A to B so that it points to B\u2032, and releasing the lock. In current versions of the Linux\u00ae kernel, pointer updates performed by updaters can be implemented using the rcu_assign_pointer( ) primitive. As an alternative to locking during the update operation, other techniques such as non-blocking synchronization or a designated update thread could be used to serialize data updates. All subsequent (post update) readers that traverse the linked list, such as the reader r, will see the effect of the update operation by encountering B\u2032 as they dereference B's pointer. On the other hand, the old reader r will be unaffected because the original version of B and its pointer to C are retained. Although r will now be reading stale data, there are many cases where this can be tolerated, such as when data elements track the state of components external to the computer system (e.g., network connectivity) and must tolerate old data because of communication delays. In current versions of the Linux\u00ae kernel, pointer dereferences performed by readers can be implemented using the rcu_dereference( ) primitive.","At some subsequent time following the update, r will have continued its traversal of the linked list and moved its reference off of B. In addition, there will be a time at which no other reader process is entitled to access B. It is at this point, representing an expiration of the grace period referred to above, that u can free B, as shown in .",{"@attributes":{"id":"p-0010","num":"0009"},"figref":["FIGS. 2A-2C","FIG. 2A","FIG. 2B","FIG. 2C"],"b":["1","1","1","1","2","1"]},"In the context of the read-copy update mechanism, a grace period represents the point at which all running tasks (e.g., processes, threads or other work) having access to a data element guarded by read-copy update have passed through a \u201cquiescent state\u201d in which they can no longer maintain references to the data element, assert locks thereon, or make any assumptions about data element state. By convention, for operating system kernel code paths, a context switch, an idle loop, and user mode execution all represent quiescent states for any given CPU running non-preemptible code (as can other operations that will not be listed here). The reason for this is that a non-preemptible kernel will always complete a particular operation (e.g., servicing a system call while running in process context) prior to a context switch.","In , four tasks 0, 1, 2, and 3 running on four separate CPUs are shown to pass periodically through quiescent states (represented by the double vertical bars). The grace period (shown by the dotted vertical lines) encompasses the time frame in which all four tasks that began before the start of the grace period have passed through one quiescent state. If the four tasks 0, 1, 2, and 3 were reader tasks traversing the linked lists of  or , none of these tasks having reference to the old data element B prior to the grace period could maintain a reference thereto following the grace period. All post grace period searches conducted by these tasks would bypass B by following the updated pointers created by the updater.","Grace periods may synchronous or asynchronous. According to the synchronous technique, an updater performs the first phase update operation, blocks (waits) until a grace period has completed, and then implements the second phase update operation, such as by removing stale data. According to the asynchronous technique, an updater performs the first phase update operation, specifies the second phase update operation as a callback, then resumes other processing with the knowledge that the callback will eventually be processed at the end of a grace period. Advantageously, callbacks requested by one or more updaters can be batched (e.g., on callback lists) and processed as a group at the end of an asynchronous grace period. This allows asynchronous grace period overhead to be amortized over plural deferred update operations.","In some RCU implementations designed for preemptible operating system kernels, asynchronous grace period processing is the norm but a synchronous expedited grace period, sometimes referred to as a \u201cBig Hammer\u201d grace period, is also available for updaters that need it. This expedited grace period forces a context switch (and thus a quiescent state) on each processor so that an updater can quickly perform its second-phase update operation. Existing callbacks associated with asynchronous grace periods are not affected. They must await the end of an asynchronous grace period before becoming ripe for processing. In other RCU implementations designed for preemptible operating system kernels, the RCU grace period mechanism includes the capability of priority boosting reader tasks that were preempted within their RCU read-side critical sections so that such tasks do not unduly delay the end of a grace period.","In the foregoing preemptible kernel-based RCU implementations, the RCU subsystem can invoke the operating system scheduler from the rcu_read_unlock( ) primitive, which is invoked by reader tasks when they exit their RCU read-side critical sections. The rcu_read_unlock( ) primitive is a companion to the rcu_read_lock( ) primitive, which is invoked by reader tasks when they enter their RCU read-side critical sections. Two scenarios in which the rcu_read_unlock( ) primitive will result in invocation of the operating system scheduler are (1) when RCU priority boosting is enabled and the reader task that invoked rcu_read_unlock( ) needs to be deboosted, and (2) when the reader task that invoked rcu_read_unlock( ) is the last reader holding up an RCU expedited grace period and the updater task that requested the expedited grace period needs to be awakened. These operations require the scheduler to acquire runqueue locks and priority inheritance locks.","However, some operating system schedulers, such as the scheduler in current versions of the Linux\u00ae kernel, can themselves implement RCU read-side critical sections. Applicant submits that there are scenarios in which such usage could cause deadlock problems if the scheduler invoked by a (non-scheduler) reader task itself invokes rcu_read_unlock( ) and attempts to obtain runqueue or priority-inheritance locks that it already holds. The present disclosure presents a solution that addresses this issue.","A method, system and computer program product are provided for resolving deadlocks between an RCU subsystem and an operating system scheduler. According to an example embodiment, an RCU registration component of the RCU subsystem allows an RCU reader to manipulate an rcu_read_lock_nesting counter when the RCU reader enters an RCU read-side critical section. An RCU unregistration component of the RCU subsystem allows an RCU reader to manipulate the rcu_read_lock_nesting counter when the RCU reader leaves an RCU read-side critical section. The unregistration component provides first and second rcu_read_lock_nesting manipulation paths that are dependent on a current value of the rcu_read_lock_nesting counter. The first rcu_read_lock_nesting manipulation path is taken when the current value of the rcu_read_lock_nesting counter is indicative of a task-context RCU reader exiting an outermost RCU read-side critical section. It includes condition-based read-side helper processing that may result in invocation of the operating system scheduler. This path further includes a deadlock protection operation that temporarily manipulates the rcu_read_lock_nesting counter to prevent any intervening RCU reader from taking the first rcu_read_lock_nesting manipulation path while a task-context RCU reader is within that path. The second rcu_read_lock_nesting manipulation path is taken when the current value of the rcu_read_lock_nesting counter is indicative of a task-context RCU reader exiting a non-outermost RCU read-side critical section, or an RCU reader being nested within the first rcu_read_lock_nesting manipulation path (such as due to an interrupt handler interrupting the path to run the scheduler or an explicit call to the scheduler from within the path). This path bypasses the condition-based read-side helper processing.","According to an example embodiment, the RCU registration component allows an RCU reader to manipulate the rcu_read_lock_nesting counter by incrementing it. The RCU unregistration component allows an RCU reader to manipulate the rcu_read_lock_nesting counter by either decrementing it or setting it to a value, depending on which manipulation path is taken by the RCU reader. The first manipulation path may include setting the rcu_read_lock_nesting counter to a deadlock protection value, and the second manipulation path may include decrementing the rcu_read_lock_nesting counter.","More particularly, the first manipulation path may taken when the rcu_read_lock_nesting counter has a first count value that is indicative of the task-context RCU reader exiting all RCU read-side critical section processing, and may comprise setting the rcu_read_lock_nesting counter to an arbitrary second count value representing the deadlock protection value, performing the read-side helper processing, and setting the rcu_read_lock_nesting counter to a third count value that is indicative of the task-context RCU reader being outside of an RCU read-side critical section. The arbitrary second count value may be a large negative number.","The second manipulation path may be taken when the rcu_read_lock_nesting counter has any value other than the first count value, and may comprise decrementing the rcu_read_lock_nesting counter and bypassing the read-side helper processing.","Turning now to the drawing  et seq., wherein like reference numerals represent like elements in all of the several views,  illustrates an example multiprocessor computer system  in which the subject matter disclosed herein may be implemented. By way of example only, the computer system  is shown as including multiple processors , . . . , a system bus , and a program memory . There are also cache memories , . . . and cache controllers , . . . respectively associated with the processors , . . . . A conventional memory controller  is associated with the memory .","The computer system  may represent any of several different types of computing apparatus. Such computing apparatus may include, but are not limited to, general purpose computers, special purpose computers, portable computing devices, communication and\/or media player devices, set-top devices, embedded systems, and other types of information handling machines. The term \u201cprocessor\u201d as used with reference to the processors , . . . encompasses any program execution unit capable of executing program instructions, including but not limited to a packaged integrated circuit device (such as a microprocessor), a processing core within a packaged integrated circuit device (such as a microprocessor core), or a hardware thread comprising one or more functional units within a processing core (such as an SMT thread). Each such execution unit may be referred to as a CPU (central processing unit). The processors , . . . may be situated within a single computing device or node (e.g., as part of a single-node SMP system) or they may be distributed over plural nodes (e.g., as part of a NUMA system, a cluster, or a cloud). The memory  may comprise any type of tangible storage medium capable of storing data in computer readable form for use in program execution, including but not limited to, any of various types of random access memory (RAM), various flavors of programmable read-only memory (PROM) (such as flash memory), and other types of primary storage (i.e., program memory). The cache memories , . . . may be implemented in several levels (e.g., as level 1, level 2 and level 3 caches) and the cache controllers , . . . may collectively represent the cache controller logic that supports each cache level. As illustrated, the memory controller  may reside separately from processors , . . . , for example, as part of a discrete chipset. Alternatively, the memory controller  could be provided by plural memory controller instances that are respectively integrated with the processors , . . . .","Each CPU embodied by a given processor  is operable to execute program instruction logic under the control of a software program stored in the memory  (or elsewhere). As part of this program execution logic, update operations (updaters)  may execute within a process, thread, or other execution context (hereinafter \u201ctask\u201d) on any of the processors . Each updater  runs periodically to perform updates on a set of shared data  that may be stored in the shared memory  (or elsewhere). In , reference numerals , . . . illustrate individual data updaters that respectively execute on the several processors , . . . . As described in the \u201cBackground\u201d section above, the updates performed by an RCU updater can include modifying elements of a linked list, inserting new elements into the list, deleting elements from the list, and other types of operations. To facilitate such updates, the processors  are programmed from instructions stored in the memory  (or elsewhere) to implement a read-copy update (RCU) subsystem  as part of their processor functions. In , reference numbers , . . . represent individual RCU instances that may periodically execute on the several processors , . . . . Any given processor  may also execute a read operation (reader) . Each reader  runs from program instructions stored in the memory  (or elsewhere) in order to periodically perform read operations on the set of shared data  stored in the shared memory  (or elsewhere). In , reference numerals , . . . illustrate individual reader instances that may respectively execute on the several processors , . . . . Such read operations will typically be performed far more often than updates, this being one of the premises underlying the use of read-copy update. Moreover, it is possible for several of the readers  to maintain simultaneous references to one of the shared data elements  while an updater  updates the same data element.","During operation of the computer system , an updater  will occasionally perform an update to one of the shared data elements . In accordance the philosophy of RCU, a first-phase update is performed in a manner that temporarily preserves a pre-update view of the shared data element for the benefit of readers  that may be concurrently referencing the shared data element during the update operation. Following the first-phase update, the updater  may register a callback with the RCU subsystem  for the deferred destruction of the pre-update view following a grace period (second-phase update). As described in the \u201cBackground\u201d section above, this is known as asynchronous grace period processing. Alternatively, the updater  may request a synchronous expedited grace period.","The grace period processing performed by the RCU subsystem  entails starting new grace periods and detecting the end of old grace periods so that the RCU subsystem  knows when it is safe to free stale data (or take other actions). Grace period processing may further entail the management of callback lists that accumulate callbacks until they are ripe for batch processing at the end of a given grace period. In addition, reader priority boosting may be performed on behalf of readers  that are blocking the end of a grace period. The foregoing grace period processing operations may be performed by periodically running RCU subsystem instances , . . . on the several processors , . . . .","Turning now to , example components of the RCU subsystem  are shown. These components include an RCU reader API (Application Programming Interface) , an RCU updater API , an RCU grace period API , and a set of grace period detection and callback processing functions . As shown in , the RCU subsystem  also uses several fields in the task structure  of each reader . As discussed in more detail below, these fields include an rcu_read_lock_nesting counter A and an rcu_read_unlock_special flag (B).","The RCU reader API  comprises a reader registration component A and a reader unregistration component B. These components are respectively invoked by readers  as they enter and leave their RCU read-side critical sections, thereby allowing the RCU subsystem  to track reader operations and determine when readers are engaged in RCU-protected read-side critical section processing. In an example embodiment, the reader registration component A and the reader unregistration component B may be respectively implemented using the rcu_read_lock( ) and rcu_read_unlock( ) primitives found in existing read-copy update implementations, but with the rcu_read_unlock( ) primitive being modified to address the deadlock problem discussed in the \u201cBackground\u201d section above.","When a reader  enters an RCU read-side critical section and invokes the reader registration component A, the latter increments the rcu_read_lock_nesting counter A (see ) in the reader's task structure. When the reader  leaves an RCU read-side critical section and invokes the reader unregistration component B, the latter decrements the rcu_read_lock_nesting counter A. The term \u201cnesting\u201d as used in the name of this counter refers to the fact that a given reader's RCU read-side critical sections can be nested or overlapping. In conventional implementations of the rcu_read_unlock( ) primitive, a counter value of zero is commonly used to indicate that a reader  is not performing any RCU read-side critical section processing. This counter value also triggers the conventional rcu_read_unlock( ) primitive to check whether special read-side helper processing (see below) is needed. As described in more detail below, a modified version of the rcu_read_unlock( ) primitive may be used to bypass this test for scheduler-based invocations of the RCU subsystem .","The RCU updater API  may comprise a register callback component A and an expedited grace period component B. The register callback component is used by the updaters  to register a callback following a first-phase update to a shared data element . In an example embodiment, this component may be implemented using the \u201ccall_rcu( )\u201d primitive found in existing read-copy update implementations. A call to the register callback component A initiates processing that places the callback on an RCU callback list (not shown) associated with the processor  that runs the updater . This starts an asynchronous grace period so that the callback can be processed after the grace period has ended as part of second-phase update processing to remove stale data (or take other actions). The expedited grace period component B is used by the updaters  to request a synchronous expedited grace period following a first-phase update to a shared data element . The updater  blocks while the expedited grace period is in progress, then performs second-phase update processing to free stale data (or take other actions). In an example embodiment, this component may be implemented using the \u201csynchronize_rcu_expedited( )\u201d primitive found in existing read-copy update implementations.","The RCU grace period API  may comprise a check callbacks component A. This component may be run periodically (e.g., in response to a scheduler clock interrupt) in order to check for new callbacks, start a new grace period if one is needed, and request callback processing. In an example embodiment, this component may be implemented using the \u201crcu_preempt_check_callbacks( )\u201d primitive found in existing read-copy update implementations. As discussed below, the check callbacks component A also manipulates the rcu_read_unlock_special flag B (see ) if necessary to advance a grace period.","The grace period detection and callback processing functions  may comprise various components conventionally found in existing read-copy update implementations, including but not limited to a quiescent state\/grace period tracking component, a callback processor, a blocked reader handler, and a reader priority boosting component. Of particular relevance to the present disclosure is a read-side helper component A that is implemented when a reader  is delaying the end of a grace period. In an example embodiment, this component may be implemented using the \u201crcu_read_unlock_special( )\u201d primitive found in existing read-copy update implementations. Its operations may include advising the RCU subsystem  that a delayed reader  is exiting an RCU read-side critical section (and that a quiescent state has been reached if this is the last reader), removing the reader from one or more blocked task lists used to identify readers that are blocking the end of a grace period, and invoking the scheduler to unboost the reader's priority if it was previously boosted.","As described in the \u201cBackground\u201d section above, the present disclosure describes a technique that addresses the problem of deadlock that may occur in modern operating systems whose schedulers make increasing use of RCU and wherein RCU makes increasing calls to the scheduler. Current versions of the Linux\u00ae kernel configured for kernel-level preemption are one example. As shown in , a preemptible operating system kernel  that implements the RCU subsystem  may operate in task context A (e.g., due to a system call) and scheduler context B (also referred to herein as \u201cthe scheduler\u201d). The RCU subsystem  can be invoked from both task context A and scheduler context B in the event that either context needs to perform RCU read-side critical section processing as an RCU reader . These respective invocations of the RCU subsystem  are shown by the arrows  and , each of which represents a pair of calls to the reader registration component A (e.g., rcu_read_lock( )) and the reader unregistration component B (e.g., rcu_read_unlock( )) of . In addition, an RCU reader  operating in task context A can invoke the scheduler B from the reader unregistration component B (e.g., rcu_read_unlock( )) via the read-side helper component A of  (e.g., rcu_read_unlock_special( )). This is shown by the arrow . For example, the read-side helper component A may need to deboost the current task (when RCU priority boosting is enabled) or to wake up the task if it requested an expedited RCU grace period and it is the last such task that was blocking the end of that grace period.","In current implementations of the Linux\u00ae kernel, the rcu_read_unlock( ) function that provides the reader unregistration component B invokes a work function known as _rcu_read_unlock( ). The _rcu_read_unlock( ) function, in turn, conditionally invokes the rcu_read_unlock_special( ) function that provides the read-side helper component A. The conditions that lead to the invocation of rcu_read_unlock_special( ) are determined from the two fields A and B in the reader's task structure  (see ). As previously stated, the first field A is an rcu_read_lock_nesting counter that maintains a count of the number of times the reader  has recursively entered an RCU read-side critical section. In a conventional RCU implementation a counter value of zero signifies that the reader  is not within such a critical section. The second field B is the rcu_read_unlock_special flag that is set by the check callbacks component (A) of . In current implementations of the Linux\u00ae kernel, the rcu_read_unlock_special( ) function that provides the read-side helper component A is invoked when (1) the rcu_read_lock_nesting counter A is zero (indicating that the reader  has completed all RCU read-side critical section processing) and (2) the rcu_read_unlock_special flag B is set (indicating that additional actions are required on behalf of this reader).","Example C language source code for a conventional _rcu_read_unlock( ) work function that performs the foregoing condition processing is shown in . Line  decrements and tests the rcu_read_lock_nesting counter A for zero. If true, and following a memory ordering barrier( ) compiler directive in line , a check is made in line  to determine the state of the rcu_read_unlock_special flag B. If the rcu_read_unlock_special flag B is set, the rcu_read_unlock_special( ) function is invoked in line .","The operations performed by the rcu_read_unlock_special( ) function that provides the read-side helper component A will not be described in detail, but, as mentioned above, may include deboosting the current task (when RCU priority boosting is enabled) or waking up the task that requested an expedited RCU grace period (when the current task is the last one executing). Referring back to , these operations can result in the RCU subsystem  invoking the task scheduler B (see arrow ) to acquire its runqueue locks and its priority inheritance locks. If the scheduler B independently invokes rcu_read_unlock( ) while holding the runqueue and priority-inheritance locks, and if that invocation of rcu_read_unlock( ) reinvokes the scheduler in the same manner as the initial rcu_read_unlock( ) operation, there is the possibility of deadlock under certain scenarios.","Such scenarios can be avoided if the scheduler B disables interrupts when acquiring its runqueue and priority-inheritance locks. As long as the scheduler's RCU read-side critical sections are completely contained in a given runqueue or priority-inheritance lock's critical section, then that RCU read-side critical section cannot be interrupted, blocked, or preempted. There can therefore be no reason for rcu_read_unlock( ) to reinvoke the scheduler. In particular, there can be no priority boosting during the scheduler's RCU read-side critical because there can be no preemption with interrupts disabled. Moreover, the scheduler's RCU read-side critical section cannot be the last RCU read-side critical section to end for an expedited grace period because interrupts are disabled and there can be no preemption by reschedule IPIs (interprocessor interrupts).","However, consider an implementation of rcu_read_unlock( ) in a hierarchical RCU implementation designed for systems with many processors. In current versions of the Linux\u00ae kernel, the hierarchical RCU kernel configuration option is known as CONFIG_TREE_PREEMPT_RCU. In this implementation it is possible for the following sequence of events to occur:\n\n","The problem in this situation is that the interrupt handler's RCU reader code path is nested within the task-level RCU reader's code path, and the interrupt handler's instance of the _rcu_read_unlock( ) primitive is seeing the state that is intended for the task-level _rcu_read_unlock( ). A proposed solution to this problem is to use separate first and second rcu_read_lock_nesting manipulation paths in the reader unregistration component B that are dependent on different values of the rcu_read_lock_nesting counter A (see ). The first rcu_read_lock_nesting manipulation path includes condition-based read-side helper processing that may result in invocation of the operating system scheduler B. This path is taken when the current value of the rcu_read_lock_nesting counter is indicative of a task-context RCU reader  exiting an outermost RCU read-side critical section. It includes a deadlock-protection operation that temporarily manipulates the rcu_read_lock_nesting counter to prevent any intervening RCU reader  from taking the first rcu_read_lock_nesting manipulation path while the task-context RCU reader  is within that path. The second rcu_read_lock_nesting manipulation path bypasses the condition-based read-side helper processing. This path is taken when the rcu_read_lock_nesting counter is indicative of a task-context RCU reader exiting a non-outermost RCU read-side critical section, or when the current value of the rcu_read_lock_nesting counter is indicative of an RCU reader being nested within the first rcu_read_lock_nesting manipulation path, such as due to an interrupt handler interrupting the path to run the scheduler or an explicit call to the scheduler from within the path.","In an example embodiment, the RCU unregistration component allows an RCU reader to manipulate the rcu_read_lock_nesting counter by either decrementing it or setting it to a value, depending on which manipulation path of the reader unregistration component is taken. In particular, the first manipulation path includes setting the rcu_read_lock_nesting counter to a deadlock-protection value, and the second manipulation path includes decrementing the rcu_read_lock_nesting counter. Still more particularly, the first manipulation path may be taken when the rcu_read_lock_nesting counter has a first count value that is indicative of the task-context RCU reader exiting all RCU read-side critical section processing, and may comprise setting the rcu_read_lock_nesting counter to an arbitrary second count value representing the deadlock-protection value, performing read-side helper processing, and resetting the rcu_read_lock_nesting counter to a third count value that is indicative of the task-context RCU reader being outside of an RCU read-side critical section. The arbitrary second count value may be a large negative number. The second manipulation path of the unregistration component may be taken when the rcu_read_lock_nesting counter has any value other than the first count value, and may comprise decrementing the rcu_read_lock_nesting counter and bypassing the read-side helper processing.","Example C language code implementing this solution is shown in . This has roughly the same overhead as the conventional code of : the decrement and assignment operation of line  of  has been replaced by the decrement operation of line  of  (for the first rcu_read_lock_nesting manipulation pathway) or the two assignment operations at lines  and  of  (for the second rcu_read_lock_nesting manipulation pathway). When a task-context reader  exits its outermost RCU read-side critical section and reaches line  of , it will find that the rcu_read_lock_nesting counter A is equal to one. Execution will jump to line , a memory-ordering barrier( ) compiler directive provided in line , and the rcu_read_lock_nesting counter A will be set to INT_MIN, which can be a large negative number, in line . This represents the above-mentioned deadlock-protection value. Following another memory ordering barrier( ) compiler directive in line , lines - will be implemented and rcu_read_unlock_special( ) will be invoked if necessary (i.e., according to the state of the rcu_read_unlock_special flag B in the reader's task structure (see ). Once the invocation of rcu_read_unlock_special( ) by the task-context RCU reader  is no longer a possibility, another memory-ordering barrier( ) compiler directive is provided on line  and the rcu_read_lock_nesting counter A is set to zero on line . The foregoing processing represents the first rcu_read_lock_nesting manipulation path mentioned above. This path is taken by task-context readers  that are exiting their outermost RCU read-side critical sections.","Advantageously, if a nested scheduler-level RCU reader  is invoked while the task-context RCU reader  is within the first rcu_read_lock_nesting manipulation path, the scheduler-level RCU reader, upon reaching line  of , will find that the rcu_read_lock_nesting counter A is not equal to 1. The rcu_read_lock_nesting counter A will equal INT_MIN+1 due to the scheduler-level RCU reader  having previously invoked the reader registration component A (which increments the rcu_read_lock_nesting counter). Line  will then decrement the rcu_read_lock_nesting counter A (setting it to INT_MIN) but the code path of lines - that leads to rcu_read_unlock_special( ), and the deadlock problem described above, will be bypassed. The foregoing processing represents the second rcu_read_lock_nesting manipulation path mentioned above. This path is taken by any RCU reader that is nested within the first rcu_read_lock_nesting manipulation path. As previously mentioned, this could be an interrupt handler that interrupts the first path or an explicit call to the scheduler B from within that path. This first rcu_read_lock_nesting manipulation path is also taken by task-context readers  that are not exiting an outermost RCU read-side critical section.","The flow diagram of  illustrates the foregoing processing. In block , the test represented by line  of  is implemented. Block  represents the decrement of line  of  and block  represents the assignment of line  of . In block , the condition test represented by line  of  is implemented. Block  represents the conditional invocation of rcu_read_unlock_special( ) set forth on line  of . Block  represents the operation of line  of  in which the rcu_read_lock_nesting counter is set to zero.","Accordingly, a technique for has been disclosed for implementing read-copy update in a manner that resolves RCU-scheduler deadlocks in an operating system kernel. It will be appreciated that the foregoing concepts may be variously embodied in any of a data processing system, a machine implemented method, and a computer program product in which programming logic is provided by one or more machine-useable storage media for use in controlling a data processing system to perform the required functions. Example embodiments of a data processing system and machine implemented method were previously described in connection with . With respect to a computer program product, digitally encoded program instructions may be stored on one or more computer-readable data storage media for use in controlling a computer or other digital machine or device to perform the required functions. The program instructions may be embodied as machine language code that is ready for loading and execution by the machine apparatus, or the program instructions may comprise a higher level language that can be assembled, compiled or interpreted into machine language. Example languages include, but are not limited to C, C++, assembly, to name but a few. When implemented on a machine comprising a processor, the program instructions combine with the processor to provide a particular machine that operates analogously to specific logic circuits, which themselves could be used to implement the disclosed subject matter.","Example data storage media for storing such program instructions are shown by reference numerals  (memory) and  (cache) of the multiprocessor system  of . The system  may further include one or more secondary (or tertiary) storage devices (not shown) that could store the program instructions between system reboots. A further example of storage media that may be used to store the program instructions is shown by reference numeral  in . The storage media  are illustrated as being portable optical storage disks of the type that are conventionally used for commercial software sales, such as compact disk-read only memory (CD-ROM) disks, compact disk-read\/write (CD-R\/W) disks, and digital versatile disks (DVDs). Such storage media can store the program instructions either alone or in conjunction with an operating system or other software product that incorporates the required functionality. The storage media could also be provided by other portable storage media (such as floppy disks, flash memory sticks, etc.), or storage media combined with drive systems (e.g. disk drives). As is the case with the memory  and the cache  of , the storage media may be incorporated in data processing platforms that have integrated random access memory (RAM), read-only memory (ROM) or other semiconductor or solid state memory. More broadly, the storage media could comprise any electronic, magnetic, optical, infrared, semiconductor system or apparatus or device, or any other tangible entity representing a machine, manufacture or composition of matter that can contain, store, communicate, or transport the program instructions for use by or in connection with an instruction execution system, apparatus or device, such as a computer. For all of the above forms of storage media, when the program instructions are loaded into and executed by an instruction execution system, apparatus or device, the resultant programmed system, apparatus or device becomes a particular machine for practicing embodiments of the method(s) and system(s) described herein.","Although various example embodiments have been shown and described, it should be apparent that many variations and alternative embodiments could be implemented in accordance with the disclosure. It is understood, therefore, that the invention is not to be in any way limited except in accordance with the spirit of the appended claims and their equivalents."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing and other features and advantages will be apparent from the following more particular description of example embodiments, as illustrated in the accompanying Drawings, in which:",{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIGS. 1A-1D"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIGS. 2A-2C"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 5","FIG. 4"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 10","FIG. 9"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
