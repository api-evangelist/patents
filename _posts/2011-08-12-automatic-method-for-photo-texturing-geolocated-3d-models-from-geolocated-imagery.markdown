---
title: Automatic method for photo texturing geolocated 3-D models from geolocated imagery
abstract: A method and system for applying photo texture to geolocated 3D models operates within a 3D modeling system. The modeling system includes a modeling application operating on a workstation and a database of geotagged imagery. A 3D model created or edited within the 3D modeling system is geolocated such that every point in the 3D modeling space corresponds to a real world location. For a selected surface, the method and system search the database of imagery to identify in the database one or more images depicting the selected surface of the 3D model. The method and system identify the boundaries of the selected surface within the image by transforming two or more sets of coordinates from the 3D modeling space to a coordinate space corresponding to the image. The portion of the image corresponding to the selected surface is copied and mapped to the selected surface of the 3D model.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08890863&OS=08890863&RS=08890863
owner: Google Inc.
number: 08890863
owner_city: Mountain View
owner_country: US
publication_date: 20110812
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF TECHNOLOGY","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The present disclosure relates to photo texturing of geolocated three-dimensional models and, more specifically, to a system and a method configured to automatically identify and capture from a geolocated photograph a view of a selected surface and to apply the captured view of the selected surface to photo texture a corresponding face of a three-dimensional geolocated model.","The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.","Advances in computer-aided three-dimensional (3D) modeling\u2014both in terms of the number of available software applications and the ease with which the available applications can be used\u2014have facilitated the proliferation of 3D digital modeling through a number of technological areas. Using computer-aided drafting (CAD) programs, digital modeling software, and the like, professionals and novices alike can generate digital, 3D models of buildings, spaces, objects, etc.","One context in which computer-aided 3D modeling has found particular utility is the field of digital cartography. One function of digital cartography is to generate maps that provide accurate visual depictions of an area. In particular, some maps now allow a user to visualize a particular location as it actually appears, by integrating photographic elements from satellite imagery, aerial imagery, and surface imagery with traditional cartographic elements (e.g., information about road locations, geographic features, etc.) and advanced location data (e.g., from the Global Positioning System (GPS)). In some instances, digital maps may allow a user to view an element such as, for example, a building, as a 3D model.","Generally, 3D models depicted in digital maps are created using computer-aided 3D modeling programs. In some instances, the 3D models are untextured. That is, the 3D models depict the shape of the building, but do not accurately depict the building's visual characteristics, such as the building's fa\u00e7ade. In other instances, the 3D models are textured using, for example, one or more photographic images of the real-world object modeled. The process of applying a photographic image to a model of a real-world object is referred to as \u201cphoto texturing.\u201d","The tedious and time-consuming nature of the photo texturing process limits the capacity of most digital map systems to implement accurate 3D modeling of mapped areas. To model and texture a given object, images of the object must be acquired, usually from several angles. In some instances, this may require traveling to the object. However, even when such images are readily available, applying the images to the surfaces of a 3D model is often a non-trivial task requiring at least selection of a portion of an image and alignment of the image characteristics (e.g., the angle from which the image was taken with respect to the surface to be photo textured) with the model.","In an embodiment, a computer-implemented method for applying photo texturing to a 3D model of a real-world object includes receiving geolocation information for a point in a model space in which the 3D model is located. Thereafter, the method receives a selection of a surface of the 3D model and receives a request to automatically apply to the selected surface a photo texture. The method proceeds to select an image to use to photo texture the selected surface and to select an area of the image to apply to the selected surface. Having selected the area, the method applies the area of the selected image to the selected surface.","If desired, the computer-implemented method may select an image to use to phototexture the selected surface by selecting a first image of a plurality of geolocated images, which, in turn, may include selecting a point of view of a panoramic image. Selecting a first image may also include determining a real-world location from which a real-world surface corresponding to the selected surface could be viewed, determining one or more geolocated images captured at the determined real-world location, and selecting as the first image one or more geolocated images captured at the determined real-world location. Further, selecting as the first image one of the one or more geolocated images captured at the determined real-world location may include selecting an image that depicts the real world surface, which may, in turn, include selecting a point of view of a panoramic image captured at the determined real-world location.","In another embodiment, a system includes a network; a database communicatively coupled to the network, a display device; a processor communicatively coupled to the database via the network and to the display device; and a memory device storing a set of machine readable instructions executable by the processor. The database stores a plurality of images, each image having associated with it a set of information specifying at least the location at which the image was captured. The instructions stored on the memory device are operable to cause the processor to display on the display device a modeling space for modeling a 3-dimensional object. The instructions cause the processor to receive a command to geolocate a point in the modeling space. Thereafter, the instructions cause the processor to create a 3-dimensional model of the 3-dimensional object, to receive a selection of a surface of the 3D model, and to receive a request to photo texture the selected surface. In response to receiving the request, the instructions cause the processor to select from the database an image to use to photo texture the selected surface, select an area of the image to apply to the selected surface; and apply the area of the selected image to the selected surface.","In yet another embodiment, a computer system for applying photo texturing to a 3D model of an object includes a display device, a processor coupled to the display device, and a memory device storing a set of machine readable instructions executable by the processor. The instructions cause the processor to display on the display device a modeling space for modeling a 3-dimensional object. Thereafter, the instructions cause the processor to geolocate a point in the modeling space and to create a 3-dimensional model of the 3-dimensional object. The processor, in accordance with the instructions, causes the processor to receive a selection of a surface of the 3D model and to receive a request to photo texture the selected surface. In response to the request to photo texture the selected surface, the instructions cause the processor to select from a database an image to use to photo texture the selected surface, select an area of the image to apply to the selected surface, and apply the area of the selected image to the selected surface.","A computer-implemented method or a computer system or a computer-readable medium storing a set of instructions for execution on a processor operates to apply a photo texture to one or more surfaces of a 3D model of a real-world object. The method or apparatus receives geolocation information a point in a model space in which the 3D model is located. Upon receipt of a selection of a surface of the 3D model, and a request to automatically apply to the selected surface a photo texture, the method or apparatus identifies an image to use to photo texture the selected surface, identifies an area of the image to apply to the selected surface, and photo textures the selected surface by applying the identified area of the image to the selected surface.","In various embodiments, the method and apparatus may identify an image to use to photo texture the selected surface of the 3D model by identifying one or more sets of coordinates corresponding to a real-world location of selected surface and, thereafter, identifying a stored image of the real world location specified by the sets of coordinates. The method and apparatus may transform the one or more sets of coordinates through a plurality of coordinate spaces to identify the portion of the image to apply to the selected surface.","Generally speaking, a 3D modeling system allows a user to design 3D models, for example, for architectural, civil and mechanical design purposes, or for digital modeling and\/or rendering of any 3D object. For example, a 3D modeling system may allow an architect to render a new building design in 3D such that the building can be displayed from any angle. An interior designer may design a space and display the space to a client as it would appear as the client entered the space from one or more directions, in various lighting conditions, or with various design options. An engineer may design a consumer device, such as a mobile phone, viewing it from all angles to gauge its aesthetic appeal.","Generally, the realism of such models is limited by the amount of time and effort available to provide detail to each of the modeled surfaces. For example, an architect may spend countless hours programming the surfaces of a 3D model to have just the right surface texturing (e.g., a brick fa\u00e7ade), to include doors and windows, to reflect light in just the right manner, etc. Sometimes, however, it is desirable to model an existing building (as opposed to a building being designed) in a fast and efficient manner. This may be the case where, for instance, an architect is modeling the site of a new building, e.g., including the existing buildings around the structure being designed. In these instances, it is significantly more efficient to apply to the models images of the existing buildings as they exist. By doing so, it is possible to create 3D models with a high degree of realism in an expeditious manner. The process of adding surface texture (e.g, color, detail, etc.) to a digitally rendered 3D model is called \u201ctexture mapping.\u201d \u201cPhoto-texturing\u201d is the process of texture mapping a digital image (such as a scanned photograph, a digital photograph, etc.) to a surface of a digitally rendered 3D model.","Of course, in order to apply a photo-texture to a surface of a 3D model, it is necessary first to have a photograph of the surface. Generally, where the object is a structure, this may require traveling to the location of the structure and acquiring photographic images (e.g., with a digital camera) of any surface of the structure that is to be photo-textured. For structures located in remote or otherwise inaccessible locations, acquiring the images may be prohibitively expensive or time consuming. Alternatively, one or more existing images may be used to photo-texture a surface or surfaces of the structure.","In any event, a user desiring to apply a photo texture to a surface of a 3D model, having determined an image to apply, may select a portion of the image that depicts a surface corresponding to the surface of the 3D model to which the photo texture will be applied. Selection of the area could be accomplished by drawing a polygonal (e.g., rectangular) shape over the digital image, locating crosshairs indicative of vertices of such a polygon, drawing a freehand selection area, sizing and\/or locating a pre-determined polygon (e.g., corresponding to the shape of the surface to be photo-textured), etc. The selected portion of the image may then be mapped to the surface of the 3D model by UV mapping, as discussed below.","A database of digital photographic images may provide a ready source of existing images that could be used to photo-texture surfaces of structures modeled in 3D. The database may include images of structures within some area. For example, a database may include images of structures on one block, images of structures in a particular city, images of structures in a particular country, etc. For ease of locating in the database a particular structure, each of the images may be associated with information (i.e., the images may be \u201ctagged\u201d) indicating the location from which the image was captured. The location indication may be in any appropriately searchable form including, by way of example and not limitation: latitude and longitude, intersection, street address, zip code, distance and direction from a known, fixed point, etc. The identification of a geographic location of a real-world object (e.g., a structure, a camera) is referred to herein as \u201cgeolocation.\u201d An object for which a geographic location has been identified is said to be \u201cgeolocated.\u201d The association (e.g., by metadata) of geolocation information with a file (e.g., a digital image) is referred to herein as \u201cgeotagging.\u201d A file (e.g., an image file) having associated metadata indicative of a geolocation (e.g., a geolocation of the camera at the time the image was captured) is said to be \u201cgeotagged.\u201d","The database of digital photographic images may be populated with geotagged images recorded by, for example, a digital camera with a GPS unit that allows the camera to geolocate. In some embodiments, the metadata associated with the image may include more information than just a simple location of the camera at the time an image was captured. For example, the metadata may include the direction the camera was facing when the image was captured, the focal point of the lens, the height and\/or angle of the camera with respect to the ground, the zoom level of the lens, the distance from the camera to nearby objects, etc.","The images and metadata may be captured, for example, by a plurality of cameras mounted on a moving vehicle so as to provide (and capture) a 360-degree view as vehicle moves. Each time a set of images is captured\u2014which may happen based on a time interval or a distance interval\u2014the images are geotagged (e.g., using information from a GPS receiver) and stored. Additional information, such as information from a laser rangefinder or information about the vehicle speed and direction, may also be stored as associated with the images. An application specific interface (API) may allow various programs to access the data stored in the database.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 1","b":["10","10","12","14","16","12","18","18","20","22","22","20","20","22","24","26","28"]},"The I\/O interface  couples the display  and the input devices  to a bus  in the workstation , which bus , in turn, communicatively couples the I\/O interface  to a processor  and a memory sub-system . The processor  may be any processor capable of executing one or more programs stored in the memory sub-system  (as described below) to perform the specific tasks associated with the 3D photo-texturing application herein described and, in particular, may be a general purpose processor, a digital signal processor, a field programmable gate array (FPGA), an application specific IC (ASIC), etc. The processor  may operate to retrieve one or more applications  from the memory sub-system  and to execute the applications . An exemplary workstation  includes an operating system (OS)  such as Windows, Mac OS, Linux, Android, etc. The exemplary workstation  may also store in the memory sub-system  a software application  embodying the 3D modeling application. For simplicity, the term \u201csoftware application\u201d is used herein to refer both to an instance of a software application executing in the workstation  and to the set of computer instructions that defines the software application. However, it will be understood that while an instance of an application executes in the workstation , machine-readable instructions of the application are stored on a non-transitory, computer-readable medium such as a persistent memory , a system cache , or both. The persistent memory  may be a hard disk, a flash drive, a CD, a DVD, a tape drive, etc. In at least some of the embodiments, the persistent memory  is significantly slower that the system cache . In particular, the persistent memory  may have one or more of a slower read speed, a slower write speed, a lower bandwidth (i.e., the size of a data block the persistent memory  can supply to the CPU  at one time), etc.","The OS  executes on the processor  (\u201ccentral processing unit\u201d or \u201cCPU\u201d) that includes one or several CPU cores, depending on the embodiment. The workstation  also includes the system cache  that may be implemented as a quickly-accessible, physical memory such as random access memory (RAM). In general, the system cache  may be provided on a same chip as the CPU  or on a separate chip. In some embodiments, the system cache  is a dedicated memory region on a RAM chip. During operation, the system cache  operates as an active storage, so that software tasks such as the application  access memory pages (or simply \u201cpages\u201d) stored in the system cache  quickly and efficiently. While the application , including the OS  and the 3D modeling application , are generally described above as being stored on the persistent memory  and\/or the system cache , the applications  may be stored on the server side system , on another workstation  on the user-side system , or some combination of the two.","With continued reference to , the workstation  and, in particular, the I\/O interface , may include a network interface card  via which the workstation  may be coupled by the communication network , which may be the Internet, for example. Specifically, the workstation  may communicate via the network  with a server  on the server-side system , which may, in turn, be in communication, for example over a local area network (LAN) , with one or more databases .","The server  may be configured similarly to the workstation , in that it may include one or more displays, CPUs, memory sub-systems, I\/O interfaces, input devices, etc. Moreover, while described herein as a single server, the server  may be any number of servers  according to the architecture of the network , the LAN , server loading and\/or demand parameters, etc.","As described in greater detail below, the server  generally operates to receive from the workstation , via the network , one or more requests generated by the 3D modeling application  and, in response to the request, to retrieve information from the database  and transmit the retrieved information back to the workstation , again via the network . The server  may complete one or more processing steps before and\/or after retrieving the information from the database , such as, by way of example and not limitation, calculating a value using as input the information from the database  or performing a lookup of a database index value. Additionally, in some embodiments, the server  may store one or more applications, including the 3D modeling application . In these instances, the workstation  may receive the application  from the server  and execute the application  locally on the workstation . In alternate embodiments, the application  may be stored and executed on the server , and the server  may provide only display and\/or user-interface data via the network  to the workstation . That is, the workstation  may serve as a display and\/or interface terminal for the application executed on the server .","In still other embodiments, the application  is stored in the memory sub-system  and executed by the CPU  on the workstation , and interacts with a second application (not shown) stored and executed on the server . The second application may be, by way of example and not limitation, an interactive map application that retrieves (e.g., from the database ) and\/or renders maps, satellite imagery, terrain data, etc. for display on an output device such as the display , provides an interactive user interface to allow a user to zoom in on a desired location, pan to a desired location, select various types of data on the map or the satellite image, etc. In one such embodiment, the second application provides two-dimensional and three-dimensional representations of geographic regions. A user may view a two-dimensional satellite image of a certain location and dynamically switch to a three-dimensional view of the location. At the time when the second application transitions from a two-dimensional rendering to a three-dimensional rendering, the second application at least sometimes requests additional resources such as bitmap images, modeling data, etc. Further, in some cases, the second application invokes additional sets of functions, such as rendering functions for three-dimensional graphics, stored in a corresponding dynamically linked library (DLL), for example.","The information retrieved and\/or used by the second application may be stored in the database . Specifically, the database  may include map data, satellite imagery, terrain data, imagery captured from a pedestrian and\/or vehicular perspective (e.g., views of buildings taken from a street or a pedestrian walkway), panoramic imagery, etc., any and\/or all of which may be have associated with it meta-data including: a location from which an image was captured; a type of camera and\/or lens used to capture an image; an angle of a camera relative to the ground and\/or relative to a velocity of the camera; a heading and or velocity of a camera and\/or the camera's viewport when an image is captured; a zoom level of an image captured; and any other parameter desired. In some embodiments, the database  includes a data store  of map and satellite images and associated data and a data store  of images captured from street level and associated data. Of course, the data stores  and  may be on a single database , may be on separate databases , or may each or collectively be spread across multiple databases .","Where the database  and, in particular, the data store , includes panoramic imagery, each panoramic image may include or be constructed from a number of individual images captured and combined to create a panoramic image. The panoramic image may be captured by any of several means and, in particular, may be captured by a special image capture system  mounted on and\/or in a vehicle , (e.g., a bicycle, an automobile, a motorcycle, a cart, a tricycle, a snowmobile, etc.), as depicted in  or, perhaps, carried in backpack. The image capture system  may include a sensor pod  having a number of cameras , each fixed with respect to the sensor pod  and collectively capturing a 360-degree view. In an embodiment of the image capture system , one or more Global Positioning System (GPS) units  operate to determine the location of the image capture system  as each panoramic image is captured, and data from the GPS units  is stored as metadata with the images in the data store . Additionally, one or more range scanners  (e.g., laser range scanners) may measure the distance from the sensor pod  to various objects captured in the panoramic images. Still further, a one or more antennas and receivers  may scan for wireless signals in the area, including mobile telephony signals, GPS signals, and\/or wireless network signals (e.g., IEEE 802.11, IEEE 802.16, etc.). The image capture system  may also include various computer equipment (not shown), including one or more processors, memory subsystems, displays, input devices, etc., which may function to control the sensor pod , capture and store images and metadata, transmit the stored images and metadata to another device (e.g., the data store ), etc.","With reference now to , a person using the 3D modeling system to model a structure may create a 3D model of the structure.  depicts a 3D model  of a structure  set on a ground plane  in a 3D program space , as the 3D model  might be depicted on the display . The structure  includes generally planar sides , , , and  that are generally perpendicular to the ground plane  which is coincident with a bottom face  of the 3D model . A planar surface, generally parallel to the ground plane  (and to the bottom face ), forms a top face  of the structure. It is worth noting the ground plane  need not necessarily correspond to the bottom face  of the 3D model , but could instead correspond to the top face  of the 3D model , one of the sides , , , and  of the 3D model  and, in fact, need not correspond to any side or face of the 3D model . However, throughout this specification the 3D model  is described as modeling an above-ground section of a real-world building and, thus, for ease, the ground plane  will be described as corresponding to the bottom face  of the 3D model .","The system  may allow a user to select a surface of the 3D model  to texture. As used in this specification, the word \u201ctexture\u201d refers to application to a surface of detail conveying texture. For example, a surface of a 3D model generally may be textured by applying to the selected surface a brick pattern, a concrete pattern, a glass fa\u00e7ade pattern, a wood siding pattern, etc. The 3D modeling application  and, in particular, a photo texturing routine  described in this specification, textures a surface by applying to the selected surface of the modeled structure  an image of a corresponding surface of a real-world structure (not shown). Throughout this specification, this is referred to as photo-texturizing (or photo-texturing). For example, after photo-texturing, the side  of the 3D model  depicted in  may appear as depicted in . Generally, photo texturing requires acquiring an image of the surface to which the photo texture is to be applied (and, possibly, manipulating an apparent viewing angle of the image to match that of the 3D model ), and selecting the portion of the acquired image to apply to the selected surface. This is often a tedious, time-consuming process.","The database  may, in some instances, store geotagged imagery that may be used by a 3D modeling application  for photo texturing. For instance, the database  may store imagery with associated geotagging metadata that indicates the location at which each image was captured. The geotagging metadata may be stored and\/or searchable by latitude and longitude, by address, by reference to a particular point or feature (e.g., \u201c100 feet north of the intersection of Main Street and First Avenue\u201d), or by any other desirable reference system. This may allow a user to search for a previously-captured image of the structure modeled by the 3D model , rather than requiring the user to acquire the image personally. The user may then select the portion of the image to apply to the selected surface as a texture, manipulate the perceived viewing angle of the image, and apply the selected portion of the image to the selected surface. This remains a tedious (if slightly less time-consuming) process.","If geotagged imagery of the 3D model  exists in the database , the photo texturing routine  may, in some instances, be able to apply the images as texture to the 3D model . To do this, the photo texturing routine  must have information about the location of the 3D model  in the 3D program space  of the modeling application . That is, the 3D model  must be geolocated within the modeling application . Once the 3D model  is geolocated such that the modeled structure's real-world location is known, the photo texturing routine  may access the database , determine whether appropriate imagery exists for the modeled structure, select an appropriate image or portion of an image to apply as texture to a selected surface of the 3D model , and apply the selected image or portion of an image to the selected surface. This process is described in greater detail below.","Referring again to , the 3D program space  may be depicted on the display  as including having an origin  at the intersection of X, Y, and Z axes , , and , respectively. The 3D model  may be disposed within the 3D program space  such that an intersection of three planes (e.g., the sides  and  and the bottom face ) is situated at the origin , as depicted in . Of course, while it may be preferable to some users for the 3D-model  to have a surface (e.g., the bottom face ) coincident with the plane defined by the X- and Y-axes  and , respectively, such alignment is not required.","The 3D program space  may be geolocated, before or after creating the 3D model , to establish the real-world location of the modeled structure. For example, a user may operate a user-interface of the modeling application  to cause the modeling application  to commence a geolocation routine  (see ) (e.g., by using an input device to \u201cclick\u201d a \u201cbutton\u201d in the user-interface, to select a \u201cgeolocate\u201d or \u201cadd location\u201d menu item, etc.). The geolocation routine  may cause the display  to display a dialog box, to display a new \u201cwindow,\u201d to change to display a different screen, etc. The geolocation routine  may be part of or separate from the modeling application , may be executed locally on the workstation  or remotely on the server , and may use data stored on the workstation  or data stored in the server  or the database . In any event, the geolocation routine  allows the user to geolocate the 3D model  in the 3D program space  of the modeling application . Specifically, the geolocation routine  accesses a database (such as, for example, the database ) of geotagged imagery and\/or maps, and displays the imagery and\/or maps on the display .",{"@attributes":{"id":"p-0062","num":"0061"},"figref":["FIG. 3","FIG. 3"],"b":["130","20","37","130","132","134","136","132","132","132","132","138","140","138","142","138","138","140","132","132"]},"For any particular image and\/or map displayed in the display area , each pixel may be mapped to a set of coordinates to indicate the location of the pixel. The data for given pixels may be stored in the database  or may be calculated with reference to one or more reference points upon accessing the image and\/or map. For instance, the image depicted in the display area  of  may have associated with it in the database  metadata indicating the coordinates of one or more reference points (e.g., a center point, a corner, multiple corners) (not shown), the resolution of the image (e.g., 800 pixels\u00d7350 pixels), the change per pixel in each coordinate value, etc. In this manner, the workstation  and\/or the server  may provide to the geolocation routine  the geolocation of each pixel. The coordinates may be selected from any coordinate system desirable for a mapping the available area (e.g., the surface of the Earth) and, in at least some embodiments, the set of coordinates comprises a latitude and a longitude. As the image and\/or map is zoomed in (i.e., as the resolution of the displayed area is increased, for example, by manipulating the zoom control ), the accuracy and\/or the precision of the set of coordinates associated with each pixel in the display area  may increase. At some minimum zoom level, the metadata for a particular image and\/or map may include multiple sets of coordinates. For example, when the zoom control  is adjusted to \u201cstreet level\u201d (i.e., when individual streets are visible in the display area ), each pixel may be associated with a first set of coordinates (e.g., a latitude and a longitude) and a second set of coordinates (e.g., a nearest street address).","The navigation bar  on the dialog box  may include a search field , into which a user may type a search term. For example, the search term could be an address, an intersection, a state, a country, a landmark name, a business name, a latitude and longitude, etc. After entering a search term into the search field , the user may cause a search to be executed by, for instance, pressing the \u201cEnter\u201d key on a keyboard, or using a pointing device (e.g., a mouse) to \u201cdepress\u201d a \u201cSearch\u201d button .  depicts the dialog box  displaying an exemplary search result. The search field  has in it an address . The display area  in  depicts the area around the address  entered into the search field . A building  having the same address as the address  entered into the search field  is marked with a star .","Once the desired area is displayed in the display area , a user may geolocate one or more points of the 3D model  in the 3D program space . In some embodiments, the user may select in the dialog box a particular point, such as a corner  of the building , and may associate the point with a point in the 3D program space . For example, the user may associate the corner  with the origin , with a point on the 3D model , or with any point in the 3D program space . By doing so, the set of coordinates of the selected point in the display  becomes associated with the selected point (e.g., the origin ) in the 3D program space . In some embodiments, the user may instead activate a user interface control  (e.g., using an input device to select a button) and select a region of the display area  to import into the 3D program space . For example, and with reference to , activation of the control  may cause the dialog box  to display, in the same dialog box  or in a separate dialog box (not shown) a selection control . The selection control  may be, by way of example, a bounded area  adjustable by moving one or more of a plurality of vertices , each of which vertices may have associated with it a set of coordinates. Alternatively, the selection control  may be a fixed-shape (e.g., square, rectangular, circular, etc.) selection window having an adjustable size or even having a fixed size. In any event, controls  and  may, respectively, allow the user to import into the 3D program space  the region indicated (e.g., by the bounded area ) or to cancel the selection and to return, for example, to the display depicted in .","Activation of the control  may import the bounded area  into the 3D program space  of the modeling application , as illustrated in .  depicts an application window  that may be displayed on the display  during execution of the modeling application . The application window  depicts the 3D program space  with the X-, Y-, and Z-axes, , , and , respectively, meeting at the origin . The bounded area  is copied into the 3D program space  and may be displayed as a two-dimensional image (i.e., a plane). By default, the plane defined by the two-dimensional image of the bounded area  may be placed within the 3D program space  such that the plane is coincident with the plane defined by the X- and Y-axes  and , and may optionally be placed such that a center (not shown) of the bounded area  is coincident with the origin . In some embodiments, the user may be able to realign the origin  and\/or the bounded area  to facilitate easier creation (or manipulation) of the 3D model . In , for example, the bounded area  and\/or the origin  are aligned such that the corner  of the building  is aligned with the origin .","In some embodiments, every point within the bounded area  may have associated with it metadata indicating an altitude. The altitude may be relative to mean sea level, relative to the points next to it, relative to the center of the Earth, etc. Regardless of how the altitudes of various points within the bounded area are indicated, the bounded area  may be displayed selectively as either a 2D plane (as depicted in ) or a 3D topography. Displaying the bounded area  as a 3D topography may allow the user to create the 3D model  with greater accuracy. In any event, the 3D model  may be constructed such that a base  of the 3D model  is coincident with the depiction of the building  and with the 2D planar depiction or 3D topography of the bounded area . Alternatively, the already-constructed 3D model  could be aligned with the depiction of the building  and with the 2D planar depiction or 3D topography of the bounded area .","Of course, with even a single point in the 3D program space  associated with a set of coordinates, every other point in the 3D program space  may be located referentially and assigned a corresponding set of coordinates. Thus, if the bounded area  is a square measuring 1000 feet per side, and the origin  is located at [X, Y, Z]=[150, 300, 0], one can determine that the vertices  of the bounded area  will be located at [\u2212150, \u2212300, 0], [850, \u2212300, 0], [850, 700, 0], and [\u2212150, 700, 0]. Likewise, every point within the 3D program space  may be determined relative to a point having known coordinates.","It should be understood that multiple coordinate systems may be in use within the system . For instance, the set of coordinates associated with the bounded area  in the database  and\/or with each pixel within the display area , may comprise a latitude value and a longitude value (and possibly an altitude value), while the 3D program space  in the application  may associate each point therein with a set of X, Y, and Z coordinates. Nevertheless, so long as a single point within the 3D program space  is associated with a corresponding latitude and longitude, the corresponding latitude and longitude of every point within the 3D program space  may be calculated. Accordingly, the location of every point on any surface of the 3D model  may be specified by a first set of coordinates in the 3D program space  and by a second set of coordinates indicating a real-world position.",{"@attributes":{"id":"p-0070","num":"0069"},"figref":["FIG. 7","FIG. 7"],"b":["174","174","160","176","120","178","174","174","106","174","106","122","124","106","174","106","122","120","124","120","126","174","106","126","174","120","120","100"]},"Once a geolocated, 3D model  is created in the 3D program space  of the modeling application , the user may select (e.g., using an input device such as a mouse, a touch screen, a stylus, etc.) a desired surface  to photo-texture as depicted in . The application  (which may be executing on the processor  or on a processor of the server ) receives the selection. The selected surface  may have vertices , , , and  (if the selected surface  is a rectangle, for example) and a center point , each of which may be specified by a set of coordinates in the 3D space . In any event, having selected the surface , the user may activate a control causing the application  to execute the photo texturing routine . For example, in , the user is depicted selecting a command  from a series of pull-down menu items , as one of ordinary skill in the art would readily understand. Of course, the modeling application  could, additionally or alternatively, include a button control (not shown) that would cause execution of the photo texturing routine . In some embodiments, the modeling application  may give the user the option to execute a manual version of the photo texturing routine  or an automatic version of the photo texturing routine .",{"@attributes":{"id":"p-0072","num":"0071"},"figref":["FIGS. 8 and 9","FIG. 8"],"b":["200","172","100","220","106","100","106","100","202","100","106","204","100","100","106","201","106","202","100","106","203","172","206","35","208"]},"In any event, the geolocation process (block ; method ) commences when the application  receives a geolocation request (block ) and, in response to receiving the request executes the geolocation routine  and displays a the geolocation dialog box  (block ). As described above, the user may navigate to, or search for, an image (which may be an aerial image) of the desired location (i.e., the location of the real-world structure represented by the 3D model ) (block ). This may involve sending a search term from the routine  to the server , and receiving in response to the search term a corresponding image from the server , which image the server  may retrieve from the database . The user, having located in the geolocation dialog box  the desired region, may manipulate a control in the geolocation routine  to indicate a desire to select a region (block ). In response to receiving the user input (block ), the geolocation routine  may display a selection control (e.g., the bounded area  and the plurality of vertices ) (block ) and the user may manipulate the selection control to specify a desired area. The geolocation routine  may next receive a selection of a region (e.g. the bounded area ) to copy into the 3D program space  of the application  (block ). This may be accomplished, for example, when the user, having adjusted the vertices , indicates that the bounded area  is adjusted according to the user's desire by manipulating a control (e.g., clicking on a button) in the geolocation dialog box  (block ). Having received the selection of a region, the geolocation routine  may copy the selected region of the geolocated image into the 3D program space  of the modeling application .","In embodiments, the geolocated image copied into the 3D program space  includes at least one geolocated point specified by a first coordinate system such as latitude and longitude (and, in some embodiments, altitude). Of course, while the geolocated point(s) are described herein as designated by latitude and longitude, the geolocated point(s) need not be specified by a latitude and longitude, but could instead be specified in any available coordinate system.","To automatically apply a photo texture to the selected surface , the application  and\/or the photo texturing routine  may, perhaps cooperatively: (1) find one or more images of the real-world structure modeled by the 3D model ; (2) select an image of the one or more images that includes the selected surface; (3) select from the selected image a portion corresponding to the selected surface; (4) map the selected portion to the selected surface; and (5) apply the selected portion to the selected surface according to the mapping. Accomplishing these tasks may required that the application  and\/or the photo texturing routine  transform one or more sets of coordinates through several coordinate spaces. That is, the application  and\/or the photo texturing routine  may operate or cooperate to change a point or a set of points from one coordinate system to another coordinate system. As but one example, the origin  depicted in  corresponds to a point [0, 0, 0] in the 3D program space  and may be geolocated as having a latitude Latand a longitude Lon, i.e., specified by a point [Lat, Lon]. A point 100 meters north of the origin , 100 meters east of the origin , and 100 meters above the ground may correspond to a point [100, 100, 100] (assuming that the 3D program space  is measured in meters). That point may correspond to a location having latitude and longitude coordinates [Lat, Lon] where Latis Latplus a latitude increment equal to 100 meters and Lonis Lonplus a longitude increment equal to 100 meters.","Turning now to , a flow chart depicts an exemplary method , executed by the photo texturing routine , for automatically applying a photo texture to a selected surface (e.g., the selected surface ). Of course, to automatically apply a photo texture to the selected surface, the photo texturing routine  attempts to find a suitable image of the selected surface. Specifically, the photo texturing routine  may send one or more search terms or requests to the server  to search geotagged images stored in the database  and, in particular, to search the data store  of images captured from street level.","To determine and select the likely best available image of the selected surface  from the database  (block ), the photo texturing routine  may execute a method  depicted in . The method  begins by calculating in the photo texturing routine  a likely viewing point from which an image of the selected surface  might be captured (blocks ). In some embodiments, the first step in calculating the likely viewing point is calculating, determining, or retrieving a set of coordinates corresponding to the center point  of the selected surface  (block ). In various embodiments, the set of coordinates corresponding to the center point  of the selected surface  may be calculated or determined by the modeling application  and stored for later retrieval and use by the photo texturing routine . However, in some embodiments, the set of coordinates corresponding to the center point  of the selected surface  may be calculated or otherwise determined by the photo texturing routine , and may include first determining for each of the vertices , , , and  a set of coordinates corresponding to the location in the 3D space  of the vertex (block ). Further, in some embodiments, the selected surface  may be represented by a set of vertices (e.g., a set of four vertices) even if the selected surface  is not rectangular. For example, the selected surface  could be a circular surface, specified by four vertices defining a square circumscribing the circular surface.","Referring now to , to determine a likely best available image of the selected surface, the photo texturing routine  may next translate the center point  a predefined distance (e.g. 15 meters) along the normal  of the selected surface  (block ). The photo texturing routine  may then project this translated point  onto the ground plane  (block ) to determine a position  from which an image is likely to have been captured. A set of coordinates in the 3D program space  corresponds to the position . The photo texturing routine  may then transform this set of coordinates from the 3D program space  to a set of coordinates used to geotag the images in the data store  (e.g., latitude, longitude, and altitude) (block ).","Having determined a set of coordinates of the likely viewing point  from which an image of the selected surface  might have been captured, the photo texturing routine  may send to the server  a request to search the data store  for one or more images having geotags indicating they were captured from the points nearest the likely viewing point . In some embodiments, the photo texturing routine  calls a sub-routine (e.g., Javascript\u2122, etc.) that, via an API, requests the server  to find an image captured from a position nearest the calculated likely viewing point  (block ). This may be accomplished, for example, by transmitting to the server  the latitude and longitude coordinates for the calculated likely viewing point  and receiving from the server  in return latitude and longitude coordinates specifying the nearest location for which an image exists in the data store . For example,  depicts a set of points A-J. Each of the set of points A-J may represent a point from which an image stored in the data store  was captured. Each of the images captured from the points A-J may be, for example, a panoramic image capturing a 360-degree view from the respective point A-J, in which case any image captured from a point \u201cin front of\u201d the plane of the surface  (e.g., from the points C-J) may include a view of the surface . Alternatively, each image in the data store  may be a single (i.e., non-panoramic) image, and the data associated with the image may, in addition to indicating the location from which the image was captured, indicate the direction the image capture device was pointed, the zoom level of the image, the size of the viewport, etc. In either event, the sub-routine or the photo texturing routine  may, in some embodiments, evaluate the points A-J to determine whether images stored in the data store  and captured from points equidistant from the likely viewing point  as to respective likelihood that the selected surface  is visible in the image. (For example, the selected surface  would not be visible from the points A or B in , because images captured from these locations were taken from \u201cbehind\u201d the plane of the surface .) In , the point H represents the point nearest to the likely viewing point , and the image in the data store  captured from the point H corresponds to the image captured nearest to the likely viewing point . The photo texturing routine  may store in the memory sub-system  (e.g., in the RAM ) the latitude and longitude coordinates returned by the sub-routine and specifying the nearest point H from which an image exists and\/or may store an identifier of a panoramic image captured from the nearest point H.","In the case of a panoramic image, the photo texturing routine , having received a set of coordinates for and\/or an identifier of a panoramic image captured from a point (e.g., the point H) nearest the likely viewing point , finds the view of the panoramic image that points at, or closest to, the center point  of the selected surface (block ). The view of the panoramic image may be specified by a set of point-of-view (POV) parameters including, by way of example and not limitation, one or more of: a panorama identifier, a panorama location (e.g., latitude and longitude coordinates), a heading, a zoom level, a pitch, etc. Determining the POV parameters may, in some embodiments, require the coordinates of the center point  to undergo a set of transformations as described below with respect to  and, in particular, with respect to blocks -.","Referring to , the photo texturing routine , having received data specifying a set of coordinates for and\/or an identifier of a panoramic image captured from the point H nearest the likely viewing point , and possibly also one or more POV parameters specifying an image taken from the panoramic image and centered about to the center point  of the selected surface , next performs a series of transformations for the coordinates of each of the vertices , , ,  of the selected surface  (blocks -). While the exemplary series of transformations described below is specific to a described embodiment of the 3D modeling system  described herein, it should be understood that in other embodiments, certain ones of the transformations may be omitted and\/or other transformations may be necessary. Moreover, while each of the series of transformations is described separately with respect to the method , the transformations may be combined into a single transformation (i.e., a single equation or matrix operation may accomplish all of the necessary transformations).","In an embodiment, the method  depicted in  returns a result ( at block ) including data specifying (e.g., with an image identifier) a panoramic image captured at the point H nearest to the position , or returns a result specifying coordinates of a point H from which such a panoramic image would have been captured, and the metadata associated with the panoramic image.","In one transformation, the set of coordinates for each of the vertices , , ,  of the selected surface  is transformed from the 3D model space  to a 3D vehicle space , specified by a set of rectangular coordinates [X, Y, Z] ( at block ). As depicted in , the 3D vehicle space  is centered at the sensor pod  when the vehicle  is located at the point H. The 3D vehicle space  is oriented with a positive Z axis  aligned with the direction of travel of the vehicle  (which may be stored as metadata associated with the image captured from the point H, or may be determined by evaluating the locations G,  from which adjacent images were captured), a positive Y axis  extending below the vehicle , and a positive X axis  extending to the right of the vehicle . In this manner, the location of each of the vertices , , ,  of the selected surface  can be determined relative to the sensor pod  at the point H (i.e., the location of the sensor pod  when the desired image was captured). The set of coordinates for each of the vertices , , ,  in the 3D vehicle space  may be stored in the memory sub-system  for later use.","In another transformation, the set of coordinates for each of the vertices , , ,  of the selected surface  is retrieved from the memory sub-system  and each set of coordinates transformed from the 3D vehicle space  to a 2-dimensional (2D) panorama space , specified by a pair of coordinates [P, Y] indicating, respectively, pitch and yaw ( at block ). As depicted in , the 2D panorama space is defined with respect to the sensor pod . The pitch is defined from \u2212\u03c0\/2 to \u03c0\/2, from top to bottom, while the yaw is defined from 0 to 2\u03c0, with \u03c0 indicating the direction of the motion of the vehicle . Thus, with reference to , respectively, the pitch and yaw of each of the vertices , , ,  of the selected surface  may be determined, specifying the coordinates on an imaginary sphere at which a line A, A, A, A, from each of the vertices , , ,  to the sensor pod  intersects the sphere. The set of coordinates for each of the vertices , , ,  in the 2D panorama space  may be stored in the memory sub-system  for later use.","In still another transformation, the set of coordinates for each of the vertices , , ,  of the selected surface  is retrieved from the memory sub-system  and each set of coordinates transformed from the 2D panorama space  to a 2D compass space  ( at block ), specified by the same pair of coordinates as the 2D panorama space , but with a yaw value defined with respect to north, instead of with respect to the direction the vehicle  is moving. For example,  shows the yaw values as in , with it indicating north. Thus, if all panoramic images in the data store  are stored such that a rotational parameter (i.e., yaw) is relative to north, the transformation accomplished in block  provides for each of the vertices , , ,  a yaw value relative to north. The set of coordinates for each of the vertices , , ,  in the 2D compass space  may be stored in the memory sub-system  for later use.","Referring for a moment to , sets , ,  of coordinates A, B, C, D are shown for a special case when the point H from which an image was captured is the same as the calculated likely viewing point , with the sensor pod  located 2.5 meters above the ground plane , and the selected surface  having a width of 30 meters and a height of 5 meters. Under these special conditions, the vertices  and  are coplanar with the likely viewing point , and the center of the sensor pod  is coincident with the point  translated from the center point  along the normal  of the surface . , meanwhile, illustrates the calculation of the angles in 2D panorama space  and 2D compass space , which is readily understood with basic trigonometric principles.","In another transformation still, the set of coordinates for each of the vertices , , ,  of the selected surface  is retrieved from the memory sub-system  and each set of coordinates transformed from the 2D compass space  to a 2D viewport space  ( at block ), specified by X and Y coordinates indicating a point on an image captured at the point H. The image may be one of the images forming or included in the panoramic image captured at the point H and, in particular, may be the image specified by the POV parameters. X- and Y-axes  and  extend, respectively, along a top side  and a left side  of the image, from an origin  at a top left corner  of each image.  depict potential 2D images ,  that may correspond to images captured from two different angles. For instance, the image  may correspond to a portion of a panoramic image captured at the point F, while the image  may correspond to a portion of a panoramic image captured at the point H. The transformation from 2D compass space  to 2D viewport space  (block ) may be based, in part, on the zoom level of the image, the angle of the particular camera  that captured the image, on POV parameters for the particular image, on metadata in the data store  indicating the bounds of the image in 2D compass space, 2D panorama space, etc., the type of lens used to capture the image, data captured by the range scanners , or some combination of these. The set of coordinates for each of the vertices , , ,  in the 2D viewport space  may be stored in the memory sub-system  for later use.","The photo texturing routine , having determined a panoramic image including all of the vertices , , ,  (or a location from which such a panoramic image was captured), determined a POV image that includes the vertices , , , , and determined (through the various transformations -) the coordinates of the vertices , , ,  in the image, may retrieve some or all of the data from the memory sub-system  and or the data store . Specifically, the photo texturing routine  may retrieve from the memory sub-system  a unique identifier of the panoramic image or a set of coordinates specifying the location from which the panoramic image was captured. The photo texturing routine  may, using the identifier or the coordinates, request from the database , via the server  and the API, the panoramic image corresponding to the identifier or the coordinates. In some embodiments, the photo texturing routine  may request via the API a particular one of the images included in the panoramic image or, alternatively, may specify POV parameters (e.g., a value of pitch and\/or yaw in either the 2D panorama space  or the 2D compass space ), and may receive from the database , via the server  and the API, an image corresponding to the request.","Thereafter, the photo texturing routine  may define an area of the received image according to the coordinates of the vertices , , ,  in the 2D viewport space .  depicts one such area  in the image , while  depicts another such area  in the image . The photo texturing routine  may capture the area ,  ( at block ) and store the corresponding data in the memory sub-system .","In some embodiments, the photo texturing routine  may evaluate whether any one image in the panoramic image captured at the point H includes all four of the vertices , , , . If, for example, the vertices  and  are in a first image of the panoramic image (or a first point of view of the panoramic image) and the vertices  and  are in a second image of the panoramic image (or a second point of view of the panoramic image), the photo texturing routine  may search for an image captured from a more distant point, such as from one of the points F, G, or , or may seek a point of view that includes all four of the vertices , , ,  (e.g., a point of view with a wider field of view). Alternatively, in some embodiments, the photo texturing routine  may retrieve from the data store  both the first and second images from the panoramic images and capture respective portions of each.","The final steps in the method  of for automatically applying a photo texture to the selected surface  are related to UV mapping. UV mapping is widely known process for applying a texture to a 3D object and, accordingly, will not be described in great detail herein. Briefly, however, the process of UV mapping involves applying a 2D mesh to a 3D object (e.g., the model  in the 3D model space ), unwrapping the mesh, creating the texture on the mesh, and applying the texture to the 3D object. Thus, the photo texturing routine  may map the coordinates, in viewport space, of the captured area ,  to the corresponding vertices in UV space ( at block ) and apply the captured area ,  to the selected surface  of the 3D model  in the modeling application , as depicted, for example, in .","In some embodiments, the 3D modeling application  may call (i.e., instantiate) the photo texturing routine  multiple times, successively or concurrently, to photo texture multiple surfaces of the 3D model . For example, prior to initiating the automatic photo texturing routine  ( at block ), a user may select multiple surfaces of the 3D model  ( at block ). Alternatively, the user may select the 3D model  before initiating the automatic photo texturing routine , and the routine  may automatically photo texture every surface of the 3D model  for which imagery is available within the database .","Throughout this specification, plural instances may implement components, operations, or structures described as a single instance. Although individual operations of one or more methods are illustrated and described as separate operations, one or more of the individual operations may be performed concurrently, and nothing requires that the operations be performed in the order illustrated. Structures and functionality presented as separate components in example configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements fall within the scope of the subject matter herein.","For example, the network  may include but is not limited to any combination of a LAN, a MAN, a WAN, a mobile, a wired or wireless network, a private network, or a virtual private network. Moreover, while only one workstation  is illustrated in  to simplify and clarify the description, it is understood that any number of workstations  are supported and can be in communication with the server .","Additionally, certain embodiments are described herein as including logic or a number of components, modules, routines, applications, or mechanisms. Applications or routines may constitute either software modules (e.g., code embodied on a machine-readable medium or in a transmission signal) or hardware modules. A hardware module is tangible unit capable of performing certain operations and may be configured or arranged in a certain manner. In example embodiments, one or more computer systems (e.g., a standalone, client or server computer system) or one or more hardware modules of a computer system (e.g., a processor or a group of processors) may be configured by software (e.g., an application or application portion) as a hardware module that operates to perform certain operations as described herein.","In various embodiments, a hardware module may be implemented mechanically or electronically. For example, a hardware module may comprise dedicated circuitry or logic that is permanently or semi-permanently configured (e.g., as a special-purpose processor, such as a field programmable gate array (FPGA) or an application-specific integrated circuit (ASIC)) to perform certain operations. A hardware module may also comprise programmable logic or circuitry (e.g., as encompassed within a general-purpose processor or other programmable processor) that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.","Accordingly, the term \u201chardware module\u201d should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in a certain manner or to perform certain operations described herein. As used herein, \u201chardware-implemented module\u201d refers to a hardware module. Considering embodiments in which hardware modules are temporarily configured (e.g., programmed), each of the hardware modules need not be configured or instantiated at any one instance in time. For example, where the hardware modules comprise a general-purpose processor configured using software, the general-purpose processor may be configured as respective different hardware modules at different times. Software may accordingly configure a processor, for example, to constitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.","Hardware modules can provide information to, and receive information from, other hardware modules. Accordingly, the described hardware modules may be regarded as being communicatively coupled. Where multiple of such hardware modules exist contemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) that connect the hardware modules. In embodiments in which multiple hardware modules are configured or instantiated at different times, communications between such hardware modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware modules have access. For example, one hardware module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further hardware module may then, at a later time, access the memory device to retrieve and process the stored output. Hardware modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information).","The various operations of example methods described herein may be performed, at least partially, by one or more processors that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations. Whether temporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions. The modules referred to herein may, in some example embodiments, comprise processor-implemented modules.","Similarly, the methods or routines described herein may be at least partially processor-implemented. For example, at least some of the operations of a method may be performed by one or processors or processor-implemented hardware modules. The performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the processor or processors may be located in a single location (e.g., within a home environment, an office environment or as a server farm), while in other embodiments the processors may be distributed across a number of locations.","The one or more processors may also operate to support performance of the relevant operations in a \u201ccloud computing\u201d environment or as a \u201csoftware as a service\u201d (SaaS). For example, at least some of the operations may be performed by a group of computers (as examples of machines including processors), these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., application program interfaces (APIs)).","The performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the one or more processors or processor-implemented modules may be located in a single geographic location (e.g., within a home environment, an office environment, or a server farm). In other example embodiments, the one or more processors or processor-implemented modules may be distributed across a number of geographic locations.","Some portions of this specification are presented in terms of algorithms or symbolic representations of operations on data stored as bits or binary digital signals within a machine memory (e.g., a computer memory). These algorithms or symbolic representations are examples of techniques used by those of ordinary skill in the data processing arts to convey the substance of their work to others skilled in the art. As used herein, an \u201calgorithm\u201d is a self-consistent sequence of operations or similar processing leading to a desired result. In this context, algorithms and operations involve physical manipulation of physical quantities. Typically, but not necessarily, such quantities may take the form of electrical, magnetic, or optical signals capable of being stored, accessed, transferred, combined, compared, or otherwise manipulated by a machine. It is convenient at times, principally for reasons of common usage, to refer to such signals using words such as \u201cdata,\u201d \u201ccontent,\u201d \u201cbits,\u201d \u201cvalues,\u201d \u201celements,\u201d \u201csymbols,\u201d \u201ccharacters,\u201d \u201cterms,\u201d \u201cnumbers,\u201d \u201cnumerals,\u201d or the like. These words, however, are merely convenient labels and are to be associated with appropriate physical quantities.","Unless specifically stated otherwise, discussions herein using words such as \u201cprocessing,\u201d \u201ccomputing,\u201d \u201ccalculating,\u201d \u201cdetermining,\u201d \u201cpresenting,\u201d \u201cdisplaying,\u201d or the like may refer to actions or processes of a machine (e.g., a computer) that manipulates or transforms data represented as physical (e.g., electronic, magnetic, or optical) quantities within one or more memories (e.g., volatile memory, non-volatile memory, or a combination thereof), registers, or other machine components that receive, store, transmit, or display information.","As used herein any reference to \u201cone embodiment\u201d or \u201can embodiment\u201d means that a particular element, feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment. The appearances of the phrase \u201cin one embodiment\u201d in various places in the specification are not necessarily all referring to the same embodiment.","Some embodiments may be described using the expression \u201ccoupled\u201d and \u201cconnected\u201d along with their derivatives. For example, some embodiments may be described using the term \u201ccoupled\u201d to indicate that two or more elements are in direct physical or electrical contact. The term \u201ccoupled,\u201d however, may also mean that two or more elements are not in direct contact with each other, but yet still co-operate or interact with each other. The embodiments are not limited in this context.","As used herein, the terms \u201ccomprises,\u201d \u201ccomprising,\u201d \u201cincludes,\u201d \u201cincluding,\u201d \u201chas,\u201d \u201chaving\u201d or any other variation thereof, are intended to cover a non-exclusive inclusion. For example, a process, method, article, or apparatus that comprises a list of elements is not necessarily limited to only those elements but may include other elements not expressly listed or inherent to such process, method, article, or apparatus. Further, unless expressly stated to the contrary, \u201cor\u201d refers to an inclusive or and not to an exclusive or. For example, a condition A or B is satisfied by any one of the following: A is true (or present) and B is false (or not present), A is false (or not present) and B is true (or present), and both A and B are true (or present).","In addition, use of the \u201ca\u201d or \u201can\u201d are employed to describe elements and components of the embodiments herein. This is done merely for convenience and to give a general sense of the description. This description should be read to include one or at least one and the singular also includes the plural unless it is obvious that it is meant otherwise.","Still further, the figures depict preferred embodiments of a photo texturing system for purposes of illustration only. One skilled in the art will readily recognize from the following discussion that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles described herein.","Upon reading this disclosure, those of skill in the art will appreciate still additional alternative structural and functional designs for a system and a method for photo texturing 3D models through the disclosed principles herein. Thus, while particular embodiments and applications have been illustrated and described, it is to be understood that the disclosed embodiments are not limited to the precise construction and components disclosed herein. Various modifications, changes and variations, which will be apparent to those skilled in the art, may be made in the arrangement, operation and details of the method and apparatus disclosed herein without departing from the spirit and scope defined in the appended claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1B"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 2B","FIG. 2A"]},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 5","FIG. 3"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 12","FIG. 11"]},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 13","FIG. 11"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 14A"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 14B","FIG. 14A"]},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 15A","FIGS. 14A and 14B"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 15B","FIG. 15A"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 15C","FIGS. 15A and 15B"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 15D","FIG. 15C"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 16","FIGS. 15A and 15B"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 17","FIGS. 14A through 16"]},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 18A"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 18B"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 19"}]},"DETDESC":[{},{}]}
