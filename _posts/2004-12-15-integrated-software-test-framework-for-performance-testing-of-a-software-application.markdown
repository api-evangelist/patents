---
title: Integrated software test framework for performance testing of a software application
abstract: In accordance with the present invention, the above and other problems are solved by an integrated test framework that can receive measurement commands from a test script, automatically configure and execute any performance metrics required to effectuate the measurements and further manage the operation of the software to be tested. The integrated test framework can interpret script commands directed at any testing tool in a testing tool library accessible to the framework. Furthermore, the integrated test framework can identify and utilize code markers during the testing of software applications that utilize embedded code markers as well as identifying script markers. Thus, the integrated framework allows testing scenarios to be created incorporating both code markers and script markers to control performance testing of the executing software application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07415635&OS=07415635&RS=07415635
owner: Microsoft Corporation
number: 07415635
owner_city: Redmond
owner_country: US
publication_date: 20041215
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["TECHNICAL FIELD","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["This application relates generally to testing software and more particularly to an integrated software test framework.","Software, such as programs or applications, must be tested after each substantial revision to determine if the changes in the new version might have detrimentally affected the operation of the software due to unanticipated conflicts or errors. Software testers utilize a number of testing tools to evaluate the performance of new versions and to identify the source of any problems they may find. Examples of commonly used testing tools include a benchmarking tool (a tool that can determine the time durations of a specific action, call, or other identifiable point in the execution of a software application occurs), a profiling tool (a tool that identifies the sequence of the calls or application programming interfaces (APIs) that are being used, what functions are being called, and when they are used during execution of a software application), a working set tool (a tool that tracks various measurements related to memory, disk cpu usage during execution of a software application), a communication tracking tools (like remote procedure call, or RPC, Bytes over the Wire\u201d\u2014a tool that counts the number of bytes transmitted over the network, etc), a file monitoring tool (a tool that tracks the creation and use of files during the execution of a software application), a registry monitor (a tool that records what registry components were accessed) and a network latency tool (a tool that measures the time it takes for communication between a sender and receiver over the network). In addition to these commonly used tools, a software tester may also use other tools that are specific to the software being tested.","Testing software is a tedious process that must be repeated after each revision. Oftentimes, performance testing starts with a benchmarking test. If the results of the benchmarking test indicates that performance of the software is not as anticipated, then additional software tests are typically performed, this time with one or more testing tools until the source of the problem is identified so that the problem can be corrected.","Each test of the software requires the tester to develop a test scenario in which the tester identifies each testing tool or tools to be used, what data each tool will track, and what operational scenario the software should perform. For example, take the situation in which a software tester finds that a new version of a word processor application fails a benchmarking test indicating that the combined operation of saving a document and closing the application is too slow. Next the tester will have to develop a second test, in which the time the \u201csave\u201d command is received, the time the document is saved and the time the application closes are recorded. After these times are recorded, a third test may be developed and run to that identify what calls are being made and the time duration of each call. This test will identify the call that is the source of the delay. After the delaying call is identified, additional tests focused on the call must be developed in order to ultimately diagnose the source of the delay.","To be used, testing tools typically require the tester to write a test script that identifies the software to be tested, the scenario to be tested, and what tests to perform. More importantly, the test script must also configure the testing computer to support the particular testing tool required, register and execute the testing tool and, if necessary, identify the exact measurements desired. In order to do this, a tester must have knowledge of the specific configuration requirements for each testing tool and the testing computer. The tester must also understand the specific script commands necessary for each testing tool.","The test script is then executed and the test data is saved or printed to the screen as directed in the script. In the above example, the test script for the second test would have been written that identified the application and included the \u201cSave and Exit\u201d command. The script would also have included a command to initialize the benchmarking testing tool and commands to record the time at the save call, the receipt of save confirmation, and at the application close.","In addition to the commands written in a test script (e.g., initialize a testing tool, run a software application and execute a specified series of actions, record the time of an action, or identify the active objects during a specific period), a test will include \u201cmarkers\u201d that identify points during the execution of the software application. Markers are typically used as part of a command that directs a testing tool to take a measurement, or begin or end taking measurements. There are two types of markers: code markers and script markers.","To assist testers, some software applications are written with testing in mind and include embedded \u201ccode markers\u201d that a tester can use when analyzing performance. For example, common benchmarking points in a software such as \u201creceipt of document save command\u201d, \u201creceipt of print command\u201d, \u201creceipt of close command\u201d, \u201creceipt of document open command\u201d, as well as internal status code markers such as \u201cdocument saved\u201d, \u201cprint complete\u201d, \u201cdocument closed\u201d, etc. may be provided in a software application to facilitate testing. Code markers may be used in a test script to direct the measurements taken by a testing tool, but only if the script and testing tool are designed to take advantage of them. Code markers are very helpful as they provide very exact feedback from known points in the software application itself. However, these code markers are typically sparingly used as they remain in the software after distribution and increase the amount of code that must be stored.","Script markers are markers that can be placed in the script and that identify points during the execution of an application without using internally embedded markers. Script markers are not as precise as code markers as there may be a delay between an actual occurrence of an action and the detection of the action by the testing tool. However, script markers are very useful in situations where there is no code marker or in situations where a scenario is being tested involving the interaction between two software applications which have the same code markers.","The drawbacks of the current software testing methods are many. First, testing is very time consuming because current methods are iterative and, oftentimes each iteration involves the development and execution of a separate script. In addition, a test script must be written for each metrics (like benchmark, working set etc. . . . ) by the tester and executed separately and each time the test script is executed the software being tested is executed. Another drawback is that the test results must be correlated and evaluated by hand to determine where the problem is. Another drawback is that if the testing tool is not written to utilize code markers, then only script markers may be used. Yet another drawback is that if the problem being diagnosed is intermittent, then one or more of the separate executions may not exhibit the problem. A similar problem occurs if the underlying execution framework changes behavior (i.e., is slower in execution of the application) for external reasons between different tests, which can possibly lead to a misdiagnosis of the problem.","In accordance with the present invention, the above and other problems are solved by an integrated test framework that can receive measurement commands from a test script, automatically configure and execute any performance metrics required to effectuate the measurements and further manage the operation of the software to be tested. The integrated test framework can interpret script commands directed at any testing tool in a testing tool library accessible to the framework. Furthermore, the integrated test framework can identify and utilize code markers during the testing of software applications that utilize embedded code markers as well as identifying script markers. Thus, the integrated framework allows testing scenarios to be created incorporating both code markers and script markers to control performance testing of the executing software application.","In accordance with still other aspects, the present invention relates to a method of testing a software application using an integrated test framework. The method includes executing a test script that identifies the software application to be tested and a plurality of measurements, wherein each measurement is associated with one of a set of testing tools. In response to executing the test script, the software application is executed, such as by loading it into memory and\/or preparing it to perform the scenario to be tested, and the integrated test framework is initiated, again such as by instantiating an object of the test framework class or executing\/loading into memory a test framework software application. The integrated test framework then initiates each of the testing tools associated with the plurality of measurements identified in the test script and directs the initiated testing tools to perform their associated measurements. The testing tools perform the measurements and return data to the integrated test framework. The integrated test framework receives the data from testing tools performing measurements and stores data generated from the received data as a data collection associated with the test script.","In accordance with still other aspects, the present invention relates to a second method of testing a software application with an integrated testing framework. The method includes receiving, by the integrated testing framework, one or more commands to perform measurements in conjunction with execution of the software application. The integrated test framework identifies one or more performance testing tools from a set of testing tools, the identified one or more testing tools necessary to perform the measurements and initiates them.","In accordance with other aspects, the present invention relates to a software testing system that includes a plurality of testing tools and an integrated testing framework. The integrated testing framework operable to identify testing tools from measurement requests in a test script and execute the identified testing tools based on the measurement requests. The integrated testing framework being further operable to receive data from each of the testing tools, collect the data into a data collection, and stores the data collection in a data structure.","The invention may be implemented as a computer process, a computing system or as an article of manufacture such as a computer program product or computer readable media. The computer program product may be a computer storage media readable by a computer system and encoding a computer program of instructions for executing a computer process. The computer program product may also be a propagated signal on a carrier readable by a computing system and encoding a computer program of instructions for executing a computer process.","These and various other features as well as advantages, which characterize the present invention, will be apparent from a reading of the following detailed description and a review of the associated drawings.","The present invention will now be described more fully hereinafter with reference to the accompanying drawings, in which embodiments of the invention are shown. This invention may, however, be embodied in many different forms and should not be construed as limited to the embodiments set forth herein; rather, these embodiments are provided so that this disclosure will be thorough and complete, and will fully convey the scope of the invention to those skilled in the art. Like numbers refer to like elements throughout.","In general, embodiments of the present invention may be considered as an integrated test framework that allows software testers to measure the performance of any given scenario from a single test script. Software performance testing refers to acts of gathering data on performance metrics related to the operation of the software application. Software performance metrics include the time specific actions, calls, or transmissions were made, what actions, calls, transmissions were made, what components such as objects and data elements were utilized, what APIs were called and when, what hardware resources, such as memory and data storage were utilized, and any other aspect of the operation of the software application to be tested.",{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 1","FIG. 1"],"b":["100","102","104","110","112","114","108","102","104"]},"Execution of the test script  causes the execution of the software application . If the test script identifies a series of operations to be performed by the software application , such as open, print, and close a document scenario, these operations are executed as directed by the test script. Thus, the test script  may cause the software application  to perform any scenario of interest to the tester. In some embodiments, the software application  may need to be modified to allow for control via the test scripts, a process referred to as \u201cautomation.\u201d","In an embodiment, the computer architecture  includes an integrated test framework . The test script  causes the execution of the integrated test framework . The integrated test framework  could alternatively be referred to as a \u201ctest management application\u201d or a \u201ctest manager.\u201d The integrated test framework  may be considered an independent test manager software application that can configure, register, execute, and may control the operation of the available independent testing tools , , . The integrated test framework  is referred to herein as a \u201cframework\u201d rather than an application to differentiate it from the software application  being tested and also because in most of the embodiments described the integrated test framework  operates in the background, without direct user interaction, for the purpose of managing the operations of testing tools , ,  during the execution of the software application  being tested.","Generally, testing tools refers to software applications designed to gather performance metrics from another software application, such as software application . Testing tools , ,  may be executed concurrently with the software application  to be tested or may be executed before or after and act on the software application  to be tested.  illustrates three exemplary testing tools, a benchmarking tool , a profiling tool , and a RPC tool . However one skilled in the art will understand that the library  may include any software testing tool, such as for example a file monitor or a registry module (not shown).","The integrated test framework  can also interpret script markers identifying measurement commands within the test script  to identify and execute any one or more of the testing tools , ,  contained in the testing tool library . In order to do this, the integrated test framework  is also capable of configuring the testing computer (not shown) to support the execution of any of the testing tools , ,  necessary to perform the measurements identified in the test script . The integrated test framework  can further interpret script markers in the test script to cause any executing testing tools , ,  to measure any performance metric or perform any test action that they are capable of.","The integrated test framework  is further capable of receiving any data output from any executing testing tools , , , translating the data into a common form, and collecting the data into a single test data collection , which may then be displayed to a user, stored in a data store, or analyzed by the integrated test framework .","Analysis of the data in the test data collection  by the integrated test framework  may be performed automatically or in response to information provided in the test script. Based on the results of the analysis, additional testing may be performed such as repeating execution of the software application  with a different set of measurements.","An embodiment of a suitable operating environment in which the present invention may be implemented is shown in . The operating environment is only one example of a suitable operating environment and is not intended to suggest any limitation as to the scope of use or functionality of the invention. Other well known computing systems, environments, and\/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and the like.","With reference to , an exemplary computing environment for implementing the embodiments of the present invention includes a computing device, such as computing device . In its most basic configuration, computing device  typically includes at least one processing unit  and memory . Depending on the exact configuration and type of computing device , memory  may be volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.), or some combination of the two. This most basic configuration of the computing device  is illustrated in  by dashed line . Additionally, device  may also have additional features\/functionality. For example, device  may also include additional storage (removable and\/or non-removable) including, but not limited to, magnetic or optical disks or tape. Such additional storage is illustrated in  by removable storage  and non-removable storage . Such computer storage media includes volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules, or other data. Memory , removable storage , and non-removable storage  are all examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage, other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by device  and processor . Any such computer storage media may be part of device .","Device  may also contain communications connection(s)  that allow the device to communicate with other devices. Communications connection(s)  is an example of communication media. Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared, and other wireless media.","Device  may also have input device(s)  such as keyboard, mouse, pen, voice input device, touch input device, etc. Output device(s)  such as a display, speakers, printer, etc. may also be included. These devices, either individually or in combination can form a user interface. All these devices are well know in the art and need not be discussed at length here.","Computing device  typically includes at least some form of computer readable media. Computer readable media can be any available media that can be accessed by processing unit . By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules, or other data. Combinations of the any of the above should also be included within the scope of computer readable media.","Computer storage media includes, but is not limited to, RAM, ROM, EPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to store the desired information and that can be accessed by the computing device .","Communication media typically embodies computer-readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared, and other wireless media. Combinations of any of the above should also be included within the scope of computer-readable media. Computer-readable media may also be referred to as computer program product.",{"@attributes":{"id":"p-0041","num":"0040"},"figref":["FIG. 3","FIG. 1","FIG. 3","FIG. 1"],"b":["100","100","102","104","102","106","108","110","112","114"]},"The software application  is provided with code markers to facilitate testing of the software application  in accordance with an embodiment of the present invention. As discussed above, code markers are elements of code embedded in the software application  for the purpose of obtaining exact measurements of when specified operations occur during the execution of the software application . Each code marker is specific to a different operation in the software application , e.g., opening a document operation or printing a document operation.","In addition to those elements described above, the computer architecture  also includes a performance module  and a shared memory . The performance module  must be loaded into computer memory (i.e. instantiated) in order to obtain any measurements related to the code markers. When a code marker is encountered during execution of the software application , the performance module  receives and captures this information directly from the application . For example, in one embodiment the code markers takes the form of API calls to the performance module  and the performance module  is a dynamic link library (DLL) which must be loaded into computer memory in order to receive the calls. In one embodiment, the performance module  is loaded by the software application along with the other DLLs necessary for the software application  to execute. In an alternative embodiment, the performance module may be loaded by the integrated test framework  in response to a script marker or by default whenever the integrated test framework  is executed.","In an embodiment, the performance module  receives information concerning every code marker encountered during execution of the software application . In order to limit the amount of measurement data generated by the performance module  during a test, the data output by the performance module  may be limited to specified code markers by means of the shared memory . In an embodiment, the shared memory  may be created as part of initiating the performance module  or it may be created by the integrated test framework . The performance module  accesses the shared memory  to determine what code markers are of interest during any given test. When the performance module  is loaded, it inspects the shared memory  for code markers. Subsequently, for each code marker listed in the shared memory , the performance module  generates measurement data, for example the time the code marker is encountered, related to that code marker.","In accordance with an embodiment of the present invention, the data generated by the performance module  is output to the shared memory . In an alternative embodiment, the data generated by the performance module  may be output to any location or data file accessible to the integrated test framework .","The shared memory  is also accessible by the integrated test framework . In one embodiment, the integrated test framework  originally populates the shared memory with the code markers of interest as determined from an inspection of the test script  and also obtains data generated by the performance module  from the shared memory. Thus, in this embodiment the shared memory  may be considered a conduit for communication between the performance module  and the integrated test framework .","In order for the integrated test framework  to identify the code markers in a test script, the integrated test framework  maintains a code marker list  of code markers for the software application. In addition to providing information that allows the integrated test framework to identify code markers in the test script, the code marker list  may include information necessary to translate a code marker into a form that should be written to the shared memory  that is understandable by the performance module .",{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 4","b":["106","106","402","402","110","112","114","106","414","414","402"]},"The measurer module  also supports the identification of the script markers and code markers, if any, within test scripts. To support the identification, the measurer module  has access to a marker list  that may or may not be a separate data store . The marker list  includes the data necessary to identify markers in the test script . The marker list  may also provide information that correlated the markers as they are found in the script with other forms of markers, such as the form in which a specific code marker must be written to shared memory , for example.","In an object-oriented programming environment embodiment, such as Microsoft's NET framework, the measurer module  may be implemented as a NET base class, e.g., PerformanceMeasurer class. This class is then instantiated by the test script and subsequently manages the testing process as described above. This implementation further allows for easy development of testing tools as each testing tool can then inherit from the PerformanceMeasurer base class. In this way, all logic common to each testing tool can be located in the base class to facilitate ease of development of testing tools.","The integrated test framework  also includes a submitter module . The submitter module  supports the collection of data from the various testing tools. The submitter module  also supports outputting the data in a specified form to a specified location, such as in a database format to a database, in ASCII to the screen, or in .XML to a specified file. In the present embodiment, the integrated test framework  has access to multiple submitter modules , each module  supporting a different output format and location, e.g., an .XML submitter  and a database submitter . In an alternative embodiment, the integrated test framework  may have one submitter module  that supports all potential output scenarios. A tester can dictate the submitter module  to be used for a particular test by specifying a submitter module  in the test script. In an embodiment, if no submitter module  is specified a default submitter module is used.","In an object-oriented programming environment embodiment, such as Microsoft's NET framework, the submitter module  may be implemented as a .NET base class, e.g., PerformanceSubmitter class. This allows for easy development of submitters as each submitter tool can then inherit from the PerformanceSubmitter base class. In this way, all logic common to each submitter module  can be located in the base class to facilitate ease of development of additional submitter modules .","The integrated test framework  may also include an analysis module . The analysis module  is capable of analyzing data generated by the testing tools and compare that data with other performance criteria, such as previous testing results, or predetermined performance thresholds. The analysis module  may obtain the performance criteria from the test script or by accessing a data store  maintained to include performance criteria. The analysis module  may perform analyses in response to commands placed in the test script  (possibly including or identifying the performance criteria to be used in the test) or may perform predetermined analyses for every test or both. In response to the analysis, the analysis module  is capable of performing additional actions such as returning information that then directs the integrated test framework  to perform additional tests, either as described in the test script or a modified version of the tests in the test script. For example, if the analysis module  determines that a test fails a benchmarking analysis, the test may be automatically repeated without the tester's interaction, but using the profiling testing tool  in addition to the benchmarking testing tool .","The directions of how to modify the test scenario in the event that the analysis fails may be explicitly provided in the script. For example, the analysis module may return or maintain an analysis failed indication that is referenced in the test script. In that embodiment, the tester can draft a test script that includes an if-then-else statement in the script explicitly describe what scenario to perform based on the results of the first analysis. In an alternative embodiment, a repeat test command could be used in the script that, based on the results of the analysis, would direct the integrated test framework  to repeat the initial test using different testing tools identified in the repeat command.",{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIGS. 5","FIG. 5","FIG. 5"],"i":["a ","b ","a ","a ","b ","b "],"b":["5","102","100","102","102","102","502","102","502"]},"After the namespace specifications, the test script includes a command that instantiates the integrated test framework. In the embodiment shown, the command \u201cperfObject =new PerformanceMeasurer (\u201cWord-Test\u201d)\u201d instantiates an object of the PerformanceMeasurer .NET class.","The test script and next includes a series of start and end measurement functions that call the methods of the PerformanceMeasurer object e.g., \u201cpublic void Setup( )\u201d, \u201cpublic void Teardown( )\u201d, \u201cpublic void OpenComplexDoc( )\u201d, and \u201cpublic void \u201cSaveComplexDoc( )\u201d. Each of these measurement functions include start and end measurement commands which include either a script marker or a code marker in the arguments of the measurement command. An example of a performance object start measurement command is \u201cperfObject.StartMeasurement(\u201cWordOpen\u201d, 503, 1, 504, 1, 0, 0);\u201d which uses the following exemplary format: StartMeasurement(String MeasurementName1,int StartMarkerID, Int AppID, Int EndMarkerID, Int AppID, Int SkipStart, Int SkipStop). In the exemplary format, MeasurementName1 is the name of the performance measurement that is being taken which identifies the testing tool , , or  to be used, StartMarkerID is the code marker for the start of the measurement, AppID identifies the software application  to measure (provided to differentiate between applications in a multi-application test scenario), EndMarkerID identifies the code marker to end the measurement, the second AppID identifies the application for the end measurement code marker, SkipStart indicates how many times the start codemarker (StartMarkerID) needs to be skipped before the measurement is taken, SkipStop indicates how many times the end code marker (EndMarkerID) needs to be skipped before the measurement is taken. The exemplary format allows measurements to begin upon encountering a code marker in a first software application and end upon encountering a code marker in a second software application, which allows very complex measurements scenarios to be created.","In the above discussed example, the \u201cperfObject.StartMeasurement(\u201cWordOpen\u201d, 503, 1, 504, 1, 0, 0);\u201d command is followed by the \u201cthis.objHost.Documents.Open( . . . )\u201d command that orders the software application  to open the identified file. The command to the software application  is then followed in the exemplary test script  by the \u201cEndMeasurement(\u201cWordOpen\u201d, 5)\u201d command. The \u201cEndMeasurement(\u201cWordOpen\u201d, 5)\u201d is an end measurement command having the format \u201cEndMeasurement(String MeasurementName, Int TimeOut)\u201d in which MeasurementName identifies the location of the test data, and TimeOut identifies a time out period that ends the measurement in case the Open command does not complete within the designated time in seconds.","In the exemplary embodiment, script markers may be used if no code markers have been provided in the measurement commands. For example, in the SaveComplexDoc( ) function, the start measurement command \u201cperfObject.StartMeasurement(\u201cWordSave\u201d, 507, 1, 0, 1, 0, 0)\u201d identifies a code marker \u201c507\u201d to start the measurement at, but does not identify an end measurement code marker (i.e., the EndMarkerID is set to \u201c0\u201d). The function next includes a command to the software application , in this case \u201cSaveAs\u201d after which a command to the performance module to end the measurement is included, \u201cperfObject.EndMeasurement(\u201cWordSave\u201d)\u201d. The EndMeasurement command with no timeout identified then acts as a script marker that ends the measurement upon signal to the integrated test framework that the \u201cSaveAs\u201d command has completed. Thus, in this function, the measurement starts when a code marker (507) is encountered during the \u201cSaveAs\u201d procedure and the measurement ends when, after control has returned to the integrated test framework, the \u201cperfObject.EndMeasurement(\u201cWordSave\u201d)\u201d script command is executed. This particular example is a mixed mode scenario measurement. We use a code marker to start the measurement and the script marker to end the measurement. \u201c0\u201d for the EndMarkerID indicates that scenario.","In addition, each of the measurement commands may also include an identification of the testing tool for that measurement. In an embodiment, if no testing tool is identified, the benchmarking tool is used by default for that measurement. These measurement commands are inspected by the newly instantiated PerformanceMeasurer object to determine what testing tools to initialized prior to beginning the actual test. As shown in and , the functions, and commands within the functions, in the test script are listed in chronological order so that each of the various operations that make up the scenario to be tested are interspersed throughout the start and end measurements commands.","At the end of the test script may be provided one or more close commands and dispose commands. For example, a close command may be provided that causes the integrated test framework to collect and output data (such as by a specified submitter module). In an embodiment, the close command may also write benchmark data to an .XML file in addition to any other output of data. A dispose command is used to close all objects instantiated by the integrated test framework or the software application during the test. The dispose command may also force a clean up of the memory used to return to the computing system to the state it was in prior to the test being performed.","Furthermore, nested measurement commands may also be used. In an embodiment, in the nested marker case you do not need to call the EndMeasurement function multiple times. Instead, you only need to call EndMeasurement( ) for the corresponding first StartMeasurement( ) call.","The logical operations of the various embodiments of the present invention are implemented (1) as a sequence of computer implemented acts or program modules running on a computing system and\/or (2) as interconnected machine logic circuits or circuit modules within the computing system. The implementation is a matter of choice dependent on the performance requirements of the computing system implementing the invention. Accordingly, the logical operations making up the embodiments of the present invention described herein are referred to variously as operations, structural devices, acts or modules. It will be recognized by one skilled in the art that these operations, structural devices, acts and modules may be implemented in software, in firmware, in special purpose digital logic, and any combination thereof without deviating from the spirit and scope of the present invention as recited within the claims attached hereto.",{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 6","b":["104","104","100","602","602"]},"After the test script has been written, an execute test script operation  is performed to run the test script. As described above, the test script includes one or more commands to execute the integrated test framework, for example commands that cause the instantiation of an object of the integrated test framework class.","In response to the execute script operation , the integrated test framework is executed in an initialize operation . This operation  may involve instantiating an integrated test framework object or otherwise running a test manager program.","In an embodiment, the initialize operation  may also include initializing the software application, such as running the software application and setting it up for the first measurement as directed by the test script. In an alternative embodiment, the initialization of the software application may occur while measurements are being taken as part of the perform script actions operation .","The integrated test framework object may inspect the test script to determine what testing tools are called for by the test script as part of the initialize operation . The identified tools are then configured and executed. For example, the integrated test framework may identify that code markers are used in some measurement commands and in response cause the performance module of the software application to be loaded when the software application is executed and then cause the code markers to be written to the shared memory prior to beginning the actual testing. Configuration and execution of may require use of data, such as configuration data, associated with the testing tools and a reconfiguration of the computing environment to support the operations of the testing tool. Alternatively, configuration and execution may be as simple as instantiating objects of the identified testing tool classes.","In an alternative embodiment discussed in greater detail with reference to , the integrated test framework may wait to configure some or all of the testing tools until action is executed , rather than during the initialization operation . In the alternative embodiment, as each measurement command is received by the integrated test framework, the integrated test framework identifies the necessary testing tool and loads that tool in a just in time manner. In the embodiment, tools that have already been loaded need not be loaded again as long as the test script has not explicitly ended the tools execution.","Next, the specific commands in the script are carried out in a perform script actions operation . Each command in the script is read in turn and the actions are performed. The commands may be application commands related to an operation of the software application to be tested or may be measurement commands identifying script or code markers and the beginning or end of some measurement. Application commands for the software application are communicated to and handled by the software application through the automation system. Measurement commands are handled by the integrated test framework, which passes the commands through to the appropriate testing tool.","Ultimately, after the commands related to the testing have been performed, the integrated test script then collects the data generated by the testing tools and outputs data in an output data operation . This may be in response to close commands in the test script or may occur by default upon determination that no close commands are included in the test script.","In an embodiment in which an analysis capability is provided in the integrated test framework, an analysis operation  may occur during the execution of a test script. The analysis may be triggered by an analysis command in the test script or may occur by default. For embodiments that use analysis commands in the test script, the analysis command may contain a threshold value, such as benchmark threshold, for the scenario to be tested. The threshold value may be provided in the script or the script may contain a reference to a data structure, such a benchmarking data file, that the integrated test framework can interpret to determine what threshold to use in the analysis. The analysis operation , then, may involve the comparison of the threshold value with the data generated in the current test. Based on the result of the comparison, the analysis operation  may perform various additional tests (i.e., flow returns to the perform script actions operation ) or the operational flow may proceed to dispose operation  depending on information provided in the test script.","For example, the test script may include an analysis command that causes the integrated test framework to compare the benchmarking results of the current test to some threshold benchmarking value. The analysis command may further include directions that, if the current benchmarking results are greater than the threshold, the test scenario should be repeated but with different measurement parameters or testing tools. In embodiments, the analysis may be performed by the integrated test framework; the results of the analysis then dictating whether the remaining commands in the test script should be performed or whether the flow should skip to the dispose operation . In an alternative embodiment, the analysis may be performed by the test script using data provided by the integrated test framework, which requires that the integrated test framework communicate specified data to the test script in some way, such as for example placing the data in a specific data structure accessible by the test script. In this case, the logic for performing the comparison may be placed in the test script.","Preferably, the operational flow ends with a dispose operation  that returns the computing system to the condition it was prior to running of the test script. This may include shutting down the integrated test framework, closing any instantiated objects, and exiting the software application and releasing any memory allocated or otherwise used during the test. The dispose operation  may occur automatically or in response to one or more dispose commands in the test script.",{"@attributes":{"id":"p-0075","num":"0074"},"figref":["FIG. 7","FIG. 7"],"b":"608"},"After initialization of the integrated test framework in initialization operation , the next command in the script is read in a read operation . For simplicity in discussion of the embodiment in , commands are considered to be either application commands or measurement commands. Other types of commands, e.g., initialization commands, close commands, analysis commands, dispose commands, etc., would not be considered commands with respect to .","After reading the next command, a determination operation  determines if the command is an application command or a measurement command. If it is an application command, then the command is passed to the software application in a perform application command operation  for execution by the software application's automation system.","If the command is a measurement command, then a second determination operation  determines if the testing tool necessary to perform the measurement command has already been initialized. If not, then an initialize testing tool operation  initializes the testing tool and the measurement command is transferred to the testing tool in a perform measurement operation . If the tool has already been initialized, then the measurement command is transferred to the testing tool in the perform measurement operation .","Measurements taken in the perform measurements operation  may include measurements related to script markers or code markers. If code markers are used, then that will involve the use of the performance module and shared memory associated with the software application.","After the perform measurement command operation  and the perform application command operation , a third determination operation  determines if any commands, i.e., application or measurement commands, remain to be executed. If so, the read next command operation  is repeated as described above. If no application or measurement commands remain, or if the next command is not an application or measurement command, the operational flow exits the perform script actions operation .","While the invention has been particularly shown and described with reference to preferred embodiments thereof, it will be understood by those skilled in the art that various other changes in the form and details may be made therein without departing form the spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 3","FIG. 1"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 4","FIGS. 1 and 3"]},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIGS. 5","i":["a ","b "],"b":"5"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 7","FIG. 6"]}]},"DETDESC":[{},{}]}
