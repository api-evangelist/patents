---
title: Gesture based human computer interaction
abstract: Apparatus, computer-readable storage medium, and method associated with human computer interaction. In embodiments, a computing device may include a plurality of sensors, including a plurality of light sources and a camera, to create a three dimensional (3-D) interaction region within which to track individual finger positions of a user of the computing device. The light sources and the camera may be complementarily disposed for the camera to capture the finger or hand positions. The computing device may further include a 3-D interaction module configured to analyze the individual finger positions within the 3-D interaction region, the individual finger movements captured by the camera, to detect a gesture based on a result of the analysis, and to execute a user control action corresponding to the gesture detected. Other embodiments may be described and/or claimed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09304597&OS=09304597&RS=09304597
owner: Intel Corporation
number: 09304597
owner_city: Santa Clara
owner_country: US
publication_date: 20131029
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","TECHNICAL FIELD","BACKGROUND","DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS","EXAMPLES"],"p":["The present application is a national phase entry under 35 U.S.C. \u00a7371 of International Application No. PCT\/US2013\/067323, filed Oct. 29, 2013, entitled \u201cGESTURE BASED HUMAN COMPUTER INTERACTION\u201d, which designated, among the various States, the United States of America. The Specification of the PCT\/US2013\/067323 Application is hereby incorporated by reference.","Embodiments of the present disclosure are related to the field of human computer interaction, and in particular, to gesture based human computer interaction.","The background description provided herein is for the purpose of generally presenting the context of the disclosure. Unless otherwise indicated herein, the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.","Human computer interactions have traditionally been relegated to user control via physical interaction of a user with one or more input devices, such as a mouse, a keyboard, a track ball, a touch pad, and so forth. The mouse, et al have essentially become the sine qua non of human computer interaction. While a user's interaction with a mouse et al may have become somewhat ingrained with users, the interaction has never been natural or intuitive.","A method, storage medium, and computing device for human computer interaction are described. In embodiments, the computing device may include a plurality of sensors, including a plurality of light sources and a camera, to create a three dimensional (3-D) interaction region within which to track individual finger positions of a user of the computing device, and thereby track finger movements. The light sources and the camera may be complementarily disposed for the camera to capture the finger or hand positions. An example of a 3D region may include a 3D region in front of a display screen. The computing device may also include a 3-D interaction module coupled with the plurality of sensors. The 3-D interaction module may be configured to analyze the individual fingers within the 3-D interaction region, captured by the camera, to detect a gesture based on a result of the analysis. Gestures may include e.g., holding an individual finger in a fixed location for a pre-determined period of time within the 3-D interaction region. On detection of the gesture the 3-D interaction module may execute a user control action corresponding to the gesture detected. For the example of holding of an individual finger, the user control action may be executed at the correlated location of the cursor rendered on the display screen. In embodiments, the display screen may be a floating display screen projected to coincide or intersect with the 3D interaction region.","In the following detailed description, reference is made to the accompanying drawings which form a part hereof wherein like numerals designate like parts throughout, and in which is shown, by way of illustration, embodiments that may be practiced. It is to be understood that other embodiments may be utilized and structural or logical changes may be made without departing from the scope of the present disclosure. Therefore, the following detailed description is not to be taken in a limiting sense, and the scope of embodiments is defined by the appended claims and their equivalents.","Various operations may be described as multiple discrete actions or operations in turn, in a manner that is most helpful in understanding the claimed subject matter. However, the order of description should not be construed as to imply that these operations are necessarily order dependent. In particular, these operations may not be performed in the order of presentation. Operations described may be performed in a different order than the described embodiment. Various additional operations may be performed and\/or described operations may be omitted in additional embodiments.","For the purposes of the present disclosure, the phrase \u201cA and\/or B\u201d means (A), (B), or (A and B). For the purposes of the present disclosure, the phrase \u201cA, B, and\/or C\u201d means (A), (B), (C), (A and B), (A and C), (B and C), or (A, B and C). The description may use the phrases \u201cin an embodiment,\u201d or \u201cin embodiments,\u201d which may each refer to one or more of the same or different embodiments. Furthermore, the terms \u201ccomprising,\u201d \u201cincluding,\u201d \u201chaving,\u201d and the like, as used with respect to embodiments of the present disclosure, are synonymous.",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1","b":["100","100","102","102","104","106","104","112","110","106","104","106","110","114","112","104"]},"The intersection of the viewing angle  of camera  and the light  emitted by the one or more light emitting diodes  may create a three-dimensional (3-D) interaction region  within which a user may utilize one or more gestures to interact with computing device . This may be accomplished by tracking finger and\/or hand positions, and thereby tracking finger and\/or hand movements, of the user in the 3-D interaction region . The combination of camera  and the one or more light emitting diodes  may enable fast and accurate tracking of the user gestures within the 3-D interaction region. In embodiments, the light  emitted by the one or more light emitting diodes  may be configured to accentuate, or highlight, gestures occurring within the 3-D interaction region and therefore may make it easier for computing device  to interpret the gestures captured by camera . For example, where a gesture is based upon a user's finger movements, the tips of the fingers may be tracked quickly and precisely due to the accentuation of the finger tips by the light  emitted from the one or more light emitting diodes . The accentuation of the finger tips may be effected by light emitted by light emitting diodes , scattered by the finger tips within the interaction region , and detected by camera .","In embodiments, computing device  may be configured to correlate actions of the user, such as positions of the user's fingertips, with a corresponding location of a positional indicator, such as a cursor, on a display  of computing device . In such an embodiment, a user may be able to move a cursor rendered on display  to a desired location near a desired object on display  and perform a gesture which may cause computing device  to carry out a user control action on the object.",{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 2","b":["200","200"]},"Computing device  may comprise processor(s) , display , sensors , storage  containing 3-D interaction module , and other input\/output (I\/O) devices . Processor(s) , display , sensors , storage  and other input\/output (I\/O) devices  may all be coupled together utilizing system bus .","Processor(s)  may be comprised of a single processor or multiple processors. In multiple processor embodiments, the multiple processors may be of the same type, i.e. homogeneous, or may be of differing types, i.e. heterogeneous and may include any type of single or multi-core processors. This disclosure is equally applicable regardless of type and\/or number of processors.","Display  may be any type of display including, but not limited to a cathode ray tube (CRT), a liquid crystal diode (LCD), an organic light emitting diode (OLED), or a 3-D display, such as that discussed in . Display  may be incorporated into computing device  or may be peripherally connected to computing device  through any type of wired and\/or wireless connection. This disclosure is equally applicable regardless of the type of display.","Sensors  may include, in some embodiments, sensors such as a camera configured to capture one or more images and one or more light emitting diodes. These sensors may configure computing device  with a 3-D interaction region within which a user of computing device  may interact with computing device . It will be appreciated that the sensors mentioned above are merely examples that are meant to be illustrative. Any 3-D sensors capable of capturing a user's gestures are contemplated.","In embodiments, storage  may be any type of computer-readable storage medium or any combination of differing types of computer-readable storage media. Storage  may include volatile and non-volatile\/persistent storage. Volatile storage may include e.g., dynamic random access memory (DRAM). Non-volatile\/persistent storage may include, but is not limited to, a solid state drive (SSD), a magnetic or optical disk hard drive, flash memory, or any multiple or combination thereof.","In embodiments 3-D interaction module  may be implemented as software, firmware, or any combination thereof. In some embodiments, 3-D interaction module  may comprise one or more instructions that, when executed by processor(s) , cause computing device  to perform one or more operations of any process described herein. In embodiments, 3-D interaction module  may be configured to receive data from sensors . In some embodiments, 3-D interaction module  may be configured to monitor a stream of data produced by sensors . In other embodiments, 3-D interaction module may be configured to periodically receive portions of data from sensors  for analysis.","In embodiments, 3-D interaction module  may be configured to analyze the data received from sensors . In some embodiments, the data may be analyzed to determine a location of a user's hand(s) and\/or finger(s) within a 3-D interaction region created by the sensors. The 3-D interaction module  may be configured to correlate the determined location with a position of a cursor, or other positional indicator, rendered on display . In embodiments, this correlation may occur in real time such that the movements of a user are contemporaneously reflected by movement of the cursor rendered on display .","In some embodiments, the data may be analyzed to determine if a pre-determined gesture occurs within the sensor data, such as those gestures described in reference to . This may be accomplished, for example, by determining coordinate movements within the data or comparing one or more images captured by one or more of the sensors with a database of images of gestures.","In embodiments, 3-D interaction module  may be configured to determine vital statistics of a user. This may be accomplished as discussed in reference to  below. In embodiments, the 3-D interaction module  may be configured to provide these vital statistics to other applications executing on computing device . For example, if a user is involved in playing a video game, the video game may be configured to receive a vital statistics from the 3-D interaction module  and may adjust the difficulty of the game accordingly. In other embodiments, the vital statistics may be utilized to determine if a user is using the user's hands and\/or fingers to interact with the computing device or an inanimate object, such as a stylus. For example, 3-D interaction module  may be configured to determine if the object(s) within the 3-D interaction region have a pulse rate, and, if so, then 3-D interaction module  may determine that a user's finger(s) and\/or hand(s) are in the 3-D interaction region. In such embodiments, the 3-D interaction module  may be configured to only attempt to determine gestures when a heart rate is detected. In other embodiments, the 3-D interaction module  may be configured to enable other contexts when a heart rate is not detected. For example, the 3-D interaction module  may enable additional drawing features when a stylus is detected, or may enable different tools depending upon whether a stylus or finger is detected. For example, in a drawing application a user may utilize the stylus to draw and a finger to erase.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 3","b":["300","300","302","304","304","302","306","310","306","310","302","302","306","308","304","310","312","308","306","314","300","314","304","304"]},"In some embodiments, floating display  may be a stereoscopic or auto-stereoscopic 3-D display. As used herein, auto-stereoscopic 3-D display refers to any display capable of being perceived by a user as 3-D without the use of additional viewing aids, such as glasses. An auto-stereoscopic display may be achieved through utilization of a display, such as a liquid crystal display (LCD) in conjunction with a parallax barrier or a lenticular array. In some embodiments, computing device  may be configured to track the location and or direction of a user's eyes utilizing camera  and may be configured to adjust floating display  accordingly.","Interacting with a floating display  via 3-D interaction region  may be beneficial in a number of ways. Because a user would not be touching any surfaces directly to interact with computing device , this implementation may reduce drawbacks associated with direct physical contact that may impact a more traditional interactive display. For example, such an implementation may prevent smudging of a physical display of computing device . This may reduce the number of times such a physical display would need to be cleaned in addition to improving security because a nefarious user would not be able to discern a previous user's password\/pin from smudges on the screen. Such an implementation may reduce or prevent the spread of germs from one user to the next. A user may also be able to wear gloves to interact with computing device  which may not be possible with a traditional interactive display. In addition, such an implementation may reduce wear, tear, and\/or scratching caused through contact with a user. It will be appreciated that this list of benefits is not exhaustive and is merely meant to be illustrative. Any number of other benefits may be realized through such an implementation.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 4","FIG. 1 and 306","FIG. 3"],"b":["104","402","404","406"]},"In embodiments, the exact location, intensity and size of the finger may be taken into account when attempting to measure PPG. This added information may allow for non-invasive, in-situ PPG measurements. In some embodiments, the computing device may be further configured to utilize a controlled strobing light source, such as an infra-red (IR) light emitting diode (LED), with a temporally synchronized global shutter camera. The combination of the IR LED and the global shutter camera may increase signal-to-noise ratio and may also act to reduce or prevent interference of computing device display illumination changes and\/or ambient lighting changes with the PPG measurement. In some embodiments, the IR LED may be configured to emit light having a wave length of approximately 950 nanometers (nm).","In some embodiments, a user's blood pressure may be able to be determined. In such embodiments, a user may place the user's wrist into a 3-D interaction region, such as that described above. One or more sensors, such as a camera, may be configured to capture images of the veins and the 3-D interaction module may be configured to determine a pulse transit time (PTT) of blood flow through the user's veins. PTT is inversely proportional to blood pressure and may therefore be utilized to determine a user's blood pressure.","In other embodiments, 3-D interaction module may be configured to track the user's vital statistics utilizing LEDs at multiple wavelengths to determine blood oxygenation. For example, one or more sensors may correspond with an IR LED at a 950 nm wavelength, while one or more other sensors may correspond with a red LED at a 650 nm wave length. The 3-D interaction module may be configured to determine a difference in reflectance, transmittance, and\/or absorption between the IR and Red wavelengths to determine the blood oxygenation of the user without the need for a traditional finger-clasp oxygenation meter.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 5","b":["508","502","508","508","502","508","504","506","504","506","504","506"]},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 6","b":["604","602","604","606","608","606","608"]},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 7","b":["702","704","706","702","706","706","702","704","702"]},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 8","b":["802","804","802"]},{"@attributes":{"id":"p-0043","num":"0042"},"figref":["FIG. 9","FIG. 2"],"b":["900","902","206"]},"In embodiments, the sensor data may include coordinates associated with objects being monitored by the sensors. For example, the sensor data may contain coordinate information for fingertips detected within a 3-D interaction region, such as that discussed above in reference to . In some embodiments, sensor data may include a number of images captured by a sensor, such as a camera, to be analyzed. The above discussed sensor data is merely meant to be illustrative and is not meant to be limiting of this disclosure. It will be appreciated that any type of sensor data may be utilized without departing from the scope of this disclosure.","In block  the sensor data may be analyzed to determine if a user of the computing device has performed a pre-determined gesture. This may be accomplished by analyzing coordinates passed in to 3-D interaction module to determine if a sequence of coordinates matches a pre-determined pattern of coordinates. In some embodiments, where images may be provided to 3-D interaction module, the images may be compared against pre-determined images of gestures to determine whether the image provided by the sensor matches a pre-determined image of a gesture.","As used herein a gesture may include any type of finger and\/or hand movement of the user. Illustrative gestures are discussed above in relation to . However, it will be appreciated that the gesture may be any type of gesture and any such gesture is expressly contemplated by this disclosure.","In some embodiments, analyzing the sensor data may include analyzing individual finger movements performed within the 3-D interaction region. In some embodiments, a location within the 3-D interaction region may be determined and this location may be correlated with a corresponding position of a cursor on display of the computing device.","At block  a determination is made as to whether or not a gesture has been detected in the sensor data. If a gesture has not been detected then the process proceeds back to block . If a gesture has been detected then the process moves to block  where a user control action corresponding to the detected gesture may be executed. As used herein, a user control action may be any type of action a user could undertake with input devices of the computing device. Example input devices may include a keyboard, mouse, track ball, touch pad, etc. While discussed in the examples given above in reference to mouse clicks, the user control actions should not be so limited. User control actions may include, for example: scrolling within a window, minimizing, maximizing, or closing a window, zooming in or zooming out on an object rendered on the screen, a right or left mouse click, copying, pasting, etc.","For the purposes of this description, a computer-usable or computer-readable medium can be any medium that can contain, store, communicate, propagate, or transport the program for use by or in connection with the instruction execution system, apparatus, or device. The medium can be an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system (or apparatus or device) or a propagation medium. Examples of a computer-readable storage medium include a semiconductor or solid state memory, magnetic tape, a removable computer diskette, a random access memory (RAM), a read-only memory (ROM), a rigid magnetic disk and an optical disk. Current examples of optical disks include compact disk-read only memory (CD-ROM), compact disk-read\/write (CD-R\/W) and DVD.","Embodiments of the disclosure can take the form of an entirely hardware embodiment, an entirely software embodiment or an embodiment containing both hardware and software elements. In various embodiments, software, may include, but is not limited to, firmware, resident software, microcode, and the like. Furthermore, the disclosure can take the form of a computer program product accessible from a computer-usable or computer-readable medium providing program code for use by or in connection with a computer or any instruction execution system.","Although specific embodiments have been illustrated and described herein, it will be appreciated by those of ordinary skill in the art that a wide variety of alternate and\/or equivalent implementations may be substituted for the specific embodiments shown and described, without departing from the scope of the embodiments of the disclosure. This application is intended to cover any adaptations or variations of the embodiments discussed herein. Therefore, it is manifestly intended that the embodiments of the disclosure be limited only by the claims and the equivalents thereof.","Some non-limiting examples are:","Example 1 is a computing device for human computer interaction comprising: a plurality of sensors, including a plurality of light sources and a camera, to create a three dimensional (3-D) interaction region within which to track individual finger positions of a user of the computing device, wherein the plurality of light sources and the camera are complementarily disposed to enable the camera to capture the individual finger positions in three dimensions; and a 3-D interaction module coupled with the plurality of sensors, the 3-D interaction module to analyze the individual finger positions, in relation to one another, within the 3-D interaction region, captured by the camera, to detect a gesture based on a result of the analysis, and to execute a user control action corresponding to the gesture detected.","Example 2 may include the subject matter of Example 1, further comprising a display coupled with the 3-D interaction module, wherein the 3-D interaction module is to further determine a location of the individual fingers within the 3-D interaction region, and to correlate the location of the individual fingers, within the 3-D interaction region, with a location of a cursor rendered on the display.","Example 3 may include the subject matter of Example 2, wherein the gesture detected comprises holding an individual finger of a hand, extending beyond remaining fingers of the hand, in a fixed location for a pre-determined period of time within the 3-D interaction region, and wherein the 3-D interaction module is to execute a user control action at the correlated location of the cursor rendered on the display.","Example 4 may include the subject matter of Example 2, wherein the gesture detected comprises holding an individual finger of a hand, extending beyond remaining fingers of the hand, in a fixed position within the 3-D interaction region for a first pre-determined period of time to cause the correlated location of the cursor to freeze for a second pre-determined period of time, and the gesture detected further comprises a forward or downward movement of a tip of the individual finger within the second pre-determined period of time, and wherein the 3-D interaction module is to execute a user control action at the correlated location of the cursor rendered on the display.","Example 5 may include the subject matter of Example 2, wherein the gesture detected comprises holding a first finger of the individual fingers in a fixed location within the 3-D interaction region and bringing a second finger of the individual fingers into contact with the first finger, and wherein the 3-D interaction module is to execute a user control action at the correlated location of the cursor rendered on the display.","Example 6 may include the subject matter of Example 2, wherein the gesture detected comprises holding a first finger of the individual fingers in a fixed location, and wherein the gesture detected further comprises bringing a second finger, to the right of the first finger, into contact with the first finger, and wherein the 3-D interaction module is to execute a user control action at the correlated location of the cursor rendered on the display.","Example 7 may include the subject matter of Example 2, wherein the gesture detected comprises holding a first finger of the individual fingers in a fixed location, and wherein the gesture detected further comprises bringing a second finger to the left of the first finger into contact with the first finger, and wherein the 3-D interaction module is to execute a user control action, at the correlated location of the cursor rendered on the display.","Example 8 may include the subject matter of Example 2, wherein the gesture detected comprises swirling an individual finger of a hand, extending beyond remaining fingers of the hand, in a clockwise or counter-clockwise direction, and wherein the 3-D interaction module is to execute a pre-determined user control action, proximate to the correlated location of the cursor rendered on the display, based on the direction of the swirling of the individual finger.","Example 9 may include the subject matter of any one of Examples 1-8, wherein the user control action is one of a left or right mouse click.","Example 10 may include the subject matter of any one of Examples 2-8, wherein the plurality of sensors and the 3-D interaction module are to further cooperate to detect one of heart-rate, blood pressure, or blood oxygenation of the user within the 3-D interaction region.","Example 11 may include the subject matter of any one of Examples 2-8, wherein the plurality of sensors and the 3-D interaction module are to cooperate to detect heart-rate of the user, and the 3-D interaction module is to further perform the analysis, detection of finger movements, and execution only when presence of the heart-rate of the user is detected within the 3-D interaction region.","Example 12 may include the subject matter of any one of Examples 2-8, wherein the display is a floating display projected near or intersecting with the 3-D interaction region and the plurality of light sources are light emitting diodes (LEDs) and wherein to be complementarily disposed further comprises disposition of the LEDs such that light from the LEDs intersects a viewing angle of the camera where the intersection between the light and the viewing angle creates the 3-D interaction region.","Example 13 is a computer-implemented method for human computer interaction comprising: analyzing, by a three dimensional (3-D) interaction module of a computing device, sensor data associated with individual finger positions of a user of the computing device, within a 3-D interaction region, sensed by a plurality of sensors of the computing device that include a plurality of light sources and a camera, wherein the plurality of light sources and the camera are complementarily disposed for the camera to capture the individual finger positions in three dimensions; detecting, by the 3-D interaction module, a gesture of the user, based at least in part on a result of the analysis; and executing, by the 3-D interaction module, a user control action corresponding to the gesture detected, to control the computing device.","Example 14 may include the subject matter of Example 13, further comprising analyzing the individual finger movements to detect a mouse click gesture based on the individual finger movements and executing a mouse click in response to detection of the mouse-click gesture.","Example 15 may include the subject matter of Example 13, further comprising: determining a location of the individual fingers within the 3-D interaction region, and correlating the location of the individual fingers, within the 3-D interaction region, with a location of a cursor rendered on a display of the computing device.","Example 16 may include the subject matter of Example 15, wherein the gesture detected comprises holding an individual finger of a hand, extending beyond remaining fingers of the hand, in a fixed location for a pre-determined period of time within the 3-D interaction region.","Example 17 may include the subject matter of Example 15, wherein the gesture detected comprises holding an individual finger of a hand, extending beyond remaining fingers of the hand, in a fixed position within the 3-D interaction region for a first pre-determined period of time to cause the correlated location of the cursor to freeze for a second pre-determined period of time, and the gesture detected further comprises a forward or downward movement of a tip of the individual finger within the second pre-determined period of time and wherein executing further comprises executing a user control action proximate to the correlated location of the cursor rendered on the display.","Example 18 may include the subject matter of Example 15, wherein the gesture detected comprises holding a first finger of the individual fingers in a fixed location within the 3-D interaction region and bringing a second finger of the individual fingers into contact with the first finger, and wherein executing further comprises executing a user control action proximate to the correlated location of the cursor rendered on the display.","Example 19 may include the subject matter of Example 15, wherein the gesture detected comprises holding a first finger of the individual fingers in a fixed location, and wherein the gesture detected further comprises bringing a second finger, to the right of the first finger, into contact with the first finger, and wherein executing further comprises executing a user control action proximate to the correlated location of the cursor rendered on the display.","Example 20 may include the subject matter of Example 15, wherein the gesture detected comprises holding a first finger of the individual fingers in a fixed location, and wherein the gesture detected further comprises bringing a second finger to the left of the first finger into contact with the first finger, and wherein executing further comprises executing a user control action proximate to the correlated location of the cursor rendered on the display.","Example 21 may include the subject matter of Example 15, wherein the gesture detected comprises swirling an individual finger of a hand, extending beyond remaining fingers of the hand, in a clockwise or counter-clockwise direction, and wherein executing further comprises executing a user control action proximate to the correlated location of the cursor rendered on the display, the user control action based on the direction of the swirling of the individual finger.","Example 22 may include the subject matter of any one of Examples 13-21, wherein the user control action is a mouse click of an item proximate to the correlated location of the cursor rendered on the display.","Example 23 may include the subject matter of any one of Examples 13-21, wherein the 3-D interaction region coincides or intersects with a floating display.","Example 24 is one or more computer-readable media having instructions stored thereon which, when executed by a computing device, cause the computing device to perform the method of any one of Examples 13-23.","Example 25 is a computing apparatus for human computer interaction comprising: means for performing the method of any one of Examples 13-23.","Example 26 is a computing device for human computer interaction comprising: means for analyzing, by a three dimensional (3-D) interaction module of a computing device, sensor data associated with individual finger positions of a user of the computing device, within a 3-D interaction region, sensed by a plurality of sensors of the computing device that include a plurality of light sources and a camera, wherein the plurality of light sources and the camera are complementarily disposed for the camera to capture the individual finger positions in three dimensions; means for detecting, by the 3-D interaction module, a gesture of the user, based at least in part on a result of the analysis; and means for executing, by the 3-D interaction module, a user control action corresponding to the gesture detected, to control the computing device.","Example 27 may include the subject matter of Example 26, further comprising means for analyzing the individual finger movements to detect a mouse click gesture based on the individual finger movements and means for executing a mouse click in response to detection of the mouse-click gesture.","Example 28 may include the subject matter of Example 26, further comprising: means for determining a location of the individual fingers within the 3-D interaction region, and means for correlating the location of the individual fingers, within the 3-D interaction region, with a location of a cursor rendered on a display of the computing device.","Example 29 may include the subject matter of Example 28, wherein the gesture detected comprises holding an individual finger of a hand, extending beyond remaining fingers of the hand, in a fixed location for a pre-determined period of time within the 3-D interaction region.","Example 30 may include the subject matter of Example 28, wherein the gesture detected comprises holding an individual finger of a hand, extending beyond remaining fingers of the hand, in a fixed position within the 3-D interaction region for a first pre-determined period of time to cause the correlated location of the cursor to freeze for a second pre-determined period of time, and the gesture detected further comprises a forward or downward movement of a tip of the individual finger within the second pre-determined period of time and wherein executing further comprises executing a user control action proximate to the correlated location of the cursor rendered on the display.","Example 31 may include the subject matter of Example 28, wherein the gesture detected comprises holding a first finger of the individual fingers in a fixed location within the 3-D interaction region and bringing a second finger of the individual fingers into contact with the first finger, and wherein executing further comprises executing a user control action proximate to the correlated location of the cursor rendered on the display.","Example 32 may include the subject matter of Example 28, wherein the gesture detected comprises holding a first finger of the individual fingers in a fixed location, and wherein the gesture detected further comprises bringing a second finger, to the right of the first finger, into contact with the first finger, and wherein executing further comprises executing a user control action proximate to the correlated location of the cursor rendered on the display.","Example 33 may include the subject matter of Example 28, wherein the gesture detected comprises holding a first finger of the individual fingers in a fixed location, and wherein the gesture detected further comprises bringing a second finger to the left of the first finger into contact with the first finger, and wherein executing further comprises executing a user control action proximate to the correlated location of the cursor rendered on the display.","Example 34 may include the subject matter of Example 28, wherein the gesture detected comprises swirling an individual finger of a hand, extending beyond remaining fingers of the hand, in a clockwise or counter-clockwise direction, and wherein executing further comprises executing a user control action proximate to the correlated location of the cursor rendered on the display, the user control action based on the direction of the swirling of the individual finger.","Example 35 may include the subject matter of any one of Examples 26-34, wherein the user control action is a mouse click of an item proximate to the correlated location of the cursor rendered on the display.","Example 36 may include the subject matter of any one of Examples 26-34, wherein the 3-D interaction region coincides or intersects with a floating display."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
