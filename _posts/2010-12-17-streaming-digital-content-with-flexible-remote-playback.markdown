---
title: Streaming digital content with flexible remote playback
abstract: Disclosed herein are representative embodiments of methods, apparatus, and systems for facilitating the streaming of digital media content to a remote device. In one exemplary embodiment, a translation layer translates markup language code and/or scripting language code (e.g., code that complies with the HTML5/W3C standard) to comply with a streaming protocol (e.g., a streaming protocol specified by the Digital Living Network Alliance (DLNA)) to facilitate streaming of digital content (e.g., digital video, digital audio, or digital images) to remote devices (e.g., a digital television, digital audio player, game console, etc.). In some embodiments, a translation layer translates streaming protocol events at a remote device into other events (e.g., events specified in the HTML5 standard) at a local computing device. Local/remote playback switching logic can also facilitate switching between local playback and remote playback.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09451319&OS=09451319&RS=09451319
owner: Microsoft Technology Licensing, LLC
number: 09451319
owner_city: Redmond
owner_country: US
publication_date: 20101217
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION","I. Overview","1. Exemplary Interfaces for <Video> and <Audio> Elements","2. Events for <Video> and <Audio> Elements","3. Interface for <img> Elements","a. AVTransport State Variables","b. AVTransport Actions","II. Detailed Examples","1. Loading, Startup and Shutdown","2. Exemplary Translations for Media Element Interface","3. Exemplary Translations of Streaming Protocol Events","1. Loading, Startup, and Shutdown","2. Exemplary Translations for Image Element Interface","III. Exemplary User Interface","IV. Exemplary Computing Environment","V. Exemplary Implementation Environment","VI. Exemplary Mobile Device","VII. Extensions and Alternatives"],"p":["As digital content becomes more diverse and readily available on the Internet, consumers are looking for more convenient ways to access such content. A modern home typically has several devices (e.g., PCs, digital televisions, game consoles, smart phones, and other such digital content playback devices) that can be used to access, edit, store, or play digital content such as video, audio, or images. Some devices provide greater convenience and ease of movement. For example, smart phones are highly portable and provide a wide variety of functionality. Other devices provide a more desirable user experience for certain kinds of content. For example, wide-screen digital televisions are well-suited for viewing digital video content in a home theater arrangement. Today's consumer wants to leverage the advantages of each of her devices when accessing the wide variety of digital content available on the Internet.","Disclosed herein are representative embodiments of methods, apparatus, and systems for facilitating streaming of digital content and remote playback of digital content at a remote device. The disclosed methods, apparatus, and systems should not be construed as limiting in any way. Instead, the present disclosure is directed toward all novel and nonobvious features and aspects of the various disclosed embodiments, alone and in various combinations and subcombinations with one another. Furthermore, any features or aspects of the disclosed embodiments can be used in various combinations and subcombinations with one another. The disclosed methods, apparatus, and systems are not limited to any specific aspect or feature or combination thereof, nor do the disclosed embodiments require that any specific advantage be present or problem be solved.","In examples described herein, code received at a local computing device can be translated to comply with a streaming protocol in order to facilitate the processing of content associated with the code by a remote playback device. For example, a translation layer on a PC that receives markup language code and\/or scripting language code (e.g., code that complies with the HTML5\/W3C standard) from a remote server (e.g., via a web browser) can automatically convert such code into corresponding calls in a streaming protocol (e.g., a streaming protocol specified by the Digital Living Network Alliance (DLNA)) to facilitate streaming of digital content (e.g., digital video, digital audio, or digital images) to a wide range of remote devices (e.g., a digital television, digital audio player, game console, etc.). Translations also can be performed in the other direction. For example, a translation layer at a local computing device can automatically translate streaming protocol events from a remote device into other events (e.g., events specified in the HTML5 standard) at a local computing device. Local\/remote playback switching logic can provide flexible remote playback by facilitating switching between local playback of content on a local computing device and remote playback at another device. Described translation and playback switching technology can be used, for example, to allow users to combine the convenience of web browsing on a local computing device (e.g., a laptop computer) with the viewing experience provided by other devices (e.g., a large-screen TV).","The foregoing and other objects, features, and advantages of the invention will become more apparent from the following detailed description, which proceeds with reference to the accompanying figures.","Disclosed herein are representative embodiments of methods, apparatus, and systems for facilitating streaming of digital content and remote playback of digital content at a remote device. The disclosed methods, apparatus, and systems should not be construed as limiting in any way. Instead, the present disclosure is directed toward all novel and nonobvious features and aspects of the various disclosed embodiments, alone and in various combinations and subcombinations with one another. Furthermore, any features or aspects of the disclosed embodiments can be used in various combinations and subcombinations with one another. The disclosed methods, apparatus, and systems are not limited to any specific aspect or feature or combination thereof, nor do the disclosed embodiments require that any one or more specific advantages be present or problems be solved.","Although the operations of some of the disclosed methods are described in a particular, sequential order for convenient presentation, it should be understood that this manner of description encompasses rearrangement, unless a particular ordering is required by specific language set forth below. For example, operations described sequentially may in some cases be rearranged or performed concurrently. Moreover, for the sake of simplicity, the attached figures may not show the various ways in which the disclosed methods, apparatus, and systems can be used in conjunction with other methods, apparatus, and systems.","The disclosed methods can be implemented using computer-executable instructions stored on one or more computer-readable media (e.g., non-transitory computer-readable media, such as one or more optical media discs, volatile memory components (e.g., DRAM or SRAM), or nonvolatile memory or storage components (e.g., hard drives)) and executed on a computer (e.g., any commercially available computer or a computer or image processor embedded in a device, such as a laptop computer, desktop computer, net book, web book, tablet computing device, smart phone, or other mobile computing device). Any of the intermediate or final data created and used during implementation of the disclosed methods or systems can also be stored on one or more computer-readable media (e.g., non-transitory computer-readable media).","For clarity, only certain selected aspects of the software-based embodiments are described. Other details that are well known in the art are omitted. For example, it should be understood that the software-based embodiments are not limited to any specific computer language or program. Likewise, embodiments of the disclosed technology are not limited to any particular computer or type of hardware. Exemplary computing environments suitable for performing any of the disclosed software-based methods are introduced below.","The disclosed methods can also be implemented using specialized computing hardware that is configured to perform any of the disclosed methods. For example, the disclosed methods can be implemented by an integrated circuit (e.g., an application specific integrated circuit (\u201cASIC\u201d), a graphics processing unit (\u201cGPU\u201d), or programmable logic device (\u201cPLD\u201d), such as a field programmable gate array (\u201cFPGA\u201d)) specially designed to implement any of the disclosed methods (e.g., dedicated hardware configured to perform any of the disclosed translations).","In examples described herein, code received at a local computing device can be translated to comply with a streaming protocol in order to facilitate processing of content at a remote device. Translation can include converting code into a different kind of code. For example, markup language code or scripting language code can be converted into translated code that complies with a streaming protocol. Translation also can include other interpretations of code. For example, different kinds of processing and data handling (e.g., in compliance with a streaming protocol) can occur (e.g., at a remote device) in response to received markup language code or scripting language code. Translation can also be used as part of the process of streaming content and\/or related information to a remote device (e.g., a digital television, digital audio player, game console, or other digital content playback device).","For example, a translation layer on a PC that receives markup language code and\/or scripting language code (e.g., code that complies with the HTML5\/W3C standard) from a remote server via a web browser can automatically convert such code into corresponding calls in a streaming protocol (e.g., a streaming protocol specified by the Digital Living Network Alliance (DLNA)) in order to stream digital content (e.g., digital video, digital audio, or digital images) to a wide range of remote devices (e.g., a digital television, digital audio player, game console, or other digital content playback device). A translation layer also can perform translations in the other direction. For example, a translation layer at a local computing device can automatically convert streaming protocol events received from a remote device into other events (e.g., events specified in the HTML5 standard).","As further described herein, local\/remote playback switching logic at a local computing device can be used to switch between local playback of acquired content on the local computing device and remote playback at another device. Described translation and playback switching technology can be used, for example, to allow users to combine the convenience of web browsing on a local computing device (e.g., a laptop computer) with the viewing experience provided by other devices (e.g., a large-screen TV).","As applied to devices described herein, the term \u201cremote\u201d is used to refer to devices other than a local computing device. Remote devices can be accessible by a local computing device over the Internet, a wide area network, a local network (e.g., an Ethernet network, Wi-Fi network, or other network covering a small geographic area, such as a home or office), or other some other network. As applied to playback or other processing described herein, the term \u201cremote\u201d is used to refer to the playback or other processing at a remote device.","A. Exemplary System with Translation Layer and Local\/Remote Switching",{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 1","b":["100","105","120","140"]},"In the example shown in , computing device  receives input . Input  comprises information representing digital content such as digital video, digital audio, or digital images. Input  also can include other types of input, such as user input. For example, computing device  can accept user input that indicates a user's selection of local playback or remote playback of the digital content.","Local\/remote playback switching logic  allows the system  to select local playback at computing device , or remote playback at a remote device. For example, in response to user input, the system  can use local\/remote playback switching logic  to switch between local playback using local playback logic , or remote playback at a remote device (e.g., a remote DLNA-compliant device) such as a digital television, game console, or other digital content playback device. Local\/remote playback switching logic  can be implemented in different ways. For example, local\/remote playback switching logic  can be included in a software element at computing device . In a described example, local\/remote playback switching logic is included in a software element that represents digital content for playback (e.g., local playback or remote playback). A software element that represents media data (e.g., video data and\/or audio data) for playback can be referred to as a media element. A software element that represents image data for playback can be referred to as an image element. Local playback logic  and playback logic at remote devices (not shown) can include, for example, video, audio, or image decoders that decode encoded video, audio, or image information, respectively.","In translation layer , translation is performed. For example, translation layer  can automatically translate markup language code and\/or scripting language code that complies with the HTML5\/W3C standard into translated code that complies with a streaming protocol (e.g., a streaming protocol specified by DLNA). The translated code can then be sent via remote playback byte stream  to another device such as a digital television, game console, or other digital content playback device for remote playback.","In practice, the systems shown herein such as system  can include additional system components, additional relationships between system components, and the like. For example, the system  can include one or more transcoders that transcode video data, audio data or image data. Transcoders can be used, for example, to convert media data to a different coded format when the media data is received in a coded format that is not supported by a remote device. The exact operations performed by such transcoders can vary depending on input and output compression formats.","The relationships shown between modules within the system  indicate general flows of information in the system; other relationships are not shown for the sake of simplicity. Depending on the implementation and the type of processing desired, modules of the system can be added, omitted, split into multiple modules, combined with other modules, and\/or replaced with like modules. Generally, the technologies described herein are generic to different operating systems or hardware and can be applied in any variety of environments to take advantage of the described features.","B. Exemplary Techniques",{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 2","FIG. 1"],"b":["200","100","200"]},"At , the system receives markup language code or scripting language code (e.g., code that complies with HTML5) associated with digital content. For example, a user of a local computing device navigates to a web page via a web browser, and the local computing device receives markup language code and\/or scripting language code via the web browser.","At , the system parses the received markup language code or scripting language code to obtain information representing an interface call associated with an element representing digital content. In described examples, elements representing digital content can include software elements (e.g., media elements that represent digital media content, or image elements that represent digital still image content) that implement interfaces. For example, markup language code can include an interface call to a media element (e.g., a <video> element or an <audio> element) that implements a media element interface (e.g., an HTMLMediaElement interface), or an image element (e.g., an <img> element) that implements an image element interface (e.g., an HTMLImageElement interface).","At , the system automatically translates the interface call into translated code that complies with a streaming protocol. For example, a translation layer at the local computing device automatically translates a call to an HTMLMediaElement interface or a call to an HTMLImageElement interface into translated code that complies with a streaming protocol (e.g., a UPnP streaming protocol specified by DLNA). The translation layer can be implemented in a software element (e.g., a media element). As described herein, translations involve receiving code to be translated and interpreting the received code in a prescribed manner. Exemplary translations for an HTMLMediaElement interface are shown in Tables 4-7, below. Exemplary translations for an HTMLImageElement interface are shown in Table 9, below. Exemplary translations described herein can be performed in different ways (e.g., by code in a media element or an image element that includes a translation layer, by code in software that is separate from a media element or an image element, by referring to a table or other data structure in which translation information is stored, or in some other way). Alternatively, the system translates interface calls differently, or translates other code.","At , the system sends information representing the translated code to a remote device operable to render the digital content (e.g., via a local network). For example, the local computing device sends information representing calls to an action of a UPnP AVTransport service via a byte stream to a remote device that is operable to render the digital content. Alternatively, the system translates interface calls differently, or translates other code.",{"@attributes":{"id":"p-0040","num":"0039"},"figref":["FIG. 3","FIG. 1"],"b":["300","100","300"]},"At , the system receives markup language code or scripting language code associated with digital media content from outside a local network. For example, a user of a local computing device on a local network navigates to a web page on the Internet via a web browser, and the local computing device receives markup language code and\/or scripting language code via the web browser.","At , the system parses the received markup language code or scripting language code to obtain information representing an interface call associated with a media element representing the digital media content. For example, markup language code can include an interface call to a <video> element or <audio> element that implements an HTMLMediaElement interface.","At , the system automatically translates the interface call into translated code that complies with a streaming protocol. For example, a translation layer at the local computing device automatically translates a call to an HTMLMediaElement interface into translated code that complies with a streaming protocol (e.g., a UPnP streaming protocol specified by DLNA). Exemplary translations for an HTMLMediaElement interface are shown in Tables 4-7, below. Exemplary translations described herein can be implemented in different ways, such as by being implemented in code in a software element that includes a translation layer. Alternatively, the system translates interface calls differently, or translates other code.","At , the system receives information indicating a streaming protocol event associated with the digital media content. For example, a local computing device receives information indicating a streaming protocol event such as a pause, play, stop, playback rate change, or error event from an audio\/visual transport service (e.g., a UPnP AVTransport service), or a change event (e.g., a volume change event) from a rendering control service (e.g., a UPnP RenderingControl service). The streaming protocol event can be generated in response to user input (e.g., video playback control input) at a remote device that renders the digital media content.","At , the system automatically translates information indicating the streaming protocol event into a markup language event associated with the media element. For example, a translation layer at the local computing device automatically translates a streaming protocol event to a corresponding HTML5 event associated with the media element. Exemplary translations for streaming protocol events are shown in Table 8, below. Exemplary translations described herein can be implemented in different ways, such as by being implemented in code in a software element that includes a translation layer. Alternatively, the system translates streaming protocol events differently, or translates other streaming protocol events.","C. Exemplary Code for Delivering Content","In examples described herein, markup language code and\/or scripting language code can be used to deliver content to a local computing device. For example, markup language and\/or scripting language code is provided by a remote server and processed by a web browser on a local computing device.","Some examples of markup language code and scripting language code described herein comply with HTML5. HTML5 is a revision of the HTML standard. However, exemplary technology described herein does not require compliance with any particular guideline or standard. For example, modified versions of exemplary interfaces, attributes, methods or events described herein, or different interfaces, attributes, methods or events, can be used.","HTML5 includes several types of elements, including the <video> element for video content, the <audio> element for audio content, and the <img> element for still image content.","The <audio> and <video> elements are examples of media elements. Media elements are used to represent media data (e.g., video data, audio data). A complete audio file or video file that includes media data can be referred to as a media resource. Media data represented by a media element can be identified by an address (e.g., a valid URL) of a media resource (e.g., an MPEG-4 video file, an MP3 audio file, or some other media resource) in an attribute of the media element (e.g., a src attribute, as shown in Table 1, below).","In HTML5, media elements implement the HTMLMediaElement interface. The HTMLMediaElement interface exposes methods and attributes relating to different states and aspects of the media element. In HTML5, the HTMLMediaElement interface includes the methods and attributes shown in Table 1, below.",{"@attributes":{"id":"p-0052","num":"0051"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Methods and attributes of HTMLMediaElement interface."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Attribute\/Method","Description"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["readonly attribute MediaError","MediaError object, which takes the"]},{"entry":["error","value of an error code (e.g.,"]},{"entry":[{},"MEDIA_ERR_ABORTED,"]},{"entry":[{},"MEDIA_ERR_NETWORK,"]},{"entry":[{},"MEDIA_ERR_DECODE, or"]},{"entry":[{},"MEDIA_ERR_SRC_NOT_SUPPORTED."]},{"entry":["attribute DOMString src","Gives an address (e.g., a valid, non-empty"]},{"entry":[{},"URL) of a media resource."]},{"entry":["readonly attribute DOMString","Address of current media resource."]},{"entry":["currentSrc",{}]},{"entry":["readonly attribute unsigned","Represents current network activity of"]},{"entry":["short networkState","media element. Events fire to indicate"]},{"entry":[{},"changes in state."]},{"entry":["const unsigned short","Possible value of networkState attribute."]},{"entry":["NETWORK_EMPTY = 0",{}]},{"entry":["const unsigned short ","Possible value of networkState attribute."]},{"entry":["NETWORK_IDLE = 1",{}]},{"entry":["const unsigned short","Possible value of networkState attribute."]},{"entry":["NETWORK_LOADING = 2",{}]},{"entry":["const unsigned short","Possible value of networkState attribute."]},{"entry":["NETWORK_NO_SOURCE 3",{}]},{"entry":["attribute DOMString preload","Provides information to user agent "]},{"entry":[{},"regarding whether resource is expected"]},{"entry":[{},"to be available for download."]},{"entry":["readonly attribute TimeRanges","TimeRanges object that represents time"]},{"entry":["buffered","ranges of a media resource that has been"]},{"entry":[{},"buffered by a user agent."]},{"entry":["void load()","Invokes a media element load algorithm."]},{"entry":["DOMString canPlay Type (in ","Returns empty string, \u201cmaybe\u201d or "]},{"entry":["DOMString type)","\u201cprobably\u201d based on confidence that"]},{"entry":[{},"media resource of the given type can be"]},{"entry":[{},"played."]},{"entry":["readonly attribute unsigned","Value that describes degree to which"]},{"entry":["short readyState","media element is ready to be rendered"]},{"entry":[{},"at current playback position."]},{"entry":["const unsigned short ","Possible value of readyState attribute."]},{"entry":["HAVE_NOTHING = 0",{}]},{"entry":["const unsigned short","Possible value of readyState attribute."]},{"entry":["HAVE_METADATA = 1",{}]},{"entry":["const unsigned short","Possible value of readyState attribute."]},{"entry":["HAVE_CURRENT_DATA = 2",{}]},{"entry":["const unsigned short","Possible value of readyState attribute."]},{"entry":["HAVE_FUTURE_DATA = 3",{}]},{"entry":["const unsigned short","Possible value of readyState attribute."]},{"entry":["HAVE_ENOUGH_DATA = 4",{}]},{"entry":["readonly attribute boolean","TRUE if the user agent is currently"]},{"entry":["seeking","seeking to a new playback position,"]},{"entry":[{},"otherwise FALSE."]},{"entry":["readonly attribute TimeRanges","TimeRanges object that represents time"]},{"entry":["seekable","ranges of a media resource to which it is"]},{"entry":[{},"possible to seek."]},{"entry":["attribute double currentTime","Current playback positioin, in seconds."]},{"entry":["readonly attribute double ","Initial playback position when media"]},{"entry":["initialTime","resource was loaded."]},{"entry":["readonly attribute double","Length of media resource, in seconds."]},{"entry":["duration",{}]},{"entry":["readonly attribute Date ","Date object that represents a media"]},{"entry":["startOffsetTime","timeline offset for the media resource."]},{"entry":["attribute double ","Default playback rate (i.e., not fast"]},{"entry":["defaultPlaybackRate","forwarding, rewinding, etc.)"]},{"entry":["attribute double playbackRate","Current playback rate (1.0 is normal"]},{"entry":[{},"speed)."]},{"entry":["readonly attribute TimeRanges","TimeRanges object that represents time"]},{"entry":["played","ranges of a media resource that a user"]},{"entry":[{},"agent has played."]},{"entry":["readonly attribute boolean","TRUE if playback is paused, otherwise"]},{"entry":["paused","FALSE."]},{"entry":["readonly attribute boolean","TRUE if playback has reached end of"]},{"entry":["ended","media resource, otherwise FALSE."]},{"entry":["attribute boolean autoplay","Indicates whether playback will begin"]},{"entry":[{},"automatically once the media resource"]},{"entry":[{},"can play without stopping."]},{"entry":["attribute boolean loop","Indicates whether media element will"]},{"entry":[{},"seek back to beginning of media"]},{"entry":[{},"resource when the end is reached."]},{"entry":["void play()","Sets paused attribute to FALSE. Loads"]},{"entry":[{},"media resource if necessary, and begins"]},{"entry":[{},"playback. If playback has ended (reached"]},{"entry":[{},"the end of the media resource), playback"]},{"entry":[{},"is restarted from the start. Causes \u201cplay\u201d"]},{"entry":[{},"event to be fired, along with either"]},{"entry":[{},"\u201cwaiting\u201d or \u201cplaying\u201d event "]},{"entry":[{},"(depending on value of readyState"]},{"entry":[{},"attribute)."]},{"entry":["void pause()","Sets paused attribute to TRUE. Loads"]},{"entry":[{},"media resource if necessary. Causes"]},{"entry":[{},"\u201ctimeupdate\u201d and \u201cpause\u201d events to be fired."]},{"entry":["attribute boolean controls","Indicates whether scripted controller has"]},{"entry":[{},"been provided. User agent can use own"]},{"entry":[{},"controls if no scripted controller is provided."]},{"entry":["attribute double volume","Current playback colume, in range from 0.0"]},{"entry":[{},"(silence) to 1.0 (full volume)."]},{"entry":["attribute boolean muted","TRUE if audio is muted, otherwise FALSE."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"In HTML5, <video> and <audio> elements also implement an HTMLVideoElement and HTMLAudioElement interface, respectively. The HTMLVideoElement interface derives from the HTMLMediaElement interface, while adding some methods and properties. The HTMLAudioElement interface also derives from the HTMLMediaElement interface, without adding methods or properties. The HTMLMediaElement interface derives from the HTMLElement interface. The HTMLSourceElement interface also relates to <audio> and <video> elements and derives from the HTMLElement interface. For brevity, the HTMLElement, HTMLVideoElement, HTMLAudioElement, and HTMLSourceElement interfaces are not described in detail herein. For further information on these and other interfaces, see the HTML5 standard.","HTML5 also defines several events that relate to media elements such as <audio> and <video> elements. In HTML5, events occur (or \u201cfire\u201d) on media elements as shown in Table 2, below.",{"@attributes":{"id":"p-0055","num":"0054"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"HTML5 events relating to media elements."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"168pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Event Name","Description"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["loadstart","Fired when user agent begins looking for media data."]},{"entry":[{},"Precondition: networkState = NETWORK_LOADING."]},{"entry":["progress","Fired when user agent is fetching media data. Precondition:"]},{"entry":[{},"networkState = NETWORK_LOADING."]},{"entry":["suspend","Fired when user agent is not fetching media data, but"]},{"entry":[{},"the entire media resource has not been downloaded. "]},{"entry":[{},"Preconditions: networkState = NETWORK_IDLE."]},{"entry":["abort","Fired when user agent stops fetching media data before the"]},{"entry":[{},"entire media resource has not been downloaded, but not due"]},{"entry":[{},"to error. Precondition: The error attribute indicates"]},{"entry":[{},"MEDIA_ERR_ABORTED; networkState = "]},{"entry":[{},"NETWORK_EMPTY or NETWORK_IDLE."]},{"entry":["error","Fired when error occurs while fetching media data."]},{"entry":[{},"Precondition: The error attribute indicates error state"]},{"entry":[{},"such as MEDIA_ERR_NETWORK. Precondition:"]},{"entry":[{},"networkState = NETWORK_EMPTY or"]},{"entry":[{},"NETWORK_IDLE."]},{"entry":["emptied","Fired when networkState has just switched to "]},{"entry":[{},"NETWORK_EMPTY state. Preconditions: "]},{"entry":[{},"networkState = NETWORK_EMPTY; other attributes"]},{"entry":[{},"are in initial states."]},{"entry":["stalled","Fired when user agent tries to fetch media data but the"]},{"entry":[{},"data is not arriving. Precondition: networkState ="]},{"entry":[{},"NETWORK_LOADING"]},{"entry":["play","Fired when playback has begun (after play() method"]},{"entry":[{},"has returned, or in response to autoplay attribute). "]},{"entry":[{},"Precondition: paused attribute has just been set to FALSE."]},{"entry":["pause","Fired when playback has been paused (after paus() method "]},{"entry":[{},"has returned). Precondition: paused attribute has just been"]},{"entry":[{},"set to TRUE."]},{"entry":["loadedmetadata","Fired when user agent has determined duration and"]},{"entry":[{},"dimensions of media resource. Precondition:"]},{"entry":[{},"readyState has just been set to"]},{"entry":[{},"HAVE_METADATA, HAVE_CURRENT_DATA,"]},{"entry":[{},"HAVE_FUTURE_DATA or HAVE_ENOUGH_DATA."]},{"entry":["loadeddata","Fired when user agent can render the media data for the "]},{"entry":[{},"first time. Precondition: readyState has just been set to"]},{"entry":[{},"HAVE_CURRENT_DATA, HAVE_FUTURE_DATA or"]},{"entry":[{},"HAVE_ENOUGH_DATA for the first time."]},{"entry":["waiting","Fired when playback is stopped because next frame is"]},{"entry":[{},"not available, but is expected to become available."]},{"entry":[{},"Preconditions: readyState has just been set to"]},{"entry":[{},"HAVE_CURRENT_DATA, ;"]},{"entry":[{},"HAVE_METATDATA, or HAVE_NOTHING"]},{"entry":[{},"paused = FALSE; either seeking = TRUE or the"]},{"entry":[{},"current playbackposition is not in a range specified"]},{"entry":[{},"by the buffered attribute."]},{"entry":["playing","Fired when playback has started. Preconditions: readyState"]},{"entry":[{},"has just been set to HAVE_FUTURE_DATA or"]},{"entry":[{},"HAVE_ENOUGH_DATA; paused = FALSE;"]},{"entry":[{},"seeking = FALSE or the current playback position is in a"]},{"entry":[{},"range specified by the buffered attribute."]},{"entry":["canplay","Fired when user agent can resume playback but it is "]},{"entry":[{},"estimated that further buffering is needed to play to the "]},{"entry":[{},"end. Precondition: readyState has just been set to"]},{"entry":[{},"HAVE_FUTURE_DATA or HAVE_ENOUGH_DATA."]},{"entry":["canplaythrough","Fired when user agent can resume playback and it is "]},{"entry":[{},"estimated that no further buffering is needed to play to "]},{"entry":[{},"the end. Precondition: readyState has just been set to"]},{"entry":[{},"HAVE_ENOUGH_DATA."]},{"entry":["seeking","Fired when seeking attribute has changed to TRUE and"]},{"entry":[{},"seek is taking long enough to fire the event."]},{"entry":["seeked","Fired when seeking attribute has changed to FALSE."]},{"entry":["timeupdate","Fired when playback position changes."]},{"entry":["ended","Fired when playback has stopped because the end of the"]},{"entry":[{},"media resource has been reached. Preconditions: "]},{"entry":[{},"currentTime is the end of the media resource; ended ="]},{"entry":[{},"TRUE."]},{"entry":["ratechange","Fired when defaultPlaybackRate or playbackRate is"]},{"entry":[{},"updated."]},{"entry":["durationchange","Fired when duration attribute is updated."]},{"entry":["volumechange","Fired when volume attribute or muted attributed is"]},{"entry":[{},"updated."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"The <img> element implements the HTMLImageElement interface. The HTMLImageElement interface exposes attributes relating to an image resource. In HTML5, the HTMLImageElement interface includes the attributes shown in Table 3, below.",{"@attributes":{"id":"p-0057","num":"0056"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 3"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Attributes of the HTMLImageElement interface."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"105pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Attribute","Description"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["attribute DOMString alt","Gives alternative text or replacement"]},{"entry":[{},"data for the image resource, to be"]},{"entry":[{},"used where the image data is not"]},{"entry":[{},"available."]},{"entry":["attribute DOMString src","Gives an address (e.g., valid, non-"]},{"entry":[{},"empty URL) of an image resource."]},{"entry":["attribute DOMString useMap","If present, indicates an associated image"]},{"entry":[{},"map for the image."]},{"entry":["attribute boolean isMap","Indicates whether the image element"]},{"entry":[{},"provides access to a server-side image"]},{"entry":[{},"map."]},{"entry":["attribute unsigned long width","Gives the rendered width of the image"]},{"entry":[{},"resource."]},{"entry":["attribute unsigned long height","Gives the rendered height of the image"]},{"entry":[{},"resource."]},{"entry":["readonly attribute ubsigned long","Gives the intrinsic width of the image"]},{"entry":["naturalWidth","resource."]},{"entry":["readonly attribute unsigned long","Gives the intrinsic height of the image"]},{"entry":["naturalHeight","resource."]},{"entry":["readonly attribute boolean complete","TRUE if image is completely "]},{"entry":[{},"downloaded or if no image is specified,"]},{"entry":[{},"otherwise FALSE."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"The HTMLImageElement interface derives from the HTMLElement interface. For brevity, the HTMLElement interface is not described in detail herein. For further information on these and other interfaces, see the HTML5 standard.","D. Exemplary Device Model","In examples described herein, devices comply with device models specified by the DLNA. However, exemplary technology described herein does not require compliance with any particular guideline or standard.","DLNA interoperability guidelines use a device model having several device classes that can be divided into one or more device categories. Each device class includes a set of device functions. The set of device functions is not tied to any particular type of device; devices having different physical attributes (e.g., different form factors) may possess the same set of device functions. Under the DLNA interoperability guidelines, a DLNA-compliant device supports at least one device class. A single DLNA-compliant device can support more than one device class.","One category of device classes is home network devices (HND). Some examples of FIND device classes are digital media server (DMS), digital media player (DMP), digital media renderer (DMR), and digital media controller (DMC). The DMS class is for devices that expose and distribute digital media content to other devices. Typical examples of DMS devices include PCs, DVRs and smartphones. The DMP class is for devices that find and render or play content on a digital media server. Typical examples of DMP devices include digital televisions, game consoles and smartphones. The DMC class is for devices that find digital media content and match them with appropriate media players or renderers. The DMR class is for devices that render or play digital media content, but do not independently find content on the network. DMC devices can be used to find content to be rendered or played on DMR devices.","Another category of device classes is mobile handheld devices (MHD). Some examples of MHD device classes are mobile digital media server (M-DMS), mobile digital media player (M-DMP), and mobile digital media controller (M-DMC). These classes share usage models with their HND counterparts (DMS, DMP and DMC, respectively), but have different media format and network requirements. Other MHD classes include mobile digital media uploader (M-DMU) and mobile digital media downloader (M-DMD). M-DMU devices can upload digital media to M-DMS devices. M-DMD devices can download digital media from M-DMS devices.","The device model specified in the DLNA interoperability guidelines is based on the Universal Plug and Play (UPnP) AV architecture specification. The UPnP AV architecture specification defines interaction between UPnP control points and UPnP AV devices.  shows an arrangement  according to a UPnP AV architecture. Control point  typically includes a user interface and coordinates interaction between digital media server (DMS)  and digital media renderer (DMR) . Control point  can control (e.g., through user interaction with a user interface) flow of media resource information between DMS  and DMR  using a UPnP AVTransport service. In the example shown in , DMS  also communicates directly with DMR . For example, DMS  transfers media resource information to DMR  using a media format and a transfer protocol (e.g., an isochronous push transfer protocol such as IEC61883\/IEEE1394 for real-time transfer, or an asynchronous pull transfer protocol such as HTTP GET) supported by DMS  and DMR . DMS  and\/or DMR  can send event notifications (e.g., event notifications relating to the state of the DMR  as it plays the media resource) back to control point .","Control point , DMS  and DMR  can each be embodied in different devices (e.g., a remote control, DVR and digital television, respectively), or two or more of the control points, DMSs and DMRs can be embodied in the same device. For example, a PC with a graphical user interface (GUI) can include control point  and DMS . A user can interact with the GUI on the PC to select digital video to be played, and to select a digital television that acts as the DMR  to play the selected video. Alternative arrangements also are possible. For example, arrangement  can include multiple control points, multiple digital media servers, and\/or multiple digital media renderers, or other types of components.","E. Exemplary Streaming Protocol","DLNA interoperability guidelines draw on industry standards developed and managed by other organizations. For example, DLNA interoperability guidelines require DLNA-compliant devices to have a network connection (e.g., Ethernet, BlueTooth, WiFi) that uses TCP\/IP, which is maintained by the Internet Engineering Task Force (IETF). For streaming of media content, DLNA interoperability guidelines specify a protocol maintained by the Universal Plug and Play (UPnP) Forum. DLNA devices typically handle some standardized media formats (e.g., JPEG, MPEG-2) by default, while other digital media formats are optional.","In examples described herein, an exemplary protocol that can be used to stream media content to devices on a local network is specified by UPnP and used by the DLNA. However, exemplary technology described herein does not require compliance with any particular guideline or standard. For example, modified versions of exemplary services, state variables, actions or events described herein, or different services, state variables, actions or events, can be used.","In the example shown in , DMS  includes several UPnP services: ContentDirectory, ConnectionManager, and AVTransport (which is optional). The ContentDirectory service allows control point  to select a media resource to be played on DMR . The ConnectionManager service (in the context of DMS ) allows DMS  to prepare and\/or terminate transfer of digital media to DMR , such as by using the PrepareForConnection( ) and ConnectionComplete( ) actions, respectively, although actions such as PrepareForConnection( ) need not be used in exemplary technologies described herein. The AVTransport service allows control point  to select the desired media resource to be transferred to DMR  (e.g., by using the SetAVTransportURI( ) action). The AVTransport service also allows control point  to control playback of the media resource (e.g., by using Play( ), Stop( ), Pause( ), Seek( ) or other actions). DMR  also includes several UPnP services: RenderingControl, ConnectionManager, and AVTransport. DMR  can include multiple instances of these services (e.g., to support rendering of more than one media resource at the same time). The ConnectionManager service (in the context of DMR ) allows DMR  to enumerate supported transfer protocols and media formats using the GetProtocolInfo( ) action. The ConnectionManager service also allows DMR  to prepare for receiving media resource information from DMS  and\/or terminate a connection, such as by using the PrepareForConnection( ) and ConnectionComplete( ) actions, respectively, although actions such as PrepareForConnection( ) need not be used in exemplary technologies described herein. The RenderingControl service allows control point  to control the rendering of a media resource at DMR . Alternative arrangements also are possible. For example, DMS  and\/or DMR  can include different combinations of services, such as a combination that omits the AVTransport service from DMS .","1. AVTransport Service","This section provides further details of the AVTransport service, including AVTransport state variables and AVTransport actions.","This section describes exemplary state variables for the AVTransport service.","The TransportState variable is a string that indicates whether the media resource associated with the AVTransport instance is playing, stopped, etc. Exemplary values for TransportState include STOPPED, PLAYING, TRANSITIONING, PAUSED_PLAYBACK, PAUSED_RECORDING, RECORDING and NO_MEDIA_PRESENT.","The TransportStatus variable is a string that indicates whether asynchronous errors (e.g., network congestion, server errors, etc.) have occurred during operation of the AVTransport service. Exemplary values for TransportStatus include ERROR_OCCURRED and OK.","The TransportPlaySpeed variable is a string representation of a fraction that indicates playback speed relative to normal speed. Exemplary values for TransportPlaySpeed include \u201c1\u201d (normal speed), \u201c\u00bd\u201d (half of normal speed), etc.","The AVTransportURI variable is a uniform resource identifier (URI) of the media resource that corresponds to the AVTransport instance. AVTransportURI also allows a control point to obtain metadata for the media resource.","The LastChange variable is a string that allows receipt of event notifications when the state of the AVTransport instance changes. LastChange contains a list of pairs (e.g., <AVTransport instance ID>, <state variable>=<new value>) that indicate the respective state changes.","Other AVTransport state variables include PlaybackStorageMedium, RecordStorageMedium, PossiblePlaybackStorageMedia, PossibleRecordStorageMedia, CurrentPlayMode, TransportPlaySpeed, RecordMediumWriteStatus, CurrentRecordQualityMode, PossibleRecordQualityModes, NumberOfTracks, CurrentTrack, CurrentTrackDuration, CurrentMediaDuration, CurrentTrackMetaData, CurrentTrackURI, AVTransportURIMetaData, NextAVTransportURI, NextAVTransportURIMetaData, RelativeTimePosition, AbsoluteTimePosition, RelativeCounterPosition, AbsoluteCounterPosition, CurrentTransportActions, A_ARG_TYPE_SeekMode, A_ARG_TYPE_SeekTarget, and A_ARG_TYPE_InstanceID.","This section describes exemplary actions for the AVTransport service.","The SetAVTransportURI( ) action specifies the URI of a media resource (e.g., a video resource) corresponding to the AVTransport instance. Input arguments for SetAVTransportURI( ) include InstanceID (which corresponds to A_ARG_TYPE_InstanceID), CurrentURI (which corresponds to AVTransportURI), and CurrentURIMetaData (which corresponds to AVTransportURIMetaData). SetAVTransportURI( ) changes TransportState to STOPPED if the media resource cannot be located at the specified URI, or if the current value of TransportState is NO_MEDIA_PRESENT. If TransportState is PLAYING, SetAVTransportURI( ) may also change TransportState to TRANSITIONING, such as where buffering is occurring before actual playback begins, before returning TransportState to PLAYING.","The GetPositionInfo( ) action returns information that describes the current position of a media resource (e.g., track number, track duration, etc.) corresponding to the AVTransport instance. The input argument for GetPositionInfo( ) is InstanceID (which corresponds to A_ARG_TYPE_InstanceID). Output arguments include Track (which corresponds to CurrentTrack), TrackDuration (which corresponds to CurrentTrackDuration), TrackMetaData (which corresponds to CurrentTrackMetaData), TrackURI (which corresponds to CurrentTrackURI), RelTime (which corresponds to RelativeTimePosition), AbsTime (which corresponds to AbsoluteTimePosition), RelCount (which corresponds to RelativeCounterPosition), and AbsCount (which corresponds to AbsoluteCounterPosition).","The Stop( ) action stops playback of a current media resource corresponding to the AVTransport instance. The input argument for Stop( ) is InstanceID (which corresponds to A_ARG_TYPE_InstanceID). Stop( ) changes TransportState to STOPPED, unless the current value of TransportState is NO_MEDIA_PRESENT. Stop( ) may also cause changes in the current position of the media resource (which can be discovered using the GetPositionInfo action).","The Play( ) action starts playback of a current media resource (e.g., at a specified speed and starting position, according to a current play mode) corresponding to an AVTransport instance. The input arguments for Play( ) are InstanceID (which corresponds to A_ARG_TYPE_InstanceID) and Speed (which corresponds to TransportPlaySpeed). Play( ) changes TransportState to PLAYING and updates TransportPlaySpeed (e.g., normal speed in forward direction). Play( ) may also change TransportState to TRANSITIONING, such as where buffering is occurring before actual playback begins.","The Pause( ) action pauses playback of a current media resource corresponding to an AVTransport instance. The input argument for Pause( ) is InstanceID (which corresponds to A_ARG_TYPE_InstanceID). Pause( ) changes TransportState to PAUSED_PLAYBACK if TransportState is PLAYING or to PAUSED_RECORDING if TransportState is RECORDING when the action is performed. Pause( ) causes the media resource to remain at its current position.","The Seek( ) action moves the current position of a current media resource corresponding to an AVTransport instance to a target position. Input arguments for Seek( ) include InstanceID (which corresponds to A_ARG_TYPE_InstanceID), Unit (which corresponds to A_ARG_TYPE_SeekMode), and Target (which corresponds to A_ARG_TYPE_SeekTarget). Seek( ) temporarily changes TransportState to TRANSITIONING if TransportState is PLAYING or STOPPED when the action is performed, before returning to the previous state when the new position is reached.","Other AVTransport actions include SetNextAVTransportURI( ), GetMediaInfo( ), GetTransportInfo( ), GetDeviceCapabilities( ), GetTransportSettings( ), Record( ), Next( ), Previous( ), SetPlayMode( ), SetRecordQualityMode( ), and GetCurrentTransportActions( ).","2. ConnectionManager Service","This section provides further details of the ConnectionManager service, including ConnectionManager state variables and ConnectionManager actions.","ConnectionManager state variables include SourceProtocolInfo, SinkProtocolInfo, CurrentConnectionIDs, A_ARG_TYPE_ConnectionStatus, A_ARG_TYPE_ConnectionManager, A_ARG_TYPE_Direction, A_ARG_TYPE_ProtocolInfo, A_ARG_TYPE_ConnectionID, A_ARG_TYPE_AVTransportID, and A_ARG_TYPE_ResID.","The GetProtocolInfo( ) action returns protocol-related information for protocols supported by an instance of ConnectionManager. Output arguments for GetProtocolInfo( ) include Source (which corresponds to SourceProtocolInfo), and Sink (which corresponds to SinkProtocolInfo).","Other ConnectionManager actions include PrepareForConnection( ), ConnectionComplete( ), GetCurrentConnectionIDs( ), and GetCurrentConnectionInfo( ).","3. RenderingControl Service","This section provides further details of the RenderingControl service, including RenderingControl state variables and RenderingControl actions.","The LastChange variable is a string that conforms to an XML schema and allows receipt of event notifications when the state of the device (as indicated by the state of a RenderingControl instance) changes. The Mute variable is a Boolean value that represents the current \u201cmute\u201d setting of an associated audio channel (with TRUE indicating that the channel has been muted). The Volume variable is an unsigned integer value that represents a current volume level (with 0 representing silence) of an associated audio channel.","Other RenderingControl state variables include PresetNameList, Brightness, Contrast, Sharpness, RedVideoGain, GreenVideoGain, BlueVideoGain, RedVideoBlackLevel, GreenVideoBlackLevel, BlueVideoBlackLevel, ColorTemperature, HorizontalKeystone, VerticalKeystone, VolumeDB, Loudness, A_ARG_TYPE_Channel, A_ARG_TYPE_InstanceID, and A_ARG_TYPE_PresetName.","The SetMute( ) action sets the Mute state variable of a RenderingControl instance and audio channel. Input arguments include InstanceID (which relates to A_ARG_TYPE_InstanceID), Channel (which relates to A_ARG_TYPE_Channel), and DesiredMute (which relates to the Mute state variable).","The SetVolume( ) action sets the Volume state variable of a corresponding RenderingControl instance and audio channel. Input arguments include InstanceID (which relates to A_ARG_TYPE_InstanceID), Channel (which relates to A_ARG_TYPE_Channel), and DesiredVolume (which relates to the Volume state variable).","Other RenderingControl actions include ListPresets( ), SelectPreset( ), GetMute( ), GetVolume( ), GetVolumeDB( ), SetVolumeDB( ), GetVolumeDBRange( ), GetLoudness( ), SetLoudness( ), and \u201cGet\u201d and \u201cSet\u201d actions for display-related state variables (e.g., Brightness, Contrast, Sharpness, RedVideoGain, etc.).","F. Exemplary Codecs and File Formats","Exemplary digital content can include digital video, digital audio, and digital still images.","Digital video described herein can be represented in a variety of formats (e.g., MPEG-2, MPEG-4, H.264\/AVC, VC-1, and\/or other formats) for raw, uncompressed video data or compressed video data. Some video formats are specified by international standards. For example, the VC-1 standard sets forth requirements for decoders to decode video encoded in a VC-1 format. A VC-1-compliant encoder and decoder (\u201ccodec\u201d) can typically provide high-quality video with good compression efficiency. Described techniques and tools can handle standard-definition video or high-definition video, 2-D or 3-D video, etc.","Digital audio described herein can be represented in a variety of file formats (e.g., WAY, MP3, AAC, WMA, and\/or other formats) for raw, uncompressed audio data or compressed audio data. Some audio formats are specified by international standards. For example, the WMA standards set forth requirements for decoders to decode video encoded in a WMA format (e.g., WMA, WMA Pro, WMA Lossless, etc.) A WMA-compliant encoder and decoder (\u201ccodec\u201d) can typically provide high-quality audio with good compression efficiency. Described techniques and tools can handle audio having different sample rates, channel configurations, etc.","Digital images described herein can be color, grey-scale, or other types of images, and can be represented in a variety of file formats (e.g., GIF, PNG, BMP, TIFF, TIFF Float32, JPEG, JPEG XR, and\/or other formats) for raw, uncompressed image data or compressed image data. For example, described techniques and tools can handle standard dynamic range (SDR) images in an SDR format such as JPEG, or high dynamic range (HDR) images in an HDR format such as PEG XR. Some image formats are specified by international standards. For example, the JPEG XR standard sets forth requirements for decoders to decode images encoded in JPEG XR format. A JPEG XR-compliant encoder and decoder (\u201ccodec\u201d) can typically provide high-quality images with good compression efficiency. The Exchangeable Image File (\u201cEXIF\u201d) format specifies a structure for image files. Image data in an EXIF file can be compressed (e.g., in JPEG format). Alternatively, image data in an EXIF file can be uncompressed (e.g., in TIFF format).","This section describes detailed examples of technology described herein. Described examples implement features such as translation and local\/remote playback switching. Described examples can allow playback of digital content at remote devices while shielding content providers from the intricacies of described streaming protocols. For web developers, content providers, and others, allowing remote playback of content can be as simple as inserting a <video> or <audio> tag in HTML code, when performed in accordance with technologies described herein. Technologies described herein can be used in combination with customizations such as scripts (e.g., JavaScript scripts) and cascading style sheets (CSS).","A. Exemplary Audio and Video Elements","This section describes detailed examples of media elements (e.g., audio and video elements) and related features, interfaces and arrangements.",{"@attributes":{"id":"p-0107","num":"0106"},"figref":["FIG. 5","FIG. 5"],"b":["500","510","515","520","530","530","530","550","530","540","550","520","560"]},"Media element  receives code . Code  can include a script that is not aware of the potential for remote playback of a media resource. In order for a transition between local and remote playback to be transparent to scripts not aware of remote playback, an extra level of indirection can be added between the script and the components implementing playback. Accordingly, media element  has a local\/remote playback switch  between local playback component  and remote playback component . Local playback component  and remote playback component  can implement a common interface. Using a common interface for both local and remote playback allows smooth transitions from local playback to remote playback, and vice versa. Playback state (e.g., current position, playback speed, volume, etc.) can be preserved in such a transition. In the example shown in , only one component is active at a time. However, when a switch happens, the playback state of the element can be preserved by reading attributes from the current playback component and setting them on the new playback component. For example, the following attributes (from the HTMLMediaElement interface) can be read on a local playback component and then set on a remote playback component: preload, defaultPlaybackRate, playbackRate, autoplay, loop, muted, volume, currentTime, paused, and src. Source elements, representing the media resources, also can be transferred from one component to the other during a switch.","Media element  can perform device selection (e.g., when more than one device is available for remote playback) based on information that describes remote devices available for remote playback. For example, remote playback component  can provide a list of remote devices and\/or other data structures to manage selection of and playback to remote device(s) .","Scripts which are aware of remote playback capabilities can subscribe to custom device connection and disconnection events (e.g., msConnected and msDisconnected events) to be notified of switching between local and remote playback. Media element  can be, for example, a feature of the Trident rendering engine functionality for Microsoft Internet Explorer that is available via the MSHTML.dll dynamic link library.","Alternatively, another arrangement for media elements can be used. For example, components that act as translation layers or local\/remote switches can be implemented outside media elements.","A media resource can be provided by a server and indicated by a single URI or a set of URIs stored as source elements. During loading, a byte stream (which can be referred to as an \u201coriginal byte stream\u201d) can be opened between a local computing device and the server providing the media resource. Opening the original byte stream on the local computing device can have advantages, such as providing accurate values for time ranges (e.g., via the buffered attribute of an HTMLMediaElement interface, as shown in Table 4, below). A streaming protocol feature allows URIs to be sent directly to remote devices, but without supporting transfer of HTTP headers to the remote device, and in described examples this feature is not relied upon for streaming content.","The original byte stream can be parsed in part to determine codec parameters and extract metadata for the media resource. Based on this information and a call to ConnectionManager::GetProtocolInfo( ), a media element can determine whether encoded media data in the byte stream can be decoded by a remote device or if transcoding is needed. In either case, another byte stream (which can be referred to as a \u201cproxied byte stream\u201d) can be opened between the local computing device and the remote device. The proxied byte stream can be created, for example, by opening a port on the local computing device and calling AVTransport::SetAVTransportURI( ) on the remote device with the URI of the local port and the metadata of the proxied byte stream.",{"@attributes":{"id":"p-0114","num":"0113"},"figref":["FIG. 6","FIG. 6"],"b":["600","640","620","610","630","630","650","620","630","650","650"]},"When a remote connection is started, the media element can subscribe to change events (e.g., LastChange events) fired by AVTransport and RenderingControl services provided by the remote device. When the media element is shut down, it can unsubscribe from these services and call AVTransport::Stop( ) on the remote device to end playback.","Alternatively, other arrangements and techniques for media element loading, startup, and shutdown can be used.","For media elements (e.g., audio elements or video elements), exemplary translations relating to the HTMLMediaElement interface are shown in Tables 4-7, below. In Tables 4-7 some methods or attributes of the HTMLMediaElement interface are not translated (indicated by \u201cn\/a\u201d). Some translations in Tables 4-7 involve translated code (e.g., a call to the src attribute leads to a call to AVTransport::SetAVTransportURI( ), while other translations involve other interpretations (e.g., a call to the buffered attribute is interpreted as measuring an amount of data buffered in an original byte stream, rather than a proxied byte stream). Calls to interfaces such as HTMLVideoElement, HTMLElement and HTMLSourceElement do not directly affect the streaming protocol and need not be translated. Alternatively, different translations, more or fewer translations, or different combinations of translations can be used.","Exemplary translations relating to network state are shown in Table 4, below.",{"@attributes":{"id":"p-0119","num":"0118"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 4"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Exemplary translations relating to network state."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"119pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Call to HTMLMediaElement",{}]},{"entry":["interface","Translation"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["attribute DOMString src","Setting this attribute starts a loading"]},{"entry":[{},"process, which triggers a call to"]},{"entry":[{},"AVTransport::SetAVTransportURI()."]},{"entry":["readonly attribute DOMString","n\/a"]},{"entry":["currentSrc",{}]},{"entry":["readonly attribute unsigned short","n\/a"]},{"entry":["networkState",{}]},{"entry":["attribute DOMString preload","n\/a"]},{"entry":["readonly attribute TimeRanges","Amount of data buffered by the original"]},{"entry":["buffered","byte stream, rather than the proxied byte"]},{"entry":[{},"stream."]},{"entry":["void load()","Triggers a loading process, which triggers"]},{"entry":[{},"a call to"]},{"entry":[{},"AVTransport::SetAVTransportURI()."]},{"entry":["DOMString canPlayType (in","Returns the string \u201cprobably\u201d if the media"]},{"entry":["DOMString type)","can be transcoded to a format the device"]},{"entry":[{},"supports, or the empty string otherwise."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"Exemplary translations relating to playback are shown in Table 5, below.",{"@attributes":{"id":"p-0121","num":"0120"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 5"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Exemplary translations relating to playback."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Call to HTMLMediaElement",{}]},{"entry":["interface","Translation"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["attribute double currentTime","AVTransport::GetPositionInfo() is called at"]},{"entry":[{},"regular intervals and returns RelTime as"]},{"entry":[{},"the current time. In between those calls,"]},{"entry":[{},"the currentTime value is interpolated"]},{"entry":[{},"linearly using the local system time. When"]},{"entry":[{},"setting the attribute, AVTransport::Seek()"]},{"entry":[{},"is called on the device."]},{"entry":["readonly attribute double","n\/a"]},{"entry":["initialTime",{}]},{"entry":["readonly attribute double","This attribute contains the TrackDuration"]},{"entry":["duration","calue returned by calls to"]},{"entry":[{},"AVTransport::GetPositionInfo()."]},{"entry":["readonly attribute Date ","n\/a"]},{"entry":["startOffsetTime",{}]},{"entry":["readonly attribute boolean","n\/a"]},{"entry":["paused",{}]},{"entry":["attribute double ","n\/a"]},{"entry":["defaultPlaybackRate",{}]},{"entry":["attribute double playbackRate","n\/a"]},{"entry":["readonly attribute TimeRanges ","The time range is updated based on"]},{"entry":["played","RelTime value returned by calls to"]},{"entry":[{},"AVTransport::GetPositionInfo()."]},{"entry":["readonly attribute TimeRanges","n\/a"]},{"entry":["seekable",{}]},{"entry":["readonly attribute boolean","n\/a"]},{"entry":["ended",{}]},{"entry":["attribute boolean autoplay","n\/a"]},{"entry":["attribute boolean loop","n\/a"]},{"entry":["void play()","AVTransport::Play() is called on the"]},{"entry":[{},"device, possibly preceded by"]},{"entry":[{},"AVTransport::SetAVTransportURI() if the"]},{"entry":[{},"media was not loaded before."]},{"entry":["void pause()","AVTransport::Pause() is called on the"]},{"entry":[{},"device, possibly preceded by"]},{"entry":[{},"AVTransport::SetAVTransportURI() if the"]},{"entry":[{},"media was not loaded before."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"Exemplary translations relating to user controls are shown in Table 6, below.",{"@attributes":{"id":"p-0123","num":"0122"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 6"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Exemplary translations of attributes relating to user controls."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"119pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Attribute of HTMLMediaElement",{}]},{"entry":["Interface","Translation"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["attribute boolean controls","n\/a"]},{"entry":["attribute double volume","When set, the value is multiplied by 100"]},{"entry":[{},"and RenderingControl::SetVolume() is"]},{"entry":[{},"called."]},{"entry":["attribute boolean muted","When set, RenderingControl::SetMute() is"]},{"entry":[{},"called."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"Table 7, below, shows two other attributes from the HTMLMediaElement interface\u2014the readyState attribute and the seeking attribute. In the example shown in Table 7, no specific translations are used that relate to the readyState attribute or the seeking attribute.",{"@attributes":{"id":"p-0125","num":"0124"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 7"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Exemplary treatment of readyState attribute and seeking attribute."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"147pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Call to HTMLMediaElement interface","Translation"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"readonly attribute unsigned short","n\/a"]},{"entry":[{},"readyState",{}]},{"entry":[{},"readonly attribute boolean seeking","n\/a"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},"During playback at a remote device, a user can perform actions to control playback. A user may, for example, use a digital television's remote control to stop playback on the digital television, which can generate one or more streaming protocol events. Streaming protocol events can be translated into scripting events and\/or markup language events. Such translations can help to keep the state of a local media player and a remote device consistent.","Exemplary translations for streaming protocol events are shown in Table 8, below. Alternatively, different translations, more or fewer translations, or different combinations of translations can be used.",{"@attributes":{"id":"p-0128","num":"0127"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 8"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Exemplary translations of streaming protocol events."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"147pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"70pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Translation"]},{"entry":["Streaming protocol event","(HTML5 event name)"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["State of original byte stream.","progress"]},{"entry":["AVTransport::LastChange is received with value","error"]},{"entry":["TransportStatus = ERROR_OCCURRED. In that",{}]},{"entry":["case, the error state of the element is set to",{}]},{"entry":["MEDIA_ERR_DECODE.",{}]},{"entry":["State of original byte stream.","stalled"]},{"entry":["Before AVTransport::Play() is called.","play"]},{"entry":["AVTransport::Pause() completes or",{}]},{"entry":["AVTransport::LastChange is received with",{}]},{"entry":["TransportState = STOPPED (if looping) or",{}]},{"entry":["TransportState = PAUSED_PLAYBACK",{}]},{"entry":["AVTransport::Play() completes or","playing"]},{"entry":["AVTransport::LastChange is received with",{}]},{"entry":["TransportState = PLAYING",{}]},{"entry":["Before AVTransport::Seek() is called.","seeking"]},{"entry":["AVTransport::Seek() completes.","seeked"]},{"entry":["RelTime value returned by calls to","timeupdate"]},{"entry":["AVTransport::GetPositionInfo() changes.41",{}]},{"entry":["AVTransport:::astChange is received with","ended"]},{"entry":["TransportState = STOPPED and the playback is",{}]},{"entry":["not looping.",{}]},{"entry":["AVTransport::LastChange is received with a","ratechange"]},{"entry":["new value for TransportPlaySpeed. The element",{}]},{"entry":["playback rate is set to this value.",{}]},{"entry":["AVTransport::LastChange is received with a","durationchange"]},{"entry":["new value for TrackDuration. The element",{}]},{"entry":["duration is set to this value.",{}]},{"entry":["RenderingControl::LastChange is received with","volumechange"]},{"entry":"a new value for either Mute or Volume."},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"B. Exemplary Image Elements","This section describes detailed examples of image elements and related features, interfaces and arrangements.",{"@attributes":{"id":"p-0131","num":"0130"},"figref":["FIG. 7","FIG. 7"],"b":["700","710","720","730","730","730","750","730","740","750","720","760"]},"Local playback component  can cause an image resource to be displayed at a local display device  at the same time that remote playback component  is causing the image resource to be displayed at a remote device. Alternatively, the image resource can be displayed at one location at a time, or at more than two locations. Transfer of playback state between local playback component  and remote playback component  is not needed. Image element  receives code . Code  can include a script that may or may not be aware of the potential for remote playback. A transition between local playback and remote playback can be transparent to a caller, although custom device connection and disconnection events (e.g., msConnected and msDisconnected events) can still be fired to indicate when connections are made and terminated.","Alternatively, another arrangement for image elements can be used. For example, components that act as translation layers can be implemented outside image elements.","For image elements, the loading process is similar to the loading process described for audio elements and video elements, above. An image resource can be provided by a server and indicated by a single URI or a set of URIs. During loading, a byte stream (which can be referred to as an \u201coriginal byte stream\u201d) can be opened between a local computing device and a server providing the image resource. The original byte stream can be parsed in part to determine codec parameters and extract metadata. Based on this information and a call to ConnectionManager::GetProtocolInfo( ), the image element can determine whether encoded data in the byte stream can be decoded by a remote device or if transcoding is needed. In either case, another byte stream (which can be referred to as a \u201cproxied byte stream\u201d) can be opened between the local computing device and the remote device. The proxied byte stream can be created by opening a port on the local machine and calling AVTransport::SetAVTransportURI( ) on the remote device with the URI of the local port and the metadata of the proxied bytestream.","Because images do not exhibit dynamic playback behavior like video and audio, on startup image elements can omit listening for LastChange events and can omit calling actions such as AVTransport::Stop( )(e.g., during shutdown) on the remote device.","For image elements, exemplary translations relating to the HTMLImageElement interface are shown in Table 9, below. In the examples shown in Table 9, no specific translations are used (indicated by \u201cn\/a\u201d in the table) for several attributes of the HTMLImageElement interface. In described examples, calls to interfaces that do not directly affect the streaming protocol (such as calls to HTMLElement) are not translated, and streaming protocol events are also not translated for image elements. Alternatively, different translations, more or fewer translations, or different combinations of translations can be used.",{"@attributes":{"id":"p-0137","num":"0136"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 9"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Exemplary translations relating to the HTMLImageElement interface."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"105pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Attribute of HTMLImageElement",{}]},{"entry":["interface","Translation"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["attribute DOMString alt","n\/a"]},{"entry":["attribute DOMString src","Setting this attribute triggers a loading"]},{"entry":[{},"process, which triggers a call to"]},{"entry":[{},"AVTransport::SetAVTransportURI()."]},{"entry":["attribute DOMString useMap","n\/a"]},{"entry":["attribute boolean isMap","n\/a"]},{"entry":["attribute unsigned long width","n\/a"]},{"entry":["attribute unsigned long height","n\/a"]},{"entry":["readonly attribute unsigned long ","n\/a"]},{"entry":["naturalWidth",{}]},{"entry":["readonly attribute unsigned long ","n\/a"]},{"entry":["naturalHeight",{}]},{"entry":["readonly attribute boolean complete","n\/a"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"C. Exemplary Extensions for Image, Audio, and Video Elements","This section describes exemplary extensions that can be implemented on image, audio, and video elements. Such extensions can allow scripts to control connections of such elements to remote devices and to obtain information about such connections. For example, extensions that comply with the HTML5 standard are described that can provide more control over playback experience. Events (e.g., msConnected and msDisconnected events) sent to servers that provide web pages can indicate switches between local and remote playback. Methods (e.g., JavaScript methods) can be used to connect to and disconnect from remote devices, test whether playback is local or remote, and transfer connections between audio and video elements to allow pre-buffering and faster transitions between pieces of media. Faster transitions can allow smoother operation of features such as playlists and smoother transitions between media segments (e.g., smoother transitions between commercial segments and other segments in video).","In described examples, a read only msPlayTo attribute is added to an interface of an element. The msPlayTo attribute returns an IPlayTo interface. Exemplary methods and attributes of the IPlayTo interface are described in Table 10, below.",{"@attributes":{"id":"p-0141","num":"0140"},"tables":{"@attributes":{"id":"TABLE-US-00010","num":"00010"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 10"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Exemplary methods and attributes of the IPlayTo interface."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"133pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Method\/Attribute","Description"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["bool connect (in optional","Connect element to a remote device. The"]},{"entry":["device)","optional device parameter identifies the"]},{"entry":[{},"device to connect to. If absent, a user"]},{"entry":[{},"interface can be displayed to let a user"]},{"entry":[{},"choose a devidce to connect to. If present, the"]},{"entry":[{},"connection can be established without"]},{"entry":[{},"prompting a user. The device parameter can"]},{"entry":[{},"be a string containing a Unique Device"]},{"entry":[{},"Name (UDN) or an object returned by the"]},{"entry":[{},"device attribute on the same interface. The"]},{"entry":[{},"method returns TRUE if the connection"]},{"entry":[{},"succeeds."]},{"entry":["void disconnect()","Disconnects element from a remote device."]},{"entry":["readonly attribute bool","Returns TRUE if the element is currently"]},{"entry":["connected","connected to a remote device."]},{"entry":["readonly attribute ","Returns an object storing information about"]},{"entry":["IPlayToDevice device","the connection."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"Exemplary attributes of the IPlayToDevice are shown in Table 11, below.",{"@attributes":{"id":"p-0143","num":"0142"},"tables":{"@attributes":{"id":"TABLE-US-00011","num":"00011"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 11"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Exemplary attributes of the IPlayToDevice interface."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Attribute","Description"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["readonly attribute string ","A name of the device which can be displayed"]},{"entry":["friendlyName","to the user."]},{"entry":["readonly attribute string ","The device manufacturer."]},{"entry":["manufacturer",{}]},{"entry":["readonly attribute string model","The device model."]},{"entry":["readonly attribute string","A URI to an icon representing the device."]},{"entry":["iconURI","This URI can be set (e.g., on an image"]},{"entry":[{},"element) for display."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"Described technologies can increase security of computing devices and networks by removing the need for installing plugins of unknown origin, while still controlling what remote servers that provide content via web pages can do with remote devices, so that users can remain in control.","This section describes exemplary approaches to user interaction.  shows an example user interface . User interface  can be used to switch seamlessly between local and remote playback of content (e.g., content provided by a media server over the Internet via a web page). Local playback can include playback at a local computing device. Remote playback can include playback at a remote device that communicates with the local computing device via a local network such as an Ethernet network, Wi-Fi network, or other network covering a small geographic area such as a home or office. Allowing a user to switch seamlessly between local playback and remote playback can provide user experience advantages, such as the ability to preview streaming video on a local computing device (e.g., a laptop PC) and then switch without interruption to viewing on a remote device with a larger screen, such as a high-definition, wide-screen television. For example, user interface  includes a local\/remote playback selection window  with check-box and button controls that can be used to select local or remote playback, and, if remote playback is selected, to select a remote device (e.g., \u201cTV\u201d or \u201cMOBILE DEVICE\u201d). User interface  also includes a playback window  comprising playback controls  for viewing and controlling playback, and a library window  for selecting video content (e.g., \u201cEPISODE 1,\u201d \u201cEPISODE 2,\u201d or \u201cEPISODE 3\u201d) to be played. User interface  can simplify the user experience by allowing remote playback and local playback to be initiated from the same user interface. Alternatively, a user interface can have more or fewer features, or different combinations of features.",{"@attributes":{"id":"p-0146","num":"0145"},"figref":"FIG. 9","b":["900","810","900"]},"At , the system receives markup language code or scripting language code associated with digital media content. At , the system performs local playback of the digital media content. At , the system receives user input via a user interface (e.g., user interface ). At , the system switches from local playback to remote playback in response to the user input. At , in response to the switching, the system parses received markup language code or scripting language code to obtain information representing an interface call associated with an element (e.g., a media element that implements an HTMLMediaElement interface) representing the digital media content. At , the system automatically translates the interface call into translated code that complies with a streaming protocol (e.g., a UPnP protocol specified by DLNA). At , the system sends information representing translated code to a remote device on a local network. The remote device is operable to render the digital media content.",{"@attributes":{"id":"p-0148","num":"0147"},"figref":"FIG. 10","b":["1000","1000"]},"With reference to , the computing environment  includes at least one processing unit  coupled to memory . In , this basic configuration  is included within a dashed line. The processing unit  executes computer-executable instructions. In a multi-processing system, multiple processing units execute computer-executable instructions to increase processing power. The memory  may be non-transitory memory, such as volatile memory (e.g., registers, cache, RAM), non-volatile memory (e.g., ROM, EEPROM, flash memory, etc.), or some combination of the two. The memory  can store software  implementing any of the technologies described herein.","A computing environment may have additional features. For example, the computing environment  includes storage , one or more input devices , one or more output devices , and one or more communication connections . An interconnection mechanism (not shown) such as a bus, controller, or network interconnects the components of the computing environment . Typically, operating system software (not shown) provides an operating environment for other software executing in the computing environment , and coordinates activities of the components of the computing environment .","The storage  may be removable or non-removable, and includes magnetic disks, magnetic tapes or cassettes, CD-ROMs, CD-RWs, DVDs, or any other non-transitory computer-readable media which can be used to store information and which can be accessed within the computing environment . The storage  can store software  containing instructions for any of the technologies described herein.","The input device(s)  may be a touch input device such as a keyboard, touchscreen, mouse, pen, or trackball, a voice input device, a scanning device, or another device that provides input to the computing environment . The output device(s)  may be a display, printer, speaker, CD- or DVD-writer, or another device that provides output from the computing environment . Some input\/output devices, such as a touchscreen, may include both input and output functionality.","The communication connection(s)  enable communication over a communication mechanism to another computing entity. The communication mechanism conveys information such as computer-executable instructions, audio\/video or other information, or other data. By way of example, and not limitation, communication mechanisms include wired or wireless techniques implemented with an electrical, optical, RF, infrared, acoustic, or other carrier.","The techniques herein can be described in the general context of computer-executable instructions, such as those included in program modules, being executed in a computing environment on a target real or virtual processor. Generally, program modules include routines, programs, libraries, objects, classes, components, data structures, etc., that perform particular tasks or implement particular abstract data types. The functionality of the program modules may be combined or split between program modules as desired in various embodiments. Computer-executable instructions for program modules may be executed within a local or distributed computing environment.","Any of the storing actions described herein can be implemented by storing in one or more computer-readable media (e.g., non-transitory computer-readable storage media or other tangible media). Any of the things described as stored can be stored in one or more computer-readable media (e.g., computer-readable storage media or other tangible media).","Any of the methods described herein can be implemented by computer-executable instructions in (e.g., encoded on) one or more computer-readable media (e.g., non-transitory computer-readable storage media or other tangible media). Such instructions can cause a computer to perform the method. The technologies described herein can be implemented in a variety of programming languages.","Any of the methods described herein can be implemented by computer-executable instructions stored in one or more non-transitory computer-readable storage devices (e.g., memory, CD-ROM, CD-RW, DVD, or the like). Such instructions can cause a computer to perform the method.",{"@attributes":{"id":"p-0158","num":"0157"},"figref":"FIG. 11","b":"1100"},"In example environment , various types of services (e.g., computing services , which can include any of the methods described herein) are provided by a cloud . For example, the cloud  can comprise a collection of computing devices, which may be located centrally or distributed, that provide cloud-based services to various types of users and devices connected via a network such as the Internet. The cloud computing environment  can be used in different ways to accomplish computing tasks. For example, with reference to the described techniques and tools, some tasks, such as processing user input and presenting a user interface, can be performed on a local computing device, while other tasks, such as storage of data to be used in subsequent processing, can be performed elsewhere in the cloud.","In example environment , the cloud  provides services for connected devices with a variety of screen capabilities A-N. Connected device A represents a device with a mid-sized screen. For example, connected device A could be a personal computer such as desktop computer, laptop, notebook, netbook, or the like. Connected device B represents a device with a small-sized screen. For example, connected device E could be a mobile phone, smart phone, personal digital assistant, tablet computer, and the like. Connected device N represents a device with a large screen. For example, connected device N could be a television (e.g., a smart television) or another device connected to a television or projector screen (e.g., a set-top box or gaming console).","A variety of services can be provided by the cloud  through one or more service providers (not shown). For example, the cloud  can provide services related to mobile computing to one or more of the various connected devices A-N. Cloud services can be customized to the screen size, display capability, or other functionality of the particular connected device (e.g., connected devices A-N). For example, cloud services can be customized for mobile devices by taking into account the screen size, input devices, and communication bandwidth limitations typically associated with mobile devices.",{"@attributes":{"id":"p-0162","num":"0161"},"figref":"FIG. 12","b":["1200","1202","1202","1204"]},"The illustrated mobile device can include a controller or processor  (e.g., signal processor, microprocessor, ASIC, or other control and processing logic circuitry) for performing such tasks as signal coding, data processing, input\/output processing, power control, and\/or other functions. An operating system  can control the allocation and usage of the components  and support for one or more application programs . The application programs can include common mobile computing applications (e.g., include email applications, calendars, contact managers, web browsers, messaging applications), or any other computing application. The mobile computing applications can further include an application for performing any of the disclosed techniques.","The illustrated mobile device can include memory . Memory  can include non-removable memory  and\/or removable memory . The non-removable memory  can include RAM, ROM, flash memory, a disk drive, or other well-known non-transitory storage technologies. The removable memory  can include flash memory or a Subscriber Identity Module (SIM) card, which is well known in GSM communication systems, or other well-known non-transitory storage technologies, such as smart cards. The memory  can be used for storing data and\/or code for running the operating system  and the application programs , including an application program for performing any of the disclosed techniques. Example data can include web pages, text, images, sound files, video data, or other data sets to be sent to and\/or received from one or more network servers or other mobile devices via one or more wired or wireless networks. The memory  can be used to store a subscriber identifier, such as an International Mobile Subscriber Identity (IMSI), and an equipment identifier, such as an International Mobile Equipment Identifier (IMEI). Such identifiers can be transmitted to a network server to identify users and equipment.","The mobile device can support one or more input devices , such as a touchscreen , microphone , camera , physical keyboard  and\/or trackball  and one or more output devices , such as a speaker  and a display device . Other possible output devices (not shown) can include a piezoelectric or other haptic output device. Some devices can serve more than one input\/output function. For example, touchscreen  and display  can be combined in a single input\/output device.","Touchscreen  can accept input in different ways. For example, capacitive touchscreens can detect touch input when an object (e.g., a fingertip) distorts or interrupts an electrical current running across the surface. As another example, resistive touchscreens can detect touch input when a pressure from an object (e.g., a fingertip or stylus) causes a compression of the physical surface. As another example, touchscreens can use optical sensors to detect touch input when beams from the optical sensors are interrupted. Physical contact with the surface of the screen is not necessary for input to be detected by some touchscreens.","A wireless modem  can be coupled to an antenna (not shown) and can support two-way communications between the processor  and external devices, as is well understood in the art. The modern  is shown generically and can include a cellular modem for communicating with the mobile communication network  and\/or other radio-based modems (e.g., Bluetooth or Wi-Fi). The wireless modern  is typically configured for communication with one or more cellular networks, such as a GSM network for data and voice communications within a single cellular network, between cellular networks, or between the mobile device and a public switched telephone network (PSTN).","The mobile device can further include at least one input\/output port , a power supply , a satellite navigation system receiver , such as a global positioning system (GPS) receiver, an accelerometer , a transceiver  (for wirelessly transmitting analog or digital signals) and\/or a physical connector , which can be a USB port, IEEE 1394 (FireWire) port, and\/or RS-232 port. The illustrated components  are not required or all-inclusive, as components can be deleted and other components can be added.","Various alternatives to the examples described herein are possible. For example, techniques described with reference to flowchart diagrams can be altered by changing the ordering of stages shown in the flowcharts, by repeating or omitting certain stages, etc. As another example, although some examples are described with reference to specific digital media formats, other formats also can be used.","The various examples described herein can be used in combination or independently. Technology described herein can be used in a computer system with software, hardware, or a combination of software and hardware for processing digital content such as digital video, digital audio or digital images, or in some other system not specifically limited to processing such digital content.","Having described and illustrated the principles of our invention with reference to described embodiments, it will be recognized that the described embodiments can be modified in arrangement and detail without departing from such principles. It should be understood that the programs, processes, or methods described herein are not related or limited to any particular type of computing environment, unless indicated otherwise. Various types of general purpose or specialized computing environments may be used with or perform operations in accordance with the teachings described herein. Elements of the described embodiments shown in software may be implemented in hardware and vice versa.","In view of the many possible embodiments to which the principles of our invention may be applied, we claim as our invention all such embodiments as may come within the scope and spirit of the following claims and equivalents thereto."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 12"}]},"DETDESC":[{},{}]}
