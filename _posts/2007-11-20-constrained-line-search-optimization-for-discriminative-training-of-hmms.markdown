---
title: Constrained line search optimization for discriminative training of HMMS
abstract: An exemplary method for optimizing a continuous density hidden Markov model (CDHMM) includes imposing a constraint for discriminative training, approximating an objective function as a smooth function of CDHMM parameters and performing a constrained line search on the smoothed function to optimize values of the CDHMM parameters. Various other methods, devices and systems are disclosed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08239332&OS=08239332&RS=08239332
owner: Microsoft Corporation
number: 08239332
owner_city: Redmond
owner_country: US
publication_date: 20071120
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["In the past few decades, discriminative training (DT) has been a very active research topic in the field of automatic speech recognition (ASR). Many DT methods have been proposed to estimate Gaussian mixture continuous density hidden continuous density Markov models (CDHMMs) in a variety of speech recognition tasks, ranging from small vocabulary isolated word recognition to large vocabulary continuous speech recognition tasks. Generally speaking, DT of CDHMMs is a typical optimization problem that starts with formulation of an objective function according to certain estimation criterion. Some popular DT criteria widely used in speech recognition include maximum mutual information (MMI), minimum error estimation (MCE), minimum word or phone error (MWE or MPE), minimum divergence (MD), and so on. Once the objective function is formulated, an effective optimization method must be used to minimize or maximize the objective function with respect to its CDHMM parameters.","With respect to optimization, in speech recognition, several different methods have been used to optimize the derived objective function, including the GPD (generalized probabilistic descent) algorithm based on the first-order gradient descent method, and the approximate second-order Quickprop method, and the extended Baum-Welch (EBW) algorithm based on growth transformation and so on.","The GPD and Quickprop methods are mainly used for optimizing the MCE-derived objective function even though they are general optimization methods which can be used for any types of differentiable objective functions. On the other hand, the EBW method has been initially proposed to maximize a rational objective function and later extended to Gaussian mixture CDHMMs for the MMI and MPE (or MWE) objective functions. Recently, the EBW method has also been generalized to optimize the MCE objective function as well as the MD objective function.","The EBW method has been widely accepted for DT because it is relatively easy to be implemented on word graphs for large scale ASR tasks and it has been demonstrated that the EBW algorithm performs well in many tasks. Essentially, all of these optimization methods attempt to search for a nearby locally optimal point of the objective function from an initial point according to both a search direction and a step size. Normally, the search direction is locally computed based on the first-order derivative (such as gradient) and the step size must be empirically determined in practice. As the result, the performance of these optimization methods highly depends on the location of the initial point and the property of objective functions. If the derived objective function is highly nonlinear, jagged and non-convex in nature, it is extremely difficult to optimize it effectively with any simple optimization algorithm, which is one of the major difficulties of DT of HMMs for speech recognition.","As described herein, various exemplary techniques can be used to optimize CDHMM parameters for applications such as speech recognition or, more generally, pattern recognition.","An exemplary method for optimizing a continuous density hidden Markov model (CDHMM) includes imposing a constraint for discriminative training, approximating an objective function as a smooth function of CDHMM parameters and performing a constrained line search on the smoothed function to optimize values of the CDHMM parameters. Various other methods, devices and systems are disclosed.","An exemplary optimization method is referred to as a constrained line search (CLS) for discriminative training (DT) of Gaussian mixture continuous density hidden Markov models (CDHMM).",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 1","b":["100","104","108","112"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 2","b":["200","220","240","200","210","212"]},"In the conventional approach , an optimization block  minimizes or maximizes the formulated objective function with respect to a set of CDHMM parameters (see, e.g., Background section). In this approach, a specification block  specifies an initial point, an iteration block  determines a search direction and a step size to move away from the initial point and a location block  finds a locally optimal point.","In the exemplary approach , an imposition block  imposes a constraint for discriminative training (DT) based on Kullback-Leibler divergence (KLD) between models (e.g., parameter values for an initial model and parameter values for an updated model). The imposition block  essentially casts discriminative training (DT) of the CDHMM as a constrained maximization problem where the constraint guarantees that an equalized updating process occurs across all parameters in a model set. Next, an approximation block  approximates the objective function as a smooth function of CDHMM parameters. As described in more detail below, based upon reasonable approximations, closed-form solutions can be obtained for CDHMM parameters. To find a locally optimal point representative of CDHMM parameter values, a search block  performs a constrained line search (CLS) on the smoothed function of the CDHMM parameters. As explained below, given a line search and some approximation assumptions, manageable closed-form solutions can be derived for CDHMMs. The exemplary method  can be used to optimize all model parameters in Gaussian mixture CDHMMs, including mean vectors, covariance matrices, mixture weights.","Trial results are presented for several benchmark speech recognition tasks, including TIDIGITS, Resource Management (RM) and Switchboard. Results indicate that the exemplary training method significantly outperforms the popular, conventional EBW (extended Baum-Welch) method in all tasks in terms of recognition performance and convergence behavior.","As mentioned, an exemplary optimization method is referred to as a CLS for DT of Gaussian mixture CDHMMs. As a general optimization method, under a unified framework, such an exemplary CLS method is capable of optimizing most DT objective functions, which are derived from one or more popular DT criteria in speech recognition, such as MMI, MCE, MPE (or MWE) and so on. In a particular aspect, a manageable closed-form optimization formula can be derived to efficiently update all parameters of Gaussian mixture CDHMMs, including mean vectors, covariance matrices and mixture weights.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 3","FIG. 2"],"b":["300","242","244","246","240","248","300","250","248","300","252"]},"In the method , the imposed constraint is based upon the nature that all the statistics are only reliable in a neighborhood of the original model. Under this constraint, the objective function can be approximated as a smooth function of CDHMM parameters and the sole critical point, if it exists, can be easily obtained by vanishing its derivative to zero. This is the role of the decision block  of the method , which is followed by a CLS to solve the constrained optimization problem.","As shown with respect to the blocks , , subject to the KLD constraint, the line search is performed either along the line joining the initial model and the critical point of the smoothed objective function if the critical point exists (the block ) or along the gradient direction at the initial point if the critical point does not exist (the block ).",{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 4","b":["400","404","400","408"]},"According to the method , a closed-form solution can be derived to solve a KLD constrained optimization as long as formulation or approximation of the KLD constraint occurs in an appropriate way. Constrained optimization relies on the fact that statistics in DT only remain reliably when models stay close to their original points. Unlike in Maximum Likelihood (ML) training, it is hard to find an auxiliary function along with which the original objective is guaranteed to increase. Hence, the exemplary method  constrains distortion of models by KLD, which is a statistically sound distribution similarity measure.","Through use of a CLS, some degree of control is introduced for updating every model to be equalized and an entire training process can be more stable and effective.","As described with respect to the trials, an exemplary method has been used to optimize the MMI as well as other DT objectives in several speech recognition tasks, including the connected digit string recognition using TIDIGITS database, the resource management (RM) task, and a large vocabulary recognition in the Switchboard task. Results show that an exemplary line search method can significantly outperform the conventional EBW method in all evaluated ASR tasks in terms of final recognition performance and convergence behavior.","Various exemplary techniques are described in more detail below, generally, as follows: a technique to formulate the discriminative training as a KLD-based constrained optimization problem under a unified framework, a constrained line search technique to solve a constrained optimization problem for DT, application of techniques to CDHMMs, and various associated algorithms. Formulation of KLD Constraint","DT criteria for a speech recognition model may assume that an acoustic model set \u039b is composed of many individual Gaussian mixture CDHMMs, each of which is represented as \u03bb=(\u03c0, A,B), where \u03c0={\u03c0,\u03c0, . . . \u03c0} is the initial state distribution and N is the number of states in the HMM, A ={a}is the transition matrix, and B is the state output distribution set, composed of Gaussian mixture distributions for all state i: b(x)=\u03a3\u03c9\u00b7(x; \u03bc, \u03a3), where B={\u03c9, \u03bc, \u03a3|1\u2266k\u2266K} with K standing for the number of Gaussian mixture components in each state, and (x; \u03bc,\u03a3) represents a multivariate Gaussian distribution with mean vector \u03bc and covariance matrix \u03a3.","For any training utterance X and its transcription W, consider how to compute an acoustic model score p(X|W) based on the composite HMM \u03bbof W. Suppose X={X, X, . . . X}, let s={s, s, . . . s} be any possible unobserved state sequence, and l={l, l, . . . l} be the associated sequence of the unobserved mixture component labels. Thus, p(X|W) is computed as Equation 1 (Eq. 1):",{"@attributes":{"id":"p-0042","num":"0041"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["X","W"],"mo":"|"}}},{"munder":{"mo":"\u2211","mi":"s"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"l"},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"msub":{"mover":{"mi":["\u03c0","_"]},"msub":{"mi":"s","mn":"1"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u220f","mrow":{"mi":"t","mo":"=","mn":"2"},"mi":"T"},"mo":"\u2062","mrow":{"msub":{"mi":"a","mrow":{"msub":[{"mi":"s","mrow":{"mi":"t","mo":"-","mn":"1"}},{"mi":["s","t"]}],"mo":"\u2062"}},"mo":"\u00b7","mrow":{"munderover":{"mo":"\u220f","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"msub":{"mi":"\u03c9","mrow":{"msub":[{"mi":["S","i"]},{"mi":["l","t"]}],"mo":"\u2062"}},"mo":"\u00b7","mrow":{"mi":"\ud835\udca9","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["x","t"]},{"mi":"\u03bc","mrow":{"msub":[{"mi":["s","i"]},{"mi":["l","t"]}],"mo":"\u2062"}}],"mo":";"},"mo":",","msub":{"mo":"\u2211","mrow":{"msub":[{"mi":["s","i"]},{"mi":["l","t"]}],"mo":"\u2062"}}}}}}}}}}}}}],"mo":"="}}},"br":{}},"Assume the whole training set consists of R different training utterances X, X, . . . , Xalong with their corresponding transcriptions, denoted as W, W, . . . , W. For a unified view to represent various discriminative training criteria, the objective functions of CDHMMs derived from various discriminative training criteria can be formulated as the following form, as Equation 2 (Eq. 2):",{"@attributes":{"id":"p-0044","num":"0043"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"\u2131","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u039b"}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"\u039b","mo":"|","msubsup":{"mrow":[{"mo":["{","}"],"mrow":{"msub":[{"mi":["X","r"]},{"mi":["W","r"]},{"mi":["\u2133","r"]}],"mo":[",",","]}},{"mi":"r","mo":"=","mn":"1"}],"mi":"R"}},"mo":[",",",",","],"mi":["f","\u03ba","G"]}}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mfrac":{"mn":"1","mi":"R"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"r","mo":"=","mn":"1"},"mi":"R"},"mo":"\u2062","mrow":{"mi":"f","mo":["(",")"],"msup":{"mrow":[{"mi":"log","mo":["[","]"],"mfrac":{"mrow":[{"msub":{"mo":"\u2211","mrow":{"mi":"E","mo":"\u2208","msub":{"mi":["\u2133","r"]}}},"mo":"\u2062","mrow":{"mrow":[{"msup":{"mi":["p","\u03ba"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["X","r"]},{"mi":["W","r"]}],"mo":"|"}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["W","r"]}}},{"mi":"G","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"W","mo":",","msub":{"mi":["W","r"]}}}}],"mo":["\u00b7","\u00b7"]}},{"msub":{"mo":"\u2211","msup":{"mi":"W","mrow":{"mi":"\u2032","mo":["\u2062","\u2208"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u2133","r"]}}}},"mo":"\u2062","mrow":{"mrow":[{"msup":{"mi":["p","\u03ba"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["X","r"]},"mo":"|","msup":{"mi":["W","\u2032"]}}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["W","\u2032"]}}}],"mo":"\u00b7"}}]}},{"mn":"1","mo":"\/","mi":"\u03c0"}]}}}}}}}]}}},"br":{},"sub":["r ","r ","r","r ","r"],"b":"500","figref":"FIG. 5"},"For constrained optimization for discriminative training, substitution of Eq. (1) into Eq. (2) yields a general DT objective function, F(\u039b). In this example, the function is a highly complicated nonlinear function, which is extremely difficult to optimize directly. An exemplary approach to smooth this function can make the following assumptions: i) assume that all competing hypothesis spaces \u2003remain unchanged during optimization; ii) use a sufficiently small scaling factor \u03ba(\u03ba<<1) to smooth the original objective function, and then assume that all the state occupancies and Gaussian kernel occupancies remain unchanged. Under these assumptions, it becomes necessary to explicitly impose the constraint that the HMM model parameters \u039b that do not significantly differ from their initial values to ensure that the above assumptions still remain valid during optimization since the initial models have been used to generate all word lattices \u2003and to calculate likelihood function p(X|\u03bb) in Eq. (1) and to accumulate statistics from training data in optimization. This kind of constraint can be quantitatively defined based on Kullback-Leibler divergence (KLD) between models. An exemplary approach formulates a discriminative training problem of CDHMMs as a constrained maximization problem. In such an approach, the updating procedure can proceed iteratively, and in each iteration, given the initial model \u039b, the optimization problem can be formulated as Equation 3 (Eq. 3):",{"@attributes":{"id":"p-0046","num":"0045"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mover":{"mi":"\u039b","mo":"^"},"mo":"=","mrow":{"mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":["max","\u039b"]},"mo":"\u2062","mrow":{"mrow":[{"mi":"\u2131","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u039b"}},{"mi":"\ud835\udc9f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u039b","mo":"||","msup":{"mi":"\u039b","mn":"0"}}}}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["subject","to"]}}},"mo":"\u2264","msup":{"mi":"\u03c1","mn":"2"}}},"mo":","}}},"br":[{},{}],"sup":["0","0"]},"To solve the constrained optimization problem cast above, an exemplary approach uses a constrained line search (CLS) algorithm. The algorithm decomposes an optimization procedure into two steps. A first step involves determining an appropriate search direction to provide a direction to search for an optimal point. In a second step, searching occurs along the direction for an optimal linear inter\/extrapolation factor which aims to maximize the objective function subject to the KLD constraint.","For purposes of explanation, any model parameter may be denoted \u03bb and its corresponding initial value in a current iteration denoted \u03bb. In the first step, the best search direction is selected. Assuming the objective function F(\u03bb) is differentiable, the critical point \u03bb*, if it exists, can be obtained by vanishing \u2207F(\u03bb) to zero. Furthermore, if \u03bb* is not a saddle point, it is reasonable to choose the line joining the critical point and the original point (i.e., \u03bb* to \u03bb) as the search direction. Otherwise, when the critical point is a saddle point or if the critical point does not exist, the following approach adopts the gradient \u2207F(\u03bb)\u2261\u2207F(\u03bb)|as the search direction. The search direction \u2003can be selected according to Equation 4 (Eq. 4):",{"@attributes":{"id":"p-0049","num":"0048"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"d","mo":"=","mrow":{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"msup":[{"mi":"\u03bb","mo":"*"},{"mi":"\u03bb","mn":"0"}],"mo":"-"},"mo":","}},{"mrow":{"mrow":{"mi":["if","exists","and","is","not","a","saddle","point"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"msup":{"mi":"\u03bb","mo":"*"}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}]},{"mtd":[{"mrow":{"mrow":{"mo":"\u2207","mrow":{"mi":"\u2131","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"\u03bb","mn":"0"}}}},"mo":","}},{"mi":"otherwise"}]}]}}}}}},"In the second step, the constrained optimization problem in Eq. 3 can be reformulated as Equation 5 (Eq. 5), a constrained search problem in a scalar space:",{"@attributes":{"id":"p-0051","num":"0050"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mover":{"mi":"\u03b5","mo":"^"},"mo":"=","mrow":{"mrow":{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mi":["max","\u03b5"]},"mo":"\u2062","mrow":{"mrow":[{"mi":"\u2131","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":"\u03bb","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b5"}}}},{"mi":"\ud835\udc9f","mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":{"mi":"\u03bb","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b5"}},"mo":"||","msup":{"mi":"\u03bb","mn":"0"}}}}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["subject","to"]}}},"mo":"\u2264","msup":{"mi":"\u03c1","mn":"2"}}},"mo":","}}},"br":{},"sup":"0"},"In general, such a constrained line search problem should be solved iteratively. However, according to an exemplary technique, if the constraint possesses the following quadratic form of Equation 6 (Eq. 6):\n\n(\u03bb\u2225\u03bb)=(\u03bb\u2212\u03bb,\u03c6)\u2261(\u03bb\u2212\u03bb)\u03c6(\u03bb\u2212\u03bb)\n","where \u03c6 is a positive-definite matrix, then the constrained line search problem can be solved directly. For conciseness of this explanation, Q() is introduced to express a quadratic form.","In the instance a solution can be obtained by finding the crossing point of the line space \u03bb(\u03b5) and the boundary of the trust region, or equivalently, solving D(\u03bb+\u03b5\u00b7d\u2225\u03bb)=\u03c1with respect to \u03b5. By substituting Eq. 6 into this Equation 7 (Eq. 7) results:\n\n\u03b5(,\u03c6)=\u03c1\n\nwhere roots can be represented as:\n",{"@attributes":{"id":"p-0055","num":"0054"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"\u03b5","mo":"=","mrow":{"mrow":[{"mo":"\u00b1","mi":"\u03c1"},{"msup":{"mi":"Q","mrow":{"mo":"-","mfrac":{"mn":["1","2"]}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["d","\u03d5"],"mo":","}}}],"mo":"\u00b7"}}}}},"An analysis of all the cases and boundary conditions provides a closed form solution for CLS with quadratic constraint. , ,  and  present graphics representing four cases (Case , ,  and ).",{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 6","b":["600","1","1","1"]},{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 7","b":["700","2","2","2"]},{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 8","b":["800","3","3","3"]},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 9","b":["900","4","4","4"]},{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 10","b":["1000","1","2","3","4"]},"CLS for Gaussian Mixture CDHMM","A general solution for CLS with quadratic constraints has been explained. This exemplary approach can be applied to provide a solution to parameters of Gaussian Mixture CDHMMs. Such an approach solves two problems: First, how to decompose constraints to update mean, covariance and Gaussian weights separately, and then reasonably approximate the constraints by quadratic forms; and second, how to obtain the critical points and corresponding conditions, for example, as listed in Table .","Constraint Decomposition for Gaussian Mixtures","It is reasonable to apply the KLD constraint to the Gaussian mixture model of each state. For the istate, it can be written as: D(b\u2225b)<\u03c1. Furthermore, decomposition of the constraint for Gaussian weights, means and covariances can occur followed by updating them separately. By applying a log-sum inequality, an upper bound of the KLD between two Gaussian mixtures is obtained as Equation 8 (Eq. 8):\n\n()\u2266()+\n","where the transpose of vector wdenotes discrete distribution composed of all the Gaussian kernel weights in the istate and where the vector eincludes\n\n(\u2225).\n","For KLD between two discrete distributions or two Gaussians, the Equation 9 (Eq. 9) represents closed-form solutions:",{"@attributes":{"id":"p-0066","num":"0065"},"maths":[{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"\ud835\udc9f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\ud835\udca9","mo":"||","msup":{"mi":"\ud835\udca9","mn":"0"}}}},{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mi":"Q","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"\u03bc","mo":"-","msup":{"mi":"\u03bc","mn":"0"}},"mo":",","msup":{"mi":"\u03a3","mn":"0"}}}},{"mi":"tr","mo":"\u2061","mrow":{"mo":["[","]"],"msup":{"mrow":[{"mi":"\u03a3","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"\u03a3","mn":"0"}}},{"mo":"-","mn":"1"}]}}},{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mfrac":{"mrow":[{"mo":["\uf603","\uf604"],"msup":{"mi":"\u03a3","mn":"0"}},{"mo":["\uf603","\uf604"],"mi":"\u03a3"}]}}],"mo":["+","+","-"],"mi":"D"}}}],"mo":"="}}},{"@attributes":{"id":"MATH-US-00007-2","num":"00007.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"\ud835\udc9f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03c9","mo":"||","msup":{"mi":"\u03c9","mn":"0"}}}},{"msup":{"mi":["\u03c9","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["log","\u03c9"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":"\u03c9","mn":"0"}}],"mo":"-"}}}],"mo":"="}}}],"br":{}},{"@attributes":{"id":"p-0067","num":"0066"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":"{","mrow":{"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mi":"Q","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["\u03bc","ik"]},"mo":"-","msubsup":{"mi":["\u03bc","ik"],"mn":"0"}},"mo":",","msubsup":{"mi":["\u03a3","ik"],"mn":"0"}}}},"mo":"\u2264","msubsup":{"mi":"\u03c1","mn":["1","2"]}}},{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":["\u2264","\u2264"],"mi":["k","K"]}}}]},{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"tr","mo":"\u2061","mrow":{"mo":["[","]"],"msup":{"mrow":[{"msub":{"mi":["\u03a3","ik"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["\u03a3","ik"],"mn":"0"}}},{"mo":"-","mn":"1"}]}}},{"mi":"log","mo":"\u2062","mfrac":{"mrow":[{"mo":["\uf603","\uf604"],"msubsup":{"mi":["\u03a3","ik"],"mn":"0"}},{"mo":["\uf603","\uf604"],"msub":{"mi":["\u03a3","ik"]}}]}}],"mo":["+","-"],"mi":"D"},"mo":"\u2264","msubsup":{"mi":"\u03c1","mn":["2","2"]}}},{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":["\u2264","\u2264"],"mi":["k","K"]}}}]},{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":["\u03c9","i","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03c9","i"]}},{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["\u03c9","i"],"mn":"0"}}],"mo":"-"}}},"mo":"\u2264","msubsup":{"mi":"\u03c1","mn":["3","2"]}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]}]},"mo":"\u2003"}}}},"br":[{},{}],"sub":["1","2","3"],"sup":["2","2","2 "]},"Based upon a basic assumption of CLS, the model parameters cannot run far away from their initial point. Given this assumption, the Taylor series approximation is applied to derive quadratic constraints for variances and weights. Consider an example that assumes a covariance matrix \u03a3that is diagonal. To guarantee the covariance is positive-definite, alternatively update the following vector logarithm domain: \u03c3=(log \u03c3, . . . , log \u03c3). Based upon the second-order Taylor series approximation e\u2212y\u22121\u02dcy\/2 and denoting y=log(\u03c3\/\u03c3), provides Equation 11 (Eq. 11):",{"@attributes":{"id":"p-0069","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"\ud835\udc9f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03a3","ik"]},"mo":"||","msubsup":{"mi":["\u03a3","ik"],"mn":"0"}}}},{"mrow":[{"mi":"tr","mo":"\u2061","mrow":{"mo":["[","]"],"msup":{"mrow":[{"msub":{"mi":["\u03a3","ik"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["\u03a3","ik"],"mn":"0"}}},{"mo":"-","mn":"1"}]}}},{"mi":"log","mo":"\u2062","mfrac":{"mrow":[{"mo":["\uf603","\uf604"],"msubsup":{"mi":["\u03a3","ik"],"mn":"0"}},{"mo":["\uf603","\uf604"],"msub":{"mi":["\u03a3","ik"]}}]}}],"mo":["+","-"],"mi":"D"}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"d","mo":"=","mn":"1"},"mi":"D"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"\u2147","msub":{"mi":["y","ikd"]}},"mo":["-","-"],"msub":{"mi":["y","ikd"]},"mn":"1"}}}}}},{"mtd":{"mrow":{"mo":["\u2248","\u2062"],"mi":{},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"d","mo":"=","mn":"1"},"mi":"D"},"mo":"\u2062","mrow":{"msubsup":{"mi":["y","ikd"],"mn":"2"},"mo":"\/","mn":"2"}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062"],"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03c3","ik"]},"mo":"-","msubsup":{"mi":["\u03c3","ik"],"mn":"0"}}},"mi":"T"},"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03c3","ik"]},"mo":"-","msubsup":{"mi":["\u03c3","ik"],"mn":"0"}}}}}}}]}}},"br":{},"sub":["ik","ik","ik"],"sup":"0"},{"@attributes":{"id":"p-0070","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"K"},"mo":"\u2062","msub":{"mi":["w","ik"]}},"mo":"=","mn":"1."}}},"br":{}},{"@attributes":{"id":"p-0071","num":"0070"},"maths":[{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"\ud835\udc9f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03c9","i"]},{"mover":{"mi":"\u03c9","mo":"~"},"mi":"i"}],"mo":"||"}}},{"msubsup":{"mi":["\u03c9","i","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03c9","i"]}},{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mover":{"mi":"\u03c9","mo":"~"},"mi":"i"}}],"mo":"-"}}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"K"},"mo":"\u2062","mrow":{"msub":[{"mi":["\u03c9","ik"]},{"mi":["z","ik"]}],"mo":["\u2062","\u2062","\u2062"],"mi":"log","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}},{"mtd":{"mrow":{"mo":["\u2248","\u2062"],"mi":{},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"K"},"mo":"\u2062","mrow":{"msub":{"mi":["\u03c9","ik"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["z","ik"]},"mo":"-","mn":"1"}}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"K"},"mo":"\u2062","mfrac":{"mrow":{"mrow":[{"msub":[{"mi":["\u03c9","ik"]},{"mi":["\u03c9","ik"]}],"mo":"\u2062"},{"mn":"2","mo":["\u2062","\u2062"],"msubsup":{"mi":["\u03c9","ik"],"mn":"0"},"msub":{"mi":["\u03c9","ik"]}},{"msubsup":{"mi":["\u03c9","ik"],"mn":"0"},"mo":"\u2062","msub":{"mi":["\u03c9","ik"]}}],"mo":["-","+"]},"msubsup":{"mi":["\u03c9","ik"],"mn":"0"}}}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"msup":[{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03c9","i"]},"mo":"-","msubsup":{"mi":["\u03c9","i"],"mn":"0"}}},"mi":"T"},{"mrow":[{"mo":["(",")"],"msubsup":{"mi":["\u03a0","i"],"mn":"0"}},{"mo":"-","mn":"1"}]}],"mo":["\u2062","\u2062"],"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03c9","i"]},"mo":"-","msubsup":{"mi":["\u03c9","i"],"mn":"0"}}}}}}}]}}},{"@attributes":{"id":"MATH-US-00011-2","num":"00011.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"where","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mrow":[{"mo":["{","}"],"msubsup":{"mi":["\u03a0","i"],"mn":"0"}},{"mi":["K","K"],"mo":"\u00d7"}]}}}}],"br":{}},{"@attributes":{"id":"p-0072","num":"0071"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msubsup":[{"mi":"\u03a0","mrow":{"mi":["i","kk"],"mo":","},"mn":"0"},{"mi":["w","ik"],"mn":"0"}],"mo":"="},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0073","num":"0072"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":"{","mrow":{"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mi":"Q","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["\u03bc","ik"]},"mo":"-","msubsup":{"mi":["\u03bc","ik"],"mn":"0"}},"mo":",","msubsup":{"mi":["\u03a3","ik"],"mn":"0"}}}},"mo":"\u2264","msubsup":{"mi":"\u03c1","mn":["1","2"]}}},{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":["\u2264","\u2264"],"mi":["k","K"]}}}]},{"mtd":[{"mrow":{"mrow":[{"mi":"Q","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["\u03c3","ik"]},"mo":"-","msubsup":{"mi":["\u03c3","ik"],"mn":"0"}},"mo":",","mi":"I"}}},{"mn":"2","mo":"\u2062","msubsup":{"mi":"\u03c1","mn":["2","2"]}}],"mo":"\u2264"}},{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":["\u2264","\u2264"],"mi":["k","K"]}}}]},{"mtd":[{"mrow":{"mrow":{"mi":"Q","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["\u03c9","i"]},"mo":"-","msubsup":{"mi":["\u03c9","i"],"mn":"0"}},"mo":",","msubsup":{"mi":["\u03a0","i"],"mn":"0"}}}},"mo":"\u2264","msubsup":{"mi":"\u03c1","mn":["3","2"]}}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]}]},"mo":"\u2003"}}}}},"Accordingly, all the constraints possess the form in Eq. 6, and general solutions can be applied directly for means, variances, and Gaussian weights.","Critical Points and Solutions","An exemplary technique determines a search direction for parameters of Gaussian mixture CDHMMs. According to this technique, critical points are derived based upon the general DT framework of Eq. 2, then concrete forms of the search directions for mean, variance and Gaussian weights are given.","According to the general assumptions in DT, the derivative of the objective function of Eq. 2 can be derived with respect to any model parameter and it can be represented as:",{"@attributes":{"id":"p-0077","num":"0076"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":"\u2207","mrow":{"mi":"\u2131","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03bb"}}},{"mrow":[{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03bb"}},"mo":"\u2062","mrow":{"mi":"\u2131","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03bb"}}},{"munder":{"mo":"\u2211","mi":"i"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"K"},"mo":"\u2062","mrow":{"msub":{"mi":["\ud835\udcaa","ik"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mo":"\u2202","mrow":{"mo":"\u2202","mi":"\u03bb"}},"mo":"\u2062","mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["\u03c9","ik"]},"mo":"\u00b7","mrow":{"mi":"\ud835\udca9","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["x","rt"]},{"mi":["\u03bc","ik"]}],"mo":";"},"mo":",","msub":{"mi":["\u03a3","ik"]}}}}}}}}}}}}],"mo":"="}],"mo":"="}}},"br":{},"b":"1000","sub":["ik","ik"]},"For mean vectors:",{"@attributes":{"id":"p-0079","num":"0078"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":"\u2207","mrow":{"mi":"\u2131","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u03bc","ik"]}}}},{"msubsup":{"mi":["\u03a3","ik"],"mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"msub":{"mi":["\ud835\udcaa","ik"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"msub":[{"mi":["\u03b3","ik"]},{"mi":["\u03bc","ik"]}],"mo":"\u2062"}],"mo":"-"}}}],"mo":"="}}},"br":{},"sub":"ik","b":["1000","1100"],"figref":["FIG. 10","FIG. 11"]},"For variances in a logarithm domain:",{"@attributes":{"id":"p-0081","num":"0080"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":"\u2207","mrow":{"mi":"\u2131","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u03c3","ik"]}}}},{"mfrac":{"msubsup":{"mi":["\u03a3","ik"],"mrow":{"mo":"-","mn":"1"}},"mrow":{"mn":"2","mo":"\u2062","msub":{"mi":["\u03b3","ik"]}}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"msub":{"mi":["\u03b3","ik"]},"mo":"\u2062","mrow":{"msub":{"mi":["\ud835\udcaa","ik"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"x","mn":"2"}}}},{"msubsup":{"mi":["\ud835\udcaa","ik"],"mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"msubsup":{"mi":["\u03b3","ik"],"mn":"2"},"mo":"\u2062","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u03c3","ik"]}}}}],"mo":["-","-"]}}}],"mo":"="}}},"br":{}},{"@attributes":{"id":"p-0082","num":"0081"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mi":["\u03c3","ik"],"mo":"*"},"mo":"=","mrow":{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mfrac":{"mrow":{"mrow":[{"msub":{"mi":["\u03b3","ik"]},"mo":"\u2062","mrow":{"msub":{"mi":["\ud835\udcaa","ik"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"x","mn":"2"}}}},{"msubsup":{"mi":["\ud835\udcaa","ik"],"mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}],"mo":"-"},"msubsup":{"mi":["\u03b3","ik"],"mn":"2"}}}}}},"br":[{},{},{}],"in-line-formulae":[{},{}],"i":["Q",",I"],"sub":["ik","ik","2","ik ","ik"],"sup":["0","2 "],"b":"1200","figref":"FIG. 12"},"For Gaussian weights:",{"@attributes":{"id":"p-0084","num":"0083"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mo":"\u2207","mrow":{"mi":"\u2131","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u03c9","i"]}}}},"mo":"=","msup":{"mrow":{"msubsup":{"mi":["\u03a0","i"],"mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"\u03b3","mrow":{"mi":"i","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"}},{"mi":"\u03b3","mrow":{"mi":"i","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2"}},{"mi":["\u03b3","iK"]}],"mo":[",",",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}}},"mi":"T"}}}},"br":{},"sub":["k","ik"]},{"@attributes":{"id":"p-0085","num":"0084"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"d","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u03c9","i"]}}},{"mrow":[{"mo":"\u2207","mrow":{"mi":"\u2131","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["\u03c9","i"],"mn":"0"}}}},{"mrow":{"mfrac":{"mn":"1","mi":"K"},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":{"mo":"\u2207","mrow":{"mi":"\u2131","mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["\u03c9","i"],"mn":"0"}}}},"mo":"\u00b7","mi":"u"}}},"mo":"\u2062","mi":"u"}],"mo":"-"}],"mo":"="}}},"br":[{},{}],"b":["4","1000"]},"To verify the effectiveness of an exemplary CLS optimization method, trials were performed on several benchmark speech recognition tasks, including connected digit string recognition using the TIDIGITS database, continuous speech recognition using the Resource Management (RM) database, and large vocabulary continuous speech recognition using the Switchboard (both the mini-train set and full h5train00 training set) database. Trial setups are summarized in Table  of . In the trials, the trial exemplary CLS method was compared to a conventional EBW method for the criterion of MMI and other criteria with higher error resolution, such as MPE and MD. For all trials using EBW, kernel dependent smoothing factor was set to twice of the corresponding denominator occupancy. When applying EBW to MPE training, I-smoothing was used with a factor set to 100 in each iteration. Some of the trial recognition results are summarized in Table  of . Additional results are summarized in Table  of .","Exemplary Computing Device",{"@attributes":{"id":"p-0087","num":"0086"},"figref":"FIG. 16","b":"1600"},"The computing device shown in  is only one example of a computer environment and is not intended to suggest any limitation as to the scope of use or functionality of the computer and network architectures. Neither should the computer environment be interpreted as having any dependency or requirement relating to any one or combination of components illustrated in the example computer environment.","With reference to , an exemplary system for implementing an exemplary optimization technique for speech recognition or other tasks that may rely on CDHMMs. In a very basic configuration, computing device  typically includes at least one processing unit  and system memory . Depending on the exact configuration and type of computing device, system memory  may be volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.) or some combination of the two. System memory  typically includes an operating system , one or more program modules , and may include program data . This basic configuration is illustrated in  by those components within dashed line .","The operating system  may include a component-based framework  that supports components (including properties and events), objects, inheritance, polymorphism, reflection, and provides an object-oriented component-based application programming interface (API), such as that of the .NET\u2122 Framework manufactured by Microsoft Corporation, Redmond, Wash.","Computing device  may have additional features or functionality. For example, computing device  may also include additional data storage devices (removable and\/or non-removable) such as, for example, magnetic disks, optical disks, or tape. Such additional storage is illustrated in  by removable storage  and non-removable storage . Computer storage media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data. System memory , removable storage  and non-removable storage  are all examples of computer storage media. Thus, computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computing device . Any such computer storage media may be part of device . Computing device  may also have input device(s)  such as keyboard, mouse, pen, voice input device, touch input device, etc. Output device(s)  such as a display, speakers, printer, etc. may also be included. These devices are well know in the art and need not be discussed at length here.","Computing device  may also contain communication connections  that allow the device to communicate with other computing devices , such as over a network. Communication connection(s)  is one example of communication media. Communication media may typically be embodied by computer readable instructions, data structures, program modules, or other data storage structure. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media. The term computer readable media as used herein includes both storage media and communication media.","Various modules and techniques may be described herein in the general context of computer-executable instructions, such as program modules, executed by one or more computers or other devices. Generally, program modules include routines, programs, objects, components, data structures, etc. for performing particular tasks or implement particular abstract data types. These program modules and the like may be executed as native code or may be downloaded and executed, such as in a virtual machine or other just-in-time compilation execution environment. Typically, the functionality of the program modules may be combined or distributed as desired in various embodiments.","An implementation of these modules and techniques may be stored on or transmitted across some form of computer readable media. Computer readable media can be any available media that can be accessed by a computer. By way of example, and not limitation, computer readable media may comprise \u201ccomputer storage media\u201d and \u201ccommunications media.\u201d","An exemplary computing device may include a processor, a user input mechanism (e.g., a mouse, a stylus, a scroll pad, etc.), a speaker, a display and control logic implemented at least in part by the processor to implement one or more of the various exemplary methods described herein for speech recognition. For speech recognition, such a device may be a cellular telephone or generally a handheld computer.","One skilled in the relevant art may recognize, however, that the techniques described herein may be practiced without one or more of the specific details, or with other methods, resources, materials, etc. In other instances, well known structures, resources, or operations have not been shown or described in detail merely to avoid obscuring aspects of various exemplary techniques. While various examples and applications have been illustrated and described, it is to be understood that the techniques are not limited to the precise configuration and resources described above. Various modifications, changes, and variations apparent to those skilled in the art may be made in the arrangement, operation, and details of the methods, systems, etc., disclosed herein without departing from their practical scope."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Non-limiting and non-exhaustive embodiments are described with reference to the following figures, wherein like reference numerals refer to like parts throughout the various views unless otherwise specified.",{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 6","b":"1"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 7","b":"2"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 8","b":"3"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 9","b":"4"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 10","b":["1","2","3","4"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 11","b":["1","2","3","4"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 12","b":["1","2","3","4"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 16"}]},"DETDESC":[{},{}]}
