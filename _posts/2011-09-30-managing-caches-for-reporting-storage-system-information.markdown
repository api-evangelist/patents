---
title: Managing caches for reporting storage system information
abstract: A method is used in managing caches for reporting storage system information. A cache is created. The cache includes information associated with a set of storage objects of a data storage system. The information of the cache is made available to a virtual system. The virtual system uses the information for reporting storage system information. The virtual system is notified for retrieving updated storage system information from the cache.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08959287&OS=08959287&RS=08959287
owner: EMC Corporation
number: 08959287
owner_city: Hopkinton
owner_country: US
publication_date: 20110930
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF EMBODIMENT(S)"],"p":["1. Technical Field","This application relates to managing caches for reporting storage system information.","2. Description of Related Art","Computer systems may include different resources used by one or more host processors. Resources and processors in a computer system may be interconnected by one or more communication connections. These resources may include, for example, data storage systems, such as the Symmetrix\u2122 or CLARiiON\u2122 (also referred to herein as Clariion) family of data storage systems manufactured by EMC Corporation. These data storage systems may be coupled to one or more host processors and provide storage services to each host processor. An example data storage system may include one or more data storage devices, such as those of the Symmetrix\u2122 family, that are connected together and may be used to provide common data storage for one or more host processors in a computer system.","In a common implementation, a Storage Area Network (SAN) is used to connect computing devices with a large number of storage devices. Management and modeling programs may be used to manage these complex computing environments.","Storage Management Initiative Specification (SMI-S), and Common Information Model (CIM) technologies, are widely used for managing storage devices and storage environments. CIM is described further below. The SMI-S is a standard management interface that allows different classes of hardware and software products to interoperate for monitoring and controlling resources. For example, the SMI-S permits storage management systems to identify, classify, monitor, and control physical and logical resources in a SAN. The SMI-S is based on CIM, and Web-Based Enterprise Management (WBEM) architecture. CIM is a model for describing management information, and WBEM is an architecture for using Internet technologies to manage systems and networks. The SMI-S uses CIM to define objects that represent storage entities such as Logical Unit Numbers (LUNs), disks, storage subsystems, switches, and hosts. (In many, but not all cases, the term \u201cvolume\u201d or \u201clogical volume\u201d is interchangeable with the term \u201cLUN\u201d.) CIM also defines the associations that may or may not exist between these objects, such as a disk being associated to a storage subsystem because it physically resides in the storage subsystem.","The CIM objects mentioned above may be managed by a CIM object manager (CIMOM). A storage management software application can use a CIM client to connect to a CIMOM, to retrieve information about the storage entities that the CIMOM manages, and also to perform active configuration of the storage entities. Storage management software that uses a CIM client may be called a CIM client application. For example, SMI-S describes how a current storage LUN is mapped. A CIM server is a CIMOM and a set of CIM providers. The SMI-S describes several methods for assigning a LUN from a disk storage system to a host, or for adding a LUN to a disk storage system.","Virtual processing such as VMware is another known area that offers advantages in data processing, including in the area of apparent configuration to a user. It would be advancement in both the virtual processing and data storage arts to exploit better the respective individual capabilities for reaping more and better benefits for users in the respective fields.","A method is used in managing caches for reporting storage system information. A cache is created. The cache includes information associated with a set of storage objects of a data storage system. The information of the cache is made available to a virtual system. The virtual system uses the information for reporting storage system information. The virtual system is notified for retrieving updated storage system information from the cache.","Described below is a technique for use in managing caches for reporting storage system information, which technique may be used to provide, among other things, creating a cache that includes information associated with a set of storage objects of a data storage system, making the information of the cache available to a virtual system, and notifying the virtual system regarding a change in the information such that the virtual system retrieves updated storage system information from the cache.","Generally, a virtual system (e.g., a virtual data center manager client or simply referred to as \u201cvirtual client\u201d) creates a session with a data storage system for reporting storage provisioned for the virtual system by the data storage system. A session is associated with a session view. A session view is represented by a usage context that includes storage system information associated with storage objects provisioned for use by a virtual system. Typically, a virtual system may send one or more queries to a data storage system for gathering storage system information associated with a usage context. Further, a virtual system may send one or more queries for gathering updated storage system information associated with changes to configuration of storage objects and health status of a data storage system.","Conventionally, a virtual system sends a query to a data storage system for retrieving storage system information regarding storage provisioned for the virtual system. In such a conventional system, a data storage system gathers a large amount of information associated with each storage object provisioned for the virtual system. In such a conventional case, the large amount of information is gathered regardless of a change in the information since the information has last been retrieved by the virtual system. Hence, in such a conventional system, a large number of virtual systems may consume a large amount of memory of a data storage system by creating a large number of sessions, which as a result, processes a large amount of data, thereby increasing a response time for a query, and an amount of overhead involved in processing the query by a storage processor of the data storage system. Further, in such a conventional system, a query initiated by a virtual system takes longer to complete because the query attempts to retrieve information regarding every storage object provisioned by the virtual system even when the virtual system needs to report information associated with a subset of the storage objects provisioned for use by the virtual system.","By contrast, in at least some implementations in accordance with the current technique as described herein, creating a cache for storing and managing storage system information regarding a set of storage objects associated with a usage context of a virtual client enables the virtual client to report the storage system information to a user faster and efficiently. In at least one embodiment of the current technique, when a virtual client creates a first session with a data storage system, a cache is created using storage system information of storage objects provisioned for use by the virtual client such that the storage system information is retrieved from the data storage system by performing one or more queries. However, in such a case, the data storage system may take a long time to initialize the cache with the storage system information, thereby causing a delay in responding to a first query issued by the virtual client. But, in such a case, once the data storage initializes the cache with the storage system information associated with storage objects, subsequent queries issued by the virtual client for retrieving storage system information are processed faster and efficiently. Thus, in at least one embodiment of the current technique, sessions created by a virtual client for querying storage system information from a data storage system retrieves the storage system information from the cache created by the data storage system. Further, in at least one embodiment of the current technique, a registration mechanism is used for notifying a virtual client regarding a change in storage system information associated with storage objects provisioned for the virtual client. Further, in at least one embodiment of the current technique, a notification regarding a change in storage system information is processed by the cache such that a virtual client is notified regarding the change, and a session view of the virtual client is updated in order to maintain consistent storage system information across each session view created by the virtual client. Additionally, in at least one embodiment of the current technique, the cache may be rebuild completely when the cache is unable to process notifications regarding changes to storage system information and becomes out of sync with up-to-date storage system information.","By contrast, in at least some implementations in accordance with the current technique as described herein, the use of the managing caches for reporting storage system information can provide one or more of the following advantages: reducing an amount of time for reporting storage system information for a virtual system by retrieving the storage system information from a cache that is managed by a data storage system, increasing performance of a data storage system by managing a cache of storage system information and providing storage system information stored in the cache to virtual clients, improving scalability of a data storage system by efficiently managing a large amount of queries from virtual clients and decreasing an amount of overhead involved in processing storage system information by using a cache.","Referring now to , shown is an example of an embodiment of a computer system that may be used in connection with performing the storage mapping technique described herein. The computer system  includes one or more data storage systems  connected to servers (also referred to as hosts or host systems) -through communication medium . At least one of the host systems -includes or provides one or more virtual machines as described below. The system  also includes a management system  connected to one or more data storage systems  through communication medium . In this embodiment of the computer system , the management system , and the N servers or hosts -may access the data storage systems , for example, in performing input\/output (I\/O) operations, data requests, and other operations. The communication medium  may be any one or more of a variety of networks or other type of communication connections as known to those skilled in the art. Each of the communication mediums  and  may be a network connection, bus, and\/or other type of data link, such as a hardwire or other connections known in the art. For example, the communication medium  may be the Internet, an intranet, network or other wireless or other hardwired connection(s) by which the host systems -may access and communicate with the data storage systems , and may also communicate with other components (not shown) that may be included in the computer system . In at least one embodiment, the communication medium  may be a LAN connection and the communication medium  may be an iSCSI or Fibre Channel connection.","Each of the host systems -and the data storage systems  included in the computer system  may be connected to the communication medium  by any one of a variety of connections as may be provided and supported in accordance with the type of communication medium . Similarly, the management system  may be connected to the communication medium  by any one of a variety of connections in accordance with the type of communication medium . The processors included in the host computer systems -and management system  may be any one of a variety of proprietary or commercially available single or multi-processor system, such as an Intel-based processor, or other type of commercially available processor able to support traffic in accordance with each particular embodiment and application.","It should be noted that the particular examples of the hardware and software that may be included in the data storage systems  and in at least one of the host computers -are described herein in more detail, and may vary with each particular embodiment. Each of the host computers -, the management system  and data storage systems may all be located at the same physical site, or, alternatively, may also be located in different physical locations. In connection with communication mediums  and , a variety of different communication protocols may be used such as SCSI, Fibre Channel, iSCSI, and the like. Some or all of the connections by which the hosts, management system, and data storage system may be connected to their respective communication medium may pass through other communication devices, such as a Connectrix or other switching equipment that may exist such as a phone line, a repeater, a multiplexer or even a satellite. In one embodiment, the hosts may communicate with the data storage systems over an iSCSI or fibre channel connection and the management system may communicate with the data storage systems over a separate network connection using TCP\/IP. It should be noted that although  illustrates communications between the hosts and data storage systems being over a first connection, and communications between the management system and the data storage systems being over a second different connection, an embodiment may also use the same connection. The particular type and number of connections may vary in accordance with particulars of each embodiment.","Each of the host computer systems may perform different types of data operations in accordance with different types of tasks. In the embodiment of , any one of the host computers -may issue a data request to the data storage systems  to perform a data operation. For example, an application executing on one of the host computers -may perform a read or write operation resulting in one or more data requests to the data storage systems .","The management system  may be used in connection with management of the data storage systems . The management system  may include hardware and\/or software components. The management system  may include one or more computer processors connected to one or more I\/O devices such as, for example, a display or other output device, and an input device such as, for example, a keyboard, mouse, and the like. A data storage system manager may, for example, view information about a current storage volume configuration on a display device of the management system . The manager may also configure a data storage system, for example, by using management software to define a logical grouping of logically defined devices, referred to elsewhere herein as a storage group (SG), and restrict access to the logical group.","An embodiment of the data storage systems  may include one or more data storage systems. Each of the data storage systems may include one or more data storage devices, such as disks. One or more data storage systems may be manufactured by one or more different vendors. Each of the data storage systems included in  may be inter-connected (not shown). Additionally, the data storage systems may also be connected to the host systems through any one or more communication connections that may vary with each particular embodiment and device in accordance with the different protocols used in a particular embodiment. The type of communication connection used may vary with certain system parameters and requirements, such as those related to bandwidth and throughput required in accordance with a rate of I\/O requests as may be issued by the host computer systems, for example, to the data storage systems .","It should be noted that each of the data storage systems may operate stand-alone, or may also included as part of a storage area network (SAN) that includes, for example, other components such as other data storage systems.","Each of the data storage systems of element  may include a plurality of disk devices or volumes. The particular data storage systems and examples as described herein for purposes of illustration should not be construed as a limitation. Other types of commercially available data storage systems, as well as processors and hardware controlling access to these particular devices, may also be included in an embodiment.","Servers or host systems, such as -, provide data and access control information through channels to the storage systems, and the storage systems may also provide data to the host systems also through the channels. The host systems do not address the disk drives of the storage systems directly, but rather access to data may be provided to one or more host systems from what the host systems view as a plurality of logical devices or logical volumes. The logical volumes may or may not correspond to the actual disk drives. For example, one or more logical volumes may reside on a single physical disk drive. Data in a single storage system may be accessed by multiple hosts allowing the hosts to share the data residing therein. A LUN (logical unit number) may be used to refer to one of the foregoing logically defined devices or volumes.","Referring now to , shown is an example  of components that may be used in connection with the current technique described herein. The example  may represent components illustrated in connection of  configured in a storage area network (SAN). Included in the example  are data storage systems  and , a switch , and hosts or servers and . The switch  may be used in connection with facilitating communications between each of the hosts and and the data storage systems  and . Communications between a host and the data storage system  may be defined in terms of a path. Host communicates with the data storage system  over a path designated as . Path is formed by the starting point, the HBA or host bus adapter and the ending point, port A of the receiving data storage system . Host communicates with the data storage system  over two paths designated as and . Path is formed by the starting point, the HBA , and the ending point, port d of the receiving data storage system . Path is formed by the starting point, the HBA , and the ending point, port b of the receiving data storage system . It should be noted that different HBAs from the same or different hosts may also communicate with the data storage system through a same port of the data storage system although each path , , and use a different port. An embodiment may represent a path using the WWN (world wide name) of a host's HBA and the WWN of a data storage system port receiving the request. As known to those skilled in the art, a WWN is a unique number assigned by a recognized naming authority that identifies a connection or a set of connections to the network. As also known to those skilled in the art, various networking technologies that may be used in an embodiment make use of WWNs.","Each HBA may include one or more ports although in the example illustrated, each HBA has only a single port. As represented with element , connections between the hosts using switch  may be made with respect to data storage system . Although only two data storage system are illustrated for purposes of simplicity in illustration, each of the hosts may have connections to other data storage systems in the SAN. Additionally, each host may be connected to the data storage systems ,  using other connections, including direct cabling, than as illustrated in .","The data storage systems ,  are illustrated as each including one or more storage devices , , one or more computer processors , , an operating system , , a storage management component , , and other inputs, outputs and\/or components , , which may include all or some of other logic described below.","An example of an embodiment of the data storage system  is the CLARiiON\u2122 data storage system by EMC Corporation which includes two computer processors as represented by the element  although an embodiment may include a different number of processors for use in connection with the storage mapping technique described herein.","The one or more storage devices  may represent one or more physical devices, such as disk drives, that may be accessed in logical units (e.g., as LUNs) as described elsewhere herein. The operating system  may be any one of a variety of commercially available, proprietary, or other operating system capable of execution by the one or more computer processors  in accordance with the particulars of the data storage system .","As used herein, the term network storage refers generally to storage systems and storage array technology, including storage area network (SAN) implementations, network attached storage (NAS) implementations, and other storage architectures that provide a level of virtualization for underlying physical units of storage. In general, such storage architectures provide a useful mechanism for sharing storage resources amongst computational systems. In some cases, computational systems that share storage resources may be organized as a coordinated system (e.g., as a cluster or cooperatively managed pool of computational resources or virtualization systems). For example, in a failover cluster it may be desirable to share (or at least failover) virtual machine access to some storage units. Similarly, in a managed collection of virtualization systems, it may be desirable to migrate or otherwise transition virtual machine computations from one virtualization system to another. In some cases, at least some computational systems may operate independently of each other, e.g., employing independent and exclusive units of storage allocated from a storage pool (or pools) provided and\/or managed using shared network storage.","Generally, either or both of the underlying computer systems and storage systems may be organizationally and\/or geographically distributed. For example, some shared storage (particularly storage for data replication, fault tolerance, backup and disaster recovery) may reside remotely from a computational system that uses it. Of course, as will be appreciated by persons of ordinary skill in the art, remoteness of shared storage is a matter of degree. For example, depending on the configuration, network storage may reside across the globe, across the building, across the data center or across the rack or enclosure.","While embodiments of the current technique, particularly cluster-organized and\/or enterprise scale systems, may build upon or exploit data distribution, replication and management features of modern network storage technology, further embodiments may be used in more modest computational systems that employ network storage technology. For example, even a single computer system may employ SAN-type storage facilities in its storage architecture. Thus, while some embodiments utilize network storage that can be shared and while at least some underlying elements thereof may be remote, persons of ordinary skill in the art will understand that for at least some embodiments, network storage need not be shared or remote.","In some embodiments of the current technique, particularly those that use SAN-type storage arrays, block-level I\/O access to virtual machine state data can afford performance advantages. Similarly, encapsulation and\/or isolation techniques may be employed in some encodings of virtual machine state data to limit access (e.g., by a guest application or operating system) to underlying data. Accordingly, certain embodiments can be provided in which non-commingled, encapsulated representations of virtual machine state are maintained in distinct storage volumes (or LUNs) of a SAN. Nonetheless, other embodiments, including those that use NAS-type or file-system-mediated access mechanisms may still allow a virtualization system to leverage storage system functionality in support of operations such as virtual machine migration, movement, cloning, check pointing, rollback and\/or failover using suitable codings of virtual machine state data.","For concreteness, embodiments are described which are based on facilities, terminology and operations typical of certain processor architectures and systems, and based on terminology typical of certain operating systems, virtualization systems, storage systems and network protocols and\/or services. That said, the embodiments are general to a wide variety of processor and system architectures (including both single and multi-processor architectures based on any of a variety of instruction set architectures), to numerous operating system implementations and to systems in which both conventional and virtualized hardware may be provided. As described herein, the embodiments are also general to a variety of storage architectures, including storage virtualization systems such as those based on storage area network (SAN) or network attached storage (NAS) technologies.","Accordingly, in view of the foregoing and without limitation on the range of underlying processor, hardware or system architectures, operating systems, storage architectures or virtualization techniques that may be used in embodiments of the current technique are described. Based on these descriptions, and on the claims that follow, persons of ordinary skill in the art will appreciate a broad range of suitable embodiments.","With respect to computational systems, generally,  depicts a collection or cluster of computational systems in which an embodiment of the current technique may be provided. In particular,  illustrates a collection or cluster in which at least a collection of virtualization systems , B, C (but more generally, a mix of virtualization systems and conventional hardware systems such as server ) are configured to share storage resources. In the illustrated collection or cluster, constituent computational systems (e.g., virtualization systems , B, C and server ) are coupled to network  which is illustrated (for simplicity) as a local area network with client systems A, B and communications interface , but will be more generally understood to represent any of a variety of networked information systems including configurations coupled to wide area networks and\/or the Internet using any of a variety of communications media and protocols. One or more of systems , B, C,  may be, include, or be included in hosts , ","In the illustrated collection, storage area network (SAN) technology is used for at least some storage needs of computational systems participating in the collection. (The current technique can also be used for NAS storage allocated to a virtual machine environment.) In general, network storage systems (including SAN-based system ) provide a level of virtualization for underlying physical storage elements (e.g., individual disks, tapes and\/or other media), where the characteristics and\/or configuration of particular storage elements may be hidden from the systems that employ the storage. SAN-based systems typically provide an abstraction of storage pools from which individual storage units or volumes may be allocated or provisioned for block level I\/O access. In the illustrated collection, a switched fabric topology consistent with Fibre Channel SAN technology is shown in which switches A, B, C and\/or directors are used to mediate high bandwidth access (typically using a SCSI, Small Computer System Interface, command set) to an extensible and potentially heterogeneous set of storage resources A, B, C, D, E, F, G, e.g., SATA (Serial ATA) and\/or SCSI disks, tape drives, as well as arrays thereof (e.g., RAID, i.e., Redundant Array of Inexpensive Disks). Such resources may be distributed and (if desirable) may provide data replication and\/or off-site storage elements. Fibre Channel is a gigabit-speed network technology standardized in the T11 Technical Committee of the Inter National Committee for Information Technology Standards (INCITS). One or more of switches A, B, C may be, include, or be included in switch . One or more of storage resources A, B, C, D, E, F, G, may be, include, or be included in one or more of data storage systems , .","In general, a variety of different types of interconnect entities, including, without limitation, directors, switches, hubs, routers, gateways, and bridges may be used in topologies (or sub-topologies) that include point-to-point, arbitrated loop, switched fabric portions. Fibre Channel and non-Fibre Channel technologies including those based on iSCSI protocols (i.e., SCSI command set over TCP\/IP) or ATA-over-Ethernet (AoE) protocols may be used in embodiments of the storage mapping technique. Similarly, any of a variety of media including copper pair, optical fiber, etc. may be used in a network storage system such as SAN .","Although not specifically illustrated in , persons of ordinary skill in the art will recognize that physical storage is typically organized into storage pools, possibly in the form of RAID groups\/sets. Storage pools are then subdivided into storage units (e.g., storage volumes  that are exposed to computer systems, e.g., as a SCSI LUN on a SAN communicating via Fibre Channel, iSCSI, etc.). In some environments, storage pools may be nested in a hierarchy, where pools are divided into sub-pools. In at least some cases, the term LUN may represent an address for an individual storage unit, and by extension, an identifier for a virtual disk of other storage device presented by a network storage system such as SAN .","Embodiments of the current technique may be understood in the context of virtual machines  (or virtual computers) that are presented or emulated within a virtualization system such as virtualization system  executing on underlying hardware facilities . However, in addition, migration from (or to) a computational system embodied as a conventional hardware-oriented system may be supported in some systems configured in accordance with the current technique. Nonetheless, for simplicity of description and ease of understanding, embodiments are described in which individual computational systems are embodied as virtualization systems that support one or more virtual machines.","Although certain virtualization strategies\/designs are described herein, virtualization system  is representative of a wide variety of designs and implementations in which underlying hardware resources are presented to software (typically to operating system software and\/or applications) as virtualized instances of computational systems that may or may not precisely correspond to the underlying physical hardware.","With respect to virtualization systems, the term virtualization system as used herein refers to any one of an individual computer system with virtual machine management functionality, a virtual machine host, an aggregation of an individual computer system with virtual machine management functionality and one or more virtual machine hosts communicatively coupled with the individual computer system, etc. Examples of virtualization systems include commercial implementations, such as, for example and without limitation, VMware\u00ae ESX Server\u2122 (VMware and ESX Server are trademarks of VMware, Inc.), VMware\u00ae Server, and VMware\u00ae Workstation, available from VMware, Inc., Palo Alto, Calif.; operating systems with virtualization support, such as Microsoft\u00ae Virtual Server ; and open-source implementations such as, for example and without limitation, available from XenSource, Inc.","As is well known in the field of computer science, a virtual machine is a software abstraction\u2014a \u201cvirtualization\u201d\u2014of an actual physical computer system. Some interface is generally provided between the guest software within a VM and the various hardware components and devices in the underlying hardware platform. This interface-which can generally be termed \u201cvirtualization layer\u201d\u2014may include one or more software components and\/or layers, possibly including one or more of the software components known in the field of virtual machine technology as \u201cvirtual machine monitors\u201d (VMMs), \u201chypervisors,\u201d or virtualization \u201ckernels.\u201d","Because virtualization terminology has evolved over time, these terms (when used in the art) do not always provide clear distinctions between the software layers and components to which they refer. For example, the term \u201chypervisor\u201d is often used to describe both a VMM and a kernel together, either as separate but cooperating components or with one or more VMMs incorporated wholly or partially into the kernel itself. However, the term \u201chypervisor\u201d is sometimes used instead to mean some variant of a VMM alone, which interfaces with some other software layer(s) or component(s) to support the virtualization. Moreover, in some systems, some virtualization code is included in at least one \u201csuperior\u201d VM to facilitate the operations of other VMs. Furthermore, specific software support for VMs is sometimes included in the host OS itself.","Embodiments are described and illustrated herein primarily as including one or more virtual machine monitors that appear as separate entities from other components of the virtualization software. This paradigm for illustrating virtual machine monitors is only for the sake of simplicity and clarity and by way of illustration. Differing functional boundaries may be appropriate for differing implementations. In general, functionality and software components\/structures described herein can be implemented in any of a variety of appropriate places within the overall structure of the virtualization software (or overall software environment that includes the virtualization software).","With respect to the virtual machine monitor, in view of the above, and without limitation, an interface usually exists between a VM and an underlying platform which is responsible for executing VM-issued instructions and transferring data to and from memory and storage devices or underlying hardware. A VMM is usually a thin piece of software that runs directly on top of a host, or directly on the hardware, and virtualizes at least some of the resources of the physical host machine. The interface exported to the VM is then the same as the hardware interface of a physical machine. In some cases, the interface largely corresponds to the architecture, resources and device complements of the underlying physical hardware; however, in other cases it need not.","The VMM usually tracks and either forwards to some form of operating system, or itself schedules and handles, all requests by its VM for machine resources, as well as various faults and interrupts. An interrupt handling mechanism is therefore included in the VMM. As is well known, in the Intel IA-32 (\u201cx86\u201d) architecture, such an interrupt\/exception handling mechanism normally includes an interrupt descriptor table (IDT), or some similar table, which is typically a data structure that uses information in the interrupt signal to point to an entry address for a set of instructions that are to be executed whenever the interrupt\/exception occurs. In the Intel IA-64 architecture, the interrupt table itself contains interrupt handling code and instead of looking up a target address from the interrupt table, it starts execution from an offset from the start of the interrupt when a fault or interrupt occurs. Analogous mechanisms are found in other architectures. Based on the description herein, interrupt handlers may be adapted to correspond to any appropriate interrupt\/exception handling mechanism.","Although the VM (and thus applications executing in the VM and their users) cannot usually detect the presence of the VMM, the VMM and the VM may be viewed as together forming a single virtual computer. They are shown and described herein as separate components for the sake of clarity and to emphasize the virtual machine abstraction achieved. However, the boundary between VM and VMM is somewhat arbitrary. For example, while various virtualized hardware components such as virtual CPU(s), virtual memory, virtual disks, and virtual device(s) including virtual timers are presented as part of a VM for the sake of conceptual simplicity, in some virtualization system implementations, these \u201ccomponents\u201d are at least partially implemented as constructs or emulations exposed to the VM by the VMM. One advantage of such an arrangement is that the VMM may be set up to expose \u201cgeneric\u201d devices, which facilitate VM migration and hardware platform-independence. In general, such functionality may be said to exist in the VM or the VMM.","It is noted that while VMMs have been illustrated as executing on underlying system hardware, many implementations based on the basic abstraction may be implemented. In particular, some implementations of VMMs (and associated virtual machines) execute in coordination with a kernel that itself executes on underlying system hardware, while other implementations are hosted by an operating system executing on the underlying system hardware and VMMs (and associated virtual machines) executed in coordination with the host operating system. Such configurations, sometimes described as \u201chosted\u201d and \u201cnon-hosted\u201d configurations, are illustrated in . However, the description herein refers to the physical system that hosts a virtual machine(s) and supporting components, whether in the \u201chosted\u201d or \u201cnon-hosted\u201d configuration, as a virtual machine host. To avoid confusion, the \u201chosted\u201d configuration will be referred to herein as \u201cOS hosted\u201d and the \u201cnon-hosted\u201d configuration will be referred to as \u201cnon-OS hosted.\u201d In the \u201cOS hosted\u201d configuration, an existing, general-purpose operating system (OS) acts as a \u201chost\u201d operating system that is used to perform certain I\/O operations. In the \u201cnon-OS hosted\u201d configuration, a kernel customized to support virtual machines takes the place of the conventional operating system.","With respect to OS hosted virtual computers,  depicts an embodiment of a virtualization system configuration referred to as an \u201cOS hosted\u201d configuration. Virtualization system  includes virtual machines , A, and B and respective virtual machine monitors VMM , VMM A, and VMM B. Virtualization system  also includes virtualization layer , which includes VMMs , A, and B. VMMs , A, and B are co-resident at system level with host operating system  such that VMMs , A, and B and host operating system  can independently modify the state of the host processor. VMMs call into the host operating system via driver  and a dedicated one of user-level applications  to have host OS  perform certain I\/O operations on behalf of a corresponding VM. Virtual machines , A, and B in this configuration are thus hosted in that they run in coordination with host operating system . Virtual machine  is depicted as including application guests , operating system guest , and virtual system . Virtualization systems that include suitable facilities are available in the marketplace. For example, VMware\u00ae Server virtual infrastructure software available from VMware, Inc., Palo Alto, Calif. implements an OS hosted virtualization system configuration consistent with the illustration of ; and VMware\u00ae Workstation desktop virtualization software, also available from VMware, Inc. also implements a hosted virtualization system configuration consistent with the illustration of .","With respect to non-OS hosted virtual computers,  depicts an embodiment of a virtualization system configuration referred to as a \u201cnon-OS hosted\u201d virtual machine configuration. In , virtualization system  includes virtual machines , A, and B as in . In contrast to , virtualization layer  of  includes VMMs , A, and B, and dedicated kernel . Dedicated kernel  takes the place, and performs the conventional functions, of a host operating system. Virtual computers (e.g., VM\/VMM pairs) run on kernel . Virtualization systems that include suitable kernels are available in the marketplace. For example, ESX Server\u2122 virtual infrastructure software available from VMware, Inc., Palo Alto, Calif. implements a non-hosted virtualization system configuration consistent with the illustration of .","Different systems may implement virtualization to different degrees\u2014\u201cvirtualization\u201d generally relates to a spectrum of definitions rather than to a bright line, and often reflects a design choice in respect to a trade-off between speed and efficiency and isolation and universality. For example, \u201cfull virtualization\u201d is sometimes used to denote a system in which no software components of any form are included in the guest other than those that would be found in a non-virtualized computer; thus, the OS guest could be an off-the-shelf, commercially available OS with no components included specifically to support use in a virtualized environment.","With respect to para-virtualization, as the term implies, a \u201cpara-virtualized\u201d system is not \u201cfully\u201d virtualized, but rather a guest is configured in some way to provide certain features that facilitate virtualization. For example, the guest in some para-virtualized systems is designed to avoid hard-to-virtualize operations and configurations, such as by avoiding certain privileged instructions, certain memory address ranges, etc. As another example, many para-virtualized systems include an interface within the guest that enables explicit calls to other components of the virtualization software. For some, the term para-virtualization implies that the OS guest (in particular, its kernel) is specifically designed to support such an interface. According to this definition, having, for example, an off-the-shelf version of Microsoft Windows XP as the OS guest would not be consistent with the notion of para-virtualization. Others define the term para-virtualization more broadly to include any OS guest with any code that is specifically intended to provide information directly to the other virtualization software. According to this definition, loading a module such as a driver designed to communicate with other virtualization components renders the system para-virtualized, even if the OS guest as such is an off-the-shelf, commercially available OS not specifically designed to support a virtualized computer system.","Unless otherwise indicated or apparent, virtualized systems herein are not restricted to use in systems with any particular \u201cdegree\u201d of virtualization and are not to be limited to any particular notion of full or partial (\u201cpara-\u201d) virtualization.","In the preferred embodiment, the embodiment operates in cooperation and may be a part of computer software, operating the preferred EMC CLARiiON or Symmetrix storage systems available from EMC Corporation of Hopkinton, Mass., although one skilled in the art will recognize that the current technique may be used with other data storage systems. In the preferred embodiment, EMC CLARiiON storage system implements aspects of the current technique as part of software that operates with such a storage system.","In the preferred embodiment, VMware virtual processing includes the VMware ESX Server technology and provides a VMM and a VM that has at least one virtual processor and is operatively connected to the VMM for running a sequence of VM instructions, which are either directly executable or non-directly executable. VMware technology, including the ESX server, is described in U.S. Pat. No. 6,397,242 to Devine et. al, issued May 28, 2002, which is hereby incorporated in its entirety by this reference.","In a preferred embodiment, referring to , a Data Storage Environment  is shown including a VMware ESX Server  having a series of Virtual Machines -, a database  and VM Kernel . Server  engages on Data Storage System  logical units -and -, designated with virtual drive designations e: \\ and c: \\, respectively.","The VMware ESX Server is configured to boot Virtual Machines (VMs) from external storage. In the example case of a preferred embodiment shown in , a Data Storage System  (e.g., EMC CLARiiON) contains both the boot volume (c: \\) and another volume (e: \\) for a preferred Windows 2000 VM. Any VMware-supported Guest operating system would work in view of the teachings herein. Currently, such Guest operating systems include most of the popular x86 operating systems, including Windows and Linux. Similarly, additional drives could be added, up to half the supported number of Logical Unit Numbers (LUNs) on an ESX Server.","Regarding , taking a closer look at one of the volume pairs that has been discussed with reference to , it can be seen that logical volume also known as LUN 1 has the VMware VM configuration (.vmx) file. It also has the two other files that comprise the e: \\ drive for Virtual Machine . First, LUN 1 has a pointer\u2014called lun2.vmdk\u2014to the \u201craw disk\u201d at logical volume also known as LUN 2, where most of the data resides. Second, there is a standard VMware ESX Server \u201c.REDO log\u201d on LUN 1. This .REDO log contains tracks that have been changed since the last time a .REDO log had been written out, or flushed, to LUN 2. This uses the preferred VMware VMFS \u201craw disk mapping\u201d (RDM) functionality. The VMkernel  of  presents one e:\\ drive to the Virtual Machine from a combination of data it finds on the two LUNs and ",{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 8","b":["201","202","231","211","212","213","215","230","203","213","213","213","213","212","212"]},"Thus, for example, portions  and  of storage  may be provisioned from RAID group or pool  as storage volume  (LUN006) which may encode an encapsulation of an exposed virtual disk(s) and virtual machine state. System  may be used to handle a failover situation () for the virtual machine instances so that, for example, virtual machine instance B\u2032 can take over for a failed virtual machine instance B using LUN006. In general, a virtual server such as VMware\u00ae vCenter\u2122 Server manages virtual machines. A virtual machine is associated with a unique identifier and information about its virtual devices, including virtual disks. Further, for example, vCenter\u2122 Server as described above is part of a virtualized environment deployed using VMware\u00ae vSphere\u2122.","Referring to , shown is detailed representation of a collection of computational systems in which an embodiment of the current technique may be provided. A virtualization management system such as VMware\u00ae vSphere\u2122 provides ability to perform cloud computing by utilizing virtualization system such as VMware\u00ae ESX server. VMware\u00ae vSphere\u2122 is a virtualization platform that delivers infrastructure and application services, and consolidate a set of virtual machines on a single physical server without impacting or with minimal impact to performance or throughput of the physical server. A file system protocol such as VMware\u00ae vStorage Virtual Machine File System (\u201cVMFS\u201d) allows virtual machines to access shared storage devices (e.g., Fibre Channel, iSCSI). A set of interfaces such as VMware\u00ae vStorage APIs for Storage Awareness (\u201cVASA\u201d) enables integration of virtualized systems (e.g. VMs) with a data storage system that provides storage to virtual machines and supports data protection solutions. Further, VMware\u00ae vSphere\u2122 provides management services such as VMware\u00ae vCenter\u2122 Agent that allows vSphere\u2122 hosts  to connect to vCenter\u2122 Server  for centralized management of the hosts and virtual machines.","VMware\u00ae vCenter\u2122 Server, formerly known as VMware\u00ae Virtual Center, is a centralized management tool for the VMware\u00ae vSphere\u2122. VMware\u00ae vCenter\u2122 Server  enables management of one or more ESX servers , , , and Virtual Machines (VMs) included in each ESX server using a single console application. VMware\u00ae vCenter\u2122 Server  provides storage administrators insight into the status and configuration of clusters, hosts, VMs, storage, operating systems, and other critical components of a virtual infrastructure from a single place (e.g., console application). Further, VMware\u00ae vCenter\u2122 Server  may be installed on a physical or virtual machine.","In a virtualized environment such as VMware\u00ae vSphere\u2122 environment, a vCenter\u2122 Server instance manages a set of ESX servers and storage resources associated with the set of ESX servers. Storage devices of a data storage system are used and managed by a vCenter\u2122 Server instance.","Further, a user of a VMware\u00ae vSphere\u2122 client  may access inventory and configuration information from one or more vCenter\u2122 Servers. A user of vSphere\u2122 client  may view storage information using an instance of a virtual server (e.g., vCenter\u2122 Server instance) if the user possess valid permissions for accessing the virtual server. A single instance of vCenter\u2122 Server provides capability to manage hundreds of hosts, and thousands of virtual machines. Further, one or more vCenter\u2122 Servers may be linked together to provide capability to manage of thousands of hosts and tens of thousands of virtual machines using one or more vCenter\u2122 Server instances via a single management console (e.g. VMware\u00ae vSphere\u2122 client ).","Further, vCenter\u2122 Server  includes a status component  that displays the health of components of the vCenter\u2122 Server  thereby enabling storage administrators to quickly identify and correct failures that may occur in the vCenter\u2122 management infrastructure . Additionally, vCenter\u2122 Server  provides alerts and\/or alarms to storage administrators such that the storage administrators may attempt to resolve failures before the failures interrupt the availability of applications executing on storage resources of the vCenter\u2122 Server .","In at least one embodiment of the current technique, a virtual system (e.g., VM, ESX Server) includes a storage tab that is displayed in a graphical user interface on a management console of the virtual system such that storage entities of a virtualized environment may be managed using storage information provided in the storage tab. Further, a user may set alerts and\/or alarms in a virtual system. Further, a virtual server (e.g. vSphere\u2122) in a virtualized environment provides a detailed view of every storage component included in a storage layout of the virtualized environment. A storage layout may provide information to storage administrators regarding available communication paths and a logical grouping of storage objects that may share storage resources. Further, a virtual server (e.g., vCenter\u2122 Server ) monitors storage resources by maintaining alarms for managed storage entities, such as data stores and clusters. An alarm may be set to trigger on an occurrence of a specific event such that the alarm may notify a storage administrator regarding the occurrence of the event. For example, an event may include a change in status (such as \u201cHost Exited Maintenance Mode\u201d), an access control operation (such as \u201cRole Created\u201d), and a license event (such as \u201cLicense Expired\u201d). In addition, an alarm is triggered only when the alarm satisfy a specific time condition in order to minimize the number of false alarms.","In at least one embodiment of the current technique, VASA interfaces are a vendor neutral set of interfaces defined by VMware\u00ae for reporting storage provisioned for a virtual machine in a virtualized environment. A VASA interface is a proprietary SOAP-based web interface that is used by a virtual machine deployed in a virtualized environment that is using one or more virtualized product or software from VMware\u00ae. A virtual machine uses a VASA interface for retrieving storage system information from data storage system . The storage system information includes information associated with storage devices of the data storage system  such that the information is used by a virtual machine for provisioning storage, monitoring storage and troubleshooting failures via a management system such as vCenter\u2122 Server  of a virtualized environment (e.g. vSphere\u2122). A data storage system  provides storage to a virtual machine such that users of the virtual machine may use the storage for operations such as storing and managing data. A server component ,  (also referred to as \u201cVASA server\u201d or \u201cVASA provider\u201d) resides on data storage system  such that the server component communicates with a virtual machine for providing storage information to the virtual machine. A client component (also referred to as \u201cVASA client\u201d) resides on a virtual machine or virtual client (e.g., vCenter\u2122 Server ) managing one or more virtual machines. A VASA client connected to data storage system  creates a usage context such that the data storage system  provides storage information that is relevant to the usage context of the VASA client. Storage information reported by a data storage system may include information associated with a set of storage elements (also referred herein as \u201cstorage entities\u201d). A storage entity is a storage object such as a LUN, file system, array, port. A storage monitoring service  executing on a virtual server  gathers storage information from data storage system  and provides the storage information to users of virtual machines (e.g., vSphere\u2122 client ).","A VASA interface provides information regarding a data storage system and storage devices of the data storage system in a vendor-neutral fashion which allows users of a virtual machine in a virtualized environment to explore the information associated with the storage devices that are consumed by the virtualized environment, and manage infrastructure of the virtualized environment in order to provide ability to monitor and troubleshoot the storage devices. For example, a user of a virtual machine may easily identify a problem affecting the virtual machine using a health status of a storage device.","In at least one embodiment of the current technique, VASA interfaces may include a set of connection interfaces (also referred to as \u201cApplication Programming Interface (API)\u201d) that help establish or remove a secure connection between vCenter\u2122 Server  and VASA provider , . VASA provider ,  uses a VASA interface to communicate with a virtual machine. Additionally, VASA interfaces may include a set of client context APIs that identifies a usage context of a virtual client which is required to retrieve storage information from data storage system . Further, VASA interfaces may include a set of storage discovery APIs that provide information regarding data storage system  and information associated with physical and logical storage devices of the data storage system  that are pertinent to a virtualized environment. Additionally, VASA interfaces may include a set of status APIs that provide information such as changes in storage configuration or system availability of data storage system . Further, A VASA interface may define a profile (such as a block, file and capability) to enable data storage system  to provide information associated with block storage devices, file systems stored on storage devices of the data storage system, storage capabilities of LUNs, and storage capabilities of file systems of the data storage system .","In at least one embodiment of the current technique, a VASA provider may be implemented as a modular generic framework that may execute a dynamically loadable library that is responsible for performing operations associated with a request issued by a VASA client. In at least one embodiment of the current technique, a VASA provider may reside on a control station of a file based data storage system. Alternatively, in at least one embodiment of the current technique, a VASA provider may reside on a storage processor of a block based data storage system. An instance of a VASA provider is created which executes on a data storage system and is configured by a user of a virtual machine included in vSphere\u2122 system such that the vSphere\u2122 may retrieve storage system information from the data storage system using the instance of the VASA provider.","Referring to , shown is a more detailed representation of components that may be included in an embodiment using the techniques described herein. With reference also to , a virtual machine or virtual client such as vSphere\u2122 client  connected to a virtual server such as vCenter\u2122 Server  (illustrated in  as virtual server-1 , virtual server-2 , virtual server-n ) initiates a connection between the virtual server and a VASA provider (e.g. VASA provider block adaptor ) executing on a data storage system . A user (e.g. storage administrator) of the virtual client may need to provide credential information such as a Uniform Resource Locator (URL) address of the data storage system (e.g. \u201chttps:\/\/<IP address of storage processor or control station>\/\u201d), a user name for the user, and a password associated with the user name for establishing the connection. The credential information is used by the virtual server in order to establish a secure connection with the VASA provider. If the credential information is valid and accepted by the data storage system , a certificate for the virtual server is registered with the data storage system . The certificate is then used to authenticate subsequent requests from the virtual server. In at least one embodiment of the current technique, a session is started when a connection is established between a virtual server and data storage system , and the session ends when a user removes information regarding the VASA provider from configuration settings of the virtual server thereby resulting into termination of the secure connection.","In at least one embodiment of the current technique, a session is based on a secure HTTPS communication between a virtual server (e.g. vCenter\u2122 Server) and a VASA Provider (e.g. VASA provider block adaptor ). A secure HTTPS communication uses information such as a SSL certificate and a VASA session identifier to manage a secure connection. A virtual server provides credentials information such as a user name, password and certificate to a VASA interface (e.g., \u201cRegisterVASACertificate\u201d API) such that the VASA interface adds the certificate to a trust store of the virtual server.","In at least one embodiment of the current technique, a virtual server uses a VASA API (e.g., \u201cSetContext\u201d API) to initialize a session after a secure connection is established between the virtual server and data storage system . The \u201cSetContext\u201d API provides a storage object (e.g., \u201cVasaVendorProviderInfo\u201d object) as a return parameter, which includes a session identifier. A session identifier uniquely identifies a session created between a virtual server and an instance of a VASA provider , . A new session identifier is generated each time the \u201cSetContext\u201d API is invoked. A virtual server includes a session identifier associated with a session in an HTTP cookie that is sent with a request (e.g., VASA API) to the VASA provider ,  of the data storage system  once the session is established. VASA provider ,  validates a session identifier every time a request is received from a virtual server. A virtual server may invoke the \u201cSetContext\u201d API to obtain a new session identifier, even when a new secure connection has not been created.","In at least one embodiment of the current technique, data storage system  provides storage system information associated with storage devices of disk array  to a virtual server such as vCenter\u2122 Server  (illustrated in  as virtual server-1 , virtual server-2 , virtual server-n ) based on a usage context provided by a virtual client of the virtual server. A virtual client of a virtual server provides a usage context to data storage  system after a connection is established between the virtual client and the data storage system . Further, the virtual client updates the usage context each time the virtual client detects a change in storage elements associated with the usage context. A usage context may include information such as a list of paths of ESX hosts' initiator and a data storage system port receiving the request (e.g. world wide names), a list of iSCSI Qualified Names (IQN), and a list of NFS mount points (e.g., server name\/IP+file system path) of the ESX hosts. Data storage system  uses information of a usage context to appropriately filter storage system information of storage devices and provides a subset of the storage system information to a virtual client such that the subset of the storage information provided is relevant to the usage context of the virtual client. For example, on a block based data storage system, block initiators may be mapped to a set of LUNs associated with the block initiators by using a storage group association. Similarly, for example, on a block and file based data storage system, IQNs and NFS mount points may be used to locate VMware\u00ae applications (such as applications using a NFS data store, VMFS data store) that are used by an ESX host managed by a vCenter\u2122 Server. In at least one embodiment of the current technique, the \u201cSetContext\u201d VASA API establishes a usage context for a virtual server. VASA provider ,  maps information associated with a usage context to storage system information of data storage system  such as list of arrays, processors, ports, LUNs, and file systems. VASA provider ,  uses information associated with a usage context for reporting storage system information to a virtual client of a virtual server by filtering information of storage elements from the storage system information that are not associated with the usage context of the virtual server.","In at least one embodiment of the current technique, a virtual server sends a full query to data storage system  to retrieve storage system information for every storage elements associated with a usage content of the virtual server after a connection is established by the virtual server with the data storage system . However, each subsequent query issued by the virtual server is a partial query that retrieves information associated with changes in the storage system information retrieved by the full query (such as instances of storage elements added and\/or removed from the storage system information). In other words, the virtual server issues a query to request a list of changed storage elements instead of issuing a full query after the connection is established and the initial full query retrieves the storage system information. A configuration change event is queued for a storage element (also referred to as \u201cstorage object\u201d) when data storage system  detects a change in the storage element that may alter any one of a VASA-defined storage property for the storage element. Data storage system  processes a queue of configuration change events in order to provide a list of changed storage elements to a virtual server. A virtual server periodically request a list of configuration change events and updates storage system information associated with a usage context of the virtual server based on the list of configuration change events. For example, if data storage system  provides a list of events associated with changes in storage system information associated with a set of LUNs, a virtual server queries the set of LUNs associated with the list of events. Events and alarms are gathered to log changes in health, capacity and status of a storage entity. An events may also include system events that provide description of an alarm.","With reference also to , in at least one embodiment of the current technique, VASA provider ,  of data storage system  provides up-to-date storage system information to a virtual client (e.g., vSphere\u2122 Client ) of a virtual server (e.g., vCenter\u2122 Server ) by maintaining lists (or queues) of events and alarms. A list of events includes configuration change events that are relevant to a usage context established for a session initiated by a virtual client of a virtual server. Similarly, a list of alarms includes storage system alarms that are relevant to a usage context established for a session initiated by a virtual client of a virtual server. A configuration scope is defined by usage context provided to the \u201cSetContext\u201d VASA API such that the configuration scope is applicable to queries sent by a virtual server to data storage system  after a session is established. Further, a configuration scope may also be defined by configuration change events and storage system alarms collected in queues by VASA provider , . However, a configuration scope may be changed by a virtual server when the virtual server executes the \u201cSetContext\u201d VASA API. VASA provider ,  may manage one or more sessions from one or more virtual servers such that a maximum number of sessions that may be managed is configurable by a user or a system. Further, a session context state associated with a session may be invalidated and reset in response to either execution of a VASA API (e.g., \u201cSetContext\u201d API) by a virtual server or as a result of an error condition. A session context reset condition may trigger a full query by a virtual server.","In at least one embodiment of the current technique, VASA protocol adapter  may be implemented as a component that converts a query received from a virtual server in a format that is used by data storage system . VASA common provider  includes a set of interfaces that are common to different types of data storage systems such as a block based storage system, a file based data storage system.","In at least one embodiment of the current technique, VASA provider block adapter  is a VASA provider that may be implemented as a dynamically loadable library  (e.g., a DLL on Microsoft\u00ae Windows\u2122, a share library on Linux). Further, VASA provider block adapter  may also be implemented as a dynamically loadable library in order to leverage a modular generic framework for querying provider components (e.g., storage provider , alert provider ), subscribing to indications, and posting alarms to VASA common provider . Further, VASA provider block adapter  reports storage system information (e.g., storage topology, configuration information) of the disk array  and storage entities of data storage system  to one or more virtual servers based on a usage context of a session established between a virtual server and VASA provider block adapter . The VASA provider block adapter  includes support for VASA APIs that may provide storage information such as a list of storage entities based on a type of a storage entity, details of storage topology of the storage entities, storage properties for the storage entities, and events for reporting configuration changes.","In at least one embodiment of the current technique, a common cache  is created for storing storage system information in a memory of data storage system  such that the storage system information is reported to a virtual client connected to a virtual server. A virtual server establishes a session with data storage system  for retrieving storage system information. For example, virtual server-1  creates a session-1, virtual server-2 creates a session-2, and virtual server-n creates a session-n. A session view is created and maintained by a virtual server when the virtual server establishes a session with data storage system . For example, virtual server-1  creates a session view-1 , virtual server-2  creates a session view-2 , and virtual server-n  creates a session view-n . A session view caches information associated with a set of storage entities that are relevant to a usage context of a session associated with the session view of a virtual server such that the information stored in the session view is reported to the virtual server instead of retrieving the information from the disk array  using provider components (e.g. storage provider ).","Referring to , shown is detailed representation of class structures used for an example implementation of VASA provider block adapter . A class such as \u201cblocksessionmanager\u201d  creates an instance of a class \u201cblocksession\u201d  for each session created between an instance of a virtual server (e.g., vCenter\u2122 Server) and VASA provider ,  (e.g. VASA provider block adapter ) of data storage system . The class \u201cblocksession\u201d  maintains a reference count for the session indicated by the class \u201cVASAsession\u201d  which includes a session identifier and a usage context indicated by a class \u201cVASAusagecontext\u201d. The class \u201cblocksession\u201d  creates a monitor class for servicing each type of a VASA storage entity. The monitor class may include class objects such as class \u201carraymonitor\u201d , \u201cSPmonitor\u201d , \u201ccontextLUNmonitor\u201d , \u201ccontextportmonitor\u201d  and \u201ccapabilitymonitor\u201d . Class \u201cblocksession\u201d  creates a usage context initiator set indicated by class \u201cuc_initiatorset\u201d  for each entry of the class \u201cVASAusagecontext\u201d for analyzing storage system information stored in the monitor classes such as classes \u201ccontextLUNmonitor\u201d , and \u201ccontextportmonitor\u201d .","Class \u201cVASAprovideruemblockadapter\u201d  processes a query issued by an instance of a virtual server (e.g., vCenter\u2122 Server). The query is processed by requesting a reference to an instance of the class \u201cblocksession\u201d  from the class \u201cblocksessionmanager\u201d . The reference is used to access a specific monitor class and execute the query for retrieving storage system information associated with the specific monitor class. A query for reporting storage system information for a specific type of storage entity is processed by a monitor class associated with the specific type of storage entity. Additionally, the monitor class associated with the specific type of storage entity manages events and alarms associated with the specific type of storage entity.","With reference also to , in at least one embodiment of the current technique, an instance of class \u201ccommoncachemanager\u201d  is created to reduce an amount of time required to process a query using a VASA API (also referred o as \u201cVASA query\u201d). Class \u201ccommoncachemanager\u201d  stores or caches storage system information associated with a set of storage entities of data storage system . Further, \u201ccommoncachemanager\u201d  stores storage system information in a memory (e.g., volatile memory) of data storage system  after retrieving the storage system information from data storage system  if the storage system information is not found in the common cache  represented by an instance of the class \u201ccommoncachemanager\u201d . Class \u201cVASAprovideruemblockadapter\u201d  queries storage provider  to retrieve storage system information in order to process a VASA query received from a virtual server. Additionally, class \u201cVASAprovideruemblockadapter\u201d  registers information to receive indications from alert provider  such that class \u201cVASAprovideruemblockadapter\u201d  may be notified regarding a change in storage system information of storage elements of data storage system . Upon receiving a notification, class \u201cVASAprovideruemblockadapter\u201d  post an event to a list of events, post an alarm to a list of alarms, and notifies a virtual server. A VASA poll thread indicated by class \u201cVASApollthread\u201d  executes and coordinates polling, and processing of indications received by class \u201cVASAblockindicationmonitor\u201d . For example, class \u201ccontextLUNmonitor\u201d  query the class \u201ccommoncachemanager\u201d  to retrieve storage information associated with a usage context for a set of LUNs associated with a session, and class \u201ccontextportmonitor\u201d  query the class \u201ccommoncachemanager\u201d  to retrieve storage information associated with a usage context for a set of ports associated with a session.","Referring to , shown is more detailed representation of class structures used for an example implementation of VASA provider block adapter . With reference also to , in at least one embodiment of the current technique, class \u201cVASAprovideruemblockadapter\u201d  caches storage system information such as a list of path names for initiators of ESX hosts, storage properties for a set of LUNs, and storage properties for a set of ports in common cache . Moreover, class \u201cVASAprovideruemblockadapter\u201d  registers information to receive indications in order to maintain consistent storage system information in common cache , and report events indicating changes in configuration and\/or storage properties of storage entities of data storage system  to VASA common provider . Storage system information associated with storage entities of data storage system  is retrieved by a polling process that is invoked at a regular time interval. Further, events and alarms are queued in VASA provider ,  by each session (indicated by an instance of class \u201cblocksession\u201d ) in data storage system  and one or more monitor classes when either an indication is processed or a poll is performed. A poll thread  co-ordinates the polling process and indication queue  manages processing of indications. In order to process indications, objects \u201cIndicationMonitor\u201d  and \u201cIndicationObservers\u201d  start executing prior to a first poll request to build \u201ccommoncachemanager\u201d  is issued by the poll thread  such that the first poll request for building the cache is issued after the first session is established by a virtual server such that an indication associated with a change in a storage entity that may occur during the first poll request can be processed. Class \u201cindicationmonitorreceiver\u201d  receives an indication notification, and add the indication to indication queue  to ensure that indications are processed in an ordered arrangement. Class \u201cVASAblockindicationmonitor\u201d  manages class \u201cindicationmonitorreceiver\u201d , and registers\/de-registers an instance of the class \u201cindicationobserver\u201d  such that the instance of the class \u201cindicationobserver\u201d  indicates an indication received by class \u201cindicationmonitorreceiver\u201d .","In at least one embodiment of the current technique, the \u201ccommoncachemanager\u201d  retrieves storage system information in order to store the storage system information in common cache  by polling. After the \u201ccommoncachemanager\u201d  completes the polling, the poll thread  issues a poll request to the \u201cblocksessionmanager\u201d  such that each instance of \u201cblocksession\u201d  may update its state information by using the updated storage system information of the common cache . Further, if a difference is found between the state information of each session and common cache , appropriate events and alarms are queued.",{"@attributes":{"id":"p-0091","num":"0090"},"figref":["FIG. 13","FIGS. 9-12"],"b":["366","422","402","402","300","402","416","300","370","300","300"]},"Storage system information associated with a session is retrieved from data storage system  by querying storage provider , and stored in common cache  in a synchronous manner when the session is initially created in order to populate common cache  with the storage system information. Each subsequent VASA query from a virtual server is performed by retrieving storage system information from common cache  instead of querying storage provider . An instance of class \u201cblocksession\u201d  creates one or more instances of monitor classes (e.g. \u201ccontextLUNmonitor\u201d ) and process a VASA query targeted for a specific type of storage entity. For example, as shown in the example sequence diagram illustrated in , a monitor class such as \u201ccontextLUNmonitor\u201d  queries \u201ccommoncachemanager\u201d  to retrieve storage system information associated with a usage context for a set of LUNs.",{"@attributes":{"id":"p-0093","num":"0092"},"figref":["FIG. 14","FIGS. 9-12"],"b":["366","422","366","422","402","400","402","400","400","400","366","402"]},{"@attributes":{"id":"p-0094","num":"0093"},"figref":["FIG. 15","FIGS. 9-12"],"b":["370","416","402","400","370","440","416","400"]},{"@attributes":{"id":"p-0095","num":"0094"},"figref":["FIG. 16","FIGS. 9-12"],"b":["420","436","440","418","440","416","402"]},"In at least one embodiment of the current technique, multiple VASA queries associated with a session may be executed concurrently using multiple processes (e.g., threads) such that each VASA query of the multiple VASA queries are processed by each process of the multiple processes. Similarly, multiple indications may be posted to the indication queue  concurrently using multiple processes (e.g., threads) such that each indication of the multiple indications are processed by each process of the multiple processes. However, each indication from the indication queue  are processed sequentially using a single process (e.g., thread). Class \u201cblocksessionmanager\u201d  provides a locking scheme for processing a VASA query. Each instance of monitor classes (e.g. \u201ccontextLUNmonitor\u201d , \u201ccontextportmonitor\u201d ) provides a reader\/writer lock to prevent contention between two or more VASA queries.","While the invention has been disclosed in connection with preferred embodiments shown and described in detail, their modifications and improvements thereon will become readily apparent to those skilled in the art. Accordingly, the spirit and scope of the present invention should be limited only by the following claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Features and advantages of the present invention will become more apparent from the following detailed description of exemplary embodiments thereof taken in conjunction with the accompanying drawings in which:",{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIGS. 2-10"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIGS. 11-12"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIGS. 13-16"}]},"DETDESC":[{},{}]}
