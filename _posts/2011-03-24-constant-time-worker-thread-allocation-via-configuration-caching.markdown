---
title: Constant time worker thread allocation via configuration caching
abstract: Mechanisms are provided for allocating threads for execution of a parallel region of code. A request for allocation of worker threads to execute the parallel region of code is received from a master thread. Cached thread allocation information identifying prior thread allocations that have been performed for the master thread are accessed. Worker threads are allocated to the master thread based on the cached thread allocation information. The parallel region of code is executed using the allocated worker threads.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08881159&OS=08881159&RS=08881159
owner: International Business Machine Corporation
number: 08881159
owner_city: Armonk
owner_country: US
publication_date: 20110324
---

{"@attributes":{"id":"description"},"GOVINT":[{},{}],"p":["This invention was made with Government support under Contract No.: B554331 awarded by the Department of Energy. The Government has certain rights in this invention.","The present application relates generally to an improved data processing apparatus and method and more specifically to mechanisms for allocating worker threads by utilizing configuration caching such that the allocation is performed in constant time.","Parallel processing systems and parallel programming are becoming more prevalent in today's computing environment. With such systems and programming, more than one computation\/operation can be executed at substantially the same time. As a result, these computations and other operations can be performed such that the speed by which such computations and operations are performed is greatly increased. The parallel processing system provides greater throughput as a result.","Various standards for parallel processing have been developed. One standard is the OpenMP Application Program Interface (API). The OpenMP API supports multi-platform shared-memory parallel programming in C\/C++ and Fortran on all architectures, including Unix platforms and Windows NT platforms. Jointly defined by a group of major computer hardware and software vendors, OpenMP is a portable, scalable model that gives shared-memory parallel programmers a simple and flexible interface for developing parallel applications for platforms ranging from the desktop to the supercomputer.","With OpenMP, as with other parallel processing standards, threads are selected to run a parallel task each time that a parallel region of code, such as a parallel loop construct in the code, is encountered during processing. There is a series of tasks that must be accomplished when creating a parallel region. After determining if the parallel region can proceed in parallel, and determining a number of threads that should be allocated for the particular parallel region, which thread(s) to use to process a parallel region of code are selected. Moreover, the selected threads must be informed of where to obtain the work to be performed so that the selected threads can execute the code associated with the parallel region.","Performing such thread selection is time consuming with the amount of time needed to perform such selection being proportional to the number of threads selected to execute the code in the parallel region. While this may be manageable when processors could execute at most a maximum of 4, 8, or even 16 threads in parallel, this becomes a significant time consumption factor in machines with a large number of parallel executing threads, e.g., 64, 128, or even more threads being executed in parallel.","In one illustrative embodiment, a method, in a data processing system, is provided for allocating threads for execution of a parallel region of code. The method comprises receiving, from a master thread, a request for allocation of worker threads to execute the parallel region of code. The method further comprises accessing cached thread allocation information identifying prior thread allocations that have been performed for the master thread. The method also comprises allocating worker threads to the master thread based on the cached thread allocation information. Moreover, the method comprises executing the parallel region of code using the allocated worker threads.","In other illustrative embodiments, a computer program product comprising a computer useable or readable medium having a computer readable program is provided. The computer readable program, when executed on a computing device, causes the computing device to perform various ones of, and combinations of, the operations outlined above with regard to the method illustrative embodiment.","In yet another illustrative embodiment, a system\/apparatus is provided. The system\/apparatus may comprise one or more processors and a memory coupled to the one or more processors. The memory may comprise instructions which, when executed by the one or more processors, cause the one or more processors to perform various ones of, and combinations of, the operations outlined above with regard to the method illustrative embodiment.","These and other features and advantages of the present invention will be described in, or will become apparent to those of ordinary skill in the art in view of, the following detailed description of the example embodiments of the present invention.","The illustrative embodiments provide mechanisms for providing constant time worker thread allocation using configuration caching. The illustrative embodiments operate under the realization that applications often allocate and free the same threads repeatedly during execution of the application code. That is, application code often involves loops and other structures that are to be performed in parallel in such a manner that the same threads are repeatedly allocated and freed.","With the mechanisms of the illustrative embodiments, past worker thread allocation information is cached so that it may be used in making decisions as to which worker threads to allocate for future allocations and execution of code in a same or different parallel region of code. With this thread allocation caching mechanism, assuming that there is a \u201chit\u201d, i.e. a current thread requests a same number of threads for executing a parallel region of code as in a past allocation, and assuming that none of the threads in the past allocation have been reclaimed for other thread allocations, then one can reuse a past allocation in constant time using compact representations such as described hereafter.","This is very effective in practice since a significant portion of parallel regions use default configuration numbers for determining the number of threads that should take part in executing a parallel region of code. By caching a past allocation, and engineering a thread selection mechanism to not reuse, to the extent feasible for appropriate load balancing, threads that were previously allocated to a given thread allocation unless a same number of threads is being requested as described hereafter, one can actually ensure a very high degree of reuse of prior allocations.","As will be appreciated by one skilled in the art, the present invention may be embodied as a system, method, or computer program product. Accordingly, aspects of the present invention may take the form of an entirely hardware embodiment, an entirely software embodiment (including firmware, resident software, micro-code, etc.) or an embodiment combining software and hardware aspects that may all generally be referred to herein as a \u201ccircuit,\u201d \u201cmodule\u201d or \u201csystem.\u201d Furthermore, aspects of the present invention may take the form of a computer program product embodied in any one or more computer readable medium(s) having computer usable program code embodied thereon.","Any combination of one or more computer readable medium(s) may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, device, or any suitable combination of the foregoing. More specific examples (a non-exhaustive list) of the computer readable storage medium would include the following: an electrical connection having one or more wires, a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), an optical fiber, a portable compact disc read-only memory (CDROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing. In the context of this document, a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system, apparatus, or device.","A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein, for example, in a baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms, including, but not limited to, electro-magnetic, optical, or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate, propagate, or transport a program for use by or in connection with an instruction execution system, apparatus, or device.","Computer code embodied on a computer readable medium may be transmitted using any appropriate medium, including but not limited to wireless, wireline, optical fiber cable, radio frequency (RF), etc., or any suitable combination thereof.","Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages, including an object oriented programming language such as Java\u2122, Smalltalk\u2122, C++, or the like, and conventional procedural programming languages, such as the \u201cC\u201d programming language or similar programming languages. The program code may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer, or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider).","Aspects of the present invention are described below with reference to flowchart illustrations and\/or block diagrams of methods, apparatus (systems) and computer program products according to the illustrative embodiments of the invention. It will be understood that each block of the flowchart illustrations and\/or block diagrams, and combinations of blocks in the flowchart illustrations and\/or block diagrams, can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions\/acts specified in the flowchart and\/or block diagram block or blocks.","These computer program instructions may also be stored in a computer readable medium that can direct a computer, other programmable data processing apparatus, or other devices to function in a particular manner, such that the instructions stored in the computer readable medium produce an article of manufacture including instructions that implement the function\/act specified in the flowchart and\/or block diagram block or blocks.","The computer program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other devices to cause a series of operational steps to be performed on the computer, other programmable apparatus, or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions\/acts specified in the flowchart and\/or block diagram block or blocks.","The flowchart and block diagrams in the figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of code, which comprises one or more executable instructions for implementing the specified logical function(s). It should also be noted that, in some alternative implementations, the functions noted in the block may occur out of the order noted in the figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and\/or flowchart illustration, and combinations of blocks in the block diagrams and\/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts, or combinations of special purpose hardware and computer instructions.","Thus, the illustrative embodiments may be utilized in many different types of data processing environments including a distributed data processing environment, a single data processing device, or the like. In order to provide a context for the description of the specific elements and functionality of the illustrative embodiments,  are provided hereafter as example environments in which aspects of the illustrative embodiments may be implemented. It should be appreciated that  are only examples and are not intended to assert or imply any limitation with regard to the environments in which aspects or embodiments of the present invention may be implemented. Many modifications to the depicted environments may be made without departing from the spirit and scope of the present invention.","With reference now to the figures,  depicts a pictorial representation of an example distributed data processing system in which aspects of the illustrative embodiments may be implemented. Distributed data processing system  may include a network of computers in which aspects of the illustrative embodiments may be implemented. The distributed data processing system  contains at least one network , which is the medium used to provide communication links between various devices and computers connected together within distributed data processing system . The network  may include connections, such as wire, wireless communication links, or fiber optic cables.","In the depicted example, server  and server  are connected to network  along with storage unit . In addition, clients , , and  are also connected to network . These clients , , and  may be, for example, personal computers, network computers, or the like. In the depicted example, server  provides data, such as boot files, operating system images, and applications to the clients , , and . Clients , , and  are clients to server  in the depicted example. Distributed data processing system  may include additional servers, clients, and other devices not shown.","In the depicted example, distributed data processing system  is the Internet with network  representing a worldwide collection of networks and gateways that use the Transmission Control Protocol\/Internet Protocol (TCP\/IP) suite of protocols to communicate with one another. At the heart of the Internet is a backbone of high-speed data communication lines between major nodes or host computers, consisting of thousands of commercial, governmental, educational and other computer systems that route data and messages. Of course, the distributed data processing system  may also be implemented to include a number of different types of networks, such as for example, an intranet, a local area network (LAN), a wide area network (WAN), or the like. As stated above,  is intended as an example, not as an architectural limitation for different embodiments of the present invention, and therefore, the particular elements shown in  should not be considered limiting with regard to the environments in which the illustrative embodiments of the present invention may be implemented.","With reference now to , a block diagram of an example data processing system is shown in which aspects of the illustrative embodiments may be implemented. Data processing system  is an example of a computer, such as client  in , in which computer usable code or instructions implementing the processes for illustrative embodiments of the present invention may be located.","In the depicted example, data processing system  employs a hub architecture including north bridge and memory controller hub (NB\/MCH)  and south bridge and input\/output (I\/O) controller hub (SB\/ICH) . Processing unit , main memory , and graphics processor  are connected to NB\/MCH . Graphics processor  may be connected to NB\/MCH  through an accelerated graphics port (AGP).","In the depicted example, local area network (LAN) adapter  connects to SB\/ICH . Audio adapter , keyboard and mouse adapter , modem , read only memory (ROM) , hard disk drive (HDD) , CD-ROM drive , universal serial bus (USB) ports and other communication ports , and PCI\/PCIe devices  connect to SB\/ICH  through bus  and bus . PCI\/PCIe devices may include, for example, Ethernet adapters, add-in cards, and PC cards for notebook computers. PCI uses a card bus controller, while PCIe does not. ROM  may be, for example, a flash basic input\/output system (BIOS).","HDD  and CD-ROM drive  connect to SB\/ICH  through bus . HDD  and CD-ROM drive  may use, for example, an integrated drive electronics (IDE) or serial advanced technology attachment (SATA) interface. Super I\/O (SIO) device  may be connected to SB\/ICH .","An operating system runs on processing unit . The operating system coordinates and provides control of various components within the data processing system  in . As a client, the operating system may be a commercially available operating system such as Microsoft Windows 7 (Microsoft and Windows are trademarks of Microsoft Corporation in the United States, other countries, or both). An object-oriented programming system, such as the Java programming system, may run in conjunction with the operating system and provides calls to the operating system from Java programs or applications executing on data processing system  (Java is a trademark of Oracle and\/or its affiliates).","As a server, data processing system  may be, for example, an IBM\u00ae eServer\u2122 System p\u00ae computer system, running the Advanced Interactive Executive (AIX\u00ae) operating system or the LINUX operating system (IBM, eServer, System p, and AIX are trademarks of International Business Machines Corporation in the United States, other countries, or both, and LINUX is a registered trademark of Linus Torvalds in the United States, other countries, or both). Data processing system  may be a symmetric multiprocessor (SMP) system including a plurality of processors in processing unit . Alternatively, a single processor system may be employed.","Instructions for the operating system, the object-oriented programming system, and applications or programs are located on storage devices, such as HDD , and may be loaded into main memory  for execution by processing unit . The processes for illustrative embodiments of the present invention may be performed by processing unit  using computer usable program code, which may be located in a memory such as, for example, main memory , ROM , or in one or more peripheral devices  and , for example.","A bus system, such as bus  or bus  as shown in , may be comprised of one or more buses. Of course, the bus system may be implemented using any type of communication fabric or architecture that provides for a transfer of data between different components or devices attached to the fabric or architecture. A communication unit, such as modem  or network adapter  of , may include one or more devices used to transmit and receive data. A memory may be, for example, main memory , ROM , or a cache such as found in NB\/MCH  in .","Those of ordinary skill in the art will appreciate that the hardware in  may vary depending on the implementation. Other internal hardware or peripheral devices, such as flash memory, equivalent non-volatile memory, or optical disk drives and the like, may be used in addition to or in place of the hardware depicted in . Also, the processes of the illustrative embodiments may be applied to a multiprocessor data processing system, other than the SMP system mentioned previously, without departing from the spirit and scope of the present invention.","Moreover, the data processing system  may take the form of any of a number of different data processing systems including client computing devices, server computing devices, a tablet computer, laptop computer, telephone or other communication device, a personal digital assistant (PDA), or the like. In some illustrative examples, data processing system  may be a portable computing device which is configured with flash memory to provide non-volatile memory for storing operating system files and\/or user-generated data, for example. Essentially, data processing system  may be any known or later developed data processing system without architectural limitation.","In accordance with the mechanisms of the illustrative embodiments, one or more of the data processing systems, e.g., the servers  or , or clients -, in , or the data processing system  in , implements a parallel processing architecture for executing a plurality of threads of execution in a parallel nature. For example, the processing unit  in  may comprise a plurality of processing cores and each core may execute one or more threads of execution simultaneously. In one example, the data processing system  may be a simultaneous multi-threaded (SMT) 2 processor (able to process two threads simultaneously), SMT 4 processor (able to process four threads simultaneously), or the like.","In accordance with the mechanisms of the illustrative embodiments, the data processing system further implements a thread allocation mechanism that utilizes one or more thread allocation cache structures for storing information regarding the thread allocations that have occurred and uses this information to make decision as to which threads to allocate for performance of a parallel region of code. The details of how such thread allocation information is cached and used to make decisions regarding future thread allocations will be described in greater detail hereafter. However, to better understand the benefits and differences between the mechanism of the illustrative embodiments with regard to a known methodology for selecting worker threads, it is best to first understand an example of such a known methodology.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":["FIGS. 3A-3C","FIGS. 3A-3C"]},"As shown in , a program is being executed using a master thread represented by the solid dark line in . The master thread, during execution of the program code, encounters a region of code that has been designated by the programmer or a compiler or the like, to be a region of code that is to be executed in parallel if possible, e.g., a loop in the code that can be parallelized by the compiler or otherwise indicated by the user as a loop that is to be executed in parallel. For example, in one illustrative embodiment, a compiler may analyze input scalar code and determine that certain portions of the scalar code can be executed in parallel. As a result, the compiler may modify the input scalar code for the identified portions so as to change the scalar code into parallelized code, e.g., single instruction multiple data (SIMD), multiple instruction multiple data (MIMD), or other type of parallelized code. In so doing, the compiler may insert statements, such as a pragma statement, that identifies the corresponding portion of code as one that can be executed in parallel and may optionally specify a number of threads to use when executing the portion of code in parallel as a parallel region of code.","When the master thread encounters the region of code that is to be executed in parallel, the master thread spawns additional worker threads by obtaining an allocation of worker threads from a pool of worker threads. In the depicted example in , the master thread continues to perform work of the parallel region in parallel with the worker threads (shown as dashed lines in ). The worker threads return the results of their parallel execution to the master thread which then continues with the execution of the code after completion of the parallel region.","Looking at , in order to spawn the worker threads, in accordance with the OpenMP standard, the master thread executes a function call to the runtime environment that instigates a request for the allocation of worker threads (). The function call is performed by the master thread using parameters such as the number of requested threads (if any, otherwise a default number is utilized), the user function performing the work being requested to be done in parallel, etc.","The master thread selects worker threads () either based on an environment variable that specifies a number of worker threads to select or a default variable value that specifies a default number of worker threads if available, or if the default or environment variable specified number of worker threads is not available, then an available number of threads from 1 to the number of threads worker threads requested may be allocated. This number of worker threads requested is actually N\u22121 threads, where N is the total number of threads, since the master thread also performs some of the work of the parallel region, i.e. the master thread+N\u22121 worker threads=total number of threads N.",{"@attributes":{"id":"p-0056","num":"0055"},"figref":["FIG. 3C","FIG. 3C"]},"The particular threads that are selected can be selected in an arbitrary manner from a pool of worker threads. Particular runtimes may chose, for example, to distribute the workload among different cores evenly. For example, in a system with 4 threads per core and 8 cores, with a total of 4*8=32 threads, the runtime may decide to allocate 16 threads by selecting two threads per core in the system. The advantage of this approach is to even out the work among as many cores as possible. A runtime may also choose to pack the threads as densely as possible. For example, when creating a team of 4 worker threads, an OpenMP runtime in accordance with the illustrative embodiments may decide to select three more threads in the same core as the master thread. The advantage of this second approach is that all 4 threads will share the same cache hierarchy when assigned to a single core, and thus benefit from additional locality if the threads access common data. Other runtimes may chose to distribute the threads for some requests, and may pack threads for other request based on various criteria any implementation being within the spirit and scope of the illustrative embodiments.","Returning again to , each of the selected threads are initialized () by notifying the thread of its assigned thread identifier, e.g., a number from 0 to Tnum\u22121 where Tnum is a total number of threads allocated, and notifying the thread of the location where it is to begin work for the parallel region, e.g., the function address or the like. Other data structures are initialized as well (), e.g., barrier and lock data structures shared by the allocated threads, OpenMP state data structures inherited by worker threads, and the like.","The master thread then sends a \u201cgo ahead\u201d command () to the worker threads which have been maintaining a wait state (). The master thread initializes its own data structures () and then performs its work with regard to the parallel region (). After completing its portion of work for the parallel region, the master thread executes a barrier instruction ().","In parallel, in response to the \u201cgo ahead\u201d command, the worker threads receive the command () and initialize their data structures (). The worker threads then perform their work of the parallel region () and when this work is completed, they execute a barrier instruction (). The worker threads then return to a wait state ().","Once the worker threads have executed the barrier instruction, which writes a value to the barrier data structure indicating which threads have completed their work, the master thread knows that the worker threads have completed and can free the worker threads for future work (). The master thread can then complete the request to execute the parallel region () returning the results of the parallel region of code's execution.","With this process, the selection of worker threads (), initialization of the selected threads (), sending of the \u201cgo ahead\u201d command to the worker threads () and the freeing of the working threads () represent significant overhead to the performance of the parallel region of code. The mechanisms of the illustrative embodiments improve the execution of these portions of the master thread process by providing a mechanism for caching thread allocation information and using that information to determine how to allocate worker threads for future executions of parallel regions. The functionality, or at least portions of the functionality, of the illustrative embodiments may be implemented in an improved OpenMP runtime library, or other runtime library, that may be invoked by threads, such as a master thread and\/or worker threads.","To improve these portions of the master thread OpenMP standard shown in , the mechanisms of the illustrative embodiments utilize a plurality of vector registers for caching thread allocation information. The vector registers maintain thread allocation information that identifies which worker threads were previously allocated and to what master thread the worker threads were allocated. Moreover, vector registers for storing a thread identifier and work identifier are provided that, along with work descriptors, are used to inform threads of the work they are to perform. In addition, vector registers are provided for storing a \u201clocal go\u201d and \u201cglobal go\u201d value for each of the possible threads and these vector register \u201clocal go\u201d and \u201cglobal go\u201d values may be used to determine which threads to instruct to start their associated work.",{"@attributes":{"id":"p-0064","num":"0063"},"figref":["FIG. 4","FIG. 4","FIG. 4"]},"As shown in , the thread allocation mechanism includes a thread allocation engine  and a thread allocation cache . The thread allocation cache  may be considered to be a data structure stored in one or more registers of one or more memories of a data processing device, for example. In one illustrative embodiment, the thread allocation cache  utilizes vector registers that store a plurality of values, one for each possible thread that may be allocated simultaneously.","The thread allocation engine  implements the logic described herein with regard to performing thread allocations from the thread pool , informing the allocated threads of their work using work descriptors - in a work descriptor storage device , and releasing threads back to the thread pool  when the threads have completed their work. The releasing of the threads back to the thread pool  may further include operations for updating the thread allocation cache  to reflect the most recent allocation of threads by a master thread.","When a master thread  requires worker threads to execute a parallel region of code, the master thread  invokes the thread allocation engine  to select an appropriate set of worker threads to be allocated to the master thread for use in performing the required work. The invocation of the thread allocation engine  involves the master thread  sending a request to the thread allocation engine  that identifies the master thread's thread identifier (thread id) and the number of worker threads required, i.e. N\u22121 where N is the total number of threads to be used to process the parallel region of code. The thread allocation engine  accesses the thread allocation cache  and uses the thread allocation information in the thread allocation cache  to determine which worker threads to allocate to the master thread , if any.","The thread allocation cache  comprises a busy data structure , a reserved data structure , thread identifier data structure , a work identifier data structure , a local go data structure , and a global go data structure . These data structures may be stored in one or more memory devices, such as one or more vector registers or the like. These data structures may be dynamically created in response to the entry by a master thread into a parallel region of code. That is, the data structures - may be associated with a particular parallel region of code and may store thread allocation information for that parallel region of code and may be discarded after the execution of the parallel region of code is completed. The data structures - may also be associated with a particular thread in a particular parallel region of code and may store thread allocation information for that thread in that parallel region of code and may be discarded after the execution of that thread in that parallel region of code is completed. Alternatively, the data structures may be more permanent and may store thread allocation information for multiple parallel regions of code or multiple entries into the same parallel region of code.","The busy data structure  stored values indicative of which threads are currently allocated. The reserved data structure  stores values indicative of which threads were previously allocated and to which master threads they were allocated. The thread identifier data structure  stores thread identifiers for threads. The work identifier data structure  stores a work descriptor identifier for the threads so as to identify which work descriptor - corresponds to which thread. The work descriptor storage  stores one or more work descriptors for use by threads in identifying a location of the work that they are to perform. The local go data structure  and global go data structure  store values that are compared to determine which worker threads should commence work, as described hereafter.","Using the thread allocation cache , the thread allocation engine  determines based on the thread identifier of the master thread  and the number of worker threads requested by the master thread , how many and which ones of the worker threads to allocate to the master thread. The selected worker threads are then allocated to the master thread  from the thread pool . The thread allocation engine  writes a thread identifier for the worker threads to corresponding memory locations of the thread identifier data structure . The master thread writes a work descriptor - for the allocated worker threads, identifying an address at which the worker threads are to being execution, and optionally a work descriptor id to a corresponding memory location in work identifier data structure . The local and global go data structures  and  are then utilized to generate a \u201cgo ahead\u201d command to the allocated threads, as will be described in greater detail hereafter, so that the worker threads may then commence their work at the location specified in the work descriptor.","Once the work has been completed for the parallel region of code, or alternatively after thread allocation, the thread allocation engine  updates the information stored in the thread allocation cache  to reflect the most recent thread allocation. The functionality summarized above will now be described in greater detail with reference to .","It is assumed, for purposes of illustration, that in the examples shown in  that the particular processor architecture supports up to 16 simultaneously executing threads, i.e. the processor is a simultaneous multi-threaded (SMT) 16 processor architecture. For purposes of illustration only, it is assumed that each thread spawns up to an additional 4 threads, however this is not a limitation of the illustrative embodiments and any number of threads may be spawned in other implementations without departing from the spirit and scope of the illustrative embodiments. It is further assumed that the data structures of the thread allocation cache are stored in vector registers where each vector register has one or more bits for a total number of threads supported by the processor architecture, e.g., in the depicted example there is a minimum of 16 bits, one bit for each of the 16 threads that is the maximum number of simultaneously executing threads supported by the example processor architecture.","While these assumptions will be used for illustration purposes, it should be appreciated that the present invention is not limited to such assumptions or the examples shown in . To the contrary, the illustrative embodiments of the present invention may include any number of threads that may be simultaneously executed and data storage that may include any number of bits for each of the threads, without departing from the spirit and scope of the present invention. For example, on a machine that can represent vectors of up to 128 bits per hardware vector register, an embodiment storing a vector of 256 bits in memory would split this 256 bit vector in two 128 bit quantities that it could load separately from memory, operate on, and then store again into memory.",{"@attributes":{"id":"p-0074","num":"0073"},"figref":["FIGS. 5A-5C","FIGS. 5A-5C"],"b":["510","520","560","510","520","560"]},"As shown in , an example set of threads (depicted as numbered ovals in ) are executing one or more parallel regions of code. In the depicted example, a master thread 0 has spawned three additional threads 4, 8, and 12. The numbering of the threads in  is for illustration purposes only and do not reflect the actual thread identifiers associated with the threads. As will be described in , the thread identifiers may be different from the thread numbers in  and are assigned in relation to their master thread in accordance with the OpenMP standard.","Thus, in the depicted example, threads 4, 8, and 12 are worker threads for master thread 0. In addition, as shown in , threads 0, 4, and 8 spawn additional worker threads 1-3, 5-7, and 9-11 to perform work for various parallel regions of code. Thus, threads 4 and 8 are master threads for threads 5-7 and 9-11.","A busy vector register  comprises a bit for each of a maximum number of threads, e.g., 16 in this example. As threads are allocated or spawned, the corresponding bit in the busy vector register  is set to indicate that the thread has been allocated for performing work. Thus, in the depicted example, the bits for threads 0-12 have been set (as represented by the shaded boxes) while the bits for threads 13-15 have not been set since these threads have not yet been allocated.","At a later time, represented in , the worker threads 1-3, 5-7, and 9-11 have completed their work on the respective parallel regions of code. As a result, their corresponding bits in the busy vector register  have been cleared or reset to indicate that these threads are free for reallocation from the thread pool. When these worker threads are freed, or alternatively in response to the allocation of the worker threads to a master thread, bits corresponding to these worker threads are set in one or more of the reserved vector registers - in order to preserve the thread allocation information for future thread allocation determinations.","As shown in , the set of reserved vector registers - comprises a global reserved vector register  that stores bit values indicative of the union of the other reserved vector registers -. In essence, the global reserved vector register  has bits set for each worker thread that has been allocated at some time in the past. Thus, by looking at global reserved vector register , the thread allocation engine can determine which threads have previously been allocated and which threads have not yet been previously allocated.","In addition to the global reserved vector register , master thread reserved vector registers - are provided for each of the master threads 0, 4, 8, and 12. Bits may be set in respective ones of the master thread reserved vector registers - to indicate which worker threads have been allocated to the master thread. For example, as shown in , since threads 1-3 were allocated to the master thread 0, the bits associated with worker threads 1-3 are set in the master thread reserved vector register  which is associated with master thread 0. Similarly, the bits for worker threads 5-7 are set in the master thread reserved vector register  which is associated with the master thread 4. In a same way, the bits for the worker threads 9-11 are set in the master thread reserved vector register  which is associated with the master thread 8.","It should be noted that the bits for master threads 0, 4, 8, and 12 are not set in this example because in one implementation of the illustrative embodiments the last allocation is stored in the reserved vector registers when the thread is freed and thus, a bit is not stored in both the \u201creserved\u201d and \u201cbusy\u201d vector registers at the same time. In such an implementation, thread 0 for example will eventually have the bits for threads 4, 8, and 12 set in its \u201creserved by 0\u201d vector register . In an alternative implementation, reserved bits could be set as threads are allocated or the reserved vector registers - may be redefined as \u201cbusy or reserved\u201d in which case the bits for threads 0, 4, 8, and 12 may be set in these vector registers -.","The reserved vector registers - may be used to determine which worker threads to allocated to master threads in response to requests from the master threads for worker threads. In one embodiment the thread allocation logic, when selecting worker threads to allocate to a master thread, first attempts to allocate worker threads that have been previously allocated to the same master thread. That is, for example, if master thread 4 again requests worker threads, and the number of worker threads requested is the same as, or smaller than, a previously allocated number of worker threads, then the previously allocated worker threads may again be allocated to the master thread 4. For example, if master thread 4 requests between 1 to 3 worker threads, and master thread 4 previously had threads 5-7 allocated to it, then threads 5-7 may again be allocated to master thread 4. In this way, the worker threads 5-7 may be reused by master thread 4. The same may be true for any of the other master threads 0, 8, and 12. This is shown in , for example.","In another embodiment, if the number of threads being requested by the master thread 4 is less than the number of worker threads previously allocated to master thread 4, then the previous allocation information in the reserved vector register  is voided and a new allocation of worker threads is generated from within the threads previously allocated to master thread 4. That is, for example, a new allocation of worker threads from within the range of threads 5-7 is generated. For example, if master thread 4 is requesting only 2 worker threads, rather than the previously allocated 3 worker threads, then 2 of the worker threads 5-7 are selected and allocated to the master thread 4, e.g., threads 5 and 7 may be allocated to master thread 4.","In one embodiment, if the number of threads being requested by the master thread 4 is greater than the number of threads previously allocated to master thread 4, then the previously cached allocation information is voided and a new allocation of threads is created with subsequent updating of the reserved vector registers -. For example, if the master thread 4 were to request 5 worker threads, then the allocation of worker threads 5-7 may be voided in reserved vector register , and a new allocation of worker threads 5-7 and 13 may be generated with corresponding bits in the reserved vector registers  and  being set.","In an other embodiment, when the number of threads requested by the master thread 4 is greater than the number of threads previously allocated to master thread 4, then the OpenMP runtime determines if it can reduce the number of threads allocated to master thread 4. Indeed, OpenMP Application Programming Interface of the illustrative embodiments allows the runtime to allocate fewer threads than requested in certain circumstances. For example, the user, or compiler, can indicate using an environment variable OMP_DYNAMIC or by using a function call omp_set_dynamic( ) that it is appropriate for the runtime to allocate fewer threads than requested as the runtime see fit. Alternatively, the user, or compiler, may have requested a number of threads that is greater than the total number of threads currently available. When this is the case, the runtime is again at liberty to assign any numbers of threads to master thread 4, ranging from no additional thread to the currently available number of threads. In summary, there are cases where the runtime is free to select the numbers of threads that will be assigned to thread 4. In such case, this other embodiment can use the size of the previously cached allocated information to determine a number of threads to assign to the master thread so as to be compatible with the previously cached allocated information. As a result, the previously cached allocation information does not need to be voided and can be reused to satisfy this current request as well, even though the user or compiler had originally asked for a larger number of threads than is presently indicated in the previously cached allocation.","With the thread allocation logic of the illustrative embodiments, stealing of threads that were previously reserved by other master threads is avoided. Thus, even though threads 9-11 may be available for allocation, and master thread 12 is requesting 3 worker threads, the thread allocation logic does not select threads 9-11 if there are other threads available that have not been previously allocated to a master thread. If possible, the thread allocation logic attempts to allocate worker threads that have been previously allocated to the same master thread. If that is not possible, then the thread allocation logic attempts to allocate worker threads from a pool of threads that have not been previously allocated to master threads. If that is not possible, then a determination is made as to whether a subset of the requested number of threads may be allocated from either available threads that have been previously allocated to the master thread or from threads that have not been previously allocated. If this is not possible, then the allocation of worker threads may return an indication that the allocation of worker threads was not possible.","In the depicted example of , since master thread 12 is requesting 3 worker threads, and worker threads - have not been previously allocated to other master threads, and to avoid stealing any of threads 1-3, 4-7, or 9-11 from the master threads to which they were previously allocated, threads 13-15 are allocated to master thread 12. In the depicted example of , in subsequent requests for worker threads, assuming that each master thread requests the same number of threads that were previously allocated to the master threads, the previously allocated worker threads are reused by reallocating them to their corresponding master threads.","Thus, with the thread allocation logic of the illustrative embodiments, when allocating worker threads from a pool of threads, if there are not enough available threads for the allocation, then a last allocation cached for the master thread is voided. If the request is for a different number of worker threads than requested a last time by the master thread, the last allocation information in the cache is voided and a new thread allocation is created with subsequent update of the cached thread allocation information. If the request is for the same number of threads as in the last thread allocation for the master thread, then the same worker threads are again allocated to the master thread.","All of these operations (with the exception of looking for new available threads when there is no prior cached allocation or the prior allocation cannot be reused, for example, because the current request is asking for more threads than reserved in the cached allocation) are performed in constant time or \u201cnear constant time,\u201d meaning that we can perform such operations by one, two, or a few operations. In particular, constant time indicates that these operations do not have costs that are proportional to the size of the team of worker threads being reserved and\/or the number of threads in the system. It is assumed here that the total number of threads in the system can be loaded, operated, or stored in one operation. This is the case for machines with, for example, 64-bit integer support and for runtimes with no more than 64 threads. This is also the case for machines with 128-bit SIMD engines supporting AND\/OR\/XOR logical operations and for runtimes with no more than 128 threads. If that were not the case, e.g., one needed to support 256 threads on a machine with 128-bit SIMD engines, then the operation time is no longer constant, as two 128 bit operations would be needed to support the 256 bit logical AND\/OR\/XOR operations required. However, having two or even 4 operations is still very low overhead, which could be qualified as \u201cnear-constant-time.\u201d","After thread allocation is performed, such as described above with regard to , the allocated worker threads are informed of their thread identifier and where to find the work that the allocated worker threads are to perform. This is done through the use of thread identifiers in a thread identifier vector register, work identifiers in a work vector register, and work descriptors. These structures and their use are described in greater detail with regard to .",{"@attributes":{"id":"p-0091","num":"0090"},"figref":["FIGS. 6A-6C","FIG. 6A"],"b":"610"},"The work identifiers in the work identifier vector register  stores a pointer to a work descriptor  specifying the starting address of the work that is to be performed by the worker threads. That is, the master thread may specify a starting address of a portion of code to be executed by the worker threads. The work identifier in the work identifier vector register  points to a corresponding work descriptor  and through the user of the work identifier, the work threads are able to obtain the starting address at which they are to begin execution. The work identifiers are preserved even after freeing the worker threads and the work identifiers can be reused when the same worker threads are being allocated to the same master thread. Thus, in order to cause the worker threads to perform different work in a subsequent allocation, the thread allocation logic need only update the work descriptor to point to a different starting address at which the worker threads are to begin execution.","For example, as shown in , the worker threads 1-3 allocated to master thread 0 are given thread identifiers 1-3 in thread identifier vector register . In addition, the thread allocation logic writes the starting address for these worker threads to the work descriptor 2  and sets a pointer, \u201c2\u201d in this example, in the work identifier vector register  that points to the work descriptor 2 . Thus, worker threads 1-3, in order to determine the work that they are to perform, identify this work by following the pointer in the work identifier vector register  to the work descriptor 2 .","When threads 0-3 are freed, as shown in , the thread identifiers in thread identifier vector register  and work identifiers in the work identifier vector register  are preserved. When the threads 0-3 are again allocated, as shown in , the same thread identifiers and work identifiers in the vector registers  and  are reused. The work descriptor 2  is updated with the new starting address of the work that is to be performed by the newly allocated threads. Thus, the need to assign new thread identifiers and new work identifiers is minimized through the use of the mechanisms of the illustrative embodiments when reusing thread allocations. All that is necessary is that the work descriptor be updated.","Once the master side initialization operations are completed, including the thread selection and allocation, and setting up of the thread identifier, work identifier, and work descriptor information in , the master thread sends a go ahead signal to the worker threads. The illustrative embodiments provide a mechanism for quickly and in constant time, providing go ahead signals to worker threads.",{"@attributes":{"id":"p-0096","num":"0095"},"figref":["FIGS. 7A-7B","FIGS. 7A and 7B"],"b":["710","710","710","710","710","720","720","720","710","720","720"]},"In response to the master thread completing its initialization operations for starting a parallel region of code, the master thread causes the bits in the global go vector register  for the master thread and its worker threads to be flipped. If there is a difference between corresponding bits of the local go and global go vector registers  and , as may be determined by an XOR operation or the like, corresponding worker threads exit their idle wait state and access the thread identifier and work identifiers in the vector registers of  to begin performing the work that they are assigned to perform for the master thread.","For example, as shown in , initially, the local and global go vector registers  and  match. The bits in the global go vector register  corresponding to master thread 0 and its worker threads are then flipped while the remaining bits are not flipped. As a result, the bits in the global go vector register  with regard to threads 0-3 are different from the bits in the local go vector register  with regard to threads 0-3. As a result, threads 0-3 exit their idling loop and initiate their work in the parallel region of code, i.e. the threads 0-3 are awakened. Thereafter, each of the awakened threads 0-3 copy the value of the bits in the global go vector registers  associated with each awakened thread 0-3 into the corresponding value in the local go vector register . When the local go and global go vector registers  and  are again different, the corresponding threads are again awakened. Note that because the master thread is already awaken (i.e. not idle), another embodiment may choose not to include the master thread in the set of threads for which the local and global go vector registers are changed. It is also understood that while the embodiments described herein use the value 0 and 1 within the local and global go vector registers -, other embodiments may use any other two or more values without departing from the spirit and scope of the illustrative embodiments. Similarly, in the above embodiments, worker threads are awaken by determining that the local and global go values are different. In another embodiment, the worker thread are awaken when the values are the same, and remain idle while the values are distinct.","Thus, the illustrative embodiments provide mechanisms for allocating threads for use in executing parallel regions of code in a parallel manner. The mechanisms of the illustrative embodiments cache past thread allocations and configurations and data structures for the threads so that they may be reused. New allocations of threads are steered away from prior thread allocations and configurations so that the prior thread allocations and configurations can increasingly be reused. The operations of the thread allocations and freeing of threads are performed in constant time. Re-computing known configurations is avoided as is having to perform communication of known configurations to worker threads by reusing thread allocations and maintaining work identifiers and thread identifiers between allocations so that they may be reused.",{"@attributes":{"id":"p-0100","num":"0099"},"figref":["FIG. 8","FIG. 8"],"b":["810","820"]},"The cached prior thread allocation information is search (step ) and the number of threads requested is compared to the number of thread in a prior allocation of threads to the master thread (step ). A determination is made as to whether a prior allocation of threads can be reused (step ). For example, this may involve determining if the number of threads requested is equal to or less than the number of threads previously allocated to the master thread in a prior allocation. If so, then the prior allocation of threads may be again allocated to the master thread by setting busy bits and preserving work id and thread id data for the threads (step ). If not, then the cached prior allocation information may be cleared (step ) and a new allocation of threads may be generated by setting busy bits and updating work id and thread id data for the threads (step ). Thereafter, or if the prior allocation of threads is reused, then the cached prior allocation information may be updated to reflect the current allocation of worker threads to the master thread as well as the current allocation of the work descriptor to the master and worker threads (step ). The work description in the work descriptor pointed to by the work ID is then updated (step ). The operation then terminates.",{"@attributes":{"id":"p-0102","num":"0101"},"figref":["FIG. 9A","FIG. 9A"],"b":["910","915","920","915","925","925"]},{"@attributes":{"id":"p-0103","num":"0102"},"figref":["FIG. 9B","FIG. 9B"],"b":["930","935","930"]},"If there is a difference in the local go and global go vector register values for the worker thread, then the local go value associated with this worker thread is set to the same value as the global go value associated with this worker thread (step ). The work descriptor for the worker thread is obtained (step ) and a determination is made as to whether the work descriptor indicates that the execution of the parallel region of code is at an end of this parallel region of code (step ). If so, the master thread and worker threads end their operation. Otherwise, the designated work identified in the work descriptor is performed (step ) and when complete, a barrier call or other synchronization function call is performed (step ). Step  may still include some initialization to be done for this worker thread by the runtime prior to and\/or after the actual work specified by the user. The operation then returns to step  and awaits a subsequent wake up of the worker thread.","As noted above, it should be appreciated that the illustrative embodiments may take the form of an entirely hardware embodiment, an entirely software embodiment or an embodiment containing both hardware and software elements. In one example embodiment, the mechanisms of the illustrative embodiments are implemented in software or program code, which includes but is not limited to firmware, resident software, microcode, etc.","A data processing system suitable for storing and\/or executing program code will include at least one processor coupled directly or indirectly to memory elements through a system bus. The memory elements can include local memory employed during actual execution of the program code, bulk storage, and cache memories which provide temporary storage of at least some program code in order to reduce the number of times code must be retrieved from bulk storage during execution.","Input\/output or I\/O devices (including but not limited to keyboards, displays, pointing devices, etc.) can be coupled to the system either directly or through intervening I\/O controllers. Network adapters may also be coupled to the system to enable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public networks. Modems, cable modems and Ethernet cards are just a few of the currently available types of network adapters.","The description of the present invention has been presented for purposes of illustration and description, and is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art. The embodiment was chosen and described in order to best explain the principles of the invention, the practical application, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated."],"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS","p":["The invention, as well as a preferred mode of use and further objectives and advantages thereof, will best be understood by reference to the following detailed description of illustrative embodiments when read in conjunction with the accompanying drawings, wherein:",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIGS. 3A-3C"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIGS. 5A-5C"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIGS. 6A-6C"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIGS. 7A-7B"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 9A"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 9B"}]},"DETDESC":[{},{}]}
