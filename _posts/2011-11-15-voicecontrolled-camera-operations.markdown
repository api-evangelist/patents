---
title: Voice-controlled camera operations
abstract: A computing device (e.g., a smart phone, a tablet computer, digital camera, or other device with image capture functionality) causes an image capture device to capture one or more digital images based on audio input (e.g., a voice command) received by the computing device. For example, a user's voice (e.g., a word or phrase) is converted to audio input data by the computing device, which then compares (e.g., using an audio matching algorithm) the audio input data to an expected voice command associated with an image capture application. In another aspect, a computing device activates an image capture application and captures one or more digital images based on a received voice command. In another aspect, a computing device transitions from a low-power state to an active state, activates an image capture application, and causes a camera device to capture digital images based on a received voice command.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09031847&OS=09031847&RS=09031847
owner: Microsoft Technology Licensing, LLC
number: 09031847
owner_city: Redmond
owner_country: US
publication_date: 20111115
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["With the increasing popularity of computing devices (e.g., smart phones) having image capture functionality, there is a need for improving the user experience by allowing quick access to image-capture functionality. Image capture operations on smart phones are typically initiated by physical contact with the device (e.g., by tapping a touchscreen or pressing a hardware button). Some computing devices provide dedicated hardware buttons (e.g., camera shutter buttons) to provide access to image capture functionality. However, there are situations when such interactions are not convenient for the user. For example, when taking a digital photograph with a smart phone, pressing a hardware button may cause camera shake, thereby distorting the resulting image.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","In one aspect, a computing device (e.g., a smart phone, tablet computer, digital camera, or other device with image capture functionality) causes an image capture device to capture one or more digital images based on audio input (e.g., a voice command) received by the computing device. For example, a user speaks a word or phrase, and the user's voice is converted to audio input data by the computing device. The computing device compares (e.g., using an audio matching algorithm) the audio input data to an expected voice command associated with an image capture application, and determines whether the word or phrase spoken by the user matches the expected voice command, In another aspect, a computing device activates an image capture application on the computing device and captures one or more digital images based on a voice command received by the computing device. In another aspect, a computing device transitions from a low-power state to an active state, activates an image capture application, and causes a camera device to capture digital images based on a voice command received by the computing device. In the low-power state, the computing device listens for voice commands that can cause a transition to the active state. The technologies described herein are useful, for example, for performing image capture functions without making physical contact with a device (e.g., to avoid camera shake, or to allow a subject of a photo in a self-portrait or group photo to initiate an image capture operation without touching the device or setting a timer) and without navigating through a menu system.","The foregoing and other objects, features, and advantages of the invention will become more apparent from the following detailed description, which proceeds with reference to the accompanying figures.","Techniques and tools are described for voice-controlled image capture operations. Certain embodiments of the disclosed technology allow the end user to perform image capture operations with voice commands to facilitate a fast, hands-free image capture experience. Certain embodiments of the disclosed technology allow recognition of voice commands based on context, and do not require a user to explicitly select a voice recognition mode prior to speaking a voice command. Certain embodiments of the disclosed technology also can be applied to other functionality (e.g., video or audio recording), in addition to image capture functionality. Certain embodiments described herein are useful, for example, for performing image capture functions without making physical contact with a device (e.g., to avoid camera shake, or to allow a subject of a photo in a self-portrait or group photo to initiate an image capture operation without touching the device or setting a timer) and without navigating through a menu system. As used herein, the term \u201cvoice-controlled\u201d refers generally to operations performed by a computing device responsive to sound. The term \u201cvoice-controlled\u201d is used in view of an exemplary scenario in which a human user issues a voice command (such as a spoken word or phrase) to control image capture operations on a computing device (e.g., a smart phone or tablet computer, a digital camera, or some other computing device with image capture functionality). However, embodiments described herein are not limited to control by human voice.","The examples described herein should not be construed as being limiting in any way. Instead, this disclosure is directed toward all novel and non-obvious features and aspects of the various disclosed embodiments, alone and in various combinations and sub-combinations with one another. The disclosed systems, methods, and apparatus are not limited to any specific aspect or feature or combinations thereof, nor do the disclosed things and methods require that any one or more specific advantages be present or problems be solved.","Although the operations of some of the disclosed methods are described in a particular, sequential order for convenient presentation, it should be understood that this manner of description encompasses rearrangement, unless a particular ordering is required by specific language set forth below. For example, operations described sequentially may in some cases be rearranged, omitted, or performed concurrently. Moreover, for the sake of simplicity, the attached figures may not show the various ways in which the disclosed things and methods can be used in conjunction with other things and methods. Additionally, the description sometimes uses terms like \u201cdetermine,\u201d \u201ccompare,\u201d and \u201ccapture\u201d to describe the disclosed methods. These terms are high-level abstractions of the actual operations that are performed. The actual operations that correspond to these terms can vary depending on the particular implementation and are readily discernible by one of ordinary skill in the art.","Any of the disclosed methods can be implemented as computer-executable instructions stored on one or more computer-readable storage media (e.g., non-transitory computer-readable media, such as one or more volatile memory components (such as DRAM or SRAM), or nonvolatile memory components (such as hard drives)) and executed on a computer (e.g., any commercially available computer, including smart phones or other mobile devices that include computing hardware). Any of the computer-executable instructions for implementing the disclosed techniques as well as any data created and used during implementation of the disclosed embodiments can be stored on one or more computer-readable media (e.g., non-transitory computer-readable media). The computer-executable instructions can be part of, for example, a dedicated software application or a software application that is accessed or downloaded via a web browser or other software application (such as a remote computing application). Such software can be executed, for example, on a single local computer (e.g., any suitable commercially available computer) or in a network environment (e.g., via the Internet, a wide-area network, a local-area network, a client-server network (such as a cloud computing network), or other such network) using one or more network computers.","For clarity, only certain selected aspects of the software-based implementations are described. Other details that are well known in the art are omitted. For example, it should be understood that the disclosed technology is not limited to any specific computer language or program. The disclosed technology can be implemented by software written in C++, Java, Perl, JavaScript, HTML5, or any other suitable programming language. Likewise, the disclosed technology is not limited to any particular computer or type of hardware. Certain details of suitable computers and hardware are well known and need not be set forth in detail in this disclosure.","Furthermore, any of the software-based embodiments (comprising, for example, computer-executable instructions for causing a computer to perform any of the disclosed methods) can be uploaded, downloaded, or remotely accessed through a suitable communication means. Such suitable communication means include, for example, the Internet, the World Wide Web, an intranet, software applications, cable (including fiber optic cable), magnetic communications, electromagnetic communications (including RF, microwave, and infrared communications), electronic communications, or other such communication means.","I. Exemplary Mobile Devices",{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 1","b":["100","102","102","104"]},"The illustrated mobile device  can include a controller or processor  (e.g., signal processor, microprocessor, ASIC, or other control and processing logic circuitry) for performing such tasks as signal coding, data processing, input\/output processing, power control, and\/or other functions. An operating system  can control the allocation and usage of the components  in various ways, such as by managing transitions between device states such as transitions between active states and low-power states, or transitions between locked and unlocked states, and can provide support for one or more application programs . The application programs can include common mobile computing applications (e.g., image-capture applications, email applications, calendars, contact managers, web browsers, messaging applications), or any other computing application.","The illustrated mobile device  can include memory . Memory  can include non-removable memory  and\/or removable memory . The non-removable memory  can include RAM, ROM, flash memory, a hard disk, or other well-known memory storage technologies. The removable memory  can include flash memory or a Subscriber Identity Module (SIM) card, which is well known in GSM communication systems, or other well-known memory storage technologies, such as smart cards. The memory  can be used for storing data and\/or code for running the operating system  and the application programs . Example data can include web pages, text, images, sound files, video data, or other data sets to be sent to and\/or received from one or more network servers or other devices via one or more wired or wireless networks. The memory  can be used to store a subscriber identifier, such as an International Mobile Subscriber Identity (IMSI), and an equipment identifier, such as an International Mobile Equipment Identifier (IMEI). Such identifiers can be transmitted to a network server to identify users and equipment.","The mobile device  can support one or more input devices , such as a touchscreen , microphone , camera , physical keyboard , trackball , and\/or proximity sensor , and one or more output devices , such as a speaker  and one or more displays . Other possible output devices (not shown) can include piezoelectric or haptic output devices. Some devices can serve more than one input\/output function. For example, touchscreen  and display  can be combined into a single input\/output device.","A wireless modem  can be coupled to an antenna (not shown) and can support two-way communications between the processor  and external devices, as is well understood in the art. The modem  is shown generically and can include a cellular modem for communicating with the mobile communication network  and\/or other radio-based modems (e.g., Bluetooth  or Wi-Fi ). The wireless modem  is typically configured for communication with one or more cellular networks, such as a GSM network for data and voice communications within a single cellular network, between cellular networks, or between the mobile device and a public switched telephone network (PSTN).","The mobile device can further include at least one input\/output port , a power supply , a satellite navigation system receiver , such as a Global Positioning System (GPS) receiver, an accelerometer , a gyroscope (not shown), and\/or a physical connector , which can be a USB port, IEEE 1394 (FireWire) port, and\/or RS-232 port. The illustrated components  are not required or all-inclusive, as any components can be deleted and other components can be added.",{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 2A","FIG. 2B"],"b":["210","200","250","200","220","222","224","230","232","234","236"]},"The mobile device  includes a microphone  and speaker , along with two proximity sensors  and , situated below the surface of the mobile device. In some examples, the proximity sensors  and  emit an infrared beam and receive a reflected infrared beam, which is reflected off the surface of a nearby object that has been illuminated by the emitted infrared beam. In other examples, the touchscreen display  can be used as a proximity sensor. In other examples, an image sensor coupled with a camera lens  can be used as a light sensor to detect an object in proximity with the mobile device . In other examples, a photodiode  can be used as a light sensor. In other examples, signals from one or more proximity sensors (e.g., proximity sensors  and , and\/or the touchscreen display ) can be combined with signals from a light sensor (e.g., an image sensor coupled with camera lens  and\/or a photodiode ) in order to determine objects in proximity with the mobile device  with improved accuracy.","The camera shutter button  of the mobile device  can be a dedicated single-action camera shutter button, or a dedicated dual-action camera shutter button with the ability to detect \u201chalf-press\u201d (partial actuation) and \u201cfull-press\u201d (further actuation) as distinct actions. In some examples, the dual action camera shutter button  has different attributes associated with half-press and full-press actions.","Turning to the rear view  shown in , the example mobile device  includes the camera lens  and an electronic flash . The individual components (e.g., the hardware buttons , , and , microphone , speaker , touchscreen display , camera lens  and flash ) can be coupled to a mobile device chassis (not shown), which is connected to internal components of the mobile device , for example: one or more processors, a power supply, and a modem.","The placement of components on the mobile device , such as the home button , power button , camera shutter button , the camera lens , electronic flash , proximity sensors  and , and the photodiode , can vary depending on implementation. The components shown in  are not required or all-inclusive, as any components can be deleted and other components can be added.","II. Exemplary Architecture For Voice-Controlled Image Capture Tool",{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 3","b":["300","310","300"]},"The architecture  includes a device operating system (OS)  and image capture tool . In the example shown in , the device OS  includes components for rendering (e.g., rendering visual output to a display, generating voice output for a speaker), components for networking, components for controlling and communicating with a camera device, and components for speech recognition. Alternatively, components indicated as being included in device OS  (e.g., speech recognition components) can be implemented in whole or in part on remote servers. For example, an audio matching algorithm can match input audio with expected voice commands on one or more local main processors, one or more local processors dedicated to speech recognition, one or more processors on a remote server, or some combination. The device OS  can manage user input functions, output functions, camera functions, storage access functions, network communication functions, and other functions for the device. The device OS  can provide access to such functions to the image capture tool .","Components for speech recognition can include, for example, a speech recognition engine that uses the Microsoft\u00ae Speech Application Programming Interface (SAPI) and\/or a Microsoft\u00ae Tellme\u00ae interactive voice response service to listen for and interpret voice input. Components for speech recognition in the device OS  can be used with the image capture tool  to listen for and interpret voice commands related to, e.g., image capture operations. Speech recognition for technologies disclosed herein can be implemented in a context of generalized voice input modes. For example, in addition to image capture operations, components for speech recognition also can be used for other applications and functionality such as voice calls (e.g., to answer, ignore, or silence incoming calls) and search functions (e.g., to perform searches on a local device, a remote server, or the Internet). Speech recognition (e.g., for generalized voice input modes) can be implemented in software such as stand-alone applications or plug-ins. Services such as the Microsoft\u00ae Bing\u00ae service, or Apple Inc.'s Siri service can be used to implement speech recognition (e.g., in generalized voice input modes) for technologies described herein.","A user can generate user input that affects camera control and handling of digital image data. The user input can be, for example, voice input or tactile input such as touchscreen input, button presses, or key presses. The device OS  can include functionality for recognizing commands from voice input, button input or key press input, recognizing taps, finger gestures, etc. to a touchscreen from tactile input, and creating user input event messages that can be used by image capture tool  or other software. The UI response engine  of the image capture tool  listens for user input event messages from the device OS  and determines appropriate responses to the UI event messages. The UI event messages can indicate voice command input or other UI events (e.g., from touchscreen gesture or tap input, button presses, directional buttons, trackball input). If appropriate, the UI response engine  can translate the UI event messages from the OS  into command events (e.g., image capture messages) sent to an image capture engine  of the image capture tool . Command events can cause the image capture engine  to invoke image capture operations.","The image capture engine  can consider current settings (possibly provided from the settings store ), and any messages from the UI response engine  that relate to image capture. From this information, the image capture engine  can determine whether and how to perform image capture operations. The image capture engine  can store captured image data to an image data store . Captured images can be stored and processed locally or uploaded from image data store  to remote storage on network resources. The device OS  mediates access to the storage and network resources. The characteristics and organization of the image data (e.g., compression, resolution, size, file format) depends on implementation. Views of captured images, audio feedback (e.g., sound effects such as shutter clicks, voice feedback), or other output can be rendered by the rendering engine . The rendering engine  can provide output commands for the rendered view to the device OS  for output on a display. The rendering engine  can also provide output commands to the device OS  for output over a speaker or headphones. The exact operations performed as part of the rendering depend on implementation.","Alternatives to the example architecture  shown in  also can be used. For example, generalized voice input tools can be used to implement generalized voice input modes. A generalized voice input tool can accept voice input for more than one function (e.g., search functionality, voice call functionality). Voice commands used in described technologies (e.g., voice commands used to control camera operations) can be received via generalized voice input tools. As another example, the image capture tool  can include more or fewer modules. A given module can be split into multiple modules, or different modules can be combined into a single layer. For example, the image capture engine can be split into multiple modules that control different aspects of image capture, or the image capture engine can be combined with the UI response engine and\/or the rendering engine. Functionality described with reference to one module (e.g., rendering functionality) can in some cases be implemented as part of another module.","III. Exemplary Voice-Controlled Image Capture Application",{"@attributes":{"id":"p-0042","num":"0041"},"figref":["FIG. 4","FIG. 4"],"b":["410","400","405","400","415","405","415","400","450","400","415"]},"A voice command can be any word, phrase, or utterance, and can be spoken in any language at any volume, distance, or orientation that is suitable for the mobile device  to recognize the command. For example, in an image capture scenario, a user can speak a command while in front of the camera (e.g., during a self-portrait), behind the camera (e.g., while looking into the viewfinder), beside the camera, wearing the camera (e.g., on a helmet while riding a bicycle), or in any other orientation. A voice command can be a default command or a custom command selected by a user. Training can be used to customize commands or to make voice recognition more accurate. Training can be useful to help audio matching algorithms accurately recognize commands given by different people, or by groups of people speaking in unison, which may have different characteristics and may benefit from the use of different acoustical models for matching. A user can be given options (e.g., via a settings menu) to perform training to improve accuracy (e.g., by recording the user's voice speaking a default command) and\/or to record a different command that can be used to replace a default command. Training can also take place automatically (e.g., by storing previously recognized commands) without user action. A voice command can be descriptive of the action to be taken (e.g., \u201ccapture image\u201d or \u201ctake photo\u201d), or some other command can be used. For example, the voice command can take the form of a word or phrase (e.g., \u201cone . . . two . . . three!\u201d) that a photographer may use to announce that the photo is about to be taken, or that photo subjects themselves might say before the photo is taken (e.g., a word or phrase that may cause the subjects to smile, such as \u201cfuzzy pickles!\u201d or \u201ccheese!\u201d or \u201ccheeseburgers!\u201d). Alternatively, other types of sound (e.g., clapping hands, snapping fingers, synthesized speech or tones) besides a human voice can be used to issue a command.","The mobile device can use audio matching algorithms to compare audio input with expected commands. The determination of whether audio input is a sufficient match for an expected command can depend on factors such as training, user settings (e.g., sensitivity settings), and environmental factors (e.g., background noise). When audio input is recognized as a sufficient match for an expected command by the mobile device , feedback can be provided to users. Such feedback can provide assurance to users that a voice command was recognized and that mobile device  will take a photo momentarily. For example, visual feedback (e.g., graphics such as an animated flashing light  or a countdown timer or text on the display , a flashing light-emitting diode (LED) indicator) can be used to indicate that a voice command has been received and recognized. As another example, audio feedback (e.g., a recorded voice or tone) can be output from speaker  to indicate that a voice command has been received. In the example shown in , the mobile device  has received a voice command to capture an image, as indicated by the animated flashing light .","As shown, the display  of the mobile device  shows a still image capture mode control  (highlighted to indicate that still capture is the currently selected capture mode), a video capture mode control , zoom-in and zoom-out controls  and , and an options control . The mobile device  also includes several hardware buttons, including a camera shutter button  located on a side surface of the mobile device, as well as a search button , a home button , and a back button , which are located on a front surface of the mobile device. These hardware buttons , , , and  can be used for invoking and\/or executing various operations using the mobile device . For example, camera shutter button  can be used for invoking and\/or executing an image capture application, as well as controlling functions within the image capture application, such as autofocusing and\/or operating a camera shutter. Search button  can be used to invoke and\/or execute a user interface for searching data. Home button  can be used for navigating software functionality on the mobile device  by, for example, setting the mobile device to a home screen state, or invoking other assigned functionality. Back button  can be used for navigating software functionality by, for example, requesting a previously requested object that was viewed using the mobile device .","Voice commands can be used for invoking or controlling functionality described with reference to controls and hardware buttons shown in  or other functionality (e.g., other image capture or image processing functionality, video or audio recording functionality). Besides commands to cause capture of images, other possible commands include commands to save a photo that has been taken (e.g., a \u201cKeep\u201d command), to delete a photo (e.g., a \u201cDelete\u201d command), to show a previously captured photo (e.g., a \u201cShow\u201d command), to record video or audio (e.g., a \u201cRecord\u201d command), to stop recording video or audio (e.g., a \u201cStop\u201d command). Voice commands used to control different functionality may comprise different words or phrases. Alternatively, the same word or phrase can be used as a voice command for different functionality, with the desired functionality being determined based on contextual cues. Exemplary contextual clues are described below.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 5","FIG. 5"],"b":["525","570","500","550","500","500","505","515","515","500","570","525","525","570","560","505","500","525","560","505","525","580","552","525"]},"IV. Exemplary States for Mobile Device with Voice-Controlled Image Capture Tool","Exemplary mobile devices described herein can receive input, perform operations on data, and provide functionality to users while in several different states (e.g., locked or unlocked, active or low-power) or combinations of states. For example, in an unlocked state, a mobile device can provide full access to the functionality of the device. Typically, a mobile device in an unlocked state is also in an active state, with the display powered on and other input\/output devices available for use. As another example, in a locked state, a mobile device can restrict access to functionality of the device while exhibiting behaviors such as displaying a lock screen (e.g., a screen that prompts a user for authentication information, such as a PIN or biometric data, to unlock the device), or displaying a blank screen or wallpaper. Depending on implementation, a mobile device in a locked state may still provide some functionality (e.g., image capture functionality) that can be accessed by users without unlocking the device. A locked mobile device may be in a locked\/active state (e.g., with the display powered on and showing an authentication screen) or a locked\/low-power (or \u201cstandby\u201d) state in which certain features of the mobile device are powered down (e.g., to reduce power consumption relative to an active state). For example, in a low-power state, the display is typically dimmed or powered off, and other components (e.g., hardware buttons, keypads, trackballs) can be deactivated, as well.","In described implementations, a mobile device having a voice-controlled image capture tool can be in a locked\/active state, a locked\/low-power state, an unlocked\/active state, or some other state. The voice-controlled image capture tool also can exist in different states. For example, on a mobile device in an unlocked\/active state, the voice-controlled image capture tool can be running in an active (or foreground) state (e.g., displaying a user interface that a user can interact with, while listening for voice commands), running in a background state (e.g., not displaying a user interface, but listening for voice commands), or \u201ctombstoned\u201d (e.g., terminated with some program state information preserved in order to allow the tool to restart from a previous state). As another example, on a mobile device in a locked\/active state or a locked\/low-power state, the voice-controlled image capture tool can be in a background state, while continuing to listen for voice commands. Alternatively, the voice-controlled image capture tool can omit listening for voice commands (e.g., to save on power consumption) in some situations, such as when the mobile device is in a locked state and\/or a low-power state, either by default or based on user settings.","Behavior of the voice-controlled image capture tool can vary depending on the state of the device or the state of the tool. For example, if the voice-controlled image capture tool is running in the foreground on a mobile device in an unlocked\/active state, the tool can interpret voice input and provide feedback (e.g., an animated flashing light on the display) if a voice command is recognized. As another example, if the tool is running in the background on a mobile device in an unlocked\/active state, the tool can interpret voice input, switch to the foreground, and initiate image capture operations (as appropriate) if a voice command is recognized. In this manner, a user does not have to launch the image capture tool before taking a photo. The recognition of a voice command can allow the image capture tool to be launched and a command event to be generated to cause an image to be captured, even when a different application (e.g., a map application) is running in the foreground when the voice command is received.","As another example, if the voice-controlled image capture tool is running on a mobile device in a locked\/active state, the tool can listen for voice commands and, if a voice command is recognized, the mobile device can, e.g., allow the voice-controlled image capture tool to perform image capture operations, display a GUI for the tool, and\/or perform other image-related operations, while remaining in a locked state and restricting access to other functionality (e.g., email, text messages, contact lists, other applications). This can allow multiple users to access image capture functionality, camera settings, or the like, without compromising the privacy and security of the device for the primary user.","As another example, if the voice-controlled image capture tool is running on a mobile device in a locked\/low-power state, the tool can listen for voice commands and, if a voice command is recognized, the mobile device can, e.g., transition to an active state, start the operating system, display a GUI for the tool, and generate command events to cause one or more digital images to be captured by an image capture device. Such functions can be performed while the device remains in a locked state and access to other functionality on the device is restricted. In this manner, the user does not need to put the device in an active state (e.g., by pressing a power button) before giving a voice command. After transitioning to an active state, the device can remain in an active state or return to a low-power state after a certain amount of time passes or some other event occurs (e.g., a button-press of a power button). To reduce power consumption (e.g., for devices in a low-power state), the voice-controlled image capture tool can listen for voice commands by using a low-power processor. The listening can be performed by, e.g., sampling audio input continuously or at regular time intervals. The listening can be performed in short bursts (e.g., enough time for a complete command to be spoken) to reduce power consumption.","Alternatively, other states or transitions between states can be used, or other events can cause transitions between states.","V. Exemplary Low-Power and Active State Components",{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIG. 6","FIG. 6","FIG. 6","FIG. 6"],"b":["600","605","610","620","630","640","642","600","650","660","670","680","690","600","640","600"]},"VI. Exemplary State Diagram for Contextual Voice Command Recognition","Described voice-controlled applications can use contextual cues to assist in recognizing commands and using resources (e.g., battery life) efficiently. For example, if a camera on the mobile device is active, a voice recognition engine can listen for specific commands related to image capture functionality. This can allow the voice recognition engine to quickly dismiss some sounds (such as background noise or conversation) and focus on a command grammar and vocabulary that is relevant to the current context (e.g., words or phrases that correspond to image capture commands). Other contextual clues include which applications or functions are active on a device, time of day, and location. For example, if the time of day and location indicate that the user is at work, voice recognition relating to image capture operations can be turned off (e.g., to avoid unintended camera operation). As another example, if a voice call is active on a smart phone, voice recognition relating to image capture operations can be turned off (e.g., to avoid unintended camera operation). As another example, voice recognition relating to image capture operations can be turned off whenever the camera is not active (e.g., to reduce power consumption).",{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 7","b":["700","702","710","712","720","714","710","722","716","710","750","700"]},"The state diagram shown in  applies to an example voice command scenario; other possible states and state transitions based on the descriptions herein also can apply depending on implementation and on desired voice command scenarios, but are omitted from  for the sake of brevity. For example, other states and state transitions apply where voice recognition can run in the background when a camera is not active. As another example, other states and state transitions apply where recognition of a voice command can cause the device to transition from a low-power state to an active state. As another example, other states and state transitions can apply where other device components (e.g., a gyroscope, an accelerometer) provide additional contextual cues (e.g., detecting the orientation of the device as being ready perform an operation such as capturing an image or recording video) for recognizing voice commands.","VII. Exemplary Techniques",{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 8","b":["800","800"]},"At , the computing device receives audio input (e.g., voice data from one or more human voices). At , the computing device compares the received audio input with an expected audio command (e.g., a word or phrase) associated with an image capture operation. For example, the computing device can convert audio input to text using a speech-to-text speech recognition system, and compare the input text to an expected command word or phrase. As another example, the computing device can compare audio input with other audio data (e.g., an audio clip of the user speaking a word or phrase) that represents the expected audio command. The expected audio command can be a default command or a command selected by a user. A user can record a new command to modify or replace a default command. The expected audio command can be a member of a set of several available commands, and the available commands can be associated with the image capture operation or other functionality. The expected audio command can map to an individual function (e.g., a single image capture operation) or to a set of several functions (e.g., a macro that performs a series of different camera functions or that repeats the same function, such as an image capture operation, a number of times). The set of available commands can be shared across different applications and\/or different types of computing devices (e.g., smart phones, personal computers, digital cameras, tablet computers, gaming consoles). At , based on the comparison at step , the computing device determines that the received audio input comprises a sufficient match for the expected audio command. At , based on the determination at step , the computing device causes an image capture device to capture one or more digital images.","The comparison at step  and the determination at step  can be based on statistical models (e.g., hidden Markov models) of speech characteristics. The comparison at step  and the determination at step  also can be based on training. For example, training can be performed to take characteristics of a user's speech into account to improve accuracy of recognition of voice commands. Training can occur based on user action (e.g., based on a user's selection of a training option from a setup menu) or automatically (e.g., based on previously recognized commands). The comparison at step  and the determination at step  can be performed at different locations (e.g., at a local device such as a smart phone, or at a remote server; on a general purpose processor, or on a dedicated special-purpose processor). Decisions on where to perform speech recognition tasks can be based on factors such as grammar size, nature of functions to be performed, resources available at local devices and remote servers (e.g., processing power, memory or storage capacity), and the quality of available communication links (e.g., latency in communication between a local device and a remote server). For example, for small grammar sizes (e.g., small sets of available voice commands), speech recognition tasks can be performed on a local device. As another example, for voice commands for functions where a fast response is desirable (e.g., triggering a camera shutter and capturing an image), speech recognition can be performed on a local device.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 9","b":["900","900"]},"At , the computing device receives a first voice command associated with an image capture application on the computing device. The computing device comprises an image capture device (e.g., a digital camera). At , responsive to the first voice command, the computing device activates the image capture application on the computing device. For example, the image capture application transitions from a background state on the computing device to a foreground state, in response to the received voice command. At , responsive to the received voice command, the computing device captures one or more digital images (e.g., still images, or images in a video sequence) with the image capture device.",{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 10","b":["1000","1000"]},"At , the computing device receives an image capture voice command. For example, a mobile phone having computer-executable instructions stored in storage media that correspond to several applications (including an image capture application) operable to be executed on the mobile phone receives the image capture voice command while in a low-power state. At , the computing device determines that the received command comprises a match for an expected command associated with an image capture application. At , the computing device transitions from a low-power state to an active state. For example, a mobile phone in a locked, low-power state can transition to an active state (e.g., by powering up a touchscreen display and\/or other components) responsive to the determination of step  that the received command comprises a match for the expected command. At , the computing device activates the image capture application. For example, a mobile phone with an image capture application running in the background can switch the image capture application to a foreground state responsive to the determination of step . At , the computing device outputs feedback (e.g., visual feedback, audio feedback) indicating that the received command was recognized as a legitimate command. At , the computing device causes a camera device to capture one or more digital images. For example, a mobile phone comprising a camera device captures one or more digital images responsive to the determination of step .","In any of the above techniques, any combination of the commands and operations described herein can be applied. Depending on implementation and the type of processing desired, processing stages shown in example techniques can be rearrange, added, omitted, split into multiple stages, combined with other stages, and\/or replaced with like stages.","VIII. Exemplary Computing Environment",{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 11","b":["1100","1100"]},"With reference to , the computing environment  includes at least one processing unit  coupled to memory . In , this basic configuration  is included within a dashed line. The processing unit  executes computer-executable instructions. In a multi-processing system, multiple processing units execute computer-executable instructions to increase processing power. The memory  may be non-transitory memory, such as volatile memory (e.g., registers, cache, RAM), non-volatile memory (e.g., ROM, EEPROM, flash memory, etc.), or some combination of the two. The memory  can store software  implementing any of the technologies described herein.","A computing environment may have additional features. For example, the computing environment  includes storage , one or more input devices , one or more output devices , and one or more communication connections . An interconnection mechanism (not shown) such as a bus, controller, or network interconnects the components of the computing environment . Typically, operating system software (not shown) provides an operating environment for other software executing in the computing environment , and coordinates activities of the components of the computing environment .","The storage  may be removable or non-removable, and includes magnetic disks, magnetic tapes or cassettes, CD-ROMs, CD-RWs, DVDs, or any other non-transitory computer-readable media which can be used to store information and which can be accessed within the computing environment . The storage  can store software  containing instructions for any of the technologies described herein.","The input device(s)  may be a touch input device such as a keyboard, touchscreen, mouse, pen, or trackball, a voice input device, a scanning device, or another device that provides input to the computing environment . The output device(s)  may be a display, printer, speaker, CD- or DVD-writer, or another device that provides output from the computing environment . Some input\/output devices, such as a touchscreen, may include both input and output functionality.","The communication connection(s)  enable communication over a communication mechanism to another computing entity. The communication mechanism conveys information such as computer-executable instructions, audio\/video or other information, or other data. By way of example, and not limitation, communication mechanisms include wired or wireless techniques implemented with an electrical, optical, RF, infrared, acoustic, or other carrier.","The techniques herein can be described in the general context of computer-executable instructions, such as those included in program modules, being executed in a computing environment on a target real or virtual processor. Generally, program modules include routines, programs, libraries, objects, classes, components, data structures, etc., that perform particular tasks or implement particular abstract data types. The functionality of the program modules may be combined or split between program modules as desired in various embodiments. Computer-executable instructions for program modules may be executed within a local or distributed computing environment.","Any of the storing actions described herein can be implemented by storing in one or more computer-readable media (e.g., non-transitory computer-readable storage media or other tangible media). Any of the things described as stored can be stored in one or more computer-readable media (e.g., computer-readable storage media or other tangible media).","Any of the methods described herein can be implemented by computer-executable instructions in (e.g., encoded on) one or more computer-readable media (e.g., non-transitory computer-readable storage media or other tangible media). Such instructions can cause a computer to perform the method. The technologies described herein can be implemented in a variety of programming languages.","Any of the methods described herein can be implemented by computer-executable instructions stored in one or more non-transitory computer-readable storage devices (e.g., memory, CD-ROM, CD-RW, DVD, or the like). Such instructions can cause a computer to perform the method.","IX. Exemplary Implementation Environment",{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 12","b":"1200"},"In example environment , various types of services (e.g., computing services , which can include any of the methods described herein) are provided by a cloud . For example, the cloud  can comprise a collection of computing devices, which may be located centrally or distributed, that provide cloud-based services to various types of users and devices connected via a network such as the Internet. The cloud computing environment  can be used in different ways to accomplish computing tasks. For example, with reference to the described techniques and tools, some tasks, such as presenting a user interface, can be performed on a local computing device, while other tasks, such as data processing or storage of data to be used in subsequent processing, can be performed elsewhere in the cloud.","In example environment , the cloud  provides services for connected devices with a variety of screen capabilities A-N. Connected device A represents a device with a mid-sized screen, For example, connected device A could be a personal computer such as desktop computer, laptop, notebook, netbook, or the like. Connected device B represents a device with a small-sized screen. For example, connected device B could be a mobile phone, smart phone, personal digital assistant, tablet computer, and the like. Connected device N represents a device with a large screen. For example, connected device N could be a television (e.g., an Internet-enabled television) or another device connected to a television or projector screen (e.g., a set-top box or gaming console).","A variety of services can be provided by the cloud  through one or more service providers (not shown). Cloud services can be customized to the screen size, display capability, or other functionality of the particular connected device (e.g., connected devices A-N). For example, cloud services can be customized for mobile devices by taking into account the screen size, input devices, and communication bandwidth limitations typically associated with mobile devices.","In view of the many possible embodiments to which the principles of the disclosed invention may be applied, it should be recognized that the illustrated embodiments are only preferred examples of the invention and should not be taken as limiting the scope of the invention. Rather, the scope of the invention is defined by the following claims. We therefore claim as our invention all that comes within the scope and spirit of these claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIGS. 2A and 2B"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 12"}]},"DETDESC":[{},{}]}
