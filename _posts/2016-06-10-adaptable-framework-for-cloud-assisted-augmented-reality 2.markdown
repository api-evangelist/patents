---
title: Adaptable framework for cloud assisted augmented reality
abstract: A mobile platform efficiently processes image data, using distributed processing in which latency sensitive operations are performed on the mobile platform, while latency insensitive, but computationally intensive operations are performed on a remote server. The mobile platform acquires image data, and determines whether there is a trigger event to transmit the image data to the server. The trigger event may be a change in the image data relative to previously acquired image data, e.g., a scene change in an image. When a change is present, the image data may be transmitted to the server for processing. The server processes the image data and returns information related to the image data, such as identification of an object in an image or a reference image or model. The mobile platform may then perform reference based tracking using the identified object or reference image or model.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09633447&OS=09633447&RS=09633447
owner: QUALCOMM Incorporated
number: 09633447
owner_city: San Diego
owner_country: US
publication_date: 20160610
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATION(S)","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application is a continuation of co-pending U.S. application Ser. No. 13\/235,847, filed Sep. 19, 2011, entitled \u201cAn Adaptable Framework For Cloud Assisted Augmented Reality,\u201d which claims under 35 USC \u00a7119 the benefit of and priority to U.S. Provisional Application No. 61\/384,667, filed Sep. 20, 2010, and entitled \u201cAn Adaptable Framework For Cloud Assisted Augmented Reality\u201d both of which are assigned to the assignee hereof and are incorporated herein by reference.","An augmented reality system can insert virtual objects in a user's view of the real world. There may be many components in a typical AR system. These include: data acquisition, data processing, object detection, object tracking, registration, refinement, and rendering components. These components may interact with each other to provide the user a rich AR experience. Several components in detection and tracking in a typical AR system, however, may utilize computationally intensive operations, which can disrupt the AR experience for the user.","A mobile platform efficiently processes sensor data, including image data, using distributed processing in which latency sensitive operations are performed on the mobile platform, while latency insensitive, but computationally intensive operations are performed on a remote server. The mobile platform acquires sensor data, such as image data and determines whether there is a trigger event to transmit the sensor data to the server. The trigger event is a change in the sensor data relative to previously acquired sensor data, e.g., a scene change in the captured image. When a change is present, the sensor data is transmitted to the server for processing. The server processes the sensor data and returns information related to the sensor data, such as identification of an object in an image. The mobile platform may then perform reference based tracking using the identified object.","In one implementation a method includes acquiring image data using a mobile platform, wherein the image data is from at least one captured image of an object; tracking the object with visual based tracking using the at least one captured image of the object; determining whether there is a trigger event comprising a change in the image data relative to previously acquired image data, wherein the trigger event comprises a scene change in which a different object appears in the at least one captured image with respect to a previous captured image; transmitting the image data to a server when there is the trigger event while continuing to track the object with visual based tracking using the at least one captured image of the object; and receiving information related to the image data from the server, wherein the information related to the image data comprises at least one of the following: a two dimensional (2D) model of the object, a three dimensional (3D) model of the object, a three-dimensional coordinate estimation of points on the object, augmentation information, saliency information about the object, and information related to object matching.","In one implementation, a mobile platform includes a sensor adapted to acquire image data, wherein the sensor is a camera and the image data is from at least one captured image of an object; a wireless transceiver; and a processor coupled to the sensor and the wireless transceiver, the processor adapted to acquire the image data via the sensor, to track the object with visual based tracking using the at least one captured image of the object, to determine whether there is a trigger event comprising a change in the image data relative to previously acquired image data, wherein the trigger event comprises a scene change in which a different object appears in the at least one captured image with respect to a previous captured image, to transmit via the wireless transceiver the image data to an external processor when the trigger event is present while continuing to track the object with visual based tracking using the at least one captured image of the object, and to receive information related to the image data from the external processor via the wireless transceiver, wherein the information related to the image data comprises at least one of the following: a two dimensional (2D) model of the object, a three dimensional (3D) model of the object, a three-dimensional coordinate estimation of points on the object, augmentation information, saliency information about the object, and information related to object matching.","In one implementation, a mobile platform includes means for acquiring image data, wherein the means for acquiring image data is a camera and the image data is from at least one captured image of an object; means for tracking the object with visual based tracking using the at least one captured image of the object; means for determining whether there is a trigger event comprising a change in the image data relative to previously acquired image data, wherein the trigger event comprises a scene change in which a different object appears in the at least one captured image with respect to a previous captured image; means for transmitting the image data to a server when there is the trigger event while continuing to track the object with visual based tracking using the at least one captured image of the object; and means for receiving information related to the image data from the server, wherein the information related to the image data comprises at least one of the following: a two dimensional (2D) model of the object, a three dimensional (3D) model of the object, a three-dimensional coordinate estimation of points on the object, augmentation information, saliency information about the object, and information related to object matching.","In one implementation, a non-transitory computer-readable medium including program code stored thereon includes program code to acquire image data, wherein the image data is from at least one captured image of an object; program code to track the object with visual based tracking using the at least one captured image of the object; program code to determine whether there is a trigger event comprising a change in the image data relative to previously acquired image data, wherein the trigger event comprises a scene change in which a different object appears in the at least one captured image with respect to a previous captured image; program code to transmit the image data to an external processor when the trigger event is present while continuing to track the object with visual based tracking using the at least one captured image of the object, and program code to receive information related to the image data from the external processor, wherein the information related to the image data comprises at least one of the following: a two dimensional (2D) model of the object, a three dimensional (3D) model of the object, a three-dimensional coordinate estimation of points on the object, augmentation information, saliency information about the object, and information related to object matching.","A distributed processing system, as disclosed herein, includes a device that may determine when to provide data to a server via a wireless network, or to another device via network in a cloud computing environment, to be processed. The device may also process the data itself. For example, latency sensitive operations may be chosen to be performed on the device and latency insensitive operations may be chosen to be performed remotely for more efficient processing. Factors for determining when to send data to the server to be processed may include whether operations being performed on the data are latency sensitive\/insensitive, an amount of computation required, processor speed\/availability at either the device or the server, network conditions, or quality of service, among other factors.","In one embodiment, a system including a mobile platform and an external server is provided for Augmented Reality (AR) applications, in which latency sensitive operations are performed on the mobile platform, while latency insensitive, but computationally intensive operations are performed remotely, e.g., on the server, for efficient processing. The results may then be sent by the server to the mobile platform. Using distributed processing for AR applications, the end-user can seamlessly enjoy the AR experience.","As used herein, a mobile platform refers to any portable electronic device such as a cellular or other wireless communication device, personal communication system (PCS) device, personal navigation device (PND), Personal Information Manager (PIM), Personal Digital Assistant (PDA), or other suitable mobile device. The mobile platform may be capable of receiving wireless communication and\/or navigation signals, such as navigation positioning signals. The term \u201cmobile platform\u201d is also intended to include devices which communicate with a personal navigation device (PND), such as by short-range wireless, infrared, wireline connection, or other connection\u2014regardless of whether satellite signal reception, assistance data reception, and\/or position-related processing occurs at the device or at the PND. Also, \u201cmobile platform\u201d is intended to include all electronic devices, including wireless communication devices, computers, laptops, tablet computers, etc. which are capable of AR.",{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 1","b":["100","100","110","130","112","114","164","110","104","102","114","104","110","110","112","166","164"]},"The mobile platform  transmits the acquired data information, such as the captured image  and\/or the sensor data, such as SPS information or position information from on-board motion sensors , to the server  via a network . The acquired data information may also or alternatively include contextual data, such as the identification of any objects that are currently being tracked by the mobile platform . The network  may be any wireless communication networks such as a wireless wide area network (WWAN), a wireless local area network (WLAN), a wireless personal area network (WPAN), and so on. The server  processes the data information provided by the mobile platform  and generates information related to the data information. For example, the server  may perform object detection and identification based on provided image data using an object database . The server  returns to the mobile platform  information that is related to the acquired data. For example, if the server  identifies an object from image data provided by the mobile platform , the server  may return an identification of the object, for example, including an identifier such as a title or identifying number or a reference image  of the object , as well as any desired side information, such as saliency indicators, information links, etc., that may be used by the mobile platform for the augmented reality application.","If desired, the server  may determine and provide to the mobile platform  a pose (position and orientation) of the mobile platform  at the time image  was captured relative to the object  in the reference image , which is, e.g., an image of the object  from a known position and orientation. The returned pose can be used to bootstrap the tracking system in the mobile platform . In other words, the mobile platform  may track all incremental changes in its pose, e.g., visually or using motion sensors , from the time it captures the image  to the time it receives the reference image  and pose from the server . The mobile platform  may then use the received pose along with its tracked incremental changes in pose to quickly determine the current pose with respect to the object .","In another embodiment, the server  returns the reference image , but does not provide pose information, and the mobile platform  determines a current pose with respect to the object  by comparing a current captured image of the object  with respect to the reference image  of the object  using an object detection algorithm. The pose may be used as an input to the tracking system so that relative motion can be estimated.","In yet another embodiment, the server  returns only the pose information but does not provide the reference image. In this case, the mobile platform  may use the captured image  along with the pose information to create a reference image which can subsequently be used by the tracking system. Alternatively, the mobile platform  may track incremental changes in position between the captured image  and a subsequently captured image (referred to as the current image) and may compute the pose of the current image relative to the mobile platform generated reference image using the pose obtained from the server  along with the incremental tracking results. In the absence of the reference image , the current image may be warped (or rectified) using the estimated pose to obtain an estimate of the reference image which may be used to bootstrap the tracking system.","Additionally, in order to minimize the frequency of detection requests sent by the mobile platform  to the server , the mobile platform  may initiate a detection request only if a trigger event is present. A trigger event may be based on a change in the image data or the sensor data from motion sensors  relative to previously acquired image data or sensor data. For example, the mobile platform  may use a scene change detector  to determine if a change in the image data has occurred. Thus, in some embodiments, the mobile platform  may communicate with the server  via network for detection requests only when triggered by the scene change detector . The scene change detector  triggers communication with the server for object detection, e.g., only when new information is present in the current image.",{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 2","b":["110","130","110","202"]},"The mobile platform  determines that there is a trigger event (), such as a change in the sensor data relative to previously acquired sensor data. For example, the trigger event may be a scene change in which a new or different object appears in the image. The acquired sensor data is transmitted to the server  after a trigger event, such as a scene change, is detected (). Of course, if no scene change is detected, the sensor data need not be transmitted to the server  thereby reducing communications and detection requests.","The server  processes the acquired information, e.g., to perform object recognition, which is well known in the art. After the server  processes the information, the mobile platform  receives from the server  information related to the sensor data (). For example, the mobile platform  may receive results of the object identification, including, e.g., a reference image. The information related to the sensor data may additionally or alternatively include information such as items that are located near the mobile platform  (such as buildings, restaurants, available products in a store, etc.) as well as two-dimensional (2D) or three-dimensional (3D) models from the server, or information that may be used in other processes such as gaming. If desired, additional information may be provided, including the pose of the mobile platform  with respect to the object in the reference image at the time that the image  was captured, as discussed above. If the mobile platform  includes a local cache, then the mobile platform  may store multiple reference images sent by the server . These stored reference images can be used, e.g., for subsequent re-detections that can be performed in the mobile platform  if tracking is lost. In some embodiments, the server identifies a plurality of objects from the sensor in the image. In such embodiments, a reference image or other object identifier may be sent to the mobile platform  for only one of the identified objects, or a plurality of object identifiers corresponding to respective objects may be transmitted to and received by the mobile platform .","Thus, information that may be provided by the server  may include a recognition result, information about the object(s) identified, reference images (one or many) about the object(s) which can be used for various functions such as in tracking, 2D\/3D model of the object(s) recognized, absolute pose of the recognized object(s), augmentation information to be used for display, and\/or saliency information about the object. Additionally, the server  may send information related to object matching that could enhance the classifier at the mobile platform . One possible example is when the mobile platform  is using decision trees for matching. In this case, the server  could send the values for the individual nodes of the tree to facilitate more accurate tree building and subsequently better matching. Examples of decision trees include, e.g., k-means, k-d trees, vocabulary trees, and other trees. In the case of a k-means tree, the server  may also send the seed to initialize the hierarchical k-means tree structure on the mobile platform , thereby permitting the mobile platform  to perform a look-up for loading the appropriate tree.","Optionally, the mobile platform  may obtain a pose for the mobile platform with respect to the object  (). For example, the mobile platform  may obtain the pose relative to the object in the reference image without receiving any pose information from the server  by capturing another image of the object  and comparing the newly captured image with the reference image. Where the server  provides pose information, the mobile platform may quickly determine a current pose, by combining the pose provided by the server , which is the pose of the mobile platform  relative to the object in the reference image at the time that the initial image  was captured, with tracked changes in the pose of the mobile platform  since the initial image  was captured. It is to be noted that whether the pose is obtained with or without the assistance of the server  may depend on the capabilities of the network  and\/or the mobile platform . For example, if the server  supports pose estimation and if the mobile platform  and the server  agree upon an application programming interface (API) for transmitting the pose, the pose information may be transmitted to the mobile platform  and used for tracking. The pose of the object  () sent by the server may be in the form of relative rotation and transformation matrices, a homography matrix, an affine transformation matrix, or another form.","Optionally, the mobile platform  may then perform AR with the object, using the data received from the server , such as tracking the target, estimating the object pose in each frame, and inserting a virtual object or otherwise augmenting a user view or image through the rendering engine using the estimated pose ().",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 3","FIG. 3"],"b":["100","130","300","302","302","302","300","302","304","304","302","304","302","304","304","308","306","308","306","308","110","130"]},"The quality of the image may be based on known image statistics, image quality measures, and other similar approaches. For example, the degree of sharpness of a captured image may be quantified by high pass filtering and generating a set of statistics representing, e.g., edge strengths and spatial distribution. The image may be classified as a good quality image if the sharpness value exceeds or is comparable to the \u201cprevailing sharpness\u201d of the scene, e.g., as averaged over several previous frames. In another implementation, a quick corner detection algorithm such as FAST (Features from Accelerated Segment Test) corners or Harris corners may be used to analyze the image. The image may be classified as a good quality image if there are a sufficient number of corners, e.g., the number of detected corners exceeds a threshold or is greater or comparable to the \u201cprevailing number of corners\u201d of the scene, e.g., as averaged over several previous frames. In another implementation, statistics from the image, such as the mean or standard deviation of the edge gradient magnitudes, may be used to inform a learning classifier, which may be used to distinguish between good quality and bad quality images.","The quality of the image may also be measured using sensor inputs. For example, images captured by the mobile platform  while moving quickly may be blurred and therefore of poorer quality than if the mobile platform  was static or moving slowly. Accordingly, motion estimates from sensor data, e.g., from motion sensors  or from visual based tracking, may be compared to a threshold to determine if resultant camera images are of sufficient quality to be sent for object detection. Similarly, the image quality may be measured based on a determined amount of image blur.","Additionally, a trigger time manager  may be provided to further control the number of requests transmitted to the server based detector . The trigger time manager  maintains the state of the system and may be based on heuristics and rules. For example, if the number of images from the last trigger image is greater than a threshold, e.g., 1000 images, the trigger time manager  may generate a trigger that may time-out and automatically initiate the detection process in the server based detector . Thus, if there has been no trigger for an extended number of images, the trigger time manager  may force a trigger, which is useful to determine if any additional objects are in the camera's field of view. Additionally, the trigger time manager  may be programmed to maintain a minimum separation between two triggers at a chosen value of \u03b7, i.e., the trigger time manager  suppresses triggers if it is within \u03b7 images from the last triggered image. Separating triggered images may be useful, for example, if the scene is changing fast. Thus, if the scene change detector  produces more than one trigger within \u03b7 images, only one triggered image is sent to the server based detector , thereby reducing the amount of communication to the server  from the mobile platform . The trigger time manager  may also manage trigger schedules. For example, if the scene change detector  produces a new trigger that is less than \u03b7 images and greater than \u03bc images ago from the last trigger, the new trigger may be stored and postponed by the trigger time manager  until a time when the image gap between consecutive triggers is at least \u03b7. By way of example, \u03bc may be 2 images and \u03b7\u2267\u03bc, and by way of example, \u03b7 may vary as 2, 4, 8, 16, 32, 64.","The trigger time manager  may also manage detection failures of the server . For example, if a previous server based detection attempt failed, the trigger time manager  may periodically produce a trigger to re-transmit a request to the server based detector . Each of these attempts may use a different query image based on the most recent captured image. For example, after a detection failure, a periodic trigger may be produced by the trigger time manager  with a period gap of \u03b7, e.g., if the last failed detection attempt was longer ago than \u03b7 images ago, then a trigger is sent, where the value of \u03b7 may be variable.","When the server based detector  is initiated, the server  is provided with the data associated with the new captured image , which may include the new captured image  itself, information about the new captured image , as well as sensor data associated with the new captured image . If an object is identified by the server based detector , the found object, e.g., a reference image, a 3D model of the object, or other relevant information is provided to the mobile platform , which updates its local cache . If no object is found by the server based detector  the process may fall back to periodic triggering, e.g., using the trigger time manager . If there is no object detected after \u0393 attempts, e.g., 4 attempts, the object is considered to not be in the database and the system resets to scene change detector based triggers.","With the found object stored in local cache , an object detector  running on the mobile platform  performs an object detection process to identify the object in the current camera view and the pose with respect to the object and sends the object identity and pose to the reference based tracker . The pose and the object identity sent by the object detector  may be used to initialize and to start the reference based tracker . In each subsequently captured image (e.g., frame of video), the reference-based tracker  may provide the pose with respect to the object to a rendering engine in the mobile platform  which places desired augmentation on top of the displayed object or otherwise within an image. In one implementation, the server based detector  may send a 3D model of the object, instead of a reference image. In such cases, the 3D model is stored in the local cache  and subsequently used as an input to the reference based tracker . After the reference based tracker  is initialized, the reference based tracker  receives each new captured image  and identifies the location of the tracked object in each new captured image  thereby permitting augmented data to be displayed with respect to the tracked object. The reference based tracker  may be used for many applications, such as pose estimations, face recognition, building recognition, or other applications.","Additionally, after the reference based tracker  is initialized, the reference based tracker  identifies regions of each new captured image  where the identified object is present and this information stored by means for a tracking mask. Thus, regions in new camera images  for which the system has complete information are identified and provided as an input to the reference-free tracker  and the scene change detector . The reference-free tracker  and scene change detector  continue to receive each new captured image  and use the tracking mask to operate on remaining regions of each new captured image , i.e., regions in which there is not complete information. Using the tracking mask as feedback not only helps reduce mis-triggers from the scene change detector  due to tracked objects, but also helps reduce the computational complexity of the reference-free tracker  and the scene change detector .","In one embodiment, illustrated by dotted lines in , the server based detector  may additionally provide pose information for an object in the new captured image  with respect to the object in the reference image. The pose information provided by the server based detector  may be used along with changes in the pose, as determined by the reference-free tracker , by a pose updater  to produce an updated pose. The updated pose may then be provided to the reference based tracker .","Additionally, when tracking is temporarily lost, subsequent re-detections may be performed using a local detector  searching the local cache . While  illustrates the local detector  and object detector  separately for clarity, if desired, the local detector  may implement the object detector , i.e., object detector  may perform the re-detections. If the object is found in local cache, the object identity is used to re-initialize and to start the reference based tracker .",{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 4","FIG. 3"],"b":["130","316","304","320","308","308","320","302","308","320","310","302","312","312","320","308","312","302","320","320","302","314","314"]},{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 5","FIG. 4"],"b":["130","308","320","312","308","320"]},"As discussed above, the scene change detector  controls the frequency of detection requests sent to the server  based on changes in a current captured image with respect to previous captured images. The scene change detector  is used as it is desirable to communicate with the external server  to initiate object detection only when significant new information is present in the image.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":["FIG. 6","FIG. 3"],"b":["304","302","302"]},"Thus, as illustrated in , an input image  is provided. The input image is the current captured image, which may be the current video frame or photo. If the last image did not trigger scene change detection (), then initialization () of the scene change detector is performed (). Initialization includes dividing the image into blocks (), e.g., 8\u00d78 blocks for a QVGA image, and extracting keypoints from each block using, e.g., a FAST (Features from Accelerated Segment Test) corner detector, in which the M strongest corners are retained (), where M may be 2. Of course, other methods may alternatively be used for extracting keypoints, such as Harris corners, Scale Invariant Feature Transform (SIFT) feature points, Speeded-up Robust Features (SURF), or any other desired method. A no trigger signal is returned ().","If the last image did trigger scene change detection (), metrics are obtained from the reference-free tracker  (), illustrated as optical flow process , and image pixel histograms, illustrated as histogram process . If desired, the reference-free tracker  may produce metrics using processes other than optical flow, such as normalized cross-correlation. The optical flow process  tracks corners from a previous image (), e.g., using normalized cross correlation, and identifies their locations in the current image. The corners may have been previously extracted by dividing the image into blocks and selecting keypoints from each block using, e.g., a FAST corner detector in which the M strongest corners based on the FAST corner threshold are retained, as discussed in the initialization  above, or in the case of Harris corners, M strongest corners based on the Hessian threshold are retained. Reference free tracking is run for the chosen corners over consecutive images to determine the location of corners in the current image and the corners that are lost in tracking. The total strength of corners lost in the current iteration (d in ), i.e., between the current image a preceding image, is calculated as a first change metric and the total strength of corners lost since the previous trigger (D in ), i.e., between the current image and the previous trigger image, is calculated as a second change metric, which are provided for a video statistics calculation . The histogram process  divides the current input image (referred to as C) into B\u00d7B blocks and generates a color histogram Hfor each block (), wherein i and j are the block indices in the image. A block-wise comparison of the histograms is performed () with corresponding block's histograms from the Npast image Husing, e.g., the Chi-Square method. The comparison of the histograms helps determine the similarity between the current image and the Npast image so as to identify if the scene has changed significantly. By means of an example, B can be chosen to be 10. To compare the histograms of the current image and the Npast image using the Chi-Square method, the following computation is performed:",{"@attributes":{"id":"p-0052","num":"0051"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["f","ij"]},"mo":"=","mrow":{"mrow":[{"mi":"d","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["H","C"],"mrow":{"mi":["i","j"],"mo":","}},{"mi":["H","N"],"mrow":{"mi":["i","j"],"mo":","}}],"mo":","}}},{"munder":{"mo":"\u2211","mrow":{"mo":"\u2200","mi":"k"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mfrac":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msubsup":{"mi":["H","C"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}},{"msubsup":{"mi":["H","N"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}],"mo":"-"}},"mn":"2"},"mrow":{"mrow":[{"msubsup":{"mi":["H","C"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}},{"msubsup":{"mi":["H","N"],"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}],"mo":"+"}}}],"mo":"="}}},{"mrow":{"mi":"eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1"}}]}}}}},"The block-wise comparison produces an array fof difference values. The array fis sorted and a histogram change metric h is determined, e.g., as the mean of half the elements in the middle of the sorted array f(). The histogram change metric h is also provided for the video statistics calculation.","As discussed above, if desired, a tracking mask provided by the reference based tracker  (), may be used during scene change detection to reduce the regions of the input image to be monitored for scene change. The tracking mask identifies regions where an object is identified and therefore scene change monitoring may be omitted. Thus, for example, when the input image is divided into blocks, e.g., at , , the tracking mask may be used to identify blocks that fall within the regions with identified objects and, accordingly, those blocks may be ignored.","The video statistics calculation  receives the optical flow metrics d, D and the histogram change metric h and produces a determination of image quality, which is provided along with metrics d, D, and h to determine if detection should be triggered. A change metric \u0394 is calculated and compared () to a threshold to return a trigger signal (). Of course, if the change metric \u0394 is less than the threshold, no trigger signal is returned. The change metric \u0394 may be calculated () based on the optical flow metrics d, D and the histogram change metric h, e.g., as follows:\n\n\u0394=\u03b1\u2003\u2003eq. 2\n","Here \u03b1, \u03b2, and \u03b3 are weights that are appropriately chosen () to provide relative importance to the three statistics, d, D, and h. In one embodiment, the values of \u03b1, \u03b2, and \u03b3 may be set to a constant during the entire run. In an alternate embodiment, the values of \u03b1, \u03b2, and \u03b3 may be adapted depending on possible feedback received about the performance of the system or depending on the use-case targeted. For example, the value of \u03b1 and \u03b2 may be set relatively high compared to \u03b3 for applications involving panning type scene change detections because the statistics d and D may be more reliable in this case. Alternatively, the values of \u03b1 and \u03b2 may be set to be relatively low compared to \u03b3 for applications which primarily involve book flipping type of use cases where the histogram statistic h may be more informative. The threshold may be adapted () based on the output of the video statistics calculation , if desired.","In one case, if desired, the scene detection process may be based on metrics from the reference-free tracker , without metrics from histograms, e.g., the change metric \u0394 from equation 2 may be used with \u03b3=0. In another implementation, the input image may be divided into blocks and keypoints extracted from each block using, e.g., a FAST (Features from Accelerated Segment Test) corner detector, in which the M strongest corners are retained, as discussed above. If a sufficient number of blocks have changed between the current image and the previous image, e.g., compared to a threshold, the scene is determined to have changed and a trigger signal is returned. A block may be considered changed, e.g., if the number of corners tracked is less than another threshold.","Moreover, if desired, the scene detection process may be based simply on the total strength of corners lost since the previous trigger (D in ) relative to strength of the total number of corners in the image, e.g., the change metric \u0394 from equation 2 may be used with \u03b1=0 and \u03b3=0. The total strength of corners lost since the previous trigger may be determined as:",{"@attributes":{"id":"p-0059","num":"0058"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["D","c"]},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mrow":{"mi":"t","mo":"+","mn":"1"}},"mi":"c"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":["j","Li"],"mo":"\u2208"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["s","j"]}}},"mo":"."}}}},{"mrow":{"mi":"eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"3"}}]}}}}},"In equation 3, sis the strength of corner j, t is the last triggered image number, c is the current image number, and Li is the set containing identifiers of lost corners in frame i. If desired, a different change metric \u0394 may be used, such as:",{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"\u0394","mo":"=","mfrac":{"msub":{"mi":["D","c"]},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"msub":{"mi":["N","T"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["s","j"]}}}}},{"mrow":{"mi":"eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"4"}}]}}}}},"where Nis the total number of corners in the triggered image. The change metric \u0394 may be compared () to a threshold.","Additionally, as discussed above, the tracking mask may be used by the scene change detector  to limit the area of each image that is searched for changes in the scene. In other words, the loss of the strength of the corners outside of the area of the trigger mask is the relevant metric. A reduction in the size of the area searched by the scene change detector  leads to a corresponding reduction in the number of corners that can be expected to be detected. Thus, an additional parameter may be used to compensate for the loss of corners due to the tracking mask, e.g., as follows:",{"@attributes":{"id":"p-0064","num":"0063"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"\u03bb","mo":"=","mfrac":{"mrow":[{"mi":["strength","of","corners","in","mask"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},{"mi":["area","of","mask"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}]}}},{"mrow":{"mi":"eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"5"}}]}}}}},"The compensating parameter \u03bb may be used to adjust the change metric \u0394. For example, if the scene detection process is based simply on the total strength of corners lost in the unmasked area since the previous trigger (D), the change metric \u0394 from equation 4 may be modified as:",{"@attributes":{"id":"p-0066","num":"0065"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"\u0394","mo":"=","mfrac":{"mrow":[{"msub":{"mi":["D","c"]},"mo":"+","mrow":{"mi":"\u03bb","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"A","mo":"-","msub":{"mi":["A","c"]}}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"msub":{"mi":["N","T"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["s","j"]}}]}}},{"mrow":{"mi":"eq","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"6"}}]}}}}},"where Dis provided by equation 3 (with Li defined as the set containing identifiers of lost corners in the unmasked area in frame i), Ais the area of the mask for image c, and A is initialized to A.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":["FIG. 7","FIG. 7","FIG. 3","FIG. 6","FIG. 3","FIG. 7"],"b":["480","482","314","484","314","486","314","305"]},{"@attributes":{"id":"p-0069","num":"0068"},"figref":["FIG. 8","FIG. 8"],"b":["110","502","110","504","130","506","110","508","130","510","512","514","130","110","114","516"]},{"@attributes":{"id":"p-0070","num":"0069"},"figref":["FIG. 9","FIG. 9","FIG. 8","FIG. 9"],"b":["130","508","504","130"]},{"@attributes":{"id":"p-0071","num":"0070"},"figref":["FIG. 10","FIG. 10"],"b":["110","520","110","522","130","526","110","524","130","528","530","532","130","534","130","110","536","110","114","538"]},{"@attributes":{"id":"p-0072","num":"0071"},"figref":["FIG. 11","FIG. 11","FIG. 10","FIG. 11"],"b":["130","526","522","130"]},{"@attributes":{"id":"p-0073","num":"0072"},"figref":["FIG. 12","FIG. 12"],"b":["110","540","110","541","110","542","130","546","110","544","130","548","550","552","554","130","555","130","110","556","110","557","110","110","114","558"]},{"@attributes":{"id":"p-0074","num":"0073"},"figref":["FIG. 13","FIG. 13","FIG. 12","FIG. 13"],"b":["130","546","542","130"]},{"@attributes":{"id":"p-0075","num":"0074"},"figref":["FIG. 14","FIG. 14"],"b":["110","560","110","562","564","110","568","130","130","570","110","572","110","110","110","114","576","130"]},"It should be noted that the entire system configuration may be adaptable depending on the capability of the mobile platform , the server , and the communication interface, e.g., network . If the mobile platform  is a low-end device without a dedicated processor, most of the operations may be off-loaded to the server . On the other hand, if the mobile platform  is a high end device that has good computation capability, the mobile platform  may select to perform some of the tasks and off-load fewer tasks to the server . Further, the system may be adaptable to handle different types of communication interfaces depending on, e.g., the available bandwidth on the interface.","In one implementation, the server  may provide feedback to the mobile platform  as to the task and what parts of a task can be off-loaded to the server . Such feedback may be based on the capabilities of the server , the type of operations to be performed, the available bandwidth in the communication channel, power levels of the mobile platform  and\/or the server , etc. For example, the server  may recommend that the mobile platform  send a lower quality version of the image if the network connection is bad and the data rates are low. The server  may also suggest that the mobile platform perform more processing on the data and send processed data to the server  if the data rates are low. For instance, the mobile platform  may compute features for object detection and send the features instead of sending the entire image if the communication link has low data rate. The server  may alternatively recommend that the mobile platform  send a higher quality version of the image or send images more frequently (thereby reducing minimum frame gap TI) if the network connection is good or if the past attempts to recognize an object in the image have failed.","Moreover, the mobile-server architecture introduced herein can also be extended to scenarios where more than one mobile platform  is used. For example, two mobile platforms  may be viewing the same 3D object from different angles and the server  may perform a joint bundle adjustment from the data obtained from both mobile platforms  to create a good 3D model of the object. Such an application may be useful for applications such as multi-player gaming or the like.",{"@attributes":{"id":"p-0079","num":"0078"},"figref":"FIG. 15","b":["110","110","112","150","114","112","150","152","110","152","114","150","154","156"]},"Mobile platform  may include a wireless transceiver , which may be used to communicate with the external server  (), as discussed above. The mobile platform  may optionally include additional features that may be helpful for AR applications, such as motion sensors  including, e.g., accelerometers, gyroscopes, electronic compass, or other similar motion sensing elements, and a satellite positioning system (SPS) receiver  capable of receiving positioning signals from an SPS system. Of course, mobile platform  may include other elements unrelated to the present disclosure.","The mobile platform  also includes a control unit  that is connected to and communicates with the camera  and wireless transceiver , along with other features, such as the user interface , motion sensors  and SPS receiver  if used. The control unit  accepts and processes data from the camera  and controls the communication with the external server through the wireless transceiver  in response, as discussed above. The control unit  may be provided by a processor  and associated memory , which may include software  executed by the processor  to perform the methods or parts of the methods described herein. The control unit  may additionally or alternatively include hardware  and\/or firmware .","The control unit  includes the scene change detector  which triggers communication with the external server based as discussed above. Additional components, such as the trigger time manager  and image quality estimator , illustrated in , may be also included. The control unit  further includes the reference free tracker , reference based tracker  and detection unit , which is used to detect objects in a current image based on objects stored in local cache, e.g., in memory . The control unit  further includes the augmented reality (AR) unit  to generate and display AR information on the display . The scene change detector , reference free tracker , reference based tracker  detection unit , and AR unit  are illustrated separately and separate from processor  for clarity, but may be a single unit and\/or implemented in the processor  based on instructions in the software  which is read by and executed in the processor . It will be understood as used herein that the processor , as well as one or more of the scene change detector , reference free tracker , reference based tracker  detection unit , and AR unit  can, but need not necessarily include, one or more microprocessors, embedded processors, controllers, application specific integrated circuits (ASICs), digital signal processors (DSPs), and the like. The term processor is intended to describe the functions implemented by the system rather than specific hardware. Moreover, as used herein the term \u201cmemory\u201d refers to any type of computer storage medium, including long term, short term, or other memory associated with the mobile platform, and is not to be limited to any particular type of memory or number of memories, or type of media upon which memory is stored.","The methodologies described herein may be implemented by various means depending upon the application. For example, these methodologies may be implemented in hardware , firmware, software , or any combination thereof. For a hardware implementation, the processing units may be implemented within one or more application specific integrated circuits (ASICs), digital signal processors (DSPs), digital signal processing devices (DSPDs), programmable logic devices (PLDs), field programmable gate arrays (FPGAs), processors, controllers, micro-controllers, microprocessors, electronic devices, other electronic units designed to perform the functions described herein, or a combination thereof. Thus, the device to acquire sensor data may comprise camera , the SPS receiver , and motion sensors , as well as the processor which may produce side information, such as text recognition or bar code reading, based on the image produced by the camera  for acquiring sensor data. The device to determine whether there is a trigger event comprising a change in the sensor data relative to previously acquired sensor data comprises the detection unit , which may be implemented by processor  performing instructions embodied in software , or in hardware  or firmware , for determining whether there is a trigger event comprising a change in the sensor data relative to previously acquired sensor data. The device to transmit the sensor data to a server when there is the trigger event comprises wireless transceiver  for transmitting the sensor data to a server when there is the trigger event. The device to receive information related to the sensor data from the server comprises the wireless transceiver  for receiving information related to the sensor data from the server. The device to obtain a pose of the mobile platform with respect to the object comprises the reference free tracker , the wireless transceiver , for obtaining a pose of the mobile platform with respect to the object. The device to track the object using the pose and the reference image of the object comprises the reference based tracker  for tracking the object using the pose and the reference image of the object. The device to determine whether there is a scene change in the captured image with respect to a previous captured image comprises the scene change detector , which may be implemented by processor  performing instructions embodied in software , or in hardware  or firmware , for determining whether there is a scene change in the captured image with respect to a previous captured image.","For a firmware and\/or software implementation, the methodologies may be implemented with modules (e.g., procedures, functions, and so on) that perform the functions described herein. Any machine-readable medium tangibly embodying instructions may be used in implementing the methodologies described herein. For example, software  may include program codes stored in memory  and executed by the processor . Memory may be implemented within or external to the processor .","If implemented in firmware and\/or software, the functions may be stored as one or more instructions or code on a computer-readable medium. Examples include non-transitory computer-readable media encoded with a data structure and computer-readable media encoded with a computer program. Computer-readable media includes physical computer storage media. A storage medium may be any available medium that can be accessed by a computer. By way of example, and not limitation, such computer-readable media can comprise RAM, ROM, Flash Memory, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer; disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.","Although the present invention is illustrated in connection with specific embodiments for instructional purposes, the present invention is not limited thereto. Various adaptations and modifications may be made without departing from the scope of the invention. Therefore, the spirit and scope of the appended claims should not be limited to the foregoing description."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWING","p":[{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIGS. 8 and 9"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIGS. 10 and 11"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIGS. 12 and 13"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 15"}]},"DETDESC":[{},{}]}
