---
title: System and method for determining line-of-sight volume for a specified point
abstract: A system and method determines a Line-of-Sight volume for a specified point within a three dimensional model. Each data file includes data about geometric entities that make-up a geographic feature that may impact a Line-of-Sight from a focus. A processor is operative for processing the data files and creating a series of polygons that represent a shell of a Line-of-Sight volume to the focus at a specified resolution based on the process data files.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07098915&OS=07098915&RS=07098915
owner: Harris Corporation
number: 07098915
owner_city: Melbourne
owner_country: US
publication_date: 20040927
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["The present invention relates to the field of imaging, and more particularly, the present invention relates to a system and method for determining a Line-of-Sight (LOS) volume for a specified point in a three-dimensional model.","When working within a three-dimensional model for applications such as personnel or camera placements, or for example, determining a Line-of-Sight for target acquisition or laser sighting in weapons delivery, a planner must verify that there is a clear Line-of-Sight from a specific position or target to any other point within the area of operation. This information is important to determine flight parameters for aircraft in target acquisition or planning or determine blind spots in ground operations for camera or personnel placement, for example.","Traditional two-dimensional maps cannot provide this information because the heights of relative objects is an unknown quantity. Additionally, placing a camera within a three-dimensional scene at a selected point does not provide this information because a camera cannot always be accurately placed and camera imaging does not provide intuitive confirmation across large distances.","A true, three-dimensional (3D) virtual environment requires the generation of an accurate Line-of-Sight (LOS) volume based upon a specific point referred to as the focus. By definition, a Line-of-Sight volume, when integrated into a three-dimensional environment, will give a user the ability to determine instantly a clear Line-of-Sight to the designated focus by observing the camera placement inside or outside of the rendered Line-of-Sight volume. A rendering of a Line-of-Sight volume will also allow its use outside of traditional data visualization where three-dimensional data analysis is performed.","Currently, there are no known applications that produce true three-dimensional Line-of-Sight volumes. U.S. Pat. No. 6,411,298 to Goto et al., the disclosure which is hereby incorporated by reference in its entirety, discloses a method and apparatus that determines visual points and direction of Line-of-Sight in three-dimensional image construction. The system and apparatus of Goto et al. primarily determines a visual point and direction for a Line-of-Sight in a three-dimensional image construction used for imaging blood vessels and similar pathways. A viewpoint is set on an intersection of two arbitrary multi-planar reconstruction (MPR) images. A Line-of-Sight is displayed on a plane, including either one of the two MPR images. The geometric positional relationship of the two MPR images, their intersection and viewpoint, and Line-of-Sight, are designated with a positional input device, for example, a computer mouse, to enable rearrangement of the geometric positional relationship. By outputting an instruction to move the position and direction of a marker on the plane, including a first sectional image, and updating the viewpoint and Line-of-Sight, it is possible to set an estimated viewpoint and Line-of-Sight. Goto et al. is limited, however, in its applicability for generating an accurate Line-of-Sight volume based upon a specific point as a focus in a three-dimensional environment.","In another application for generating a Line-of-Sight, ESRI Company of Redlands, Calif., has developed a software program that produces multi-colored lines radiating out from ground based points along a terrain surface. These multi-colored lines indicate those areas that have Line-of-Sight to a point, but only along a surface. This program, however, does not have sufficient resolution for some end-use applications. This program is perhaps advantageous from a point out across the scene, but it is not sufficiently adequate for scenarios that determine Line-of-Sight capabilities for antenna placement, blind spots across a radar sweep, or similar applications requiring a Line-of-Sight volume for a specified point in a three-dimensional model.","It is therefore an object of the present invention to determine Line-of-Sight capabilities for producing three-dimensional Line-of-Sight volumes that overcome the drawbacks indicated above.","In accordance with the present invention, a point is designated in three-dimensional space and the","initial shape of a volume to be displayed is defined. These volumes could be, for example, a full sphere, an upper hemisphere or lower hemisphere. The initial shape is defined and the resolution at which the volume is to be displayed is defined, for example, at 2\u00b0, 5\u00b0 or 10\u00b0 increments. The radius of the volume is defined and is calculated from a specified point.","The system incorporates appropriate algorithms and calculates the volume. It automatically loads the data into a program once the calculations are complete. Thus, a situation planner can have immediate information as to what locations in three-dimensional space have Line-of-Sight to a specific location within a three-dimensional model of an area of interest. It is possible for a user, for example, a situation planner, to move to any point in a scene and determine the Line-of-Sight to that point.","In accordance with the present invention, a system and method determines a Line-of-Sight volume for a specified point within a three dimensional model. A focus point is defined in three dimensional space. A plurality of data files each correspond to a geographic feature that may impact a Line-of-Sight from the focus. Each file includes data about geometric entities that make-up a geometric feature. A processor is operative for processing the data files and creating a series of polygons that represent a shell of a Line-of-Sight volume to the focus at a specified resolution based on the process data files.","In one aspect of the present invention, the processor is operative for processing each data file as a minimally-sized bounding volume that contains the geometric entities that make-up a geographic feature and determining if a ray cast from the focus to the closest point of the bounding volume intersects the initial shape of the volume within the specified distance. The geographic feature can comprise a building or other geographic feature that could intersect the Line-of-Sight from the focus.","In another aspect of the present invention, a single output file is output as a series of polygons that represent the shell of the Line-of-Sight volume. The processor is also operative for computing the azimuth and elevation range of a polygon in determining the distance from the focus to a point line within the polygon. Each data file could be formed as a collection of polygons with each polygon's vertices contained within a common plane. A representative example of a polygon could be a triangle.","The processor is also operative for changing the resolution to enhance rendering speed for calculating the shell of the Line-of-Sight volume. Different surfaces of the shell could be defined, including a general spline based or NURBS (non-uniform rational B-splines) surface.","In yet another aspect of the present invention, the focus point could be defined in three dimensional space as part of an initial shape for a volume defined as if there were no obstructions from the focus based on a specified distance. This initial shape could be a full hemisphere shape, one of an upper or lower hemisphere, a horizontal cross-section of a sphere having user defined upper and lower vertical distances either relative to the focus or if geospatially referenced in absolute altitudes, longitudinal hemispheres based on either selected directions or an amount of obscuration, or conical shaped from one selected point towards the focus with a selected field of view angle.","The present invention will now be described more fully hereinafter with reference to the accompanying drawings, in which preferred embodiments of the invention are shown. This invention may, however, be embodied in many different forms and should not be construed as limited to the embodiments set forth herein. Rather, these embodiments are provided so that this disclosure will be thorough and complete, and will fully convey the scope of the invention to those skilled in the art. Like numbers refer to like elements throughout, and prime notation is used to indicate similar elements in alternative embodiments.","The present invention advantageously provides a system and method that determines a Line-of-Sight volume for a specified point within a three-dimensional model. It overcomes the disadvantages of working within a three-dimensional model in which the placement of personnel or cameras or weapons delivery cannot be verified for a clear Line-of-Sight from a specific position or target to any other point within the area of operation. It overcomes the disadvantages of using traditional two-dimensional maps that cannot provide this information because the heights of all relative objects is an unknown.",{"@attributes":{"id":"p-0042","num":"0041"},"figref":["FIG. 1","FIGS. 5\u201312"],"b":["20","21","22","24","30","32"]},"As understood by those skilled in the art, ray tracing simulates the physics of a photograph in a three-dimensional environment, for example, by tracing a path taken by a ray of light through a scene and calculating reflection, refraction, or absorption of the ray when it intersects an object. Ray tracing can encompass forward or backward ray tracing, and encompass the formation of ray trees and ray-object intersections by computing the intersection of an object and the ray using a point-sampling technique. Of course, one essential aspect in any ray tracing algorithm is the repeated intersection of rays with objects. The present invention could use specialized system hardware to accelerate mathematical operations, for example, by using a chip that computes intersections of rays with a class of curved surfaces. Circuit structures or enhanced software or a combination ob both can be used to accelerate ray-object intersections as hybrid hardware\/software solutions.","Bounding volume hierarchies can also be used in accordance with the present invention. The bounding volume, for example, could be a shape that completely encloses one or more smaller shapes. It usually takes the form of a simplified box or sphere to simplify initial calculations, instead of initially processing extensive data for complicated objects. When a ray does not intersect a bounding volume, it does not intersect any of the objects within the bounding volume and those objects do not have to be examined. Thus, objects in a scene can be arranged in a hierarchy of bounding volumes. A ray can thus be traced by intersecting it against each of the bounding volumes. Only those objects that are intersected require analysis. The present invention, however, is not limited to the use of bounding volumes. Space subdivision can also be used in which three-dimensional space is subdivided into smaller regions, for example rectangular boxes. Directional data structures could also be used.","The coordinate system used for the present invention is described as a right-handed xyz coordinate system, with the xy plane being the horizontal reference plane. The y-axis preferably represents north and the x-axis represents east. The z-axis represents \u201cup,\u201d i.e., vertical relative to the xy plane. The origin (0,0,0) is an arbitrarily selected point within the three-dimensional environment. This origin should not extend far outside the bounds of a model because earth curvature can cause distortion if the model is geospatially accurate and modeled to properly lie on the Earth's ellipsoid. In this non-limiting description, all azimuths are measured in a clockwise direction from the north (y) axis, and all elevations are measured relative to the horizontal (xy) plane (+90\u00b0 to \u221290\u00b0).","While there are many methods that might be used to optimize processing of the geometry and improving the algorithms described below as non-limiting examples, all algorithms, which could be used with the present invention would follow a similar processing path. In this description for the described example, the geometry represents an urban model. Each building is its own separate file of geometric entities. The underlying terrain is broken into a series of non-overlapping grids and each represented within a single file. Other methods of geometric data could also be used without extensively modifying the data processing. Additionally, each file of geometric entities could also include the coordinates of a minimally sized xyz axis oriented cube, i.e., a bounding cube, which will contain all the geometry within the file. The geometry contained within each file could be a collection of polygons, with each polygon's vertices in a common plane. Other geometric surfaces could also be used by the algorithms and will only impact the intersection calculations.","While geometry will normally dictate the actual shape of the Line-of-Sight volume, the user can define the configuration (shape) that the volume would have if there were no obstructions anywhere within the scene. While there are any number of initial shapes that might be made available to the user, the choices presented in the example flow charts are: (a) a full sphere with the focus at the center of the sphere; (b) an upper hemisphere with the focus at the center of the base equator rim; (c) a lower hemisphere with the focus at the center of the top equator rim; and (d) a horizontal cross section of a sphere where the user defines the upper and lower vertical distances either relative to the focus, or if geospatially referenced, in absolute altitudes.","Additional shapes for consideration depending on the specific application might include: (a) longitudinal hemispheres based on either selected directions or amount of obscuration; (b) conical shapes from one selected point towards the focus with a selected field of view angle; and (c) other shapes as suggested by those skilled in the art.","The examples as described could calculate the Line-of-Sight volume in one degree (1\u00b0) increments. This generated volume, however, could cause unacceptable rendering delays or be more than the operator requires. Therefore, the following description presents the final Line-of-Sight volume that is generated in 2\u00b0, 5\u00b0 or 10\u00b0 segments as its resolution. It is also possible to change the resolution with the Line-of-Sight volume. The resolution can be more fine or more coarse. Additionally, the output resolutions can be easily adjusted to a resolution that better suits the application to which the Line-of-Sight volume is applied.","In this description, the operating radius (distance) from the focus can represent the maximum distance out to which the Line-of-Sight volume will extend when unobstructed.","A single file can be output in accordance with the present invention. This output file can contain a series of triangles that represent the shell of the Line-of-Sight volume to the focus, which is calculated against any supplied files, at the specified resolution, and out to the specified distance. The output can be three-dimensional polygons, for example, triangles as described. Different surfaces, however, can be defined, for example, general spline based, NURBS (Nonuniform Rational B-Splines) and similar surfaces with the processed Line-of-Sight volume information. The output file can be of any file format capable of defining selected three-dimensional geometric shapes. Once the Line-of-Sight volume has been calculated, different output files can be created. Formats vary greatly and any format that is capable of adequately containing the selected 3D geometric shapes would be sufficient as an output file.",{"@attributes":{"id":"p-0052","num":"0051"},"figref":["FIG. 2","FIG. 3","FIG. 2","FIG. 4","FIGS. 2 and 3"],"b":["40","42","44","50","52"]},"Referring now to , a series of high level flow charts are set forth, which illustrate examples of different steps used for determining a Line-of-Sight volume in accordance with the present invention. As shown at block  on , a data structure as a spoke is created to store a single real value for each azimuth (0\u00b0 to 359\u00b0). This data structure would contain a full set of elevation readings, for example, +90\u00b0 to \u221290\u00b0. There could be [65, 160 entries] or [0 . . . 359, +90\u00b0 to \u221290\u00b0]. For all the determined spokes, a distance is created (block ) in which the spoke [I]=the distance. and are a table and graph showing details of the data structure and spoke as an example. The azimuth\/elevation data structure in the table of are data representative of each ray distance for d=50. Its physical representation is shown in the graph of with an initial Line-of-Sight sphere about a point \u201cp\u201d.","After determining a distance for the spokes, a determination is made whether there are any remaining files in the list of three-dimensional files for the environment (block ). If there are no remaining files, the algorithm proceeds to build its volume (block ) using the algorithm shown in the flow chart of . If there are remaining files, for the next file in the environment, a bounding cube is obtained for the geometric entities (block ). Geometry files (block ) can be input to aid in establishing the bounding cube. This geometry data can be determined from a database, for example as part of an image modeling software, as will be explained below.","A determination is made whether a ray is cast from the focus to the closest point of the bounding cube and whether it intersects the bounding cube within the distance (block ). If not, then the loop back occurs and a determination is made whether there are remaining files in the list of three-dimensional files (block ). If yes, the algorithm continues with the process shown in  (block ). User supplied or other data relating to the focus, shape, resolution, and distance (block ) can be input to aid in determining whether a ray intersects the cube within the distance. Once the volume is built an output file is written (block ) and the system returns (block ).",{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 14","b":["130","132"]},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 6","b":["140","142","144","146","148"]},"At this time a determination is made whether the focus (,) is within the two-dimensional polygon (block ). If not, then the two vertices with the largest angular separation and azimuth are determined (block ). Any vertices directly above or below the focus can be disregarded. A determination is made which of the vertices is a \u201cstart\u201d vertex based on a clockwise sweep while also containing all other vertices in the polygon based on the horizontal plane (block ). The azimuth start and end vertex and spread are determined relative to the azimuth minimum, maximum and sweep (block ) such that az=az; az=az; and az=spread. If the azimuth minimum is greater than the azimuth maximum (block ), then the azimuth maximum equals the azimuth maximum plus 360 (block ). In any event the next process of the algorithm shown in  is initiated (block ). If the focus is within the two-dimensional polygon (block ), the azimuth minimum equals zero, the azimuth maximum equals 359, and the azimuth sweep equals 360 (block ), and the process shown in  is initiated (block ).","As shown in , the minimum elevation and maximum elevation of all the vertices in the polygon and the sweep elevation between the minimum and maximum is determined. The plane of the polygon is defined in a common format, such as, Ax+By+Cz+D=0 (block ). A determination is made which for a=minimum azimuth, through the azimuth maximum, the system proceeds one step. The system determine that a=a and may wrap over 360\u00b0 (block ). The system can return (block ). If ais greater than 359 (block ), then ais equal to a+360 (block ). If not, then the minimum elevation through the maximum elevation is stepped plus one (block ). A unit direction vector (dir) is formed using the current azimuth (a) and the elevation (e) (block ). Using a planar equation and direction vector from the focus for a common point P, the equation AP+BP+CP+D=0 applies (block ). \u201cP\u201d equals the focus plus the direction times the t. T can be solved for P. A determination is made whether t is greater than or equal to zero (block ). If yes, a determination is made whether P lies within a polygon (block ). If yes, the distance (d) is calculated from the focus to P (block ). If P does not lie within a polygon, the system loops back. If t is not greater than or equal to zero (Block ), the loop returns. If after calculating the distance (d) from the focus to P (block ) a determination is made whether the distance is less than the spoke for the current azimuth and elevation [a,e] (block ). If yes, the spoke [a,e] is equal to the distance (block ). If not, the loop returns.",{"@attributes":{"id":"p-0060","num":"0059"},"figref":["FIG. 8","FIG. 10","FIG. 11"],"b":["106","200","202","204","206","208","210","212","214"]},"If only upper or lower hemispheres are to be selected as an output shape, the horizontal equatorial base\/cap is created using the 360 azimuths along the zero elevation (spoke [i,a]) in to the focus (block ). Steps are based on resolution. Once this is accomplished (and also for a full sphere), certain steps can be made by resolution as shown in blocks  through . For e=upper elevation, while if less than the lower elevation, the system steps by resolution (block ). The next elevations can equal the elevation minus the resolution (block ). For azimuth equaling 0 through 359, the system steps by resolution (block ). The next azimuth is calculated based on the azimuth plus resolution (block ). Loops can be completed as indicated at various stages. Minimum spoke values are located as shown at block  and direction vectors formed and points generated in three-dimensional space as shown in block . A surface is built (block ). In other stages as indicated, the system returns (block ).","At this time a surface can be built as shown by the algorithm of  (block ). If the elevation equals 90 (block ), triangles are created (block ) and added to an output file (block ) and the system returns (block ). If elevation does not equal 90, a determination is made whether the next elevation is \u221290 (block ) and, if yes, a triangle is created with the indicated parameters (block ) and the triangle added to the output file (block ). If it is not \u221290, the triangle is created with the indicated parameters (block ) and the triangle added to the output file (block ) and another triangle created as indicated (block ).",{"@attributes":{"id":"p-0063","num":"0062"},"figref":["FIG. 10","FIG. 12"],"b":["202","260","262","264","266"]},{"@attributes":{"id":"p-0064","num":"0063"},"figref":["FIGS. 17 and 18","FIG. 17","FIG. 18"]},"The different steps for building the slice are shown after a determination is made that the next elevation is within the elevation resolution (block ). For a=0, through 359, steps occur by resolution (block ). The next azimuth is the azimuth and the resolution plus 360 degrees (block ). The minimum spoke values within a certain range are located (block ) and the direction vectors formed (block ) using the indicated parameters shown in both blocks. If the elevation equals 90 (block ) the triangle is created with the indicated parameters (block ) and the sliver evaluated (block ). If the elevation does not equal 90, a determination is made whether the next elevation is \u221290 (block ) and, if yes, a triangle is created using the indicated parameters (block ) and the sliver evaluated (block ). If the next elevation is not less than 90, a triangle created with the parameters as indicated (block ) and the sliver evaluated (block ). A triangle is created with the indicated parameters (block ) and then the sliver evaluated (block ).",{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 11","sub":["0","1","2"],"b":["300","302","304","306","308","310","312","314","316"]},{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 12","b":["264","320","322","324","326","328","330","332","334"],"sub":["0","0 ","f ","0","n","n","f ","n "]},{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 16"},"In one aspect of the present invention, a feature extraction program and geographic image database, such as the RealSite\u2122 image modeling software developed by Harris Corporation of Melbourne, Fla., can be used for determining geometry files of . This program can be operative with the InReality\u2122 viewer software program also developed by Harris Corporation of Melbourne, Fla. Using this viewer with the RealSite\u2122 image modeling software, it is possible for a user to designate a point in three-dimensional space and find the initial shape of the volume to be displayed, for example a full sphere, upper hemisphere or lower hemisphere and define the resolution at which the volume is to be displayed, for example, 2\u00b0, 5\u00b0or 10\u00b0 increments. It is also possible by the present invention to define the radius of the volume to be calculated from the specified point. The InRealty\u2122 viewer system can generate a process used for calculating the volume and automatically load it into the InRealty\u2122 viewer once the calculations are complete. The Line-of-Sight volume can be calculated by applying the intersection calculations and volume creation algorithms of the present invention from a user selected point with display parameters and scene geometry as developed by RealSite\u2122 and InRealty\u2122, as one non-limiting example. This solution would provide a situation planner immediate information as to what locations in a three-dimensional space have a Line-of-Sight to a specific location within a three-dimensional model of an area of interest. Thus, it would be possible for a user to move to any point in the scene and determine the Line-of-Sight to the point. For example, by selecting the corner of a building, a police SWAT team can immediately know the Line-of-Sight that a sharpshooter, at that location, has throughout a city. The speed of calculations is dependent upon the amount of geometry in the model, but the volumes are normally computed in under a minute. By using the InReality\u2122 viewer program, the system goes beyond providing basic mensuration and displaying capabilities.","As noted before, the present invention uses modified ray tracing for three-dimensional computer graphic generation and rendering an image. For purposes of description, the location, i.e., the latitude and longitude of any object that would effect the Line-of-Sight can be located and determined via a look-up table of feature extraction from the geographic image database associated with RealSite\u2122 program as described above. This geographic database could include data relating to the natural and man-made features in a specific area, including data about buildings and natural land formations, such as hills, which all would effect the Line-of-Sight calculations.","For example, a database could include information about a specific area, such as a tall building or water tower. A look-up table could have similar data and a system processor would interrogate and determine from the look-up table the type of buildings or natural features to determine the geometric features.","For purposes of illustration, a brief description of an example of a feature extraction program that could be used with the present invention, such as the described RealSite\u2122, is now set forth. Naturally, many different types of feature extraction software are available to one skilled in the art, and can be used with the present invention. The database could also be used with two-dimensional or three-dimensional feature imaging. Optical reflectivity can be used for finding building plane surfaces and building edges.","Further details of a texture mapping system used for creating three-dimensional urban models is disclosed in commonly assigned U.S. Pat. No. 6,744,442, the disclosure which is hereby incorporated by reference in its entirety. For purposes of description, a high level review of feature extraction using RealSite\u2122 is first set forth. This type of feature extraction software can be used to validate results and find the natural and man-made objects for Line-of-Sight calculations and can be used in two-dimensional and three-dimensional modes.","RealSite\u2122 allows the creation of three-dimensional models in texture mapping systems and extends the technology used for terrain texturing to building texture by applying clip mapping technology to urban scenes. It can be used to determine optical reflectivity values and even radio frequency reflectivity.","It is possible to construct a single image of a building from many images that are required to paint all the sites. Building site images can fit into a composite image of minimum dimension, including rotations and intelligent arrangements. Any associated building vertex texture coordinates can be scaled and translated to match new composite images. The building images can be arranged in a large \u201cclip map\u201d image, preserving the horizontal relationships of the buildings. If the horizontal relationships cannot be accurately preserved, a \u201cclip grid\u201d middle layer can be constructed, which can be used by the display software to accurately determine the clip map center.","At its highest level, the system creates a packed rectangle of textures for each of a plurality of three-dimensional objects corresponding to buildings to be modeled for a geographic site. The system spatially arranges the packed rectangle of textures in a correct position within a site model clip map image. The texture mapping system can be used with a computer graphics program run on a host or client computer having an OpenGL application programming interface. The location of a clip center with respect to a particular x,y location for the site model clip map image can be determined by looking up values within a look-up table, which can be built by interrogating the vertices of all building polygon faces for corresponding texture coordinates. Each texture coordinate can be inserted into the look-up table based on the corresponding polygon face vertex coordinate.","In these types of systems, the graphics hardware architecture could be hidden by a graphics API (Application Programming Interface). Although different programming interfaces could be used, a preferred application programming interface is an industry standard API such as OpenGL, which provides a common interface to graphics functionality on a variety of hardware platforms. It also provides a uniform interface to the texture mapping capability supported by the system architecture.","OpenGL allows a texture map to be represented as a rectangular pixel array with power-of-two dimensions, i.e., 2\u00d72. To increase rendering speed, some graphics accelerators use pre-computed reduced resolution versions of the texture map to speed up the interpolation between sampled pixels. The reduced resolution image pyramid layers are referred to as MIPmaps by those skilled in the art. MIPmaps increase the amount of storage each texture occupies by 33%.","OpenGL can automatically compute the MIPmaps for a texture, or they can be supplied by the application. When a textured polygon is rendered, OpenGL loads the texture and its MIPmap pyramid into the texture cache. This can be very inefficient if the polygon has a large texture, but happens to be far away in the current view such that it only occupies a few pixels on the screen. This is especially applicable when there are many such polygons.","Further details of OpenGL programming are found in Neider, Davis and Woo, OpenGL Programming Guide, Addison-Wesley, Reading, Mass., 1993, Chapter 9, the Guide disclosure which is hereby incorporated by reference in its entirety.","Clip texturing can also be used, which improves rendering performance by reducing the demands on any limited texture cache. Clip texturing can avoid the size limitations that limit normal MIPmaps by clipping the size of each level of a MIPmap texture to a fixed area clip region.","Further details for programming and using clip texturing can be found in Silicon Graphics, IRIS Performer Programmer's Guide, Silicon Graphics, Chapter 10: Clip Textures, the Programmer's Guide, which is hereby incorporated by reference in its entirety.","IRIS Performer is a three-dimensional graphics and visual simulation application programming interface that lies on top of OpenGL. It provides support for clip texturing that explicitly manipulates the underlying OpenGL texture mapping mechanism to achieve optimization. It also takes advantage of special hardware extensions on some platforms. Typically, the extensions are accessible through OpenGL as platform specific (non-portable) features.","In particular, IRIS Performer allows an application to specify the size of the clip region, and move the clip region center. IRIS Performer also efficiently manages any multi-level paging of texture data from slower secondary storage to system RAM to the texture cache as the application adjusts the clip center.","Preparing a clip texture for a terrain surface (DEM) and applying it can be a straightforward software routine in texture mapping applications, as known to those skilled in the art. An image or an image mosaic is orthorectified and projected onto the terrain elevation surface. This single, potentially very large, texture is contiguous and maps monotonically onto the elevation surface with a simple vertical projection.","Clip texturing an urban model, however, is less straightforward of a software application. Orthorectified imagery does not always map onto vertical building faces properly. There is no projection direction that will map all the building faces. The building textures comprise a set of non-contiguous images that cannot easily be combined into a monotonic contiguous mosaic. This problem is especially apparent in an urban model having a number of three-dimensional objects, typically representing buildings and similar vertical structures. It has been found that it is not necessary to combine contiguous images into a monotonic contiguous mosaic. It has been found that sufficient results are achieved by arranging the individual face textures so that spatial locality is maintained.",{"@attributes":{"id":"p-0087","num":"0086"},"figref":"FIG. 19","b":["1000","1020","1040"]},"Referring now to , a more detailed flow chart sets forth the sequence of steps. A composite building texture map (CBTM) is created (block ). Because of tiling strategies used later in a site model clip mapping process, all images that are used to texture one building are collected from different viewpoints and are packed into a single rectangular composite building texture map. To help reduce the area of pixels included in the CBTM, individual images (and texture map coordinates) are rotated (block ) to minimize the rectangular area inside the texture map actually supporting textured polygons. After rotation, extra pixels outside the rectangular footprint are cropped off (block ).","Once the individual images are pre-processed, image sizes for each contributing image are loaded into memory (block ). These dimensions are sorted by area and image length (block ). A new image size having the smallest area, with the smallest perimeter, is calculated, which will contain all the building's individual textures (block ). The individual building textures are efficiently packed into the new image by tiling them alternately from left to right and vice versa, such that the unused space in the square is minimized (block ).",{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 21"},"A site model clip map image is next created. Because each composite building texture map (CBTM) is as small as possible, placing each one spatially correct in a large clip map is realizable. Initially, each composite building texture map is placed in its correct spatial position in a large site model clip map (block ). A scale parameter is used to initially space buildings at further distances from each other while maintaining relative spatial relations (block ). Then each composite building texture map is checked for overlap against the other composite building texture maps in the site model clip map (block ). The site model clip map is expanded from top right to bottom left until no overlap remains (block ). For models with tall buildings, a larger positive scale parameter may be used to allow for the increased likelihood of overlap. All texture map coordinates are scaled and translated to their new positions in the site model clip map image.","Referring now to , a flow chart illustrates the basic operation that can be used to process and display building clip textures correctly. A clip map clip grid look-up table is used to overcome these limitations and pinpoint the exact location of where the clip center optimally should be located with respect to a particular x,y location. To build the table, the vertices of all the building polygon faces are interrogated for their corresponding texture coordinates (block ). Each texture coordinate is inserted into a look-up table based on its corresponding polygon face vertex coordinates (block ).","A clip center or point in the clip map is used to define the location of the highest resolution imagery within the clip map (block ). Determining this center for a terrain surface clip map is actually achievable with little system complexity because a single clip texture maps contiguously onto the terrain elevation surface, so the camera coordinates are appropriate. The site model clip map has a clip center of its own and is processed according to its relative size and position on the terrain surface (block ). The site model clip map, however, does introduce some locality limitations resulting from tall buildings or closely organized buildings. This necessitates the use of an additional look-up table to compensate for the site model clip map's lack of complete spatial coherence. The purpose of the clip grid is to map three-dimensional spatial coordinates to clip center locations in the spatially incoherent clip map.","The clip grid look-up table indices are calculated using a x,y scene location (the camera position) (block ). If the terrain clip map and site model clip map are different sizes, a scale factor is introduced to normalize x,y scene location for the site model clip map (block ). It has been found that with sufficient design and advances in the development of the spatial correctness of the building clip map, the need for the clip grid look-up table can be eliminated in up to 95% of the cases.","It is also possible to extend the algorithm and use multiple site model clip maps. Using many smaller clip maps rather than one large clip map may prove to be a useful approach if clip maps of various resolutions are desired or if the paging in and out of clip maps from process space is achievable. However, it requires the maintenance of multiple clip centers and the overhead of multiple clip map pyramids.","The RealSite\u2122 image modeling software has advantages over traditional methods because models can be very large (many km) and can be created in days versus weeks and months of other programs. Features can be geodetically preserved and can include annotations and be geospatially accurate, for example, one meter or two meter relative. Textures can be accurate and photorealistic and chosen from the best available source imagery and are not generic or repeating textures. The InReality\u2122 program can provide mensuration where a user can interactively measure between any two points and obtain an instant read-out on the screen of a current distance and location. It is possible to find the height of a building, the distance of a stretch of highway, or the distance between two rooftops along with Line-of-Sight information in accordance with the present invention. There are built-in intuitive navigation controls with motion model cameras that \u201cfly\u201d to a desired point of view. The InReality\u2122 viewer can be supported under two main platforms and operating systems: (1) the SGI Onyx2 Infinite Reality2\u2122 visualization supercomputer running IRIX 6.5.7 or later and an X86-based PC running either Microsoft WindowsNT 4.0 or Windows 98 or more advanced systems. The IRIX version of the InReality\u2122 viewer can take full advantage of high-end graphics capabilities provided by Onyx2 such as MIPMapping in the form of clip textures, multi-processor multi-threading, and semi-immersive stereo visualization that could use crystallized by stereographics. InReality\u2122 for Windows allows great flexibility and scalability and can be run on different systems.","Crystal Eyes produced by Stereo Graphics Corporation can be used for stereo 3D visualization. Crystal Eyes is an industry standard for engineers and scientists who can develop, view and manipulate 3D computer graphic models. It includes liquid crystal shutter eyewear for stereo 3D imaging.","Many modifications and other embodiments of the invention will come to the mind of one skilled in the art having the benefit of the teachings presented in the foregoing descriptions and the associated drawings. Therefore, it is understood that the invention is not to be limited to the specific embodiments disclosed, and that modifications and embodiments are intended to be included within the scope of the appended claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Other objects, features and advantages of the present invention will become apparent from the detailed description of the invention which follows, when considered in light of the accompanying drawings in which:",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 3","FIG. 1"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 9","FIG. 8"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 13","i":"a "},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 13","i":"b "},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 15","FIG. 6"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIGS. 19 and 20"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":["FIG. 22","FIGS. 19 and 20"]}]},"DETDESC":[{},{}]}
