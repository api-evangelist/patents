---
title: Method and system for enhanced granularity in fencing operations
abstract: A key set is registered. The registering the key set includes registering a first shared data resource key. The first shared data resource key includes a first identifier associating a first process with a first shared data resource. The registering the key set further includes registering a second shared data resource key, and the second shared data resource key includes a second identifier associating a second process with a second shared data resource. A failure of a first process is detected, and in response to the detecting the failure of the first process, the first shared data resource key is de-registered. The second shared data resource key remains registered after the de-registering the first shared data resource key.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08707082&OS=08707082&RS=08707082
owner: Symantec Corporation
number: 08707082
owner_city: Mountain View
owner_country: US
publication_date: 20091029
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Distributed computing systems are an increasingly important part of research, governmental, and enterprise computing systems. Among the advantages of such computing systems are their ability to handle a variety of different computing scenarios including large computational problems, high volume data processing situations, and high availability situations. For applications that require the computer system to be highly available, e.g., the ability to maintain the system while still providing services to system users, a cluster of computer systems is a useful implementation of the distributed computing model. In the most general sense, a cluster is a distributed computer system that works together as a single entity to cooperatively provide processing power and mass storage resources. With a cluster, the processing load of the computer system is typically spread over more than one computer, thereby eliminating single points of failure. Consequently, programs executing on the cluster can continue to function despite a problem with one computer in the cluster. In another example, one or more computers of the cluster can be ready for use in the event that another computer in the cluster fails. While each computer in a cluster typically executes an independent instance of an operating system, additional clustering software is executed on each computer in the cluster to facilitate communication and desired cluster behavior.","A key set is registered. Registering the key set includes, in one embodiment, registering a first shared data resource key. In this embodiment, the first shared data resource key includes a first identifier associating a first process with a first shared data resource. Registering the key set further includes registering a second shared data resource key, and the second shared data resource key includes a second identifier associating a second process with a second shared data resource. A failure of a first process is detected, and in response to detecting the failure of the first process, the first shared data resource key is de-registered. The second shared data resource key remains registered after the de-registering the first shared data resource key.","In one embodiment, in response to detecting a failure of a node, a shared coordinator resource key associated with a shared coordinator resource is de-registered.","In one embodiment, the shared coordinator resource stores system administrative data, such as a key set. Registering the key set further includes generating in a user space a data structure, which in some embodiments is called a dependency table, including a first mapping between the first process and the first shared data resource, and a second mapping between the second process and the second shared data resource. Registering the key set further includes storing on the shared coordinator resource the first shared data resource key, the second shared data resource key, and the shared coordinator resource key.","In one embodiment, detecting the failure of the node includes detecting a loss of a heartbeat signal. The heartbeat signal is transmitted across a private network connection, and the heartbeat signal is from the node.","In one embodiment, a shared coordinator resource key remains registered after de-registering the first shared data resource key. The first shared data resource key further includes a fourth identifier associating a node with the first shared data resource. The first shared data resource stores first user data. The second shared data resource key further comprises a fifth identifier associating the node with the second shared data resource. The first shared data resource key further includes a sixth identifier associating the first shared data resource and a protocol version applicable to a sixth shared data resource. The second shared data resource stores second user data.","In one embodiment, detecting the failure of the first process includes detecting a process failure indicator associated with the failure of the first process, and the detecting the failure of the first process further includes detecting the heartbeat signal.","In one embodiment, registering the key set further includes registering a shared coordinator resource key. The shared coordinator resource key includes a third identifier associating the node with the shared coordinator resource. The process failure indicator is a failure of the process to respond to a kill process command. In response to the detecting a failure of the node, the first data disk key and the second data disk key are de-registered.","The foregoing is a summary and thus contains, by necessity, simplifications, generalizations and omissions of detail; consequently those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting. Other aspects, inventive features, and advantages of the present invention, as defined solely by the claims, will become apparent in the non-limiting detailed description set forth below.","In a cluster environment, current solutions allow for the isolation of a failed node. When a node is isolated in response to a failure, all of the shared resources (e.g., coordinator disks and data disks) allocated to the node can then be reallocated (by de-registering on the coordinator disk any keys allocating to the node the coordinator disks and data disks) and the node can be restarted. Node isolation assumes (or, alternatively, causes) the failure of all processes executing on the node. Unfortunately, the isolation of an entire node and the cancellation of all processes operating on the node does not reflect or support the reality of nodes executing multiple processes (such as multiple applications or multiple virtual environments). Users of a healthy process executing on a node that is also executing a failed process express significant dissatisfaction at the loss of service resulting from isolation and shutdown of a node that is running a healthy process in addition to a failed process.","For instance, some nodes execute a set of multiple processes, one of which may hang or crash. If one of the processes or virtual environments executing on a particular node hangs or crashes, the only available solution to reset the hung process or virtual environment and reallocate its resources is to isolate the entire node and all of its processes. This results in the loss of healthy processes currently executing on the node, creating unnecessary service interruptions through the loss of the healthy processes.","A mechanism is provided for the detection of failures at the process level, thereby allowing a cluster to selectively isolate, reset and reallocate the resources associated with particular failed processes, rather than an entire node. In one embodiment, the present invention thereby minimizes the likelihood of significant cost or inconvenience resulting from a service interruption a service interruption in a healthy process through the use of a series of methods and structures.","A bifurcated key set is created and registered. A first type of key associates a coordinator disk with a node. A second type of key associates a data disk with a node and a process. Both types of keys are registered with a coordinator disk. Registration is achieved by storing a key on the coordinator disk. A system of bifurcated failure detection is provided. Node failure is detected through, in one embodiment, the loss of heartbeat messages from the failed node on a private network link. Process failure is detected through the receipt of a process failure indicator, which is a triggering signal from one of a series of algorithms for monitoring process health. A system of bifurcated enforcement is provided. Node failure is addressed by de-registering from a coordinator disk all keys associated with a failed node, so that the coordinator disk and all of data disks associated with the failed node can be reallocated. Process failure is addressed by de-registering from a coordinator disk all data disk keys associated with a failed process, so that the coordinator disk and all of data disks associated with a healthy process remain allocated to the node and process, respectively, while the data disks associated with the failed process are de-registered and can be reallocated.","The computer on the average user's desk has opened up the world in incredible ways over the course of the last 15 years. Users can now receive goods, services, and information from anywhere in the world and can generally transact business at any time of the day or night.","This revolution in commerce and knowledge has been, to a substantial degree, enabled by two concepts. The first is the concept of a network. At its simplest, a network is a connection (analogous to a telephone line) between two computers. In a simple example, if a user wants a score from a football game, the user can tell the computer on his or her desk to contact the computer at the local TV station and request a file containing the sports scores for the evening. The file containing the sports scores is usually returned in the form of a web page containing the requested scores, as well as graphics and video related to the games that the station wants to promote. More complex networks, reaching around the globe, enable computers all over the world to share data. Most users are aware of networks, frequently because they had a role in connecting their computer to the network. The network is a concept with which the user interacts directly.","The other concept that has enabled the revolution in commerce and knowledge over the past 15 years is the concept of clustering. Because the user typically has no direct interaction with the components of a cluster as separate entities, the existence of the cluster is entirely transparent and unknown to the user. The concept of a cluster builds on the concept of a network to allow a group of computer systems to act in a coordinated manner and fulfill requests. One type of clustering is most easily explained in terms of the operation of a fast food restaurant. When a user drives up to a drive-through window and orders a burger, French fries and a milk shake, the user interacts with (and sometimes sees only) one person at the window. The user does not interact with and frequently does not see a group of multiple people working to fill the order. A first person operates the cash register and takes orders. A second person cooks the French fries and operates the soft serve ice cream machine to make the milkshake. A third person cooks the hamburger. A fourth person cleans the restaurant and packages orders before handing them to the first person at the window.","In computing, a cluster can exhibit an analogous division of labor. Returning to the example of a user requesting sports scores from the computer at a local television station, there may be a cluster of computers used to answer the request, but that cluster may be entirely invisible to the user. Just as there are four people involved in serving a fast food order, a computing system cluster can be composed of a first machine that takes orders and sends completed files, a second machine that supplies video and pictures in the completed file, a third machine that provides storage for all of the elements of the completed file. There can be a fourth machine that handles network security by making sure that files are sent only to authorized users. When the machines of a cluster work well together, they may be entirely invisible to the user.  provides an example of a clustered system (only two machines) on a network.",{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 1","FIG. 1"],"b":["100","110","120","100","110","120","110","120","150","150","140","110","120"]},"Returning to the sports scores example, discussed above, a user at client  could send a request for sports scores to server A  over network . Server A  could then generate a file containing the requested sports scores and graphics that are provided by server B . Server A could then send the file containing the requested scores and the graphics over network  to client . Each task that a node is performing is an example of a concept referred to as a process. Thus, in the \u201csports score\u201d example, server A  is running a \u201cfile generating process\u201d and a \u201cfile sending process.\u201d Server B  is running a \u201cgraphics process.\u201d","In addition to network , servers  and  can also communicate with each other over private network . As shown, private network  is only accessible by cluster nodes, i.e., server A  and server B . To support the high availability of cluster , private network  typically includes redundancy such as two network paths instead of one. Private network  is used by the nodes for cluster service message passing including, for example, the exchange of so-called \u201cheart-beat\u201d signals. Heartbeat signals are, in one embodiment, packets containing messages indicating that a node is currently available to the cluster and functioning properly.","Other elements of cluster  include storage area network (SAN) , SAN switch , and storage devices such as one or more of tape library  (typically including one or more tape drives), one or more groups of disk drives  (i.e., \u201cjust a bunch of disks\u201d or \u201cJBOD\u201d), and one or more of intelligent storage array . These devices are examples of the type of storage used in cluster . Other storage schemes include the use of shared direct-attached storage (DAS) over shared SCSI buses. SAN  can be implemented using a variety of different technologies including fibre channel arbitrated loop (FCAL), fibre channel switched fabric, IP networks (e.g., iSCSI), Infiniband, etc.","SAN switch  and storage devices , , and  are examples of shared resources. The most common shared resource in a cluster is some form of shared data resource, such as one or more disk drives. Using a shared data resource gives different nodes in the cluster access to the same data, a feature that is common for most cluster applications. Although a disk device is perhaps the most common example of both a shared resource and a shared data resource, a variety of other types of devices will be well known to those having ordinary skill in the art. Moreover, although servers  and  are shown connected to storage array storage devices through SAN switch  and SAN , this need not be the case. Shared resources can be directly connected to some or all of the nodes in a cluster, and a cluster need not include a SAN. Alternatively, servers  and  can be connected to multiple SANs. Additionally, SAN switch  can be replaced with a SAN router or a SAN hub.","Just as a major problem can arise in a fast food restaurant if one of the workers is not performing his or her duties, problems can arise in a cluster if one of the nodes fails to execute the tasks assigned to it. In a cluster environment, current solutions allow for the isolation of a failed node. This is roughly analogous to dismissing a worker who is not doing his or her job. Typically, the chief indicator that a node has failed is the loss of the node's heartbeat signals across private network . When a node is isolated in response to a failure, all of the resources assigned to the node are reassigned to other nodes in the cluster. This is roughly analogous to assigning the cash register to another worker if the worker currently operating the cash register has to be dismissed.","In the environment of a cluster operating system, the primary resource assigned to nodes is storage, such as group of disk drives , for storing information. Typically, a group of disk drives (also called \u201cdisks\u201d) will be assigned to a node. If the node fails, the disks are reassigned to another node. Two types of disk are relevant to the current discussion. Data disks are used for storing user data (for example, sports scores) and coordinator disks are used for storing cluster system administrative data (e.g., data for performing administrative functions such as tracking which nodes are using which disks).","Tracking the assignment of a disk drive to a node is most easily explained in terms of tracking the assignment of a register to a worker in a modern restaurant. In the restaurant, the employee who is operating the cash register will typically log in to the cash register by swiping a key card or entering an ID code, or both. If an employee is dismissed, a newly-selected employee can log the previous employee out of the cash register, and the newly-selected employee can log in to the cash register. In a clustered storage system, a node declares its ownership of a disk in a very similar manner, by depositing a \u201ckey\u201d on a coordinator disk. The key is a short file that contains information about the node that owns the disk. The coordinator disk will store a key indicating which node owns the coordinator disk and will also store keys indicating which node owns each of the data disks that are associated with the coordinator disk.","When a node fails, resources (for example, coordinator disks and data disks) assigned to the node can then be reassigned by erasing any keys assigning to the failed node the coordinator disks and data disks, and the node can be restarted. Restarting a node, which is part of the process of node isolation, works reasonably well for situations in which the node is doing one thing, and the node is failing at that one thing, and the node is not doing anything else.","A more complex problem, which is addressed by the present invention, is created when a node is doing multiple things at one time. It was previously mentioned that, in the \u201csports score\u201d example, server A  is running a \u201cfile generating process\u201d and a \u201cfile sending process.\u201d Server B  is running a \u201cgraphics process.\u201d The presence of multiple processes on a single node can cause problems, as described below.","Returning to the example of the fast food restaurant, the second person cooks the French fries and operates the machine that makes the milkshake. Imagine that this second person is doing a horrible job at milkshakes, but does a splendid job at operating a deep fryer. Thus, the \u201cdeep fryer process\u201d is working well and the \u201cmilkshake process\u201d has failed. In such a situation, the manager of the restaurant has the option to either 1.) dismiss the employee and hope that the new employee to whom the deep fryer and the milkshake machine are assigned can execute both processes simultaneously or 2.) reassign the milkshake machine and let the employee continue to do splendid work with the deep fryer process.","Using the technology that existed before the present invention, which relies exclusively on node-level heartbeat messages to determine when a node should be dismissed and its resources should be reassigned, it was impossible for the operator of a cluster to close down a failed process on a node while leaving a working process in operation. For instance, some nodes execute a set of multiple processes, one of which may hang or crash. If one of the processes executing on a particular node hung or crashed, the only available solution was to reset the hung process is to isolate the entire node and cancel all of its processes. This node-wide shutdown resulted in the loss of healthy processes currently executing on the node, creating unnecessary service interruptions through the loss of the healthy processes.","Given the choice, users would prefer to keep healthy processes alive and end failed processes. Unfortunately, the technology that existed before the present invention required that, in order to shut down a failed process on a node, a cluster must shut down the healthy processes running on the node. In the world of clustered computer systems, failure on the milkshake process could shut down the deep fryer process. Users waiting on the healthy process, which was shut down to accommodate the need to shut down the failed process, were displeased with the service outages that resulted.","The present invention, in one embodiment, detects failures at the process level and failures at the node level, rather than relying exclusively on failure at the node-level. In response to a node-level failure, all resources associated with a node are re-allocated. In response to a process-level failure, the resources associated with failed individual processes, rather than the entire node, are reallocated.","This process involves several steps and structures. The first step is the creation of a key set with multiple types of keys. A first type of key associates a coordinator disk with a node. A second type of key associates a data disk with a node and a process. Keys are data structures that are, during operations that access a shared resource, checked to confirm that the entity performing the operation has a legitimate claim to access the resource. The first type of key is used to verify the access of a particular node to a coordinator disk. The second type of key is used to verify the access of a particular process to a data disk.","The second step is a system of detection that allows separate detection of node-level failures and process-level failures. Node failure is detected through the loss of heartbeat messages from the failed node on a private network link. Process failure is detected through a series of algorithms for monitoring individual process health, which are called triggers. The third step is a system of multiple enforcement mechanisms. Node failure is addressed by de-registering from a coordinator disk all keys associated with a failed node, so that the coordinator disk and all of data disks associated with the failed node can be reallocated. Process failure is addressed by de-registering from one or more coordinator disks all data disk keys associated with a failed process, so that the coordinator disk and all of data disks associated with a healthy process remain allocated to the node and process, respectively, while the data disks associated with the failed process are de-registered and can be reallocated.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 2A"},"Memory  is a representative memory space of a cluster node. In general, the software components are divided into those components operating at the kernel level, and those operating at the user level. Kernel level components include some basic software components supporting cluster operation. For example, low latency transport  provides high-speed kernel-to-kernel communications and monitors network connections between nodes. Node membership and messaging  is a kernel component that maintains and monitors node membership in the cluster. Node membership and messaging  can also provide a messaging mechanism to other software components, e.g., file system , process membership , volume manager , and cluster server software . Alternatively, the functions performed by node membership and messaging  could be performed by software operating at the user level. SCSI pass through  is an example of a hardware driving component for sending SCSI commands from other components to SCSI devices, such as the data disks in devices  and  and certain coordinator resources used by I\/O fence mechanisms that are designed to receive SCSI commands, i.e., SCSI-2 and SCSI-3 compliant devices.","Fence driver  operates in conjunction with fence daemon  and various fence mechanisms -to prevent ejected cluster nodes, i.e., nodes that have lost cluster membership because of some connectivity failure, from accessing shared storage resources, causing data corruption, or otherwise disrupting expected cluster behavior (e.g., shared-nothing cluster operation). For example, fence driver  receives node membership information either directly or indirectly from node membership and messaging component . Once node membership and messaging  learns that communication with another cluster node has been lost, i.e., that the cluster has partitioned, it can directly inform fence driver , which in turn begins taking action to protect shared resources. Alternately, node membership and messaging  informs other kernel components such as file system , process memberships , or volume manager , that communication with another cluster node has been lost. Subsequent communication between the informed kernel component(s) and fence driver  is the mechanism by which partition event information is passed to fence driver . For example, upon receiving cluster membership change information from component , a component such as process membership  can query fence driver  to determine whether it is safe, e.g., the split-brain condition has been resolved, to process membership change information. Fence driver  will not typically allow process membership  to proceed until fencing operations are sufficiently complete. File system kernel component  provides and\/or supports additional (beyond typical operating system file system capabilities) file system features including for example: quick-recovery, journaling, backup without data lock out, and online file system resizing. Process membership component  monitors and controls the membership of processes in the node and ensures that information on current member processes remains the same on all cluster nodes. Volume manager  enables physical resources configured in the cluster to be managed as logical devices or volumes.","Those having ordinary skill in the art will readily recognize that a variety of different additional kernel components can be (and typically are) utilized by a cluster node. Many of the components described above as well as some of the user level components described below are part of one or more of the VERITAS Volume Manager\u2122, VERITAS File System\u2122, and VERITAS Cluster Server\u2122 products provided by VERITAS Software Corporation.","Fence driver , fence daemon , and fence mechanisms  and , can operate in conjunction with fence configuration software , fence administration software , and fence data . For example, fence configuration software  can be used by a system operator to specify and initialize information stored in a coordinator resource, e.g., which nodes are part of the cluster, as well as to configure fence driver  and fence daemon . For those devices to which an attached volume maps, volume manager  can issue to SCSI pass through  commands for reading and displaying keys, registering keys or key sets with devices by storing the key or key set on the device, making a reservation with a device, removing registrations made by other devices, reading reservations, and removing registrations (also called de-registering). In some embodiments, a key set comprises both a shared coordinator resource key and a group of shared data resource keys. Together with fence driver , fence daemon , and fence mechanisms  and , components , , and  provide core functionality for the I\/O fencing services used to prevent data corruption. Note that one or more of components , , , , , and  can be included within other components, and\/or several components can be combined.","The user level also typically includes software components such as the previously mentioned cluster server software  and processes -. Processes -may be embodied as any of a wide variety of applications (e.g., database administration systems (DBMS), file servers, application servers, web servers, backup and restore software, customer relationship management software, and the like) or may be embodied by virtual machines, which simulate the existence of an independent processor providing an independent instance of an operating system for executing applications. On one embodiment, each of processes -is monitored by one of process monitors -, which are typically scripts that send commands to processes -and receive the results of those commands as feedback. When the results of such a command indicate a failure of one of processes -, the corresponding one of process monitors -sends a failure signal (also called a trigger) to notification engine . A process failure indicator will typically indicate a process failure in spite of the fact that a heartbeat will continue to be received from the node housing the process. Examples of indicators of process failure also include the failure to respond to a \u2018kill process\u2019 command.","Notification engine  receives results from process monitors and, in the event of a process failure, informs process membership  and fence driver . Additionally, notification engine  parses a configuration file  to identify resources associated with any of processes -that has failed. In some embodiments, notification engine  generates during registration of a key and maintains thereafter a dependency table  that contains an identification of the resources associated with processes -. Such a dependency table contains mappings between processes and the shared data resources that support the processes and store user data.","Fencing components use one or more coordinator resources to register and de-register keys as part of the I\/O fencing operation. The use of coordinator resources enables the fencing components to address failed nodes and failed processes occurring in cluster computer systems.","In general, when a node fails or isolates itself, the resulting condition is called a \u201csplit brain\u201d because the failure or isolation of a node can lead separate parts of a cluster to each conclude that they have exclusive access to a shared resource. In general, when a split-brain condition occurs, a designated node in each subcluster, e.g., the lowest numbered node, \u201craces\u201d to gain control of the coordinator resource(s). The winning node remains in the cluster, and fence components are used to fence losing nodes off from the shared data storage and\/or remove the nodes from the cluster. This can be accomplished by causing nodes to lose their membership in the cluster, e.g., as reflected in membership records stored in a coordinator resource. The nodes remove themselves from the cluster (\u201ccommit suicide\u201d) upon recognizing that they have lost membership. In still other examples, nodes are forcibly removed from the cluster by, for example, cycling their power. Ejected systems cannot write to the data disks, and therefore the potential for corrupt data is greatly reduced or eliminated.","In operations in which only a process on a node has failed, however, fencing components are used to fence off only the failed process from the resources associated with the failed process. All nodes will typically remain associated with their coordinator resources. A split-brain is not developed, and no racing to control a coordinator resource is conducted. The selected processes are fenced off from shared resources, prohibiting them from reading or writing to data disks. Ejected processes cannot write to the data disks, and therefore the potential for corrupt data is greatly reduced or eliminated. At the same time, healthy processes executing on the same nodes as the failed processes are allowed to remain in contact with the data disks, and the healthy processes continue to execute. In one embodiment, whether and how to shut down failed processes, once they are isolated from shared data resources such as data disks, is left to the individual node.","In the generalized I\/O fence framework illustrated in , much of the functionality for implementing various I\/O fence techniques is contained in each of the fence mechanisms -. In general, each fence mechanism is a separate module, e.g., a separate executable, script, DLL, etc., that is used in conjunction with the other fence components illustrated to implement a particular I\/O fencing scheme. Numerous different I\/O fence mechanisms can be implemented, some of which are discussed in greater detail below with respect to .","Fence mechanisms can be differentiated by the type of coordinator resource or resources used to determine which processes or nodes gain and lose access to shared resources in the cluster. Examples of the different coordinator resources used by fence mechanisms include: SCSI-3 compliant devices where SCSI-3 persistent reservation and registration commands are used to register with and gain control of the devices in order to determine the node or nodes that will remain in the cluster; SCSI-2 compliant devices where SCSI-2 reservation and registration commands are used to register with and gain control of the devices in order to determine the node or nodes that will remain in the cluster; remote access power distribution units that provide power to cluster nodes and can be used to cycle the power or turn nodes off; node hardware and\/or software that supports a standard for receiving remote management commands such as the Intelligent Platform Management Interface (IPMI) standard; virtual devices such as specifically defined volumes for use in a manner similar to SCSI devices; management processors or computer systems that are coupled to the cluster but are not nodes of the cluster and provide some cluster management functionality (e.g., the blade management processor of a blade server system); storage area network devices (e.g., switches, routers, etc) that can provide SAN zoning functionality to \u201czone out\u201d node access to certain storage elements; arbitrator processes operating on a computer system, typically remote from the cluster nodes, control or ownership of which can confer success in a race for the coordinator resource; and contact-response systems where a person or program is contacted (e.g., via e-mail or telephone) and a designated response indicates control of the resource. Numerous other examples of coordinator resources and corresponding fence mechanisms will be known to those having ordinary skill in the art.","Thus, the coordinator resource or resources can include a variety of physical devices, logical devices, processes, and combinations thereof. One of the advantages of the framework illustrated in  is that it can, in general, accommodate any type of fence mechanism. Functionality specific to the particular I\/O fencing technique is concentrated in a corresponding I\/O fence mechanism (-). Fence daemon  manages the various fence mechanisms, invokes them as necessary, provides supporting functionality, and interacts with fence driver  to, for example, get information about cluster membership and cluster partition conditions. For example, when node membership and messaging  receives information indicating that the node has lost contact with another node, it will inform fence driver  which in turn communicates with fence daemon . Fence daemon  operates in association with an appropriate fence mechanism to perform the corresponding fencing operation.","While various fence mechanisms will tend to differ based on differences among the coordinator resources used, fence driver , fence daemon  and any fence mechanism will typically operate in conjunction with each other to provide one or more of the following functionalities: the ability to generate and compare mechanism comparison information to ensure that the there are no critical differences in the instance\/version of the fence mechanism used by the nodes of a cluster; the ability to join a particular node to a cluster; the ability to register and de-register keys on a coordinator resource for various nodes and processes; the ability to race for the coordinator resource(s) so that only one node can win the race; the ability to fence off data resources that must be protected from possible data corruption from a failed node or process; the ability to unjoin a node from a cluster under certain circumstances (typically when the node gracefully and successfully leaves the cluster); the ability to isolate a process; and the ability to exit a node from the cluster (either in a un-graceful, error-driven situation or in an unjoin situation where there is a related error condition).","In some cases, the manner in which one or more of these functionalities is implemented will be very specific to the fencing technique used by the fence mechanism. For example, the race operation for SCSI-2 or SCSI-3 devices includes the issuing of various SCSI commands to try to gain control of the coordinator resource(s), while the race for control of power distribution units supplying power to nodes of the cluster might simply entail issuing \u201cpower off\u201d commands. Similarly, the isolation of a process running on a particular node may simply entail commands to de-register the process by erasing keys stored on the coordinator node that assign a data disk to the process. In other examples, different fence mechanisms may share the same or similar implementation of a particular functionality. Additionally, the type of fence mechanism and\/or simple implementation details my dictate how the different system components perform different parts of the needed tasks. In one embodiment, communication between fence daemon  and fence driver  is performed according to an application programming interface (API). Such communication typically includes instructions to perform certain tasks, e.g., begin race for coordinator resource, and messages about operation status, e.g., race success\/failure. Similarly, communication between fence daemon  and fence mechanisms can be via API calls or using other techniques well known to those having skill in the art.",{"@attributes":{"id":"p-0059","num":"0058"},"figref":["FIG. 2B","FIG. 5A","FIG. 2B"],"b":["213","512","213"],"i":"a"},"Key storage area  includes a coordinator key . A coordinator disk is associated to a node when coordinator key  is registered on a coordinator disk by storing coordinator key  in key storage area . In one embodiment, coordinator key  includes a node identifier that associates a node with the coordinator disk on which coordinator key  is stored. Coordinator key  further includes an operation identifier that associates a node with a protocol version used by the coordinator disk on which coordinator key  is stored. Coordinator key  also includes a disk group identifier that associates a node with a disk group used by the coordinator disk on which coordinator key  is stored.","Key storage area  additionally includes shared data resource keys in the form of data disk keys -. A data disk is an example of a shared data resource associated to a process executing on a node when one of data disk keys -is registered on a coordinator disk by storing one of data disk keys -in key storage area . In one embodiment, each of data disk keys -includes node identifiers -that associates a node with the data disk identified by the respective data disk key . Data disk keys -further include operation identifier -that associate a data disk with a protocol version used by the data disk identified by the respective data disk key . Data disk keys -also include disk group identifiers -that associate a data disk with a disk group using the data disk identified by the respective data disk key . Data disk keys -finally include disk application identifiers -that associate a data disk with a process, such as an application or a virtual machine using the data disk identified by the respective data disk key .",{"@attributes":{"id":"p-0062","num":"0061"},"figref":["FIG. 3","FIG. 1","FIG. 4"],"b":["302","130","304","304"]},"Returning to step , if a node level failure is not detected, the process proceeds to step , which depicts detecting a process failure. If no process failure is identified, the process returns to step , which is described above. Turning briefly to , in one embodiment, a failure of a process from among processes -is detected by one of process monitors -. Each of processes -is monitored by one of process monitors -, which are typically scripts that send commands to processes -and receive the results of those commands as feedback. When the results of such a command indicate a failure of one of processes -, the corresponding one of process monitors -sends a failure signal (also called a trigger) to notification engine . Notification engine  receives results from process monitors and, in the event of a process failure, informs process membership  and fence driver .","Returning to  and step , if a process failure is identified, the process next moves to step . Step  illustrates identifying data disks associated with a failed process. Referring briefly to , in one embodiment of the present invention, notification engine  parses a configuration file  to identify resources associated with any of processes -that has failed. In alternative embodiments, notification engine  maintains a dependency table  that contains an identification of the resources associated with processes -, and reference to dependency table  is made, rather than parsing configuration file .","Returning to , the process next proceeds to step , which depicts identifying data disk keys associated with a failed process. Referring briefly to  and , in one embodiment, one or more of fence mechanisms -will contact a coordinator disk and read the key storage data structure housed in key storage area . Specifically, one or more of fence mechanisms -will read the application identifiers -of data disk keys -to identify data disk keys associated with a failed process. In one embodiment, this identification is accomplished with a READ KEYS command set to identify a particular pattern in the process identifier associated with a failed process.","Returning to , the process then moves to step , which depicts removal of data disk keys associating data disks with the failed process. Referring briefly to  and , in one embodiment, one or more of fence mechanisms -will contact a coordinator disk and delete from the key storage data structure embodied by key storage area  any of data disk keys -containing application identifiers -identifying data disk keys associated with a failed process. In one embodiment, this identification is accomplished with a READ KEYS command set to identify a particular pattern in the process identifier associated with a failed process. Coordinator key  will remain registered and will not be deleted. Similarly, any of data disk keys -containing application identifiers -that do not identify data disk keys associated with a failed process will remain registered and will not be deleted.","Returning to , the process next moves to step . Step  illustrates notifying node membership to start recovery of resources for which keys have been deleted in step . The process then ends. Such a notification is performed through an interprocess (IPC) call or through a command directed to a command-line interface.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":["FIG. 4","FIG. 3"],"b":["304","410","410"]},"In , the racer node races for the coordinator resource, such as a coordinator disk, designated by the fence mechanism in use. Next, a determination is made at step  as to whether the node has successfully gained control of the coordinator resource. If not, operation transitions to  where a timeout determination is made. For example, a given node's authorization to act as the racer may be limited by a requirement that it must succeed within a given time period. Such a feature is particularly useful where there are multiple nodes in a subcluster and it may be desirable to allow another node to operate as the racer after a certain time period. In another example, the timeout period is used to facilitate implementation of multiple fence mechanisms in the same cluster system. Thus, if a timeout period has occurred, operation can proceed to  where it is determined whether another fence mechanism is available. For example, a cluster can be configured to use a primary fence mechanism, e.g., SCSI-3 devices, and a secondary fence mechanism, e.g., power distribution units, in the event that the first mechanism fails to work properly. Alternatively, multiple fence mechanisms can be used in parallel (not illustrated). If there is another fence mechanism to use, operation transitions to  where that mechanism is selected. Operation then returns to  to proceed using the alternate mechanism. If there are no further mechanisms to be used, an error condition occurs (). In many implementations, only one fence mechanism will be available, and there will be no need to perform operations like those illustrated at  and . Moreover, for a given race mechanism, there will typically be only one racer node in a given subcluster and if that racer node fails to win the race for any reason, all nodes in that subcluster will be ejected. In still other examples, expiration of a timeout time period can cause an error condition to occur.","If the node succeeds in gaining control of the coordinator resource, operation transitions to  where the winning node informs other nodes of its success. Those nodes may take subsequent action based on such a \u201crace success\u201d message. Next, on step , the winning node takes any additional steps necessary to fence of data and protect it from corruption, such as deleting coordinator disk keys and data disk keys associated with a failed node. Referring briefly to  and , one or more of fence mechanisms -will read the node identifiers and -of coordinator key  and data disk keys -, respectively, to identify coordinator disk keys and data disk keys associated with the failed node. One or more of fence mechanisms -will contact a coordinator disk and delete from the key storage data structure housed in key storage are  any of data disk keys -or coordinator key  containing node identifiers and -identifying keys associated with a failed node.","In step , a determination is made whether the data fencing has been successful. This might include confirming that any nodes from \u201closing\u201d subclusters are unregistered from a coordinator device, ensuring that only nodes in the winning subcluster have access to the data, and the like. Although not illustrated, this process can also include steps similar to those illustrated at , , and . Thus, there may be a timeout associated with the data fencing step (e.g., when this process is explicitly separate from the race process) and there can also be multiple data fencing mechanisms to try. If the fence has failed, an error condition has occurred () and subsequent steps can be performed as described above. If the fence has succeeded that the remaining nodes can resume normal operation and the process ends ().",{"@attributes":{"id":"p-0072","num":"0071"},"figref":["FIG. 5A","FIG. 5A","FIGS. 3 and 4"],"b":["500","502","530","504","530","502","504","510","514","512","512","512","512","512"],"i":["a","b ","c ","a","f ","a","c","a","c ","a","c ","a","c ","a","c "]},"The following registration steps can be performed before, during, or after a cluster join operation. A node (e.g., node A ) registers with the designated coordinator disks -using the PERSISTENT OUT-REGISTER command and stores coordinator disk keys in key storage areas on coordinator disks -. Alternately, registration can occur manually when a system administrator configures the system. For example, a system administrator starts an application on node A, such as process 1 using appropriate cluster server software. Alternately, this step can be performed as part of an automatic start-up process. Node A then registers with the data disks  in storage array  by issuing a PERSISTENT OUT-REGISTER command on the path between the node and storage array  using a registration key, e.g., \u201cK1.\u201d Node A  checks if other nodes are registered with any of the data disks  using the PERSISTENT IN-READ KEYS command.","Next, node A  prevents data disks  from accepting I\/Os from other nodes by issuing a PERSISTENT OUT-RESERVE with a \u201cwrite exclusive-registrants only\u201d (WXRO) reservation and placing data disk keys in key storage areas on coordinator disks -. This means that data disks -will only accept write requests from a registered node. A node which is not registered and attempting to write to the disk will receive a SCSI RESERVATION CONFLICT error. Any cluster applications can now proceed normally. For example, a database contained one or more of data disks -is started on node A. Node A  reads and writes to the data disks normally.","When Node A  has failed (), the two nodes lose contact with each other. Node B  will then unregister Node A  from the coordinator disks -using the PERSISTENT OUT-PREEMPT AND ABORT.","To perform that task, node B  sends PERSISTENT OUT-REGISTER commands to each coordinator disks -using node B's key, e.g., \u201cK2\u201d. In general, the commands to each disk are sent in parallel. This task is performed because node B  has not previously registered with data disks -. Node B  can then issue PERSISTENT IN-READ KEYS commands to determine the keys registered with respect to data disks -. If any of the data disks have a key not belonging to node B , e.g., not K2, node B  can then issue PERSISTENT OUT-PREEMPT and ABORT commands with respect to the appropriate data disks -with a victim key value corresponding to that read in the previous command. In this example, node B  finds that node A (key K1) is still registered, and accordingly unregisters node A  by deleting the key for node A . Thus, node B  takes over operation with an I\/O fence in place. At this point additional action can be taken by node B . For example, node B can prevent data disks -from accepting I\/Os from other nodes by issuing a PERSISTENT OUT-RESERVE with a \u201cwrite exclusive\u2014registrants only\u201d (WXRO) reservation.",{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 5B","b":["546","532","540","534","540","532","532","534","550","544","542"],"i":["a","b ","c ","a","f ","a","c. "]},"In this example, coordinator disk 1 , is registered to node A . Data disk 1 and data disk 2 are registered to process 1 . Similarly, coordinator disk 2 is registered to node A . Data disk 3 and data disk 4 are registered to process 2 . Likewise, coordinator disk 3 is registered to node B . Data disk 5 and data disk 6 are registered to process 3 ","Thus, in the example of , coordinator disks -serve as the coordinator resources used by a corresponding fence mechanism. If coordinator disks -are SCSI-2 disks, then a SCSI-2 based fence mechanism can be implemented where nodes accessing the coordinator disks -are allowed to reserve a disk drive using the SCSI-2 \u201creserve\u201d command and subsequently release the disk drive for use by another device via the \u201crelease\u201d command. If coordinator disks -are SCSI-3 compliant devices, the more robust persistent reservation commands can be used. The following example outlines the use of a SCSI-3 fencing technique and makes reference to steps described in .","The following registration steps can be performed before, during, or after a cluster join operation. A node (e.g., node A ) registers with the designated coordinator disks -using the PERSISTENT OUT-REGISTER command and stores coordinator disk keys in key storage areas on coordinator disks -. Similarly, node B  registers with the designated coordinator disks using the PERSISTENT OUT-REGISTER command and stores coordinator disk keys in key storage area on coordinator disks . Node A  then registers with the data disks -in storage array  by issuing a PERSISTENT OUT-REGISTER command on the path between the node and storage array  using a registration key, e.g., \u201cK3.\u201d Node B  then registers with the data disks -in storage array  by issuing a PERSISTENT OUT-REGISTER command on the path between the node and storage array  using a registration key, e.g., \u201cK4.\u201d","Next, node A  prevents data disks -from accepting I\/Os from other nodes by issuing a PERSISTENT OUT-RESERVE with a \u201cwrite exclusive-registrants only\u201d (WXRO) reservation and placing data disk keys in key storage areas on coordinator disks -. This means that data disks -will only accept write requests from a registered node. Node B  prevents data disks -from accepting I\/Os from other nodes by issuing a PERSISTENT OUT-RESERVE with a \u201cwrite exclusive-registrants only\u201d (WXRO) reservation and placing data disk keys in key storage areas on coordinator disk . This means that data disks -will only accept write requests from a registered node. A node which is not registered and attempting to write to the disk will receive a SCSI RESERVATION CONFLICT error. Any cluster applications can now proceed normally.","Once process 2 on Node A  has failed (), process 2 will be unregistered from coordinator disks using the PERSISTENT OUT-PREEMPT AND ABORT. Processes -continue to operate and their keys remain registered. Similarly, coordinator keys for both Node A  and node B  remain registered.","To perform that task, a PERSISTENT OUT-REGISTER command is sent to coordinator disks using node A's key, e.g., \u201cK3\u201d. A node can then issue PERSISTENT IN-READ KEYS commands to determine the keys registered with respect to data disks -. If any of the data disks have a key belonging to process 2 , a node can then issue PERSISTENT OUT-PREEMPT and ABORT commands with respect to the appropriate data disks -with a victim key value corresponding to that read in the previous command. At this point additional action can be taken. For example, a node can prevent data disks -from accepting I\/Os from other process 2 by issuing a PERSISTENT OUT-RESERVE with a \u201cwrite exclusive-registrants only\u201d (WXRO) reservation. The registrations of coordinator disks -and data disks and remain unchanged.",{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIG. 6","FIG. 1","FIG. 1"],"b":["610","110","140","610","612","610","614","617","618","620","622","624","626","628","630","632","633","634","637","638","635","690","635","639","640","642","646","612","628","647","612","630","648","612"]},"Bus  allows data communication between central processor  and system memory , which may include read-only memory (ROM) or flash memory (neither shown) and random access memory (RAM) (not shown), as previously noted. RAM is generally the main memory into which OS and application programs are loaded. ROM or flash memory can contain, among other code, the Basic Input-Output system (BIOS) which controls basic hardware operation such as the interaction with peripheral components. Applications resident with computer system  are generally stored on and accessed via a computer-readable storage medium, such as hard disk drive (e.g., fixed disk ), an optical drive (e.g., optical drive ), a floppy disk unit .","Storage interface , as with other storage interfaces of computer system , can connect to a standard computer-readable storage medium for storage and\/or retrieval of information, such as a fixed disk drive . Fixed disk drive  may be a part of computer system , or may be separate and accessed through other interface systems. Modem  can be employed to provide a direct connection to a remote server via a telephone link or to the Internet via an interne service provider (ISP). Network interface  may provide a direct connection to a remote server via a direct network link to the Internet via a point-of-presence (POP). Network interface  may provide such connection using wireless techniques, including digital cellular telephone connection, Cellular Digital Packet Data (CDPD) connection, digital satellite data connection or the like.","Many other devices or subsystems (not shown) may be connected in a similar manner (e.g., document scanners, digital cameras, and so on). Conversely, all of the devices shown in  need not be present. The devices and subsystems can be interconnected in different ways from that shown in . The operation of the computer system such as that shown in  is readily known in the art and is not discussed in detail in this application. Code to implement the previously-described features can be stored in computer-readable storage media such as one or more of system memory , fixed disk , optical disk , or floppy disk . The OS provided on computer system  can be, for example, MS-DOS\u00ae, MS-WINDOWS\u00ae, OS\/2\u00ae, UNIX\u00ae, Linux\u00ae, or other known OS.","Moreover, regarding the messages and\/or data signals described herein, those skilled in the art will recognize that a signal may be directly transmitted from a first block to a second block, or a signal may be modified (e.g., amplified, attenuated, delayed, latched, buffered, inverted, filtered, or otherwise modified) between the blocks. Although the signals of the above described embodiment are characterized as transmitted from one block to the next, other embodiments may include modified signals in place of such directly transmitted signals as long as the informational and\/or functional aspect of the signals is transmitted between the blocks. To some extent, a signal input at a second block may be conceptualized as a second signal derived from a first signal output from a first block due to the physical limitations of the circuitry involved (e.g., there will inevitably be some attenuation and delay). Therefore, as used herein, a second signal derived from the first signal includes the first signal or any modifications to the first signal, whether due to circuit limitations or due to passage through other circuit elements which do not change the informational and\/or final functional aspect of the first signal.","Although the present invention has been described with respect to a specific preferred embodiment thereof, various changes and modifications may be suggested to one skilled in the art and it is intended that the present invention encompass such changes and modifications that fall within the scope of the appended claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The present invention may be better understood, and its numerous objects, features and advantages made apparent to those skilled in the art by referencing the accompanying drawings.",{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
