---
title: System for background subtraction with 3D camera
abstract: A system for background image subtraction includes a computing device coupled with a 3D video camera, a processor of the device programmed to receive a video feed from the camera containing images of one or more subject that include depth information. The processor, for an image: segments pixels and corresponding depth information into three different regions including foreground (FG), background (BG), and unclear (UC); categorizes UC pixels as FG or BG using a function that considers the color and background history (BGH) information associated with the UC pixels and the color and BGH information associated with pixels near the UC pixels; examines the pixels marked as FG and applies temporal and spatial filters to smooth boundaries of the FG regions; constructs a new image by overlaying the FG regions on top of a new background; displays a video feed of the new image in a display device; and continually maintains the BGH.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09530044&OS=09530044&RS=09530044
owner: THE BOARD OF TRUSTEES OF THE UNIVERSITY OF ILLINOIS
number: 09530044
owner_city: Urbana
owner_country: US
publication_date: 20150721
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","TECHNICAL FIELD","BACKGROUND","DETAILED DESCRIPTION"],"p":["This application is a continuation of U.S. application Ser. No. 14\/174,498, filed Feb. 6, 2014, entitled \u201cSYSTEM FOR BACKGROUND SUBTRACTION WITH 3D CAMERA\u201d, which is a continuation of U.S. application Ser. No. 12\/871,428, filed Aug. 30, 2010, entitled \u201cSYSTEM FOR BACKGROUND SUBTRACTION WITH 3D CAMERA\u201d, now U.S. Pat. No. 8,649,592.","The present disclosure relates generally to 3D image processing, and more particularly, to a system for background subtraction from images in a video stream using a three-dimensional camera.","Background subtraction (BGS) refers to the ability to remove unwanted background from a live video. Some current video conferencing programs use BGS technology to subtract and replace the background with another prerecorded still or moving background.","There have been several methods developed for BGS using color information only. These methods are either not robust for challenging, but common, situations such as a moving background and changing lighting, or too computationally expensive to be able to run in real-time. The recent emergency of depth cameras provides an opportunity to develop robust, real-time BGS systems using depth information. However, due to current hardware limitations, some of which are fundamental, recorded depth video has poor quality. Notable problems with recorded depth are noisy and instable depth values around object boundaries, and the loss of depth values in hair of a person or shiny object areas, such as belt buckles. As a result, background removal by a simple depth thresholding-referred to as Basic BGS herein-inherits a lot of annoying visual artifacts. Ideally, a robust system will detect and eliminate visual artifacts, and reduce jitter and roughness around edges contiguous with a removed background.","By way of introduction, the present disclosure relates to a system having a computing device (or other computer) coupled with a three-dimensional (3D) camera for subtracting a background (BG) from a video feed. The system may also replace the removed background with a new background, whether a still or video image. The system executes various, or all, of the steps executable by a background subtraction module disclosed herein to achieve step-by-step improvement in a robustness and quality of the result. That is, the module as executed by a processor eliminates the artifacts, noise, and the instability of the depth information around edges of one or more target person\u2014also referred to as subject herein\u2014that is to remains as foreground (FG) when the background is subtracted.","The system receives a video feed from the 3D camera that contains colored images of the one or more subject that includes depth information. For each colored image extracted from the video feed, the system segments colored pixels and corresponding depth information of the images into three different regions including foreground (FG), background (BG), and unclear (UC). The system may then categorize UC pixels as FG or BG using a function that considers the color and background history (BGH) information associated with the UC pixels and the color and BGH information associated with pixels near the UC pixels. Pixels that are near other pixels may also be referred to herein as neighbor pixels, which are pixels within a predetermined-sized window that includes the pixel of reference.","The system may also examine the pixels marked as FG and apply temporal and spatial filters to smooth boundaries of the FG regions. The system may then construct a new image by overlaying the FG regions on top of a new background, and display a video feed of the new image in a display device coupled with the computing device. The new background may include still images or video. The FG region that remains preferably includes one or more target subjects that are to be transferred from the processed image to the new image. The system may also continually maintain the BGH to keep it up to date for continued processing across multiple images within a video stream. Additional or different steps are contemplated and explained with reference to the Figures herein.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 1","b":["100","101","103","107","109","101"]},"The 3D camera  includes, among other components, a red\/green\/blue (RGB) sensor , an infrared (IR) sensor , and an IR illuminator . The IR illuminator  shines light through a lens of the camera  and the infrared sensor  receives the depth information of the reflected light, giving definition to objects within view or in the \u201cscene\u201d of the camera . The RGB sensor  captures the colored pixel information in the scene of the captured video image. The 3D camera  may also include synchronization hardware and\/or software  embedded therein to temporally synchronize the IR illuminator , the IR sensor , and the RGB sensor  together. The 3D camera  may also include a 3D application programming interface (API) , which may be programmed to receive the depth information (Z) , the brightness (B) , and RGB pixel  information of a reflected video image as captured by the 3D camera . The 3D API  provides the JO structure and interface programming required to pass this information , , and  to the computer or computing device .","The computing device  may further include, or be coupled with, a background subtraction module  stored in memory and executable by a processor, a post-processing module , background subtraction application programming interface (API) , a background history (BGH) storage  part of memory, and a display  such as a computer screen\/monitor or a plasma or LCD screen of a television or smart device. Accordingly, the computing device  may include a desktop, laptop, smart phone, or other mobile or stationary computing device having sufficient processing power to execute the background subtraction module . Where X and Y axes may be referred to herein, it is with reference to a two-dimensional (2D) plane cut through some point along the Z axis.","The computing device  may process the background subtraction module with reference to sequential sets of images from the video feed continually in real time. The post-processing module  may, for instance, overlay the surviving FG regions onto a new background image, whether from a still or a video, to create a new image. Sequential, real-time processing may yield a series of such new images over the top of the new background to create a new video feed having the old Background replaced with the new background. The computer  may then display the one or more subject in front of the new background on the display screen  for viewing by the user.","During the process of processing sequential colored images from an incoming video feed, background history of the sequential colored images may be kept up to date in the BGH storage . This history allows tracking the BG status of pixels in previous frames, e.g., whether the pixels were previously categorized as BG. This process and the way the background module incorporates BGH into a decision whether to categorized UC regions as BG will be discussed in more detail below.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":["FIG. 2","FIG. 1","FIG. 2","FIGS. 3-29"],"b":"129"},"At block , the system  may receive depth  and color  information of a colored image and perform depth and IR thresholding, thus segmenting colored pixels and corresponding depth information of the images into three different regions including foreground (FG), background (BG), and unclear (UC). The result of the depth and IR thresholding of the image is a region map that shows the three regions pictorially. In block , the system  may identify and clean FG, BG, and UC three-dimensional connected components. At block , the system  may enable a user  to select a user mode that depends on how close a target subject is located with reference to the camera . At block , the system  may clean the UC region under a center of mass (COM) of the target subject. At block , the system  may warp the image from a depth point of view to a color point of view, so that the depth and color information are aligned in 3D-space. At block , the system  may receive RGB color information  and clean the remaining UC region with background history (BGH). At block , the system  may interpolate the region map to categorize uncategorized pixels in the RGB image which have unknown depth value and unknown region value as FG or UC depending on region information of neighbor pixels. At block , the system  may dilate the UC region outward to surrounding pixels that are not in the FG region. At block , the system  may detect a FG fringe, which may include a thin area along the boundaries of the FG edges, e.g., those edges between the FG region and the UC region or the BG region. At block , the system  may update the BGH.","At block , the system  may clean the UC region using neighbor pixels, which step focuses on cleaning along the colored edge of the FG region. At block , the system  may clean the UC region under the COM of the target subject. At block , the system  may apply a median filter to the UC region to remove very small UC region, then merge the remaining UC regions into the FG regions. At block , the system  may stabilize and smooth the edges of the FG region(s). At block , the system  may check for reset conditions, and if present, sets a reset flag. At block , the system  determines if the reset flag is true, and if so, resets the flag. At block , the system may reset both the BGH and a BG mask of the region map. Processing by the background subtraction module  of the system  may then continue with another image from the video feed. Sequential processing of colored images may lead to a continuous, real-time video feed having the BG subtracted therefrom. At block , if the reset flag has not been set, e.g., it has a false value, the system  continues operation at block  again to continue processing sequential images. The same is true after resetting the BG mask and BGH at block .",{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 3","FIG. 4","FIG. 3","FIG. 5","FIG. 3","FIG. 6","FIG. 3","FIG. 2"],"b":["202","202","131"]},"As discussed earlier, the \u201cz\u201d as used herein is with reference to a depth value of a particular pixel. A smaller value of z indicates that a pixel is closer to the camera . The term \u201cb\u201d refers to brightness or, in other words, the IR intensity collected by the IR sensor. With regards to a particular pixel, the higher the intensity (b) value is, the more confidently the system  can differentiate the real signal from ambient noise, and the more the system  can trust the depth value. Values segmented into a FG or BG region are done with high confidence, whereas pixels initially segmented into the UC region are pixels with regards to which the system  is unsure how to categorize. Accordingly, if pixels of a colored image are not categorizable as either FG or BG, the pixels may be categorized as UC. Note that pixels in the same region do not need to be adjacent or near each other to be categorized, as displayed in .","One set of rules to drive this segmentation of the pixels of an image is for the system  to: (1) categorize the pixel as foreground (FG) if a depth thereof is less than a predetermined threshold distance from the camera and a intensity thereof is greater than a predetermined threshold intensity; (2) categorize the pixel as unclear (UC) if a depth thereof is less than the predetermined threshold distance and an intensity thereof is less than the predetermined threshold strength; and (3) categorize all other pixels not categorized as FG or UC as background (BG). These rules are cast below in Equation 1, which depicts a region map, rmap[i].",{"@attributes":{"id":"p-0050","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":"\u2003","mrow":{"mo":"{","mtable":{"mtr":[{"mtd":{"mrow":{"mi":["FG","if"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":[{"mrow":{"mn":"0","mo":["<","<"],"mrow":{"mi":"z","mo":"\u2061","mrow":{"mo":["[","]"],"mi":"i"}},"msub":{"mi":["z","thresh"]}},"mo":["\u2062","&"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mi":"b","mo":"\u2061","mrow":{"mo":["[","]"],"mi":"i"}}],"mo":"\u2062"},"mo":">","msub":{"mi":["b","thresh"]}}}}}},{"mtd":{"mrow":{"mi":["UC","if"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":[{"mrow":{"mn":"0","mo":["<","<"],"mrow":{"mi":"z","mo":"\u2061","mrow":{"mo":["[","]"],"mi":"i"}},"msub":{"mi":["z","thresh"]}},"mo":["\u2062","&"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mi":"b","mo":"\u2061","mrow":{"mo":["[","]"],"mi":"i"}}],"mo":"\u2062"},"mo":"<","msub":{"mi":["b","thresh"]}}}}}},{"mtd":{"mrow":{"mi":["BG","else"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}}}]}}}}}},{"@attributes":{"id":"p-0051","num":"0050"},"figref":["FIG. 7","FIG. 6","FIG. 2"],"b":["204","204"]},"The system , in executing block , begins by detecting and labeling pixels that are adjacent to each other, in the same region, and that have similar depth values as region-specific connected components. In other words, the depth values of two adjacent pixels in the same component is smaller than a predetermined threshold. For instance, the system may detect and label FG-connected components in 3D space (XY plane plus depth, Z). The system  thus groups pixels that are determined to be connected components for common processing. In the follow expressions, D is the depth image, p is a pixel, R is the region-labeled map, N(p) are adjacent pixels around pixel p. A 3D connected-component label C\u03b5C is defined as C={p\u03b5D:\u2200p\u03b5N(p), R(p)=R(p), ID(p)\u2212D(p) I<\u03b4}. Let M be a connected component label map. For example M(p) may be equal to Cwhere C is a set of connected components and where Cis a connected component (k) in that set.","Note that there may be many components in a region; however, every pixel in the same component includes the same region label. When a UC component is referred to, reference is being made to a connected component in the UC region, for instance.","A meaningful component is a component whose area is larger than some threshold value, \u03b3. A large UC component, however, is most likely a meaningless component, for example, a part of a wall, a ceiling, or a floor. There are, however, some small-but-meaningful UC component such as human hair, a belt, and a cell phone because these objects tend to absorb infrared (IR) and are objects that should be kept for further processing. The trick is differentiating between meaningful UC components with other noisy small UC components. In general, the meaningful UC components are going to be found adjacent to large, meaningful FG components. From these observations, the system  is programmed to delete components based on the following rules:","Rule 1: Categorize as BG any FG connected component having a cross-sectional area less than a predetermined threshold area, \u03b3.","Rule 2: Categorize as BG any UC connected component having a cross-sectional area greater than \u03b3\u2032, where \u03b3\u2032 may be different than \u03b3.","Rule 3: Categorize as BG any UC connected component having a cross-sectional area less than \u03b3 and for which no adjacent component thereof includes a FG connected component having a cross-sectional area greater than \u03b3.","Note that categorizing FG or UC connected components as BG will have the result of ultimately removing those components when the BG is subtracted.","In preparation for image processing under other blocks, the system may, at or near block , find the center of mass (COM) of large FG connected components, such as a target subject, and compute the average depth value for each FG component. In other words, for a FG component C,",{"@attributes":{"id":"p-0060","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["COM","x"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}},"mo":"=","mfrac":{"mrow":[{"munder":{"mo":"\u2211","mrow":{"mi":"p","mo":"\u2208","msub":{"mi":["C","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"p"}}},{"mi":"area","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["C","i"]}}}]}}}},"br":{},"sub":"y"},{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"d","mrow":{"msub":{"mi":["avg","x"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"i"}}},"mo":"=","mfrac":{"mrow":[{"munder":{"mo":"\u2211","mrow":{"mi":"p","mo":"\u2208","msub":{"mi":["C","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"p"}}},{"mi":"area","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["C","i"]}}}]}}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}}},{"@attributes":{"id":"p-0062","num":"0061"},"figref":["FIG. 8","FIG. 7"],"b":["103","103","103","100","103","103","100","109","100"]},{"@attributes":{"id":"p-0063","num":"0062"},"figref":["FIG. 9","FIG. 8","FIG. 2","FIG. 2"],"b":["208","208","100"]},"For each of the FG components, the system  categorizes all the UC pixels that lie under the COM as BG, thus cleaning those portions from further processing within the UC region. The follow is example pseudo code for block :","For each pixel p\u03b5D such that y(p)<COM\/\/vertically under the COM point","If (R(p)==UC) then R(p)=BG: \/\/clean it=put it in BG region","End.","The purpose of block  is to help reduce errors caused by unexpected noise around the user and reduce processing time. Simultaneously, the system  is still able to keep a hair part, for instance, in the UC region for further processing in subsequent steps that the system  may execute, which are shown in .",{"@attributes":{"id":"p-0067","num":"0066"},"figref":["FIG. 10","FIG. 2"],"b":["210","100"]},"More particularly, each point of an image in 2D space can be mapped one to one with a ray in 3D space that goes through the camera position. Given a 2D image plane with basis vectors ({right arrow over (s)},{right arrow over (t)}) and a 3D space ({right arrow over (i)},{right arrow over (j)},{right arrow over (k)}), the 2D point to 3D ray mapping relation is:",{"@attributes":{"id":"p-0069","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"r","mo":"=","mrow":{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":["r","i"]}}},{"mtd":{"msub":{"mi":["r","j"]}}},{"mtd":{"msub":{"mi":["r","k"]}}}]}},{"mrow":[{"mrow":[{"mo":["[","]"],"mrow":{"msub":[{"mover":{"mi":"s","mo":"\u2192"},"mi":"ijk"},{"mover":{"mi":"t","mo":"\u2192"},"mi":"ijk"}],"mo":["\u2062","\u2062"],"mrow":{"mi":"f","mo":"\u2297","msub":{"mover":{"mi":"w","mo":"\u2192"},"mi":"ijk"}}}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"u"}},{"mtd":{"mi":"v"}},{"mtd":{"mn":"1"}}]}}],"mo":"\u00b7"},{"mi":"P","mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"u"}},{"mtd":{"mi":"v"}},{"mtd":{"mn":"1"}}]}}}],"mo":"="}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}},"br":{},"sub":["ijk","ijk","ijk "]},"Consider a point X in 3D space {{right arrow over (i)},{right arrow over (j)},{right arrow over (k)}}. Let {right arrow over (x)}, and {right arrow over (x)}be homogeneous coordinates of X in the reference image plane and the desired image plane as shown in . Let P, and Pbe mapping matrices of the reference camera and the desired camera. It has been proven that the warping equation between {right arrow over (x)}, and {right arrow over (x)}is:",{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mover":{"mi":"x","mo":"\u2192"},"mi":"d"},"mo":"=","mrow":{"msubsup":{"mi":["P","d"],"mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mfrac":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"msub":[{"mi":["P","r"]},{"mover":{"mi":"x","mo":"\u2192"},"mi":"r"}],"mo":"\u2062"}},{"mi":"d","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mover":{"mi":"x","mo":"\u2192"},"mi":"r"}}}]},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mover":{"mi":"C","mo":"\u2192"},"mi":"r"},{"mover":{"mi":"C","mo":"\u2192"},"mi":"d"}],"mo":"-"}}},{"msub":[{"mi":["P","r"]},{"mover":{"mi":"x","mo":"\u2192"},"mi":"r"}],"mo":"\u2062"}],"mo":"+"}}}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{},"sub":["r","r"]},{"@attributes":{"id":"p-0072","num":"0071"},"figref":["FIG. 11","FIG. 10","FIG. 12","FIG. 11"]},{"@attributes":{"id":"p-0073","num":"0072"},"figref":["FIG. 13","FIG. 12","FIG. 2"],"b":"212"},"The BGH is a frame that contains only background (BG) pixels. The frame is built in an accumulated fashion from the previous frame. At block  of , for each UC pixel, if the BGH is available for the pixel, the system  compares the RGB value of the pixel with the corresponding one in the BGH. If the BGH of the pixel is unavailable for some reason, the system  searches for the BG H of a neighbor of the pixel and compares the two. If they match, the system  sets the pixel to BG. Accordingly, one function for categorizing the UC pixels may be based on color dissimilarity between UC pixels and neighbor pixels of the colored image and based on color dissimilarity between the UC pixels and neighbor pixels of the BGH.",{"@attributes":{"id":"p-0075","num":"0074"},"figref":["FIG. 14","FIGS. 11-13","FIG. 15","FIGS. 11-13"],"b":["214","214","100","100","100","100"]},{"@attributes":{"id":"p-0076","num":"0075"},"figref":["FIG. 16","FIG. 15","FIG. 2"],"b":["216","216","100"]},"Dilation is one of the two basic operators in the area of mathematical morphology, the other being erosion. It is typically applied to binary images, but there are versions that work on grayscale images. The basic effect of the mathematical morphology operator on a binary image is to gradually enlarge the boundaries of regions of foreground pixels (i.e. white pixels, typically). Thus areas of foreground pixels grow in size while holes within those regions become smaller.",{"@attributes":{"id":"p-0078","num":"0077"},"figref":["FIG. 17","FIG. 16","FIG. 2"],"b":["218","218","100"]},"The purpose of detecting the FG fringe and merging it into the UC region is as follows. Due to the tolerance in registration (or warping between the depth information and color image), depth resolution, interpolation and flickering artifacts, the region map edges shown in  may not be good cutting edges. In fact, there is usually a small mismatch between region map edges and the RGB edges, assuming the RGB edges lie close to the region map edges. With the above opening operator, the system  can narrow down the area along the edge to perform further processing to get a FG-BG cut at the RGB edges. This helps significantly reduce processing time.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIG. 18","FIG. 17"],"b":["220","100","100"],"sub":["BG ","BG","BG","BG"],"sup":["(t)","(t-1)","(t)","(t)","(t) "]},{"@attributes":{"id":"p-0081","num":"0080"},"figref":["FIG. 19","FIG. 18","FIG. 2","FIG. 20","FIG. 19","FIG. 2"],"b":["222","222","222","100","100"]},{"@attributes":{"id":"p-0082","num":"0081"},"figref":["FIG. 21","FIG. 20"],"b":["224","100","224"]},"Block  repeats this cleaning step because the system  expanded the UC region around the region map edges at block , and after block , there may still exist some unresolved UC pixels. Because, after the next step, the UC pixels are set to FG (to recover the top part of the hair), so block  helps reduce errors caused by unexpected noisy edges around the user without affecting the hair part (or other reflectance-sensitive area).",{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIG. 22","FIG. 21","FIG. 2","FIG. 23","FIG. 21","FIG. 2","FIG. 23"],"b":["226","226","228"]},"To execute block , the system  may remove very small remaining UC connected components, also referred to as fragments, but keep and smoothen the edges of big UC connected components such as part or all of the hair of a target subject. A 7\u00d77 support window may be applied by the median filter to the UC connected components, for instance, or another suitably-sized window may be applied. Then the UC region may be merged with the FG region. Pseudo code to be executed by the system  at block  may include:",{"@attributes":{"id":"p-0086","num":"0085"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003","For each pixel p in UC region {"]},{"entry":[{},"\u2003\u2003\u2003Count = O;"]},{"entry":[{},"\u2003\u2003\u2003For each pixel pin the NxN support window around pixel p {"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003If R(p) = UC, count++;"]},{"entry":[{},"\u2003\u2003\u2003}"]},{"entry":[{},"\u2003\u2003\u2003If (count<N*N\/2), R(p) = BG;"]},{"entry":[{},"\u2003\u2003\u2003\u2002Else R(p) = FG;"]},{"entry":[{},"}."]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},{"@attributes":{"id":"p-0087","num":"0086"},"figref":["FIG. 24","FIG. 23"],"b":["228","139","228","100","224","100","100","100"]},{"@attributes":{"id":"p-0088","num":"0087"},"figref":["FIG. 25","FIG. 2","FIG. 26","FIG. 28","FIG. 2","FIG. 27","FIG. 2","FIG. 28","FIG. 27","FIG. 2"]},"At block  of , the system  may detect reset conditions, which is a block available to the system  throughout the background subtraction process. If a reset condition is detected, a reset flat is set to true. A reset condition may include, but not be limited to the following examples. (1) The system  may receive an indication that the camera is shaken, which makes the background history (BGH) useless. (2) The target subject may be too close to the camera , which causes a large IR saturation area, resulting in a large unknown or background area, wherein the system  may mistakenly update the BGH. (3) The user may move from the BG to the FG. When the target subject was in the background (BG), the BGH of corresponding pixels was updated. When the target subject moves into the FG of the scene, the BGH behind the target subject is no longer correct and needs to be reset. (4) The system  may detect a significant lighting change, which also makes the BGH useless. At block  of , the system  may detect whether the reset flag has been set. If it has, the system  resets the background (BG) mask and the BGH at block .",{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 29","b":["2900","101","2900","2902","2900","2900","116"]},"In a networked deployment, the computer system  may operate in the capacity of a server or as a client-user computer in a server-client user network environment, or as a peer computer system in a peer-to-peer (or distributed) network environment. The computer system  may also be implemented as or incorporated into various devices, such as a personal computer or a mobile computing device capable of executing a set of instructions  that specify actions to be taken by that machine, including and not limited to, accessing the Internet or Web through any form of browser. Further, each of the systems described may include any collection of sub-systems that individually or jointly execute a set, or multiple sets, of instructions to perform one or more computer functions.","The computer system  may include a processor , such as a central processing unit (CPU) and\/or a graphics processing unit (GPU). The Processor  may include one or more general processors, digital signal processors, application specific integrated circuits, field programmable gate arrays, digital circuits, optical circuits, analog circuits, combinations thereof, or other now known or later-developed devices for analyzing and processing data. The processor  may implement the set of instructions  or other software program, such as manually-programmed or computer-generated code for implementing logical functions. The logical function or any system element described. may, among other functions, process and\/or convert an analog data source such as an analog electrical, audio, or video signal, or a combination thereof, to a digital data source for audio-visual purposes or other digital processing purposes such as for compatibility for computer processing.","The computer system  may include a memory  on a bus  for communicating information. Code operable to cause the computer system to perform any of the acts or operations described herein may be stored in the memory . The memory  may be a random-access memory, read-only memory, programmable memory, hard disk drive or any other type of volatile or non-volatile memory or storage device.","The computer system  may also include a disk or optical drive unit . The disk drive unit  may include a computer-readable medium  in which one or more sets of instructions , e.g., software, can be embedded. Further, the instructions  may perform one or more of the operations as described herein. The instructions  may reside completely, or at least partially, within the memory  and\/or within the processor  during execution by the computer system . Accordingly, the BGH database described above in  may be stored in the memory  and\/or the disk u nit .","The memory  and the processor  also may include computer-readable media as discussed above. A \u201ccomputer-readable medium,\u201d \u201ccomputer-readable storage medium,\u201d \u201cmachine readable medium,\u201d \u201cpropagated-signal medium,\u201d and\/or \u201csignal-bearing medium\u201d may include any device that includes, stores, communicates, propagates, or transports software for use by or in connection with an instruction executable system, apparatus, or device. The machine-readable medium may selectively be, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, device, or propagation medium.","Additionally, the computer system  may include an input device , such as a keyboard or mouse, configured for a user to interact with any of the components of system . It may further include a display , such as a liquid crystal display (LCD), a cathode ray tube (CRT), or any other display suitable for conveying information. The display  may act as an interface for the user to see the functioning of the processor , or specifically as an interface with the software stored in the memory  or the drive unit .","The computer system  may include a communication interface  that enables communications via the communications network . The network  may include wired networks, wireless networks, or combinations thereof. The communication interface  network may enable communications via any number of communication standards, such as 802.11, 802.17, 802.20, WiMax, cellular telephone standards, or other communication standards.","Accordingly, the method and system may be realized in hardware, software, or a combination of hardware and software. The method and system may be realized in a centralized fashion in at least one computer system or in a distributed fashion where different elements are spread across several interconnected computer systems. Any kind of computer system or other apparatus adapted for carrying out the methods described herein is suited. A typical combination of: hardware and software may be a general-purpose computer system with a computer program that, when being loaded and executed, controls the computer system such that it carries out the methods described herein. Such a programmed computer may be considered a special-purpose computer.","The method and system may also be embedded in a computer program product, which includes all the features enabling the implantation of the operations described herein and which, when loaded in a computer system, is able to carry out these operations. Computer program in the present context means any expression, in any language, code or notation, of a set of instructions intended to cause a system having an information processing capability to perform a particular function, either directly or after either or both of the following: a) conversion to another language, code or notation; b) reproduction in a different material form.","The above-disclosed subject matter is to be considered illustrative, and not restrictive, and the appended claims are intended to cover all such modifications, enhancements, and other embodiments, which fall within the true spirit and scope of the present disclosure. Thus, to the maximum extent allowed by law, the scope of the present embodiments are to be determined by the broadest permissible interpretation of the following claims and their equivalents, and shall not be restricted or limited by the foregoing detailed description. While various embodiments have been described, it will be apparent to those of ordinary skill in the art that many more embodiments and implementations are possible within the scope of the above detailed description. Accordingly, the embodiments are not to be restricted except in light of the attached claims and their equivalents."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["A more particular description of the disclosure briefly described above will be rendered by reference to the appended drawings. Understanding that these drawings only provide information concerning typical embodiments and are not therefore to be considered limiting of its scope, the disclosure will be described and explained with additional specificity and detail through the use of the accompanying drawings.",{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0011","num":"0010"},"figref":["FIG. 5","FIG. 3"]},{"@attributes":{"id":"p-0012","num":"0011"},"figref":["FIG. 6","FIG. 3","FIG. 2"],"b":"202"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 7","FIG. 6","FIG. 2"],"b":"204"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 8","FIG. 7"]},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 9","FIG. 8","FIG. 2"],"b":"208"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 10","FIG. 2"],"b":"210"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 11","FIG. 10"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 12","FIG. 11"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 13","FIG. 12","FIG. 2"],"b":"212"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 14","FIGS. 11-13"],"b":"214"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIG. 15","FIGS. 11-13"],"b":"214"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 16","FIG. 15","FIG. 2"],"b":"216"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 17","FIG. 16","FIG. 2"],"b":"218"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 18","FIG. 17"],"b":"220"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 19","FIG. 18","FIG. 2"],"b":"222"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":["FIG. 20","FIG. 19","FIG. 2"],"b":"222"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 21","FIG. 20"],"b":"224"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 22","FIG. 21","FIG. 2"],"b":"226"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 23","FIG. 21","FIG. 2"],"b":"226"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 24","FIG. 23"],"b":"228"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 25","FIG. 2"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 26","FIG. 25","FIG. 2"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 27","FIG. 2"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 28","FIG. 27","FIG. 2"]},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 29"}]},"DETDESC":[{},{}]}
