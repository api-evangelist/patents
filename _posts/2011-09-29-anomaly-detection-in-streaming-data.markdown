---
title: Anomaly detection in streaming data
abstract: An example method for anomaly detection in streaming data includes applying statistical analysis to streaming data in a sliding window. The method also includes extracting a feature. The method also includes determining class assignment for the feature using class conditional probability densities and a threshold.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09218527&OS=09218527&RS=09218527
owner: Hewlett-Packard Development Company, L.P.
number: 09218527
owner_city: Houston
owner_country: US
publication_date: 20110929
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","DETAILED DESCRIPTION"],"p":["Data collected from sensors in real-time applications is commonly referred to as time series data, streaming data, and\/or data streams, and represents a substantially continuous flow of data. For example, modern industrial facilities often have multiple sensors to gather a wide variety of data types for monitoring the state or condition of various operations at the facility. The streaming data may be analyzed to detect \u201cevents\u201d and thus warn of impending failures. By way of illustration, oil and gas production equipment can be highly specialized (and even custom manufactured for a site). Repair and replacement is often expensive, particularly for offshore assets. Early detection and prevention of problems can result in higher production and lower costs.","Oil and gas production equipment is often located in remote areas, offshore, or in extremely hot, cold, or even dangerous environments. For all of these reasons, it is often desired in the oil drilling and production industry to utilize automated surveillance systems to monitor various stages of production and aid the operators with ensuring production with few, if any, interruptions.","The oil and gas industry often equips oil and gas wells with thousands of sensors and gauges to measure flow rates, pressure, and temperature, among other parameters. Any variations in flow rate, pressure and\/or temperature may indicate an issue that needs to be addressed in order to avoid a partial or even complete shutdown of the oil well, which can lead to lost productivity and lower profit margins.","But data collected from these sensors can be \u201cnoisy,\u201d the data often does not have a constant amplitude, and the data can be plagued by shifts in the mean. These aspects of the data make it difficult to accurately model the data stream and extract relevant events. In addition, quickly detecting changes can be difficult in a real-time or \u201conline\u201d environment, due to the reliance on intensive mathematical analysis which can take significant time to compute.","The supply of petroleum products that can be easily reached and refined is finite, which motivates oil and gas producers to extract as much as possible from a given well. Therefore, it is desirable to maintain a continuous, or substantially continuous flow to enhance production. Any interruptions to the flow can result in lost production capacity and the associated costs.","Modern oil fields are equipped with thousands of sensors and gauges to measure various physical and chemical characteristics of the surrounding terrain, in addition to the production and distribution systems. Continuous streams of sensor readings are analyzed to understand the various stages of oil production and distribution.","By way of illustration, after drilling into the earth's subsurface to tap oil deposits, the bore well is managed to yield maximum capacity. Several aspects introduce tremendous variability into the production process. An often monitored variable in oil and gas production is the flow rate. Higher flow rates with fewer disruptions, result in greater yields.","Two common occurrences in oil production that cause disruptions to flow rates are known as \u201cslugging\u201d and \u201cchurn.\u201d Slugging refers to turbulent flows where gas bubbles coalesce, expand, and collapse continuously. Changes in fluid composition from wholly liquid to wholly gaseous over time, can lead to churn which is a frequent cause of flow disruption. Some factors that affect flow rates include fluid composition, oil viscosity, compressibility, specific gravity, specific gravity of water, and solids content, among others. These factors can produce varying flow regimes.","Flow rates along a bore well tend to be oscillatory, which are detrimental to the oil extraction process, by leading to fluctuations along the shafts affecting the integrity of the equipment. Detecting changes to flow patterns can be accomplished, for example, by monitoring streaming data for 1) high amplitude, high oscillation, 2) low amplitude, high oscillation, 3) low oscillation with pseudo-periodic behavior, 4) normal flow rate followed by a jump, and anomalous flows that are some combination of the characteristics described in 1-4 above. Monitoring for oscillation can be used for early detection and prediction of churn in flow rates.","Detecting anomalies in data streams (such as slugging and churn in flow regimes) can be used to issue an alert in advance of an event so that corrective action can be taken before the event leads to a disruption. Of course, analyzing data streams are not limited to the oil and gas industry and the techniques described herein have application in a wide variety of fields.","Briefly, the systems and methods described herein enable anomaly detection in streaming data, using frequency domain analysis and pattern recognition. In an example, program code stored on non-transient computer-readable media is executable by a processor to apply statistical analysis to streaming data in a sliding window. A coefficient is extracted with maximum magnitude. The coefficient is then used to determine class assignment using class conditional probability densities.","Accordingly, the systems and methods described herein can be implemented to uncover hidden time varying periodicities in flow rate, which can be used to invoke nonlinear modeling techniques to separate high oscillation regimes (churn) from non-oscillating regimes (normal flow). The approaches described herein are computationally \u201clightweight,\u201d and thus can be implemented in an online setting (e.g., in near real-time) for process interdiction and correction. When sensors are networked, the systems and methods described herein may be extended to detect trends, patterns, affinities, and correlations across the network.","Before continuing, the terms \u201cincludes\u201d and \u201cincluding\u201d mean, but is not limited to, \u201cincludes\u201d or \u201cincluding\u201d and \u201cincludes at least\u201d or \u201cincluding at least.\u201d The term \u201cbased on\u201d means \u201cbased on\u201d and \u201cbased at least in part on.\u201d",{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 1","b":["100","100","110"]},"In an example, the computing device  may receive streaming data from one or more source , such as sensors -. For purposes of illustration, the sensors shown in  are used to gather flow rate data from oil well(s). However, it is noted that data streams are not limited to use in the oil and gas industry, and can include other sources. Other sources of streaming data may include weather data, vehicle traffic, network traffic for a data center, electricity for a smart grid, water measurements for a treatment facility, and even vitality data for a person or biological system, to name only a few examples of streaming data.","There is no limit to the type or amount of data that may be provided by a source. In addition, the content may include unprocessed or \u201craw\u201d data, or the content may undergo at least some level of processing. For example, data may be filtered prior to executing the operations described herein to reduce noise injected into the data stream which is not representative of actual data from the sensor(s).","The data stream may be accessed for online data processing by a computing device  configured as a server computer with computer-readable storage . Program code  executing on the computing device  may analyze the streaming data and issue alerts, e.g., indicative of a change in regime in the data stream. Program code  may also include interfaces to application programming interfaces (APIs) and related support infrastructure, including hosted monitoring services  which can be used to provide the alerts  to a facility operator or other customer based on the change in regime so that additional monitoring and\/or corrective action can be taken in a timely manner.","Although, it is noted that the operations described herein may be executed by program code  residing on a server device, other computing devices may also be implemented. Other computing devices may include, but are not limited to a personal computer, a tablet or other mobile device. In an example, mobile devices used on-site by an end-user  such as a facility operator may be implemented in conjunction with a \u201cback-end\u201d computer system having more processing capability, such as the server computer , or a plurality of server components in a data center or \u201ccloud computing\u201d environment.","The system  may also include a communication network , such as a local area network (LAN) and\/or wide area network (WAN). In one example, the network  includes the Internet or other mobile communications network (e.g., a 3G or 4G mobile device network). Network  may also provide greater accessibility for use in distributed environments, for example, where more than one source is providing the streaming data. The various sensor(s) and computing device(s) may be provided on the network  via a communication connection, such as via an Internet service provider (ISP). In this regard, access may be provided directly via the network , or via an agent, such as another network. Such an implementation may be particularly desirable where an operator is responsible for monitoring multiple, geographically distributed production sites, for example, in the oil and gas or other industries.","As mentioned above, the program code  may be executed by any suitable computing device to analyze data stream. In addition, the program code may analyze one or more than one data stream. The operations described herein are not limited to any specific implementation with any particular type of program code. In an example, the program code may be implemented in machine-readable instructions (such as but not limited to, software or firmware). The machine-readable instructions may be stored on a non-transient computer readable medium and are executable by one or more processor to perform the operations described herein. It is noted, however, that the components shown herein are provided only for purposes of illustration of an example operating environment, and are not intended to limit implementation to any particular system.","The program code executes the function of the architecture of machine readable instructions as self-contained modules. These modules can be integrated within a self-standing tool, or may be implemented as agents that run on top of an existing program code. Operations performed by executing the program code can be understood with reference to .",{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 2","b":["1","2"]},"More specifically, at  input signals for a training data set may be received for a first class. A window length is defined at , and windowed data is acquired at . Fourier transform analysis is applied at  and the coefficient having the largest magnitude is extracted at . The process may be repeated  for a second signal class. At , class conditional probability densities are estimated.","Then, given a stream of actual data at , a sliding window is defined at . Fourier transform analysis is applied at  and the coefficient having the largest magnitude is extracted at . At , the class conditional probability densities determined at  are used to determine class assignment for the actual data received at . The process may be repeated  for the entire stream by moving the window (e.g., one time unit).",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 3","b":["300","310","320","330"]},"Although, flow rate time-series is quasi-periodic, the data represents a narrow band signal. In contrast, the non-oscillatory regions do not have any discernible structure. Furthermore, different artifacts appear in the data, such as missing values, sudden drops to zero value or \u201csignal clipping\u201d (e.g., due to sensor malfunction or the sensor being turned off).","In order to fully analyze the signal, the time series is deconstructed into oscillatory and non-oscillatory segments, as shown in plots  and , respectively. It can be seen in plot  that the flow rates are periodic in the oscillatory regions. Examining the time series carefully, there are different regions of varying frequencies. This periodic structure in the flow-rate signal makes a case for examining the time-series via frequency domain methods.","Fourier analysis may be used for frequency domain analysis. In classical Fourier analysis, bandwidth is defined in relation to the Fourier transform. Bandwidth is a measure of the range of frequencies (spectrum), and is usually measured in Hertz (Hz). The Fourier transform of a function f(t) provides a view of the signature of data known as the frequency spectrum, but masks the relationship between frequencies and time over which the data is analyzed. In other words bandwidth is a global characteristic of the function.","To overcome this deficiency, the systems and methods described herein disclose a moving window Fourier transform. The windowed Fourier transform can be used to determine local bandwidth by analyzing the data over windows, similar to the short time Fourier transform (STFT). The STFT involves computing Fourier coefficients over windows of the time series.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 4","b":["400","410","420","420","420","425"]},"While the SIFT provides a mechanism to analyze frequencies over time, STFT is limited by conflict between time-frequency localization. The Heisenberg uncertainty principle states that time and frequency resolutions are inversely related, leading to the condition where analyzing a signal over longer windows compromises frequency resolution and vice-versa. Also in the STFT implementation, the window size is fixed which limits the frequency range.","As STFT cannot be used to find exact timing of regime shifts, an alternative approach based on dynamical systems is introduced herein to identify the time-varying periodic structures in the oscillatory segments of the data. In addition to the time-frequency spectrum shown in , the behavior of oscillations is examined by considering the underlying dynamical system which generated the time series.","The dynamical system is regulated by a set of parameters, and the evolution over time is known as the phase space of the process. The quasi-periodic regime is produced by a well-organized attractor. An \u201cattractor\u201d is defined herein in the context of dynamical systems, as a set towards which the process converges over time. An anomaly detector implementing this approach is able to identify the quasi-periodic region by an attractor. The properties of attractors in a reconstructed phase space can be understood using Takens time delay embedding theorem.","Takens theorem reconstructs the phase space with m-dimensional vectors with components sampled from the univariate time series having a time spacing of \u0394T. The parameters m and \u0394T are known as the embedding dimension and time delay, respectively. Both of these parameters are estimated using segments of the data stream that is known to include oscillations.","The embedding dimension, m, is larger than the expected correlation dimension of the attractor, so that the time series is fully unfolded into phase space. The time delay, \u0394T, is chosen such that components of the time delay vector are minimally correlated. Mutual information between a time series, and a time series delayed by \u0394T, can be found as a function of \u0394T. The time delay that produces the first local minimum can be chosen in mutual information.","The correlation dimension is a measure of the dimensionality of the space occupied by a set of points. The correlation dimension can be estimated using the Grassberger-Procaccia algorithm. In this algorithm, the correlation sum, C(r), is calculated as a fraction of pairs of points that are within a distance r of each other. In D dimensional space, as r decreases, C(r) tapers off proportional to r, and therefore:",{"@attributes":{"id":"p-0046","num":"0045"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"D","mo":"=","mrow":{"msub":{"mi":"lim","mrow":{"mi":"r","mo":"->","mn":"0"}},"mo":"\u2062","mfrac":{"mrow":[{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":["r","D"]}},{"mi":["log","r"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]}}}}}},"Plotting log C(r) versus log r reveals a linear relationship with a slope equal to D for small r. With finite data, the curve levels off for small r, because each point is the only point in a neighborhood of radius r. These quantities can be used in generating the recurrence plot, as a matrix of all the pairwise distances between the different vectors generated considering the lag and embedding dimension.","This approach enables the time series to be characterized in terms of the reconstructed phase space, which provides an estimate of the embedding dimension. The estimate of the embedding dimension enables the order (number of lags) of the linear model to be selected to approximate the flow rate time-series.","As previously discussed, STFT can be applied to achieve time-frequency localization to separate the flow-rate regimes. This is achieved by breaking the time-series into contiguous windows. The Fourier coefficients are computed in each window, and the Fourier coefficient with largest magnitude over each window is selected as a classification feature. This approach works well because the larger Fourier coefficients are highly correlated to the signal. The window slides through the time series, each shifted by one time unit-producing one feature value over every window.","But using STFT coefficients may have some drawbacks, including time-frequency resolution. To improve, local methods may be used. These approaches can be sensitive to the length of the window. For example, a small window size provides frequency features that describe the rising and falling edge of the quasi triangular wave in the oscillations. But a small window size may produce false alarms because it is only sensitive to linear behavior. A larger window near the period of the triangular wave can be used to reduce the number of false alarms because it is covering a range of frequencies over the period of the signal.","In another example, the time-frequency uncertainty can be managed by quantifying the structure of the time series. Quantifications are based on locally determined time features particular to the structure of the oscillatory signal. In the time domain, frequency can be defined as the time between consecutive peaks (or troughs), or the time between pairs of zero crossings. Zero crossings are time instances when the time series crosses zero, and this technique is capable of recognizing a single cycle of a periodic signal. This approach can be strengthened by considering amplitude-based features, in addition to the local period estimates.","In the oscillatory region, the time between local extrema is the half-period of a single oscillation, and hence can be considered an estimate of local frequency. As used herein, the term \u201cpeak\u201d is used to refer to both local maxima and minima. The ith peak is denoted P, and the time of this peak as t(P). The feature time between peaks is therefore \u0394t(P)=t(P)\u2212t(P).","Empirical distributions for t(P) can be seen in based on training data of known classes (oscillatory and non-oscillatory regimes). A threshold, based on likelihood ratio of empirical densities, can be determined using the training data. The samples between peaks can then be classified based on time differences relative to the threshold.","Peak-to-peak heights can be used as another discriminatory feature variable. The height of the ith peak is denoted h(P), and the height between peaks is \u0394h(P)=h(P)\u2212h(P). The classes have greater separation in this feature variable, thereby lowering the chances of misclassification. When a peak-to-peak feature is classified, all of the samples between peaks are classified to the same class. This grouping of sample classifications is more robust than the sample-by-sample approach, because oscillatory behavior is persistent over time. But this technique is also based on a constant amplitude. Therefore, both techniques may be combined to create a two-dimensional discriminant feature vector. An example algorithm to detect local extrema and classify the data based on their features is shown below as Algorithm 1:",{"@attributes":{"id":"p-0055","num":"0054"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"center"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":[{},"Algorithm 1 On-line classification based on peak features"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003","\u2003Given time series s, with N samples"]},{"entry":[{},"\u2003L = window length for peak detection"]},{"entry":[{},"Require: L odd"]},{"entry":[{},"\u2003\n\n"]},{"entry":[{},"\u2003Y = {y= (\u0394t(P), \u0394h(P)|j is the index of feature pairs"]},{"entry":[{},"\u2003in the training set}"]},{"entry":[{},"\u2003Set \u03b8 = threshold on |\u0394h(P)|"]},{"entry":[{},"\u2003i = 0"]},{"entry":[{},"\u2003for n = a + 1 to N \u2212 a do"]},{"entry":[{},"\u2003\u2003W= s to s"]},{"entry":[{},"\u2003\u2003if s== max(W) or s== min(W) then"]},{"entry":[{},"\u2003\u2003\u2003i = i + 1"]},{"entry":[{},"\u2003\u2003\u2003t(P) = n"]},{"entry":[{},"\u2003\u2003\u2003h(P) = s"]},{"entry":[{},"\u2003\u2003\u2003if i > 1 then"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u0394t(P) = t(P) \u2212 t(P) "]},{"entry":[{},"\u2003\u2003\u2003\u2003\u0394h(P) = h(P) \u2212 h(P) "]},{"entry":[{},"\u2003\u2003\u2003\u2003x= (\u0394t(P), \u0394h(P))"]},{"entry":[{},"\u2003\u2003\u2003\u2003if 1-dimensional feature space then"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003if \u0394h(P) > \u03b8 or \u0394h(P) > \u2212\u03b8 then"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003Assign samples from P to Pto the oscilla-"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003tory class"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003else"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003Assign samples from Pto Pto the non-"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003oscillatory class"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003end if"]},{"entry":[{},"\u2003\u2003\u2003\u2003else if 2-dimensional feature space then"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\n\n"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003Assign samples from P to Pto the class of"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003y"]},{"entry":[{},"\u2003\u2003\u2003\u2003end if"]},{"entry":[{},"\u2003\u2003\u2003end if"]},{"entry":[{},"\u2003\u2003end if"]},{"entry":[{},"\u2003end for"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 5","b":"500"},"Although the peak-to-peak height feature provides separation between classes, the latency can be reduced by considering differences between adjacent samples, referred to herein as sample differencing. A threshold may be selected based on the likelihood ratio of the empirical probability density functions (EPDF) of the differences \u0394s=s\u2212s, given a discrete time series s at time n. An example algorithm for implementing online sample differencing is shown below as Algorithm 2:",{"@attributes":{"id":"p-0058","num":"0057"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},{},"Algorithm 2 On-line classification based on sample "]},{"entry":[{},{},"differences"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u2003","Given time series s, with N samples"]},{"entry":[{},{},"W = window size for grouping"]},{"entry":[{},{},"M = number of samples in group that must be labeled"]},{"entry":[{},{},"oscillatory"]},{"entry":[{},{},"Y = {y= (\u0394s, \u0394s)|j is the index of feature pairs in"]},{"entry":[{},{},"the training set}"]},{"entry":[{},{},"Feature pairs are labeled with 1 if oscillatory and with 0 if"]},{"entry":[{},{},"non-oscillatory"]},{"entry":[{},{},"Set \u03b8 = threshold on |\u0394s|"]},{"entry":[{},{},"for n = 3 to N do"]},{"entry":[{},{},"\u2003\u0394s= s\u2212 s"]},{"entry":[{},{},"\u2003\u0394s = s \u2212 s"]},{"entry":[{},{},"\u2003x= (\u0394s, \u0394s)"]},{"entry":[{},{},"\u2003if 1-dimensional feature space then"]},{"entry":[{},{},"\u2003\u2003if \u0394s> \u03b8 or \u0394s< \u2212\u03b8 then"]},{"entry":[{},{},"\u2003\u2003\u2003Set C= 1"]},{"entry":[{},{},"\u2003\u2003else"]},{"entry":[{},{},"\u2003\u2003\u2003Set C= 0"]},{"entry":[{},{},"\u2003\u2003end if"]},{"entry":[{},{},"\u2003else if 2-dimensional feature space then"]},{"entry":[{},{},"\u2003\u2003\n\n"]},{"entry":[{},{},"\u2003\u2003Set C= binary label of training feature vector y"]},{"entry":[{},{},"\u2003end if"]},{"entry":[{},{},"\u2003if n > 2W then"]},{"entry":[{},{},"\u2003\u2003\n\n"]},{"entry":[{},{},"\u2003\u2003then"]},{"entry":[{},{},"\u2003\u2003\u2003Classify s as oscillatory"]},{"entry":[{},{},"\u2003\u2003else"]},{"entry":[{},{},"\u2003\u2003\u2003Classify s as non-oscillatory"]},{"entry":[{},{},"\u2003\u2003end if"]},{"entry":[{},{},"\u2003end if"]},{"entry":[{},{},"end for"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},"It is noted that sample differencing may be sensitive to noise in the data stream. If high oscillations persist over a long period, the approach can be strengthened by waiting for a set of differences around a point in question to satisfy the threshold. Although waiting may introduce latency (in order to observe future samples before making a decision), the number of points observed are also fewer compared to the peak-to-peak method.","At the peaks of the oscillatory region, the consecutive differences of the signal decrease in magnitude. Therefore, these areas are prone to missed detections. To reduce missed detections, a small number of samples M may be selected for a window of W samples such that M<W satisfies the threshold. At the onset of oscillation, only future samples satisfy the threshold, while at the end of oscillation only past samples satisfy the threshold.","In an example, two windows may be used to reduce or altogether prevent failed detection at the edges. One window includes the sample in question and the next W\u22121 samples, while the other window includes the sample in question and the previous W\u22121 samples. A warning is issued if either window contains M or more points above the threshold, with a latency of W\u22121 samples.","The selection of M is a tradeoff between missed detections and false alarms. If M is low, warnings can be triggered by only a few points, therefore M can be selected to include a majority of points in the window. For example, a two-thirds majority may be used. The sample differences procedure is controlled by two parameters: window size (W), and the adjacent differences parameter (M).","The discussion heretofore, focused on the characterization of the high oscillation region of the signal via a single and bivariate features extracted from the data. These features include the amplitude of the Fourier coefficients in a local neighborhood, the peak-to-peak time \u0394t(P) and height \u0394h(P), and the sample differences \u0394s. The Fourier coefficients and sample differences are determined from current and previous data, and therefore can be used to classify on a sample-by-sample basis in real time.","Training sets of known classes were used to generate a distribution of the desired feature for each class. -show the distributions for each of the features. The distributions are used to establish a threshold, \u03b8 on the feature. It can be seen that the distribution of peak-to-peak height has the best separation between classes for this data set. Therefore, this feature may be used in the single-feature case, rather than time between peaks which has overlapping oscillatory and non-oscillatory distributions. But time between peaks may be used for other data sets. The performance of the Algorithms 1 and 2 can be compared using the receiver operating characteristic (ROC) curves in .",{"@attributes":{"id":"p-0065","num":"0064"},"figref":["FIG. 7","FIG. 7"]},"The ROC curves were generated by testing a range of thresholds, including the Maximum Likelihood (ML) threshold obtained from EPDF estimates. It is noted that although the ML threshold does not offer the best performance, the ML threshold does provide a reasonably good estimate. From the ROC, in the optimal case, peak-to-peak height outperforms both sample differences and the maximum Fourier coefficient.","The result clearly follows from the large separation between the distributions of peak-to-peak heights. The tradeoff is latency in determining peak-to-peak features. The TPR, FPR, and total error rate can be seen in Table 1.",{"@attributes":{"id":"p-0068","num":"0067"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"21pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}},{"entry":[{},{},{},"Error",{},{}]},{"entry":["Features","Class. Method","Parameters","Rate","TPR","FPR"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"Peak-to-Peak Features"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"21pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["\u0394h(P)","Threshold","\u03b8 = 11","1.9%","97.2%","1.7%"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"Sample Differences"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"6"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"21pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u0394sn","Threshold","\u03b8 = 0.44 W = 1 M = 1","5.2%","88.9%","3.1%"]},{"entry":{"@attributes":{"namest":"1","nameend":"6","align":"center","rowsep":"1"}}}]}}]}}},"The values in Table 1 were calculated using testing data of approximately 62,000 samples. Table 1 compares the performance of single, and multi-feature classifiers. Column 1 is the feature(s) used: column 2 is the classification method, column 3 is the tuning parameter(s) of the classifier scheme, and column 4 is the error rate which is equal to the ratio of misclassifications of both types to total number of samples. The threshold was set at 11, very close to the oscillatory heights as can be seen in . In doing so, the oscillatory training data represents the testing values well. By setting a tight threshold, false positives can be avoided.","Before continuing, it should be noted that the examples described above are provided for purposes of illustration, and are not intended to be limiting. Other devices and\/or device configurations may be utilized to carry out the operations described herein.",{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 8","b":"800"},"Operation  includes applying statistical analysis to streaming data in a sliding window. Operation  includes extracting a feature. For example, when STFT is implemented, the feature may be the coefficient with maximum magnitude. For example, the statistical analysis may include applying Fourier transform to the streaming data to determine the coefficient, squaring the coefficient and ordering coefficients from largest to smallest, and then moving the sliding window (e.g., by one unit) and repeating. But statistical analysis is not limited to STFT, and may also include determining features using peak-to-peak and\/or sample differences techniques, as discussed in more detail above.","Operation  includes determining class assignment for the feature using class conditional probability densities. In an application example, the class assignment indicates a flow regime, and may be used to identify an anomaly in a flow regime (e.g., in an oil or gas production environment).","The operations shown and described herein are provided to illustrate example implementations. It is noted that the operations are not limited to the ordering shown. Still other operations may also be implemented.","Further operations may include defining a size of the sliding window. Operations may also include estimating the class conditional probability densities for two classes. Operations may also include classifying the streaming data based on peak features and\/or classifying the streaming data based on sample differences.","The operations may be implemented at least in part using an end-user interface (e.g., web-based interface). In an example, the end-user is able to make predetermined selections, and the operations described above are implemented on a back-end device to present results to a user. The user can then make further selections. It is also noted that several of the operations described herein may be automated or partially automated.","It is noted that the examples shown and described are provided for purposes of illustration and are not intended to be limiting. Still other examples are also contemplated."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
