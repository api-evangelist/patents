---
title: Method and system for queuing a request by a processor to access a shared resource and granting access in accordance with an embedded lock ID
abstract: A hardware-based method is provided for allocating shared resources in a system-on-chip (SoC). The SoC includes a plurality of processors and at least one shared resource, such as an input/output (IO) port or a memory. A queue manager (QM) includes a plurality of input first-in first-out memories (FIFOs) and a plurality of output FIFOs. A first application writes a first request to access the shared resource. A first application programming interface (API) loads the first request at a write pointer of a first input FIFO associated with the first processor. A resource allocator reads the first request from a read pointer of the first input FIFO, generates a first reply, and loads the first reply at a write pointer of a first output FIFO associated with the first processor. The first API supplies the first reply, from a read pointer of the first output FIFO, to the first application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08918791&OS=08918791&RS=08918791
owner: Applied Micro Circuits Corporation
number: 08918791
owner_city: Sunnyvale
owner_country: US
publication_date: 20110310
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION","Functional Description"],"p":["1. Field of the Invention","This invention generally relates to an integrated circuit (IC) system-on-chip (SoC) and, more particularly, to a system and method for allocating SoC shared resources between processors using a hardware queue manager and resource allocator.","2. Description of the Related Art","A system may include multiple central processing units (CPUs), also referred to as processors, cores, or microprocessors, running in the same system. A single SoC may have multiple systems or a system may include multiple SoCs, with multiple CPUs on each SoC. Due to contention between CPUs, it is difficult to share the same resource (e.g., a memory or input\/output (IO) port). Because of this issue, a conventional system is setup to have IO resource dedicated to each CPU. For example, if the SoC has two cores and two peripheral component interconnect (PCI) ports, each PCI port is dedicated to a corresponding CPU. Thus, even though there are two physical PCI ports available in the system, effectively only one is available for a given CPU. As another example, an SoC can have four CPU cores, 4 PCIe (PCI express) interfaces, 3 universal serial bus (USB) interfaces, and 4 Ethernet interfaces, where each interface is dedicated to a different CPU.","As today's applications require more computing power and IO data centric processing, there is commercial pressure to reduce the die size of the semiconductor device, increase performance, reduce cost, reduce power, and increase throughput. It would be advantageous if the number of CPU dedicated resources could be reduced by having the CPUs share. However, because CPUs run different applications, which are generally not aware of the usage of other CPU's applications, it is difficult to share resources. Further, even though there are multiple IOs in the system, the dedication of resources to specific CPUs introduces an overall restriction in system usage if each application cannot efficiently perform using only one dedicated IO resource.","It would be advantageous if a simple hardware mechanism could be used to establish resource access priority, with software-based locking to support resource grants and tracked resource usage.","The system and method described herein permit resources to be efficiently shared and accessed by all the central processing units (CPUs) of a system-on-chip (SoC). As a result, each CPU has system wide access, able to use any shared resource, as long as it follows certain rules to protect the resource. For example, if there is only one serial advanced technology attachment (SATA) port in the system, and a SATA disk (memory) is attached to a system of multiple processor cores, it becomes possible to have each core boot from a different operating system (OS) in a different partition of the same SATA disk. As another example, if there are two Ethernet ports available in a system with two CPUs, both ports can be used by both CPUs. One CPU can process a certain kind of network traffic (i.e. control traffic) received via both ports, and the other core can process a different kind of network traffic (i.e. data traffic) received via both ports. Such an arrangement improves system load balancing.","Accordingly, a hardware-based method is provided for allocating shared resources in a SoC. The method begins with an SoC having a first plurality of processors and at least one shared resource, such as an input\/output (IO) port or a memory. A queue manager (QM) is provided with a first plurality of input first-in first-out memories (FIFOs) and a first plurality of output FIFOs. A resource allocator is enabled as a sequence of software instructions executed by an allocation processor. A first application, enabled as a sequence of software instructions stored in memory and executed by a first processor, writes a first request to access the shared resource. A first application programming interface (API), enabled as a sequence of software instructions stored in memory and executed by a first processor, loads the first request at a write pointer of a first input FIFO associated with the first processor. The resource allocator reads the first request from a read pointer of the first input FIFO, generates a first reply responsive to the first request, and loads the first reply at a write pointer of a first output FIFO associated with the first processor. The first API supplies the first reply, from a read pointer of the first output FIFO, to the first application, and the first application performs a function in response to the first reply. If the first reply is a resource grant, the first application would access the shared resource as instructed in the reply. The QM enqueues the output FIFOs and dequeues the input FIFOs using a simple hardware-based algorithm.","Additional details of the above-described method and an SoC with a hardware-based system for allocating shared resources are provided below.",{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 1","b":["100","102","104","106","106","0","106","102","105","105","106","104","105","108","108","106"],"i":"n "},"The system  includes a queue manager (QM)  with a first plurality of input first-in first-out memories (FIFOs)  and a first plurality of output FIFOs . Shown are input FIFOs - through -and output FIFO - through , where m is an integer variable not limited to any particular value. The QM  dequeues requests from input FIFOs  and enqueues replies into output FIFOs . That is, the QM  determines the order in which the FIFOs are loaded with replies and unloaded with requests. In one simple aspect, the QM is a hardware logic state machine device that uses a round robin method of loading and unloading FIFOs. Alternatively, some FIFOs (i.e. some processors) may be given priority over other FIFOs. A variety of allocation schemes are known in the art that can be implemented in hardware. Hardware-based FIFO allocation decisions are a de facto form of resource allocation, assuming that shared resource grants are made on a first-come first-serve basis. Hardware-based resource allocation is both simple and fast.","A first application -, enabled as a sequence of software instructions stored in the memory  and executed by a first processor -, writes a first request to access the shared resource. A first application programming interface (API) -, enabled as a sequence of software instructions stored in the memory  and executed by a first processor -, loads the first request at a write pointer of a first input FIFO - associated with the first processor. That is, the requests is loaded into the first input FIFO tail. A resource allocator  reads the first request from a read pointer of the first input FIFO -. That is, the request is read from the first input FIFO head. The resource allocator  generates a first reply responsive to the first request, and loads the first reply at a write pointer of a first output FIFO - associated with the first processor -. Then, the first API - supplies the first reply, from a read pointer of the first output FIFO -, to the first application -, and the first application - performs a function in response to the first reply. For example, the first application - may access the shared resource if the first reply is a grant. Alternatively, if a grant is not received, the first application - may perform a task not involving the shared resource, or the first application may wait until access is permitted.","The first application - may write a request such as a go\/no-go (yes or no) request for resource access or a scheduled time request for resource access. Further the request may include a length of time request for resource access (how long the resource will be used by the first application) or a resource access availability estimate (when will the resource be available). The request for a first resource (e.g., a first IO port) may also include a request for access to a comparable resource (e.g., a second IO port), in the event that the first resource is not available. The request may also include combinations of the above-mentioned parameters.","The resource allocator  includes an allocation microprocessor , and an allocation application , enabled as a sequence of instructions stored in the memory  and executed by the allocation microprocessor , for tracking shared resource status and allocations. As noted above, in a simple aspect of the system , the QM hardware allocation scheme is the de facto means of allocating resources to requesting applications. However, the allocation application  may be designed to give resource access priority to some applications (or some processors), over other applications. Further, it should be understood that although the QM may be seen as the de {octo resource allocator, the allocation application  is required to track available resources, in-use resources, and scheduled resource use.","The resource allocator  may generate a reply such as a grant with an embedded lock ID, where the lock ID is required for resource access, or a grant with the embedded lock ID and scheduled use time. That is, the reply may be a grant for a certain time in the future. The reply may also be a grant with the embedded lock ID and a use time limit, a reply with a resource availability estimate, a grant to a compatible resource, a rejection, or combinations of the above-mentioned replies. A resource availability estimate may be an estimate of when the resource allocator will likely send a grant to a requesting application, or it may be prediction of when a grant will be made available upon a subsequent request. Note: this is not an exhaustive list of every type of possible reply.","In one aspect, a second application -, enabled as a sequence of software instructions stored in memory  and executed by a second processor -, writes a second request to access the shared resource. A second API -, enabled as a sequence of software instructions stored in memory  and executed by the second processor -, loads the second request at a write pointer of a second input FIFO -, associated with the second processor.","The resource allocator  reads the second request from a read pointer of the second input FIFO -, subsequent to reading the first request, generates the first reply granting the first application - access to the shared resource, and generates a second reply rejecting the second request, in response to granting the first request. The second reply is returned via the second output FIFO -.","In another aspect, the first application - writes a third request terminating granted access to the shared resource, and the first API - loads the third request at the write pointer of the first input FIFO -. The resource allocator  reads the third request from the read pointer of the first input FIFO - and sends a notice to the second application - that the shared resource is available. In one aspect, the notice that the shared resource is available also includes a grant to access the shared resource.","In another aspect not explicitly shown, multiple applications associated with the same processor can share a resource. Similar to the example described above, a set of input and output FIFOs is assigned to each application running in a single processor, and the resource allocator manages a resource shared between the applications.",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 2","b":["110","1","1","1","2","111","1","111","2","1","1","1","2","1","0","1","2","0","2","1","2","2","2","2","2"],"i":["n ","n "]},"The system of  makes it possible to protect and share the IO, and other shared resources in a system running multiple CPU cores, or multiple applications running on the same core. As shown, an SoC may be comprised of multiple CPU cores. Each core can have its own operating system and set of applications. It is desirable that each OS and the applications of each core have all the IO resources accessible and usable by them. This introduces a challenge of protecting and synchronizing the resources.","To enable such accessibility, a QM (queue manager) block is used. The QM is purely a hardware-based queue management and arbitration device\u2014no software is involved in managing messages in the queue. All the allocation software  has to do is enqueue and dequeue messages from the queue based upon the hardware provided pointers. The low power small scale resource allocator microcontroller  is able to use the QM block  to enqueue\/dequeue from the same queues as main CPUs. The number of messages residing in the queues is only limited by how much physical memory is available in system. That means that the number of locks can be defined in terms of thousands as opposed to the (at most) dozen locks enabled in a conventional system.","A software library in the allocation application  is used to acquire the lock, specify the timeout, and release the lock. The system is as simple as an application taking the lock from the resource allocator, using the resource, and releasing it. All an application needs to do is to call the API for all these operations. The application receives a reply, determines whether it received the lock or not, and if not, formulates a response. For example, the application can retry, or wait for the lock, or wait a period of time before retrying.",{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 3"},"The software APIs may be arranged as follows:","int sl_create(u32 lock_id, lockopt_t options);","int sl_lock(u32 lock_id, u32 timeout, u8 block);","void sl_unlock(u32 lock_id);","void sl_delete(u32 lock_id);","int sl_setLockOpt(u32 lock_id, lockopt_t options);","lockopt_t sl_getLockOpt(u32 lock_id).","As shown in the block diagram, the application calls the \u2018create\u2019 API to create a lock. The resource allocator creates the lock and provides the lock id to application. The resource allocator also stores state information regarding the lock, such as the lock is just created, the lock is not assigned, and it is available to requester at this point. Then, the application calls the \u2018lock\u2019 API, which asks the resource allocator to grant the lock for the given lock id. The resource allocator looks at the state of this lock id, and grants\/denies the lock. When the application is done with the lock, it calls \u2018unlock\u2019 with given lock id. The resource allocator then frees up the lock and updates its state information. Then, the lock is granted to any requester that is waiting to get the same lock. If application is done using this lock completely, it can also delete this lock.","These APIs manage creating, removing, acquiring, and releasing locks, and provide some other options. The API ensures that the application is not locked up by specifying a timeout mechanism for waiting. That way, conventional deadlock problems can also be avoided. These APIs enqueue\/dequeue messages to QM to acquire\/release the locks. The APIs may use operating system independent code, easily usable by any application, and may be enabled using locking APIs, like the ones generally provided for semaphores.","In one aspect, as shown in , there are two queues given per processor. So, if there are four processors in the SoC, there are eight queues defined for locking purpose, one for each direction (i.e. CPU - to resource allocator  and resource allocator to CPU -). Whenever an application running on a CPU needs to create a lock, it enqueues a request to its assigned queue, which is dequeued by the resource allocator. The resource allocator reads the message and determines what operation needs to be performed. If the resource allocator needs to create a lock, it will just create it and send the lock ID back to the application.","For example, if two separate applications, running on two different processors, need to share a PCIe device, they need a lock ID to use to acquire\/release the lock. As they are running on two different processors, they are running completely in parallel and don't know if the other application is asking for the same lock. In the system of , it is possible to ask for the same lock for multiple different applications from multiple processors at the same exact time. This is possible using QM hardware. The applications can create a message, specifying the lock ID to be acquired. The messages can be enqueued in the assigned queues, at the same time. The QM hardware delivers the messages to resource allocator. As the resource allocator dequeues from different queues with the help of QM hardware, the requests are prioritized using a scheduling mechanism. In one aspect, the scheduling is round robin, but other scheduling algorithms like strict priority, weighted round robin. etc. can also be used.","The QM hardware takes care of all the arbitration between all the queues and provides only one message (at a time) from the selected queue to resource allocator. The resource allocator looks at the message and determines the requested operation. If the operation is \u2018acquire lock\u2019, then it will check whether the lock has already been provided to another application. If it is not provided, it will grant the lock acquire and send the grant message back to the application. Otherwise, it will send a negative acknowledgement (NAK) message to application. In some aspects, the NAK message may include other information such as an operation timeout value or that the lock ID is not found.","Then, the resource allocator dequeues the next request. If it is an acquire lock request for a different lock ID, and lock is available, a grant reply message is sent. If it is for the same lock, then a NAK message, with parameters, is sent.",{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 4","b":"400"},"Step  provides an SoC with a first plurality of processors, at least one shared resource, a queue manager (QM) with a first plurality of input FIFOs and a first plurality of output FIFOs, and a resource allocator enabled as a sequence of software instructions stored in memory and executed by an allocation processor. As noted above, IO ports and memories are examples of shared resources. In Step  a first application, enabled as a sequence of software instructions stored in memory and executed by a first processor, writes a first request to access the shared resource. In Step  a first APT, enabled as a sequence of software instructions stored in memory and executed by a first processor, loads the first request at a write pointer of a first input FIFO associated with the first processor. In Step  the resource allocator reads the first request from a read pointer of the first input FIFO. In Step  the resource allocator generates a first reply, responsive to the first request, and in Step  loads the first reply at a write pointer of a first output FIFO associated with the first processor. In one aspect, the resource allocator reading requests in Step  and loading replies in Step  includes the QM dequeuing requests from input FIFOs and enqueuing replies into output FIFOs. In Step  the first API supplies the first reply, from a read pointer of the first output FIFO, to the first application. In Step  the first application performs a function in response to the first reply.","In one aspect, generating the first reply in Step  includes the resource allocator generating a reply such as a grant with an embedded lock ID, where the load ID is required for resource access, a grant with the embedded lock ID and scheduled use time, a grant with the embedded lock ID and a use time limit, a grant with the embedded lock ID to a compatible resource, a reply with a resource availability estimate, a rejection, or combinations of the above-mentioned replies.","In another aspect, writing the first request in Step  includes writing a request such as a go\/no-go request for resource access, a scheduled time request for resource access, a length of time request for resource access, a resource access availability estimate, and a request for access to a comparable resource, or combinations of the above-mentioned replies.","In one aspect, in Step  a second application, enabled as a sequence of software instructions stored in memory and executed by a second processor, writes a second request to access the shared resource. In Step  a second API, enabled as a sequence of software instructions executed by a second processor, loads the second request at a write pointer of a second input FIFO, associated with the second processor. Then, Step  includes the resource allocator reading the second request from a read pointer of the second input FIFO, subsequent to reading the first request. Generating the first reply in Step  includes the resource allocator generating the first reply, granting the first application access to the shared resource, and generating a second reply rejecting the second request, in response to granting the first request.","In another aspect, in Step  the first application writes a third request terminating granted access to the shared resource. In Step  the first API loads the third request at the write pointer of the first input FIFO. In Step  the resource allocator reads the third request from the read pointer of the first input FIFO, and in Step  the resource allocator sends a notice to the second application that the shared resource is available. For example, the notice may include a grant to access the shared resource.","A system and method have been provided for sharing resources in an SoC. Examples of particular message structures, processors, and hardware units have been presented to illustrate the invention. However, the invention is not limited to merely these examples. Other variations and embodiments of the invention will occur to those skilled in the art."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"}]},"DETDESC":[{},{}]}
