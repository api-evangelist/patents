---
title: System and method for implementing shadows using pre-computed textures
abstract: The present invention provides an improved system and method for rendering shadows in a computer graphics system. Textures representing the area of influence resulting from a combination of light sources and shadow casters are pre-computed. Scenes are then rendered using the pre-computed textures. A first step entails generating sets of directions and associated pre-computed textures for each light source and shadow caster pair in a simulation frame. Next, a first scene in the simulation is rendered. During this step one or more of the pre-computed textures are used to darken the area of influence or shadow portion of the scene.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07064755&OS=07064755&RS=07064755
owner: Silicon Graphics, Inc.
number: 07064755
owner_city: Mountain View
owner_country: US
publication_date: 20020524
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","BRIEF SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["1. Field of the Invention","The present invention relates generally to computer graphics.","2. Background Art","Computer graphics systems are used to generate the visual images presented in screen displays. In an effort to create the most realistic visual images several techniques such as texture, lighting, and blending are utilized. Casting and rendering shadows (also called \u201cshadowing\u201d) is another important part of the process of generating realistic images. Shadows reinforce the visual reality of an image. For example, shadows can be used to give a two-dimensional image the look of a three dimensional image.","In order to render a scene, a mathematical representation of the scene must first be calculated. This mathematical representation defines several attributes for each image or object depicted in the scene. For example, such attributes can define an object's size, shape, color, location within the scene, and\/or position and orientation with respect to other objects in the scene. Given the volume and complexity of the mathematical calculations necessary to render a scene, an extensive amount of computer processing is required.","In addition, the computer system must be very fast in order to respond to changing scenes. For example, during flight simulations, the scene is constantly changing in response to the input of the pilot. The computer system must recompute a new scene in response to the pilot's input. Games and other applications involving graphics also have frequently changing shadow effects. As the scene changes, the shadow effects must also be changed, resulting in an even greater demand for computer processing.","As one would expect, given the number of complex mathematical calculations, the time required for rendering a scene can be lengthy. The need to account for shadowing further adds to processing time. When generating animated movies or simulated still images, delays of several seconds, minutes, or perhaps even hours might be acceptable. However, in real-time applications, such as flight simulation or gaming, delays in rendering and ultimately presenting graphic simulations to the user cannot be tolerated.","One approach to shadowing involves the use of projected shadows. Projected shadows are simple to use, but every time the shadow casting object in a scene moves or changes its orientation with respect to the light source, the shadow has to be changed. This requires projecting the object from the view of the light source and reading back the image into a texture. Real-time simulation does not allow for this process to be done every frame.","Therefore, what is needed is a system and method which reduces the amount of time needed to render and present a computer generated scene with shadowing.","The present invention provides an improved system and method for rendering shadows in a computer graphics system. An implementation of generating shadows in real-time is realized by projecting a single pre-computed texture or a set of pre-computed textures. In this way, computing textures at each frame is avoided. The pre-computed texture or set of textures are rotated and\/or scaled so that they approximate the shadow created by a shadow caster that has changed its position or orientation with respect to a light source. A shadow caster can be any object that casts a shadow in a scene with respect to light source(s) in the scene.","In embodiments of the present invention, a method for rendering a scene in a computer graphics system using pre-computed textures is provided. A first step entails generating sets for the light source and shadow caster pairs in a simulation frame. Each set provides texture data corresponding to an area of influence created by a shadow caster with respect to its orientation or direction to a given light source. The area of influence is representative of the shadow resulting from a light source and shadow caster pair. Next, each set of directions and associated textures for each light source and shadow caster pair are stored for subsequent access when rendering a scene during run-time. Once a simulation or game has begun, a scene is rendered. Each light source and shadow caster pair in the scene is rendered iteratively. Texture from the stored set of directions and associated textures corresponding to the current light source and shadow caster pair being rendered is used to render the area of influence. Once each of the light source and shadow caster pairs have been rendered, the resulting scene data can be drawn into a frame buffer for subsequent processing and display. In a next step, a determination is made as to whether additional rendering is necessary. For example, a change to the scene (i.e., additional rendering) would be necessary in response to a user's action. If necessary, the next scene would be rendered in accordance with the aforementioned steps and reflects the accepted change to the original scene.","In another embodiment of the present invention, the rendering of the scene involves determining if an area of influence created by a shadow caster is visible in the scene. If so, the closest direction from the stored set of directions and textures corresponding to the light source and shadow caster pair being rendered is selected. Rendering of the scene is then performed using pre-computed texture data associated with the closest direction in the selected set. This process is iteratively performed for each shadow caster and light source pair in the scene.","In yet another embodiment of the present invention, multiple textures are selected to render the scene. Here, a plurality of directions from the set corresponding to the light source and shadow caster pair being rendered are selected. The directions selected are those that are closest to the current position of the shadow caster and the light source. Next, the textures associated with the selected closest directions are used to render the area of influence. A cumulative alpha channel value is generated as a result of drawing the textures for the selected closest directions. Next, for each shadow caster, the cumulative alpha channel value is used to blend the area of influence. Finally, for each shadow caster, the scene is rendered using the blended area of influence. These steps are iteratively repeated for each light source in the next scene.","Further features and advantages of the present invention, as well as the structure and operation of various system, method, and computer logic embodiments of the present invention are described in detail below with reference to the accompanying drawings.","Area of Influence","Rendering refers to the drawing phase of computer graphics processing. During this phase, the images in a scene are converted into textures. Textures are made up of pixels. The pixels represent the individual colors that make up the images displayed in the scene. An area of influence or shadow area, is created when an object is placed in the light path emanating from light sources in the scene. To create shadow effects, those pixels in the area of influence are darkened. In order to determine what pixels are in shadow (i.e., should be assigned darker color values) it is necessary to know the orientation or direction of the shadow casting objects with respect to the light sources. Further explanation of this concept will be provided with reference to .",{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIGS. 1A-1D","FIG. 1A"],"b":["1","1","0","7","1","1"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 1B","b":["1","1","2","0","7","1","1","1","2","8","12"]},"As shown in , in the case of two light sources (S and S) and two shadow casters (C and C), four different sets could be used to represent the respective areas of influence. The area of influence created by the light source (S) and shadow caster (C) pair is represented by the set composed of directions - and associated textures A-H. The area of influence created by the light source (S) and shadow caster (C) pair is represented by the set composed of directions - and associated textures I-M.","The areas of influence created by the pairing of the light source (S) with shadow casters (C and C) could also be determined. However, it should be apparent that the area of influence created by the light source (S) and the shadow caster (C) would overlap with the area of influence created by the light source (S) and the shadow caster (C). The same is true for the area of influence created by the light source (S) and shadow caster (C) in comparison to the area of influence created by the light source (S) and shadow caster (C) pair. Therefore, the area of influence for the light source (S) and shadow caster (C) pair can be used to represent the area of influence for the light source (S) and shadow caster (C) pair. Likewise, the area of influence for the light source (S) and shadow caster (C) pair can be used to represent the area of influence for the light source (S) and the shadow caster (C).","A fourth scenario involving a different combination of two light sources and two shadow casters is illustrated in FIG. D. Here again, the set of directions - and textures A-H represent the area of influence created by light source (S) and shadow caster (C) in eight different directions. The set comprised of directions - and textures I-M is representative of the areas of influence resulting from the pairing of the light source (S) and the shadow caster (C). Additional areas of influence are represented in the set composed of directions - and textures N-Q. These areas of influence result from the pairing of the light source (S) and the shadow caster (C). It should be apparent that no area of influence would result from light source (S) and shadow caster (C). An exemplary method for generating shadows using pre-computed textures in accordance with embodiments of the present invention will now be described.","Exemplary Method For Implementing Shadows using Pre-computed Textures","Routine  () is an exemplary method for rendering shadows using pre-computed textures in accordance with an embodiment of the present invention. In real-time graphics applications it is vitally important that scenes be rendered and drawn quickly. To this end, an important advantage is gained by pre-processing, that is, performing many calculations needed to render a scene before an application is run. The present invention will be described using simulation of the image depicted in  as an example. This example is intended for explanation only and is not intended to limit the present invention.","Pre-Processing","In an exemplary embodiment of the present invention, the scene depicted in  is to be displayed in a real-time simulation. Sets of directions and associated textures for respective pairs of the light sources (S and S) and the shadow casters (C and C) are generated in a pre-processing operation  prior to run-time. The pre-computed textures are generated by drawing the shadow casters and then reading them into texture memory.","The number of pre-computed textures in a set can vary depending on a number of factors, such as, user preferences, computing resources, and the application in which the present invention is being utilized. Furthermore, the user might think one light source or shadow caster more important to the scene than another. Accordingly, a different number of directions and textures might be calculated for one light source and shadow caster pair as compared to another. In the present example, textures representing the area of influence for light source (S) and shadow caster (C) from eight different directions are pre-calculated. Set () contains the following directions and textures: direction , texture A; direction , texture B; direction , texture C; direction , texture D; direction , texture E; direction , texture F; direction , texture F; direction , texture G; and direction , texture H. Each texture in the set represents the area of influence created by shadow caster (C) with respect to light source (S) from a particular direction. The textures are saved for subsequent use during run-time.","Set () contains the following directions and textures: direction , texture I; direction , texture J; direction , texture K; direction , texture L; and direction , texture M. These textures represent the areas of influence created by light source (S) and shadow caster (C) from each of the given directions.","The following directions and textures in set () represent the areas of influence created by light source (S) and shadow caster (C): direction , texture N; direction , texture ; direction , texture P; and direction , texture Q. As previously discussed, there is no area of influence with respect to the light source (S) and shadow caster (C).","Next, the generated sets of directions and associated textures for each light source and shadow caster pair would be stored (step ).","Upon completion of the preprocessing operation, sets for the respective light source and shadow caster pairs will have been generated and stored. Each set contains one or more pre-computed textures representing the areas of influence for a specific shadow caster and light source pair with respect to a particular orientation or direction. The sets and pre-computed textures associated therewith can then be used to render scenes once simulation has begun. Since fewer computations in real-time are necessary, processing time between scenes or frames can be reduced.","Run-Time","Once the pre-processing operation is completed, actual simulation or running of the graphics application is begun. In step , the scene () is rendered. The scene can be rendered according to Z buffer or equivalent applications or similar methods that would be apparent to a person of ordinary skill in the relevant arts given this description. During the rendering operation, texture data is needed to draw the scene. Accordingly, texture data for rendering the light sources (S and S) and the shadow casters (C and C) would need to be calculated before these objects can be drawn in the scene. Displaying the shadows created by the objects present in the scene would add a higher degree of visual realism to the scene. To represent these shadows, it will also be necessary to obtain the texture data for the shadow or area of influence created by the objects given their present location in the scene being rendered. In an embodiment of the present invention, shadows are rendered in accordance with step . Step  is further described below with respect to .",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 3","b":["300","305","345","300"]},"In step  a determination is made as to whether blending of the shadow features is desired. When textures are changed in a scene a \u201cjump \u201d or visual distortion is created. Blending is used to minimize the visual distortion by blending the textures. In this way, the shadow changes from one frame to another are less noticeable. In the case where no blending is to be performed, processing continues with the routine for darkening a scene using texture (step ). This step is described further below with respect to . In the case where blending is to be performed, steps - are performed. These steps are described with respect to .","First the case where no blending of the shadow features is to be performed will be described while referring to step  of FIG.  and . In step , for each light source and shadow caster pair, texture data for rendering the area of influence is obtained to darken the scene. During a first iteration, texture data for rendering the area of influence created by light source (S) and shadow caster (C) is obtained. Step  is further described with reference to FIG. .","A determination is made as to whether the area of influence for shadow caster (C) is visible (step ). The area of influence refers to the portions of the scene lying in the path of the shadow projected by shadow caster (C) with respect to light source (S). If this area of influence is not visible from the viewer's perspective then there is no need to render the area of influence and control returns to step . Alternatively, if the area of influence is visible from the viewer's perspective then shadow processing continues with step .","In step , the pre-computed texture associated with the direction that is closest to the current light direction in the scene being rendered is selected. This step is further described with reference to .","In step , the current light direction is determined. In an embodiment, the current light direction is determined in accordance with the steps shown in FIG. .","First, in step  a transformation matrix expressing the difference between the current position and orientation of the shadow caster and its original position and orientation is created. In an embodiment, the transformation matrix can provide both rotation and translation data.","Next, in step , the inverse of the transformation matrix is calculated to generate an inverse transformation matrix.","In step , a determination is made as to whether the light source is a local light source. A local light source is a light source that is defined by a single point in space. Consequently, light radiates uniformly in all directions.","If the light source is a local light source, then a variable light vector is calculated as the difference between the light source position and the shadow caster's center (step ).","The variable light vector is then multiplied by the inverse transformation matrix to generate the current light direction (step ).","If the light source is not a local light source then the light direction is multiplied by the inverse matrix (step ).","Finally, in step , the vector resulting from step  or step  is returned as the current light direction. This vector (current light direction) could be equal to or close to one of the directions chosen during the pre-processing phase. Description of the present invention will proceed as if the current light direction is in close proximity to direction  (FIG. D). Processing returns to step  (FIG. ).","Once the current light direction is determined, then in step , the direction from the previously stored set of directions and associated textures for light source (S) and shadow caster (C) that is closest to the current light direction is selected. For example, direction  (FIG. D). Methods for identifying the closest direction set to the current light direction will be apparent to a person of ordinary skill in the relevant arts given this description.","Next in step , the texture data (texture E) associated with the selected closest direction (direction ) is retrieved from memory. Once the texture data (texture E) is retrieved, then control returns to step .","As shown in , step , blending parameters are set to multiply the existing color value of the area of influence by (1\u2014the alpha value) of the retrieved texture data (texture E). Here, blending is used to combine the texture data for the area of influence, in this example, texture E, with the texture data for the shadow casting object (e.g., shadow caster (C)).","Next, the texture weight is set equal to one (1) (step ).","The area of influence is then drawn using the retrieved texture data (texture E) (step ). Further description of step  is provided with reference to FIG. .","Texture E provides the closest representation of the area of influence currently being rendered. In some cases the selected pre-computed texture will have been calculated for the current view being rendered; in some other cases an exact match is not available. Therefore, in step , the texture matrix for texture E is added, rotated, and scaled to compensate for any differences between the current view being rendered and the view represented by the selected pre-computed texture (texture E). An exemplary routine for step  will now be described with reference to steps - of FIG. .","To begin, in step , a first set of texture coordinates are computed. This first set of texture coordinates represent two control points projected into the selected texture. The two points are selected from the object being rendered, for example, the edge of the roof or hood of shadow caster (C).","Next, in step , a second set of texture coordinates are computed based upon the two control points and the current light direction. The difference between the first and second set of computed texture coordinates is used to scale and rotate the texture matrix.","Next, in step , the scale and rotate operation needed to map the first set of texture coordinates into the second set is determined.","Finally, in step , the scale and rotate operation is added to the texture matrix. In this way, the texture projection matrix is scaled and rotated to apply shadowing from a pre-computed texture that approximates a correct shadow for the current view. Upon the completion of step , control returns to step  (FIG. ).","In step , the texture and texture matrix are set.","Next, the color of the area of influence is set to (0,0,0,weight) (step ).","Finally, in step , the area of influence is rendered and control returns to step  (FIG. ). Step  results in the multiplication of the pixel colors in the area of influence by the values stored in the alpha channel of the selected shadow textures, thus darkening the area of influence.","Rendering has now been completed with respect to light source (S) and shadow caster (C). Step  of  is now repeated in order to darken the scene using texture for light source (S) and shadow caster (C). Once routine  has been performed for each shadow caster, control would pass to step  (FIG. ).","Upon returning to step , processing continues for the remaining light sources. Accordingly, routine  would be repeated for light source (S) and shadow casters (C) and (C). The light source (S) and shadow caster (C) do not have a visible area of influence and would therefore not be rendered. Regarding the light source (S) and the shadow caster (C), one of one of the directions - and associated textures N-Q would be selected from set () and used to darken the scene being rendered. Step  is iteratively performed until each light source and shadow caster pair has been rendered. Once this process is done, rendering of the current scene is completed.","As the simulation continues, additional scenes will need to be rendered. For example, in response to a user's input, the position of the objects in the scene might need to be repositioned. Accordingly, a new scene reflecting the position changes and consequently, the shadows, would need to be rendered. Thus, in step , a determination is made as to whether another scene is to be rendered. If so, then control returns to step  and the next scene is rendered; otherwise, processing ends at step .","In order to describe the case where blending of shadow features is desired,  will be rendered again (step ). To begin, in step , all shadow casters (C and C) would be processed for light source (S). Step  is further described below with respect to routine  () steps (-). Further details of these steps will be described with respect to  and .","Referring again to step , if blending of the shadow features is turned on, processing control passes to step . In this case multiple textures from the pre-computed textures will be combined or blended to yield the final texture value of the area of influence.","In step , the alpha channel value of all the areas of influence being rendered is reset to zero. In this way, the alpha channel values of the selected pre-computed textures being blended will determine the final alpha channel value of each area of influence.","Next, blending is initialized (step ).","Processing is next set-up to only allow writing into the alpha channel (step ).","Next in step , for each shadow caster, the alpha channel value for each shadow caster is added to the alpha channel value for the area of influence. In the present example, step  is performed for shadow caster (C) and then repeated for shadow caster (C). This step will be further described with reference to FIG. .","In scenes where multiple light sources are present, it is likely that some of the areas of influence will overlap. The effects of each shadow caster's impact on the common area of influence are combined. Furthermore, it is possible that the resulting shadow is cast out of the viewable area. Thus, in step , it is first determined whether the shadow caster's area of influence is visible or not. If not, then there is no need to render the shadow cast by, for example shadow caster (C). Control would then return to step  and processing would continue with regard to shadow caster (C). If the area of influence for shadow caster (C) is visible, then processing continues with step .","In step , textures and weights are chosen for the shadow caster. In this case, shadow caster (C). Step  will be described in further detail with reference to FIG. .","In step , the current light direction is determined in accordance with steps - described above with reference to FIG. .","Next, in step , the set containing directions and textures for the light source and shadow caster pair being rendered is obtained. In the present example, the set corresponding to light source (S) and shadow caster (C) is obtained (FIG. D). From set (), a plurality of directions that are in close proximity to the current light direction determined in step  are selected (for example, direction  and direction  from FIG. D). The number of directions selected can vary depending on the distribution of directions. Generally two to four directions are selected. In an embodiment, where the directions are distributed in a single plane, two nearest directions are selected. In an alternative embodiment, where the directions are distributed in three dimensions, three or four nearest directions are selected.","Next, in step , the pre-computed textures associated with the selected closest directions are retrieved (e.g., texture I and texture J).","Next, in step , the texture weights for the retrieved pre-computed textures are selected. Control then returns to step . Once the pre-computed textures and corresponding weights are obtained, processing continues with step .","Step  is executed for each selected pre-computed texture. During execution of step , the area of influence is drawn in accordance with steps - described above with reference to FIG. . In the previous description of steps -, only one pre-computed texture was used to draw the area of influence. However, in the present example, steps - would be repeated such that the area of influence drawn (i.e. computed alpha channel value) is a blended product of each of the selected pre-computed textures (i.e., texture I and texture J). Once step  is completed, control returns to step . Step  is repeated until each shadow caster has been processed, then control passes to step .","In step , writing to the color channels for the area of influence is enabled.","Next, in step , for each shadow caster, the original color of the area of influence is blended using the alpha channel value obtained from step . An exemplary blending routine will now be described with reference to FIG. .","Beginning with the light source (S) and shadow caster (C) pair, in step , a determination is made as to whether the shadow caster's (C) area of influence is visible. If not, then the shadow caster's area of influence is not rendered. Step  is repeated for the light source (S) and shadow caster (C) combination. Further description will continue as if the area of influence of light source (S) and shadow caster (C) is not visible and the area of influence of light source (S) and shadow caster (C) is visible.","In step , blending parameters are set to multiply the existing color of the area of influence being rendered by the value of (1\u2014the existing alpha) which is equivalent to 1\u2014destination alpha in OPENGL.","Finally, in step , the area of influence for the light source (S) and the shadow caster (C) is drawn according to steps - described above with reference to FIG. . Having rendered the scene with respect to light source (S) and shadow casters (C and C) control returns to step .","Step  would next be repeated for light source (S) and shadow casters (C and C). Once step  is completed for each light source and shadow caster pair, rendering ends at step . The rendered scene can then be read into the frame buffer for subsequent processing or display.","Example Graphics Implementations","The present invention is described with reference to example computer graphics environments (FIGS. -). These example environments are illustrative and not intended to limit the present invention.",{"@attributes":{"id":"p-0089","num":"0088"},"figref":"FIG. 12","b":"1200"},"Architecture  includes six overlapping layers. Layer  represents a high level software application program. Layer  represents a three-dimensional (3D) graphics software tool kit, such as OPENGL PERFORMER, available from Silicon Graphics, Incorporated, Mountain View, Calif. Layer  represents a graphics application programming interface (API), which can include but is not limited to OPENGL, available from Silicon Graphics, Incorporated. Layer  represents system support such as operating system and\/or windowing system support. Layer  represents firmware. Finally, layer  represents hardware, including graphics hardware. Hardware  can be any hardware or graphics hardware including, but not limited to, a computer graphics processor (single chip or multiple chip), a specially designed computer, an interactive graphics machine, a gaming platform, a low end game system, a game console, a network architecture, server, et cetera. Some or all of the layers - of architecture  will be available in most commercially available computers.","As will be apparent to a person skilled in the relevant art after reading the description of the invention herein, various features of the invention can be implemented in any one of the layers - of architecture , or in any combination of layers - of architecture . In particular, one or more of steps - can be implemented in any one of the layers - of architecture , or in any combination of layers - of architecture .","In one embodiment, a shadow module  is provided according to the present invention. The shadow module  provides control steps necessary to carry out routine . The shadow module  can be implemented in software, firmware, hardware, or in any combination thereof. As shown in , in one example implementation shadow module  is control logic (e.g., software) that is part of an application layer  that provides control steps necessary to carry out routine . In alternative implementations, shadow module  can be implemented as control logic in any one of the layers - of architecture , or in any combination of layers - of architecture . In particular, shadow module  can control the carrying out of one or more of steps - in any one of the layers - of architecture , or in any combination of layers - of architecture  as would be apparent to a person skilled in the art given this description.",{"@attributes":{"id":"p-0093","num":"0092"},"figref":"FIG. 13","b":["1300","1300","1310","1320","1370","1300"]},"Host system  comprises an application program , a hardware interface or graphics API , and a processor . Application program  can be any program requiring the rendering of a computer image or scene. The computer code of application program  is executed by processor . Application program  accesses the features of graphics subsystem  and display  through hardware interface or graphics API . As shown in , in one example implementation shadow module  is control logic (e.g., software) that is part of application .","Graphics subsystem  comprises a vertex operation module , a pixel operation module , a rasterizer , a texture memory , and a frame buffer . Texture memory  can store one or more texture images . Texture memory  is connected to a texture unit  by a bus or other communication link (not shown). Rasterizer  comprises texture unit  and a blending unit . The operation of these features of graphics system  would be known to a person skilled in the relevant art given the description herein.","In embodiments of the present invention, texture unit  can obtain either a point sample, a bilinearly filtered texture sample, or a trilinearly filtered texture sample from texture image . Blending unit  blends texels and\/or pixel values according to weighting values to produce a single texel or pixel. The output of texture unit  and\/or blending module  is stored in frame buffer . Display  can be used to display images or scenes stored in frame buffer .","An embodiment of the invention shown in  has a multi-pass graphics pipeline. It is capable of operating on each pixel of an object (image) during each pass that the object makes through the graphics pipeline. For each pixel of the object, during each pass that the object makes through the graphics pipeline, texture unit  can obtain a single texture sample from the texture image  stored in texture memory .","Referring to , an example of a computer system  is shown which can be used to implement computer program product embodiments of the present invention. This example computer system is illustrative and not intended to limit the present invention. Computer system  represents any single or multi-processor computer. Single-threaded and multi-threaded computers can be used. Unified or distributed memory systems can be used.","Computer system  includes one or more processors, such as processor , and one or more graphics subsystems, such as graphics subsystem . One or more processors  and one or more graphics subsystems  can execute software and implement all or part of the features of the present invention described herein. Graphics subsystem  can be implemented, for example, on a single chip as a part of processor , or it can be implemented on one or more separate chips located on a graphic board. Each processor  is connected to a communication infrastructure  (e.g., a communications bus, cross-bar, or network). After reading this description, it will become apparent to a person skilled in the relevant art how to implement the invention using other computer systems and\/or computer architectures.","Computer system  also includes a main memory , preferably random access memory (RAM), and can also include secondary memory . Secondary memory  can include, for example, a hard disk drive  and\/or a removable storage drive , representing a floppy disk drive, a magnetic tape drive, an optical disk drive, etc. The removable storage drive  reads from and\/or writes to a removable storage unit  in a well-known manner. Removable storage unit  represents a floppy disk, magnetic tape, optical disk, etc., which is read by and written to by removable storage drive . As will be appreciated, the removable storage unit  includes a computer usable storage medium having stored therein computer software and\/or data.","In alternative embodiments, secondary memory  may include other similar means for allowing computer programs or other instructions to be loaded into computer system . Such means can include, for example, a removable storage unit  and an interface . Examples can include a program cartridge and cartridge interface (such as that found in video game devices), a removable memory chip (such as an EPROM, or PROM) and associated socket, and other removable storage units  and interfaces  which allow software and data to be transferred from the removable storage unit  to computer system .","In an embodiment, computer system  includes a frame buffer  and a display . Frame buffer  is in electrical communication with graphics subsystem . Images stored in frame buffer  can be viewed using display .","Computer system  can also include a communications interface . Communications interface  allows software and data to be transferred between computer system  and external devices via communications path . Examples of communications interface  can include a modem, a network interface (such as Ethernet card), a communications port, etc. Software and data transferred via communications interface  are in the form of signals which can be electronic, electromagnetic, optical or other signals capable of being received by communications interface , via communications path . Note that communications interface  provides a means by which computer system  can interface to a network such as the Internet.","Computer system  can include one or more peripheral devices , which are coupled to communications infrastructure  by graphical user-interface . Example peripheral devices , which can from a part of computer system , include, for example, a keyboard, a pointing device (e.g., a mouse), a joy stick, and a game pad. Other peripheral devices , which can form a part of computer system  will be known to a person skilled in the relevant art given the description herein.","The present invention can be implemented using software running (that is, executing) in an environment similar to that described above with respect to FIG. . In this document, the term \u201ccomputer program product\u201d is used to generally refer to removable storage unit , a hard disk installed in hard disk drive , or a carrier wave or other signal carrying software over a communication path  (wireless link or cable) to communication interface . A computer useable medium can include magnetic media, optical media, or other recordable media, or media that transmits a carrier wave. These computer program products are means for providing software to computer system .","Computer programs (also called computer control logic) are stored in main memory  and\/or secondary memory . Computer programs can also be received via communications interface . Such computer programs, when executed, enable the computer system  to perform the features of the present invention as discussed herein. In particular, the computer programs, when executed, enable the processor  to perform the features of the present invention. Accordingly, such computer programs represent controllers of the computer system .","In an embodiment where the invention is implemented using software, the software may be stored in a computer program product and loaded into computer system  using removable storage drive , hard drive , or communications interface . Alternatively, the computer program product may be downloaded to computer system  over communications path . The control logic (software), when executed by the one or more processors , causes the processor(s)  to perform the functions of the invention as described herein.","In another embodiment, the invention is implemented primarily in firmware and\/or hardware using, for example, hardware components such as application specific integrated circuits (ASICs). Implementation of a hardware state machine so as to perform the functions described herein will be apparent to a person skilled in the relevant art.","Conclusion","Various embodiments of the present invention have been described above, which are capable of being implemented on a graphics machine. It should be understood that these embodiments have been presented by way of example only, and not limitation. It will be understood by those skilled in the relevant art that various changes in form and details of the embodiments described above may be made without departing from the spirit and scope of the present invention as defined in the claims. Thus, the breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS\/FIGURES","p":["The accompanying drawings, which are incorporated herein and form part of the specification, illustrate the present invention and, together with the description, further serve to explain the principles of the invention and to enable a person skilled in the pertinent art to make and use the invention. In the drawings, like reference numbers indicate identical or functionally similar elements. Additionally, the left-most digit(s) of a reference number identifies the drawing in which the reference number first appears.",{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIGS. 1A-1D"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIGS. 3-11","b":"1"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 14"}]},"DETDESC":[{},{}]}
