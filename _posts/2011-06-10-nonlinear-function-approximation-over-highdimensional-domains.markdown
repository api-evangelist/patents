---
title: Nonlinear function approximation over high-dimensional domains
abstract: An algorithm is disclosed for constructing nonlinear models from high-dimensional scattered data. The algorithm progresses iteratively adding a new basis function at each step to refine the model. The placement of the basis functions is driven by a statistical hypothesis test that reveals geometric structure when it fails. At each step the added function is fit to data contained in a spatio-temporally defined local region to determine the parameters, in particular, the scale of the local model. The proposed method requires no ad hoc parameters. Thus, the number of basis functions required for an accurate fit is determined automatically by the algorithm. The approach may be applied to problems including modeling data on manifolds and the prediction of financial time-series. The algorithm is presented in the context of radial basis functions but in principle can be employed with other methods for function approximation such as multi-layer perceptrons.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08521488&OS=08521488&RS=08521488
owner: National Science Foundation
number: 08521488
owner_city: Arlington
owner_country: US
publication_date: 20110610
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","STATEMENT OF GOVERNMENT INTEREST","BACKGROUND","REFERENCES","TERMS AND DESCRIPTIONS","SUMMARY","DETAILED DESCRIPTION"],"p":["The present application is a continuation of U.S. patent application Ser. No. 11\/899,625, filed Sep. 5, 2007, which claims the benefit of U.S. Provisional Application Ser. No. 60\/969,903, filed Sep. 4, 2007, and the present application claims the benefit of U.S. Provisional Application Ser. No. 60\/842,401, filed Sep. 5, 2006; each of the above-identified applications are incorporated fully herein by reference.","Subject matter disclosed herein was partially supported by the National Science Foundation award DMS-0434351, DOD-USAF-Office of Scientific Research under contract FA9550-04-1-0094. Subject matter disclosed herein was also partially supported by National Science Foundation Grant No. ATM0530884. Additionally, subject matter disclosed herein was partially supported by DOD\/Navy Grant No. N002444-07-1-0004. The government may have certain rights in the claimed invention.","The problem of extracting nonlinear relationships in large high-dimensional scattered data sets is of central importance across fields of science, engineering and mathematics. In particular, diverse areas such as machine learning, optimal control, mathematical modeling of physical systems often rely significantly on the ability to construct relationships from data. Subsequently there have been a multitude of applications including financial time-series analysis, voice recognition, failure prediction and artificial intelligence all of which provide evidence of the importance for nonlinear function approximation algorithms.","The beginnings empirical data fitting may be traced to Gauss's work on using least squares to construct linear models. Over the last two decades we have seen a tremendous growth in this area motivated by new ideas for computing nonlinear models. Numerous references of prior art articles are cited herein. In most cases, when a reference is referred to herein, it is cited by its number (in square brackets) from the References section hereinbelow. Thus, e.g., for computing nonlinear models, the following references [11, 45, 40, 37, 38] in the References Section disclose exemplary prior art techniques.","Diverse areas such as machine learning, optimal control, mathematical modeling of physical systems often rely significantly on the ability to construct relationships from data such as provided by constructing robust approximation models. Moreover, there have been a multitude of applications including financial time-series analysis, voice recognition, failure prediction and artificial intelligence all of which provide evidence of the importance for nonlinear function approximation algorithms. Our interest in this problem relates to representing data on a manifold as the graph of a function [8, 9] and the reduction of dynamical systems (see, e.g. [10]).","A common element in empirical data fitting applications is that the complexity of the required model including the number and scale of representation functions is not known a priori and must be determined as efficiently as possible. A variety of approaches have been proposed to determine the number of model functions, i.e., the model order problem. A generally accepted measure of quality of such data fitting algorithms is that the resulting models generalize well to testing data, i.e., data associated with the same process but that was not used to construct the model. This requirement is essentially that the data not be overfit by a model with too many parameters or underfit by a model with too few parameters.","One general approach to this problem is known as regularization, i.e., fitting a smooth function through the data set using a modified optimization problem that penalizes variation. A standard technique for enforcing regularization constraints is via cross-validation [18, 44]. Such methods involve partitioning the data into subsets of training, validation and testing data; for details see, e.g., [19].","Additionally, a variety of model growing and pruning algorithms have been suggested, e.g.,\n\n","Statistical methods have also been proposed that include, e.g., Akaike information criteria (AIC), Bayesian information criteria (BIC) and minimum description length (MDL) [42], [2] and Bayesian model comparison [29]. In [39], [31] and [20] the issue of selecting the number of basis functions with growing and pruning algorithms from a Bayesian prospective have been studied. In [5], a hierarchical full Bayesian model for RBFs is proposed. The maximum marginal likelihood of the data has also been used to determine RBF parameters [34]. For a more complete list of references the reader, is referred to [21].","In general, model order determination via both regularization and growing and pruning algorithms can be computationally intensive and data hungry. More importantly, however, is that these algorithms do not explicitly exploit the geometric and statistical structure of the residuals (see Terms and Descriptions section hereinbelow) during the training procedure. In addition, many algorithms in the literature require that anywhere from a few to a dozen ad hoc parameters be tuned for each data set under consideration.","Accordingly, it is desirable to alleviate the modeling difficulties in the prior art, and in particular, at least provide model generating methods and systems that are computationally less intensive, and that reduce (preferably to zero) the number of model generation parameter required for generating a model of appropriate accuracy.","The following are cited herein, and are fully incorporated herein by reference.\n\n","Radial Basis Functions. Radial Basis Functions (RBFs) were introduced for the function approximation problem as an alternative to multilayer perceptrons [11]. Part of their appeal is the variety of efficient algorithms available for their construction. In the extreme, the basis functions may be selected randomly (following the distribution of the data) with fixed scales. In this instance the resulting optimization problem is simply an over-determined least squares problem to determine the expansion coefficients. One may improve on this approach at modest expense by employing a clustering algorithm to determine the basis function centers [32]. Furthermore, RBFs may be adapted with rank one updates or down-dates [33, 35]. Over the years RBFs have been used successfully to solve a wide-range of function approximation and pattern classification problems [6, 21]. More recently, RBFs have been proposed as a tool for the simulation of partial differential equations, see, e.g., [1].","An RBF expansion is a linear summation of special nonlinear basis functions (although there seems to be some ambiguity in the literature in that both the expansion and the expansion functions may be referred to as RBFs). In general, an RBF is a",{"@attributes":{"id":"p-0016","num":"0168"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"mi":"Ax","mo":["+","+"],"msub":{"mi":"\u03b1","mn":"0"},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"K"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["\u03b1","k"]},"mo":"\u2062","mrow":{"msub":{"mi":["\u03d5","k"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"x","mo":"-","msub":{"mi":["c","k"]}}},"mo":"\u2062","mi":"w"}}}}}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2.1"}}]}}}},"br":[{},{},{}],"sub":["k ","k","k ","W"],"in-line-formulae":[{},{}],"sup":["T","n","m"],"i":"W\u03c7"},"Note that the metric \u2225*\u2225 used in the above equations may be the standard Euclidean norm or an Lp norm where 1<=p<infinity or an 1-infinity norm, and in each case, the weighting matrix W is selected from nonsingular matrices to optimize a data fitting procedure using nonlinear programming techniques well known in the art; e.g., conjugate gradient, Newton's method, and other quasi-Newton methods. The term Ax+\u03b1affords an affine transformation of the data and is useful so that the nonlinear terms are not attempting to fit flat regions. More general polynomials may be used for this purpose [41]. As usual, the dimensions of the input n and output m are specified by the dimensions of the input-output pairs.","Residual. The difference between an actual value, and a value determined by a model approximating the actual value.","IID. An abbreviation of \u201cindependent and identically distributed\u201d. That is, in the present disclosure the term \u201cIID\u201d is directed to the residuals obtained from an approximation of a particular data set being modeled. In particular, the approximation method and system disclosed hereinbelow determines whether such residuals are IID.\n\nDomain Neighborhood (Spatio-temporal window): An extent consisting of a cluster of data in a local region of the input data (i.e., in the domain space of the approximation model (to be) generated). E.g., if a process is evolving both in space and time then a domain neighborhood is referred to as a spatio-temporal window, or alternatively as a space-time ball. Such a spatio-temporal window is distinguished from a temporal window in that points in spatio-temporal window may have been generated at any time whereas a temporal window contains only data that was produced in the time interval that defines the window.\n","A novel approximation method and system for performing nonlinear function approximation or modeling for scattered data is disclosed. In particular, such modeling is performed without requiring the use of ad hoc parameters. The approximation method and system is based on detecting (any) structure in residuals between the data and its approximation. Accordingly, structure or information content in the residuals is quantified via a statistical hypothesis test to determine whether the residuals are IID (as described in the Terms and Description section hereinabove). In particular, an hypothesis test is iteratively applied to the residuals for determining whether a new approximation function (e.g., a basis function, and more particularly, a radial basis function as described in the Terms and Description section above) should be added, and if so, where should the support for the new approximation be located relative to samples of the data set. When a determination is made that this test has been passed using the 95% confidence criterion it may be inferred that there is no (geometric) structure left in the residuals and thus no additional approximation functions are added to the model.","One aspect of the present approximation method and system is its application to high dimensional domains (e.g., domains having 2 or more dimensions) using a spatio-temporal ball rather than a temporal window (as in [3, 4]) for constructing local training sets from the data set being modeled. This innovation is particularly critical if the data set is periodic or quasi-periodic or, more generally, can be represented on a manifold.","Since the approximation method and system does not use ad hoc parameters that must be set per data set modeled, no parameters are varied nor tuned for particular data sets. Thus, diverse data sets can be modeled without making any changes to embodiments of the approximation method and system. This aspect greatly accelerates approximation of the data set being modeled.","In at least one embodiment of the approximation method and system, radial basis functions are used as the approximation functions. However, it is within the scope of the present disclosure that other type of approximation functions may be used in various embodiments of the approximation method and system, in particular, multi-layer perceptrons, and\/or feed-forward neural networks.","To illustrate the absence of ad hoc parameters, numerous example data sets are presented and approximated hereinbelow by exactly the same programmatic embodiment of the novel approximation method and system disclosed herein. That is, no adjustments or parameter settings were made to the programmatic embodiment based on the data set being approximated. Hence, the present approximation method and system approaches a black-box methodology for nonlinear function approximation. This feature of the present approximation method and system permits the advancement of a variety of other processes, e.g., the representation of data on manifolds as graphs of functions [8, 9], pattern classification [22, 26], as well as the low-dimensional modeling of dynamical systems [10].","For simplicity, it is assumed herein that a data set to be modeled by the present approximation method and system represents a functional relationship, or signal, with IID additive noise. For fitting data without noise, such as data generated to high precision by numerical simulations, it is useful to add IID noise to the signal before applying the algorithm. Also note, however, that if the functional relationship or signal contains multiplicative noise we can take the natural logarithm of the functional relationship or signal to make the IID additive as one of skill in the art will understand.","It is an aspect of the present approximation method and system to use a spatio-temporal window, i.e., space-time balls (as described in the Terms and Description section hereinabove), for determining the samples within the data set being modeled when determining what portion of the data set is to be fitted with a new approximation function. It is believed that the use of such a spatio-temporal window is critical for approximating a data set over high-dimensional domains, and in particular, for a data set generated by dynamical systems such as those arising in a physical apparatus such as a heat exchanger and a chemical processing plant, or those arising in financial markets such as the value of a stock or stock index. In addition such dynamical systems arise in scientific and physical models such as the Navier Stokes equations in fluid dynamics as one skilled in the art will understand. The applicants have observed that significantly more data samples are located in such space-time balls than in the temporal windows used in prior art approximation techniques, in particular see [3, 4]. Moreover, it has been observed that the use of such spatio-temporal windows results in the construction of significantly improved models.","Additionally, we extend the prior art by proposing an algorithm where the model output lives in spaces of dimension more than one. The simplest approach is to use as many models as the dimension of the output space (one for each dimension). Alternatively, a more parsimonious representation arises when we extend the IID test on the residuals to the multivariate case.","The present approximation method and system may be used successfully with data sets from signals composed with additive IID noise. This is an extension over prior approximation techniques based on statistical hypotheses that are substantially restricted to Gaussian noise, or that do not provide information concerning where the basis functions should be placed.","The present approximation method and system may be used to model batch or predetermined data sets, or to model streaming data sets that are determined, e.g., in (near) real-time. Moreover, the present approximation method and system does not require the streaming data to be repeatedly reviewed as some \u201con-line\u201d approximation techniques require. Also, although the algorithm was presented here in the context of growing radial basis functions, in principle it can be employed with other architectures for fitting nonlinear functions.","Additional features and benefits will become evident from the accompanying drawing and the description hereinbelow. Such additional feature and benefits are considered subject matter for seeking patent protection even though they may not be identified or discussed in this Summary section. In particular, the invention herein is defined by the claims as supported by the entire specification.","Radial Basis Functions.","Referring to the description of radial basis functions (RBF) in the Terms and Description section hereinabove, the general training problem for the RBF is the determination of the unknown parameters: {A, \u03b1, c, W, K}. One object of the present disclosure is to determine an optimal value for K, i.e., the model order. Of course optimizing K depends on high quality values for the remaining parameters and we propose algorithms for this purpose.","The requirements of the RBF expansion are the same as standard data fitting problems, i.e., given a set of L input-output pairs\n\n{(\u03c7)},\u03c7={\u03c7}and },\n","the goal is to find the underlying mapping f such that y=f(x). In the standard situation, there is more data than equations so it is not possible to approximate each such mapping f exactly. Thus the problem is to minimize the cost function",{"@attributes":{"id":"p-0066","num":"0218"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["A","W","K"],"mo":[",",",",",",","],"msub":[{"mi":["\u03b1","k"]},{"mi":["c","k"]}]}}},{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"L"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mrow":{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","l"]}}},"mo":"-","msub":{"mi":["y","l"]}}},"mn":"2"}}}],"mo":"="},"mo":","}}},"br":{}},"One of the attractive features of RBFs is the variety of basis functions available for the expansion. In particular, these functions come in both local and global flavors and include\n\n\u03c6()={}.\n\nThese functions satisfy the criteria of radial basis functions as described in [38] and the associated theorem states that if D is a compact subset of R, then every continuous real valued function on D can be approximated uniformly by linear combinations of radial basis functions with centers in D. Thus, given any data set of D, such as X above, a function that approximates X can be constructed as a linear combination of radial basis functions with the center of each such radial basis function being in D as one of skill in the art will understand.\n","For simplicity, the present description is restricted to (local) Gaussian radial basis functions, i.e., \u03c6(r)=exp(\u2212r) but the scope of the present disclosure is not limited to such specific functions. Indeed, the method disclosed herein is applicable to any admissible RBF; i.e., the interpolation matrix is nonsingular in the square case. Moreover, disclosed hereinbelow are several candidates for compact radial basis functions that are believed to have excellent conditioning properties associated with the over determined least squares problem.","It is conventional in function approximation problems to distinguish between: (a) situations where: the data for which a model is desired is available at the outset and (b) situations where the data becomes available as the model is being built. In keeping with standard terminology, situations as in (a) above are referred to as \u201cbatch\u201d training problems, and situations as in (b) above are referred to \u201con-line training problems. Although, the approximation method and system disclosed herein is described for batch training, a similar method may be also used to generate an approximation model from on-line training. Accordingly, determining approximation models for on-line training problems is within the scope of the present disclosure as one of ordinary skill in the art will appreciate upon review of the novel approximation techniques described herein.","Testing for Structure in Model Residuals.","As indicated earlier, essential information can be inferred about the appropriateness of an approximation model by examining its residuals with the data set from which the model was derived. A premise for the present approximation method and system is that if there is (non-random) structure remaining in the residuals, then one or more additional basis functions should be added to the model to capture the structure residing in the residuals. On the other hand, if there is no discernible structure in such residuals, then it is presumed that there is substantially no information content in the residuals to be models, and according such lack of structure constitutes a stopping criterion for the development of the corresponding model.","Thus, the present approximation method and system is useful for modeling data that may be viewed as the superposition of, e.g., a signal together with IID noise. However, if perchance, the data is noise free then IID noise may be added to the data so that the present approximation method and system may be employed thereon. In view of this, when a model is derived according to the present approximation method and system, it is expected that the model residuals will be IID while the model itself should represent substantially all of the geometric structure residing in the data. Hence, an indication that an RBF model, derived from an embodiment of the present approximation method and system, is unsatisfactory is the failure of the residuals to satisfy a hypothesis test for being entirely or substantially IID noise. This observation forms the basic idea for the stopping criterion used herein. The primary advantage of such a criterion is that the hypothesis test for the criterion does not involve any ad hoc parameters that require adjustment.","The IID test for determining structure in the residuals and its associated algorithm are described.","Statistical Background for Testing for IID Noise.","Assuming the terminology described hereinabove, wherein f denotes the (current version of the) approximation model, residual for the nth data point or sample from which the model was derived is defined as\n\n(\u03c7).\n\nFollowing [4], we denote the set of residuals for a model of order K, as\n\n},\u2003\u2003(3.1)\n\nwhere L is the cardinality of the training set. The standard definition for the sample autocorrelation function, {tilde over (\u03c1)}(h), (ACF) for a set of residuals e, e, e, . . . , ewith sample mean \u0113 and lag h is defined as\n",{"@attributes":{"id":"p-0076","num":"0228"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":{"mover":{"mi":"\u03c1","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},"mo":"=","mfrac":{"mrow":[{"mover":{"mi":"\u03b3","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},{"mover":{"mi":"\u03b3","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mn":"0"}}]}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"3.2"}}]}}}},"br":{}},{"@attributes":{"id":"p-0077","num":"0229"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":"\u03b3","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},{"mfrac":{"mn":"1","mi":"L"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"i","mo":"=","mn":"1"},{"mi":"L","mo":"-","mrow":{"mo":["\uf603","\uf604"],"mi":"h"}}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mi":"a","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"h","mo":",","msub":{"mi":["e","i"]}}}},"mo":"."}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"3.3"}}]}}}},"br":[{},{}],"in-line-formulae":[{},{}],"i":["h,e","e","\u2212\u0113","e","\u2212\u0113"],"sub":["i","i+|h|","i"]},"For a fixed lag h (where \u2212L+1\u2266h\u22660, h an integer), the quantity \u03b1(h, e) is the contribution of the iresidual to the autocorrelation function. Below, the quantity \u03b1 is further described, and it is shown that \u03b1 reveals critical information concerning where the support for new basis functions should be relative to the series of points in the data set being modeled. Given the importance of \u03b1, it is referred to herein as the autocorrelation contribution. Accordingly, there is a function ACC(or simply ACC if there is no confusion in the value of h) which may written as ACC(i)=\u03b1(h, e), i=1, . . . , L\u2212h, for fixed h.","IID Hypothesis Test.","As indicated above, the addition of new basis functions is terminated when the residuals appear to have no further structure. As a test for structure in the residuals, a determination is made as to whether the sequence of residuals is IID (i.e., independent and identically distributed). The relevant theorem from statistics states that for large L, the sequence of sample autocorrelations of an IID sequence U, U, . . . , Uwith finite variance is approximately IID with normal distribution with mean zero and variance 1\/L i.e., N(0, 1\/L), as referred to in the literature [7]. Hence, if e, e, . . . , eis a realization of such an IID sequence, then the sequence of autocorrelations {tilde over (\u03c1)}(h), for 0<|h|\u2266L\u22121 is approximately IID, as one skilled in the art will understand. Accordingly, this implies that at least 95% of the sample autocorrelations should be bounded as follows:",{"@attributes":{"id":"p-0081","num":"0233"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mrow":{"mo":"-","mn":"1.96"},"msqrt":{"mi":"L"}},"mo":["<","<"],"mrow":[{"mover":{"mi":"\u03c1","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},{"mfrac":{"mn":"1.96","msqrt":{"mi":"L"}},"mo":"."}]}},{"mrow":{"mo":["(",")"],"mn":"3.5"}}]}}}},"br":{},"sub":["0 ","0"]},"Note that if the underlying model is known, i.e., the data to be modeled is known, there is a more accurate bound for IID hypothesis test, described in [7].","The above test of (3.5) can equivalently be written in terms of \u03c7distribution. Given",{"@attributes":{"id":"p-0084","num":"0236"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"Q","mo":"=","mrow":{"mrow":[{"mi":"L","mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mi":"T"},"mover":{"mi":"\u03c1","mo":"^"}},{"mi":"L","mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"j","mo":"=","mn":"1"},{"mi":"L","mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msup":{"mover":{"mi":"\u03c1","mo":"^"},"mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}}}}],"mo":"="}},"mo":","}}},"br":{},"sup":["2 ","2 "]},"Additional Testing Possibilities.","To show rigorously that a sequence of random variables is truly IID, higher moments (if they exist) also need to be considered, wherein such each higher moment n (n=3, 4, . . . ) is the expectation of the residuals of the model raised to the npower. Thus, a 3order moment is the expectation of the cube of the random variables. Note that if a sequence of random variables is IID, then any function of the random variables is also IID. Thus, the autocorrelation function (ACF) is not only the sequence of residuals that may be used. In addition, e.g., squares, cubes and the absolute values of such residuals must also satisfy the termination test of (3.5) or its equivalent. Although such alternative\/additional functions for higher moments are within the scope of the present disclosure, for simplicity the present disclosure primarily describes using the ACF of the residuals for determining IID and thereby terminating the generation of additional basis functions.","Note that it is believed that in many cases, the autocorrelation tests of (3.5) above and its equivalent may be sufficiently effective as a termination condition. Moreover, the test of (3.5) and its equivalent do indeed provide a necessary and sufficient condition for a sequence to be white noise, i.e., the residuals have no discernible pattern indicative of information content.","Lastly, it is noted that there are alternatives to the test described above based on the autocorrelation function for testing ND or white noise. Other such tests include the difference-sign test, the rank test, and a test based on turning point, as described in [7]. These tests might also be applied as stopping criteria individually, or in conjunction with the current test based on the autocorrelation function. In another embodiment, such a termination test may be formulated by considering the relationship between the data inputs and the residuals of the outputs, as described in [28]. For example, the covariance, g(h), between the residuals and input data, i.e.,",{"@attributes":{"id":"p-0089","num":"0241"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"\/","mi":"L"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"L"},"mo":"\u2062","mrow":{"msub":[{"mi":["e","i"]},{"mi":"x","mrow":{"mo":["(",")"],"mrow":{"mi":["i","h"],"mo":"-"}}}],"mo":"*"}}],"mo":"*"}],"mo":"="}}},"br":[{},{}],"sub":["i ","(i-h) "],"sup":["th ","th "]},"In this section the details for generating an approximation model from batch data is described with reference to . In particular, the question of whether a new basis function should be added is answered by the IID test of (3.5) above (or its equivalent). Note that this test also indicates where the new basis function should be added. The concept of a space-time ball for defining local regions within the batch data is also described.","Initially, it is assumed that the residuals of the data are equal to the original data (i.e., the x's hereinabove), and\/or the original data with noise purposefully added thereto.","The variable ran_flag is a flag indicating whether to continue with the derivation of the approximation model, i.e., if ran_flag is 1, the derivation continues, and if ran_flag is 0, then the derivation terminates. Accordingly, in step , ran_flag is set to 1. The variable K identifies the current order of the model. Accordingly, in step , K is set to 0. In step , the identifier, MODEL, that provides access to the data structure for the approximation function being derived is initialized to NULL. Note that the data structure accessed by MODEL may be, e.g., a list of pointers, wherein each pointer may be used to access a particular approximation function. In step , a loop commences, the loop ending at step . Each iteration of this loop inserts an additional approximation function (e.g., a RBF) into the approximation model for MODEL. Accordingly, this loop continues for as long as ran_flag is set to 1. In the first step of the loop (step ), a determination is made as to whether the approximation model being derived has no basis functions therein; i.e., MODEL is equal to NULL. If so, then in step  the values for the residuals, {e}, for this currently empty model are set to the training data set {f(x)}used for generating the approximation model; i.e., each residual eof {e}is assigned the value f(x).","Alternatively, if there is already at least one basis function in the model identified by MODEL, then the steps commencing at step  is performed.","Accordingly, in step , an inner loop (including steps  and ) commences for computing an autocorrelation function ACF. In particular, in step , for each lag (or shift) in the time domain h, 0<h\u2266L\u2212h, the following the autocorrelation term is computed for each residual ewhere 1\u2266i\u2266L\u2212|h|:\n\n\u03b1()=()()\n\nwherein \u0113 is the mean of the residual samples e. Note that various data structures for \u03b1 may be utilized. For example, since both h and i are integers, \u03b1(h, e) may be represented as an array or list ALPHA[h, i], \u2212L+1\u2266h\u22660, 1\u2266i\u2266L\u2212|h|, wherein each ALPHA[h, i] refers to a corresponding data structure including the ordered pair (e, \u03b1(h, e)).\n","Then in step , the value of the autovariance function ACF for h is computed as",{"@attributes":{"id":"p-0096","num":"0248"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"ACF","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},{"mfrac":{"mn":"1","mi":"L"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"i","mo":"=","mn":"1"},{"mi":"L","mo":"-","mrow":{"mo":["\uf603","\uf604"],"mi":"h"}}]},"mo":"\u2062","mrow":{"mrow":{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"h","mo":",","msub":{"mi":["e","i"]}}}},"mo":"."}}}],"mo":"="}}},"br":{},"b":["136","140"]},"Subsequently, in step , a determination is made as to whether at least 95% of the ACF(h)'s do not satisfy equation (3.5), or at least one of the values ACF(h) is substantially outside of the range identified in (3.5) hereinabove. If the sequence of ACF(h)'s is not determined to be IID, then steps  through  are performed. Accordingly, in step  the value h* is determined as a lag h having a largest ACF value. Note, if there are two or more ACF(h) for this maximum, then one can be chosen at random. Accordingly, in step , a location for the center of a new basis function is determined. In one embodiment, this location is determined by first determining the index i* of an autocorrelation contribution term that is largest, i.e., determine a largest \u03b1(h*, e) for 1\u2266i\u2266L\u2212|h*|. Then, the corresponding domain point x* is determined, i.e., x*\u2190x*. Step  is now performed, wherein the autocorrelation function ACC*(i)=\u03b1(h*, e), i=1, . . . , L\u2212|h*| is constructed. Note that since the array or list ALPHA[h, i] discussed above may be already computed, it is a simple matter to utilize the subarray\/sublist ALPHA[h*, j], 1\u2266j\u2266L\u2212|h| for the function ACC*. Subsequently, in step  as an optional step, the function may be denoised by performing a denoising algorithm such as a wavelet denoising algorithm from one or more such algorithms well known to those skilled in the art.","Now that the center x* of the new basis function has been found as described above, in step , a determination is made as to what data should be used to determine the scale and weight of the new basis function to be added to MODEL. For the function ACC*(i)=\u03b1(h*, e), the index i is inherited from the data labels of the input data (x), and in the case of a time-series, i corresponds to a time ordering. In practice, if ACC* is plotted (as a function of i), the values of ACC*(i) decrease as i gets further away from i* which is what is to be expected since i* was selected to correspond with a local maximum in Equation (4.2). How quickly the values of ACC*(i) decrease for both i>i* and i<i* is a property of the scale of the data and the approximation model (MODEL) being generated.","For simplicity, it is assumed that ACC*(i) decreases monotonically for both increasing and decreasing values of i until local minima are reached at the indices l*<i* and r*>i*; wherein l, r denote such left and right local minima, respectively. Accordingly, the following distances are computed in step \n\n(*) and (*),\n\nwherein these distances indicate the size of the data ball around the center x*. Accordingly, in step , the subset of the data employed to update the new basis function to be added is then X={x\u03b5X:\u2225x\u2212x*\u2225\u2266d},\n\nwhere X is the entire training set, and the distance dcan be selected in a variety of ways. In one embodiment, dmay be selected as\n\n=max{}.\n\nHowever, alternative selections for dmay be where the ACC function either changes sign (where again we are moving away from the peak of the ACC function in both directions) or where the ACC function has a minimum where the value of the ACC function is small.\n","Note that Xmay contain data whose indices have values that are substantially different from i*, l* and r*. For the original training data being time-series data, it is apparent that spatial neighbors within Xmay not necessarily be temporal neighbors. Hence, this spatial-temporal windowing provided by Xhas the potential to capture substantial training data that would not otherwise be captured. For periodic or quasi-periodic data, the inventors have found that the present space-time windowing is at least preferred if not essential. Note also that no smoothing of the ACC*(i) was required to accurately determine Xfor the examples provided herein. However, it may be possible that the ACC function can be very irregular. In such cases, the application of a smoothing algorithm to the ACC function can facilitate the computation of the zero crossings or local minima of the ACC function.","Updating the Model.","Still referring to the pseudo code of , in step , a new basis function to be added to MODEL is initialized, wherein preferably the new basis function is a radial basis function. However, other types of basis functions may be also added such as circle functions, bump functions, smooth piecewise defined functions with compact support where the non-zero segments could be cosines. Such other types of basis functions are within the scope of the present disclosure. For simplicity in the following description, it is assumed that the new basis function is radial basis function.","Note that the expansion of the approximation model MODEL having K adapted previously added basis function or terms together with the one new basis function h(x; v) may be written as\n\n()=\u0192()+(),\n\nwhere\n\n()=\u03b1\u03c6(\u2225),\n\nwith\n\n","The new radial basis term h(x; v) is determined by an iterative optimization method using the error as a cost function, wherein the data set Xis used to perform the optimization according to Step . In one instance the optimization is unconstrained, but in another implementation it is constrained such that the error on the data not being used to fit the model parameters does not increase, or does not increase significantly. In our nonlinear optimization routine we use variations of steepest descent, the conjugate gradient method as well as other quasi-Newton methods and Newton's method in addition to alternating direction descent. Accordingly, h(x; v) is first initialized as\n\n()=\u03b1\u03c6(\u2225)\n\nwhere:\n\n","Note that the vector \u03c3 can be initialized to the diagonal elements of the covariance matrix of the local data; i.e., \u03c3 is initialized to\n\n\u03c3=\u221a{square root over (diag(cov()))}.\u2003\u2003(4.3)\n\nNote that W is then the diagonal matrix having the elements of \u03c3on its diagonal. Accordingly, the diagonal matrix W is the weighting term in the inner product and has its diagonal elements equal to the components in \u03c3. This initialization of the weights has proven to be extremely valuable in accelerating the convergence of the conjugate gradient iteration. The initial value for the weight, \u03b1, is calculated via least squares using the initial values for center location and widths. That is, we use a singular value decomposition of the interpolation matrix to compute the least squares solution, as one skilled in the art will understand.\n","Given the above initializations, in step , the scale (width), weight and the center location, (i.e., these values collectively being the parameter v above), of the new basis function are optimized using the conjugate gradient method with cost function",{"@attributes":{"id":"p-0106","num":"0266"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"E","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c5"}},{"munder":{"mi":["min","\u03c5"]},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"x","mo":"\u2208","msub":{"mi":["\u03c7","local"]}}},"mo":"\u2062","mrow":{"msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mrow":{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","\u03c5"],"mo":";"}}},"mo":"-","mi":"y"}},"mn":["2","2"]},"mo":"."}}}],"mo":"="}}},"br":{},"sub":["local ","2"],"sup":"2 "},"Subsequently, in step , the optimized new basis function h(x; v) is added to the approximation model MODEL, and in step , the order of MODEL is updated to indicate the current number of basis functions therein.","After step , step  is performed wherein MODEL is evaluated over the entire training data set. Then in step , a new set of residuals is determined over the entire training set for the current version of MODEL. Subsequently, in step , runtime performance measurements may be also computed. In particular, a root mean square error (RMSE), as provided in (4.4) hereinbelow, may be computed together with the value, ACF(h*), of the autocorrelation function ACF. Then in step , a confidence level or measurement is computed that is indicative of how closely MODEL approximates the training set. In one embodiment, such a confidence level may be determined using the hypothesis test for IID noise using the autocorrelation function, as described in the IID Hypothesis Test section hereinabove. Note that this test is only a necessary condition when applied to the residuals. In one embodiment, the confidence level may be determined by applying the autocorrelation test to functions of the residuals, e.g., the squares of the residuals. The confidence levels for the different functions of residuals may be computed simultaneously to produce a more robust test. For example, the IID test may be applied simultaneously to the residuals, the squares of the residuals and the cubes of the residuals and the confidence level of each set of residuals may be required to be at the 95% confidence level. Alternatively, the different functions of residuals may be computed sequentially to reduce the expense of step .","The confidence level is used in step  to determine whether the approximation model fits the training data close enough. That is, if the confidence level is above a predetermined threshold (e.g., 95% or higher), then in step , the flag, ran_flag is set to 0 which will cause the process to terminate when step  is subsequently evaluated.","Examples are presented hereinbelow that illustrate the effectiveness of this stopping criterion of step . However, other stopping conditions are also within the scope of the present disclosure. In particular, the stopping confidence level may be provided as a parameter that may vary depending, e.g., on the training set, etc. Thus, confidence levels in the range of 87% to 97% may be appropriate. Additionally, it is within the scope of the present disclosure, to determine trends in the autocovariance function as well as the RMSE. Note that these stopping criteria are evaluated on the training data as well as the validation data, wherein the training data is a subset of the data from which training progress may be inferred.","If in step , it is determined that the disjunction of this step is false, then step  is",{"@attributes":{"id":"p-0112","num":"0272"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"RMSE","mo":"=","msqrt":{"mrow":{"mfrac":{"mn":"1","mi":"T"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","msubsup":{"mi":["e","i"],"mn":"2"}}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"4.4"}}]}}}},"br":[{},{}],"b":"112"},{"@attributes":{"id":"p-0113","num":"0273"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"msub":{"mi":"NPE","mn":"1"},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["e","i"]}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","mrow":{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["y","i"]},"mo":"-","mover":{"mi":["y","_"]}}}}]}},"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}},"mi":"and"}},{"mrow":{"mo":["(",")"],"mn":"4.5"}}]},{"mtd":[{"mrow":{"msub":{"mi":"NPE","mn":"2"},"mo":"=","mrow":{"mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","msubsup":{"mi":["e","i"],"mn":"2"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["y","i"]},"mo":"-","mover":{"mi":["y","_"]}}},"mn":"2"}}]},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"4.6"}}]}]}}},"br":{},"figref":"FIGS. 1A-1C"},"Thus, unlike currently available techniques for nonlinear function fitting over scattered data, the method disclosed hereinabove requires no ad hoc parameters. Thus, the number of basis functions required for an accurate fit is determined automatically by the algorithm of . After the introduction of a collection of new radial basis functions in the following section, approximation modeling to several illustrative problems including modeling data on manifolds and the prediction of financial time-series are presented, wherein the models are determined using the algorithm of . Moreover, note that  is presented in the context of RBFs but in principle can be employed with other methods for function approximation such as multi-layer perceptrons.","Extensions of the Algorithm to High-Dimensional Domains and Ranges","An algorithm for constructing nonlinear models from high-dimensional domains to high-dimensional ranges from scattered data is now disclosed. Similar to the univariate case hereinabove, the algorithm progresses iteratively adding a new function at each step to refine the model. The placement of the functions is driven by a statistical hypothesis test in higher dimension that reveals geometric structure when it fails. At each step the added function is fit to data contained in a higher dimensional spatio-temporally defined local region to determine the parameters, in particular, the scale of the local model. Unlike the available non-linear function fitting methods that leave the extension of the algorithm to higher-dimensional ranges as a trivial extension of the single-dimensional range, we provide more parsimonious models using inter-correlation among the successive outputs. As in the univariate range case, this algorithm does not require ad hoc parameters. Thus, the number of basis functions required for an accurate fit is determined automatically by the algorithm. These advantages extend the scope of applicability of the univariate algorithm to a much larger class of problems that arise in nature and addressed in different areas of science. A novel feature of present disclosure is the development of the statistical hypothesis test that leads to a geometrical interpretation of structure in higher dimensions.","Testing for Structure in Multivariate Model Residuals","Denote the set of residuals for a model of order K, as\n\n},\n\nwhere e=y\u2212f(x), is the m-variate residual of the ndata point. L is the cardinality of the training set, u is the mean vector E(e), and \u0393(h)=E(ee\u2032)\u2212\u03bc\u03bc\u2032 is the covariance matrix at lag h. An unbiased estimate for u is given by\n",{"@attributes":{"id":"p-0117","num":"0277"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mover":{"mi":["e","_"]},"mo":"=","mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"\/","mi":"L"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"L"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["e","n"]},"mo":"."}}],"mo":"\u2062"}}}},"br":{},"sub":["n+h","n","ij","i,j=1 "],"sup":"m"},{"@attributes":{"id":"p-0118","num":"0278"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mover":{"mi":"\u0393","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mfrac":{"mn":"1","mi":"L"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"k","mo":"=","mn":"1"},{"mi":["L","h"],"mo":"-"}]},"mo":"\u2062","mrow":{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"h","mo":",","msub":{"mi":["e","k"]}}}}}},"mo":","}},{"mrow":{"mrow":[{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"0"},{"mi":"n","mo":"-","mn":"1"}],"mo":["\u2264","\u2264"],"mi":"h"}}]},{"mtd":[{"mrow":{"mrow":{"msup":{"mover":{"mi":"\u0393","mo":"^"},"mi":"\u2032"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":"-","mi":"h"}}},"mo":","}},{"mrow":{"mrow":{"mi":["if","n"],"mo":["\u2062","-","+"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"1"},"mo":["\u2264","\u2264"],"mi":"h","mn":"0"}}]}]}}],"mo":"="}}}},"Similar to the univariate case we decompose the ACVF into its components as \u03b1(h, e)=(e\u2212\u0113) (e\u2212\u0113)\u2032. Further more \u03b1(h, e, e)=(e\u2212\u0113) (e\u2212\u0113) is the (i, j)-component of \u03b1(h, e). In other words,",{"@attributes":{"id":"p-0120","num":"0280"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["\u03b3","ij"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},{"mrow":[{"mi":"Cov","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["e","i"],"mrow":{"mi":["k","h"],"mo":"+"}},{"mi":["e","k","j"]}],"mo":","}}},{"mfrac":{"mn":"1","mi":"L"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"k","mo":"=","mn":"1"},{"mi":["L","h"],"mo":"-"}]},"mo":"\u2062","mrow":{"mrow":{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"h","mo":[",",","],"msubsup":[{"mi":["e","k","i"]},{"mi":["e","k","j"]}]}}},"mo":"."}}}],"mo":"="}],"mo":"="}}},"br":{},"sub":["k","k","k"],"sup":["i","j"]},"The estimate of the correlation matrix function R(\u00b7) is given by",{"@attributes":{"id":"p-0122","num":"0282"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mover":{"mi":"R","mo":"^"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},{"msubsup":{"mrow":[{"mo":["[","]"],"mrow":{"msub":{"mover":{"mi":"\u03c1","mo":"^"},"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}}},{"mi":"i","mo":",","mrow":{"mi":"j","mo":"=","mn":"1"}}],"mi":"m"},"mo":"=","mrow":{"msubsup":{"mrow":[{"mo":["[","]"],"mrow":{"mrow":{"msub":{"mover":{"mi":"\u03b3","mo":"^"},"mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mover":{"mi":"\u03b3","mo":"^"},"mi":"ii"},"mo":"\u2061","mrow":{"mo":["(",")"],"mn":"0"}},{"msub":{"mover":{"mi":"\u03b3","mo":"^"},"mi":"jj"},"mo":"\u2061","mrow":{"mo":["(",")"],"mn":"0"}}],"mo":"\u2062"}},"mfrac":{"mrow":{"mo":"-","mn":"1"},"mn":"2"}}}},{"mi":"i","mo":",","mrow":{"mi":"j","mo":"=","mn":"1"}}],"mi":"m"},"mo":"."}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"4.3"}}]}}}}},"Where {circumflex over (\u03b3)}(h) is the (i, j)-component of {circumflex over (\u0393)}(h). If i=j, {circumflex over (\u03c1)}reduces to the sample autocorrelation function of the ith series. For the asymptotic behavior and the convergence properties of the sample mean and covariance functions see [7].","As was mentioned in the univariate case described hereinabove, we seek to terminate the addition of new basis functions when the residuals appear to have no further structure. As a test for structure, we consider whether the residuals are IID. In what follows we provide the definition of multivariate white noise. The m-variate series {e}, t\u03b5 is said to be white noise with mean 0 and covariance matrix \u03a3, written as {e}\u02dcWN(0, \u03a3) if and only if eis stationary with mean vector 0 and covariance matrix function",{"@attributes":{"id":"p-0125","num":"0285"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"\u0393","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"\u03a3","mo":","}},{"mrow":{"mrow":{"mi":["if","h"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":"=","mn":"0"}}]},{"mtd":[{"mrow":{"mn":"0","mo":","}},{"mi":"otherwise"}]}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"4.4"}}]}}}}},"{e}\u02dcIID(0, \u03a3) indicates that the random vectors {e} are independently and identically distributed with mean 0 and variance \u03a3.","In general, the derivation of the asymptotic distribution of the sample cross-correlation function is quite complicated even for multivariate moving averages, [7]. The methods of the univariate case are immediately adaptable to the multivariate case. An important special case arises when the two component time series are independent moving averages. The asymptotic distribution of {circumflex over (\u03c1)}(h) for such a process is given in the following theorem,","Theorem [7]: Suppose that",{"@attributes":{"id":"p-0128","num":"0288"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"X","mrow":{"mi":"t","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"}},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"msub":[{"mi":["\u03b1","j"]},{"mi":"Z","mrow":{"mrow":{"mi":["t","j"],"mo":"-"},"mo":",","mn":"1"}}],"mo":"\u2062"}}},{"mrow":[{"mo":["{","}"],"msub":{"mi":"Z","mrow":{"mi":"t","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"}}},{"mi":"IID","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"0","mo":",","msubsup":{"mi":"\u03c3","mn":["1","2"]}}}}],"mo":"\u223c"}],"mo":[",",","]}},{"mrow":{"mo":["(",")"],"mn":"4.5"}}]},{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"X","mrow":{"mi":"t","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2"}},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"msub":[{"mi":["\u03b2","j"]},{"mi":"Z","mrow":{"mrow":{"mi":["t","j"],"mo":"-"},"mo":",","mn":"2"}}],"mo":"\u2062"}}},{"mrow":[{"mo":["{","}"],"msub":{"mi":"Z","mrow":{"mi":"t","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2"}}},{"mi":"IID","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"0","mo":",","msubsup":{"mi":"\u03c3","mn":["2","2"]}}}}],"mo":"\u223c"}],"mo":[",",","]}},{"mrow":{"mo":["(",")"],"mn":"4.6"}}]}]}}},"br":{},"sub":["t1","t2","j","j","j","j"]},{"@attributes":{"id":"p-0129","num":"0289"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mover":{"mi":"\u03c1","mo":"^"},"mn":"12"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"h"}},{"mrow":{"mi":"AN","mo":["\u2062","(",")"],"mrow":[{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},{"mi":"o","mo":",","mrow":{"msup":{"mi":"n","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":"\u03c1","mn":"11"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}},{"msub":{"mi":"\u03c1","mn":"22"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}}],"mo":"\u2062"}}}}]},"mo":"."}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"is"}},{"mrow":{"mo":["(",")"],"mn":"4.7"}}]}}}}},"If h, k\u22670 and h\u2260k, then the vector ({circumflex over (\u03c1)}(h), {circumflex over (\u03c1)}(h))\u2032 is asymptotically normal with mean 0, variances as above and covariance,",{"@attributes":{"id":"p-0131","num":"0291"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"n","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mrow":{"mo":"-","mi":"\u221e"}},"mi":"\u221e"},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":"\u03c1","mn":"11"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}},{"mrow":{"msub":{"mi":"\u03c1","mn":"22"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["j","k","h"],"mo":["+","-"]}}},"mo":"."}],"mo":"\u2062"}}}},{"mrow":{"mo":["(",")"],"mn":"4.8"}}]}}}}},"Without knowing the correlation function of each of the processes it is impossible to decide if the two processes are uncorrelated with one another. The problem is resolved by prewhitening the two series before computing the cross-correlation {circumflex over (\u03c1)}(h), i.e., transfer the two series to white noise by application of suitable filters. In other words any test for independence of the two component series cannot be based solely on estimated values of the cross-correlation without taking in to account the nature of the two component series. Note that since in practice the true model is nearly always unknown and since the data X, t\u22660, are not available, it is convenient to replace the sequences {Z} by the residuals, which if we assume that the fitted models are in fact the true models, are white noise sequences. To test the hypothesis Hthat {X} and {X} are independent series, we observe that under H, the corresponding two prewhited series {Z} and {Z} are also independent. Under H, the above theorem implies that the sample autocorrelations {circumflex over (\u03c1)}(h) and {circumflex over (\u03c1)}(k), h\u2260k of {Z} and {Z} are asymptotically independent normal with mean 0 and variances n. An appropriate test for independence can therefore be obtained by comparing the values of |{circumflex over (\u03c1)}(h)| with",{"@attributes":{"id":"p-0133","num":"0293"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mn":"1.96","mo":"\u2062","mrow":{"msup":{"mi":"n","mfrac":{"mrow":{"mo":"-","mn":"1"},"mn":"2"}},"mo":"."}}}},"br":{},"sub":["t1","0 ","12","12","t1","t2","22","12","0 "],"sup":["\u22121 ","\u22121"]},{"@attributes":{"id":"p-0134","num":"0294"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mo":"\u00b1","mn":"1.96"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mi":"n","mfrac":{"mrow":{"mo":"-","mn":"1"},"mn":"2"}}}}},"br":{}},"Therefore, if one computes the sample cross-correlation up to lag h and finds that more than 0.05 h of the samples fall outside the bound, or that one value falls far outside the bounds, the IID hypothesis is rejected. This test can equivalently be written in terms of \u03c7distribution. Given",{"@attributes":{"id":"p-0136","num":"0296"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"Q","mo":"=","mrow":{"mrow":[{"mi":"L","mo":["\u2062","\u2062"],"msubsup":{"mover":{"mi":"\u03c1","mo":"^"},"mn":"12","mi":"T"},"msub":{"mover":{"mi":"\u03c1","mo":"^"},"mn":"12"}},{"mi":"L","mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"j","mo":"=","mn":"1"},{"mi":"L","mo":"-","mn":"1"}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mover":{"mi":"\u03c1","mo":"^"},"mn":["12","2"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"j"}}}}],"mo":"="}},"mo":","}}},"br":[{},{}],"sup":["2 ","2"],"in-line-formulae":[{},{}],"i":["Q>\u03c7","L\u2212"],"sub":"1-\u03b1"},"The discussion hereinabove of the Theorem closely follows the presentation in [7].","Multivariate Incremental Algorithm","Pseudo code of this algorithm is provided in .","Although the notation is somewhat different, the main difference between the univariate algorithm of  and the multivariate algorithm of  is the statistical hypothesis test. The question of whether a new basis function should be added is answered by the IID test. We shall see that this test also indicates where the new basis function should be initialized. First we compute the autocorrelation functions of all m time series. If all of these pass the WN test, then the cross-correlations among the time series are considered. If there is structure in the auto-correlations or cross-correlations of the time series then the IID will be rejected.","The next requirement is to determine where the new basis function should be located to optimally reduce the structure in the model residuals. We look for the point in the domain that makes the largest contribution to the auto or cross correlation which has caused the test to fail.","This is accomplished by observing that the residuals are associated with the data in the domain in a one-to-one manner, i.e., there is a mapping, say \u03c8, from a data point to its higher dimensional residual of the form e=\u03c8(\u03c7). Thus, by identifying the residual associated with the largest contribution to auto or cross correlation we may identify the location in the domain where the basis function should be added. To actually find this point first we determine the exact lag for which the correlation function, (h) reaches its maximum value h*, i.e.,\n\n*=arg max{circumflex over (\u03b3)}(),0.\u2003\u2003(4.9)\n\nThen, we find the point in the spatial domain that has the may contribution to the associated ACF for lag h=h* by solving\n",{"@attributes":{"id":"p-0142","num":"0302"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"i","mo":"*"},"mo":"=","mrow":{"mi":"arg","mo":"\u2062","mrow":{"munder":{"mi":"max","mrow":{"mrow":[{"mi":"k","mo":"=","mn":"1"},{"mi":"n","mo":"-","msup":{"mi":"h","mo":"*"}}],"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},"mo":"\u2062","mrow":{"mrow":{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"h","mo":"*"},"mo":[",",","],"msubsup":[{"mi":["e","k","i"]},{"mi":["e","k","j"]}]}}},"mo":"."}}}}},{"mrow":{"mo":["(",")"],"mn":"4.10"}}]}}}}},"Thus the center for the new basis function is given by\n\n\u03c7*=\u03c8(*),\n\nwhere \u03c8is the inverse of the function For simplicity, we will refer to this center location as \u03c7*.\n","Now that the center of the new basis function has been found it is necessary to determine what data should be used to determine the scale and weight of the new RBF. Similar to the univariate case, [65,89], consider the function \u03b2=\u03b1(h*, e, e). The index k is inherited from the data labels and in the case of a time-series corresponds to a time ordering. For simplicity, we assume that \u03b2decreases monotonically for both increasing and decreasing values of k until crosses zero at the indices l*<i* and r*>i*; here we use l, r to indicate left and right, respectively. We now compute the distances\n\n(\u03c7*,\u03c7*)\n\nand\n\n(\u03c7*,\u03c7*)\n\nas these indicate the size of the data ball around the center \u03c7*. The subset of the data employed to update the added basis function is then\n\n\u03c7}\n\nwhere \u03c7 is the entire training set. The distance dcan be selected in a variety of ways and here we select\n\n=max{}.\n\nNote that \u03c7now may contain data whose indices have values that are substantially different from i*, l* and r*.\n","The new term in the expansion is initialized and optimized similar to the univariate case. The center cis initialized at the point of most structure according to our test, i.e., c=\u03c7*. The vector of widths a is very effectively initialized using the diagonal elements of the covariance matrix of the local data,\n\n\u03c3=\u221a{square root over (diag(cov(\u03c7)))}.\n\nNote here that W=diag(\u03c3). The initial value for the multivariate weight, \u03b1, is calculated via least squares using the initial values for center location and widths. Then the parameters associated to the new basis function is optimized in a nonlinear optimization procedure.\n","Similar to the univariate case we could use one of the statistical tests, RMSE or normalized prediction error or another measure of structure as stopping criteria.","A Novel Alternative Method","In the present section, the multivariate range values are considered as points in higher dimensions. We intend to use a test analogous to the univariate case, but extended for higher dimensions. , provides an embodiment for such an extension for higher dimensions.","Note that although the notation appears very similar to the univariate algorithm in this algorithm we are dealing with multi-dimensional domain and range values.","As an initial step to adapt a statistical test we employ \u03c7with m\u22121 degrees of freedom for each lag. Note that both the nature of the hypothesis test as well as the number of degrees of freedom employed may be varied.","The details of the algorithm for the determination of the local ball (i.e., the domain neighborhood), initialization of the center, width and weight of the new RBF are given in .","Modulated Asymmetric Radial Basis Functions","In the present section, a new class of functions is presented that can be used as RBFs and show their flexibility in data fitting over high dimensional domains. This includes non-symmetric RBFs with compact or non-compact support. The developed RBFs are well suited for a wide range of training algorithm that is being used for data fitting, [21]. We refer to this extended class of functions as modulated asymmetric RBFs or, alternatively, skew RBFs.","In general, model order determination via both regularization and growing and pruning algorithms can be computationally intensive and data hungry, however the right choice of RBFs makes this job substantially easier. Hereinabove we have observed that the nature of the condition number associated to an RBF model depends very significantly on the type of RBFs and the number and scale of representation functions. The advantage of modulated asymmetric RBFs is that they are able to better fit the geometric structure of the data at each step during the training procedure. For certain types of data, in particular data which is heavily skewed such as boundaries, or edges, in images, modulated asymmetric RBFs afford more parsimonious models than their non-modulated RBF counterparts.","Carefully observing data fitting using the non-modulated symmetric RBFs, indicates that there is significant room to improve on the way these functions match the structure of the data. We present a simple example to illustrate this idea. For purposes of illustration, we have created a time series from a single mode skew Gaussian with Gaussian IID noise with standard deviation of 0.01. The training and testing data sets are shown in , wherein the testing and training data sets for the skewed data set were generated with the parameters, c=2, t=\u22127, \u03b1=1, \u03c3=1. And the output of the single mode model using modulated RBFs. The fit using modulated Gaussian-Gaussian RBFs via the algorithm given hereinabove, is shown in . The skew Gaussian fit is done only with one RBF and the RAISE of the final fit is 0.0029. The algorithm terminates with 98.8% of confidence. The fit using regular Gaussian RBFs is shown in . In this case, as shown in , the fit requires 13 RBFs and RMSE of the final model is 0.0035, while 96.80% of confidence was achieved at termination of the algorithm.","Definition of Modulated RBFs","Based on the observations made in the previous section, if the data being fit is asymmetric, then breaking the symmetry of the basis functions may produce models of lower order and potentially higher accuracy. For example, fitting a discontinuous Heaviside function with symmetric radial basis functions gives rise to a Gibbs type phenomenon that is not present with appropriately skewed RBFs. Modulated RBFs may be generated by multiplying a non-symmetric scalar function (that possesses multiple shape parameters) to any form of symmetric RBFs. This basic idea is reminiscent of skewed distributions studied in probability theory but is more general in that we do not require the shape function to be a cumulative distribution function nor the symmetric RBF to be a probability distribution function. Note that it is also distinct from asymmetric RBFs referred to as normalized RBFs which are motivated by probabilistic considerations and do not afford the same variety of shape parameters.","The idea of modeling skewed distribution functions in statistics can be traced back to 1908, [82], where perturbation of the normal density via a uniform distribution function leads to a form of skew-normal density. Although it is mathematically somewhat different from the form that is presented in current literature, its underlying stochastic mechanism is intimately related. Fundamental skew-symmetric distributions are studied in [53]. For specific references on skew Cauchy distributions, see [66, 85, 54], for skew t distributions, [96, 59], skew-logistic distributions, [141], and skew-elliptical distributions, [70]. We would like to concentrate on the specific formulation of skew-normal distribution. The formal definition of the univariate skew-normal (SN) family is due to Azzalini [56]. A random variable Z has an SN distribution with skewness parameter \u03bb, and is denoted by Z\u02dcSN(\u03bb), if its density is \u0192(z|\u03bb)=2\u03c6(z)\u03a6(\u03bbz), with z is a member of R, \u03bb is a member of R. \u03c6 and \u03a6 are pdf and cdf of N(0, 1), respectively. The case where \u03bb=0, reduces to N(0, 1). Further probabilistic properties of this distribution are studied in [56, 57]. The multivariate SN family of densities are introduced in [60] which is given by \u0192(z|\u03bb)=2\u03c6(z)\u03a6(\u03bbz), z is a member of R, \u03bb is a member of R. \u03c6is the probability density functions of k-dimensional normal distribution, N(0, 1). Similar to what is mentioned above the case where \u03bb=0 corresponds to N(0, I). Further properties of the multivariate SN distribution are studied in [58].","A closer look at two types of multivariate skew-normal distributions is provided, and connections are drawn to RBFs, [92]. They are:",{"@attributes":{"id":"p-0157","num":"0317"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["f","p"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["y","\u03bc"],"mo":";"},"mo":[",",","],"mi":["\u03a3","D"]}}},{"mfrac":{"mn":"1","mrow":{"msub":{"mi":["\u03a6","p"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"0","mo":";","mrow":{"mi":"I","mo":"+","mrow":{"mi":["D","\u03a3"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msup":{"mi":["D","\u2032"]}}}}}}},"mo":["\u2062","\u2062"],"mrow":[{"msub":{"mi":["\u03d5","p"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["y","\u03bc"],"mo":";"},"mo":",","mi":"\u03a3"}}},{"msub":{"mi":["\u03a6","p"]},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mi":"D","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","\u03bc"],"mo":"-"}}}}}]}],"mo":"="}}},"br":[{},{},{}],"img":{"@attributes":{"id":"CUSTOM-CHARACTER-00002","he":"3.13mm","wi":"2.46mm","file":"US08521488-20130827-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},"sup":["p","T"],"sub":["p","p","p","p","1","1 "],"in-line-formulae":[{},{}]},{"@attributes":{"id":"p-0158","num":"0318"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"msub":{"mi":"\u03a6","mn":"1"},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"msup":{"mi":["\u03bb","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","\u03bc"],"mo":"-"}}}}},{"msubsup":{"mo":"\u222b","mrow":[{"mo":"-","mi":"\u221e"},{"msup":{"mi":["\u03bb","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["y","\u03bc"],"mo":"-"}}}]},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":"\u03d5","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["x","\u03bc"],"mo":";"},"mo":",","mi":"\u03a3"}}},{"mo":"\u2146","mi":"x"}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}}}}],"mo":"="},"mo":[",","."]}},{"mrow":{"mo":["[","]"],"mn":"60"}}]}}}}},"The latter formulations is used to motivate modulated RBFs.","Let",{"@attributes":{"id":"p-0161","num":"0321"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["w","i"]},"mo":"\u2062","mrow":{"msubsup":{"mi":["\u03d5","s"],"mrow":{"mo":["(",")"],"mi":"i"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}}],"mo":"="},"mo":","}}},"br":{},"sup":"(i)","sub":["s","p","i","i","i","i"]},{"@attributes":{"id":"p-0162","num":"0322"},"maths":{"@attributes":{"id":"MATH-US-00027","num":"00027"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["s","p"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":[",",","],"msub":[{"mi":["\u03bc","i"]},{"mi":["W","i"]}]}}},{"msubsup":{"mo":"\u222b","mrow":{"mo":"-","mi":"\u221e"},"msup":{"mover":{"mi":"x","mo":"~"},"mrow":{"mi":["T","\u03bb"],"mo":"\u2062"}}},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":"\u03d5","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"y"}},{"mo":"\u2146","mi":"y"}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}}}}],"mo":"="}}},"br":{}},{"@attributes":{"id":"p-0163","num":"0323"},"maths":{"@attributes":{"id":"MATH-US-00028","num":"00028"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mover":{"mi":"x","mo":"~"},"mi":"j"},"mo":"=","mrow":{"mfrac":{"mrow":{"msub":[{"mi":["x","j"]},{"mi":["\u03bc","ij"]}],"mo":"-"},"msub":{"mi":["\u03c3","ij"]}},"mo":"."}}}},"br":{},"sub":["p","i","i","i","i ","i "]},"To generate modulated RBFs we could combine different cdfs and pdfs of variety of distributions. For examples, Gaussian-Cauchy RBFs, Cosine-Sine RBFs, Cosine-Cauchy RBFs, and many others. All we need is a nonlinear modulator that can produce flexibility.",{"@attributes":{"id":"p-0165","num":"0325"},"figref":["FIGS. 12A","FIGS. 13A and 13B"],"b":["12","12"]},"It is important to note that compactly supported modulated RBFs can be generated using compactly supported RBFs introduced hereinabove.","We propose a class of RBFs that are symmetric with adaptive curvature. The flexibility in the curvature produces a better fit to data. Note that some of these classes remain positive definite for different values of the curvature parameter.","Modulated RBFs that are Positive Definite","Traditionally, RBFs are developed in the context of interpolation. Here we would like to show that it is possible to generate modulated RBFs that produce positive definite interpolation matrix. We pursue an approximation theoretic solution to the problem. We start with a positive definite function and show that up to certain limits for the modulation parameter the interpolation matrix remains positive definite. We utilize ideas form perturbation theory and the lower bounds of the inverse of an interpolation matrix [69, 118, 128, 133, 114, 117, 129].","5.3 Impact on Other Type of Networks","We have also considered the connections to other type of networks, e.g., support vector machines. The support vector machines (see e.g., [139]) are connected to radial basis functions via the RBF Kernels [132]. At the heart of the SVM there is an inner product which could be computationally expensive. Based on Mercer's theorem [113], the positive definite inner product kernel's are used to facilitate this work. In this study we would like to introduce modulated Kernel SVMs. Clearly modulated RBFs possess the property that K(\u03c7, \u03c7\u2032)=K(\u03c7\u2032, \u03c7) and we have shown the bounds in which the given kernel remains positive definite. For relation between RBFs and multilayered perceptrons see, e.g., [11, 88]. The method of mixture models immediately benefit from this structure if it assumes the basis functions to be modulated.","5.4 Impact on Signal and Image Processing","To show the promise of these RBFs we look at Mackey-Glass time series, which produces a map from to . In [89] we report 76 RBFs to get the 95% confidence with RMSE of 0.0116. The same confidence with a similar error rate is achieved with 42 modulated Gaussian RBFs.","The numerical experiments suggest that the proposed radial functions significantly extend the scope of currently used RBFs and improve the computational cost as well as the the complexity of the models. It is important to note that the modulated RBFs are capable of producing models with smaller error even if the the model based on symmetric RBFs has the same order as the model based on asymmetric RBFs. We intend to explore the capability of these innovative RBFs in the context of modeling data on manifold and prediction of financial time-series using the algorithms proposed hereinabove (i.e., , , and ).","Image Reconstruction Application","Also within the scope of the present disclosure are applications of these RBFs in the context of image reconstruction, or, more generally, where the data is spatial and there is no time parameterization. For this purpose, it is believed that the following algorithm is useful.","Algorithm A Proposed Algorithm for Image Reconstruction.","Identify the edges using a simple edge detector.","Find the most dominate edge.","Adapt a modulated RBF to a patch of data around the identified point. Repeat the procedure till a satisfactory result is achieved.","This algorithm differs from that described in  in that there is no time ordering of the data points. Of course one may artificially endow a set of data with a time-parameterization by connecting the points arbitrarily by a curve. Alternatively, it is useful to identify points in the spatial image that have the most structure and these should be candidates for adding basis functions. In image processing a measure of structure is the image gradient which reflects the presence of an edge when it is large. Thus, one embodiment of the proposed algorithm proceeds by iteratively placing functions where the image gradient is a maximum. This process continues until the image gradient at all points on the image is sufficiently small. The size of small is application dependent. Of course the IID test may be employed in conjunction with this algorithm.","More generally, one may use other measures of geometric structure to guide the placement of basis functions.","Since the above algorithm fits the structure of the data and does not capture the noise, by design, this approach is well suited for image de-noising as well as image compression.","Another Variation (Symmetric) on RBFs","An RBF based on subtraction of two log-sigmoid functions is used in [111], to generate localized robust RBFs. In [55], an extension of the robust RBFs is presented that uses a composite product of log-sigmoidal functions to make localized RBFs. In [98], and related prior work on reformulated RBFs aim to facilitate training by supervised learning based on gradient descent. The approach is based on selecting an admissible generator functions. For example, for in >1, the exponential generator function g(x)=exp(\u03b2\u03c7), \u03b2>0, corresponds to g(\u03c7)=exp(\u03b2\u03c7\/1\u2212m) which leads to Gaussian RBF",{"@attributes":{"id":"p-0181","num":"0341"},"maths":{"@attributes":{"id":"MATH-US-00029","num":"00029"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"msub":{"mi":["\u03d5","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"mrow":[{"msub":{"mi":["g","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"x","mn":"2"}}},{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":{"mo":"-","msup":{"mi":"x","mn":"2"}},"msubsup":{"mi":["\u03c3","j"],"mn":"2"}}}}],"mo":"="}],"mo":"="},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0182","num":"0342"},"maths":{"@attributes":{"id":"MATH-US-00030","num":"00030"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mi":["\u03c3","j"],"mn":"2"},"mo":"=","mrow":{"mfrac":{"mrow":{"mi":"m","mo":"-","mn":"1"},"msub":{"mi":["\u03b2","j"]}},"mo":"."}}}},"br":{},"sub":["j0","j","j","j","j"]},{"@attributes":{"id":"p-0183","num":"0343"},"maths":{"@attributes":{"id":"MATH-US-00031","num":"00031"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"msub":{"mi":["\u03d5","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"mrow":{"msub":{"mi":["g","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"x","mn":"2"}}},"mo":"=","msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["a","j"]},"mo":"\u2062","msup":{"mi":"x","mn":"2"}},"mo":"+","msub":{"mi":["b","j"]}}},"mfrac":{"mn":"1","mrow":{"mn":"1","mo":"-","mi":"m"}}}}],"mo":"="},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0184","num":"0344"},"maths":{"@attributes":{"id":"MATH-US-00032","num":"00032"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["\u03d5","j"]},"mo":"=","mrow":{"mrow":[{"msub":{"mi":["g","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":"x","mn":"2"}}},{"msup":{"mrow":{"mfrac":{"mn":"1","mrow":{"msup":{"mi":"x","mn":"2"},"mo":"+","msubsup":{"mi":["\u03b3","j"],"mn":"2"}}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mfrac":{"mn":["1","2"]}},"mo":"."}],"mo":"="}}}},"br":{}},{"@attributes":{"id":"p-0185","num":"0345"},"maths":{"@attributes":{"id":"MATH-US-00033","num":"00033"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":{"msub":{"mi":["\u03d5","d"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"r"}},"mo":"=","mfrac":{"mrow":{"msub":{"mi":"J","mrow":{"mfrac":{"mi":"d","mn":"2"},"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u025b","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"r"}}}},"msup":{"mrow":[{"mo":["(",")"],"mrow":{"mi":"\u025b","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"r"}}},{"mfrac":{"mi":"d","mn":"2"},"mo":"-","mn":"1"}]}}},"mo":","}}},"br":[{},{}],"sub":"\u03b1"},"Most applications employ activation or basis functions from a relatively small list, including Gaussians, multi-quadrics and thin plate splines. Recently several functions with compact support have proposed as candidate RBFs, see, e.g., [143, 142, 144]. For example, the Cfunction\n\n\u03c6()=(1\u2212)+(1+4),\u2003\u2003(5.1)\n\nhas been derived as an RBF explicitly for domain dimension 4 in the sense that the resulting square interpolation matrix is a (conditional) positive definite matrix [143]. In many cases of practical interest it appears that this interpolation condition is overly restrictive. In general, data fitting problem one is usually confronted with solving an over determined least squares problem. In this setting it seems adequate to require only that the approximating basis functions be dense in an appropriate function space. As described in [121], the conditions required of basis functions to be dense in L\u2033 (R\u2033) are very weak. We introduce several new candidate compactly supported RBFs for approximating functions in L\u2033 (R\u2033) via over-determined least squares.\n","First, in [23], we propose the bump function widely used in differential geometry",{"@attributes":{"id":"p-0188","num":"0348"},"maths":{"@attributes":{"id":"MATH-US-00034","num":"00034"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"\u03d5","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"r"}},{"mrow":[{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mn":"1","mrow":{"msup":[{"mi":"r","mn":"2"},{"mi":"\u03b3","mn":"2"}],"mo":"-"}}}},{"mi":"H","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msup":{"mi":"r","mn":"2"}}}}],"mo":"\u2062"}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"5.2"}}]}}}},"br":{},"figref":"FIG. 2A","sup":"2"},"A compact activation function with constant curvature is provided by\n\n\u03c6()=\u221a{square root over (1)}(1\u2212).\u2003\u2003(5.3)\n\nThis is just the quarter circle shown in . Clearly this function satisfies the postulates of Park and Sandberg's theorem.\n","Our last proposed activation function in [23] with compact support is the Hanning filter\n\n\u03c6()=(cos(\u03c0)+1)(1\u2212).\u2003\u2003(5.4)\n\nLike the bump function, this function is also infinitely differentiable; see . It has advantages over the mollifier function in the manner in which the function approaches zero, i.e., there is no vanishing term in a denominator.\n","While the quality of performance of a model can be assessed using a variety of directions (e.g., regularization methods and cross validation) the inherent conditioning of the model plays a critical role in its ability to generalize. In practice, if the data model is represented generally by the mapping y=\u0192(\u03c7), of importance is how the output of the model changes as a consequence of perturbation of the input.","For nonlinear mappings, such as those generated by multi-layer perceptrons, the estimation of the condition number is complicated by the fact that the Jacobian of the map must be estimated at every point of interest [105]. This is also true in general for RBFs. However, in the case of RBFs we can determine the condition number associated with the perturbation of the parameters simply by computing the singular values of the interpolation matrix. This information provides an important measure of the sensitivity of the approximation model.","We have observed that the nature of the condition number of the interpolation matrix (here we mean both the interpolation problem where this matrix is square and the overdetermined least squares problem where this matrix is tall) depends very significantly on the type of RBFs that are employed. The proposed three compactly supported functions (5.2)-(5.4) above possess especially good conditioning properties. We illustrate their utility on the benchmark Mackey-Glass time series data, [48, 36, 25, 47], in [23]. The Mackey-Glass time series is a mapping from a time-delay embedding of the univariate time-series to a future value. The Mackey-Glass time-delay equation",{"@attributes":{"id":"p-0194","num":"0354"},"maths":{"@attributes":{"id":"MATH-US-00035","num":"00035"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":[{"mo":"\u2146","mrow":{"mi":"s","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"mo":"\u2146","mi":"t"}]},"mo":"=","mrow":{"mrow":[{"mo":"-","mrow":{"mi":"bs","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"mi":"a","mo":"\u2062","mrow":{"mfrac":{"mrow":[{"mi":"s","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","\u03c4"],"mo":"-"}}},{"mn":"1","mo":"+","msup":{"mrow":{"mi":"s","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","\u03c4"],"mo":"-"}}},"mn":"10"}}]},"mo":"."}}],"mo":"+"}}}},"br":{}},"As a measure the performance of the various RBFs on the Mackey-Glass time series, we compare the root-mean-square error (RMSE), the number of basis functions required and the sensitivity of the models via the condition number of the interpolation matrix of the full model. We present the final result of the fit using the mollifier in . In this figure the output of the model and the associated target values are presented. The results of using other RBFs are summaries in Table 5.1. Note that all the results are aimed for 95% confidence in the statistical test [3, 4].",{"@attributes":{"id":"p-0196","num":"0356"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"This table shows the performance of"},{"entry":"different RBFs under identical strategy of fit."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"Wendland RBF","Circle RBF","Mollifier"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]},{"entry":[{},"ConditionNumber","\u20023.0057e+003","12.5845","284.3114"]},{"entry":[{},"RMSE","\u20020.0109","\u20020.0344","\u20030.0167"]},{"entry":[{},"NumberofRBFs","51","26","\u200237"]},{"entry":[{},"Confidence %","95","95.27","\u200295.53"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}},"br":{}},"This section presents several additional applications to demonstrate the performance of the algorithm of  in higher dimensional domains (e.g., algorithms such as those disclosed in ). The successful extension of this algorithm from one to higher dimensional domains requires the introduction of the notion of a space-time window; i.e., the spatio-temporal ball described hereinabove. Here we illustrate the impact of this concept on several applications. Note that throughout all the following examples the same code was employed, in particular, there were no parameters that were adjusted or tuned to the data set.","A Simple Manifold Example.","In the present example a representation of data on a manifold as the graph of a function is provided for modeling. In particular, a graph  of the pringle data set is shown in , named as such given its similarity to the boundary of a potato chip by the same name; see also [9, 8]. The task is to construct a mapping from an (x, y) value in the plane to its corresponding z value on the pringle data set. Thus, we are attempting to fit or approximate the graph of a function from Rto R. Such graph fitting problems are at the center of the Whitney's manifold embedding theorem where 2m+1 dimensional domains suffice (in general) to write m dimensional manifolds as graphs; see [9, 8] for a discussion.","Note that the pringle data set, as proposed in [10], can be generated as the solution to the following systems of ordinary differential equations",{"@attributes":{"id":"p-0201","num":"0361"},"maths":[{"@attributes":{"id":"MATH-US-00036","num":"00036"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":[{"mo":"\u2146","mi":"x"},{"mo":"\u2146","mi":"t"}]},"mo":"=","mi":"y"}}},{"@attributes":{"id":"MATH-US-00036-2","num":"00036.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":[{"mo":"\u2146","mi":"y"},{"mo":"\u2146","mi":"t"}]},"mo":"=","mrow":{"mrow":[{"mo":"-","mi":"x"},{"mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":"x","mn":"2"},{"mi":"y","mn":"2"}],"mo":["+","-"],"mn":"1"}},"mo":"\u2062","mi":"y"}],"mo":"-"}}}},{"@attributes":{"id":"MATH-US-00036-3","num":"00036.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mfrac":{"mrow":[{"mo":"\u2146","mi":"z"},{"mo":"\u2146","mi":"t"}]},"mo":"=","mrow":{"mrow":[{"mrow":{"mo":"-","mi":"\u03bb"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"z"},{"mn":"2","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["\u03bb","xy"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"w","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":"x","mn":"2"},{"mi":"y","mn":"2"}],"mo":"-"}}}],"mo":"+"}}}],"mo":"+"}},"mo":","}}}],"br":{},"figref":["FIG. 3A","FIG. 3A","FIG. 3B","FIG. 3B","FIGS. 1A-1C"],"b":["308","304"]},{"@attributes":{"id":"p-0202","num":"0362"},"figref":["FIGS. 4A through 4D","FIG. 4A","FIGS. 4B through 4D"],"sub":["h","h","local"]},{"@attributes":{"id":"p-0203","num":"0363"},"figref":["FIGS. 5A through 5D","FIGS. 1A-1C","FIG. 3B","FIGS. 1A-1C"],"sub":"local","b":["504","504","508","508"],"i":["a ","d","a ","d"]},{"@attributes":{"id":"p-0204","num":"0364"},"figref":["FIG. 6A","FIGS. 1A-1C"],"sub":"h","b":["112","212"]},{"@attributes":{"id":"p-0205","num":"0365"},"figref":["FIG. 6B","FIG. 6C","FIG. 7","FIGS. 4A through 6C"]},"In contrast to [89], in the present disclosure the notion of \u03c7has been modified in such a way that it includes the data points between the two zero crossing of the ACC function, as one of skill in the art will understand. The reason for this modification is due to the fact that there is structure between the two places that the ACC(h) departs from the null state and there is less structure in the places that the amount of the ACC(h) is less than a certain value. There is a modification to the optimization routine. Additionally, in one embodiment, we have implemented the alternative decent directions method which only optimizes one parameter in each iteration.","In addition, to avoid the overflow effect of the RBFs to the regions that are not in the local ball, in one embodiment, constraints in the optimization procedure may be implemented such that it penalizes the growth of RBF when increasing the error on the complement of the local space-time ball.","Mackey-Glass Time Series.","The Mackey-Glass Time-Delay Equation:",{"@attributes":{"id":"p-0210","num":"0370"},"maths":{"@attributes":{"id":"MATH-US-00037","num":"00037"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":"\u2146","mrow":{"mi":"s","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"mo":"\u2146","mi":"t"}]},"mo":"=","mrow":{"mrow":[{"mo":"-","mrow":{"mi":"bs","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"mi":"a","mo":"\u2062","mrow":{"mfrac":{"mrow":[{"mi":"s","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","\u03c4"],"mo":"-"}}},{"mn":"1","mo":"+","msup":{"mrow":{"mi":"s","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","\u03c4"],"mo":"-"}}},"mn":"10"}}]},"mo":"."}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"5.1"}}]}}}}},"generates a chaotic time series with short-range time coherence, where long time prediction is very difficult; it has become a standard benchmark for testing model fitting algorithms [36, 25, 47].","The time series is generated by integrating the equation with model parameters \u03b1=0.2, b=0.1 and \u03c4=17 using the trapezoidal rule with \u0394t=1, with initial conditions \u03c7(t\u2212\u03c4)=0.3 for 0<t<\u03c4 (\u03c4=17). The initial 1000 data points corresponding to transient behavior are discarded. Then 4000 data points are reserved for the training set. The test set consists of 500 data points starting from point 5001. Note that not all 4000 training points collected were actually used for training the model. (These conditions are very similar to those in Platt [36].)","For purposes of comparison with [48], the series is predicted with \u03bd=50 samples ahead using four past samples: s, s, 4and 4. Hence, we require that the model fit the input value\n\n\u03c7=()\n\nto the output value s. The \u03b7 step prediction error is then \u03b5=s\u2212f(s, s, s, s). As such, this time series provides a good example for illustrating the construction of a nontrivial mapping from Rto R.\n",{"@attributes":{"id":"p-0214","num":"0374"},"figref":["FIG. 14A","FIGS. 1A-1C","FIG. 14B","FIGS. 1A-1C","FIG. 14C","FIG. 14D","FIG. 14A","FIG. 14B","FIGS. 15A through 15C","FIG. 16"],"b":["140","144","188"],"sub":"i"},"The algorithm of, e.g., , based on space-time balls, provides a result similar to MRAN [47] (RMSE of 0.035) using 1500 data points with 21 centers. However, at this level of RMSE, both our algorithm (21 modes) and MRAN (24 modes and 4000 data points), produce sporadic but significant overshoots and undershoots of the function in regions of high gradient. These large pointwise errors are hidden to some degree by a relatively small RMSE. The IID test is, of course, point-wise and reveals local un-modeled structure in the data and prevents the algorithm from terminating prematurely.","Yet, one might argue that stopping at 95% confidence and 76 modes is still premature stopping since a slightly improved final RMSE value of 0.0090 on the test data is achieved with 109 modes (but then does not improve with more). However, this example is for the special case of noise-free data. In such instances it is recommended that the IID test be coupled with the RMSE test to draw optimal conclusions, unless, of course, one chooses to add noise artificially to the data. Given how close the RMSE errors are at 76 and 109 modes one must seriously consider that even in this case the 95% confidence level is arguably superior.","Time Series Prediction Using Exchange Rate Data Set.","The data set for the present example consists of daily values of the Deutsche Mark\/French Franc exchange rate over 701 days;  shows a graph of this data set. As mentioned in [30], this data set has irregular non-stationary components due to government intervention in the Europe exchange rate mechanism. Following [30], since there can be \u201cday of week\u201d effects in such data, a window of 5 previous values can be used as input, giving a data set of 696 patterns. Hence, this data set forms an interesting example of a mapping from Rto R.","The training data for the approximation model generated was taken to be the first 600 data points. The test data set was taken to be the last 96 data points.  shows the output of the resulting model (1-step prediction values) and the target (market) values for the test data. The modeling process terminated with a model of order three when the 95% confidence threshold was attained (actually 97.17%). The ACC and RMSE criteria are also in agreement with the model order of three; see . The 3 mode model produces the RMSE value of 0.0043, NPE=0.2760 and NPE=0.1033. The model has centers at (3.4933, 3.9292, 3.2870, 3.8574, 4.0983), (3.2793, 3.3475, 3.3337, 3.18433.2718) and (3.3666, 3.4187, 3.6620, 3.2056, 3.6457) with widths (0.4501, 2.7037, 2.2175, 2.5672, 2.9234), (0.1136, 0.1336, 8.5380, 0.6561, 0.5541) and (0.0555, 0.0358, 0.1740, 0.1939, 0.4015), and weights 3.8595, 0.5751 and 1.3805 respectively.",{"@attributes":{"id":"p-0219","num":"0379"},"figref":["FIGS. 20A through 20C","FIGS. 20B and 20C","FIG. 20B","FIG. 20B"]},"The embodiments of the various methods disclosed hereinabove for generating approximations of data sets may be applied to various fields of endeavor. In addition to the fields of financial analysis and image reconstruction described above, applications of ,  and\/or  may be applied to simulations of:\n\n","Note that since there are no ad hoc parameters that require setting in order to generate an appropriate model, an embodiment of one or more of the approximation techniques of ,  and\/or  may be provided on a network such as the Internet, wherein a user need only provide a data type (or data structure) characterization of the training and testing data sets. Accordingly, once such a data characterization is provided, then the training and testing data sets may be provided for analysis by a user having substantially no background in modeling, simulation, approximation theory and\/or artificial intelligence.  shows an illustrative embodiment of a system for providing modeling services according to one or more of the processes of , , and\/or . Accordingly, an approximation server  may be accessed by users at various user stations , or directly as shown. In particular, a user may:\n\n","Accordingly, at a high level a user first request to initiate a modeling session with the server , wherein the user interface  contacts the controller  for initiating the session. Since the user data repository  contains identification of each user as well as modeling data (both training and testing data) for each model (to be) generated, the controller  accesses the user data repository  for identifying the user, for initiating a new modeling session (e.g., with new data), for continuing a previous modeling session. Accordingly, the controller  may input training and\/or testing data into the user data repository, wherein such data is identified both by, e.g., a user identifier and an identifier indicative of the model (to be) generated. After at least the training data for a particular model is stored in the user data repository , the user whose identification is associated with the training data may request that the controller  activate the approximation model generator  for generating a model, e.g., based on the pseudo code of ,  or . Note that as an option, a user may provide control information such as the type of basis functions to use (e.g., regular Gaussian RBFs, or Gaussian-Gaussian RBFs, RBFs based on the mollifier function or the quarter circle), and\/or a stopping criterion (e.g., a confidence level of 95% or 98%, or a RMSE of some particular value). However, it is within the scope of the present disclosure that such options can be automatically activated in various combinations by the controller  upon receiving feedback from the approximation model generator . Additionally, note that the generator  may perform utilize parallel computing techniques for generating a model. For example, each of a plurality of processors (CPUs) may be activated for determining one of the basis functions of the model on a particular Xas described hereinabove. Note that in one embodiment, different processors may perform the methods described herein (e.g., those of , , and\/or ), wherein the optimization tasks are performed according to different cost functions, e.g., different norms such as an Lnorm, Lnorm, or L norm.","Accordingly, once the generator  has completed an instance of training, the generator  outputs the data defining the generated model to the approximation models database  together with various other types of information such as the final ACF, the final RMSE, a confidence level (e.g., as described hereinabove) the number of basis functions used in generating the model, etc. Moreover, the user may be notified of such results.","As indicated above, the user may also input testing data. Such testing data is also associated with the particular user and model to which such testing data corresponds. Accordingly, the controller  may activate an approximation model analyzer  performing various statistical tests regarding how well the model conforms to and\/or predicts the testing data.","Output from the model generator  and\/or the approximation model analyzer  may include various statistics and\/or graphs related to how well the generated model conforms to the training data. In particular, one or more of the various graphs and\/or statistics illustrated in , A-D, A-C, , A-B, A, B, A-C, A, B, A-D, A-C, , , , , and\/or . Note that the data for such output may be stored in the approximation models database , and associated with the data for defining the corresponding generated model (e.g., the list of RBFs instantiated in the generated model). Moreover, note that the user data repository  and the approximation models database  can be combined into a single data repository as one skilled in the art will understand.","The foregoing discussion of the invention has been presented for purposes of illustration and description. Further, the description is not intended to limit the invention to the form disclosed herein. Consequently, variation and modification commiserate with the above teachings, within the skill and knowledge of the relevant art, are within the scope of the present invention. The embodiment described hereinabove is further intended to explain the best mode presently known of practicing the invention and to enable others skilled in the art to utilize the invention as such, or in other embodiments, and with the various modifications required by their particular application or uses of the invention."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication with color drawings will be provided by the Office upon request and payment of the necessary fee.",{"@attributes":{"id":"p-0031","num":"0183"},"figref":"FIGS. 1A-1C"},{"@attributes":{"id":"p-0032","num":"0184"},"figref":["FIGS. 2A through 2C","FIGS. 1A-1C","FIG. 2A"]},{"@attributes":{"id":"p-0033","num":"0185"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0034","num":"0186"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0035","num":"0187"},"figref":["FIGS. 4A through 4D","FIG. 3B"]},{"@attributes":{"id":"p-0036","num":"0188"},"figref":["FIGS. 5A through 5D","FIGS. 1A-1C","FIG. 3B"],"b":["504","504"],"i":["a ","d "]},{"@attributes":{"id":"p-0037","num":"0189"},"figref":["FIG. 6A","FIGS. 1A-1C"],"sub":"h","b":["112","212"]},{"@attributes":{"id":"p-0038","num":"0190"},"figref":["FIG. 6B","FIG. 3B"]},{"@attributes":{"id":"p-0039","num":"0191"},"figref":["FIG. 6C","FIG. 3B"]},{"@attributes":{"id":"p-0040","num":"0192"},"figref":["FIG. 7","FIGS. 4A through 6C"]},{"@attributes":{"id":"p-0041","num":"0193"},"figref":["FIG. 8","FIGS. 1A-1C"]},{"@attributes":{"id":"p-0042","num":"0194"},"figref":["FIG. 9","FIGS. 1A-1C"]},{"@attributes":{"id":"p-0043","num":"0195"},"figref":"FIG. 10A"},{"@attributes":{"id":"p-0044","num":"0196"},"figref":["FIG. 10B","FIGS. 1A-1C"]},{"@attributes":{"id":"p-0045","num":"0197"},"figref":["FIG. 11A","FIGS. 1A-1C"]},{"@attributes":{"id":"p-0046","num":"0198"},"figref":["FIG. 11B","FIG. 11A"]},{"@attributes":{"id":"p-0047","num":"0199"},"figref":"FIGS. 12A","b":["12","12"]},{"@attributes":{"id":"p-0048","num":"0200"},"figref":"FIGS. 13A and 13B"},{"@attributes":{"id":"p-0049","num":"0201"},"figref":["FIG. 14A","FIGS. 1A-1C"],"b":"140"},{"@attributes":{"id":"p-0050","num":"0202"},"figref":["FIG. 14B","FIGS. 1A-1C"],"b":["144","188"]},{"@attributes":{"id":"p-0051","num":"0203"},"figref":["FIG. 14C","FIG. 14D","FIG. 14A","FIG. 14B"],"sub":"i"},{"@attributes":{"id":"p-0052","num":"0204"},"figref":"FIG. 15A"},{"@attributes":{"id":"p-0053","num":"0205"},"figref":"FIG. 15B"},{"@attributes":{"id":"p-0054","num":"0206"},"figref":"FIG. 15C"},{"@attributes":{"id":"p-0055","num":"0207"},"figref":["FIG. 16","FIGS. 1A-1C"]},{"@attributes":{"id":"p-0056","num":"0208"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0057","num":"0209"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0058","num":"0210"},"figref":"FIG. 19A"},{"@attributes":{"id":"p-0059","num":"0211"},"figref":"FIG. 19B"},{"@attributes":{"id":"p-0060","num":"0212"},"figref":"FIG. 19C"},{"@attributes":{"id":"p-0061","num":"0213"},"figref":["FIGS. 20A through 20C","FIGS. 20B and 20C","FIG. 20B"]},{"@attributes":{"id":"p-0062","num":"0214"},"figref":["FIG. 21","FIGS. 1A-1C"],"b":"20"}]},"DETDESC":[{},{}]}
