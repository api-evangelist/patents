---
title: Instruction culling in graphics processing unit
abstract: Aspects of the disclosure are directed to a method of processing data with a graphics processing unit (GPU). According to some aspects, the method includes executing a first work item with a shader processor of the GPU, wherein the first work item includes one or more instructions for processing input data. The method also includes generating one or more values based on a result of the first work item, wherein the one or more values represent one or more characteristics of the result. The method also includes determining whether to execute a second work item based on the one or more values, wherein the second work item includes one or more instructions that are distinct from the one or more instructions of the first work item for processing the input data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09195501&OS=09195501&RS=09195501
owner: QUALCOMM Incorporated
number: 09195501
owner_city: San Diego
owner_country: US
publication_date: 20110712
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Aspects of the disclosure relate to processing data with a graphics processing unit (GPU).","Graphics processing devices may be implemented to carry out a variety of image processing or other general purpose processing applications. For example, a graphics processing unit (GPU), sometimes referred to as a general purpose graphics processing unit (GPGPU), may execute applications that benefit from a high degree of parallelism, such as color correction algorithms, face detection algorithms, pattern recognition algorithms, augmented reality applications, a variety of algorithm applications (e.g., wavelet transforms, Fourier transforms, and the like), or a variety of other applications.","In general, GPUs are designed to process a series of instructions, which may be referred to as shader instructions, using one or more shader processors residing in the GPU. In an example image processing application, shader instructions may define one or more mathematical operations to be performed by the shader processors on the pixels that make up the image. By applying a shader instruction to a pixel, the pixel value is changed or evaluated according to the mathematical operation defined by the shader instruction.","Shader instructions may be organized into shader program code known as a kernel. A kernel may define a function or task that is performed by the GPU. In order to execute a kernel, the program code is divided into work items (e.g., a basic unit of work in a GPU), which are organized into one or more workgroups (e.g., a set of work items).","In general, aspects of the disclosure relate to identifying irrelevant shader instructions for execution by a GPU, and preventing the irrelevant instructions from being executed. To prevent irrelevant instructions from being executed, irrelevant instructions may be \u201cculled,\u201d or removed, from a set of instructions before they are executed. According to some aspects, a GPU may implement a cull buffer to carry out instruction culling. For example, after executing an instruction, a GPU may store one or more values that represent one or more characteristics (e.g., such as a relevance characteristic) of a subsequent instruction. The GPU can use the stored values to determine whether to remove the subsequent instruction before the instruction is executed.","In one example, aspects of the disclosure are directed to a method of processing data with a graphics processing unit (GPU). According to some aspects, the method includes executing a first work item with a shader processor of the GPU, wherein the first work item includes one or more instructions for processing input data. The method also includes generating one or more values based on a result of the first work item, wherein the one or more values represent one or more characteristics of the result. The method also includes determining whether to execute a second work item based on the one or more values, wherein the second work item includes one or more instructions that are distinct from the one or more instructions of the first work item for processing the input data.","In another example, aspects of the disclosure are directed to an apparatus for processing data with a graphics processing unit (GPU). The apparatus includes a shader processor configured to execute a first work item that includes one or more instructions for processing input data, and to generate one or more values based on a result of the first instruction, wherein the one or more values represent one or more characteristics of the result. The apparatus also includes a cull module configured to determine whether to execute a second work item that includes one or more instructions that are distinct from the one or more instructions of the first work item based on the one or more values.","In another example, aspects of the disclosure are directed to a computer-readable storage medium encoded with instructions for causing one or more programmable processors of a computing device to execute a first work item, wherein the first work item includes one or more instructions for processing input data, and to generate one or more values based on a result of the first work item, wherein the one or more values represent one or more characteristics of the result. The computer-readable storage medium is also encoded with instructions for causing the one or more programmable processors of the computing device to determine whether to execute a second work item based on the one or more values, wherein the second work item includes one or more instructions that are distinct from the first work item for processing the input data.","In another example, aspects of the disclosure are directed to an apparatus for processing data with a graphics processing unit (GPU). The apparatus includes a means for executing a first work item, wherein the first work item includes one or more instructions for processing input data. The apparatus also includes a means for generating one or more values based on a result of the first work item, wherein the one or more values represent one or more characteristics of the result. The apparatus also includes a means for determining whether to execute a second work item based on the one or more values, wherein the second work item includes one or more instructions that are distinct from the first work item for processing the input data.","The details of one or more examples of the disclosure are set forth in the accompanying drawings and the description below. Other features, objects, and advantages of the disclosure will be apparent from the description and drawings, and from the claims.","Aspects of this disclosure generally relate to increasing efficiency of a graphics processing unit (\u201cGPU\u201d), which may be implemented as a general purpose graphics processing unit (\u201cGPGPU\u201d). In general, a GPU includes a plurality of shader processors (\u201cSPs\u201d) for performing calculations. The calculations may be structured in a GPGPU application that includes one or more shader programs (referred to herein as \u201ckernels\u201d). Kernels define functions that can be implemented to analyze or modify a variety of input data. Examples include functions for processing relatively large numerical data sets in parallel. In an image processing context, functions may include, for example, color correction algorithms, face detection algorithms, or functions for carrying out augmented reality applications. Other examples include transform functions, functions for ray tracing, or a variety of other functions.","Kernels are comprised of individual work items (e.g., a basic unit of work in a GPU) that may be grouped into workgroups. In an example in which a GPU is implemented to process an image (e.g., a frame of video data, a computer-generated graphics image, a still image, and the like), work items may include one or more instructions that define a function for analyzing or modifying pixels of the image. In addition, a plurality of work items can be organized into workgroups (e.g., a set of work items) for analyzing or modifying a group of pixels of the image.","Some applications may include multiple kernels for carrying out multiple functions on the same input data. Further, applications having multiple kernels may include some kernels that are dependent on other kernels. For example, an application may include two kernels, with a second kernel that is dependent on the results of the first kernel. Accordingly, in some examples, the results of a kernel may make the results of a subsequent kernel \u201cirrelevant.\u201d As described herein, an \u201cirrelevant instruction\u201d may be an instruction that does not advance the purpose of the application. Stated differently, an \u201cirrelevant\u201d instruction may be an instruction that does not change, or does not factor into, the outcome of the application.","In a simple example, an application includes two kernels for identifying a feature in dataset [x, y, z]. The first kernel and the second kernel each have three instructions, which may be defined in three distinct work items, associated with [x, y, z], respectively. The three instructions evaluate to \u201ctrue\u201d if the feature is identified by executing the instruction or \u201cfalse\u201d if the feature is not identified by executing the instruction. In addition, the second kernel is dependent on the results of the first kernel. For example, the instructions of both kernels must evaluate to \u201ctrue\u201d to identify the feature, so the instructions of the second kernel are dependent on the instructions of the first kernel evaluating to \u201ctrue.\u201d After executing the first kernel, the GPU determines that the desired feature may be included in \u201cx\u201d and \u201cy,\u201d but not in \u201cz.\u201d The instruction of the second kernel related to \u201cz,\u201d then, is irrelevant to the result of the application. For example, the second instruction related to \u201cz\u201d does not factor into the outcome of the application, because even if the second instruction related to \u201cz\u201d evaluates to \u201ctrue,\u201d the instruction has already failed the first kernel. In particular, the instruction related to \u201cz\u201d in the first kernel did not identify the desired feature. In this example application, the second kernel includes an individual irrelevant instruction (e.g., a work item), such that only a portion of the kernel is irrelevant and the remainder of the kernel may be relevant to the result.","A kernel having more than one irrelevant instruction may be described, in some circumstances, as being \u201csparse.\u201d For example, a \u201csparse\u201d kernel may include relevant instructions that are separated by many irrelevant instructions. Typically, a GPU distributes all instructions (e.g., work items and workgroups of kernels) to shader processors (SPs) for execution, regardless of the relevance of the instruction. For example, a GPU generally does not include a mechanism for identifying irrelevant instructions. Accordingly, the SPs of the GPU typically must execute all workgroups and work items of a kernel, regardless of the relevance of the workgroup or work item. Executing sparse kernels may detract from GPU performance, because SPs of the GPU are occupied executing irrelevant instructions, consuming GPU processing power.","Aspects of the disclosure relate to increasing efficiency and utilization of SPs of a GPU by avoiding execution of at least some irrelevant instructions. In some examples, a GPU identifies irrelevant instructions and prevents the irrelevant instructions from being processed by SPs of the GPU. That is, the GPU may identify irrelevant work items, which are a basic unit of work (e.g., executable code) in the GPU an may include one or more individual instructions, and may prevent the irrelevant work items from being processed by SPs of the GPU. To prevent the irrelevant work items from being processed, GPU may \u201ccull,\u201d or remove, the irrelevant work items before they are executed. According to some aspects, a GPU may implement a cull buffer to carry out culling. For example, the cull buffer may hold one or more values that the GPU can use to determine whether to remove a work item before the work item can be executed.","In general the terms \u201cwork item\u201d and \u201cinstruction\u201d may be used interchangeably. This disclosure generally describes an instruction as at least a portion of a function for processing input data. This disclosure generally refers to a work item as a basic unit of work (e.g., a basic unit of executable code) for a GPU, which may include one or more individual instructions. Thus, it should be understood that the term \u201cwork item\u201d refers generically to one or more instructions that define a function for processing input data. A \u201cwork item\u201d may also be referred to in Compute Unified Device Architecture (\u201cCUDA\u201d developed by NVIDIA Corporation, version 3.2 released Sep. 17, 2010) as a \u201cthread.\u201d","A GPU may cull irrelevant instructions from a set of instructions prior to organizing and distributing the instructions to SPs for execution. Culling irrelevant instructions before they can be distributed to, and processed by the SPs, may help to increase efficiency of the GPU, because SPs are not occupied executing irrelevant instructions. In addition, with culling of irrelevant instructions, the GPU does not devote resources to organizing and distributing irrelevant instructions. Rather, the GPU is able to distribute relevant instructions to the SPs, which are able to continuously execute instructions that are relevant.","According to aspects of the disclosure, a GPU may implement a cull buffer to carry out instruction culling. The cull buffer may store one or more values that represent a characteristic of an executed instruction. The characteristic can be used to determine whether a subsequent instruction is relevant or irrelevant. For example, a GPU may execute an application having two kernels. While executing the first kernel, the GPU may store one or more values to the cull buffer that represent whether, based on the results of the instructions associated with the first kernel, the instructions of the second kernel are relevant. If one or more instructions of the second kernel are not relevant, the GPU may prevent the irrelevant instructions from being executed by the SPs. For example, a component of the GPU responsible for organizing and distributing instructions to the SPs may cull the irrelevant instructions based on the values stored in the cull buffer.","Thus, in general, techniques of this disclosure include executing an application for processing input data using more than one kernel. Upon executing work items and workgroups of a first kernel, the GPU may set cull buffer values that represent whether the work items and workgroups of a second kernel are relevant. That is, the GPU may set cull buffer values based on the results of the first kernel, which represent whether respective work items and workgroups of a second kernel that are associated with the same input data of each of the work items and workgroups of the first kernel are relevant. Thus, after executing the first kernel, the GPU may utilize the cull buffer values to cull irrelevant work items and workgroups in the second kernel so that the irrelevant work items and workgroups are not executed.","In some examples, the cull buffer may be designed to store one or more values that represent a characteristic of every instruction of an application. For example, after executing each instruction, the GPU may store a cull buffer that represents a characteristic of each instruction indicating whether the instruction is relevant or irrelevant. In another example, the GPU may store a cull buffer value only when GPU determines that the subsequent instruction is irrelevant.","The cull buffer may be designed to store one or more values that represent a characteristic of a workgroup, as well as one or more values that represent a characteristic of individual work items within the work group. Accordingly, the cull buffer can be used to determine whether instructions are irrelevant on both a workgroup and a work item level. If all of the work items of a given workgroup are rendered irrelevant by a previously executed kernel, the GPU can cull the entire workgroup, so that the irrelevant workgroup is not executed by the SPs. Alternatively, if only some of the work items of a given workgroup are rendered irrelevant by a previously executed kernel, the GPU can cull individual work items, so that the irrelevant work items are not executed by the SPs.","It should be understood that the term \u201cworkgroup\u201d refers generically to a predefined group of instructions, such as a predefined group of work items. A \u201cworkgroup\u201d may also be referred to in Compute Unified Device Architecture (\u201cCUDA\u201d developed by NVIDIA Corporation, version 3.2 released Sep. 17, 2010) as a \u201cthread block.\u201d","Aspects of the disclosure include the use of a variety of different cull buffer sizes and a variety of different cull buffer capabilities. In one example, a cull buffer may store a single binary value (e.g., a single bit of storage) that represents a single characteristic of each work item of a kernel. In such an example, the GPU may use the cull buffer value to identify whether a work item is relevant or irrelevant based on the result of a previously executed work item.","A cull buffer that stores a single binary value is merely one example of a cull buffer configuration. In another example, a cull buffer may store more than one value per work item. For example, the cull buffer may store more than one value that corresponds to more than one characteristic of an executed work item. In a non-limiting image processing example, a GPU may execute a work item on pixel data to determine Red Green Blue (RGB) characteristics of the pixel data. Accordingly, a cull buffer may be implemented to store three values corresponding to the red, green, and blue components of the executed instruction. In this example, the GPU may cull instructions based on any of the values stored in the cull buffer. It should be understood that the configuration (e.g., size and capability) of the cull buffer is application and GPU dependent. For example, some GPUs may be designed to provide a relatively large amount of memory for a relatively large and multifunctional cull buffer. Other GPUs may not be capable of providing such versatility. Accordingly, it should be understood that the examples provided are not the only configurations of cull buffers that may be implemented.","A GPU may store values to a cull buffer in a variety of ways. In one example, after an SP executes a work item, the SP may write one or more cull buffer values to a cull buffer based on the result of the executed work item. In this example, after executing a new work item of another kernel (e.g., a work item associated with the same input data) the SP may update the one or more cull buffer values by reading the one or more cull buffer values stored in the cull buffer and comparing the one or more cull buffer values to the new value generated by execution of the new work item. In this example, there may be some latency associated with reading the one or more cull buffer values stored in the cull buffer prior to comparing the stored values to the new value generated by execution of the new work item.","In another example, after an SP executes a work item, an SP may provide one or more cull buffer values to programmable or fixed function hardware, which writes the one or more cull buffer values to the cull buffer. For example, the programmable or fixed function hardware may be configured to receive an input from the SP upon execution of the work item. The programmable or fixed function hardware can then be implemented to interpret the input and store one or more values to the cull buffer that corresponds to the input. In this example, the programmable or fixed function hardware may be responsible for comparing the one or more values stored in the cull buffer to the new value generated by execution of a new work item. Allowing programmable or fixed function hardware to perform the cull buffer update may result in lower latency, because the SP does not need to read the stored one or more cull buffer values. Rather, the programmable or fixed function hardware would be responsible for reading the one or more cull buffer values.","According to some aspects, the buffer values may be accessed and utilized by a component of the GPU responsible for sequencing and distributing instructions to SPs of the GPU. For example, the sequencing component may read the buffer values and remove the instructions while preparing the instructions for distribution to the SPs. Accordingly, the sequencing component can remove the instructions before distributing the instructions to the SPs. In some examples, the sequencing component may be responsible for both workgroup and work item setup and distribution. The sequencing component may read the values stored in the cull buffer and remove instructions on both a workgroup and work item level before distributing the instructions to the SP. For example, the sequencing module may read the values stored in the cull buffer and determine that an entire workgroup of work items is irrelevant based on the values. The sequencing module may also read the values stored in the cull buffer and determine that individual work items are irrelevant based on the values.","According to some aspects, an application may include instructions that enable instruction culling. For example, the application may include instructions that enable a cull buffer to function. To enable instruction culling, executed instructions of an application provide both a result, as well as one or more values to be stored in a cull buffer that represent a characteristic of the result, which can be used to determine the relevance of a subsequent instruction.","GPU application programming is typically performed by a user (e.g., a computer programmer) with an application program interface (API) that provides a standard software interface that can run on multiple platforms, operating systems, and hardware. Examples of APIs include Open Graphics Library (\u201cOpenGL,\u201d version 4.1 released Jul. 26, 2010 and publically available), Compute Unified Device Architecture (\u201cCUDA\u201d developed by NVIDA Corporation, version 3.2 released Sep. 17, 2010), and DirectX (developed by Microsoft, version 11 released Oct. 27, 2009). In general, an API includes a predetermined, standardized set of commands that are executed by associated hardware. API commands allow a user to instruct hardware components of a GPU to execute commands without user knowledge as to the specifics of the hardware components.","Certain aspects of the disclosure relate to one or more API commands that allow a user (e.g., a computer programmer) to define and implement instruction culling. For example, the API commands may also a user to define cull buffer values that will be generated by an application after executing an instruction. The API commands can also instruct the GPU to store the one or more cull buffer values to a designated cull buffer. The API commands can also instruct the GPU to analyze the cull buffer before organizing and distributing instructions to the SPs. Upon receiving and executing the commands, the GPU generates and stores the one or more cull buffer values to a cull buffer, and culls irrelevant instructions before they are distributed to the SPs.","A variety of different GPGPU applications may benefit from irrelevant instruction culling. A non-limiting and simplified image processing application (e.g., a face detection application) is provided as an example of an application that may have multiple kernels, with one kernel having instructions that make instructions of another kernel irrelevant. An example face detection application includes three kernels for detecting different predefined features associated with a face included in an image (e.g., a frame of video data, a computer-generated graphics image, a still image, and the like).","In this example, each kernel may be implemented to identify a specific feature in pixels of the image (e.g., color, hue, saturation, brightness, or other properties) that is associated with a face. According to this example, any pixel or group of pixels in the frame of video data that satisfies all of the criteria set forth in the kernels is a face candidate. In addition, the kernels of this example are organized from more conservative to less conservative.","For example, a first kernel may identify many candidate face pixels in order to avoid excluding possible candidates. The second and third kernels, however, may be more aggressive in excluding pixels from being considered as candidate face pixels in order to sufficiently narrow the number of candidates. In this example, portions of kernels become irrelevant after each kernel is executed. For example, after executing the first kernel, only some pixels are identified as candidate face pixels, and the remaining pixels are excluded from consideration. Accordingly, any instructions associated with the excluded pixels in the second and third kernels are irrelevant. Those instructions, even if executed, do not contribute to identifying a face candidate.","As kernels are executed in the face detection example provided, the second and third kernels become sparse. For example, after each kernel is executed, more instructions of subsequent kernels are rendered irrelevant. The irrelevant instructions consume GPU resources even though they do not contribute to finding a face. According to aspects of the disclosure, a GPU can implement instruction culling in order to remove irrelevant instructions before the irrelevant instructions are distributed to SPs for execution. In this way, GPU efficiency is increased, because the SPs of the GPU implemented to execute irrelevant instructions.","When executing the face detection example provided, the GPU may first execute the first kernel and generate one or more cull buffer values. For example, SPs of the GPU may execute the instructions of the first kernel and populate a cull buffer with the cull buffer values that represent a characteristic of the executed instructions (e.g., whether a subsequent instruction is relevant or irrelevant) of the first kernel. Upon executing the second kernel, the GPU may utilize the values stored in the cull buffer to identify and cull irrelevant instructions from the second kernel. For example, the GPU may read the values stored in cull buffer  and determine whether to cull the instructions before the instructions are distributed to SPs  for execution. After the GPU has completed instruction culling, the GPU executes the second kernel and updates the cull buffer with new cull buffer values. The GPU may then repeat this example process to execute the third kernel.","Image processing is merely one example in which instructions of one kernel may make instructions of another kernel irrelevant. Other examples include, for instance, augmented reality applications, ray tracing, and pattern recognition. In general, a GPU can be implemented to carry out a variety of applications on a variety of input data. Kernels are application specific and provide only a framework for organizing instructions. Accordingly, it should be understood that the concept of culling irrelevant instructions is applicable to a variety of other applications.","Certain examples and aspects described herein refer to identifying and culling irrelevant work items and workgroups of different kernels before the work items and workgroups are distributed to SPs of a GPU. The kernel\/workgroup\/work item delineation, however, is merely one example of a GPGPU application structure. It should be understood that identifying irrelevant work items and workgroups, and generating, storing, and using work item and workgroup cull values are provided as an example only. Aspects of the disclosure related to identifying irrelevant instructions, and generating, storing, and using cull buffer values may be applied in other GPU application structures. For example, other GPU applications may include a single relatively larger \u201ckernel\u201d that includes instructions that use the same input data more than once during execution. In such an example, aspects of the disclosure may still be applied to maximize SP efficiency. Irrelevant instructions related to the same input data may be culled, despite the instructions belonging to the same kernel.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":["FIG. 1","FIG. 1"],"b":["20","20","24","28","32","36","40","44","20","48","20","20"]},"The illustrated computing device  of  is merely one example. Techniques for identifying and culling irrelevant instructions may be carried out by a variety of other computing devices having other components. In some examples, computing device  may include additional components not shown in  for purposes of clarity. For example, computing device  may include one or more communication bridges for transferring data between components of the computing device . Moreover, the components of computing device  shown in  may not be necessary in every example of computing device . For example, user interface  and display  may be external to computing device  in examples where computing device  is a desktop computer.","Host processor  may include any one or more of a microprocessor, a controller, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field-programmable gate array (FPGA), or equivalent discrete or integrated logic circuitry. Additionally, the functions attributed to host processor , in this disclosure, may be embodied as software, firmware, hardware or any combination thereof.","Host processor  processes instructions for execution within computing device . Host processor  may be capable of processing instructions stored on storage device  or instructions stored in memory . Example applications include applications for processing viewable images (e.g., filtering images, analyzing images for predefined features, and the like). Host processor  may execute the one or more applications based on a selection by a user via user interface . In some examples, host processor  may execute the one or more applications without user interaction.","According to some aspects of the disclosure, and as described in greater detail below with respect to GPU , host processor  may collaborate with GPU  to carry out applications. For example, host processor  may initialize execution of an application and delegate certain processing functions associated with the application to GPU . In an example, host processor  may initialize execution of an image processing application, and offload certain processing functions associated with the application to GPU .","Storage device  may include one or more computer-readable storage media. Storage device  may be configured for long-term storage of information. In some examples, storage device  may include non-volatile storage elements. Examples of such non-volatile storage elements may include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of electrically programmable memories (EPROM) or electrically erasable and programmable (EEPROM) memories. Storage device  may, in some examples, be considered a non-transitory storage medium. The term \u201cnon-transitory\u201d indicates that the storage medium is not embodied in a carrier wave or a propagated signal. However, the term \u201cnon-transitory\u201d should not be interpreted to mean that storage device  is non-movable. As one example, storage device  may be removed from computing device , and moved to another device. As another example, a storage device, substantially similar to storage device , may be inserted into computing device .","Storage device  may store instructions for execution of one or more applications by host processor  or GPU . Storage device  may also store data for use by host processor  or GPU . For example, storage device  may store image data for processing by host processor  or GPU .","Memory  may be configured to store information within computing device  during operation. In some examples, memory  is a temporary memory, meaning that a primary purpose of memory  is not long-term storage. Memory  may, in some examples, be described as a computer-readable storage medium. Accordingly, memory  may also be considered \u201cnon-transitory,\u201d despite storing data that can change over time. Memory  may also, in some examples, be described as a volatile memory, meaning that memory  does not maintain stored contents when the computer is turned off. Examples of volatile memories include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of volatile memories known in the art.","In some examples, memory  may be used to store program instructions for execution by host processor  or GPU . Memory  may be used by software or applications running on computing device  to temporarily store information during program execution. As such, memory  may be accessed by other components of computing device  such as host processor  and GPU .","Computing device  may utilize network module  to communicate with external devices via one or more networks, such as one or more wireless networks. Network module  may be a network interface card, such as an Ethernet card, an optical transceiver, a radio frequency transceiver, or any other type of device that can send and receive information. In some examples, computing device  may utilize network module  to wirelessly communicate with an external device such as a server, mobile phone, or other networked computing device.","Computing device  also includes user interface . Examples of user interface  include, but are not limited to, a trackball, a mouse, a keyboard, and other types of input devices. User interface  may also include a touch-sensitive screen that is incorporated as a part of display . Display  may comprise a liquid crystal display (LCD), an organic light emitting diode (OLED) display, a plasma display, or another type of display device.","GPU  of computing device  may be a dedicated hardware unit having fixed function and programmable components for executing GPU applications. GPU  may also include a DSP, a general purpose microprocessor, an ASIC, an FPGA, or other equivalent integrated or discrete logic circuitry. GPU  may also include other components, such as dedicated memory, as described in greater detail with respect to . Furthermore, although shown as separate components in , in some examples, GPU  may be formed as part of host processor . GPU  may be configured to utilize processing techniques in accordance with a variety of application programming interfaces (APIs). For example, a user may program an application to be executed by GPU  using a standard software interface that can run on multiple platforms, operating systems, and hardware. In some examples, GPU  may be configured to utilize applications generated using OpenCL, CUDA, or the DirectX collection of APIs.","According to some examples, GPU  can be implemented as a general purpose graphics processing unit (GPGPU). For example, GPU  may carry out a variety of general purpose computing functions traditionally carried out by host processor . Examples include a variety of image processing functions, including video decoding and post processing (e.g., de-blocking, noise reduction, color correction, and the like) and other application specific image processing functions (e.g., facial detection\/recognition, pattern recognition, wavelet transforms, and the like). In some examples, GPU  may collaborate with host processor  to execute applications. For example, host processor  may offload certain functions to GPU  by providing GPU  with instructions for execution by GPU .","When implemented as a GPGPU, GPU  and host processor  may execute an application that has one or more shader programs, referred to herein as kernels. Each kernel of an application can define a function for carrying out a specific task, and each kernel may be executed on the same input data. For example, GPU  and host processor  may execute an example image processing application that has a plurality of kernels for identifying features in the image, and each of the plurality of kernels may be executed on the image data to identify the features. Further, GPU  may execute applications that include kernels that are dependent on the results of other kernels. In such examples, the results of a kernel may make instructions of a subsequent kernel, such as work items and workgroups of a subsequent kernel, \u201cirrelevant\u201d (e.g., one or more instructions that do not advance the purpose of the application).","According to some aspects of the disclosure, GPU  may avoid executing irrelevant instructions by preventing the irrelevant instructions from being executed. For example, to prevent the irrelevant instructions from being executed, GPU  may \u201ccull,\u201d or remove the instructions before they are executed. That is, GPU  may cull irrelevant work items and\/or workgroups before they are executed. As described above, the term \u201cwork item\u201d includes a basic unit of work for a GPU that may include one or more individual instructions, which define a function for processing input data. Moreover, the term \u201cworkgroup\u201d refers generically to a predefined group of instructions, such as a predefined group of work items.","Accordingly, techniques of this disclosure generally include executing an application for processing input data using more than one kernel. Upon executing work items and workgroups of a first kernel, GPU  may set cull buffer values that represent whether the work items and workgroups of a second kernel are relevant. That is, GPU  may set cull buffer values based on the results of the first kernel, which represent whether respective work items and workgroups of a second kernel that are associated with the same input data of each of the work items and workgroups of the first kernel are relevant. Thus, after executing the first kernel, GPU  may utilize the cull buffer values to cull irrelevant work items and workgroups in the second kernel so that the irrelevant work items and workgroups are not executed.",{"@attributes":{"id":"p-0065","num":"0064"},"figref":["FIG. 2","FIG. 2"],"b":["48","48","64","72","76","80","48","84","84","84"]},{"@attributes":{"id":"p-0066","num":"0065"},"figref":["FIG. 2","FIG. 2","FIG. 2","FIG. 2"],"b":["48","48","48","84","48","84"]},"GPU memory  may be a dedicated memory module within GPU  for storing instructions to be processed by GPU . In some examples, GPU memory  is similar to memory  shown in . For example, GPU memory  may be a temporary computer-readable storage medium. Examples of GPU memory  include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of memories known in the art. In examples where GPU  is formed as part of another processor, such as host processor  shown in , GPU memory  may be accessed by components other than GPU .","GPU memory  may be configured as a global memory for GPU . For example, GPU memory  may be configured to store instructions and information within GPU  during operation (e.g., image data and instructions for processing by GPU ). GPU memory  may also be configured to store results of data that has been processed by GPU .","Sequencer module  may initially prepare instructions and data for processing by SPs . For example, sequencer module  may receive instructions and data from a host processor, such as host processor  shown in , or GPU memory  and prepare input data to be processed by SPs . In some examples, sequencer module  receives one or more kernels of instructions that define functions that are to be carried out by GPU . The sequencer module  may organize the instructions into work items (e.g., a basic unit of work) and group the work items into workgroups.","Sequencer module  may also be implemented to control instruction and data flow within GPU . For example, sequencer module  may route instructions and associated data to SPs  for execution. Sequencer module  may be comprised of a combination of fixed function and programmable components for distributing instructions (e.g., work items and workgroups) and associated input data to SPs . According to some aspects of the disclosure, sequencer module  includes a cull module  and a cull buffer  for identifying irrelevant instructions and preventing the irrelevant instructions from being processed by SPs  of the GPU . That is, sequencer module  may include a cull module  and a cull buffer  for identifying irrelevant work items and work groups and preventing the irrelevant work items and work groups from being processed by SPs  of the GPU .","In the example shown in , the SPs  each include a thread setup module A-D (collectively, thread setup modules ), as well as a plurality of arithmetic logic units (\u201cALUs\u201d) A-D (collectively, ALUs ). SPs  may be referred to as \u201cunified shader processors,\u201d in that the SPs  can perform geometry, vertex, or pixel shading operations to render graphics. SPs  can also be used to perform general purpose calculations when executing instructions of a GPGPU application. For example, SPs  may receive instructions from sequencer module  and execute the instructions.","In some examples, thread setup module  of SPs  is responsible for receiving instructions from sequencer module  and generating threads for execution by ALUs . For example, thread setup module  may receive instructions (e.g., a workgroup of work items) from sequencer module , temporarily store the instructions, generate threads, i.e., work items, for execution by the ALUs , and distribute the threads to the ALUs . According to some aspects, thread setup module  distributes threads to the ALUs  in such a way that allows ALUs  to process more than one thread in parallel. Performance and efficiency of GPU  may be maximized by continuously occupying ALUs  of SPs  with relevant threads (e.g., threads having relevant instructions).","According to some aspects of the disclosure, GPU  may increase efficiency by implementing sequencer module  to identify \u201cirrelevant\u201d instructions (e.g., an instruction that does not advance the purpose of an application) and prevent the irrelevant instructions from being processed by SPs . For example, to prevent an irrelevant work item from being processed, sequencer module  may cull, or remove, irrelevant work items before routing the instructions to SPs . Accordingly, thread setup module  of SPs  does not generate threads that include irrelevant work items, and the ALUs  of SPs  are not occupied executing irrelevant work items.","Sequencer module  may implement cull buffer  and cull module  to carry out instruction culling. Although described and represented as two distinct modules in , in some examples, cull buffer  and cull module  may be implemented in the same component. Further, according to some examples, cull buffer  and cull module  may not be implemented as distinct components. Rather, in some examples, cull buffer  and cull module  may be integrated into other components of GPU . For example, cull buffer  may be implemented as a partition of GPU memory . In another example, cull buffer  may be a memory that is external to GPU . Alternatively or additionally, the instruction culling techniques described with respect to cull module  may be integrated into sequencer module  or SPs .","Cull buffer  may store one or more values that represent a characteristic of an executed work item. The characteristic can be used to determine whether a subsequent work item associated with a subsequent kernel is relevant or irrelevant. For example, GPU  may execute an application having two kernels. After executing the first kernel, the GPU  may store one or more values to cull buffer  that represents whether, based on the results of the first kernel, the work items of the second kernel are relevant.","Cull buffer  may receive input defining the cull buffer values from a variety of sources. In one example, after one of the SPs , such as SP A, executes a work item, SP A may write one or more cull buffer values to cull buffer  based on the result of the executed work item directly. In another example, cull buffer  may receive cull buffer values from programmable or fixed function hardware, as described above.","According to some aspects, cull buffer  may be designed to store one or more values that represent a characteristic of a workgroup, as well as one or more values that represent a characteristic of individual work items within the work group. For example, cull buffer  may store a workgroup cull value that represents that all of the work items of the particular workgroup are irrelevant if all of the work items of a given workgroup are rendered irrelevant by a previously executed kernel. Alternatively or additionally, cull buffer  may store one or more work item cull values that represent that only some of the work items of a given workgroup are rendered irrelevant by a previously executed kernel. Accordingly, the values stored in cull buffer  can be used to determine characteristics (e.g., such as the relevancy of an instruction) on both a workgroup level and a work item level.","Cull buffer  may have a variety of different capacities, with greater capacities offering additional capabilities. In one example, cull buffer  may include a single bit of storage for each work item of a kernel. In this example, the single bit of storage may be used to store a binary value that represents a single characteristic of each work item of a kernel. GPU  may use the single characteristic to identify whether a work item is relevant or irrelevant based on a result of a previously executed work item.","In other examples, cull buffer  may include more than a single bit of storage for each work item of a kernel. For example, more than one value may be stored in cull buffer  to describe a variety of characteristics of each work item of a kernel. In a non-limiting image processing example, GPU  may execute a work item on pixel data to determine Red Green Blue (RGB) characteristics of the pixel data. Accordingly, cull buffer  may include storage that allows three values to be stored that corresponding to the red, green, and blue components of the executed work item. In this example, GPU  and cull module  may cull work items based on any of the values stored in cull buffer . It should be understood that the configuration (e.g., capacity and capability) of cull buffer  may be application and GPU dependent. For example, some GPUs may be designed to provide a relatively large amount of memory for a relatively large and multifunctional cull buffer. Other GPUs may not be capable of providing such versatility. Accordingly, it should be understood that the examples provided are not the only configurations of cull buffers that may be implemented.","Cull module  may be responsible for accessing values stored in cull buffer  and culling irrelevant instructions based on the values stored in cull buffer . According to some examples, cull module  accesses the values stored in cull buffer  before sequencer module  prepares and distributes instructions to SPs . As described in greater detail with respect to , cull module  may read values stored in cull buffer  and cull instructions on both a work item and workgroup level. For example, cull module  may read values stored in cull buffer  and determine that an entire workgroup of work items is irrelevant based on the values. Cull module  may also read the values stored in cull buffer  and determine that individual work items are irrelevant based on the values. After cull module  has culled irrelevant instructions, sequencer module  can prepare and distribute the remaining instructions to SPs .","Aspects of the disclosure relate to GPU  utilizing cull buffer  and cull module  to cull irrelevant instructions when executing an application having more than one kernel that processes the same input data. In an example, GPU  receives instructions and data defining a plurality of kernels from a host processor, such as host processor  shown in , or other computing unit. Upon receiving the kernels, sequencer module  may initially prepare the instructions and data associated with a first kernel for processing by SPs . For example, the sequencer module  may organize the instructions of the first kernel into work items and workgroups.","Upon generating the grids of workgroups, cull module  of sequencer module  may query cull buffer  to determine whether to perform instruction culling. According to some examples, cull module  does not perform instruction culling on a first kernel of instructions, because in some examples, cull buffer values are generated based on the results of executed instructions. Accordingly, cull buffer  is empty before execution of the first kernel. Sequencer module  proceeds to distribute instructions (e.g., workgroups and work items) associated with the first kernel to SPs , which execute the instructions.","After SPs  have executed the instructions of the first kernel, SPs  populate or update cull buffer  with cull buffer values. For example, SPs  may populate cull buffer  with one or more values that represent a characteristic of an executed work item of the first kernel. The characteristic can be used to determine whether a subsequent work item associated with the next kernel is relevant or irrelevant.","GPU may then continue by executing a second kernel. For example, sequencer module  may prepare instructions and data associated with the second kernel for processing by SPs  by organizing one or more grids of workgroups associated with the second kernel. Cull module  then queries cull buffer  to identify instructions for culling. For example, cull module  reads the values stored in cull buffer  and determines whether to cull instructions before the instructions are distributed to SPs  by sequencer module . In some examples, cull module  culls instructions on both a work item and workgroup basis. After cull module  has completed instruction culling, sequencer module  distributes the instructions to SPs , which execute the instructions and update the values of cull buffer . This example process may be repeated until GPU  has executed all kernels.","The example described with respect to  refers to sequencer module  performing all instruction culling (e.g., work item culling and workgroup culling) using cull buffer  and cull module . It should be understood, however, that other modules of GPU  may be responsible for instruction culling, and other modules may interact with cull buffer . According to one example, sequencer module  may be responsible for performing workgroup culling only, while thread setup modules  of SPs  are responsible for work item culling. For example, sequencer module  may perform workgroup culling using cull buffer  and cull module , as described above. After performing workgroup culling, sequencer module  may distribute the remaining workgroups (e.g., the workgroups remaining after culling) to SPs . In this example, thread setup modules  may be responsible for work item culling. For example, after receiving a workgroup from sequencer module , the thread setup modules  may utilize the cull buffer  to perform work item culling. According to some aspects, the thread setup modules  read cull buffer  and determine whether to cull work items based on the one or more values stored in cull buffer . In addition, the thread setup modules  may remove irrelevant work items prior to organizing threads for execution by ALUs  based on the cull buffer values stored in cull buffer .","According to some aspects, to enable instruction culling in GPU , GPU  executes an application having instruction culling commands. For example, GPU  executes instruction culling commands such that when executing an instruction, GPU  generates both a result, as well as one or more values for storing in cull buffer  that represent a characteristic of the result. GPU  may also execute instruction culling commands that enable cull module  to read the values of cull buffer  and perform instruction culling.","Certain aspects of the disclosure relate to one or more API commands that allow a user (e.g., a computer programmer) to create an application having instruction culling commands. For example, the API commands can also be used to instruct the GPU  to generate and store one or more cull buffer values to a designated cull buffer, such as cull buffer . The API commands can also instruct the GPU  to analyze cull buffer values before organizing and distributing instructions to the SPs , as well as perform instruction culling based on the values. For example, the API commands can instruct GPU  to discard instructions that are not relevant based on the cull buffer values so that the instructions are not distributed to SPs  of GPU .",{"@attributes":{"id":"p-0088","num":"0087"},"figref":["FIG. 3","FIG. 3","FIG. 3","FIG. 2"],"b":["0","84","48","48"]},"Work items, such as the work items shown in Workgroup  of , may be organized into threads (e.g., one or more instructions organized in a configuration that provides efficient execution by ALUs) before being executed by SPs . For example, one of the SPs , such as SP A, may receive Workgroup  from sequencer module  and prepare the Workgroup  for execution by generating threads using the work items associated with Workgroup .","According to some examples, the efficiency of SPs  may be impacted by the relevance of the work items that are organized into threads. For example, the efficiency of SPs  may be negatively impacted if threads are generated with, and SPs  are occupied executing, irrelevant work items. In some examples, workgroups of a multi-kernel program may become \u201csparse,\u201d based on the results of previously executed instructions. For example, a \u201csparse\u201d workgroup may include relevant work items that are separated by one or more irrelevant work items. In the example shown in , Workgroup  may be considered \u201csparse\u201d if several work items are rendered irrelevant by a previously executed instruction.","Aspects of the disclosure relate to identifying irrelevant work items, and removing the irrelevant work items before the work items are organized into threads. In some examples, work items may be culled prior to being distributed to SPs . For example, cull module  may utilize values stored in cull buffer  to determine whether to cull a work item before the work item is distributed to SPs . In some examples, GPU  may perform workgroup culling, as described with respect to , prior to performing work item culling.",{"@attributes":{"id":"p-0092","num":"0091"},"figref":["FIG. 4","FIG. 2","FIG. 4","FIG. 2"],"b":["124","128","132","48","124","132","124","132","48"]},"The grids of workgroups shown in  may be generated by sequencer module . For example, sequencer module  may receive instructions and data from a host processor, such as host processor  shown in , or GPU memory  and prepare the instructions for execution by organizing the instructions into grids of workgroups. In some examples, the workgroups of each of the kernels - relate to the same input data. For example, the Workgroup  of kernel  includes instructions associated with the same input data as Workgroup  of kernel , as well as Workgroup  of kernel . In addition, GPU  may execute kernels - in succession, such that kernel  is executed prior to kernel , and kernel  is executed prior to kernel .","According to some aspects of the disclosure, the results of one or more workgroups of one kernel may render workgroups of another kernel irrelevant. For example, GPU  may execute Workgroup  of kernel , and the results of Workgroup  of kernel  may render the instructions associated with Workgroup  of kernel  irrelevant. Workgroup  of kernel  may be considered irrelevant if the instructions associated with Workgroup  do not advance an overreaching goal of kernels -. For example, in an example image processing application, one overreaching goal of kernels - may be to identify a human face in the image.","Aspects of the disclosure may be implemented to cull, or remove, irrelevant workgroups before the workgroups can be executed. In the example described above, GPU  may remove the irrelevant Workgroup  before Workgroup  can be distributed to SPs . For example, cull module  may utilize values stored in cull buffer  to determine that Workgroup  is irrelevant, and cull Workgroup  before it can be distributed to SPs .","As mentioned, according to one application-specific implementation, GPU  may execute kernels - to carry out a specific task on an image, such as the image shown in .",{"@attributes":{"id":"p-0097","num":"0096"},"figref":["FIGS. 5A-5C","FIG. 5A","FIG. 5B","FIG. 5C","FIGS. 5A-5C","FIG. 2"],"b":["146","150","158","164","150","158","164","146","48"]},"According to the example shown in , image  is a square, approximately 16 megabyte (MB) image that includes 1024 pixels. Each kernel , , and  includes work items (e.g., represented as relatively smaller blocks) that may be related to a particular pixel of image. Accordingly, when GPU  executes a work item, the corresponding pixel of image  may be processed (e.g., analyzed) according to the instruction associated with the work item.","Each kernel , , and  also includes workgroups (e.g., represented as relatively larger blocks) that include instructions related to a particular group of pixels of image . The relationships between pixel data, work items, and workgroups described with respect to  are merely an example of possible instruction structures. In other examples, a work item may relate to more or less than one pixel of image .","According to one non-limiting example, kernels , , and  are implemented as part of a face detection application for detecting one or more human faces in image . In this example, kernels , , and  are implemented to identify pixels that include certain predetermined properties (e.g., color, hue, saturation, brightness, or other properties) that are associated with a human face. GPU  may identify any pixel or group of pixels in image  that satisfy all of the criteria set forth in kernels , , and  as candidate pixels of a face. For example, if a pixel does not include the feature of kernel , the instructions related to that pixel in kernel  and kernel  are rendered irrelevant.","As shown and described in greater detail with respect to , as GPU  executes each of the kernels , , and , relevant instructions associated with subsequent kernels become increasingly \u201csparse.\u201d For example, after each kernel is executed, more instructions of subsequent kernels are rendered irrelevant, because GPU  identifies more pixels that do not include the predetermined characteristics of face pixels. According to aspects of the disclosure, GPU  can implement instruction culling in order to remove irrelevant instructions before the irrelevant instructions are distributed to SPs  for execution. In this way, the efficiency of GPU  is increased, because SPs  are not occupied executing irrelevant instructions.",{"@attributes":{"id":"p-0102","num":"0101"},"figref":["FIG. 5A","FIG. 5A","FIG. 1"],"b":["150","146","154","154","154","156","48","150","156","154","154","48","150","24","150","72","150","154"]},"Sequencer module  may then proceed to distribute all workgroups  and work items  to SPs . The SPs  then execute the workgroups  and associated work items , and populate cull buffer  with one or more cull buffer values. The cull buffer values may represent a characteristic of an executed work item of the first kernel . The characteristic can be used to determine whether a subsequent instruction associated with the next kernel is relevant or irrelevant. For example, if the result of a particular work item indicates that the pixel associated with the particular work item does not include the desired predefined property of kernel , such as a predefined color, hue, saturation, brightness, or other property, the SP responsible for executing the instruction may store a value to cull buffer  indicating that subsequent work items associated with that pixel are irrelevant. If the same SP is responsible for executing an entire workgroup, the SP may also store a value to cull buffer  indicated that pixels associated with an entire workgroup are irrelevant. After GPU  has executed kernel  and populated cull buffer , GPU  may proceed to execute kernel , as shown in .",{"@attributes":{"id":"p-0104","num":"0103"},"figref":["FIG. 5B","FIG. 5A","FIG. 5A","FIG. 5A"],"b":["158","158","160","160","154","154","160","160","162","156","160","162","146","150"]},"In the example shown in , GPU  performs instruction culling on irrelevant workgroups  and work items  from the second kernel . For example, after executing the first kernel , GPU  may identify a number of pixels that do not include the predetermined property of kernel . Subsequent workgroups and work items associated with those pixels can be considered irrelevant to detecting a face in image . GPU  may be used to perform instruction culling on the irrelevant workgroups and work items.","According to some aspects of the disclosure, GPU  culls instructions by implementing cull buffer  and cull module  on both a work item and workgroup basis. For example, after executing the first kernel , GPU  may determine that subsequent instructions for pixels in the locations of workgroups M-P are irrelevant. In addition, GPU  may determine that subsequent instructions for certain pixels within workgroup H (shown in ) are irrelevant. Accordingly, GPU  stores values to cull buffer , which indicate that the pixel data associated with workgroups M-P and certain work items  are irrelevant. Upon executing kernel , GPU  can cull workgroups M-P and certain work items  using cull module .","After cull module  has completed instruction culling, sequencer module  organizes the remaining instructions and distributes the instructions to SPs . For example, according to some aspects, SPs  are configured to receive instructions that are organized in a way that allows the instructions to be easily distributed in parallel to ALUs , which may be GPU dependent. Accordingly, sequencer module  may reorganize the instructions associated with the second kernel  to optimize the distribution and execution processes.","After receiving the culled set of instructions, SPs  execute the instructions. In addition, SPs  may update the value in cull buffer . For example, if the result of a particular work item  indicates that the pixel associated with the particular work item  does not include the desired predefined property of kernel , the SP responsible for executing the work item  may store a value to cull buffer  indicating that subsequent work items associated with that pixel are irrelevant. If the same SP is responsible for executing an entire workgroup, the SP may also store a value to cull buffer  indicated that pixels associated with an entire workgroup are irrelevant. After GPU  has executed kernel  and populated cull buffer , GPU  may proceed to execute kernel , as shown in .",{"@attributes":{"id":"p-0109","num":"0108"},"figref":["FIG. 5C","FIG. 5A","FIG. 5A","FIG. 5A","FIG. 5B"],"b":["164","164","166","166","154","154","166","166","168","156","166","168","146","150","158"]},"As shown in , GPU  may carry out instruction culling before executing the third kernel , further reducing the number of instructions for processing by SPs  of GPU . For example, after executing the second kernel , GPU  may determine that subsequent instructions for pixel data in the locations of workgroups A, B, E, , and M-P are irrelevant. In addition, GPU  may determine that subsequent instructions for certain pixel data within workgroup H (shown in ) are irrelevant. Accordingly, GPU  updates values in cull buffer , which indicate that the pixel data associated with workgroups A, B, E, , and M-P and certain work items  are irrelevant. Upon executing kernel , GPU  can cull workgroups A, B, E, , and M-P and certain work items  using cull module , and execute the remaining instructions, as described with respect to .","As the example shown in  illustrates, instruction culling may greatly reduce the number of computations for a GPU when an application includes some instructions that render others irrelevant. Culling instructions may increase overall efficiency of GPU, because GPU does not have to devote resources to distributing and executing irrelevant instructions.",{"@attributes":{"id":"p-0112","num":"0111"},"figref":["FIGS. 5A-5C","FIGS. 5A-5C"],"b":["48","48","48","48","48"]},{"@attributes":{"id":"p-0113","num":"0112"},"figref":["FIG. 6","FIG. 2","FIG. 2"],"b":["200","200","48","200","48"]},"According to aspects of the disclosure, GPU  performs instruction culling after executing one or more instructions. GPU  initially receives one or more instructions from, for example, a host processor such as host processor  shown in . After receiving, for example, a first work item, GPU  executes the first work item (). In some examples, GPU  may execute the first work item by distributing the work item to SPs  with sequencer module . The SPs  may then execute the work item by constructing one or more hardware threads and distributing the hardware threads to ALUs  for execution.","After the first work item has been executed, GPU  generates one or more cull values (). The cull buffer value may represent a characteristic of the executed work item. In some examples, the characteristic can be used to determine whether a subsequent work item associated with the same input data as the first work item is relevant or irrelevant. Again, a work item may be considered irrelevant if it does not advance the goal or purpose of an application that the instruction is incorporated in. For example, in an image processing context, such as the face detection example described with respect to , work items may be considered irrelevant if they do not advance the purpose of identifying a face in an image.","In one example, the cull buffer value may be a single binary value that represents whether the subsequent work item is relevant or irrelevant. In other examples, more than one cull buffer value may be stored for a single work item. For example, more than one cull buffer value may be stored for a single work item, where each cull buffer value corresponds to a different characteristic of an executed work item. In a non-limiting image processing example, GPU  may execute a work item on pixel data to determine Red Green Blue (RGB) characteristics of the pixel data. Accordingly, three cull buffer values may be stored that correspond to the red, green, and blue components of the executed work item. In this example, GPU  and cull module  may cull work items based on any of the values stored in cull buffer . After generating the cull buffer value, GPU  may populate buffer, such as cull buffer  with the one or more cull values ().","GPU  then receives another work item (). In some examples, the second work item depends on the first work item and is associated with the same input data as the first work item. For example, the second work item may only be relevant if a result of the first work item evaluates to a certain predetermined value, or one of a plurality of predetermined values. Upon receiving the second work item, GPU  may analyze the cull buffer value associated with the second work item (). According to some aspects, GPU  indexes the cull buffer values such that GPU  can identify that the second work item corresponds to the same input data and depends on the first work item. Accordingly, GPU  can determine whether to execute the second work item based on the cull buffer value ().","According to some examples, GPU  may execute the second work item if GPU  determines that the second work item is relevant based on the cull buffer value (). After executing the second work item, GPU  may begin repeating the process by generating a cull value that represents a characteristic of the result of the second work item (step ). Alternatively, GPU  may not execute the second work item if GPU  determines that the second work item is irrelevant based on the cull buffer value, and may discard the second work item (). If GPU  does not execute the second work item, GPU  may proceed directly to receiving the next work item ().","The method of  describes instruction culling on a per-work item basis. For example, work items are described as being individually executed, the cull values are described as being individually generated, and the like. It should be understood, however, that in practice a GPU may execute more than one instruction in parallel. According, more than one value may be written to cull buffer  substantially concurrently. In addition, GPU may analyze more than one cull buffer value as they relate to new instructions substantially concurrently. As described with respect to , in some examples, GPU  may perform instruction culling after executing a certain number of instructions (e.g., after executing a kernel of instructions), and may perform instruction culling on an individual work item or group (e.g., workgroup) basis.",{"@attributes":{"id":"p-0120","num":"0119"},"figref":["FIG. 7","FIG. 2","FIG. 2","FIG. 7"],"b":["250","250","48","250","48","48"]},"According to aspects of the disclosure, GPU  receives instructions and organizes the instructions into one or more workgroups (). In some examples, GPU  may organize the instructions into one or more grids of workgroups, such as the grids of workgroups shown in . After the instructions have been organized into workgroups, GPU  performs workgroup culling (). For example, GPU  may perform workgroup culling by analyzing values stored in cull buffer  and relating the values to the current set of workgroups. GPU  may cull any workgroups that GPU  identifies as being comprised entirely of irrelevant instructions. After culling workgroups, GPU  may set up the workgroups for execution (). For example, GPU  may reorganize remaining workgroups into a configuration that allows the workgroups to be distributed to SPs .","In some examples, GPU  then performs work item culling (). GPU  may perform work item culling on a per-workgroup basis. For example, GPU  may analyze values stored in cull buffer  and relate the values to work items of a workgroup one workgroup at a time. GPU  may cull any work items that GPU identifies as irrelevant (e.g., according to cull buffer values). After culling work items, GPU  may set up work items for execution (). For example, GPU  may reorganize remaining work items into a configuration that allows the workgroups to be built into threads and executed by SPs .","Next, GPU  may execute the one or more workgroups and associated work items (). After executing the instructions, GPU  may update values for culling subsequent workgroups and work items (). For example, GPU  may update cull buffer  with values that represent a characteristic of the result of the executed instructions.","Techniques of this disclosure generally include executing an application for processing input data using more than one kernel. Upon executing work items and workgroups of a first kernel, a GPU may set cull buffer values that represent whether the work items and workgroups of a second kernel are relevant. That is, the GPU may set cull buffer values based on the results of the first kernel, which represent whether respective work items and workgroups of a second kernel that are associated with the same input data of each of the work items and workgroups of the first kernel are relevant. Thus, after executing the first kernel, GPU  may utilize the cull buffer values to cull irrelevant work items and workgroups in the second kernel so that the irrelevant work items and workgroups are not executed.","Thus, aspects of the disclosure generally relate to instruction culling. It should be understood that the examples and aspects described with respect to the figures above are provided as examples only. Aspects of the disclosure also relate to other manners of performing instruction culling. For example, while instruction culling is described as being wholly carried out by GPU, in another example, instruction may be carried out by a combination of computing components or devices. In one example, a cull module and cull buffer, such as the cull module  and cull buffer  shown in , may be implemented in a component that is external to a GPU. In this example, central processing unit or host processor may access the cull buffer and implement instruction culling prior to providing the instructions to the GPU.","In addition, it should be understood that aspects of the disclosure relate generally to culling irrelevant work items and workgroups before they are executed. As described above, the term \u201cwork item\u201d generally describes one or more instruction that defines a function for processing input data, which may include one or more individual instructions. A \u201cwork item\u201d may also be referred to in some architectures as a \u201cthread.\u201d Moreover, it should be understood that the term \u201cworkgroup\u201d refers generically to a predefined group of work items. A \u201cworkgroup\u201d may also be referred to in some architectures as a \u201cthread block.\u201d","In one or more examples, the functions described may be implemented in hardware, software executed on hardware, firmware executed on hardware, or any combination thereof. In some examples, instructions stored on a computer-readable media may cause the hardware components to perform their respective functions described above. The computer-readable media may include computer data storage media. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and\/or data structures for implementation of the techniques described in this disclosure. By way of example, and not limitation, such computer-readable media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Combinations of the above should also be included within the scope of computer-readable media.","The code may be executed by one or more processors, such as one or more DSPs, general purpose microprocessors, ASICs, FPGAs, or other equivalent integrated or discrete logic circuitry. Accordingly, the term \u201cprocessor,\u201d as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition, in some aspects, the functionality described herein may be provided within dedicated hardware and\/or software modules configured for encoding and decoding, or incorporated in a combined codec. Also, the techniques could be fully implemented in one or more circuits or logic elements.","The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a wireless handset, an integrated circuit (IC) or a set of ICs (e.g., a chip set). Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units. Rather, as described above, various units may be combined by a collection of interoperative hardware units, including one or more processors as described above, in conjunction with suitable software and\/or firmware.","Various examples and aspects of the disclosure have been described. These and other examples and aspects are within the scope of the following claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF DRAWINGS","p":[{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 5B","FIG. 5A"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 5C","FIG. 5A"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
