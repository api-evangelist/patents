---
title: Rich context modeling for text-to-speech engines
abstract: Embodiments of rich context modeling for speech synthesis are disclosed. In operation, a text-to-speech engine refines a plurality of rich context models based on decision tree-tied Hidden Markov Models (HMMs) to produce a plurality of refined rich context models. The text-to-speech engine then generates synthesized speech for an input text based at least on some of the plurality of refined rich context models.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08340965&OS=08340965&RS=08340965
owner: Microsoft Corporation
number: 08340965
owner_city: Redmond
owner_country: US
publication_date: 20091202
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","Example Scheme","Example Components","Example Processes","Example Computing Device","CONCLUSION"],"p":["This application claims priority to U.S. Provisional Patent Application No. 61\/239,135 to Yan et al., entitled \u201cRich Context Modeling for Text-to-Speech Engines\u201d, filed on Sep. 2, 2009, and incorporated herein by reference.","A text-to-speech engine is a software program that generates speech from inputted text. A text-to-speech engine may be useful in applications that use synthesized speech, such as a wireless communication device that reads incoming text messages, a global positioning system (GPS) that provides voice directional guidance, or other portable electronic devices that present information as audio speech.","Many text-to-speech engines use Hidden Markov Model (HMM) based text-to-speech synthesis. A variety of contextual factors may affect the quality of synthesized of human speech. For instance, parameters such as spectrum, pitch and duration may interact with one another during speech synthesis. Thus, important contextual factors for speech synthesis may include, but are not limited to, phone identity, stress, accent, position. In HMM-based speech synthesis, the label of the HMMs may be composed of a combination of these contextual factors. Moreover, conventional HMM-based speech synthesis also uses a universal Maximum Likelihood (ML) criterion during both training and synthesis. The ML criterion is capable of estimating statistical parameters of the HMMs. The ML criterion may also impose a static-dynamic parameter constraint during speech synthesis, which may help to generate a smooth parametric trajectory that yields highly intelligible speech.","However, speech synthesized using conventional HMM-based approaches may be overly smooth, as ML parameter estimation after decision tree-based tying usually leads to highly averaged HMM parameters. Thus, speech synthesized using the conventional HMM-based approaches may become blurred and muffled. In other words, the quality of the synthesized speech may be degraded.","Described herein are techniques and systems for using rich context modeling to generate Hidden Markov Model (HMM)-based synthesized speech from text. The use of rich context modeling, as described herein, may enable the generation of synthesized speech that is of higher quality (i.e., less blurred and muffled) than speech that is synthesized using conventional HMM-based speech synthesis.","The rich context modeling described herein initially uses a special training procedure to estimate rich context model parameters. Subsequently, speech may be synthesized based on the estimated rich context model parameters. The spectral envelopes of the speech synthesized based on the rich context models may have crisper formant structures and richer details than those obtained from conventional HMM-based speech synthesis.","In at least one embodiment, a text-to-speech engine refines a plurality of rich context models based on decision tree-tied Hidden Markov Models (HMMs) to produce a plurality of refined rich context models. The text-to-speech engine then generates synthesized speech for an input text based at least on some of the plurality of refined rich context models.","This Summary is provided to introduce a selection of concepts in a simplified form that is further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","The embodiments described herein pertain to the use of rich context modeling to generate Hidden Markov Model (HMM)-based synthesized speech from input text. Many contextual factors may affect HMM-based synthesis of human speech from input text. Some of these contextual factors may include, but are not limited to, phone identity, stress, accent, position. In HMM-based speech synthesis, the label of the HMMs may be composed of a combination of context factors. \u201cRich context models\u201d, as used herein, refer to these HMMs as they exist prior to decision-tree based tying. Decision tree-based tying is an operation that is implemented in conventional HMM-based speech synthesis. Each of the rich context models may carry rich segmental and suprasegmental information.","The implementation of text-to-speech engines that uses rich context models in HMM-based synthesis may generate speech with crisper formant structures and richer details than those obtained from conventional HMM-based speech synthesis. Accordingly, the use of rich context models in HMM-based speech synthesis may provide synthesized speech that is more natural sounding. As a result, user satisfaction with embedded systems, server system, and other computing systems that present information via synthesized speech may be increased at a minimal cost. Various example use of rich context models in HMM-based speech synthesis in accordance with the embodiments are described below with reference to .",{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 1","b":"102"},"The text-to-speech engine  may be implemented on an electronic device . The electronic device  may be a portable electronic device that includes one or more processors that provide processing capabilities and a memory that provides data storage\/retrieval capabilities. In various embodiments, the electronic device  may be an embedded system, such as a smart phone, a personal digital assistant (PDA), a digital camera, a global position system (GPS) tracking unit, or the like. However, in other embodiments, the electronic device  may be a general purpose computer, such as a desktop computer, a laptop computer, a server, or the like. Further, the electronic device  may have network capabilities. For example, the electronic device  may exchange data with other electronic devices (e.g., laptops computers, servers, etc.) via one or more networks, such as the Internet.","The text-to-speech engine  may ultimately convert the input text  into synthesized speech . The input text  may be inputted into the text-to-speech engine  as electronic data (e.g., ACSCII data). In turn, the text-to-speech engine  may output synthesized speech  in the form of an audio signal. In various embodiments, the audio signal may be electronically stored in the electronic device  for subsequent retrieval and\/or playback. The outputted synthesized speech  (i.e., audio signal) may be further transformed by electronic device  into an acoustic form via one or more speakers.","During the conversion of input text  into synthesized speech , the text-to-speech engine  may generate rich context models  from the input text . The text-to-speech engine  may further refine the rich context models  into refined rich context models  based on decision tree-tied Hidden Markov Models (HMMs) . In various embodiments, the decision tree-tied HMMs  may also be generated by the text-to-speech engine  from the input text .","Subsequently, the text-to-speech engine  may derive a guiding sequence  of HMM models from the decision tree-tied HMMs  for the input text . The text-to-speech engine  may also generate a plurality of candidate sequences of rich context models  for the input text . The text-to-speech engine  may then compare the plurality of candidate sequences  to the guiding sequence of HMM models . The comparison may enable the text-to-speech engine  to obtain an optimal sequence of rich context models  from the plurality of candidate sequences . The text-to-speech engine  may then produce synthesized speech  from the optimal sequence .",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 2","b":"102"},"The selected components may be implemented on an electronic device  () that may include one or more processors  and memory . For example, but not as a limitation, the one or more processors  may include a reduced instruction set computer (RISC) processor.","The memory  may include volatile and\/or nonvolatile memory, removable and\/or non-removable media implemented in any method or technology for storage of information, such as computer-readable instructions, data structures, program modules or other data. Such memory may include, but is not limited to, random access memory (RAM), read-only memory (ROM), electrically erasable programmable read-only memory (EEPROM), flash memory or other memory technology; CD-ROM, digital versatile disks (DVD) or other optical storage; magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices; and RAID storage systems, or any other medium which can be used to store the desired information and is accessible by a computer system. Further, the components may be in the form of routines, programs, objects, and data structures that cause the performance of particular tasks or implement particular abstract data types.","The memory  may store components of the text-to-speech engine . The components, or modules, may include routines, programs instructions, objects, and\/or data structures that perform particular tasks or implement particular abstract data types. The components may include a training module , a pre-selection module , a HMM sequence module , a least divergence module , a unit pruning module , a cross correlation search module , a waveform concatenation module , and a synthesis module . The components may further include a user interface module , an application module , an input\/output module , and a data storage module .","The training module  may train a set of rich context models , and in turn, a set of decision tree-tied HMMs , to model speech data. For example, the set of HMMs  may be trained via, e.g., a broadcast news style North American English speech sample corpus for the generation of American-accented English speech. In other examples, the set of HMMs  may be similarly trained to generate speech in other languages (e.g., Chinese, Japanese, French, etc.). In various embodiments, the training module  may initially derive the set of rich context models . In at least one embodiment, the rich context models may be initialized by cloning mono-phone models.","The training module  may estimate the variance parameters for the set of the rich context models . Subsequently, the training module  may derive the decision tree-tied HMMs  from the set of rich context models . In at least one embodiment, a universal Maximum Likelihood (ML) criterion may be used to estimate statistical parameters of the set of decision tree-tied HMMs .","The training module  may further refine the set of rich context models  based on the decision tree-tied HMMs  to generate a set of refined rich context models . In various embodiments of the refinement, the training module  may designate the set of decision-tree tied HMMs  as a reference. Based on the reference, the training module  may perform a single pass re-estimation to estimate the mean parameters for the set of rich context models . This re-estimation may rely on the set of decision tree-tied HMMs  to obtain the state-level alignment of the speech corpus. The mean parameters of the set of rich context models  may be estimated according to the alignment.","Subsequently, the training module  may tie the variance parameters of the set of rich context models  using a conventional tree structure to generate the set of refined context rich models . In other words, the variance parameters of the set of rich context models  may be set to be equal to the variance parameters of the set of decision tree-tied HMMS . In this way, the data alignment of the rich context models during training may be insured by the set of the decision tree-tied HMMs . As further described below, the refined rich context models  may be stored in a data storage module .","The pre-selection module  may compose a rich context model candidate sausage. The composition of a rich context model candidate sausage may be the first step in the selection and assembly of a sequence of rich context models that represents the input text  from the set of refined context models .","In some embodiments, the pre-selection module  may initially extract the tri-phone-level context of each target rich context label of the input text  to form a pattern. Subsequently, the pre-selection module  may chose one or more refined rich context models  that match this tri-phone pattern to form a sausage node of the rich candidate sausage. The pre-selection module  may further connect successive sausage nodes to compose a sausage node. The use of tri-phone-level, context based pre-selection by the pre-selection module  may maintain the size of sequence selection search space at a reasonable size. In other words, the tri-phone-level pre-selection may maintain a good balance between sequence candidate coverage and sequence selection search space size.","However, in alternative embodiments in which the pre-selection module  is unable to obtain a tri-phone pattern, the pre-selection module  may extract bi-phone level context of each target rich context label of the input text  to form a pattern. Subsequently, the pre-selection module  may chose one or more refined rich context models  that match this bi-phone pattern to form a sausage node.","The pre-selection module  may connect successive sausage nodes to compose a rich context model candidate sausage, as shown in . The rich context model candidate sausage may encompass a plurality of rich context model candidate sequences .",{"@attributes":{"id":"p-0038","num":"0037"},"figref":["FIG. 3","FIG. 3"],"b":["302","302","208","106","304","1","304","302","306","1","306","306","1","306"],"i":["n","n","n"]},"Returning to , the HMM sequence module  may obtain a sequence of decision tree-tied HMMs that correspond to the input text . This sequence of decision tree-tied HMMs  is illustrated as the guiding sequence  in . In various embodiments, the HMM sequence module  may obtain the sequence of decision tree-tied HMMs from the set of decision tree-tied HMMs  using conventional techniques.","The least divergence module  may determine the optimal sequence  from a rich context model candidate sausage, such as the candidate sausage  of the input text . The optimal sequence  may be further used to generate a speech trajectory that is eventually converted into synthesized speech.","In various embodiments, the optimal sequence  may be a sequence of rich context models that exhibits a global trend that is \u201cclosest\u201d to the guiding sequence . It will be appreciated that the guiding sequence  may provide an over-smoothed but stable trajectory. Therefore, by using this stable trajectory as a guide, the least divergence module  may select a sequence of rich context models, or optimal sequence , that has the smoothness of the guiding sequence  and the improved local speech fidelity provided by the refined rich context models .","The least divergence module  may search for the \u201cclosest\u201d rich context model sequence by measuring the distance between the guiding sequence  and a plurality of rich context model candidate sequences  that are encompassed in the candidate sausage . In at least one embodiment, the least divergence module  may adopt an upper-bound of a state-aligned Kullback-Leibler divergence (KLD) approximation as the distance measure, in which spectrum, pitch, and duration information are considered simultaneously.","Thus, given P={p, p, . . . p} as the decision tree-tied guiding sequence , the least divergence module  may determine the state-level duration of the guiding sequence  using the conventional duration model, which may be denoted as T={t, t, . . . t}. Further, for each of rich context model candidate sequences , the least divergence module  may set the corresponding state sequence to be aligned to the guiding sequence  in a one-to-one mapping. It will be appreciated that due to the particular structure of the candidate sausage , the guiding sequence  and each of the candidate sequences  may have the same number of states. Therefore, any of the candidate sequences  may be denoted as Q={q, q, . . . q}, and share the same duration with the guiding sequence .","Accordingly, the least divergence module  may use the following approximated criterion to measure the distance between the guiding sequence  and each of the candidate sequences  (in which S represents spectrum, and f0 represents pitch):\n\n()=\u03a3()\u00b7\u2003\u2003(1)\n\nand in which D(p,q)=D(p,q)+D(p,q) is the sum of the upper-bound KLD for the spectrum and pitch parameters between two multi-space probability distribution (MSD)-HMM states:\n",{"@attributes":{"id":"p-0045","num":"0044"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msubsup":{"mi":["D","KL"],"mrow":{"mrow":{"mi":["S","f"],"mo":"\/"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"0"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["p","q"],"mo":","}}},{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["w","p"],"mn":"0"},{"mi":["w","q"],"mn":"0"}],"mo":"-"}},"mo":["\u2062","\u2062"],"mi":"log","mfrac":{"msubsup":[{"mi":["w","p"],"mn":"0"},{"mi":["w","q"],"mn":"0"}]}},{"mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["w","p"],"mn":"1"},{"mi":["w","q"],"mn":"1"}],"mo":"-"}},"mo":["\u2062","\u2062"],"mi":"log","mfrac":{"msubsup":[{"mi":["w","p"],"mn":"1"},{"mi":["w","q"],"mn":"1"}]}},{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062"],"mi":"tr","mrow":{"mo":["{","}"],"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["w","p"],"mn":"1"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mi":"p","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"mrow":{"mo":"+","msubsup":{"mi":["w","q"],"mn":"1"}},"mo":"\u2062","munderover":{"mo":"\u2211","mi":"q","mrow":{"mo":"-","mn":"1"}}}}}},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03bc","p"]},{"mi":["\u03bc","q"]}],"mo":"-"}},"mo":"\u22a4"}},"mo":"+"}}},{"mtd":{"mrow":{"mrow":[{"msubsup":{"mi":["w","l","p"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mi":"p"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mi":"q","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"mo":"-","mi":"I"}}}}},{"msubsup":{"mi":["w","l","q"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mi":"p"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mi":"q","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"mo":"-","mi":"I"}}}}}],"mo":"+"}}}]}}},{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062","\u2062"],"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["w","q"],"mn":"1"},{"mi":["w","p"],"mn":"1"}],"mo":"-"}},{"mo":["\uf603","\uf604"],"mrow":{"munder":{"mo":"\u2211","mi":"p"},"mo":"\u2062","munderover":{"mo":"\u2211","mi":"q","mrow":{"mo":"-","mn":"1"}}}}],"mi":"log"}],"mo":["+","+","+"]}],"mo":"\u2264"}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{},"sub":["0","1 ","KL","0","1"],"sup":"S"},"By using equations (1) and (2), spectrum, pitch and duration may be embedded in a single distance measure. Accordingly, the least divergence module  may select an optimal sequence of rich context models  from the rich context model candidate sausage  by minimizing the total distance D(P,Q). In various embodiments, the least divergence module  may select the optimal sequence  by choosing the best rich context candidate models for every node of the candidate sausage  to form the optimal global solution.","The unit pruning module , in combination with the cross correlation module  and the waveform concatenation module , may also determine the optimal sequence  from a rich context model candidate sausage, such as the candidate sausage  of the input text . Thus, in some embodiments, the combination of the unit pruning module , the cross correlation module , and the wave concatenation module , may be implemented as an alternative to the least divergence module .","The unit pruning module  may prune sequences of candidate sequences of rich context models  encompassed in the candidate sausage  that are farther than a predetermined distance from the guiding sequence . In other words, the unit pruning module  may select for one or more candidate sequences  with less than a predetermined amount of distortion from the guiding sequence .","During operation, the unit pruning module  may first consider the spectrum and pitch information to perform pruning within each sausage node of the candidate sausage . For example, given sausage node i, and that the guiding sequence  is denoted by P={p(1), p(2), . . . p(S)}, the corresponding state duration of node i may be represented by T={t(1), t(2), . . . t(S)}. Further, for all Nrich context model candidates Qin the node i, the state sequences of each candidate may be assumed to be aligned to the guiding sequence  in a one-to-one mapping. This is because in the structure of candidate sausage , both the guiding sequence  and each of the candidate sequences  may have the same number of states. Therefore, the candidate state sequences may be denoted as Q={q(1), q(2), . . . q(S)}, wherein each candidate sequence share the same duration Twith the guiding sequence .","Thus, the unit pruning module  may use the following approximated criterion to measure the distance between the guiding sequence  and each of the candidate sequences :\n\n()=\u03a3((),())\u00b7()\u2003\u2003(3)\n\nin which D(p,q)=D(p,q)+D(p,q) is the sum of the upper-bound KLD for the spectrum and pitch parameters between two multi-space probability distribution (MSD)-HMM states:\n",{"@attributes":{"id":"p-0051","num":"0050"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msubsup":{"mi":["D","KL"],"mrow":{"mrow":{"mi":["S","f"],"mo":"\/"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"0"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["p","q"],"mo":","}}},{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["w","p"],"mn":"0"},{"mi":["w","q"],"mn":"0"}],"mo":"-"}},"mo":["\u2062","\u2062"],"mi":"log","mfrac":{"msubsup":[{"mi":["w","p"],"mn":"0"},{"mi":["w","q"],"mn":"0"}]}},{"mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["w","p"],"mn":"1"},{"mi":["w","q"],"mn":"1"}],"mo":"-"}},"mo":["\u2062","\u2062"],"mi":"log","mfrac":{"msubsup":[{"mi":["w","p"],"mn":"1"},{"mi":["w","q"],"mn":"1"}]}},{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062"],"mi":"tr","mrow":{"mo":["{","}"],"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["w","p"],"mn":"1"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mi":"p","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"mrow":{"mo":"+","msubsup":{"mi":["w","q"],"mn":"1"}},"mo":"\u2062","munderover":{"mo":"\u2211","mi":"q","mrow":{"mo":"-","mn":"1"}}}}}},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03bc","p"]},{"mi":["\u03bc","q"]}],"mo":"-"}},"mo":"\u22a4"}},"mo":"+"}}},{"mtd":{"mrow":{"mrow":[{"msubsup":{"mi":["w","l","p"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mi":"p"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mi":"q","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"mo":"-","mi":"I"}}}}},{"msubsup":{"mi":["w","l","q"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"munder":{"mo":"\u2211","mi":"p"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mi":"q","mrow":{"mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"mo":"-","mi":"I"}}}}}],"mo":"+"}}}]}}},{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062","\u2062"],"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mi":["w","q"],"mn":"1"},{"mi":["w","p"],"mn":"1"}],"mo":"-"}},{"mo":["\uf603","\uf604"],"mrow":{"munder":{"mo":"\u2211","mi":"p"},"mo":"\u2062","munderover":{"mo":"\u2211","mi":"q","mrow":{"mo":"-","mn":"1"}}}}],"mi":"log"}],"mo":["+","+","+"]}],"mo":"\u2264"}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{},"sub":["0","1 ","KL","0","1"],"sup":"S"},"Moreover, by using equations (3) and (4), as well as a beam width of \u03b2, the unit pruning module  may prune those candidate sequences  for which:\n\n()>min()+\u03b2\u03a3\u2003\u2003(5).\n","Accordingly, for each sausage node, only the one or more candidate sequences  with distortions that are below a predetermined threshold from the guiding sequence  may survive pruning. In various embodiments, the distortion may be calculated based not only on the static parameters of the models, but also their delta and delta-delta parameters.","The unit pruning module  may also consider duration information to perform pruning within each sausage node of the candidate sausage . In other words, the unit pruning module  may further prune candidate sequences  with durations that do not fall within a predetermined duration interval. In at least one embodiment, for a sausage node i, the target phone-level mean and variance given by a conventional HMM-based duration model may be represented by \u03bcand \u03c3, respectively. In such an embodiment, the unit pruning module  may prune those candidate sequences  for which:\n\n|\u2212\u03bc|>\u03b3\u03c3\u2003\u2003(6)\n\nin which dis the duration of the jcandidate sequence, and \u03b3 is a ratio controlling the pruning threshold.\n","In some embodiments, the unit pruning module  may perform the calculations in equations (3) and (4) in advance, such as during an off-line training phase, rather than during an actual run-time of the speech synthesis. Accordingly, the unit pruning module  may generate a KLD target cost table  during the advance calculation that stores the target cost data. The target cost table  may be further used during a search for an optimal rich context unit path.","The cross correlation module  may search for an optimal rich context unit path through rich context models of the one or more candidate sequences  in the candidate sausage  that have survived pruning. In this way, the cross correlation module  may derive the optimal rich context model sequence . The optimal rich model sequence  may be the smoothest rich context model sequence. In various embodiments, the cross correlation module  may implement the search as a search for a path with minimal concatenation cost. Accordingly, the optimal sequence  may be a minimal concatenation cost sequence.","The waveform concatenation module  may concatenate waveform units along a path of the derived optimal rich context model sequence  to form an optimized waveform sequence. The optimized waveform sequence may be further converted into synthesized speech. In various embodiments, the waveform concatenation module  may use a normalized cross correlation as the measure of concatenation smoothness. Given two time series x(t), y(t), and an offset of d, the cross correlation module  may calculate the normalized cross correlation r(d) as follows:",{"@attributes":{"id":"p-0058","num":"0057"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"d"}},"mo":"=","mfrac":{"mrow":[{"munder":{"mo":"\u2211","mi":"t"},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}}},{"msub":{"mi":["\u03bc","x"]},"mo":"\u00b7","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"y","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","d"],"mo":"-"}}},"mo":"-","msub":{"mi":["\u03bc","y"]}}}}],"mo":"-"}}},{"msqrt":[{"mrow":{"munder":{"mo":"\u2211","mi":"t"},"mo":"\u2062","msup":{"mrow":{"mo":["[","]"],"mrow":{"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},"mo":"-","msub":{"mi":["\u03bc","x"]}}},"mn":"2"}}},{"mrow":{"munder":{"mo":"\u2211","mi":"t"},"mo":"\u2062","msup":{"mrow":{"mo":["[","]"],"mrow":{"mrow":{"mi":"y","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["t","d"],"mo":"-"}}},"mo":"-","msub":{"mi":["\u03bc","y"]}}},"mn":"2"}}}],"mo":"\u00b7"}]}}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}}},"in which \u03bc, and \u03bcare the mean of x(t) and y(t) within the calculating window, respectively. Thus, at each concatenation point in the sausage , and for each waveform pair, the waveform concatenation module  may first calculate the best offset d that yields the maximal possible r(d), as illustrated in .",{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 4","sub":["prec ","foll ","prec ","foll ","foll ","prec ","foll "],"b":["402","404","218","402","218","404","404","402","404"]},"Returning to , it will be appreciated that the calculation of the normalized cross-correlation in equation (7) may introduce a lot of input\/output (I\/O) and computation efforts if the waveform units are loaded during run-time of the speech synthesis. Thus, in some embodiments, the waveform concatenation module  may calculate the normalized cross-correlation in advance, such as during an off-line training phase, to build a concatenation cost table . Thus, the concatenation cost table  may be further used during waveform concatenation along the path of the selected optimal rich context model sequence.","Following the selection of the optimal sequence of the rich context models  or a waveform sequence that is derived from the optimal sequence , the text-to-speech engine  may further use the synthesis module  to process the optimal sequence  or the waveform sequence into synthesized speech .","The synthesis module  may process the optimal sequence , or the waveform sequence that is derived from the optimal sequence , into synthesized speech . In various embodiments, the synthesis module  may use the predicted speech data from the input text , such as the speech patterns, line spectral pair (LSP) coefficients, fundamental frequency, gain, and\/or the like, in combination with the optimal sequence  or the waveform sequence to generate the synthesized speech .","The user interface module  may interact with a user via a user interface (not shown). The user interface may include a data output device (e.g., visual display, audio speakers), and one or more data input devices. The data input devices may include, but are not limited to, combinations of one or more of keypads, keyboards, mouse devices, touch screens, microphones, speech recognition packages, and any other suitable devices or other electronic\/software selection methods. The user interface module  may enable a user to input or select the input text  for conversion into synthesized speech .","The application module  may include one or more applications that utilize the text-to-speech engine . For example, but not as a limitation, the one or more applications may include a global positioning system (GPS) navigation application, a dictionary application, a text messaging application, a word processing application, and the like. Accordingly, in various embodiments, the text-to-speech engine  may include one or more interfaces, such as one or more application program interfaces (APIs), which enable the application module  to provide input text  to the text-to-speech engine .","The input\/output module  may enable the text-to-speech engine  to receive input text  from another device. For example, the text-to-speech engine  may receive input text  from at least one of another electronic device, (e.g., a server) via one or more networks. Moreover, the input\/output module  may also provide the synthesized speech  to the audio speakers for acoustic output, or to the data storage module .","As described above, the data storage module  may store the refined rich context models . The data storage module  may further store the input text , as well as rich context models , decision tree-tied HMMs , the guiding sequence of HMM models , the plurality of candidate sequences of rich context models , the optimal sequence , and the synthesized speech . However, in embodiments in which the target cost table  and the concatenation cost able  are generated, the data storage module may store tables - instead of the rich context models  and the decision tree-tied HMMs . The one or more input texts  may be in various forms, such as documents in various formats, downloaded web pages, and the like. The data storage module  may also store any additional data used by the text-to-speech engine , such as various additional intermediate data produced during the production of the synthesized speech  from the input text , e.g., waveform sequences.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":["FIGS. 5-6","FIGS. 5-6"],"b":"102"},{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 5"},"At block , the training module  of the text-to-speech engine  may derive rich context models  and trained decision tree-tied HMMs  based on a speech corpus. The speech corpus may be a corpus of one of a variety of languages, such as English, French, Chinese, Japanese, etc.","At block , the training module  may further estimate the mean parameters of the rich context models  based on the trained decision tree-tied HMMs . In at least one embodiment, the training module  may perform the estimation of the mean parameters via a single pass re-estimation. The single pass re-estimation may use the trained decision tree-tied HMMs  to obtain the state level alignment of the speech corpus. The mean parameters of the rich context models  may be estimated according this alignment.","At block , based on the estimated mean parameters, the training module  may set the variance parameters of the rich context models  equal to that the trained decision tree-tied HMMs . Thus, the training module  may produce refined rich context models  via blocks -.","At block , the text-to-speech engine  may generate synthesized speech  for an input text  using at least some of the refined rich context models .","At block , the text-to-speech engine  may output the synthesized speech . In various embodiments, the electronic device  on which the text-to-speech engine  resides may use speakers to transmit the synthesized speech  as acoustic energy to be heard by a user. The electronic device  may also store the synthesized speech  as data in the data storage module  for subsequent retrieval and\/or output.",{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 6","b":["600","600","508","500"]},"At block , the pre-selection module  of the text-to-speech engine  may perform a pre-selection of the refined rich context models . The pre-selection may compose a rich context model candidate sausage .","At block , the HMM sequence module  may obtain a guiding sequence  from the decision tree-tied HMMs  that corresponds to the input text . In various embodiments, the HMM sequence module may obtain the guiding sequence of decision tree-tied HMMs  from the set of decision tree-tied HMMs  using conventional techniques.","At block , the least divergence module  may obtain the optimal sequence  from a rich context model candidate sausage, such as the candidate sausage  of the input text . The candidate sausage  may encompass the plurality of rich context model candidate sequences . In various embodiments, the least divergence module  may select the optimal sequence  by finding a rich context model sequence with the \u201cshortest\u201d measured distance from the guiding sequence  that is included in the plurality of rich context model candidate sequences .","At block , the synthesis module  may generate and output synthesized speech  based on the selected optimal sequence  of rich context models.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIG. 7"},"At block , the pre-selection module  of the text-to-speech engine  may perform a pre-selection of the refined rich context models . The pre-selection may compose a rich context model candidate sausage .","At block , the HMM sequence module  may obtain a guiding sequence  from the decision tree-tied HMMs  that corresponds to the input text . In various embodiments, the HMM sequence module may obtain the guiding sequence of decision tree-tied HMMs  from the set of decision tree-tied HMMs  using conventional techniques.","At block , the unit pruning module  may prune sequences of rich context model candidate sequences  of rich context models encompassed in the candidate sausage  that are farther than a predetermined distance from the guiding sequence . In other words, the unit pruning module  may select one or more candidate sequences  that are within a predetermined distance from the guiding sequence . In various embodiments, the unit pruning module  may perform the pruning based on spectrum, pitch, and duration information of the candidate sequences . In at least one of such embodiments, the unit pruning module  may generate the target cost table  in advance of the actual speech synthesis. The target cost table  may facilitates the pruning of the sequences of rich context model candidate sequences .","At block , the cross correlation search module  may conduct a cross correlation-based search to derive the optimal rich context model sequence  encompassed in the candidate sausage  from the one or more candidate sequences  that survived the pruning. In various embodiments, the cross correlation module  may implement the search for the optimal sequence  as a search for a minimal concatenation cost path through the rich context models of the one or more surviving candidate sequences . Accordingly, the optimal sequence  may be a minimal concatenation cost sequence. In some embodiments, the waveform concatenation module  may calculate the normalized cross-correlation in advance of the actual speech synthesis to build a concatenation cost table . The concatenation cost table  may be used to facilitate the selection of the optimal rich context model sequence .","At block , the waveform concatenation module  may concatenate waveform unit along a path of the derived optimal sequence  to form an optimized wave sequence. The synthesis module  may further convert the optimized wave sequence into synthesized speech.",{"@attributes":{"id":"p-0086","num":"0085"},"figref":["FIG. 8","FIG. 8"],"b":["800","102","800","800"]},"In at least one configuration, computing device  typically includes at least one processing unit  and system memory . Depending on the exact configuration and type of computing device, system memory  may be volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.) or some combination thereof. System memory  may include an operating system , one or more program modules , and may include program data . The operating system  includes a component-based framework  that supports components (including properties and events), objects, inheritance, polymorphism, reflection, and provides an object-oriented component-based application programming interface (API), such as, but by no means limited to, that of the .NET\u2122 Framework manufactured by the Microsoft\u00ae Corporation, Redmond, Wash. The computing device  is of a very basic configuration demarcated by a dashed line . Again, a terminal may have fewer components but may interact with a computing device that may have such a basic configuration.","Computing device  may have additional features or functionality. For example, computing device  may also include additional data storage devices (removable and\/or non-removable) such as, for example, magnetic disks, optical disks, or tape. Such additional storage is illustrated in  by removable storage  and non-removable storage . Computer storage media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data. System memory , removable storage  and non-removable storage  are all examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computing device . Any such computer storage media may be part of device . Computing device  may also have input device(s)  such as keyboard, mouse, pen, voice input device, touch input device, etc. Output device(s)  such as a display, speakers, printer, etc. may also be included.","Computing device  may also contain communication connections  that allow the device to communicate with other computing devices , such as over a network. These networks may include wired networks as well as wireless networks. Communication connections  are some examples of communication media. Communication media may typically be embodied by computer readable instructions, data structures, program modules, etc.","It is appreciated that the illustrated computing device  is only one example of a suitable device and is not intended to suggest any limitation as to the scope of use or functionality of the various embodiments described. Other well-known computing devices, systems, environments and\/or configurations that may be suitable for use with the embodiments include, but are not limited to personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-base systems, set top boxes, game consoles, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and\/or the like.","The implementation of text-to-speech engines that uses rich context models in HMM-based synthesis may generate speech with crisper formant structures and richer details than those obtained from conventional HMM-based speech synthesis. Accordingly, the use of rich context models in HMM-based speech synthesis may provide synthesized speech that is more natural sounding. As a result, user satisfaction with embedded systems that present information via synthesized speech may be increased at a minimal cost.","In closing, although the various embodiments have been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended representations is not necessarily limited to the specific features or acts described. Rather, the specific features and acts are disclosed as exemplary forms of implementing the claimed subject matter."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The use of the same reference number in different figures indicates similar or identical items.",{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
