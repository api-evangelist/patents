---
title: System and method for tracking an object in a video and linking information thereto
abstract: An image processing system for use in development and playback of interactive video. In a development mode of operation, pixel or video objects are selected in a frame by way of a development graphical user interface. The system automatically tracks the selected pixel objects in the preceding and succeeding video frames by determining range limits for various color variables of the selected pixel object to compensate for the effects in lighting changes and decompression effects. The system automatically locates pixel objects within the calculated range limits in the preceding and succeeding video frames and generates a pixel object file which identifies the coordinates of the selected pixel object in each frame. The pixel object file is linked to a data object file which links the selected pixel objects to data objects. The pixel object file and data object file, collectively “linked video files,” are created during a development mode of operation. During a playback mode of operation, the linked video files are imported to a video hosting platform which includes a video playback application and a common media player application programming interface (API) for playback of the video content. The video playback application supports processing of the linked video files to enable pixel objects to be selected by a pointing device and linked to data objects by way of a client side graphical user interface.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07804506&OS=07804506&RS=07804506
owner: Creatier Interactive, LLC
number: 07804506
owner_city: Los Angeles
owner_country: US
publication_date: 20040225
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","COMPUTER LISTING APPENDIX","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["This application is a continuation of U.S. patent application Ser. No. 09\/944,543, filed on Aug. 31, 2001, now U.S. Pat. No. 6,774,908, which is a continuation-in-part of commonly-owned, co-pending patent application Ser. No. 09\/679,391, filed Oct. 3, 2000, entitled \u201cMethod and Apparatus for Associating the Color of an Object with an Event\u201d.","This application includes computer program listing appendix on compact disc, hereby incorporated by reference.","1. Field of the Invention","The present invention relates to an interactive video system and more particularly to a system and method for creating and playback of interactive video. In a development mode of operation, the system automatically tracks a pixel object in a video sequence that has been selected in a video frame and generates one or more linked video files representative of the location of the selected object in all of the frames in the sequence, along with links to data objects for use in conjunction with an interactive video system. In a playback mode of operation, the system enables the pixel objects to be selected during subsequent playback of the video frames with a pointing device, such as a mouse, providing access to linked data objects.","2. Description of the Prior Art","There is a trend toward interactive video for various commercial, educational and entertainment purposes. To provide such interactivity, video content producers need to link various desired video or pixel objects within each of the video frames of the video content with data objects, such as web pages. In order to provide links for selected pixel objects in the various video frames, video content producers are known to implement image processing techniques for linking selected pixel objects in each of the various video frames to other platforms. Both manual and automatic image processing techniques are known.","Examples of systems in which the links are manually \u201cauthored\u201d are disclosed in U.S. Pat. Nos. 5,774,666; 6,076,104 and 5,929,849. In general, manual image processing techniques involve editing each video frame in a video sequence and manually embedding links or hot spots in each of the frames for the desired pixel objects. Such a task is extremely labor intensive and thus expensive to implement. For example, a 30-minute television show is known to include 22 minutes of video content and 8 minutes of advertising inserts. At a playback rate of 30 frames per second, the 22 minutes of video content is comprised of (30 frames\/second\u00d760 seconds\/minute\u00d722 minutes) 39,600 frames. In order to provide the ability for interactivity, a video content producer thus would need to edit 39,600 frames for a 22-minute broadcast and embed links in each of the frames for one or more various objects. As mentioned above, such a task is extremely labor intensive and thus relatively expensive.","In order to improve image processing of video content for the purpose of providing interactivity by providing links to various objects within the video frames, automatic image processing systems have been developed that are able to automatically track an object in a video frame sequence and link the pixel objects to other platforms. Examples of such automatic authoring systems are disclosed in U.S. Pat. Nos. 6,205,231; 6,169,573 and 5,867,584.","U.S. Pat. No. 6,169,573 relates to a system for tracking objects in a sequence of video frames which track objects in a compressed format using variables encoded in MPEG compressed video content. Unfortunately, the system disclosed in the '573 patent is only useful for tracking objects in MPEG format and is not suitable for use with video content in an uncompressed format or other video format, such as .mov.","U.S. Pat. No. 6,205,231 discloses an image processing system that is able to automatically track an object in a video frame sequence. The system disclosed in the '231 patent incorporates known image processing techniques for tracking an edge of an object based on its color or luminosity characteristics. The system disclosed in the '231 patent \u201ctags\u201d selected objects in the video frame sequence which enables the tags and thus the selected object to be linked to other platforms, such as websites.","There are various problems with the system such as disclosed in the '231 patent, such as inaccurate tracking of the pixel object. In particular, image processing systems, such as disclosed in the '231 patent locate pixel objects within a video frame by locating pixels having a certain color variable value. Unfortunately, the color variable values for an object may change from scene to scene due to lighting effects or due to the effects of the decompression. The system disclosed in the '231 patent does not compensate for such changes and thus is not able to accurately track pixel objects in a video frame sequence. In addition, embedding tags in the original video content is resource intensive requiring relatively high level computing platforms.","U.S. Pat. No. 5,867,584 also relates to an image processing system for automatically tracking objects in a video frame sequence. This system attempts to anticipate the position of the selected object in succeeding frames by generating a number of virtual wire frames along a predetermined anticipatory trajectory of the pixel object in succeeding frames and comparing the pixel variables within the test windows of the anticipatory trajectory with the original test window. As such, the system disclosed in the '584 patent is resource intensive. In addition, the system utilizes known image processing techniques based on the intensity characteristic of the pixels within the windows. As such, the system disclosed in the '584 patent does not take into account changes in brightness and shifts in the hue due to lighting effects in the video frame and thus is unable to accurately track an object in a sequence of video frames. Thus, there is a need for image processing system for automatically tracking a selected object in a video frame sequence that is able to relatively accurately track the object and is not resource intensive.","The present invention relates to an image processing system for use in development and playback of interactive video. In a development mode of operation, pixel or video objects are selected in a frame by way of a developmental graphical user interface. The system automatically tracks the selected pixel objects in the preceding and succeeding video frames by determining range limits for various color variables of the selected pixel object to compensate for the effects in lighting changes and decompression effects. The system automatically locates pixel objects within the calculated range limits in the preceding and succeeding video frames and generates a pixel object file which identifies the coordinates of the selected pixel object in each frame. The pixel object file is linked to a data object file which links the selected pixel objects to data objects. The pixel object file and data object file, collectively \u201clinked video files,\u201d are created during a development mode of operation. During a playback mode of operation, the linked video files are imported to a video hosting platform which includes a video playback application and a common media player application programming interface (API) for playback of the video content. The video playback application supports processing of the linked video files to enable pixel objects to be selected by a pointing device and linked to data objects by way of a client side graphical user interface.","The present invention relates to a system and method for providing interactivity to various types of video content, such as streaming video content and on-demand video content, for example, from a DVD player. In a development mode of operation, the present invention includes an image processing system for automatically tracking a pixel object, selected in a frame of a video frame sequence, in preceding and succeeding video frames for the purpose of linking the selected object to one or more data objects, such as a uniform resource locator, fixed overlay information, a streaming video link, database interaction link or other resource platform (hereinafter \u201cdata object\u201d). In accordance with an important aspect of the invention, the image processing system compensates for changes in brightness and shifts in hue on a frame by frame basis due to lighting effects and decompression effects by determining range limits for various color variable values, such as hue (H), red-green (R-G), green-blue (G-13) and saturation value(SV) to provide relatively accurate tracking of a pixel object. Moreover, unlike some known image processing systems, the image processing system that forms part of the present invention does not embed tags in the video content. Rather the system, in accordance with the present invention, generates linked video files, which identify the pixel coordinates of the selected pixel object in each video frame as well as data object links associated with each pixel object. The linked video files are exported to a playback platform which includes a video playback application which supports playback of content of various compression schemes such as those used by various commonly known media players, such as Real Player, Windows Media Player and Quick Time and enables pixel objects to be selected during playback with a pointing device, such as a mouse which enables access to linked to data objects. The video playback application may be hosted in a video hosting platform or reside directly within a playback platform.","Graphical user interfaces (GUI) may be provided to facilitate the development of linked video files during a development mode of operation as well as facilitate playback during a playback mode of operation. In particular, a developmental GUI, for example, as illustrated in , may be used to facilitate processing of the original video content by either a video content provider or an application service provider, to develop the linked video files as discussed above. A client side or playback GUI, for example, as illustrated in , may be provided to facilitate playback.","Various embodiments of the invention are contemplated. For example, referring to , the invention may be implemented by way of a resource platform, shown within the dashed box , formed from one or more servers or work stations, which may constitute an Application Service Provider or may be part of the video content producer. In this implementation, a source of video content , for example, an on-demand source from, for example, a DVD player or streaming video source from a video content producer, is transferred to the resource platform , which, in turn, processes the video content  and links selected pixel objects within the video content  to data objects and generates linked video files .","The resource platform  is used to support a development mode of operation in which the linked video files  are created from the original video content . As shown in , the resource platform  may include an exemplary resource computing platform  and a video processing support computing platform . The resource computing platform  includes a pixel object capture application , a video linking application  and generates the linked video files  as discussed above. The pixel object capture application  is used to capture a pixel object selected in a frame of video content . The video linking application  automatically tracks the selected pixel object in preceding and successive frames in the video sequence and links the pixel objects to data objects by way of a pixel object file and data object file, collectively referred to as linked video files .","The resource computing platform  may be configured as a work station with dual 1.5 GHz processors, 512 megabits of DRAM, a 60 gigabit hard drive, a DVD-RAM drive, a display, for example, a 21-inch display; a 100 megabit Ethernet card, a hardware device for encoding video and various standard input devices, such as a tablet, mouse and keyboard. The resource computing platform  is, preferably provided with third party software to the hardware.","The video processing support computing platform  includes a show information database  and a product placement database . The show information database  includes identifying information relative to the video content, such as show name, episode number and the like. The product placement database  includes data relative to the various data objects, such as website addresses, to be linked to the selected pixel objects. The show information database  as well as the product placement database  may be hosted on the video processing support computing platform  or may be part of the resource computing platform .","In accordance with an important aspect of the invention, the linked video files  are created separately from the original video content  and are amenable to being exported to a video hosting platform  for use during a playback mode of operation. The video hosting platform  includes a common media player application programming interface (API)  and a playback application  for enabling playback of either streaming or on-demand video content with interactivity. In particular, the video playback application  enables pixel objects to be selected during playback of the video content by a standard input device, such as a mouse, and linked to data objects, for example, other resource platforms.","As shown in , the video hosting platform  is shown separate from the playback platform , which may be a personal computing platform or even a set top box. Alternatively, the video hosting platform  may be included within the playback platform . The video hosting platform  may be, for example, a personal computing platform.","Development Mode of Operation","The development mode of operation is discussed with reference to . Turning to , a video source, such as, a streaming video source, for example, from the Internet or an on-demand video source, such as a DVD player, is imported by the pixel object capture application  () which captures, for example, 12 frames per second of the video content  and converts it to a bit map file . In particular, the video content , for example, in MPEG format, is decompressed using public domain decoder software, available from the MPEG website (www.mpeg.org) developed by the MPEG software simulation group, for example, MPEG 2 DEC, an executable MPEG 2 decoder application. As is known in the art, such MPEG decoder software decodes an entire MPEG file before providing global information on the file itself. Since the video content must be identified by frame for use by the pixel object capture application  and the video linking application , the frame information may be read from the decoded MPEG file once all of the frames have been decoded or alternatively determined by a frame extraction application which stores the frame information in a memory buffer as the MPEG file is being loaded into the pixel capture application  as illustrated in  and described below.","Frame Extraction Application","The frame extraction application is illustrated in  and described below. Referring to , the MPEG file is imported into the pixel object capture application  in compressed format in step . In this embodiment, the pixel object capture application  works in conjunction with the standard MPEG decoder software as illustrated in  to avoid waiting until the entire file is decoded before obtaining the frame information. While the MPEG file is being imported, the pixel object capture application  reads the header files of the MPEG data in step  and stores data relating to the individual frame type and location in a memory buffer in step . As such, the pixel object capture system  is able to decode selected frames of the compressed MPEG file without the need for decoding all of the previous frames in step . Based upon the frame information stored in the memory buffer in step , the decoded MPEG files may then be converted to a bit map file  (), as discussed above in step .","Section Break Application","The pixel object capture application  may optionally be provided with a section break application  () to facilitate downstream processing and aid partitioning of the content among several users. The section break application  analyzes the video content during loading. The section break data is stored in a temporary buffer  () and used for pixel object analysis of a selected frame and proceeding and succeeding frames by the pixel object capture application  and the video linking application .","The section break application  automatically analyzes the video content to determine how changes in lighting affect RGB values creating large shifts in these values. In particular, the median average of the pixel values for a series of frames is computed. The section break application  compares the changes in the pixel values with the median average. A section break may be determined to be an approximately 5\u00d7 change in pixel values from the median average. These section breaks are stored in a buffer  as a series of sequential frame numbers representing (start frame, end frame) where each start frame equals the proceeding frame plus one frame until the end of the video. This information may be edited by way of the graphical user interface  (), discussed below. If changes are made to the frame numbers corresponding to the section breaks, the new information is sent to the section break memory buffer  () where the original information is replaced.","As will be discussed in more detail below, the frames in the video content are analyzed for a selected pixel object during a session with the pixel object capture application  (). A pixel object may be selected in any frame of a video sequence  (). The video linking application  processes preceding and subsequent frames  by automatically tracking the selected pixel object and generating linked video files  for an entire segment as defined by the segment break application, or for a length of frames determined by the operator. The segment may be as small as a single frame or may include all the frames in the content.","Developmental Graphical User Interface","In order to facilitate development, a developmental graphical user interface  may be provided, as illustrated in . As shown, the developmental graphical user interface  includes a viewing window  for displaying a frame of video content and a number of exemplary data fields to associate information with the video content.","An exemplary product placement list display window  is used to provide a graphic list of all of the data objects associated with a particular video frame sequence. The product placement list display window  is populated by the product placement database  (). The list of data objects is propogated anytime the developmental graphical user interface  is created or an existing graphical user interface  is opened.","As shown in , available data objects are displayed in the product placement list display window  as text and\/or icons. In order to facilitate linking of the data objects to various pixel objects within the video frame sequence, the data objects displayed in the product placement display window  may be displayed in different colors. For example, one color may be used for data objects which have been linked to pixel objects while a different color may be used for data objects which have not been assigned to pixel objects. Such technology is well within the ordinary skill in the art, for example, as disclosed in U.S. Pat. No. 5,983,244, hereby incorporated by reference.","A \u201cShow Info\u201d data field  may also be provided in the developmental graphical user interface . The show information data field  is populated by the show information database  and may include various data associated with the video frame sequence, such as production company name; show name; episode number\/name; initial broadcast date; and proposed ratings.","A \u201cProduct Placement Info\u201d data field  and an associated display  may also be provided. The display area  is a reduced size image of the image displayed in the display window . The Product Placement Info data field  include various information regarding the data objects stored in the product placement database  () for a selected data object. For example, these product placement information data object fields may include the following fields: product name; placement description; action, for example, redirect to another server; address of the alternate server; a product identifier; a locator descriptor as well as a plurality of data fields ,  and  which indicate the frame locations of the data objects in the product placement list display  that have been linked to pixel objects. In particular, the data field  indicates the first frame in the video frame sequence in which the data object, identified in the Product Placement Info data field  is been linked to a pixel object. Similarly, the data field  identifies the last frame in the video frame sequence in which the data object has been linked to a pixel object. Lastly, the data field  identifies the total number of frames in the video frame sequence in which the selected data object has been linked to pixel objects.","In order to facilitate automatic authoring of the video frame sequence, the developmental graphical user interface  may be provided with a number of control buttons -. These control buttons - are selected by a pointing device, such as a mouse, and are collectively referred to as \u201cEnabling Tools.\u201d A \u201cSet Scope\u201d control button , when selected, allows a user to select a pixel object in the display window  by way of a point device. An x, y display  identifies the x and y coordinates within the display window  corresponding to a mouse click by the user in connection with the selection of the pixel object within the display window .","A \u201cSet First Frame\u201d control button  allows the first frame of the video frame sequence to be selected by the user. Once the \u201cSet First Frame\u201d button  is selected, a number of control buttons ,  and  as well as a scroll bar  may be used to advance or back up the frame being displayed in the display window . A counter display  is provided which identifies the selected frame.","Once the first frame is selected by the user, as discussed above, a \u201cBound Object\u201d button  may be selected. The Bound Object button  causes the system to automatically draw a boundary around the selected pixel object based upon image processing edge boundary techniques as discussed below. The boundary may take the shape of a geometric object, such as a square, rectangle or circle as discussed in more detail below in connection with the pixel object capture application . After initial object has been captured, the Track Object button  may be selected for initiating automatic tracking or authoring of the selected pixel object in both proceeding and succeeding frames. As will be discussed in more detail below, the pixel object locations video frames and are used to create the linked video files .","In order to facilitate development of the linked video file , markers may be used under the control of the control buttons -. The markers are used to identify the first frame associated with a marker. For example, a marker display window  is provided. The \u201cInsert Marker\u201d button  is selected to mark the first frame linked to a specific pixel object. The markers may be displayed in text and include a reduced size version of the marked frame.","The markers can be changed and deleted. The \u201cChange Marker\u201d button  allows a marker to be changed. In particular, by selecting the \u201cChange Marker\u201d button , the frame associated with that marker can be changed. This may be done by advancing or backing up the video frame sequence until the desired frame is displayed in the display window . The current marker and the marker display window  may then be changed to refer to a different frame number by simply selecting the \u201cChange Marker\u201d button .","A \u201cDelete Marker\u201d button  allows markers in the marker display window  to be deleted. In order to delete a marker, the marker is simply highlighted in the marker display window  and the \u201cDelete Marker\u201d button  is selected.","A \u201cShow Marker\u201d button  may also be provided. The \u201cShow Marker\u201d button  controls the display of markers in the marker display window . The \u201cShow Marker\u201d button  may be provided with a toggle-type function in which a single click shows the markers in the marker display window  and a subsequent click clears the marker display window .","Each of the markers are displayed in a content map display window . The content map display window  displays a linear representation of the entire content with all markers depicted along with the frame numbers where the markers appear.","Pixel Object Capture Application","The pixel object capture application  is initiated after the first frame is selected by the user by way of the development graphical user interface . In particular, After the section breaks are determined, the estimated first frame of the content is displayed in a viewing window  on the graphical user interface . Once this frame is loaded in the viewing window , the user may choose to specify another frame to be notated as the first frame. This is done to ensure that any extra frames captured with the content that do not actually belong to the beginning of the content can be skipped. The user may select a specific frame as the first frame as discussed above. The selected video frame is then loaded into the viewing window  for frame analysis as discussed below. The process of choosing the first frame is only performed once at the beginning of the program content, it is not necessary to do this at the start of each section.","When the viewing window  is loaded with content, the resource computing platform  accesses the show information database  and the product placement database  () to populate the various data fields in the developmental graphical user interface  () as discussed above.","Once a frame has been loaded into the viewing window  () in the developmental graphical user interface , pixel objects are selected and captured during a session with the pixel object capture application  (). The video linking application  automatically tracks the selected pixel objects in the preceding and succeeding frames and generates linked video files , which link the selected pixel objects with data objects, stored in the product placement data base .","Selection and capturing of a pixel object is illustrated in connection with . In general, a pixel object is visually located in the viewing window  () during a session with the pixel object capture application  by selecting a pixel in a single frame corresponding to the desired pixel object by way of a pointing device coupled to the resource computing platform  () and processed as illustrated in . The selected pixel is captured in step . The captured pixel is analyzed in step  for either RGB (red, green, blue) values or Hue. In step , the system determines whether the hue value is defined. If so, range limits for the hue value are determined in step . Alternatively, the RGB color variable value component for the selected pixel may be calculated along with its range limits in step . The initial determination of the range limits for the hue or ROB color variables is determined by, for example, \u00b110 of the Hue or RGB color variable value. After the range limits for either the hue or the RGB color variables have been determined, the system analyzes the pixels in a 10-pixel radius surrounding the selected pixel for pixels with hue\/value components falling within the first calculated range limits in step . The pixels that fall within these range limits are captured for further analysis. Range values for the pixels captured in step  are calculated in step .","For example, range limits for the color variables: hue (H), red-green (R-G), green-blue (G-B) and the saturation value(SV) are determined for each of the variables. The range limits are determined by first determining the mean of the color variable from the sample and then for each variable, calculating the range limits to be, for example, 3\u00d7 the sigma deviation from the mean to set the high and low range limit for each variable. Once the range limit for the variables are determined, known image processing techniques, for example, edge processing techniques, for example, as disclosed on pages 1355-1357 of Hu et al., \u201cFeature Extraction and Matching as Signal Detection\u201d Vol. 8, No. 6, 1994, pages 1343-1379, hereby incorporated by reference, may be used to determine the boundaries of the color within a frame as indicated in step . All of the pixels within the bounding area are captured that fall within the range limits for the variables, hue, R-G, G-V, SVin step . Next, in step , a centroid is calculated for the bounding area and the range limits for the color variables are recalculated in step . The recalculated range limits determined in step  are used for determination of the edges of the bounding area in step  to define a finalized bounding area in step  for the object. In step , the location of the bounding area of the selected object is determined by capturing the (x, y) coordinates for the upper left corner and the lower right corner as well as the coordinates of the centroid of the bounded area. Thus far, selection of an object in a single frame of the video content has been discussed.","Automatic Pixel Object Tracking","Automatic tracking of the selected pixel object is described in connection with . In particular,  represents a flow chart for the automatic tracking system while  represents a visual illustration of the operation of the automatic tracking system. Referring first to , an exemplary frame  is illustrated, which, for simplicity, illustrates a red object  against a blue background. As shown, the pixel object  has a centroid at point) Xalong the Xaxis . As shown in frame  identified with the reference numeral , the example assumes that the pixel object  has moved along the x-axis  such that its centroid is located at position x along the x-axis .","Referring to , the video linking application  () begins automatic tracking by starting at the centroid of the previous frame in step . Thus, the video linking application  samples a 10-pixel radius  relative to the previous frame centroid in step  as illustrated in . Using the range limits for the color variables previously determined, the video linking application  locates pixels in the sample within the previous color variable range in step . As shown in , this relates to the cross-hatched portion  in frame . In order to compensate for variances in the color variables due to lighting effects and decompression effects, the video linking application  next determines a rough color variable range for the pixels within the cross-hatched area  in step  using the techniques discussed above. After the rough color variable range is calculated, the video linking application  samples a larger radius, for example, an 80 pixel radius, based on the previous frame centroid in step . As shown in , this example assumes that a substantial portion of the pixel object  is within the second sample range. In step , the pixels in the new sample which fall within the rough color variable range are located and are indicated by the cross-hatched area  in . In order to further compensate for variances in the color variables, the video linking application  recalculates the color variable ranges for the located samples in step . Once the refined color variable range has been determined, the pixels within the recalculated color variable range are located in step . As shown by the double cross-hatched area  in , the pixels within the recalculated color variable range are illustrated in . As can be seen from , the pixels falling within the rough color range, in the example, are shown to cover a larger area than the pixel object . Once the color range values are recalculated in step  in the pixels within the recalculated color variable range are determined in step  the pixel object  is located and in essence filters out pixels falling outside of the pixel object  as shown in . Once the pixels are located with the recalculated color variable range in step , a new centroid is determined in step . In addition to calculating the centroid, the video linking application  also determines the coordinates of the new bounding box, for example, as discussed above in connection with steps -. In step , the system stores the coordinates of the centroid in the (x, y) coordinates of the bounding box in memory. The system checks in step  to determine if the last frame has been processed. If not, the system loops back to step  and processes the next frame by repeating steps  to . As mentioned above, the frame data is extracted from the video content and utilized to define the frames within a segment. Thus, this process may be repeated for all the frames identified in the first frame found and last frame found fields in the developmental graphical user interface . Alternatively, the video linking application can be configured to process more frames than those found within segment. However, by breaking down the processing in terms of segments, tracking of the pixel objects will be relatively more accurate because of the differences in the color variable values expected during segment changes.","Linked Video Files","In order to further optimize the image processing of the video linking application , the resource computing platform  may process all or part of the video frames and store the coordinates in step  (), Assuming the fastest possible human reaction time to be \u2153 of a second, it follows that an extraction rate of 10 frames per second will provide adequate tracking information Thus, the linked video files  store the centroid coordinates of the upper left and lower right coordinates of the selected objects within the \u2153 second intervals known as clusters. At 30FPS, a cluster is defined as a ten frame segment of video. The file information illustrating object movement contained within the ten frame segment is represented by the co-ordinates used (upper left, and lower right corners) to draw the object bounding boxes. Thus, ten frames of information are compressed into one. The number of frames per cluster depends on the frame rate. Using standard frame rate clusters are defined as follows:",{"@attributes":{"id":"p-0058","num":"0057"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"center"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Standard (FPS = frames\/second)","Frames\/Cluster"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"NTSC (29.97 FPS)","10"]},{"entry":[{},"30 FPS","10"]},{"entry":[{},"PAL (25 FPS)","8, 8, 9\/video section"]},{"entry":[{},"15 FPS","\u20025"]},{"entry":[{},"12 FPS","\u20024"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"Since the linked video files  are based on a sample rate of three (3) frames per second, the linked video files  will be usable at any playback rate of the original content. Moreover, by limiting the sample rate to three (3) frames per second, the linked video files  are suitable for narrowband transmission, for example, with a 56 K bit modem as well as broadband streaming applications, such as ISDN, DSL, cable and T1 applications.","Exemplary linked video files  are described and illustrated below.",{"@attributes":{"id":"p-0061","num":"0060"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Exemplary Linked Video File"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"49pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Line 1:","569","0","2172","30","0"]},{"entry":[{},"Line 2:","129","0","0","0","0"]},{"entry":[{},"Line 3:","001","001","010","4","132"]},{"entry":[{},{},"002","011","025","4","137"]},{"entry":[{},{},"003","026","040","4","142"]},{"entry":[{},{},"004","041","055","4","147"]},{"entry":[{},{},"005","056","070","4","152"]},{"entry":[{},{},"."]},{"entry":[{},{},"."]},{"entry":[{},{},"."]},{"entry":[{},{},"128","2136","2150","2","564"]},{"entry":[{},"Line 131:","129","2151","2172","2","567"]},{"entry":[{},"Line 132:","001","001","010","4","132"]},{"entry":[{},{},"6","125","276","199","1"]},{"entry":[{},{},"138","75","179","119","2"]},{"entry":[{},{},"213","60","246","83","3"]},{"entry":[{},{},"207","92","241","117","4"]},{"entry":[{},"Line 137:","002","011","025","4","137"]},{"entry":[{},{},"9","123","278","199","1"]},{"entry":[{},{},"133","52","177","119","2"]},{"entry":[{},{},"212","56","250","83","3"]},{"entry":[{},{},"208","89","243","118","4"]},{"entry":[{},"Line 142:","003","026","040","4","142"]},{"entry":[{},"Line 1"]},{"entry":[{},"Line 1:","569","0","2172","30","0"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]}]}}]}}},"The first number in Line 1 (569) identifies the total number of lines in the linked video file  file. The next two numbers in Line 1 (0, 2172) are the first and last frame numbers for the movie clip associated with the linked video file . The next number in Line 1(30) indicates the playing of the movie clip in frames-per-second.",{"@attributes":{"id":"p-0063","num":"0062"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]},{"entry":[{},"Line 2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"56pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Line 2:","129","0","0","0","0"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]}]}}]}}},"Line 2 only uses the first space, and the number in this space indicates the total numbers of video frame \u201cclusters\u201d in the video content.",{"@attributes":{"id":"p-0065","num":"0064"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]},{"entry":[{},"Line 3"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Line 3:","001","001","010","4","132"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]}]}}]}}},"In this example, Lines 3-131 contain information on the one hundred twenty-nine (129) video clusters. Each such line follows a similar format. The first number, 001 in this example, is the cluster number. The next two numbers (001,010) are the starting and ending frames of the video segment. The next number (4) indicates that this video cluster has four clickable areas or objects within it. The final number (132) indicates the line of the linked video file  where a detailed description of the video cluster can be found.",{"@attributes":{"id":"p-0067","num":"0066"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]},{"entry":[{},"Line 132"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"49pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Line 132:","001","001","010","4","132"]},{"entry":[{},"Line 133:","6","125","276","199","1"]},{"entry":[{},{},"138","75","179","119","2"]},{"entry":[{},{},"213","60","246","83","3"]},{"entry":[{},{},"207","92","241","117","4"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]}]}}]}}},"In this example, the detailed descriptions of the video clusters begins on line 132 for video cluster #1. The first line repeats the general video cluster information from prior in the linked video file . Each of the following four lines provide information on a separate clickable area. The first four numbers are the (x,y) coordinates for the upper left corner and the lower right corner, respectively. In Line 133, for instance, (6, 125) are the (x,y) coordinates for the upper left corner and (276, 199) are the (x,y) coordinates for the lower right corner of that video cluster. The last number in the line (\u201c1\u201d in Line 133) is the \u201clink index\u201d. The \u201clink index\u201d links the pixel object coordinates with the data object coordinates from the product placement database  ().","Playback Mode of Operation","Playback of the video content and linked video files  is by way of a video hosting platform  and a playback platform . As shown in , the video hosting platform  is shown separate from the playback platform  and connected thereto by either a narrowband or wideband transmission link. The video hosting platform  can alternatively be located in the playback platform .","The video hosting platform  includes a video playback application  which supports a common media player API  for playback of the video content and provides resources for accessing the linked video files  to enable pixel objects to be selected with a standard pointing device, such as a mouse, and linked to one or more data objects.","In particular, the video playback application  reads the linked data files  and stores these files in two arrays. The first array may be single dimensional and may contain information about the video content and in particular the segments. The second array may be used to provide information regarding the location of the pixel objects of clickable areas for each movie segments. Exemplary code for storing the linked data files into a first array and a second array is provided in an Appendix.","The video playback application  supports pixel objects within the video content  being selected with a standard pointing device, such as a mouse. The (x, y) coordinates of the location selected by the pointing device and the frame time that the location was selected are captured and compared with information in the linked video files  to determine whether the selected location corresponds to a selected pixel object. In particular, the (x, y) coordinates and frame time information of the pointing device are compared to the pixel object file to determine if the selected location in the display area  corresponds to a pixel object. This is determined by determining the elapsed time since the beginning of the video content, and multiplying the elapsed time by the playback rate. For example, for a mouse click at 2.5 seconds into the playback, the approximate frame will be 30 based on a 12 frame per second playback rate. Once the approximate frame is determined, the video playback application  determines the section from the section break application . Upon determining the section, all clickable areas in the section are scanned to determine the clickable area or pixel object that contains the x, y coordinates associated with the mouse click, if any. If so, the system displays the data object that has been linked to the pixel object by way of the link index in the object file in an display window . In particular, a data object, which is linked to the pixel object, is displayed. Exemplary code for returning a link index is provided in the Appendix.","The video playback application  may also be used to support a graphical user interface  as illustrated in . The graphical user interface  may include a display area  for displaying the video content by way of a common media player API.","The playback application  may also provide for additional capability. For example, the graphical user interface  may be provided with buttons for categorizing the various data objects that have been linked to the video content. As shown, in , the graphical user interface  may include categorical buttons, such as the entertainment, commerce and education buttons ,  and  to display the data objects in each of the exemplary categories. These category titles may be customized for each program, and are dynamically written to reflect the content of the program being shown. In this configuration, the data object files are configured with such categorical information. As such, when one of the categorical buttons is selected, all of the selected links in that category are retrieved from the product placement database  () and displayed in the inventory display window .","The \u201cPause on Click\u201d and \u201cClick and Connect\u201d buttons or check boxes  and  may be provided. The \u201cPause on Click\u201d button , once selected, pauses the video content in the display area  to allow a user time to decide whether to connect to an alternate resource platform at the present time after a selected pixel object has been linked to a data object or retrieve the data object from the inventory display window  at a later time. The \u201cClick and Connect\u201d button  may be used to display the link data object, for example, a web page in the background on the display area of the playback platform  outside of the graphical user interface , while the video content continues to display in the display window .","Additional functionality, such as \u201cShow All Links in a Frame\u201d and \u201cShow All Links in Program\u201d buttons  and  may also be provided. The \u201cShow All Links in Frame\u201d button  displays all links in a given frame in the inventory display window  when selected. This function allows a user to scroll through the access content, for example, by way of a scroll buttons  to locate the scene or frame in which the desired item appears. Once the frame has been located, the user can click within the displayed frame and all of the available items contained within the display frame are sorted and displayed in the inventory display window . The \u201cShow All Links\u201d button , when selected, displays all of the data object links to the video content. The data objects are displayed in the inventory display window .","A resume video button  may be provided. The resume video button  is used after selecting an object from the inventory display window . In particular, as mentioned above, objects selected from the inventory display window  link the application to another resource platform, such as a website. As mentioned above, in a click and connect mode of operation the video content continues to be displayed in the display window  in the player while the web page is displayed in the background on the display of the playback platform . The resume video button  simply allows playback of the video from the point at which the video playback application  was linked to another resource platform.","\u201cHide\/Show List\u201d, \u201cLogin\u201d, \u201cClear List\u201d and \u201cOpen Link\u201d buttons , ,  and  may also be provided. The \u201cHide\/Show List\u201d button  may be used to hide or show the functions of the graphical user interface . In particular, when the high\/show list button  is selected, an on\/off state is toggled and stored in memory.","The Login button  may be used to prevent or limit access by playback platform . The login capability may be used to capture valuable data about the user's habit and requested information. In this application, a web server (not shown) may be used to host a database of user information and password information commonly known in the industry. When the Login button  is selected, a request is sent from the playback platform  to a login web server for authentication. An authentication message may then return to the video hosting platform  to enable playback of the linked video content.","The Clear List button  may be provided to delete all of the data objects in the inventory display window . When the Clear List button  is selected the playback application  deletes all of the data objects in a temporary memory used for the inventory display window .","An Open Link button  allows for additional information for selected data objects to be accessed. In particular, once a data object is selected from the inventory display window , selection of the open link button  may be used to provide any additional information available for the selected data object.","A \u201cthumbnail\u201d display area  may be provided to display an image of a selected pixel object in a reduced size, or the frame from which it was selected, or another representational image or advertisement. The video playback application  may also support a chat room dialog box . The chat room dialog box  may be implemented using standard conventional software and provided with additional functionality. For example, images for an object listing within the inventory display area  may be dragged into the chat area dialog box . In response to such action, the video playback application  displays the clickable text space hyperlink in the chat dialog box . Clicking on the hyperlink functions transmits the thumbnail to the address generated.","Obviously, many modifications and variations of the present invention are possible in light of the above teachings. Thus, it is to be understood that, within the scope of the appended claims, the invention may be practiced otherwise than as specifically described above.","What is claimed and desired to be covered by a Letters Patent is as follows:",{"@attributes":{"id":"p-0085","num":"0084"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"Exemplary Code for Reading Data into First Array"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"numberOfLine = readFirstNumberOfFirstLine( );"},{"entry":"startFrame = readNextNumber ( );"},{"entry":"endFrame = readNextNumber ( );"},{"entry":"trueFramePerSecond = readNextNumber ( );"},{"entry":"numberOfMovieSegment = readFirstNumberOfSecondLine ( );"},{"entry":"for (int i=0; i<numberOfMovieSegments; i++) {"},{"entry":"\u2003\u2003firstArray [i*5] = readNextNumber ( );"},{"entry":"\u2003\u2003firstArray [i*5+1] = readNextNumber ( );"},{"entry":"\u2003\u2003firstArray [i*5+2] = readNextNumber ( );"},{"entry":"\u2003\u2003firstArray [i*5+3] = readNextNumber ( );"},{"entry":"\u2003\u2003firstArray [i*5+4] = readNextNumber ( );"},{"entry":"\u2003\u2003numberOfClickableAreas ="},{"entry":"\u2003\u2003\u2003\u2003calculateTheSumOfClickableAreas"},{"entry":"\u2003\u2003\u2003\u2003(firstArray [i*5+3]);"},{"entry":"}"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"Exemplary Code for Reading Data into Second Array"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"for (int i=0; i<numberOfClickableAreas; i++) {"},{"entry":"\u2003\u2003readLine ( );"},{"entry":"\u2003\u2003secondArray [i*5] = readNextNumber ( );"},{"entry":"\u2003\u2003secondArray [i*5+1] = readNextNumber ( );"},{"entry":"\u2003\u2003secondArray [i*5+2] = readNextNumber ( );"},{"entry":"\u2003\u2003secondArray [i*5+3] = readNextNumber ( );"},{"entry":"\u2003\u2003secondArray [i*5+4] = readNextNumber ( );"},{"entry":"}"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"Exemplary Code for Returning a Link Index"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"int getLinkIndex(int x, int y, in frameNumber) {"},{"entry":"\u2003approximatedFrameNumber = frameNumber *"},{"entry":"\u2003trueFramePerSecond \/ 12;"},{"entry":"\u2003segmentNumber = getSegmentNumber (approximateFrameNumber);"},{"entry":"\u2003numberOfClickableAreas = firstArray[segmentNumber*5 + 3];"},{"entry":"\u2003segmentStart = firstArray[segmentNumber*5 + 4]"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"- numberOfSegments \u2212 3;"]},{"entry":[{},"\/\/ 3 is the offset needed due to extra lines"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\u2003for (int i=0; i < numberOf ClickableAreas; i++) {"},{"entry":"\u2003\u2003x0 = secondArray[ (segmentStart + i)*5];"},{"entry":"\u2003\u2003y0 = secondArray[ (segmentStart + i)*5 + 1];"},{"entry":"\u2003\u2003x2 = secondArray[ (segmentStart + i)*5 + 2];"},{"entry":"\u2003\u2003xy2 = secondArray[ (segmentStart + i)*5 + 3];"},{"entry":"\u2003\u2003if (x0 <= x && x <= x2 && y0 <= y && y <= y2) {"},{"entry":"\u2003\u2003\u2003return secondArray [(segmentStart + i)*5 + 4];"},{"entry":"\u2003\u2003\u2003}"},{"entry":"\u2003}"},{"entry":"\u2003return \u22121;"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF THE DRAWINGS","p":["These and other advantages of the present invention will be readily understood with reference to the following specification and attached drawing wherein:",{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIGS. 6A and 6B"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
