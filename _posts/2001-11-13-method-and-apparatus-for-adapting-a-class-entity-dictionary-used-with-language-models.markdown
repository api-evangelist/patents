---
title: Method and apparatus for adapting a class entity dictionary used with language models
abstract: A method and apparatus are provided for augmenting a language model with a class entity dictionary based on corrections made by a user. Under the method and apparatus, a user corrects an output that is based in part on the language model by replacing an output segment with a correct segment. The correct segment is added to a class of segments in the class entity dictionary and a probability of the correct segment given the class is estimated based on an n-gram probability associated with the output segment and an n-gram probability associated with the class. This estimated probability is then used to generate further outputs.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07124080&OS=07124080&RS=07124080
owner: Microsoft Corporation
number: 07124080
owner_city: Redmond
owner_country: US
publication_date: 20011113
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS"],"p":["The present invention relates to language models. In particular, the present invention relates to adapting language models based on user input.","Language models provide a measure of the likelihood of a series of words appearing in a string of text. Such models are used in speech recognition, Chinese word segmentation, and phonetic-to-character conversion, such as pinyin-to-hanzi conversion in Chinese, to identifying a most likely sequence of words given a lattice of possible sequences. For example, in speech recognition, a language model would identify the phrase \u201cgo to bed\u201d as being more likely than the phonetically similar phrase \u201cgo too bed\u201d.","Typically, language models are trained on a corpus of sentences. Although such corpora are effective for training language models to handle general words, they are not very effective for training language models to handle proper nouns such as the names of people and businesses. The reason for this is that proper names do not occur with enough frequency in a corpus to be accurately modeled.","Some systems allow users to correct mistakes made by the language model. However, even after a system knows about the correction, there is no way for the system to adjust the language model based on the correction because there is no way to assess the probability of the word sequence formed by the correction. Because of this, the system will generally make the same mistake later when it encounters the same input.","Thus, a system is needed that allows a language model and a dynamic dictionary to be modified based on corrections made by a user.","A method and apparatus are provided for augmenting a language model with a class entity dictionary based on corrections made by a user. Under the method and apparatus, a user corrects an output that is based in part on the language model by replacing an output segment with a correct segment. The correct segment is added to a class of segments in the class entity dictionary and a probability of the correct segment given the class is estimated based on an n-gram probability associated with the output segment and an n-gram probability associated with the class. This estimated probability is then used to generate further outputs.",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1","b":["100","100","100","100"]},"The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems, environments, and\/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, telephony systems, distributed computing environments that include any of the above systems or devices, and the like.","The invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit . The system bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way of example, and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard , a microphone , and a pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  may operate in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 2","b":["200","200","202","204","206","208","210"]},"Memory  is implemented as non-volatile electronic memory such as random access memory (RAM) with a battery back-up module (not shown) such that information stored in memory  is not lost when the general power to mobile device  is shut down. A portion of memory  is preferably allocated as addressable memory for program execution, while another portion of memory  is preferably used for storage, such as to simulate storage on a disk drive.","Memory  includes an operating system , application programs  as well as an object store . During operation, operating system  is preferably executed by processor  from memory . Operating system , in one preferred embodiment, is a WINDOWS\u00ae CE brand operating system commercially available from Microsoft Corporation. Operating system  is preferably designed for mobile devices, and implements database features that can be utilized by applications  through a set of exposed application programming interfaces and methods. The objects in object store  are maintained by applications  and operating system , at least partially in response to calls to the exposed application programming interfaces and methods.","Communication interface  represents numerous devices and technologies that allow mobile device  to send and receive information. The devices include wired and wireless modems, satellite receivers and broadcast tuners to name a few. Mobile device  can also be directly connected to a computer to exchange data therewith. In such cases, communication interface  can be an infrared transceiver or a serial or parallel communication connection, all of which are capable of transmitting streaming information.","Input\/output components  include a variety of input devices such as a touch-sensitive screen, buttons, rollers, and a microphone as well as a variety of output devices including an audio generator, a vibrating device, and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition, other input\/output devices may be attached to or found with mobile device  within the scope of the present invention.","The present invention provides a means for using and adapting a statistical language model and a class-based dictionary in various applications. A statistical language model provides the likelihood that a sequence of words will appear in a language. In general, an n-gram language model defines the probability of a sequence of words as:\n\n()=()*()* . . . *()*. . .*()\u2003\u2003EQ. 1\n\nwhere H is a sequence of words w,w, . . . , w, t is the number of word in the sequence, n\u22121 is the number of past words that are used to predict the next word, and Pr(w|w, . . . ,w) is the probability of the ith word given the n\u22121 preceding words. Thus in a bigram language model, n=2, and in a trigram language model n=3.\n","One problem with statistical language models is that they do not provide accurate probabilities for unknown or rarely used words such as proper nouns. To overcome this, the present invention utilizes a class-based language model.","In the class-based language model of the present invention, the model predicts the probability of sequences of classes of words and individual words. To do this, equation 1 is modified when a class is used in place of one or more words. For example, for a trigram language model, the probabilities calculated in connection with a class token N are:\n\n()= . . . *()*()*()*()*()*. . .\u2003\u2003EQ. 2\n\nwhere only those probabilities near the class probabilities are shown in equation 2 for simplicity, Nis the class at the ith position in the sequence, Tis an entity in class N, and Pr(T|N) is referred to as an inside probability that provides the probability of entity T given class N. Under one embodiment, the inside probability is provided by a class entity dictionary that defines the words found in each class. Under one embodiment, the class entity dictionary provides a set of grammar rules that define known words that are found in particular classes. For example, a context-free grammar for the class [NAME] may include the rule \u201cHank Hanson\u201d. The outside probabilities (the probability of a class given previous words) is provided by the class-based language model.\n","A class-based language model helps to overcome the sparseness problem associated with certain classes of words such as proper nouns. Such words appear so infrequently in the training data that a language model that does not use classes will always prefer more common words over the infrequently used words. By using classes, the language model of the present invention increases the likelihood of a class being identified since a class of words occurs more frequently in the training data than an individual word of the class.","Before a class-based language model or class entity dictionary can be used, they must be trained. Under one embodiment, the class-based language model and the class entity dictionary are initially trained by first tagging a training corpus to identify words that fall within classes based on a set of heuristics. The heuristics provide a set of rules that predict the location of a class of words based on other words in the input. For example, if the verb \u201ccall\u201d is a possible word in the input, the heuristic rules may indicate that the next word or next two words after \u201ccall\u201d should be considered part of the [NAME] class. (For example, \u201cCall Jack Jones\u201d).","The words that are identified using the heuristics are replaced with their class and the class-based language model is then trained using standard training techniques on the words and classes in the corpus.","The class entity dictionary is initially trained by dividing the words identified for each class into sub-components. These sub-components are then used in a standard n-gram training technique to identify probabilities for the words given the class. Such probabilities form the inside probabilities for the class.","An additional aspect of the present invention provides for updating and expanding the class entity dictionary and the class-based language model based on input provided by the user. In particular, the class entity dictionary is expanded when a user changes a decoded sequence of words so that the modified sequence includes a word that is not in the class entity dictionary. For example, if the class-based language model and class entity dictionary decode the sequence of words \u201cwrite a letter to Phil\u201d and the user changes the sequence to \u201cwrite a letter to Bill\u201d, \u201cBill\u201d will be added to the class-entity dictionary if it was not previously in the class-entity dictionary.","In order to add an entity to the class-entity dictionary, an inside probability for the entity must determined. As noted above, the inside probability provides the probability of an entity given a class. This probability cannot be calculated exactly because there is not enough data to establish the likelihood of the entity given the class. Instead, under embodiments of the present invention, this inside probability is estimated by assuming that, at a minimum, the product of the inside probability for the entity and the language model probability for the class should be equal to the language model probability for the word that was incorrectly identified by the decoder.","In terms of an equation for a trigram language model, this assumption reads as:\n\n()*()=()\u2003\u2003EQ. 3\n\nwhere Pr(T|N) is the inside probability of the modified entity Tgiven the class N, Pr(N|w,w) is the language model probability for class Ngiven the two preceding words in the sequence, and Pr(p|w,w) is the language model probability for the incorrect entity pthat was decoded and later modified to form the modified entity T.\n","Using this assumption, the inside probability is then estimated as:",{"@attributes":{"id":"p-0041","num":"0040"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["T","i"]},{"mi":["N","i"]}],"mo":"\u2758"}}},"mo":"=","mfrac":{"mrow":[{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["p","i"]},{"mi":"w","mrow":{"mi":"i","mo":"-","mn":"2"}}],"mo":"\u2758"},"mo":",","msub":{"mi":"w","mrow":{"mi":"i","mo":"-","mn":"1"}}}}},{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["N","i"]},{"mi":"w","mrow":{"mi":"i","mo":"-","mn":"2"}}],"mo":"\u2758"},"mo":",","msub":{"mi":"w","mrow":{"mi":"i","mo":"-","mn":"1"}}}}}]}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"4"}}]}}}}},"However, this estimate is highly dependent on the preceding words in the sequence. To lower this dependence and thus make the estimate more general, the probability is re-written as:",{"@attributes":{"id":"p-0043","num":"0042"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"Pr","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["T","i"]},{"mi":["N","i"]}],"mo":"\u2758"}}},"mo":"=","mfrac":{"mrow":[{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["p","i"]},"mo":"\u2758","mrow":{"mstyle":{"mtext":"<"},"mo":["\u2062","\u2062"],"mi":"unknown","msub":{"mstyle":{"mtext":">"},"mrow":{"mi":"i","mo":"-","mn":"2"}}}},{"mstyle":{"mtext":"<"},"mo":["\u2062","\u2062"],"mi":"unknown","msub":{"mstyle":{"mtext":">"},"mrow":{"mi":"i","mo":"-","mn":"1"}}}],"mo":","}}},{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["N","i"]},"mo":"\u2758","mrow":{"mstyle":{"mtext":"<"},"mo":["\u2062","\u2062"],"mi":"unknown","msub":{"mstyle":{"mtext":">"},"mrow":{"mi":"i","mo":"-","mn":"2"}}}},{"mstyle":{"mtext":"<"},"mo":["\u2062","\u2062"],"mi":"unknown","msub":{"mstyle":{"mtext":">"},"mrow":{"mi":"i","mo":"-","mn":"1"}}}],"mo":","}}}]}}},{"mrow":{"mi":"EQ","mo":[".","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"5"}}]}}}},"br":{},"sub":["i","i\u22122","i\u22121","i ","i","i\u22122","i\u22121","i ","i","i\u22122","i\u22121","i","i\u22122","i\u22121","i ","i "]},"Once the probability has been estimated for the modified entity, the modified entity and the estimated inside probability are added to the class entity dictionary under the appropriate class.","User modifications to the decoded sequence of words do not always involve words that were not present in the class-entity dictionary. Instead, either the original decoded word or the modified word may have been present in the class-entity dictionary.  provides a flow diagram of the steps used to determine how to alter the class entity dictionary based on user modifications.","As an overview, the process of  can adjust the inside probabilities in three ways. For a modified word that was already in the class entity dictionary, the fact that the word was not decoded indicates that its inside probability is too low. As such, its probability must be increased. For a decoded word that is in the class entity dictionary, the fact that the user modified the word indicates that the decoded word's inside probability is too high. As such, its probability must be decreased. For a modified word that is not in the dictionary, the modified word must be added to the dictionary and its initial probability calculated using Equation 5 above.","To determine which adjustment to make, the process of  begins at step  where the sequence of words produced by the user modification is examined to determine if the modified word is in the class entity dictionary. If the modified word is in the class entity dictionary, a determination is made as to whether the modified words are found in only a single class at step .","If the modified words are found in more than one class, the class-based language model is used to select the most likely class by using each of the possible classes in a separate sequence and identifying the sequence that provides the highest likelihood. This is shown as step  in .","If the modified words are only found in a single class in step  or after a single class has been identified at step , the inside probability for the modified characters needs to be adjusted because even though the modified words were in the class entity dictionary, the decoder did not identify them from the input because their inside probability was too low. To correct this, the inside probability stored in the class entity dictionary for the modified characters is increased at step . Under some embodiments, the inside probability is increased by multiplying it by a factor of 1.5.","If the modified characters are not in the class entity dictionary at step , a set of heuristics is used at step  to determine possible classes for the modified characters. Each of these classes is then used to build a separate sequence or words with the other decoded words. The class-based language model is then used to identify the most likely sequence and thus the most-likely class for the modified word.","If a class can be identified for the modified word at step , an inside probability for the modified word is determined using equation 5 above at step  and the modified word and probability are added to the class entity dictionary at step .","If a class cannot be identified for the modified word at step , the word that was decoded and modified by the user is examined at step  to determine if the decoded word is in the class entity dictionary. If the decoded word is in the dictionary at step , the fact that the decoded word was identified instead of the modified word means that the inside probability for the decoded word is set too high. To correct this, the inside probability for the decoded words is decreased at step . Under many embodiments, the inside probability is reduced by a factor of 1.5. (In other words, the inside probability is divided by 1.5 to form the new probability).","If the decoded word is not in the class entity dictionary at step , no changes need to be made to the class entity dictionary since neither the decoded nor the modified word falls within a class. As such, the class entity dictionary is left unchanged at step .","The class-based language model and the method of updating a class-based language model under the present invention may be used in many systems. For example,  provides a block diagram of a phonetic-to-character conversion system  that can be implemented in the environments of  and that utilizes an embodiment of the present invention. The operation of this system is shown in the flow diagram of .","At step  of , phonetic input , which is the phonetic description of characters found in a character-based language such as Chinese, Japanese, or Korean, is provided to a decoder . In Chinese, one embodiment of the phonetic input is pinyin input. At step , decoder  first builds a lattice of possible words that can be represented by the phonetic input using a lexicon . The lattice is then expanded at step  by identifying class entities from the words in the lattice using class entity dictionary  and heuristic rules . The identified classes are added as separate nodes in the lattice.","At step , decoder  determines a probability for each path through the lattice using a phonetic model , which provides the probability that each word along the path will represent a phonetic segment, the class entity dictionary, which provides the inside probability for the classes, a language model , which provides the probability of a sequence of words and\/or classes occurring in a language and equation 2 above. The sequence of words along the path that provides the highest probability is then output as the decoded string of words at step .","After the decoded sequence has been provided to the user, the system can receive user modifications  at step . This modification indicates the correct words that the user intended by their input. At step , this user modification is examined to determine how it should be used to alter the class entity dictionary using the process of . In particular, class extraction unit  uses heuristics  and class entity dictionary  to identify a class for the modified word and to determine if the decoded word or the modified word is in the class entity dictionary. A probability determination unit  then calculates a probability for the modified word if it was not present in the dictionary or determines a new probability for the modified word or the decoded word to improve the performance of the decoder as indicated above in .","In a second embodiment, the class-based language model of the present invention is used in a speech recognition system such as the speech recognition system of . In , an input speech signal from a speaker  and additive noise  are converted into an electrical signal by a microphone , which is connected to an analog-to-digital (A-to-D) converter .","A-to-D converter  converts the analog signal from microphone  into a series of digital values. In several embodiments, A-to-D converter  samples the analog signal at 16 kHz and 16 bits per sample, thereby creating 32 kilobytes of speech data per second.","The digital data created by A-to-D converter  is provided to an optional noise reduction module , which removes some of the noise in the digital signal using one or more noise reduction techniques.","The output of noise reduction module  is provided to a feature extractor , which extracts a feature from the digital speech signal. Examples of feature extraction modules include modules for performing Linear Predictive Coding (LPC), LPC derived cepstrum, Perceptive Linear Prediction (PLP), Auditory model feature extraction, and Mel-Frequency Cepstrum Coefficients (MFCC) feature extraction. Note that the invention is not limited to these feature extraction modules and that other modules may be used within the context of the present invention.","The feature extraction module receives the stream of digital values from noise reduction module  and produces a stream of feature vectors that are each associated with a frame of the speech signal. In many embodiments, the centers of the frames are separated by 10 milliseconds.","Note that although noise reduction module  is shown before feature extractor  in the embodiment of , in other embodiments, noise reduction module  appears after feature extractor .","The stream of feature vectors produced by the extraction module is provided to a decoder , which identifies a most likely sequence of words based on the stream of feature vectors, a lexicon , a language model , an acoustic model , heuristic rules  and a class entity dictionary .","Acoustic model  provides a probability that an input feature vector was created by the pronunciation of a linguistic unit such as a senone, phoneme, diphone, or triphone.","Language model , class entity dictionary  and heuristic rules  are used by decoder  in a manner similar to the way decoder  uses language model , class entity dictionary , and heuristic rules .","Based on the acoustic model, the language model, the lexicon, the class entity dictionary, and the heuristic rules, decoder  identifies a most likely sequence of words from all possible word sequences. In particular, decoder  uses steps , , , , and  of  to identify the most likely word sequence.","The most probable word sequence is then subjected to possible user modification . If the user modifies words in the decoded sequence, the modified words are provided to a class extraction unit  and a probability determination unit , which operate in a manner similar to class extraction  and probability determination unit  of . Using the process of , the class entity dictionary  is then modified based on the user modifications of the decoded words.","Although the present invention has been described with reference to particular embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
