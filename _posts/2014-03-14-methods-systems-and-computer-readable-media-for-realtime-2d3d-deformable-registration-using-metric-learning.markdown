---
title: Methods, systems, and computer readable media for real-time 2D/3D deformable registration using metric learning
abstract: Methods, systems, and computer readable media for real-time 2D/3D deformable registration using metric learning are disclosed. According to one aspect, a method for real-time 2D/3D deformable registration using metric learning includes creating a catalog of simulated 2D projection images based on a reference 3D image and a shape space of 3D deformations, where each entry in the catalog is created by: applying to the reference 3D image a set of deformation parameters from the shape space of deformations; simulating a 2D projection of the result; associating the simulated 2D projection image with the deformation parameters used to create the simulated 2D projection image; and storing the simulated 2D projection image and associated deformation parameters in the catalog. The method also includes receiving a 2D image, and, in response to receiving the 2D image: calculating a value of distance between the received 2D image and a simulated 2D projection image for each of the simulated 2D projection images in the catalog; using the calculated distances to calculate weighting factors to be applied to the deformation parameters of each of the simulated 2D projection images in the catalog; and calculating deformation parameters for the received 2D image based on the weighted deformation parameters in the catalog. The calculated deformation parameters are then used to deform a 3D volume of interest to produce a 3D volume that represents the 3D layout of the tissue at the time that the received 2D image was acquired.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09495746&OS=09495746&RS=09495746
owner: THE UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL
number: 09495746
owner_city: Chapel Hill
owner_country: US
publication_date: 20140314
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["PRIORITY CLAIM","GOVERNMENT INTEREST","TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","Deformation Modeling at Planning Time","Metric Learning at Planning Time","Results","CONCLUSION"],"p":["This application claims the benefit of U.S. Provisional Patent Application Ser. No. 61\/789,000, filed Mar. 15, 2013; the disclosure of which is incorporated herein by reference in its entirety.","This invention was made with government support under Grant No. CA128510 awarded by the National Institutes of Health. The government has certain rights in the invention.","The subject matter described herein relates to methods and systems for registration of 2D projection images to 3D volume images for use during real-time image-guided radiation therapy (IGRT) or any other real-time image-guided remote therapy. More particularly, the subject matter described herein relates to methods, systems, and computer readable media for real-time 2D\/3D deformable registration using metric learning.","Tumor localization in 3D is the main goal of IGRT. Another important goal is the localization of non-cancerous objects at risk to radiation. They are usually accomplished by computing the patient's treatment-time 3D deformations based on an on-board imaging system, which is usually an x-ray based system. The treatment-time 3D deformations can be computed by doing image registration between the treatment-time reconstructed 3D image and the treatment-planning 3D image (3D\/3D registration) or between the treatment-time on-board 2D projection images and the treatment-planning 3D image (2D\/3D registration).","Recent advances of the IGRT registration methods emphasize real-time computation and low-dose image acquisition. Russakoff et al. [1,2], Khamene et al. [3], Munbodh et al. [4], Li et al. [5,6] rejected the time-consuming 3D\/3D registration and performed 2D\/3D registration by optimizing similarity functions defined in the projection domain. Other than the optimization-based methods, Chou et al. [7,8] recently introduced a faster and low-dose 2D\/3D image registration by using a linear operator that approximates the deformation parameters. However, all of the above registration methods involve computationally demanding production of Digitally-Reconstructed Radiographs (DRRs) in each registration iteration (e.g., 15 ms on a modern GPU to produce a 256\u00d7256 DRR from a 256\u00d7256\u00d7256 volume [9]), which makes them difficult to be extended to support real-time (>30 fps) image registration.","Accordingly, there exists a need for real-time 2D\/3D deformable registration that is fast, accurate, and robust. More specifically, there exists a need for metric-learning enabling real-time 2D\/3D deformable registration.","According to one aspect, the subject matter described herein includes a method for real-time 2D\/3D deformable registration using metric learning. The method includes creating a catalogue of simulated 2D projection images based on a reference 3D image and a shape space of 3D deformations, where each entry in the catalogue is created by applying to the reference 3D image a set of deformation parameters from the shape space of deformations, simulating a 2D projection of the result; associating the simulated 2D projection image with the deformation parameters used to create the simulated 2D projection image, and storing the simulated 2D projection image and associated deformation parameters in the catalogue. The method also includes receiving an acquired 2D image, and, in response to receiving the 2D image, calculating a value of distance between the acquired 2D image and a simulated 2D projection image for each of the simulated 2D projection images in the catalogue, using the calculated distances to calculate weighting factors to be applied to the deformation parameters of each of the simulated 2D projection images in the catalogue, and calculating 3D deformation parameters inferred from the acquired 2D image based on the weighted 3D deformation parameters in the catalogue. The calculated deformation parameters are then used to deform a 3D volume of interest to produce a 3D volume that represents the 3D layout of the tissue at the time that the received 2D image was acquired.","The method also includes two approaches that learn the distance metrics used in computing the distances between 2D images. The first approach uses linear regressions to learn the relationship between Euclidean inter-projection-image distance metrics and the respective 3D deformation parameter values. The second approach uses a leave-one-out strategy that optimizes, over Riemannian metrics, the accuracy of the prediction of the 3D deformation parameter values of the left-out projection images.","According to another aspect, the subject matter described herein includes a system for real-time 2D\/3D deformable registration using metric learning. The system includes a data store for storing a catalogue of simulated 2D projection images that were created based on a reference 3D image and a shape space of 3D deformations, wherein each entry in the catalogue was computed by applying to the reference 3D image a set of deformation parameters from the shape space of deformations, simulating a 2D projection of the result; associating the simulated 2D projection image with the deformation parameters used to create the simulated 2D projection image, and storing the simulated 2D projection image and associated deformation parameters in the catalogue. The system also includes a hardware module for receiving an acquired 2D image, and, a software module that, in response to receiving the 2D image, calculates a value of distance between the acquired 2D image and a simulated 2D projection image for each of the simulated 2D projection images in the catalogue, uses the calculated distances to calculate weighting factors to be applied to the deformation parameters of each of the simulated 2D projection images in the catalogue, calculates 3D deformation parameters inferred from the acquired 2D image based on the weighted deformation parameters in the catalogue, and uses the calculated deformation parameters to deform a 3D volume of interest to produce a 3D volume that represents the 3D layout of the tissue at the time that the 2D image was acquired. The system also includes two software modules that learn the distance metrics used in computing the distances between 2D images. The first module uses linear regressions to learn the relation between Euclidean inter-projection-image distance metrics and respective 3D deformation parameter values. The second module uses a leave-one-out strategy that optimizes, over Riemannian metrics, the accuracy of the prediction of the 3D deformation parameter values of the left-out projection images.","The subject matter described herein can be implemented in software in combination with hardware and\/or firmware. For example, the subject matter described herein can be implemented in software executed by a processor. In one exemplary implementation, the subject matter described herein can be implemented using a non-transitory computer readable medium having stored thereon computer executable instructions that when executed by the processor of a computer control the computer to perform steps. Exemplary computer readable media suitable for implementing the subject matter described herein include non-transitory computer-readable media, such as disk memory devices, chip memory devices, programmable logic devices, and application specific integrated circuits. In addition, a computer readable medium that implements the subject matter described herein may be located on a single device or computing platform or may be distributed across multiple devices or computing platforms.","In accordance with the subject matter disclosed herein, systems, methods, and computer readable media for real-time 2D\/3D deformable registration using metric learning are provided.","We present a novel real-time 2D\/3D registration method, called Registration Efficiency and Accuracy through Learning Metric on Shape (REALMS), that does not require DRR production in the registration. It calculates the patient's treatment-time 3D deformations by kernel regression. Specifically, each of the patient's deformation parameters is interpolated using a weighting Gaussian kernel on that parameter's training case values. In each training case, its parameter value is associated with a corresponding training projection image. The Gaussian kernel is formed from distances between training projection images. This distance for the parameter in question involves a Riemannian metric on projection image differences. At planning time, REALMS learns the parameter-specific metrics from the set of training projection images using a Leave-One-Out (LOO) training.","To the best of our knowledge, REALMS is the first 2D\/3D deformable registration method that achieves real-time (>30 fps) performance. REALMS uses the metric learning idea firstly introduced in Weinberger and Tesauro [10] to tackle the 2D\/3D image registration problem. Particularly, in order to make the metric learning work for the high dimensional (D>>10) projection space, REALMS uses a specially-designed initialization approximated by linear regression. The results have led to substantial error reduction when the special initialization is applied.","Given a 3D reference image, such as a computed tomography (CT) image, and a scale space of deformations, the REALMS (Registration Efficiency and Accuracy through Learning Metric on Shape) method described herein interpolatively and very quickly (a few milliseconds on a standard desktop computer) calculates a deformation in the scale space from a 2D projection image, e.g., a radiograph, consistent with the reference image. It is designed to improve radiation treatment of cancer via image-guided radiotherapy.","The subject matter described herein uses a scale space consisting of a mean deformation and a limited number of modes of variation, such that a deformation in the scale space is formed as the sum of the mean and linear combinations of the modes with mode weights given by the n-tuple  (formed by weights c, c, . . . c).","The subject matter described herein involves computing n 2D \u201cinterest images\u201d An capture distance values \u03c3, and a catalogue of simulated 2D projection images I, each computed by applying the deformation determined by an associated vector to the reference image and calculating a projection of the result. Then, given a new 2D image I, for each coefficient cthe method calculates the weight wassociated with the icomponent of as follows, and interpolates each coefficient cas \u03a3w\/\u03a3W. The weight wis computed as exp[\u22120.5 distance(I,I)\/\u03c3] e.g., as a decreasing function of the distance of I from I, with that distance computed by the Euclidean magnitude of the dot product A(I-I).","Each interest image Ais computed as the regression matrix linearly predicting cfrom the images J\u2212I, where J ranges over all of the images Iand k ranges over all images in the catalogue but the j. The capture distance \u03c3is selected to yield a minimum average error magnitude in the over all images in the catalogue. Acan also be optimized using an objective function made from some norm of the REALMS prediction accuracies of the catalogue values.","A multiscale version with successive locality in either or both spatial region of interest and coefficient cregion of interest can increase the accuracy of the method, at the expense of some of its speed.","Reference will now be made in detail to exemplary embodiments of the present invention, examples of which are illustrated in the accompanying drawings. Wherever possible, the same reference numbers will be used throughout the drawings to refer to the same or like parts.",{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 1"},"Step  includes creating a catalogue of simulated 2D projection images based on a reference 3D image and a shape space of deformations. In one embodiment, the reference 3D image is a 3D image of a tissue to be treated, usually taken before treatment time, during a planning stage. The process of creating the catalogue will be described in more detail in , below.","Step  includes learning the inter-projection-image distance metrics that measure weighting factors on the 3D deformation parameters using the catalogue. In one embodiment, the learned projection metrics may be learned by performing linear regressions to learn the relation between Euclidean inter-projection-image distance metrics and respective 3D deformation parameter values. In another embodiment, the learned projection metrics may be learned by performing a leave-one-out analysis, over Riemannian metrics, the accuracy of the prediction of the 3D deformation parameter values of the left-out projection images.","At step , a 2D image is received, which may be a 2D image of a tissue being treated, acquired at treatment time.","At step , a distance value is calculated between the received 2D image and each of the simulated 2D projection images in the catalogue. In one embodiment, the distance value is calculated as a difference between the received 2D image and a 2D image from the catalogue, calculated on a pixel-by-pixel basis. In one embodiment, the distance value equals the sum of the differences between the intensity value of a pixel on one image and the intensity value of the corresponding pixel on the other image. In alternative embodiments, other distance functions may be used, including comparing groups of pixels, where the comparison may be weighted or un-weighted, and so on. In one embodiment, a distance value is calculated for each image in the catalogue. Alternatively, a distance value may be calculated for a select subset of images in the catalogue.","At step , the calculated distances are used to calculate weighting factors to be applied to the deformation parameters for each of the simulated 2D projection images in the catalogue, and at step , the weighted deformation parameters for each of the 2D images in the catalogue are used to calculate 3D deformation parameters inferred from the received 2D image. In this manner, the deformation parameters for catalogue images that are more similar to the received image are given more weight than is given to the deformation parameters for catalogue images that are less similar to the received image. In one embodiment, a weighting function is used to map distance to a weighting coefficient to be applied to the deformation parameters of the catalogue image. In one embodiment, a separate weighting function may be used for each separate parameter. In one embodiment, the weighting function is determined based on analysis of the 2D projection images in the catalogue, such as by a leave-one-out analysis or regression analysis.","At step , the equivalent deformation parameters that were calculated for the received 2D image are applied to deform a 3D volume of interest, such as a 3D volume of a tissue being treated (or a 3D model of such tissue), to produce a 3D volume that represents the 3D layout (e.g., shape and location) of the tissue at the time that the received 2D image was acquired. If this method is used to provide real-time 2D\/3D deformable registration during treatment time, then at step , the 3D image volume produced by step  is used to inform or adjust treatment during treatment time. For example, the real-time 2D\/3D registration allows accurate calculation of the target or actual location within the tissue that is being treated, as well as the target or actual dose, dose accumulation, or dose exposure for the tissue being treated.","Because the methods of 2D\/3D registration described herein do not require using a 3D volume to generate, at treatment time, a set of 2D simulated projection images that are then compared to the 2D images received at treatment time, as is done by conventional methods, the registration methods described herein are very fast and can perform registration and 3D volume generation at more than 30 frames per second on currently available hardware.",{"@attributes":{"id":"p-0038","num":"0037"},"figref":["FIG. 2","FIG. 2"]},"Step  includes applying to the reference 3D image a set of deformation parameters from the shape space of deformations. For example, if it has been determined, via principal component analysis (PCA) or some other method, that the shape space of deformations of the tissue in question can be characterized using only four deformation parameters P1, P2, P3, and P4, and that each parameter has its own range of values, then the shape space of deformations comprises a four-dimensional space, in which case N different 4-tuples of parameter values may be generated. In step , the set of deformation parameters would comprise one of the N different 4-tuples. Application of the particular 4-tuple deforms the 3D volume according to the particular values of the deformation parameters in the 4-tuple.","At step , a 2D projection of the deformed 3D image is simulated. When the catalogue of images is being generated for the purpose of providing real-time 2D\/3D registration during treatment time, the 2D projections are likely to be generated at a projection angle that is essentially the same as the expected angle from which the treatment time 2D images will be taken. However, 2D projection images of the deformed 3D image may be generated using other projection angles, or from multiple projection angles.","At step , the simulated 2D projection image is associated with the deformation parameters used to create the simulated 2D projection image.","At step , the simulated 2D projection image and its associated deformation parameters are stored in the catalogue.",{"@attributes":{"id":"p-0043","num":"0042"},"figref":["FIG. 3","FIG. 3","FIG. 3","FIG. 3","FIGS. 1 and 2"],"b":["300","302","304","304","300","306","308","310","312","308"]},"The operation of REALMS will now be illustrated graphically in .",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 4A","FIG. 4A","FIG. 4A","FIG. 4A","FIG. 4A","FIGS. 4B through 4D"],"b":["400","402","400","400","404","406","408","400","402","402"]},"In , a target radiograph  is taken during treatment. Its location within projection space  is represented by the square containing a star , which is located at an arbitrary location within projection space . REALMS attempts to determine the location of target radiograph  within parameter space , represented by the circle containing a star, referred to herein as starred circle . REALMS achieves this via interpolation at treatment time.","In , a projection space distance metric, d, is used to determine a difference between target radiograph  and training DRRs, represented as the distance between the starred box representing target radiograph  and the other boxes within projection space . An interpolation weight function  (d,\u03c3) is a function of projection distance to the training DRR, shown as the distance between the starred circle  and the other circles within parameter space . The measurement of distance in projection space  and the calculation of distance in parameter space  is repeated for multiple training DRRs, with the results shown in .","In , the position of target radiograph  within parameter space  is established via multiple distance measurements within projection space  and interpolations of distance into parameter space . The set of parameter values represented by the location of starred circle  within parameter space  may then be used to deform a model of the structure of interest\u2014in this example a lung\u2014and present a reconstructed 3D image of the structure to a surgeon or other use during treatment time. In one embodiment, the 3D image may be presented as an augmented reality view of that deformed model to a surgeon or other user during treatment time.","This novel approach of using a distance metric in projection space to map to a location in parameter space and then using those parameters to deform a model of the structure in question allows registration to occur very quickly and accurately, with a mean target registration error of only 2.56\u00b11.11 mm at 10.89\u00b10.26 ms, or about 92 frames per second. As will be described in more detail below, metric tensor M and kernel width \u03c3 learning may be optimized via leave one out training.","The REALMS method will now be described in more detail. In this section, we describe REALMS's 2D\/3D registration framework. REALMS uses kernel regression (eq. 1) to interpolate the patient's n 3D deformation parameters c=(c, c, . . . , c) separately from the on-board projection image \u03a8(\u03b8) where \u03b8 is the projection angle. It uses a Gaussian kernel Kwith the width \u03c3and a metric tensor Mon projection intensity differences to interpolate the patient's ideformation parameter cfrom a set of N training projection images {P(I\u2218T(c); \u03b8)|k=1, 2, . . . , N} simulated at planning time. Specifically, the training projection image, P(I\u2218T(c); \u03b8), is the DRR of a 3D image deformed from the patient's planning-time 3D mean image | with sampled deformation parameters c=(c, c, . . . , c). T and P are the warping and the DRR operators, respectively. P simulates the DRRs according to the treatment-time imaging geometry, e.g., the projection angle \u03b8.","In the treatment-time registration, each deformation parameter ci in c can be estimated with the following kernel regression:",{"@attributes":{"id":"p-0052","num":"0051"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"msup":{"mi":["c","i"]},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":["c","k","i"]},"mo":"\u00b7","mrow":{"msub":{"mi":"K","mrow":{"msup":[{"mi":["M","i"]},{"mi":["\u03c3","i"]}],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"\u03a8","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b8"}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","k"]}}}},"mo":";","mi":"\u03b8"}}}],"mo":","}}}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"K","mrow":{"msup":[{"mi":["M","i"]},{"mi":["\u03c3","i"]}],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"\u03a8","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b8"}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","k"]}}}},"mo":";","mi":"\u03b8"}}}],"mo":","}}}}]}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"1"}}]},{"mtd":[{"mrow":{"msub":{"mi":"K","mrow":{"msup":[{"mi":["M","i"]},{"mi":["\u03c3","i"]}],"mo":","}},"mo":"(","mrow":{"mrow":[{"mi":"\u03a8","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b8"}},{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","k"]}}}},"mo":";","mi":"\u03b8"}}},{"mfrac":{"mn":"1","mrow":{"msqrt":{"mrow":{"mn":"2","mo":"\u2062","mi":"\u03c0"}},"mo":"\u2062","msup":{"mi":["\u03c3","i"]}}},"mo":"\u2062","msup":{"mi":"\u2147","mrow":{"mo":"-","mfrac":{"mrow":{"msubsup":{"mi":"d","msup":{"mi":["M","i"]},"mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"\u03a8","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b8"}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","k"]}}}},"mo":";","mi":"\u03b8"}}}],"mo":","}}},"msup":{"mrow":{"mi":"s","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["\u03c3","i"]}}},"mn":"2"}}}}}],"mo":"="}],"mo":[",",","]}}},{"mrow":{"mo":["(",")"],"mn":"2"}}]},{"mtd":[{"mrow":{"mrow":{"mrow":[{"msubsup":{"mi":"d","msup":{"mi":["M","i"]},"mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"\u03a8","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b8"}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","k"]}}}},"mo":";","mi":"\u03b8"}}}],"mo":","}}},{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"\u03a8","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b8"}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","k"]}}}},"mo":";","mi":"\u03b8"}}}],"mo":"-"}},"mi":"T"},"mo":"\u2062","mrow":{"msup":{"mi":["M","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"\u03a8","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b8"}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","k"]}}}},"mo":";","mi":"\u03b8"}}}],"mo":"-"}}}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}]}}}},"where Kis a Gaussian kernel (kernel width=\u03c3) that uses a Riemannian metric Min the squared distance dand gives the weights for the parameter interpolation in the regression. The minus signs in eq. 3 denote pixel-by-pixel intensity subtraction. We now describe how REALMS parameterizes the deformation space at planning time.","REALMS limits the deformation to a shape space. It models deformations as a linear combination of a set of basis deformations calculated through PCA analysis. In our target problem\u2014lung IGRT, a set of respiratory-correlated CTs (RCCTs, dimension: 512\u00d7512\u00d7120) {J|\u03c4=1, 2, 10} are available at planning, time. From these a mean image I= and a set of deformations \u03c6between Jand  can be computed. The basis deformations can then be chosen to be the primary eigenmodes of a PCA analysis on the \u03c6.","Deformation Shape Space and Mean Image Generation. REALMS computes a respiratory Fr\u00e9chet mean image  from the RCCT dataset via an LDDMM (Large Deformation Diffeomorphic Metric Mapping) framework described in Lorenzen et al. [11]. The Fr\u00e9chet mean , as well as the diffeomorphic deformations \u03c6 from the mean  to each image J, are computed using a fluid-flow distance metric:",{"@attributes":{"id":"p-0056","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mover":{"mi":["J","_"]},"mo":"=","mrow":{"mrow":[{"munder":{"mi":["argmin","J"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"\u03c4","mo":"=","mn":"1"},"mn":"10"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mo":"\u222b","mn":["0","1"]},"mo":"\u2062","mrow":{"msub":{"mo":"\u222b","mi":"\u03a9"},"mo":"\u2062","mrow":{"msup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":{"mi":"v","mrow":{"mi":["\u03c4","\u03b3"],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},"mn":"2"},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}},{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}}],"mrow":[{"mo":"\u2146","mi":"x"},{"mo":"\u2146","mi":"\u03b3"}]}}}}},{"mfrac":{"mn":"1","msup":{"mi":"s","mn":"2"}},"mo":"\u2062","mrow":{"msub":{"mo":"\u222b","mi":"\u03a9"},"mo":"\u2062","mrow":{"msup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mrow":[{"mi":"J","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["\u03d5","\u03c4"],"mrow":{"mo":"-","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}}},{"msub":{"mi":["J","\u03c4"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}],"mo":"-"}},"mn":"2"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}},"mrow":{"mo":"\u2146","mi":"x"}}}}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}}},"where J(x) is the intensity of the pixel at position x in the image J, \u03bdis the fluid-flow velocity field for the image Jin flow time \u03b3, s is the weighting variable on the image dissimilarity, and \u03c6(x) describes the deformation at the pixel location x: \u03c6(x)=x+\u222b\u03bd","Statistical Analysis.","With the diffeomorphic deformation set {\u03c6|\u03c4=1,2, . . . , 10} calculated, our method finds a set of linear deformation basis vectors \u03c6by PCA analysis. The scores \u03bbon each \u03c6yield \u03c6in terms of these basis vectors.\n\n\u03c6=+\u03a3\u03bb\u00b7\u03c6\u2003\u2003(5)\n\nWe choose a subset of n eigenmodes that captures more than 95% of the total variation. Then we let the n scores form the n-dimensional parametrization c.\n\n=()=\u03bb,\u03bb, . . . \u03bb)\u2003\u2003(6)\n\nFor most of our target problems, n=3 satisfies the requirement. We now describe how REALMS learns the metric tensor Mand decides the kernel width \u03c3.\n","Metric Learning and Kernel Width Selection. REALMS learns a metric tensor Mwith a corresponding kernel width \u03c3for the patient's ideformation parameter cusing a LOO training strategy. At planning time, it samples a set of N deformation parameter tuples {c=(c, C, . . . , c)|k=1, 2, . . . , N} to generate training projection images {P(I\u2218T(c); \u03b8)|k=1, 2, . . . , N} where their associated deformation parameters are sampled uniformly within three standard deviations of the scores \u03bb observed in the RCCTs. For each deformation parameter cin c, REALMS finds the best pair of the metric tensor Mand the kernel width \u03c3that minimizes the sum of squared LOO regression residuals  among the set of N training projection images:",{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"msup":{"mi":"M","mrow":{"mi":["i","\u2020"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mo":[",",","],"mrow":{"msup":{"mi":"\u03c3","mrow":{"mi":["i","\u2020"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mo":"=","mrow":{"munder":{"mi":"argmin","mrow":{"msup":[{"mi":["M","i"]},{"mi":["\u03c3","i"]}],"mo":","}},"mo":"\u2062","mrow":{"msub":{"mi":"\u2112","msup":{"mi":["c","i"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":["M","i"]},{"mi":["\u03c3","i"]}],"mo":","}}}}}}},{"mrow":{"mo":["(",")"],"mn":"7"}}]},{"mtd":[{"mrow":{"mrow":{"mrow":[{"msub":{"mi":"\u2112","msup":{"mi":["c","i"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":["M","i"]},{"mi":["\u03c3","i"]}],"mo":","}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["c","k","i"]},"mo":"-","mrow":{"msubsup":{"mover":{"mi":"c","mo":"^"},"mi":["k","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":["M","i"]},{"mi":["\u03c3","i"]}],"mo":","}}}}},"mn":"2"}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"8"}}]},{"mtd":[{"mrow":{"mrow":{"mrow":{"msubsup":{"mover":{"mi":"c","mo":"^"},"mi":["k","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":["M","i"]},{"mi":["\u03c3","i"]}],"mo":","}}},"mo":"=","mfrac":{"mrow":[{"munder":{"mo":"\u2211","mrow":{"mi":["x","k"],"mo":"\u2260"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msubsup":{"mi":["c","k","i"]},"mo":"\u00b7","mrow":{"msub":{"mi":"K","mrow":{"msup":[{"mi":["M","i"]},{"mi":["\u03c3","i"]}],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","k"]}}}},"mo":";","mi":"\u03b8"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","x"]}}}},"mo":";","mi":"\u03b8"}}}],"mo":","}}}}},{"munder":{"mo":"\u2211","mrow":{"mi":["x","k"],"mo":"\u2260"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"K","mrow":{"msup":[{"mi":["M","i"]},{"mi":["\u03c3","i"]}],"mo":","}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","k"]}}}},"mo":";","mi":"\u03b8"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","x"]}}}},"mo":";","mi":"\u03b8"}}}],"mo":","}}}}]}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}]}}},"br":{},"sub":["k","k"],"sup":["i","i","i","i ","i ","i ","i ","i "],"img":{"@attributes":{"id":"CUSTOM-CHARACTER-00003","he":"2.79mm","wi":"3.89mm","file":"US09495746-20161115-P00003.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}},"To avoid high-dimensional optimization over the constrained matrix M, we structure the metric tensor Mas a rank-1 matrix formed by a basis vector \u03b1:M=\u03b1\u03b1. Therefore, we can transform eq. 7 into a optimization over the unit vector \u03b1where \u2225\u03b1\u2225=1:",{"@attributes":{"id":"p-0063","num":"0062"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":"a","mrow":{"mi":["i","\u2020"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mo":[",",","],"mrow":{"msup":{"mi":"\u03c3","mrow":{"mi":["i","\u2020"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mo":"=","mrow":{"munder":{"mi":"argmin","mrow":{"msup":[{"mi":["a","i"]},{"mi":["\u03c3","i"]}],"mo":","}},"mo":"\u2062","mrow":{"msub":{"mi":"\u2112","msup":{"mi":["c","i"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msup":[{"mi":["a","i"]},{"mi":["a","iT"]}],"mo":"\u2062"},"mo":",","msup":{"mi":["\u03c3","i"]}}}}}}}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}}},"Then we can rewrite the squared distance d=dused in the Gaussian kernel Kas follows:\n\n((();\u03b8),(();\u03b8))=(\u03b1\u00b7)(\u03b1\u00b7),\u2003\u2003(11)\n\n=(();\u03b8)\u2212(();\u03b8),\u2003\u2003(12)\n","where  is a vector of intensity differences between projection images generated by parameters  and ; and \u03b1is a metric basis vector where the magnitude of the inner product of \u03b1and the intensity difference vector , \u03b1\u00b7 gives the Riemannian distance for the parameter c(eq. 11).","The learned metric basis vector \u03b1and the selected kernel width \u03c3form a weighting kernel Kto interpolate the parameter cin the registration (see eq. 1).","Linear-Regression Implied Initial Metric.","Since the residual functional \u03b6 (see eq. 7) that we want to minimize is non-convex, a good initial guess of the metric basis vector \u03b1 is essential. Therefore, REALMS uses a vector was an initial guess of the metric basis vector \u03b1for the parameter c. Let w=(ww. . . w) list these initial guesses. The matrix W is approximated by a multivariate linear regression (eq. 13 and eq. 14) between the projection difference matrix R=(rr. . . r)and the parameter differences matrix \u0394C. In particular, the projection difference vector r=P(I\u2218T(c); \u03b8)\u2212P(I; \u03b8) is the intensity differences between the DRRs calculated from the deformed image I\u2218T(c) and the DRRs calculated from the mean image I (where c=0).",{"@attributes":{"id":"p-0069","num":"0068"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":["\u0394","C"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mrow":[{"mrow":{"mo":["(",")"],"mtable":{"mtr":[{"mtd":[{"msubsup":{"mi":"c","mn":["1","1"]}},{"msubsup":{"mi":"c","mn":["1","2"]}},{"mi":"\u2026"},{"msubsup":{"mi":["c","n"],"mn":"1"}}]},{"mtd":[{"msubsup":{"mi":"c","mn":["2","1"]}},{"msubsup":{"mi":"c","mn":["2","2"]}},{"mi":"\u2026"},{"msubsup":{"mi":["c","n"],"mn":"2"}}]},{"mtd":[{"mi":"\u22ee"},{"mi":"\u22ee"},{"mi":"\u22f1"},{"mi":"\u22ee"}]},{"mtd":[{"msubsup":{"mi":["c","N"],"mn":"1"}},{"msubsup":{"mi":["c","N"],"mn":"2"}},{"mi":"\u2026"},{"msubsup":{"mi":["c","N","n"]}}]}]}},"mo":"-","mn":"0"},{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"msubsup":{"mi":["r","T"],"mn":"1"}}},{"mtd":{"msubsup":{"mi":["r","T"],"mn":"2"}}},{"mtd":{"mi":"\u22ee"}},{"mtd":{"msubsup":{"mi":["r","N"],"mrow":{"mi":"T","mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}}]}},{"mo":["(",")"],"mrow":{"msup":[{"mi":"w","mn":"1"},{"mi":"w","mn":"2"},{"mi":["w","n"]}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"\u2026"}}],"mo":"\u00b7"}],"mo":"\u2248"}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"13"}}]},{"mtd":[{"mrow":{"mrow":{"mi":"W","mo":"=","mrow":{"msup":[{"mrow":[{"mo":["(",")"],"mrow":{"msup":{"mi":["R","T"]},"mo":"\u2062","mi":"R"}},{"mo":"-","mn":"1"}]},{"mi":["R","T"]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mi":["\u0394","C"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"14"}}]}]}}}},"The inner product of the matrix W, calculated by the pseudo-inverse in eq. 14, and the projection intensity difference matrix R, WR, gives the best linear approximation of the parameter differences \u0394C. Therefore, we use was the initial guess of the metric basis vector \u03b1for the parameter c.","Optimization Scheme.","REALMS uses a two-step scheme to optimize the metric basis vector \u03b1and the kernel width \u03c3in eq. 10.","First, for each candidate kernel width \u03c3, it optimizes the metric basis vector \u03b1using the quasi-Newton method (specifically, the BFGS method) with the vector was the initialization. The gradient of the function Lwith respect to \u03b1can be stated as",{"@attributes":{"id":"p-0074","num":"0073"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":"\u2202","msub":{"mi":"\u2112","msup":{"mi":["c","i"]}}},{"mo":"\u2202","msup":{"mi":["a","i"]}}]},"mo":"=","mrow":{"mfrac":{"mroot":{"mn":["2","2"]},"msup":{"mi":["\u03c3","i"]}},"mo":["\u2062","\u2062"],"msup":{"mi":["a","i"]},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"\ud835\udca6","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mover":{"mi":"c","mo":"^"},"mi":["\ud835\udca6","i"]},{"mi":["c","\ud835\udca6","i"]}],"mo":"-"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"\ud835\udcb3","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"msubsup":[{"mover":{"mi":"c","mo":"^"},"mi":["\ud835\udcb3","i"]},{"mi":["c","\ud835\udcb3","i"]}],"mo":"-"}},{"msub":{"mi":"K","mrow":{"mrow":{"msup":[{"mi":["a","i"]},{"mi":["a","iT"]}],"mo":"\u2062"},"mo":",","msup":{"mi":["\u03c3","i"]}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","\ud835\udca6"]}}}},"mo":";","mi":"\u03b8"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"I","mo":"\u00b7","mrow":{"mi":"T","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["c","\ud835\udcb3"]}}}},"mo":";","mi":"\u03b8"}}}],"mo":","}}}],"mo":["\u2062","\u2062","\u2062"],"msub":{"mi":"r","mrow":{"mo":["(",")"],"mrow":{"mi":["\ud835\udca6","\ud835\udcb3"],"mo":","}}},"msubsup":{"mi":["r","T"],"mrow":{"mi":["\ud835\udca6","\ud835\udcb3"],"mo":","}}}}],"mo":"\u2062"}}}}},{"mrow":{"mo":["(",")"],"mn":"15"}}]}}}}},"Second, REALMS selects a kernel width \u03c3among the candidate kernel widths where its learned metric basis vector \u03b1yields minimum LOO regression residuals \u03b6for parameter c.","Projection Normalization.","To account for variations caused by x-ray scatter that produces inconsistent projection intensities, REALMS normalizes both the training projection images P(I\u2218T(); \u03b8) and the on-board projection image \u03a8(\u03b8). In particular, it uses a localized Gaussian normalization that has shown promise in removing the undesired scattering artifacts.","Synthetic Tests.","We used coronal DRRs (dimension: 64\u00d748) of the target CTs as synthetic on-board cone-beam projection images. The target CTs were deformed from the patient's Fr\u00e9chet mean CT by normally distributed random samples of the first three deformation parameters. In our lung datasets, the first three deformation parameters captured more than 95% of lung variation observed in their RCCTs. We generated 600 synthetic test cases from 6 lung datasets and measured the registration quality by the average mTRE (mean Target Registration Error) over all cases and all voxels at tumor sites.","With REALMS's registrations, the average mTRE and its standard deviation are down from 6.89\u00b13.53 mm to 0.34\u00b10.24 mm using N=125 training projection images. The computation time for each registration is 11.39\u00b10.73 ms (87.79 fps) on Intel Corel Quad CPU Q6700. As shown in , REALMS reduces the minimum errors produced by kernel regressions that use the Euclidean metric (M=I).  shows the average mTRE over 600 test cases projected onto the (a) first, (b) second, and (c) third deformation basis vector versus the candidate kernel widths using N=125 training projection images.","Tradeoff of Time Versus Accuracy.",{"@attributes":{"id":"p-0082","num":"0081"},"figref":"FIG. 6"},"We tested REALMS on 6 lung datasets with an on-board CBCT system where a single coronal on-board CB projection (dimension downsampled to 64\u00d748 for efficient computation) at both EE (End-Expiration) and EI (End-Inspiration) phases were used for the testing. For each dataset, we generated N=125 training DRRs to learn the metrics and select optimal interpolation kernel widths. The learned metrics and the selected kernel widths were used to estimate deformation parameters for the testing EE and EI on-board projections. The estimated CTs were deformed from Fr\u00e9chet mean CT with the estimated deformation parameters. The results were validated with reconstructed CBCTs at target phases.","Table 1 shows the 3D tumor centroid differences (TCDs) between REALMS-estimated CTs and the reconstructed CBCTs at the same respiratory phases. Tumor centroids were computed via Snake active segmentations. As shown in table 1, REALMS reduces the TCD from 5.58\u00b13.14 mm to 2.56\u00b11.11 mm in 10.89\u00b10.26 ms (91.82 fps). Numbers inside the parentheses are the initial TCDs.",{"@attributes":{"id":"p-0085","num":"0084"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"77pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}},{"entry":["dataset#","TCD at EE phase (mm)","TCD at EI phase (mm)","Time (ms)"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["1","2.42 (9.70)","4.06 (7.45)","10.40"]},{"entry":["2","3.60 (4.85)","3.60 (4.89)","10.92"]},{"entry":["3","2.30 (8.71)","3.60 (4.03)","10.91"]},{"entry":["4","1.27 (2.69)","2.80 (2.29)","10.91"]},{"entry":["5","0.70 (9.89)","3.28 (8.71)","11.15"]},{"entry":["6","1.98 (2.03)","1.12 (1.72)","11.08"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}}}},{"@attributes":{"id":"p-0086","num":"0085"},"figref":["FIG. 7","FIG. 7"]},"The Learned Metric Basis Vector.","As shown in , the learned metric basis vector \u03b1will emphasize projection pixels that are significant for the distance calculation of the deformation parameter c(e.g. give high positive or high negative values). Results showed that the learned metric basis vector \u03b1emphasized the diaphragm locations and the lung boundaries as its corresponding deformation basis vector \u03c6covers the expansion and contraction motion of the lung. In , (a) shows the initial guess of the metric basis vector \u03b1=w(top) and the optimized metric basis vector \u03b1(bottom) of a lung dataset. They are reshaped into projection image domain for visualization. As can be seen in , the diaphragm locations and the lung boundaries (white boxes) were emphasized after metric learning. In , (b) top shows a coronal on-board CB projection at EE phase of the lung dataset used in . The white boxes in (a) and (b) correspond to the same 2D locations. In , (b) bottom shows the first deformation basis vector \u03c6(the arrows indicate heat maps of the deformation magnitudes) overlaid with the volume rendering of the Fr\u00e9chet mean CT of the lung dataset used in . For this dataset, \u03c6covers the expansion and contraction motion of the lung.","The subject matter described herein presents an accurate and real-time 2D\/3D registration method, REALMS, that estimates 3D deformation parameters from a single projection image using kernel regressions with learned rank-1 projection distance metrics. The learned distance metrics that are optimized with an initialization approximated by linear regression results in success for this high dimensional metric learning, and avoids convergence to local minima and the wrong distance metrics that would result. With this initialization, the regression estimation on both synthetic and real test cases are well suited for real-time and low-dose IGRT by using a single projection image.","The disclosure of each of the following references is incorporated herein by reference in its entirety:\n\n","It will be understood that various details of the subject matter described herein may be changed without departing from the scope of the subject matter described herein. Furthermore, the foregoing description is for the purpose of illustration only, and not for the purpose of limitation."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Preferred embodiments of the subject matter described herein will now be explained with reference to the accompanying drawings, wherein like reference numerals represent like parts, of which:",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIGS. 4A through 4D"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
