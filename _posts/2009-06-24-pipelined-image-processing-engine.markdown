---
title: Pipelined image processing engine
abstract: The present invention related to processing image frames through a pipeline of effects by breaking the image frames into multiple blocks of image data. The example method includes generating a plurality of blocks from each frame, processing each block through a pipeline of effects in a predefined consecutive order, and aggregating the processed blocks to produce an output frame by combining the primary pixels from each processed block. The pipeline of effects may be distributed over a plurality of processing nodes, and each effect may process a block, provided as input to the node. Each processing node may independently process a block using an effect.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08754895&OS=08754895&RS=08754895
owner: Sony Electronics Inc.
number: 08754895
owner_city: Park Ridge
owner_country: US
publication_date: 20090624
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["This application is related to U.S. Provisional Application Ser. No. 61\/191,557 filed on Sep. 9, 2008 the entire contents of which are hereby incorporated by reference.","1. Field of the Invention","This invention relates generally to a Pipelined Image Processing Engine and more particularly to a system and method for processing a plurality of image frames through an effect pipeline on a multi-core processing system.","2. Description of the Related Art","As manufacturing technology continues to improve, the physical limits of semiconductor-based microelectronics expand to meet the demand for more capable microprocessors. Limitations in processing speed have led to adaptations that leverage existing processor speeds and capabilities through parallelism by employing multi-core or distributed architectures to distribute workloads and thereby increase processing capacity and reduce processing time.","Applications in the film\/video\/imaging space often chain together multiple image processing effects in a pipelined fashion. These image processing pipelines can benefit from parallelism offered through multi-core or distributed architectures. Applications performing such image processing share common characteristics, including the need to both process and move vast amounts of data at real-time rates.","Conventional implementations of such systems suffer from a combination of problems, ranging from poor throughput, high latency, and increased complexity, which reduces extensibility and scalability. These limitations are compounded when chaining together multiple, discrete effects which depend on consecutive execution, as in a pipelined system.","When processing video, an \u2018effect pipeline\u2019 refers to the application of visual effects in a defined order to an image. Similarly, an \u2018effect\u2019 refers to each stage of the effect pipeline.","Prior approaches to distributing hardware resources among the various effects in effect pipelines generally suffer from a number of limitations with respect to parallelism.","One approach defines each image frame of data as the minimum quantum of work, allowing each effect to operate on a given frame independently or other frames. While this approach allows multiple effects to coexist in a shared system without enforcing tight integration amongst them, this approach also results in the high latency of the overall system. This latency correlates with the number of effects in the effect pipeline.","\u2018Pipeline performance\u2019 measures performance as a combination of latency and computation time. \u2018Latency\u2019 refers to the time required by the pipelined system to emit a given unit of data. With respect to an effect pipeline, \u2018latency\u2019 describes the time spent by each image frame in the pipeline, from the moment it enters the first effect in the pipeline, to the time when it exits the last effect in the pipeline.","\u2018Computation time\u2019 refers to the time required to process a standard unit of data, e.g., an image frame. Furthermore, computation time may be represented as a function of frame rate for a video system, or a function of actual time to process a frame.",{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 11","FIG. 11"]},"At time t, frame  enters the pipeline, and the first effect is applied to frame  by processor . At time t, frame  is loaded and processed by processor , and frame  is loaded and processed by processor . At time t, frame  is loaded and processed by processor , frame  is loaded and processed by processor , and frame  is loaded and processed by processor . From time t to t, frames  to  are introduced into the pipeline, and frames - proceed along the pipeline. At the conclusion of time t, frame  emerges from the pipeline.","Pipeline latency can be measured as a function of the time required to process a frame through all of the stages in the pipeline, or in \u2018frame time,\u2019 i.e. the number of frames processed by the first effect in the pipe before the frame N exits the effect pipeline. As a function of frame time, pipeline latency is computed as:\n\n\u2003\u2003(Eq. 1)\n\nwhere:\n\n","For a frame-based architecture, pipeline latency can also be defined as a product of the processing time for the slowest effect in the chain and the number of effects (or stages) in the pipeline. This is computed as:\n\n\u2003\u2003(Eq. 2)\n\nwhere:\n\n","By assuming that a new frame is fed into the pipeline every 120 ms, that the pipeline contains 6 stages or effects, and that the slowest effect in the pipeline also takes 120 ms to process one frame of data, the Pipeline Latency (PL) for the frame-based system in  is 6*120=960 ms.","Furthermore, since the pipeline latency is measured as a function of time from when the first frame, i.e., frame , enters and exits the pipeline, no benefit is incurred by having each processor run every effect consecutively in parallel. That is, even if every frame were immediately available for processing, instead of a new frame being fed consecutively into the pipeline every 120 ms, the pipeline would still have a 6 frame latency and a 960 ms computation time latency, based on the time from when the first frame entered and exited the pipeline.","The above system is limited by the hardware resources available, i.e. processing becomes considerably slower if the number of effects exceeds the number of available independent processing components, or if the cache memory available at each processor is incapable of storing an entire frame.","The present invention provides a Pipelined Image Processing Engine that implements a flexible and scalable pipeline that manages to adjust for limited hardware resources. The embodiment may also include both static and dynamic load balancing. The example embodiments remove or reduce many of the above restrictions while also offering a generic framework to execute effects in a more efficient manner.","An example embodiment of the present invention may include a method for processing image frames through a pipeline of effects. The method may include generating a plurality of blocks from each frame, processing each block through a pipeline of effects in a predefined consecutive order, and aggregating the processed blocks to produce an output frame by combining the pixels from each processed blocks. Each block may contain a group of primary pixels and a group of total pixels, the total pixels including any pixels required as input by an effect, from the pipeline of effects, to produce output for the primary pixels. The primary pixels in each block are the pixels that will be used for the final frame generated by the pipeline of effects.","The pipeline of effects may be distributed over a plurality of processing nodes, and each effect may process a block, provided as input to the node, to produce output for the primary pixels of the block. Each processing node may independently processes a block using an effect. The effects may be analyzed for scheduling purposes, in order to reduce latency between processing a first block in each frame and the output of a last block in each frame.","The effects may include either spatially or temporal effects. When processing spatial effects, the total pixels in a block may include a plurality of pixels from neighboring blocks. When processing temporal effects, the total pixels may include a plurality of pixels from temporally neighboring image frames.","Block processing and generation may employ different strategies, such as the reducing-extent or a fixed-extent strategies described below, but is not limited thereto and may include combinations and sub-combinations of different strategies. For example, different portions of the effects pipeline may operate using different strategies.","In one embodiment, generating the total pixels may include identifying any pixels required to serially process a block through a plurality of effects, from the pipeline of effects, to produce output for the primary pixels. This may include analyzing a first effect to determine the total pixels required to produce output for the primary pixels, and analyzing a second effect to determine the total pixels required to generate output for the total pixels required by the first effect to produce output for the primary pixels. The processing step may then reduce the number of total pixels in a block after processing the block through the second effect and before processing the first effect.","In another embodiment, the processing step may include updating the total pixels in a block with primary pixels from at least one neighboring block after processing a first effect and before processing a second effect.","Yet another example embodiment may be an apparatus for processing a chain of image frames through an effects pipeline. The apparatus may include a primary processor, a plurality of secondary processors, and a bus. The bus may interconnect the primary processor, the plurality of secondary processors, and a memory interface.","The primary processor may include a block generator, effect distributor, and block aggregator. The block generator may generate a plurality of blocks from an input image frame provided through the memory interface. The effect distributor may manage the distribution and processing order of the effects and the plurality of blocks among the plurality of secondary processors. Finally, the block aggregator may combine the processed blocks.","The plurality of secondary processors may each include a minimum memory cache to store the contents of a block from the block generator. The plurality of secondary processors may each process a block at a time through the pipeline of effects in a consecutive order. The pipeline of effects may be distributed over the plurality of secondary processors. Each secondary processor may execute an effect independently to produce output for the primary pixels for a given block.","In the following description, for purposes of explanation, numerous details are set forth, such as flowcharts, system configurations, and processing architectures in order to provide an understanding of one or more embodiments of the present invention. However, it is and will be apparent to one skilled in the art that these specific details are not required in order to practice the present invention.","The present invention provides a Pipelined Image Processing Engine that implements a flexible and scalable pipeline that adjusts for limited hardware resources. The embodiment reduces the minimum quotient of work processed by each \u2018effect\u2019 from a frame to a \u2018block\u2019. Each block represents a subset of an entire frame. Block-based processing breaks down each frame worth of data into N partially-overlapping or non-overlapping blocks, and allows each effect at any stage of the pipeline to process the block independently of other blocks. This allows multiple effects to work simultaneously without waiting for an entire frame or field to be available, and also does away with the need for complicated synchronization mechanisms. Each block is self-contained, and holds enough frame data for each effect to work on it independently. Because the blocks are self-contained and the various effects working on the various blocks are loosely coupled, the framework can easily assign more physical processors to slower effects in the pipeline, while allocating fewer physical processors to lightweight, simple effects. Furthermore, the processor assignments can change from one frame to the next, without any user intervention.","The present invention may be embodied in various forms, including business processes, computer implemented methods, computer program products, computer systems and networks, user interfaces, application programming interfaces, and the like.",{"@attributes":{"id":"p-0050","num":"0055"},"figref":"FIG. 1","b":"100"},"The multi-core processor  may include a pool of discrete single processor units (SPUs)  connected together by a very fast internal bus . The multi-core processor  may also include a parallel processing unit (PPU) , a memory interface , and an Input\/Output interface . PPU  may coordinate the distribution of processing tasks between SPUs . PPU  may also contain a local memory cache  for storing information and instruction relating to running processes or effects. Each SPU  may include a memory cache , which is used to store information related to a running process or effect allocated to that SPU .","During operation, input containing an effect pipeline and a plurality of image frames may be provided to the multi-core processor  via Input\/Output interface . The PPU  may parse the effect pipeline into a plurality of discrete effects. Each image frame may be analyzed in conjunction with the plurality of effects and separated into pixel blocks. The PPU  may then distribute the effects and the blocks among the SPUs  for processing. After the SPUs  process the blocks based on the effects, the PPU  may aggregate the processed blocks into complete output image frames.","While  illustrates an architecture having distinct PPU  and SPUs , one of ordinary skill will appreciate that the invention is not limited thereto, and may be implemented on a general purpose architecture containing uniform multi-core processors or other types of specialized architectures.",{"@attributes":{"id":"p-0054","num":"0059"},"figref":"FIG. 2","b":["200","200","204","210","200","202","208","202","204","202","204"]},"During operation, the effect pipeline and a plurality of image frames may be provided to the CMU  from terminal  or another source having access to network . The CMU  may parse the effect pipeline into discrete effects. Each image frame may be analyzed in conjunction with the effects and separated into pixel blocks. The CMU  may then distribute the effects and the pixel blocks among the DPUs  for processing.","Types of Effects","With respect to blocks of pixel data, an \u2018effect\u2019 is a process that changes or distorts a block of pixels. Each block is composed of a plurality of pixels, e.g., a rectangular or square grid of pixels. A block having M\u00d7N pixels could be processed in parallel all at once if M\u00d7N processing elements were simultaneously available. This is only possible if each input pixel provides all the necessary data for a given effect to produce a corresponding output pixel. However, a lot of effects rely on an input pixel and neighboring pixels to properly change or distort the input pixel into the output pixel.","Effects can be categorized into three types: point effects, area effects, and range effects. A \u2018point effect\u2019 is an effect which produces an output pixel based on just one input pixel. Color correcting effects fall in this category. An \u2018area effect\u2019 is an effect requiring a certain amount of neighboring pixels apart from a given input pixel to produce an output pixel. Convolution filters, such as blur effects, are area effects. Area effects offer some challenges for parallelism, especially relating to memory footprints and performance. A \u2018range effect\u2019 is an effect which needs a whole frame as input to produce a single output pixel. Histogram effects are commonly range effects, since they require an analysis of the entire frame prior to generating any output pixels. Range effects pose the largest challenge to parallelism.","While the above effects are described with respect to the spatial domain (i.e., they operate using a single frame of data) these effects may also operate in the temporal domain. Temporal domain effects may require data from temporally neighboring frames. For example, a temporal area effect may need neighboring frame input pixel data to produce the output pixel data in a single frame, and a temporal Range Effect may require all of the frames of a clip or sequence to produce the pixel data for a single output frame. By example, a Box Blur is a spatial area effect, whereas as a Motion Blur is a temporal area effect. Furthermore, effects may work in both domains. For example, Interlaced-to-Progressive Conversion is a spatial-temporal area effect, as it needs neighboring data in both the spatial (single-frame) domain and the temporal domain (neighboring frames).","Block Division","In order to reduce the quantum of work associated with a frame, each frame may be divide into smaller sub-frames, or \u2018blocks\u2019 of data.",{"@attributes":{"id":"p-0062","num":"0067"},"figref":"FIG. 3","b":["302","304","1","16","302","304","304","1","304","16","302","304","302"]},"The example embodiment may maintain the same dimension for all of the blocks in a given frame. This may provide an even distribution of work among all of the SPUs . However, the distribution of block sizes may follow a different scheme. For example, in the case where greater dependency exists among certain pixels in an effect, those pixels may be grouped into a single block to promote parallelism. Furthermore, a particular frame may be divided based on various criteria, such a color relationships, object identification, etc. Finally, the criteria for block sizes may also produce blocks having similar work quotients, such that the processing time for a given effect on different blocks remains roughly uniform.","Block division (or aggregation) by itself is not an inherently parallel task, and hence may be relegated to a single processor, e.g., PPU , that can access the results of the plurality of the SPUs  and may have access to greater available memory resources. Since the PPU  has the ability to access the entire frame at once, whereas the SPUs  may preferably operate on discrete blocks, the block division and aggregation processes may be relegated to the PPU . Furthermore, the interaction between the PPU  and SPUs  may preferably be minimized once the blocks  are offloaded to the SPUs . This promotes maximum computational throughput by leveraging the plurality of high-speed SPUs , while avoiding delays associated with cross-processes or communications.","Breaking up the frame into non-overlapping, unique blocks works well when dealing with point effects. However, with respect to area effects, the same non-overlapping blocks may still contribute to the output, but require extra pixel data around the edges. For example, a 5\u00d75 pixel block may be necessary to modify or output a 3\u00d73 pixel block .","With respect to blocks , the portion of the block  which may be modified by the effect is referred to as the \u2018primary data\u2019 . Additional data bordering the primary data , referred to as \u2018edge data\u2019  may be necessary to facilitate processing the area effect. The combination of the primary data  and edge data  is referred to as the \u2018total data\u2019 for a block . Similarly, the area covered by the primary data  is referred to as the \u2018inner extent\u2019 , while the area covered by the combination of the primary data  and the edge data  is referred to as the \u2018outer extent\u2019 .","The amount of edge data  required is dependent on the area effect. For example, a Convolution Effect may need a 1 pixel wide perimeter worth of edge data  for each block .  illustrates the block division of a 12\u00d712 pixel frame . In , the pixel frame  is divided into sixteen 3\u00d73 pixel blocks  (- to -). For example, while block - may have a 3\u00d73 pixel primary data  having inner extent , the total data encompasses a 5\u00d75 pixel region, including both primary data  and bordering pixel-wide edge data  having outer extent .","In this example, the edge data  of each block overlaps with the primary data  of at least one adjoining block . This edge data  may be included redundantly to make the blocks  as self-contained as possible with respect to the effect processing, in order to achieve maximum performance. Blocks  which lie at the edge of the frame  may include manufactured edge data  to ensure that they can be processed uniformly. For example, this edge data  may include copies of the frame's  border pixels, or be set to a fixed value.","In the example embodiment of , all of the edge data  of block - lies within the inner extent  of adjoining blocks ; as such, block - is considered to be an \u2018interior block\u2019. By contrast, two edges of block - coincide with the edges of the frame ; as such, a portion of the edge data  for block - will need to be manufactured as described above. Manufacturing the edge data  for edge blocks  at block creation time may avoid requiring individual effects to have special boundary condition checks, and avoid requiring the effects to distinguish between interior or edge blocks .","The size of the block  excluding any edge data  is also referred \u2018primary block size\u2019, because this is the only part of the block  which contributes to the final output image frame. The size of the block, inclusive of edge data, is referred to as the \u2018total block size\u2019.","In , the primary block size for each block  may be 9 pixels, while the total block size may be 25. Because these blocks are square, each side can be computed by just adding the pixels from the edge data  to the output block size.","While the disclosed block division is described with respect to spatial effects, it will be understood by one of ordinary skill, that the block division may also function in the temporal domain. When functioning in the temporal domain, the blocks  mainly include pixels from temporally neighboring frames, and the total block size may be either two or three dimensional, instead of being two dimensional as in the spatial domain.","While  illustrates a primary block size for each block  of 9 pixels and a total block size of 25, one of ordinary skill will appreciate that this implementation can be extended to non-square blocks by considering the edge requirements of each effect in the pipeline for each dimension independently. Furthermore, the implementation may also extend to multi-dimensional domains (where the blocks would not be two-dimensional, but multi-dimensional instead i.e., n-dimensional).","Block Edges in Pipelined Systems","In the context of an effect pipeline containing a plurality of effects, multiple area effects will each consecutively compound the edge requirements for the total block size. The following example embodiment will be described with respect to an effect pipeline including three area effects, each area effect requiring an edge of 1 pixel width. However, it will be understood that a given effect may require various edge pixel widths, which may or may not be uniformly distributed around the inner extent .","The primary data  in this example may include blocks  having an inner block size of 9 pixels and a total block size of 25 pixels. Adding the edge data  results in a block of size 25 pixels, which carries enough information for the first effect in the pipeline to produce output for the primary data . However, once the first effect processes the block , the next area effect in the pipeline may not have enough data to produce data for the primary data  using the remaining 3\u00d73 pixels of data, or alternatively the next effect may have to access unmodified edge data  which is inconsistent with the modified primary data . This is because the edge data  of the block  will still be the same as when the first effect received it while the primary data  will reflect the output of the first effect. To address the pipeline situation, it may be necessary to provide a larger total block size, since each successive area effect may only be capable of producing output for an incrementally smaller region.",{"@attributes":{"id":"p-0077","num":"0082"},"figref":"FIG. 5A","b":["406","406"]},"In the exemplified reducing-extent strategy, a pixel block is processed successively by three area effects. With this strategy, the outer extents of the block  shrink successively as the block  is processed by each effect. Within the context of the above example, the first effect processes 81 pixels, and outputs or modifies 49 pixels, the second effect processes 49 modified pixels and outputs or modifies 25 pixels, and the third effect processes the 25 modified pixels, and outputs or modifies 9 pixels within the inner extent .","In the first step, the primary block size is 9 pixels, while the total block size is 81 pixels. The first area effect is provided with a block  having outer extent -, which produces output for total data having outer extent -. The second area effect is provided with total data having outer extent -, which produces output for total data having outer extent -. The third area effect is then provided with a block  having outer extent -, and produces output for primary data .",{"@attributes":{"id":"p-0080","num":"0085"},"figref":["FIG. 5B","FIG. 5A"],"b":["408","402","408","404","406","408","1","408","2","506","408","3","408","3"]},"An alternate way of computing this is to simply add the edge dimensions required by each effect in the pipeline to the dimensions of the primary data . Given that the block's raw dimensions of the primary data  are 3\u00d73 pixels, and each effect needs a 1 pixel boundary (an additional 2 pixels in each dimension), the total dimensions are 9\u00d79.","The reducing-extent strategy benefits from a high degree of independence granted to each block  while traversing the pipeline. That is, once a block  enters the pipeline, it does not need to synchronize with any other adjacent blocks , nor is any controlling mechanism necessary to ensure data integrity until the data block  exits the pipeline. This also offers the maximum latitude in reducing pipeline latency.","While the reducing-extent strategy incurs an extra cost in the form of processing time and storage of the total data for all the effects in the pipeline, excluding the last stage, the cost and latency may be reduced by distinguishing between point and area effects and placing any point effects towards the end of the pipeline (after the last area effect).",{"@attributes":{"id":"p-0084","num":"0089"},"figref":"FIG. 6A","b":["304","304","304"]},{"@attributes":{"id":"p-0085","num":"0090"},"figref":["FIG. 6A","FIG. 6A"],"b":["404","408","402","304","304","402","304"]},"The efficiency of the fixed-extent strategy may be effected by the order in which the blocks  are processed. Since the fixed-extent strategy requires the synchronization of post-effect data between neighboring blocks , a given second (or current) effect cannot begin processing a given block until a first (or prior) effect has processed all of the given blocks neighboring blocks , and the given block's edge data  has been synchronized with its neighboring blocks . By example, after being processed by a first effect, block - must be synchronized with its surrounding blocks, i.e., blocks -, -, and -, prior to being processed by a second effect. Similarly, block - must be synchronized with blocks -, -, -, -, -, -, -, and -.",{"@attributes":{"id":"p-0087","num":"0092"},"figref":"FIG. 6B","b":["100","104","406","304","304","1","304","2","304","6","304","6","304","7","304","11"]},{"@attributes":{"id":"p-0088","num":"0093"},"figref":["FIG. 6C","FIG. 6B","FIG. 6B","FIG. 6C"],"b":["304","304","1","304","2","304","5","304","6","304","1","304","304","304","304","304","1","304","304","304","2","304","4","304","5","304","6","304","2","304","5","304","6","304","3","304","4","0","304"]},"While  shows one alternative strategy for processing blocks , this strategy is only exemplary and intended to show the benefit of changing the potential traversal order for processing blocks . Further, alternative traversal strategies would be possible within the spirit of the invention. For example, alternative strategies may improve the performance by optimizing for different block division strategies, effects in the pipeline, memory capacity, processor speeds, etc.","Pipeline Processing",{"@attributes":{"id":"p-0091","num":"0096"},"figref":"FIG. 7","b":["700","100","700","100"]},"At step , multi-core processor  may receive input identifying an effect pipeline. The effect pipeline may identify a plurality of effects that may be executed by SPUs  to process pixel blocks . The plurality of effects may be analyzed by PPU  to identify the spatial\/temporal edge requirements for each effect.","At step , the effects may be assigned and distributed among the plurality SPUs , based on the analysis in step . The PPU  may assign one effect to each SPU . Alternatively, the PPU  may assign multiple simple, quick running effects to a single SPU , and assign complex, slow-running effects to multiple SPUs . In yet another alternative embodiment, the system may rely on dynamic load balancing to manage the allocation of effects among the various SPUs .","At step , the PPU  may receive an image frame  for processing in accordance with the effect pipeline. Alternatively, the PPU  may read the image frames  from a memory via memory interface  or any other accessible data source.","At step , the PPU  may generate data blocks  from an image frame . The size and structure of the data blocks  may be predefined or may be dependent on the processing requirements of the individual effects, as well as the processing strategy employed.","At step , the PPU  may provide each data block  to an SPU  assigned to the first effect in the effect pipeline. Each data block  may then traverse the plurality of effects from the effects pipeline in an order defined by the effect pipeline.","At step , after the data block  is processed by the last effect, as determined by the effect pipeline, the data blocks  may be aggregated by the PPU  in the MEMORY cache  or other memory. The PPU  may then combine the aggregated data blocks  into an image frame .","At step , if any further image frames  need to be processed through the effects pipeline, the process returns to step ; otherwise, the process ends.",{"@attributes":{"id":"p-0099","num":"0104"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0100","num":"0105"},"figref":["FIG. 8","FIG. 8"],"b":["304","304","302"]},"In , the first half-frame FB is processed within 3 frames (\u00bd frame\u00d76 effects). The second half-frame FB is also processed within 3 frames, but only enters the pipeline \u00bd frame after first half-frame FB. As a result, the second half-frame FB exits the pipeline within 3\u00bd frames (\u00bd frame\u00d76 effects+\u00bd frame delay), resulting in a total frame time latency of 3\u00bd frames for the entire first frame .","The example embodiment illustrated in  uses the same effect pipeline within the block-based framework of , but with the block size reduced to a quarter-frame. This embodiment generates a pipeline latency improvement from 6 frames to 2\u00bc frames by reducing the block size again, as compared to the prior art.","In , the first quarter-frame FB is processed within 1\u00bd frames (\u00bc frame\u00d76 effects). The quarter-frame FB is also processed within 1\u00bd frames, but only enters the pipeline \u00bc frame after first quarter-frame FB. As a result, the second quarter-frame FB exits the pipeline within 1\u00be frames (\u00bc frame\u00d76 effects+\u00bc frame delay). The third and fourth quarter-frames, FB and FB, also exit the pipeline \u00bd frame and \u00be frame after the first quarter-frame FB. As a result, the first frame is processed within 2\u00bc frames.","The above examples clearly show the advantages of Block-Based Effect Processing over Frame-Based Effect Processing. As the block sizes continue to reduce, it becomes possible to obtain higher gains with respect to pipeline latency; however, the gains become incrementally smaller. Eventually, the latency introduced from the data transfer of the data blocks , and other computational aspects of the process exceed the gains from further block reduction.","Similarly to traditional frame-based architecture, pipeline latency for the block-based system can also be defined as a product of the processing time for the slowest effect in the chain and the number of effects (or stages) in the pipeline. However, switching to a block-based architecture reduces this to the sum of time taken by the slowest effect to process one frame  of data added to the product of the time taken by the slowest effect M to process one block  of data and the number of effects (or stages) minus 1. Block-based pipeline latency, as computation time, can be calculated as:\n\n=()+((1)*) \u2003\u2003(Eq. 3)\n\nwhere:\n\n","For example, the PLfor the block-based system illustrated in  where S=2 becomes (2*60)+(5*60)=420 ms. Tis simply computed as T\/S because we ignore any overhead associated with the switch to block-processing.","Computing the PL in frame-times yields the formula:\n\n\u2003\u2003(Eq. 5)\n","where:\n\n","Using the above numbers, the PLfor the example in  is 420\/120=3.5 frames, which is identical to the number computed earlier. The example illustrated in  yields a PLof 270 ms and PLof 2.25 frames.","As noted earlier, these examples assume the overhead of block-based processing is negligible. Real-world scenarios may have either higher computation times for effects located earlier on in the pipeline (Reducing-extent Strategy), or greater synchronization overhead, and hence reduce pipeline latency (Fixed-extent Strategy).","Memory Requirements","The block-based system offers significant advantages in terms of the memory footprint of the pipeline. The traditional frame-based N stage pipeline needs at least N frame buffers to keep all stages operating. Because frame I\/O is handled outside of the effect pipeline, such a system would need a few extra frame buffers for that purpose also (2 at the very minimum, one for input, and one for output, and 4 if double-buffering of the I\/O operations is employed). Hence, the minimum frame buffers required to process the pipeline is effectively N+2.","If P represents the number of processor elements available in the system for the pipeline, the buffers required would reduce to a minimum of (P, N) buffers because, if the number of processor elements is less than the number of stages in the pipeline, the system can only process P frames  simultaneously. Conversely, if the number of pipeline stages is less than the number of processor elements, then the system can only process N buffers simultaneously, as the additional processor elements will remain idle.","The formula to compute the minimum number of frame buffers required by the traditional frame-based pipeline can be derived as:\n\n=min()+2 \u2003\u2003(Eq. 6)\n\nwhere:\n\n","The actual memory requirement for the buffers then becomes:\n\n\u2003\u2003(Eq. 7)\n\nwhere:\n\n","The block-based system offers significant advantages in terms of the memory footprint of the pipeline. Because a block is smaller than a frame, the memory saving can be substantial for even a short length pipe. Following the earlier logic, the minimum number of buffers needed in a block-based system may be min (P, N) block buffers, plus at least 2 frame buffers, again for the I\/O operation. As above:\n\n=min() \u2003\u2003(Eq. 8)\n\nwhere:\n\n","However, the actual memory requirement for all of the buffers would be:\n\n+2*\u2003\u2003(Eq. 9)\n\nwhere:\n\n","By example, in a 6-stage pipeline on a 4 SPU system where the memory required to hold one frame  is 8192 KB (size required to hold a 4-byte per pixel HD frame of dimensions 2048\u00d71024), and each frame  is divided into blocks  of dimensions 256\u00d7256, the size required to hold each block  will be 256 KB, and there will be 32 blocks  per frame  (edge size=0 for simplicity).","Using the formulas for the traditional frame-based approach, the minimum memory required for buffers in a frame-based pipeline system will be:\n\n=[min()+2]*=[min(4, 6)+2]*8192=6*8192=49152 KB\n","By comparison, the minimum memory required for processing the same pipeline using a block based approach will be:\n\n=[min()]*2*=[min(4, 6)]*256+2*8192=4*256+2*8192=17408 KB.\n","Furthermore, it can be noted that the two mandatory frame buffers consume 16 MB of the 17 MB used in the block-based system. Therefore, by removing these mandatory buffers, the difference in comparable memory usage becomes 32 MB for the frame-based system versus 1.5 MB for the block-based system. Even if the block-based approach were to require significantly more memory for successive area effects, the memory consumption is unlikely to approach that of the frame-based system.","III. Load Balancing","The example embodiments illustrated thus far assume that all of the constituent effects take up the same amount of time to process the same amount of data (be it a frame  or a block ). However, effects may take varying amounts of time, based both on their algorithmic and implementation complexity. The varying processing times of the different effects may degrade the pipeline latency, with the slowest effect causing a bottleneck. The bottleneck may then starve downstream SPUs  of work. If the number of buffers is limited, this may also starve upstream SPUs  as memory is consumed.",{"@attributes":{"id":"p-0124","num":"0149"},"figref":["FIG. 10A","FIG. 10A"],"b":["1","2","4","3","5","6","1","2","6","3"]},"The processing pipeline in  is similar to the embodiment discussed with respect to  where each SPU  is assigned a single effect. However, whereas in  the block latency was 6 blocks, in , the total pipeline latency is significantly higher, due to the bottleneck created in effect . This causes the downstream effects (and hence the other SPUs ) to waste time in an idle state. Long-term, this bottleneck may end up using a very large number of buffers just to keep the upstream SPUs  occupied, without any benefit at the output end.","Static Load Balancing offers a first solution to this problem. Static Load Balancing includes performing a static analysis of the effects, by taking into account the computation times of the various effects (FX-FX) involved. This allows for prior distribution of the effects which can include combining short computation effects into a single resource (i.e., one or a few SPU), while increasing the number of resources (e.g., dedicating a plurality of SPUs) to the slower effects.",{"@attributes":{"id":"p-0127","num":"0152"},"figref":["FIG. 10B","FIG. 10B","FIG. 10A","FIG. 11B"],"b":["104","104","1","104","6","104","2","104","5","3","104","1","1","2","104","6","4","5","6","104","3","104","104","2","104","5","304","2","104","3","304","3","104","4","6"]},"Static Load Balancing can work well for fixed function pipeline, but fails to scale well with the needs of a generic effect pipeline, or even a fixed function pipeline where the effect processing times vary based on either the complexity of the input data, or external parameters.","Dynamic Load Balancing offers an alternative to static load balancing in cases where processing load varies from one frame to the next or one block to the next, either based on external input (such as an effect whose processing time changes with the parameters provided by the user), or based on the work load itself (such as a decoder, based on compression variation within the encoded data).","Switching to block-based processing from frame-based processing allows for more effective dynamic load balancing in a scheduling system, as there are more synchronization points available to the scheduler to balance things out, and localized work differences can be taken into account (e.g. blocks within the same frame containing low contrast data and blocks containing high contrast data will have different performance characteristics for different effects, and switching to block-based dynamic load balancing allows the system to adapt to this at a more fine-grained level).","With regard to the processes, systems, methods, heuristics, etc. described herein, it should be understood that, although the steps of such processes, etc. have been described as occurring according to a certain ordered sequence, such processes could be practiced with the described steps performed in an order other than the order described herein. It further should be understood that certain steps could be performed simultaneously, that other steps could be added, or that certain steps described herein could be omitted. Processes may also be implemented as computer-executable instructions (e.g., as one or more scripts), stored procedures, executable programs, etc. on a client, server, and\/or database. In other words, the descriptions of processes herein are provided for the purpose of illustrating certain embodiments, and should in no way be construed so as to limit the claimed invention.","Accordingly, it is to be understood that the above description is intended to be illustrative and not restrictive. Many embodiments and applications other than the examples provided would be apparent to those of skill in the art upon reading the above description. The scope of the invention should be determined, not with reference to the above description, but instead with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled. It is anticipated and intended that future developments will occur in the arts discussed herein, and that the mentioned systems and methods will be incorporated into such future embodiments. In sum, it should be understood that the invention is capable of modification and variation and is limited only by the following claims.","The disclosed parallel processing methods can be applied to audio effects pipelines, 3D effects pipelines, or any situation where data can be broken down into separate blocks and processed through a pipeline of effects.","Computing devices (e.g., processors, clients, servers, terminals, etc.), such as those discussed herein generally may include executable instructions. Furthermore, processors may include any device itself containing any number of processing components, such as SPU, PPUs, GPUs, etc. Computer-executable instructions may be compiled or interpreted from computer programs created using a variety of programming languages and\/or technologies known to those skilled in the art, including, without limitation, and either alone or in combination, Java\u2122, C, C++, assembly, etc. In general, a processor (e.g., a microprocessor), receives instructions (e.g., from a memory, a computer-readable medium, etc.), and executes these instructions, thereby performing one or more processes, including one or more of the processes described herein. Such instructions and other data may be stored and transmitted using a variety of known computer-readable media.","A computer-readable medium includes any medium that participates in providing data (e.g., instructions), which may be read by a computer. Such a medium may take many forms, including, but not limited to, non-volatile media, volatile media, and transmission media. Non-volatile media include, for example, optical or magnetic disks and other persistent memory. Volatile media include dynamic random access memory (DRAM), which typically constitutes a main memory.","Communications between computing devices, and within computing devices may employ transmission media including coaxial cables, copper wire, and fiber optics, including the wires that comprise a system bus coupled to the processor. Transmission media may include or convey acoustic waves, light waves, and electromagnetic emissions, such as those generated during radio frequency (RF) and infrared (IR) data communications. Common forms of computer-readable media include, for example, a floppy disk, a flexible disk, hard disk, magnetic tape, any other magnetic medium, a CD-ROM, DVD, any other optical medium, punch cards, paper tape, any other physical medium with patterns of holes, a RAM, a PROM, an EPROM, a FLASH-EEPROM, any other memory chip or cartridge, a carrier wave as described hereinafter, or any other medium from which a computer can read.","Thus, embodiments of the present invention produce and provide a pipelined image processing engine. Although the present invention has been described in considerable detail with reference to certain embodiments thereof, the invention may be variously embodied without departing from the spirit or scope of the invention. Therefore, the following claims should not be limited to the description of the embodiments contained herein in any way."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["These and other more detailed and specific features of the present invention are more fully disclosed in the following specification, reference being had to the accompanying drawings, in which:",{"@attributes":{"id":"p-0032","num":"0037"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0033","num":"0038"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0034","num":"0039"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0035","num":"0040"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0036","num":"0041"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0037","num":"0042"},"figref":["FIG. 5B","FIG. 5A"]},{"@attributes":{"id":"p-0038","num":"0043"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0039","num":"0044"},"figref":"FIG. 6B"},{"@attributes":{"id":"p-0040","num":"0045"},"figref":"FIG. 6C"},{"@attributes":{"id":"p-0041","num":"0046"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0042","num":"0047"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0043","num":"0048"},"figref":["FIG. 9","FIG. 8"]},{"@attributes":{"id":"p-0044","num":"0049"},"figref":"FIG. 10A"},{"@attributes":{"id":"p-0045","num":"0050"},"figref":"FIG. 10B"},{"@attributes":{"id":"p-0046","num":"0051"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
