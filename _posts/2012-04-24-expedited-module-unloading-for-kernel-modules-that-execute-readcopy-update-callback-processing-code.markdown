---
title: Expedited module unloading for kernel modules that execute read-copy update callback processing code
abstract: A technique for expediting the unloading of an operating system kernel module that executes read-copy update (RCU) callback processing code in a computing system having one or more processors. According to embodiments of the disclosed technique, an RCU callback is enqueued so that it can be processed by the kernel module's callback processing code following completion of a grace period in which each of the one or more processors has passed through a quiescent state. An expediting operation is performed to expedite processing of the RCU callback. The RCU callback is then processed and the kernel module is unloaded.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09262234&OS=09262234&RS=09262234
owner: International Business Machines Corporation
number: 09262234
owner_city: Armonk
owner_country: US
publication_date: 20120424
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["This application is a continuation under 35 U.S.C. 120 of application Ser. No. 13\/316,476, filed Dec. 10, 2011, entitled \u201cExpedited Module Unloading For Kernel Modules That Execute Read-Copy Update Callback Processing Code.\u201d","1. Field","The present disclosure relates to computer systems and methods in which data resources are shared among data consumers while preserving data integrity and consistency relative to each consumer. More particularly, the disclosure concerns an implementation of a mutual exclusion mechanism known as \u201cread-copy update\u201d in a computing environment wherein loadable modules contain code that is used to process read-copy update callbacks.","2. Description of the Prior Art","By way of background, read-copy update (also known as \u201cRCU\u201d) is a mutual exclusion technique that permits shared data to be accessed for reading without the use of locks, writes to shared memory, memory barriers, atomic instructions, or other computationally expensive synchronization mechanisms, while still permitting the data to be updated (modify, delete, insert, etc.) concurrently. The technique is well suited to both uniprocessor and multiprocessor computing environments wherein the number of read operations (readers) accessing a shared data set is large in comparison to the number of update operations (updaters), and wherein the overhead cost of employing other mutual exclusion techniques (such as locks) for each read operation would be high. By way of example, a network routing table that is updated at most once every few minutes but searched many thousands of times per second is a case where read-side lock acquisition would be quite burdensome.","The read-copy update technique implements data updates in two phases. In the first (initial update) phase, the actual data update is carried out in a manner that temporarily preserves two views of the data being updated. One view is the old (pre-update) data state that is maintained for the benefit of read operations that may have been referencing the data concurrently with the update. The other view is the new (post-update) data state that is seen by operations that access the data following the update. In the second (deferred update) phase, the old data state is removed following a \u201cgrace period\u201d that is long enough to ensure that the first group of read operations will no longer maintain references to the pre-update data. The second-phase update operation typically comprises freeing a stale data element to reclaim its memory. In certain RCU implementations, the second-phase update operation may comprise something else, such as changing an operational state according to the first-phase update.",{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIGS. 1A-1D"},"It is assumed that the data element list of  is traversed (without locking) by multiple readers and occasionally updated by updaters that delete, insert or modify data elements in the list. In , the data element B is being referenced by a reader r, as shown by the vertical arrow below the data element. In , an updater u wishes to update the linked list by modifying data element B. Instead of simply updating this data element without regard to the fact that r is referencing it (which might crash r), u preserves B while generating an updated version thereof (shown in  as data element B\u2032) and inserting it into the linked list. This is done by u acquiring an appropriate lock (to exclude other updaters), allocating new memory for B\u2032, copying the contents of B to B\u2032, modifying B\u2032 as needed, updating the pointer from A to B so that it points to B\u2032, and releasing the lock. In current versions of the Linux\u00ae kernel, pointer updates performed by updaters can be implemented using the rcu_assign_pointer( ) primitive. As an alternative to locking during the update operation, other techniques such as non-blocking synchronization or a designated update thread could be used to serialize data updates. All subsequent (post update) readers that traverse the linked list, such as the reader r, will see the effect of the update operation by encountering B\u2032 as they dereference B's pointer. On the other hand, the old reader r will be unaffected because the original version of B and its pointer to C are retained. Although r will now be reading stale data, there are many cases where this can be tolerated, such as when data elements track the state of components external to the computer system (e.g., network connectivity) and must tolerate old data because of communication delays. In current versions of the Linux\u00ae kernel, pointer dereferences performed by readers can be implemented using the rcu_dereference( ) primitive.","At some subsequent time following the update, r will have continued its traversal of the linked list and moved its reference off of B. In addition, there will be a time at which no other reader process is entitled to access B. It is at this point, representing an expiration of the grace period referred to above, that u can free B, as shown in .",{"@attributes":{"id":"p-0011","num":"0010"},"figref":["FIGS. 2A-2C","FIG. 2A","FIG. 2B","FIG. 2C"],"b":["1","1","1","1","2","1"]},"In the context of the read-copy update mechanism, a grace period represents the point at which all running tasks (e.g., processes, threads or other work) having access to a data element guarded by read-copy update have passed through a \u201cquiescent state\u201d in which they can no longer maintain references to the data element, assert locks thereon, or make any assumptions about data element state. By convention, for operating system kernel code paths, a context switch, an idle loop, and user mode execution all represent quiescent states for any given CPU running non-preemptible code (as can other operations that will not be listed here). The reason for this is that a non-preemptible kernel will always complete a particular operation (e.g., servicing a system call while running in process context) prior to a context switch. In preemptible operating system kernels, additional steps are needed to account for readers that were preempted within their RCU read-side critical sections. In current RCU implementations designed for the Linux\u00ae kernel, a blocked reader task list is maintained to track such readers. A grace period will only end when the blocked task list indicates that is safe to do so because all blocked readers associated with the grace period have exited their RCU read-side critical sections. Other techniques for tracking blocked readers may also be used, but tend to require more read-side overhead than the current blocked task list method.","In , four tasks , , , and  running on four separate CPUs are shown to pass periodically through quiescent states (represented by the double vertical bars). The grace period (shown by the dotted vertical lines) encompasses the time frame in which all four tasks that began before the start of the grace period have passed through one quiescent state. If the four tasks , , , and  were reader tasks traversing the linked lists of  or , none of these tasks having reference to the old data element B prior to the grace period could maintain a reference thereto following the grace period. All post grace period searches conducted by these tasks would bypass B by following the updated pointers created by the updater.","Grace periods may be synchronous or asynchronous. According to the synchronous technique, an updater performs the first phase update operation, invokes an RCU primitive such as synchronize_rcu( ) to advise when all current RCU readers have completed their RCU critical sections and the grace period has ended, blocks (waits) until the grace period has completed, and then implements the second phase update operation, such as by removing stale data. According to the asynchronous technique, an updater performs the first phase update operation, specifies the second phase update operation as a callback using an RCU primitive such as call_rcu( ) then resumes other processing with the knowledge that the callback will eventually be processed at the end of a grace period. Advantageously, callbacks requested by one or more updaters can be batched (e.g., on callback lists) and processed as a group at the end of an asynchronous grace period. This allows the grace period overhead to be amortized over plural deferred update operations.","Modern operating systems, including current versions of the Linux\u00ae kernel, use loadable modules to implement device drivers, file systems and other software. Loadable modules allow software functionality to be installed on an as-needed basis and then removed when the software is no longer required. This reduces the memory footprint of the base kernel. In operating systems that implement read-copy update with asynchronous grace period detection, some or all of the callback function code that processes a callback following the end of a grace period may be located within a loadable module. If the module containing the callback function code is unloaded before a pending callback that requires such code can be invoked, problems will arise when an attempt is made to implement the callback function because its code is no longer part of the running kernel.","A response to this scenario was the development of the \u201crcu_barrier( )\u201d primitive, which can be called by a module's exit code during module unloading. The rcu_barrier( ) primitive waits for the end of the current grace period and for all RCU callbacks associated with the grace period to be invoked. When using the rcu_barrier( ) primitive, the sequence of operations performed by a kernel module's exit code is to (1) prevent any new RCU callbacks from being posted, (2) execute rcu_barrier( ) and (3) allow the module to be unloaded. The rcu_barrier( ) primitive is for use by process context code. For the non-preemptible uniprocessor version of RCU known as TINY_RCU, the rcu_barrier( ) primitive is set forth at lines 41-44 of the Linux\u00ae version 3.1 source code file named Linux\/include\/linux\/rcutiny.h. This primitive is a wrapper function for a helper function called \u201crcu_barrier_sched( ), which is set forth at lines 298-309 of the Linux\u00ae version 3.1 source code file named Linux\/kernel\/rcutiny.c. For the preemptible uniprocessor version of RCU known as TINY_PREEMPTIBLE_RCU, the rcu_barrier( ) primitive is set forth at lines 700-711 of the Linux\u00ae version 3.1 source code file named Linux\/kernel\/rcutiny_plugin.h. For the hierarchical multiprocessor versions of RCU known as TREE_RCU and TREE_PREEMPTIBLE_RCU, the rcu_barrier( ) primitive is set forth at lines 854-857 of the Linux\u00ae version 3.1 source code file named Linux\/kernel\/rcutree_plugin.h. This is a wrapper function that calls a helper function named rcu_barrier( ) which may be found at lines 1778-1807 of the Linux\u00ae version 3.1 source code file named Linux\/kernel\/rcutree.c.","In many instances, it is desirable to expedite module unloading so that the module's kernel memory can be reclaimed for other uses. Unfortunately, the rcu_barrier( ) primitive can delay module unloading due to the latency associated with waiting for the end of a current RCU grace period and for all prior RCU callbacks to be invoked. The present disclosure presents a technique for improving this situation by speeding up RCU grace period detection and callback processing operations during module unloading.","A method, system and computer program product are provided for expediting the unloading of an operating system kernel module that executes read-copy update (RCU) callback processing code in a computing system having one or more processors. According to embodiments of the disclosed technique, an RCU callback is enqueued so that it can be processed by the kernel module's callback processing code following completion of a grace period in which each of the one or more processors has passed through a quiescent state. An expediting operation is performed to expedite processing of the RCU callback. The RCU callback is then processed and the kernel module is unloaded.","In an example embodiment, the computing system is a uniprocessor system that runs a non-preemptible operating system kernel, and the callback processing code runs in a deferred non-process context of the operating system kernel. In that case, the expediting operation may comprise invoking the deferred non-process context to force the callback processing code to execute.","In another example embodiment, the computing system is a uniprocessor system that runs a preemptible operating system kernel. In that case, the expediting operation may comprise implementing a priority boost for blocked reader tasks that are preventing completion of the grace period.","In another example embodiment, the computing system is a multiprocessor system that runs a non-preemptible operating system kernel. In that case, the expediting operation may comprise forcing each processor to note a new grace period and forcing a quiescent state on each processor, such as by implementing a rescheduling operation on each processor. The expediting operation may be repeated as necessary until the RCU callback is processed.","In another example embodiment, the computing system is a multiprocessor system that runs a preemptible operating system kernel. In that case, the expediting operation may comprise forcing each processor to note a new grace period and forcing a quiescent state on each processor by implementing a priority boost for blocked reader tasks that are preventing completion of the grace period. The expediting operation may be repeated as necessary until the RCU callback is processed.","Turning now to the figures, wherein like reference numerals represent like elements in all of the several views,  respectively illustrate example uniprocessor and multiprocessor computing environments in which the expedited module unloading technique disclosed herein may be implemented. In , a uniprocessor computing system  includes a single processor , a system bus  (or other interconnection pathway) and a program memory . A conventional cache memory  and a cache controller  are associated with the processor . A conventional memory controller  is associated with the memory . As shown, the memory controller  may reside separately from processor  (e.g., as part of a chipset). Alternatively, the memory controller  could be integrated with the processor  (as is known in the art). In , a multiprocessor computing system A includes multiple processors , . . . , a system bus , and a program memory . There are also cache memories , . . . and cache controllers , . . . respectively associated with the processors , . . . . A conventional memory controller  is again associated with the memory . As shown, the memory controller  may reside separately from processors . . . (e.g., as part of a chipset). Alternatively, the memory controller  could be provided by plural memory controller instances respectively integrated with the processors . . . (as is known in the art).","In each of , the example computing systems  and A may represent any of several different types of computing apparatus. Such computing apparatus may include, but are not limited to, general purpose computers, special purpose computers, portable computing devices, communication and\/or media player devices, set-top devices, embedded systems, to name but a few. In , the processor  may be implemented as a single-core CPU (Central Processing Unit) device. In , the processors , . . . may each be a single-core CPU device. Alternatively, the processors , . . . could represent individual cores within a multi-core CPU device. Each CPU device embodied by any given processor  of  is operable to execute program instruction logic under the control of a software program stored in the memory  (or elsewhere). The memory  may comprise any type of tangible storage medium capable of storing data in computer readable form, including but not limited to, any of various types of random access memory (RAM), various flavors of programmable read-only memory (PROM) (such as flash memory), and other types of primary storage. In , the processors  and the memory  may be situated within a single computing device or node. In , the processors , . . . may be situated within a single computing device or node (e.g., as part of a single-node SMP system) or they may be distributed over plural nodes (e.g., as part of a NUMA system, a cluster, a cloud, etc.).","An update operation (updater)  may periodically execute within a process, thread, or other execution context (hereinafter \u201ctask\u201d) on any processor  of . Each updater  runs from program instructions stored in the memory  (or elsewhere) in order to periodically perform updates on a set of shared data  that may be stored in the shared memory  (or elsewhere).  illustrates a single updater  executing on the lone processor . In , reference numerals , . . . illustrate individual data updaters that may periodically execute on the several processors , . . . . As described in the \u201cBackground\u201d section above, the updates performed by an RCU updater can include modifying elements of a linked list, inserting new elements into the list, deleting elements from the list, and other types of operations. To facilitate such updates, the processors  of  are programmed from instructions stored in the memory  (or elsewhere) to implement a read-copy update (RCU) subsystem  as part of their processor functions.  illustrates a single RCU subsystem executing on the lone processor . In , reference numbers , . . . represent individual RCU instances that may periodically execute on the several processors , . . . . Any given processor  in  may also periodically execute a read operation (reader) . Each reader  runs from program instructions stored in the memory  (or elsewhere) in order to periodically perform read operations on the set of shared data  stored in the shared memory  (or elsewhere).  illustrates a single reader  executing on the lone processor . In , reference numerals , . . . illustrate individual reader instances that may periodically execute on the several processors , . . . . Such read operations will typically be performed far more often than updates, this being one of the premises underlying the use of read-copy update. Moreover, it is possible for several of the readers  to maintain simultaneous references to one of the shared data elements  while an updater  updates the same data element. Embodiments of the updaters  and the readers  may be preemptible, and embodiments of the systems  and A may, for example, support real-time operations.","During run time, an updater  will occasionally perform an update to one of the shared data elements . In accordance the philosophy of RCU, a first-phase update is performed in a manner that temporarily preserves a pre-update view of the shared data element for the benefit of readers  that may be concurrently referencing the shared data element during the update operation. Following the first-phase update, the updater  may register a callback with the RCU subsystem  for the deferred destruction of the pre-update view following a grace period (second-phase update). As described in the \u201cBackground\u201d section above, this is known as asynchronous grace period processing.","The RCU subsystem  may handle both asynchronous and synchronous grace periods. Each type of grace period processing entails starting new grace periods and detecting the end of old grace periods so that the RCU subsystem  knows when it is safe to free stale data (or take other actions). Asynchronous grace period processing further entails the management of callback lists that accumulate callbacks until they are ripe for batch processing at the end of a given grace period. As part of this batch processing, it is assumed for purposes of the present disclosure that at least some of the code that processes RCU callbacks is implemented by a loadable operating system kernel module. It will be appreciated that the kernel module should not be unloaded unless and until it has no further callback processing work remaining to be done.","Grace period processing operations may be performed by periodically running the RCU subsystem  on the lone processor  in  or on each of the several processors , . . . in . As is known, different aspects of such processing may be variously invoked by an operating system scheduler and an scheduling clock interrupt handler, and run in a combination of process context and bottom half context or kernel thread context.","In current versions of the Linux\u00ae kernel, there are four main variants of RCU designed for different processor and operating system configurations. Two uniprocessor variants called TINY_RCU and TINY_PREEMPT_RCU may be used with the uniprocessor system  of FIG. . TINY_RCU is for non-preemptible kernels and TINY_PREEMPT_RCU is for preemptible kernels. Two multiprocessor variants called TREE_RCU and TREE_PREEMPT_RCU may be used with the multiprocessor system A of . TREE_RCU is for non-preemptible kernels and TREE_PREEMPT_RCU is for preemptible kernels. Each of the above-listed RCU variants may be used as a starting point for implementing embodiments of the subject matter disclosed herein. Table 1 below lists the salient Linux 3.1 source code files for these RCU variants. Persons of ordinary skill in the art to whom the present disclosure is directed will be familiar with such Linux-based RCU implementations. It should be understood, however, that other RCU implementations may also be used in lieu of the four Linux-specific RCU variants mentioned above.",{"@attributes":{"id":"p-0047","num":"0046"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["RCU ","LINUX\u2009\u00ae 3.1 "]},{"entry":["VARIANT","SOURCE CODE FILES"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["TINY_RCU\/TINY_PREEMPT_RCU","Linux\/kernel\/rcutiny.c"]},{"entry":[{},"Linux\/include\/linux\/rcutiny.h"]},{"entry":[{},"Linux\/kernel\/rcutiny_plugin.h"]},{"entry":["TREE_RCU\/TREE_PREEMPT_RCU","Linux\/kernel\/rcutree.c"]},{"entry":[{},"Linux\/kernel\/rcutree.h"]},{"entry":[{},"Linux\/kernel\/rcutree_plugin.h"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},{"@attributes":{"id":"p-0048","num":"0047"},"figref":["FIG. 6","FIG. 7","FIGS. 8A-8C"],"b":["20","30","32","34","32","32","34","32","34"]},"With continuing reference to , additional components of the RCU subsystem  that are variously present in the above-referenced RCU variants include several RCU subsystem support functions , namely, an RCU reader API (Application Programming Interface) , an RCU updater API , and various grace period detection and callback processing functions . Further details of the support functions  are described in more detail below in connection with .","Turning now to , an example embodiment of an RCU callback list  comprises a linked list of RCU callbacks A. In all of the above-referenced RCU variants, each processor maintains its own callback list . Although  shows five RCU callbacks A on the callback list , this is for purposes of illustration only. As is conventionally known, each RCU callback A may comprise an rcu_head pointer (\u2192next) to the next RCU callback on the callback list , and a pointer (\u2192func) to a callback processing function. In an example embodiment, each RCU callback A may be coded in software as an rcu_head structure using the following C programming declaration:",{"@attributes":{"id":"p-0051","num":"0050"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"struct rcu_head {"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"struct rcu_head *next;"]},{"entry":[{},"void (*func)(struct rcu_head *head);"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"};"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"Each processor's RCU callback list A is accessed using list pointers that are maintained as part of the grace period\/callback processing information  (See ). A list head pointer points to the head of the RCU callback list . Several list portion tail pointers are used to partition the callback list  into list portions that are used to accumulate callbacks during different grace periods. Each list portion tail pointer references the \u2192next pointer of the last RCU callback A of a particular list portion. The number of callback list portions varies by RCU implementation. The TINY_RCU variant maintains two list portions, known as the \u201cdonelist\u201d and the \u201ccurlist.\u201d The list portion tail pointers that mark the end of these list portions are respectively called the \u201cdonetail\u201d pointer and the \u201ccurtail\u201d pointer. The donelist accumulates callbacks that are ready to be invoked because their asynchronous grace period has completed. The curlist accumulates callbacks that will be ready to be invoked when the current asynchronous grace period ends. The TINY_PREEMPT_RCU variant maintains three list portions, namely, the donelist and curlist described above, and an additional list portion known as the \u201cnextlist.\u201d The nextlist follows the curlist. The list portion tail pointer that marks the end of the nextlist is called the \u201cnexttail\u201d pointer. The nextlist is used to handle callbacks that are registered while there are blocked readers preventing the end of an asynchronous grace period. The TREE_RCU and TREE_RCU_PREEMPT variants maintain four list portions, namely, the donelist, curlist and nextlist described above, and an additional list portion known as the \u201cnextreadylist.\u201d The nextreadylist is between the curlist and the nextlist. The list portion tail pointer that marks the end of the nextreadylist is called the nextreadytail pointer. The nextreadylist is needed because hierarchical RCU implementations must handle that fact different processors will note quiescent states and grace periods at different times. The nextreadylist is used to accumulate callbacks that are known to have arrived on the callback list  before the end of the current asynchronous grace period. It differs from the nextlist in that the latter is used to accumulate callbacks that might have arrived after the current grace period ended.","Turning now to , the RCU grace period detection\/callback processing information  maintained by the RCU subsystem  varies by RCU implementation.  illustrates the grace period detection\/callback processing information  used in a TINY_RCU implementation. This information comprises an rcu_ctrlblk structure A that contains a set of callback list pointers A-, namely, the list head pointer, the donetail pointer and the curtail pointer described above in connection with .",{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIG. 8B","FIG. 8A","FIG. 7"],"b":["34","34","34","34","1","34","34","2","34","34","3","34","3"]},{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIG. 8C","FIG. 7"],"b":["34","34","34","34","34","34","1","34","2","34","1"]},"The rcu_node structure D are arranged in a tree hierarchy that is embedded in a linear array in the rcu_state structure E. The rcu_node hierarchy comprises one or more leaf rcu_node structures, zero or more levels of internal rcu_node structures, and a top-level root rcu_node structure (if there is more than one leaf rcu_node structure). Each leaf rcu_node structure D is assigned to some number of rcu_data structures C. Each internal rcu_node structure D is then assigned to some number of lower level rcu_node structures, and so on until the root rcu_node structure is reached. The number of rcu_node structures D depends on the number of processors in the system. Very small multiprocessor systems may require only a single rcu_node structure D whereas very large systems may require numerous rcu_node structures arranged in a multi-level hierarchy.","Each rcu_node structure comprises a set of grace period\/quiescent state counters and flags D-, and a set of blocked task tracking information D-. The grace period\/quiescent state counters and flags D- track grace period and quiescent state information at the node level. The blocked task tracking information D- is used by the TREE-PREEMPT_RCU implementation to track reader tasks that were preempted inside their RCU read-side critical sections. The rcu_state structure E also contains a set of a set of grace period\/quiescent state counters and flags E- that track grace period and quiescent state information at a global level.","Turning now to , the RCU reader API  within the RCU subsystem  comprises a reader registration component A and a reader unregistration component B. These components are respectively invoked by readers  as they enter and leave their RCU read-side critical sections. This allows the RCU subsystem  to track reader quiescent states, with all processing performed outside of a set of bounded calls to the reader registration and unregistration components being treated as a quiescent state for a given reader. The reader registration component A and the reader unregistration component B may be respectively implemented using the conventional \u201crcu_read_lock( )\u201d and \u201crcu_read unlock\u201d primitives found in existing RCU implementations. As is known, these primitives differ depending on the RCU variant being used, with the preemptible RCU variants TINY_PREEMPT_RCU and TREE_PREEMPT_RCU implementing operations in the rcu_read unlock primitive to handle readers that were preempted while in their RCU read-side critical sections.","As further shown in , the RCU updater API  comprises a register callback component A that is associated with asynchronous grace periods and an expedited grace period component B that is associated with synchronous grace periods. The register callback component A is invoked by updaters  in order to register a callback following a first-phase update to a shared data element . It may be implemented using the conventional \u201ccall_rcu( )\u201d primitive found in existing RCU implementations. Invocation of the register callback component A of the RCU updater API  initiates processing that places an RCU callback A on the RCU callback list  of the processor  that runs the updater . In some RCU variants, the register callback component A may also starts an asynchronous grace period (if one is not already in progress) (see TINY_PREEMPT_RCU) or may force a quiescent state if one is needed (see TREE_RCU AND TREE_PREEMPT_RCU).","The expedited grace period component B of the RCU updater API  may be implemented using the synchronize_rcu_expedited( ) function found in each of the above-referenced RCU variants for implementing expedited synchronous grace periods. The expedited grace period component B is invoked by updaters  to request an expedited grace period following a first-phase update to a shared data element . The updater  blocks while the expedited grace period is in progress, then performs second-phase update processing to free stale data (or perform other actions). In the non-preemptible TINY_RCU and TREE_RCU implementations, the expedited grace period component B performs a scheduling action that forces the invoking processor to pass through a quiescent state. A scheduling action is also performed in TREE_PREEMPT_RCU to force all currently executing RCU readers  onto the blocked task list (see D- of ). In both TINY_PREEMPT_RCU and TREE_PREEMPT_RCU, the expedited grace period component B checks the blocked task list (see B- of FIGS. B and D- of ) for blocked readers . If there are any, the expedited grace period component B makes note of them (by setting a pointer), then waits for such readers to complete their RCU read-side critical sections so that the expedited grace period can end. If necessary, the synchronous expedited grace period component B will be aided by boosting the priority of one or more of the blocked readers . This will expedite reader completion of their RCU read-side critical sections and allow the synchronous expedited grace period to end more quickly.","The RCU grace period detection and callback processing functions  include a set of standard RCU grace period detection\/callback processing components A, as well as a new component B, referred to as \u201cRCU barrier expedited,\u201d that may be used for expediting the unloading of kernel modules that contain RCU callback processing code. The standard components A implement conventional RCU grace period detection and callback processing operations. The details of these operations will not be described herein insofar as they are well known to persons of ordinary skill in the art who are familiar with the RCU source code files identified in Table 1 above. The basic approach is to have the operating system task scheduler and timer tick (scheduling clock interrupt) functions drive the RCU subsystem state changes (by respectively initiating such processing via calls to the RCU functions \u201crcu_note_context_switch( )\u201d and \u201crcu_check_callbacks( )\u201d). Once invoked by the task scheduler or the scheduling clock interrupt handler, the grace period detection and callback processing operations performed by the standard components A differ between RCU implementations. In TINY_RCU, the standard components A implicitly note a quiescent state, check for pending callbacks and invoke deferred callback processing if any callbacks are found (using softirq context or a kernel thread). In TINY_PREEMPT_RCU, TREE_RCU and TREE_PREEMPT_RCU, the standard components A perform far more complex processing involving the data structures shown in  to manipulate one or more grace period\/quiescent state counters and flags that explicitly note quiescent states and grace periods, check for pending RCU callbacks on the callback list(s)  of , and invoke deferred callback processing when is permissible to do so. In the TINY_PREEMPT_RCU and TREE_PREEMPT_RCU variants, the standard components A also manage reader tasks that were preempted within their RCU read-side critical sections (using the blocked task lists referenced by the data structures B- of FIGS. B and D- of , respectively), and may boost their priority if necessary in order to expedite grace periods. In the hierarchical TREE_RCU and TREE_PREEMPT_RCU variants, the standard components A must coordinate quiescent state and grace period tracking between multiple processors, which includes propagating quiescent state and grace period information up through the rcu_node hierarchy. All of the above-mentioned RCU variants are capable of handling processor low power states (dynticks idle). The hierarchical RCU variants are also capable of handling hot-plugging activity.","Turning now to the RCU barrier expedited component B, different versions thereof are provided for each of the above-referenced RCU variants. Each version may be implemented by modifying the existing RCU barrier primitive used for the corresponding RCU variant. The existing RCU barrier primitives are of two basic type, one type being used for the uniprocessor TINY_RCU and TINY_PREMPT_RCU variants, and the other type being used for the multiprocessor TREE_RCU and TREE_PREEMPT_RCU variants. The source code function names and file locations of these existing RCU barrier primitives are set forth in the \u201cBackground\u201d section above.","Turning now to , an example implementation of the RCU barrier expedited component B for use with TINY_RCU will now be described. For this implementation, which may be used with the uniprocessor system  of , a kernel module's exit code would take steps to stop updaters  from posting new callbacks (such as by stopping the kernel module's updater threads) before invoking the RCU barrier expedited component B. When the RCU barrier expedited component B is invoked, block  initializes a completion structure that is used to signal the RCU barrier expedited component when callback processing has ended. Block  then enqueues a special RCU callback A on the processor  by adding the callback to the end of the processor's RCU callback list  (see ). When invoked, the special RCU callback A will implement a callback function that manipulates the completion structure initialized in block  so that a signal is sent to the RCU barrier expedited component B to advise that the special callback has been processed to completion.","Block  explicitly forces callback processing to be performed by the processor . For example, if RCU callbacks are processed in a deferred manner by a Linux\u00ae kernel thread (kthread), block  may implement the TINY_RCU function called \u201cinvoke_rcu_kthread( )\u201d in order wake up the kthread. In block , the RCU barrier expedited component B sleeps until the special RCU callback A that was enqueued in block  is processed and signals the RCU barrier expedited component B to wake up. Insofar as the special RCU callback A represents the last callback on the processor's RCU callback list , the RCU barrier component B will return and the kernel module that invoked this component may be safely unloaded without leaving behind any unprocessed callbacks.","Blocks ,  and  of  are implemented by current versions of TINY_RCU. Block  of  represents an expediting operation that is not currently present in TINY_RCU. This difference may be seen in Code Listing 1 below, which sets forth example C language code that may be used to define the RCU barrier expedited component B for TINY_RCU. Lines 2-7 and 9-12 represent statements that are found in the TINY_RCU rcu_barrier_sched( ) helper function defined at lines 298-309 of the Linux\u00ae version 3.1 source code file Linux\/kernel\/rcutiny.c. With the exception of lines 3, 4 and 11, which are used for debugging purposes, these statements embody the operations of blocks ,  and  of . The statement at line 8 represents the new operation added by block .",{"@attributes":{"id":"p-0066","num":"0065"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Code Listing 1"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u20021","void rcu_barrier_expedited(void)"]},{"entry":[{},"\u20022","{"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u20023","struct rcu_synchronize rcu;"]},{"entry":[{},"\u20024","init_rcu_head_on_stack(&rcu.head);"]},{"entry":[{},"\u20025","init_completion(&rcu.completion);"]},{"entry":[{},"\u20026","\/* Will wake me after RCU finished. *\/"]},{"entry":[{},"\u20027","call_rcu_sched(&rcu.head, wakeme_after_rcu);"]},{"entry":[{},"\u20028","rcu_sched_qs( );"]},{"entry":[{},"\u20029","\/* Wait for it. *\/"]},{"entry":[{},"10","wait_for_completion(&rcu.completion);"]},{"entry":[{},"11","destroy_rcu_head_on_stack(&rcu.head);"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"12","}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},"Turning now to , an example implementation of the RCU barrier expedited component B for use with TINY_PREEMPT_RCU will now be described. This implementation is substantially similar to the TINY_RCU implementation of , except that block  is replaced with block A, representing a different operation. Block B invokes the expedited grace period component B of . This component may be implemented by the rcu_synchronize_expedited( ) function set forth at lines 765-823 of the Linux\u00ae version 3.1 source code file Linux\/kernel\/rcutiny_plugin.h. When used in the RCU expedited barrier component B, the rcu_synchronize_expedited( ) function will cause any needed priority boosting to occur if there are readers  blocked within their RCU read-side critical sections. The presence of such blocked readers  will be indicated by the blocked task tracking information B- in the rcu_preempt_ctrlblk structure B of . This will help expedite the end of a grace period so that the special RCU callback A enqueued in block  can be processed more quickly.","Blocks ,  and  of  are implemented by current versions of TINY_PREEMPT_RCU. Block A of  represents an expediting operation that is not currently present in TINY_PREEMPT_RCU. These differences may be seen in Code Listing 2 below, which sets forth example C language code that may be used to define the RCU barrier expedited component B for TINY_PREEMPT_RCU. Lines 2-7 and 9-12 represent statements that are found in TINY_PREEMPT_RCU rcu_barrier( ) function defined at lines 700-711 of the Linux\u00ae version 3.1 source code file Linux\/kernel\/rcutiny_plugin.h. With the exception of lines 3, 4 and 11, which are used for debugging purposes, these statements embody the operations of blocks ,  and  of . The statement at line 8 represents the new operation added by block A.",{"@attributes":{"id":"p-0069","num":"0068"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Code Listing 2"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"168pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u20021","void rcu_barrier_expedited(void)"]},{"entry":[{},"\u20022","{"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"154pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u20023","struct rcu_synchronize rcu;"]},{"entry":[{},"\u20024","init_rcu_head_on_stack(&rcu.head);"]},{"entry":[{},"\u20025","init_completion(&rcu.completion);"]},{"entry":[{},"\u20026","\/* Will wake me after RCU finished. *\/"]},{"entry":[{},"\u20027","call_rcu(&rcu.head, wakeme_after_rcu);"]},{"entry":[{},"\u20028","synchronize_rcu_expedited( );"]},{"entry":[{},"\u20029","\/* Wait for it. *\/"]},{"entry":[{},"10","wait_for_completion(&rcu.completion);"]},{"entry":[{},"11","destroy_rcu_head_on_stack(&rcu.head);"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"168pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"12","}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},"Turning now to , an example implementation of the RCU barrier expedited component B for use with TREE_RCU will now be described. For this implementation, which may used with the multiprocessor system A of , a kernel module's exit code would take steps to stop updaters  from posting new callbacks (such as by stopping the module's updater threads) before invoking the RCU barrier expedited component B. When the RCU barrier expedited component B is invoked, block  initializes a completion structure that is used to signal the RCU barrier expedited component when callback processing has ended. Block  initializes an RCU barrier processor counter that is used to track the invocation of a special RCU callback A on each of system A's multiple processors . Block  is then implemented to enqueue the special RCU callback A on each processor , while incrementing the RCU barrier processor counter as each special callback is posted. When invoked, the special RCU callback A will implement a callback function that decrements the RCU barrier processor counter and tests it for zero. The special RCU callback A that detects this condition will then manipulate the completion structure initialized in block  so that a signal is sent to the RCU barrier expedited component B to advise that the special RCU callback has been processed to completion.","Once the special RCU callbacks A are enqueued, blocks  and  are used to expedite callback invocation. Block  forces each processor  to take note a new grace period. This can be accomplished by calling the \u201cinvoke_rcu_core\u201d function found at lines 1501-1504 of the Linux\u00ae version 3.1 source code file Linux\/kernel\/rcutree.c. This function wakes up the RCU kthread associated each processor , which in turn causes the processor to acknowledge the new grace period. Block  then forces a quiescent state on each processor . This may be accomplished by calling the TREE_RCU version of the force_quiescent_state( ) function found at lines 1424-1427 of the Linux\u00ae version 3.1 source code file Linux\/kernel\/rcutree.c. Invoking this function forces a rescheduling action on each processor .","Block  causes the operations of blocks  and  to be repeated until all of the special RCU callbacks A enqueued in block  have been invoked. This repetition is needed because each processor  in the multiprocessor system A might have an arbitrarily large number of pending RCU callbacks enqueued on the various portions of their callback lists  that are associated with different grace periods. Even though blocks  and  might forced the end of an existing grace period, it might take more than one invocation cycle to force the end of the old grace period, then begin a new grace period, then force each processor through a quiescent state, then report the quiescent state up the rcu_node hierarchy, and then actually process the callbacks. Eventually, the repeated cycling of blocks  and  will cause the completion structure that was initialized in block  to be reset, which will be detected in block . At this point, the kernel module that invoked the component RCU barrier expedited component B may be safely unloaded without leaving behind any unprocessed callbacks.","Blocks , ,  and  of  are implemented by current versions of TREE_RCU. Blocks  and  of  represent expediting operations that are not currently present TREE_RCU. This difference may be seen in Code Listing 3 below, which sets forth example C language code that may be used to define the RCU barrier expedited component B for TREE_RCU. Lines 2-21 and 30-34 represent statements that are found in the TREE_RCU rcu_barrier( ) helper function defined at lines 1782-1807 of the Linux\u00ae version 3.1 source code file Linux\/kernel\/rcutree.c. With the exception of lines 4-7 and 33, which are used for mutex locking purposes, these statements embody the operations of blocks , ,  and  of . The statements at lines 22-29 represents the new operations added by blocks -. In addition, lines 36-40 of Code Listing 3 set forth a new function called fqs_wrapper( ) that may be used for block . This function calls the existing force_quiescent_state function for TREE_RCU found at lines 1424-1427 of the Linux\u00ae version 3.1 source code file Linux\/kernel\/rcutree.c, which in turns calls the function \u201cset_need_resched( ) that forces rescheduling on each processor .",{"@attributes":{"id":"p-0074","num":"0073"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Code Listing 3"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["\u20021","static void_rcu_barrier_expedited(struct rcu_state *rsp,"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["\u20022","void (*call_rcu_func)(struct rcu_head *head,"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"147pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["\u20023","void (*func)(struct rcu_head *head)))"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["\u20024","{"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u20025","BUG_ON(in_interrupt( ));"]},{"entry":["\u20026","\/* Take mutex to serialize concurrent rcu_barrier( ) requests. *\/"]},{"entry":["\u20027","mutex_lock(&rcu_barrier_mutex);"]},{"entry":["\u20028","init_completion(&rcu_barrier_completion);"]},{"entry":["\u20029","\/*"]},{"entry":["10","\u2009* Initialize rcu_barrier_cpu_count to 1, then invoke"]},{"entry":["11","\u2009* rcu_barrier_func( ) on each CPU, so that each CPU also has"]},{"entry":["12","\u2009* incremented rcu_barrier_cpu_count. Only then is it safe to"]},{"entry":["13","\u2009* decrement rcu_barrier_cpu_count -- otherwise the first CPU"]},{"entry":["14","\u2009* might complete its grace period before all of the other CPUs"]},{"entry":["15","\u2009* did their increment, causing this function to return too"]},{"entry":["16","\u2009* early. Note that on_each_cpu( ) disables irqs, which prevents"]},{"entry":["17","\u2009* any CPUs from coming online or going offline until each"]},{"entry":["18","\u2009* online CPU has queued its RCU-barrier callback."]},{"entry":["19","\u2009*\/"]},{"entry":["20","atomic_set(&rcu_barrier_cpu_count, 1);"]},{"entry":["21","on_each_cpu(rcu_barrier_func, (void *)call_rcu_func, 1);"]},{"entry":["22","while (callbacks remain) {"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["23","\/*This requires that invoke_rcu_core( ) be modified to"]},{"entry":["24","\u2009* take a single parameter that it ignores. The"]},{"entry":["25","\u2009* invoke_rcu_core( ) function replaces the older use of"]},{"entry":[{},"\u2009raise_softirq( )."]},{"entry":["26","\u2009*\/"]},{"entry":["27","on_each_cpu(invoke_rcu_core, NULL, 1);"]},{"entry":["28","on_each_cpu(fqs_wrapper, rsp, 1);"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["29","}"]},{"entry":["30","if (atomic_dec_and_test(&rcu_barrier_cpu_count))"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["31","complete(&rcu_barrier_completion);"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["32","wait_for_completion(&rcu_barrier_completion);"]},{"entry":["33","mutex_unlock(&rcu_barrier_mutex);"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["34","}"]},{"entry":["35",{}]},{"entry":["36","void fqs_wrapper(void *rsp_in)"]},{"entry":["37","{"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["38","struct rcu_state *rsp = (struct rcu_state *)rsp_in;"]},{"entry":["39","force_quiescent_state(rsp, 0);"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["40","}"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}}},"Turning now to , an example implementation of the RCU barrier expedited component B for use with TREE_PREEMP_RCU will now be described. This implementation is substantially similar to the TREE_RCU implementation of , except that block  is replaced with block A representing a modified force quiescent state operation that forces RCU priority boosting on blocked readers . Block A may be implemented by invoking the TREE_PREEMPT_RCU version of the force_quiescent_state( ) function set forth at lines 1358-1420 of the Linux\u00ae version 3.1 source code file Linux\/kernel\/rcutree.c. This function calls a function known as \u201cforce_qs_rnp( )\u201d (see lines 1307-1352 of the Linux\u00ae version 3.1 source code file Linux\/kernel\/rcutree.c), which in turn calls an \u201crcu_initiate_boost( )\u201d function found at lines 1259-1283 of the Linux\u00ae version 3.1 source code file Linux\/kernel\/rcutree_plugin.h. Note that Code Listing 3 for TREE_RCU is the same for TREE_PREEMPT_RCU, the only difference being the use of a different version of force_quiescent_state( ) to produce the above-mentioned priority boosting on processors  that have blocked readers .","Accordingly, a technique for has been disclosed for expeditiously unloading operating system kernel modules that implement RCU callback processing code. It will be appreciated that the foregoing concepts may be variously embodied in any of a data processing system, a machine implemented method, and a computer program product in which programming logic is provided by one or more machine-useable storage media for use in controlling a data processing system to perform the required functions. Example embodiments of a data processing system and machine implemented method were previously described in connection with . With respect to a computer program product, digitally encoded program instructions may be stored on one or more computer-readable data storage media for use in controlling a computer or other digital machine or device to perform the required functions. The program instructions may be embodied as machine language code that is ready for loading and execution by the machine apparatus, or the program instructions may comprise a higher level language that can be assembled, compiled or interpreted into machine language. Example languages include, but are not limited to C, C++, assembly, to name but a few. When implemented on a machine comprising a processor, the program instructions combine with the processor to provide a particular machine that operates analogously to specific logic circuits, which themselves could be used to implement the disclosed subject matter.","Example data storage media for storing such program instructions are shown by reference numerals  (memory) and  (cache) of the uniprocessor system  of  and the multiprocessor system A of . The systems  and A may further include one or more secondary (or tertiary) storage devices (not shown) that could store the program instructions between system reboots. A further example of media that may be used to store the program instructions is shown by reference numeral  in . The media  are illustrated as being portable optical storage disks of the type that are conventionally used for commercial software sales, such as compact disk-read only memory (CD-ROM) disks, compact disk-read\/write (CD-R\/W) disks, and digital versatile disks (DVDs). Such media can store the program instructions either alone or in conjunction with an operating system or other software product that incorporates the required functionality. The data storage media could also be provided by portable magnetic storage media (such as floppy disks, flash memory sticks, etc.), or magnetic storage media combined with drive systems (e.g. disk drives). As is the case with the memory  and the cache  of , the storage media may be incorporated in data processing platforms that have integrated random access memory (RAM), read-only memory (ROM) or other semiconductor or solid state memory. More broadly, the storage media could comprise any electronic, magnetic, optical, infrared, semiconductor system or apparatus or device, or any other tangible entity representing a machine, manufacture or composition of matter that can contain, store, communicate, or transport the program instructions for use by or in connection with an instruction execution system, apparatus or device, such as a computer. For all of the above forms of storage media, when the program instructions are loaded into and executed by an instruction execution system, apparatus or device, the resultant programmed system, apparatus or device becomes a particular machine for practicing embodiments of the method(s) and system(s) described herein.","Although various example embodiments have been shown and described, it should be apparent that many variations and alternative embodiments could be implemented in accordance with the disclosure. It is understood, therefore, that the invention is not to be in any way limited except in accordance with the spirit of the appended claims and their equivalents."],"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION OF EXAMPLE EMBODIMENTS"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing and other features and advantages will be apparent from the following more particular description of example embodiments, as illustrated in the accompanying Drawings, in which:",{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIGS. 1A-1D"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIGS. 2A-2C"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 6","FIGS. 4 and 5"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 8A","FIG. 6"]},{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 8B","FIG. 6"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 8C","FIG. 6"]},{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 9","FIG. 6"]},{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 10A","FIG. 4","FIG. 6"]},{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 10B","FIG. 4","FIG. 6"]},{"@attributes":{"id":"p-0037","num":"0036"},"figref":["FIG. 11A","FIG. 5","FIG. 6"]},{"@attributes":{"id":"p-0038","num":"0037"},"figref":["FIG. 11B","FIG. 5","FIG. 6"]},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 12"}]},"DETDESC":[{},{}]}
