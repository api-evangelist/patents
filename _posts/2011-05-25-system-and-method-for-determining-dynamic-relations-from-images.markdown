---
title: System and method for determining dynamic relations from images
abstract: A system and method are provided for determining a dynamic relation tree based on images in an image collection. An example system includes a memory for storing computer executable instructions, and a processing unit for accessing the memory and executing the computer executable instructions. The computer executable instructions include an event classifier to classify main characters of images in an image collection as to an event identification based on events in which the main characters appear, wherein each main character is characterized as to at least one attribute; a relation determination engine to determine relation circles of the main characters; and a construction engine to construct a dynamic relation tree representative of relations among the main characters, where the dynamic relation tree provides representations of the positions of the main characters in the relation circles, and where views of the dynamic relation tree change when different time periods are specified.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08832080&OS=08832080&RS=08832080
owner: Hewlett-Packard Development Company, L.P.
number: 08832080
owner_city: Houston
owner_country: US
publication_date: 20110525
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","DETAILED DESCRIPTION"],"p":["With the advent of digital cameras and advance in massive storage technologies, people now have the ability to capture many casual images. It is not uncommon to find tens of thousands, if not hundreds of thousands of images taken over a number of years in a personal computer or database. The many images captured of individuals, whether alone or in group, can be used to provide useful information. A tool that facilitates determination of a dynamic relation tree based on images in the image collection would be beneficial.","In the following description, like reference numbers are used to identify like elements. Furthermore, the drawings are intended to illustrate major features of exemplary embodiments in a diagrammatic manner. The drawings are not intended to depict every feature of actual embodiments nor relative dimensions of the depicted elements, and are not drawn to scale.","An \u201cimage\u201d broadly refers to any type of visually perceptible content that may be rendered on a physical medium (e.g., a display monitor or a print medium). Images may be complete or partial versions of any type of digital or electronic image, including: an image that was captured by an image sensor (e.g., a video camera, a still image camera, or an optical scanner) or a processed (e.g., filtered, reformatted, enhanced or otherwise modified) version of such an image; a computer-generated bitmap or vector graphic image; a textual image (e.g., a bitmap image containing text); and an iconographic image.","A \u201ccomputer\u201d is any machine, device, or apparatus that processes data according to computer-readable instructions that are stored on a computer-readable medium either temporarily or permanently. A \u201csoftware application\u201d (also referred to as software, an application, computer software, a computer application, a program, and a computer program) is a set of machine-readable instructions that a computer can interpret and execute to perform one or more specific tasks. A \u201cdata file\u201d is a block of information that durably stores data for use by a software application.","The term \u201ccomputer-readable medium\u201d refers to any medium capable storing information that is readable by a machine (e.g., a computer system). Storage devices suitable for tangibly embodying these instructions and data include, but are not limited to, all forms of non-volatile computer-readable memory, including, for example, semiconductor memory devices, such as EPROM, EEPROM, and Flash memory devices, magnetic disks such as internal hard disks and removable hard disks, magneto-optical disks, DVD-ROM\/RAM, and CD-ROM\/RAM.","As used herein, the term \u201cincludes\u201d means includes but not limited to, the term \u201cincluding\u201d means including but not limited to. The term \u201cbased on\u201d means based at least in part on.","In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the present systems and methods. It will be apparent, however, to one skilled in the art that the present systems and methods may be practiced without these specific details. Reference in the specification to \u201can embodiment,\u201d \u201can example\u201d or similar language means that a particular feature, structure, or characteristic described in connection with the embodiment or example is included in at least that one example, but not necessarily in other examples. The various instances of the phrase \u201cin one embodiment\u201d or similar phrases in various places in the specification are not necessarily all referring to the same embodiment.","Identifying significant people and their relations in an image collection can provides useful semantic information that may be used in many applications, including image management, personalized advertisement, social networking, and homeland security.","Described herein are novel systems and methods for determining a dynamic relation tree based on images in an image collection. An example system includes a memory for storing computer executable instructions and a processing unit for accessing the memory and executing the computer executable instructions. The computer executable instructions includes an event classifier to classify main characters of images in an image collection as to an event identification based on events in which the main characters appear, where each image has a time stamp, and where each main character is characterized as to at least one attribute. The system includes a relation determination engine to determine relation circles of the main characters based on the event identifications, and a construction engine to construct a dynamic relation tree representative of relations among the main characters. The dynamic relation tree provides representations of the positions of the main characters in the relation circles. The views and constituents of the dynamic relation tree change when different time periods are specified during display.","In an example system and method herein, images are grouped into events based on time-based clustering, wherein the images comprise relationship information. The main characters of the images are classified as to an event identification based on events in which they appear. The relation circles of the main characters are determined based on the event identifications. A dynamic relation tree representative of relations among the main characters is constructed. The dynamic relation tree provides representations of the positions of the main characters in the relation circles. The representations and constituents of the dynamic relation tree change when different time periods are specified during display.","In a non-limiting example, the dynamic relation tree includes time information for estimating the relations between main characters so that people's relations can be estimated more accurately. The views of a dynamic relation tree can be configured to change with the selection of time range, and thus can reveal more information than a static relation tree. For example, views of the dynamic relation tree can be used to show people's relations as they change over time.","In an example system and method herein, an automated workflow can be created that derives a relation tree for main characters in an image collection by integrating technologies for face clustering and facial feature analysis, together with contextual information. Since people's relations may evolve over time, for image collections spanning a period of time (e.g., a number of years), the dynamic relation tree is constructed to visually represent people's relations.","In an example system and method herein, a workflow is provided that automatically estimates a relation tree from a collection of images. For example, a family relation tree can be derived from a collection of family photos. The dynamic relation trees herein can provide more accurate estimation of the relations of individuals. The dynamic relation trees herein can be used to provide more details of people's relations. The dynamic relation tree also can provide a life-log to remind people of events and changes in their lives with glance views.","In another example, an example system and method are provided for an automated workflow for deriving family relations and social circles from image collections. The system and method integrates results from face clustering and face analysis technologies, together with people co-occurrence and position information using learning algorithms and heuristic rules, to construct the dynamic relation tree. Views of the dynamic relation tree are created based on timestamps and event detection to show the evolution of people's life and relations.",{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 1A","b":["10","14","12","10","12","14","12","10"]},"An example source of images is personal photos of a consumer taken of family members and\/or friends. As non-limiting examples, the images can be photos taken during an event, including, a wedding, a christening, a birthday party, a holiday celebration (such as but not limited to Christmas, Memorial Day, Independence Day, July 4, Easter, etc), a vacation, or any other occasion. Another example source is images of an event captured by an image sensor of, e.g., entertainment or sports celebrities, or reality television individuals. The images can be taken of one or more members of a family at an event, such as but not limited to an attraction at an amusement park. In an example use scenario, a system and method disclosed herein is applied to images in a database of images captured at events, such as but not limited to images captured using imaging devices (such as but not limited to surveillance devices, or film footage) of an area located at an airport, a stadium, a restaurant, a mall, outside an office building or residence, etc. In various examples, each image collection can be located in a separate folder in a database, or distributed over several folders. The image collection also may be distributed over multiple locations, such as but not limited to, over multiple machines, over multiple servers (including servers in the cloud), over multiple websites, etc. it will be appreciated that other sources are possible.",{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 1B","b":["140","10","140","142","144","146","142","140","142","144","140","146","140","148","146"]},"A user may interact (e.g., enter commands or data) with the computer system  using one or more input devices  (e.g., a keyboard, a computer mouse, a microphone, joystick, touch screen, and touch pad). Information may be presented through a user interface that is displayed to a user on the display  (implemented by, e.g., a display monitor), which is controlled by a display controller  (implemented by, e.g., a video graphics card). The computer system  also typically includes peripheral output devices, such as speakers and a printer. One or more remote computers may be connected to the computer system  through a network interface card (NIC) .","As shown in , the system memory  also stores the relation tree generation system , a graphics driver , and processing information  that includes input data, processing data, and output data. In some examples, the relation tree generation system  interfaces with the graphics driver  to present a user interface on the display  for managing and controlling the operation of the relation tree generation system .","The relation tree generation system  can include discrete data processing components, each of which may be in the form of any one of various commercially available data processing chips. In some implementations, the relation tree generation system  is embedded in the hardware of any one of a wide variety of digital and analog computer devices, including desktop, workstation, and server computers. In some examples, the relation tree generation system  executes process instructions (e.g., machine-readable instructions, such as but not limited to computer software and firmware) in the process of implementing the methods that are described herein. These process instructions, as well as the data generated in the course of theft execution, are stored in one or more computer-readable media. Storage devices suitable for tangibly embodying these instructions and data include all forms of non-volatile computer-readable memory, including, for example, semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices, magnetic disks such as internal hard disks and removable hard disks, magneto-optical disks, DVD-ROM\/RAM, and CD-ROM\/RAM.","The principles set forth in the herein extend equally to any alternative configuration in which relation tree generation system  has access to image collection . As such, alternative examples within the scope of the principles of the present specification include examples in which the relation tree generation system  is implemented by the same computer system, examples in which the functionality of the relation tree generation system  is implemented by a multiple interconnected computers (e.g., a server in a data center and a user's client machine), examples in which the relation tree generation system  communicates with portions of computer system  directly through a bus without intermediary network devices, and examples in which the relation tree generation system  has stored local copies of image collection .","Referring now to , a block diagram is shown of an illustrative functionality  implemented by relation tree generation system  for determining a dynamic relation tree based on images, consistent with the principles described herein. Each module in the diagram represents an element of functionality performed by the processing unit . Arrows between the modules represent the communication and interoperability among the modules. In brief, image data representative of images in an image collection is received in block . Event classification of the images is performed in block  based on the image data. Relation determination is performed in block , and relation tree construction is performed in bock  to provide the dynamic relation tree .","Referring to block , image data representative of images in an image collection is received. The image data can include metadata connected with an image, such as but not limited to a time stamp that indicates when the image was captured. In an example, image data includes pixel values of the image. The image data can be data indicative of face regions of individuals in the images.","Referring to block , to classify images as to an event, event classification is performed by a module based on the image data. In an example, the event classification is performed by a module on image data resulting from the operation of a face detector on the images of the image collection. In an example, an event classifier is applied to classify main characters of images in an image collection as to an event identification, based on events in which the main characters appear, where each image has a time stamp. Each main character is characterized as to at least one attribute.","In an example, a face detector is applied to locate the main characters in the images based on image data. Face detection is applied to detect at least one face region in the images, for example, using a face detection algorithm. The face detection can be performed on the image forming elements of the images. Face detection can be performed by the same module that applies the event classification or by a different module. In an example, all images in the image collection are processed in order to detect faces. In another example, it may be desirable to limit the face detection to only some of the images in the collection, particularly if the image collection is very large.  shows examples of images from an image collection to which a face detector can be applied to identify the main characters.","Face clustering can be applied to group the detected faces into respective clusters representing a sub-set of images from the image collection. The detected faces, represented as image data representative of a face, can be clustered into sub-sets that include images having at least one face in common. Non-limiting examples of applicable clustering analysis include a hierarchical clustering analysis, such as an agglomerative hierarchical clustering, or a partitional clustering analysis, such as k-means clustering, fuzzy c-means clustering, or quality threshold (QT) clustering. In an example, an unsupervised machine learning tool is applied for clustering the image collection. A result of the image clustering is the identification of image dusters. The common face is the person who is considered the subject of the image cluster. The common face of an image duster can be a main character. Accordingly, multiple dusters may contain a proportion of images that are the same, but that have several people therein such that different people are the subject of the respective dusters. According to an example, major face dusters are defined that include sub-sets of images from the collection in which the face of the main characters appear as the common face. An image cluster in which a main character appears in a large number of images can be classified as a major cluster. The number required to classify an image cluster as a major cluster can be a predetermined number (e.g., a number between 5 and 10), or it can be a proportion of the number of images in the collection (such as 10% for example), or can be determined in response to the number of dusters that have been determined. For example, the largest 10% of determined image dusters can be classified as major dusters that is to say, major dusters can be defined as those which are the largest sub-sets of images from those generated. It will be appreciated that other ways of categorizing the sub-sets can be used, and the above is not intended to be limiting. In an example, all sub-sets may be classified as major dusters. With the face clustering technology, images of the main characters can be automatically collected.","Each main character can be characterized as to at least one attribute. The characterization of a main character as to at least one attribute can be performed by the same module that applies the event classification or by a different module. Examples of attributes include gender, age group, a face similarity measure, co-appearances, and relative positions information. In an example, demographic assessment can be performed on each face duster to determine the gender and age group of each main character. For example, age estimation can be performed to classify the individual as a baby, a child, a youth, an adult, a senior, or other age-based classification. In an example, a face similarity measure is used to find people who look alike, which can serve as a due to the identity of blood relatives. The face similarity measure can be computed between pairs of image dusters based on the main character's face, and can be used to identify people who look alike and can serve as a due to identify blood relatives. In an example, people's co-appearances and relative positions in the images are extracted to derive closeness among people. For example, people who co-appear a lot in images can be determined as dose to each other. Levels of intimacy also can be determined. People's positions in images also can be an indication of closeness. For example, face-touching positions can appear between intimate people such as couple, lovers, parent-child, grandparent-grandchild, and siblings.","An example system and method for characterizing a main character is described in U.S. patent application Ser. No. 12\/637,977, titled \u201cEstimating Relation Tree From Image Collections Based On Face Analysis,\u201d filed Dec. 15, 2009, which is incorporated by reference in its entirety, including drawings.","A non-limiting example of the results of implementation of the characterization of main characters is as follows. Example collections of family images are used. The family structure includes husband, wife and children, extended family members, relatives and friends. Each image collection dataset includes 1000 to 3000 images that span 7 to 9 years. A workflow is applied to estimate relations. Table 1 shows the major face dusters obtained from an image collection with face clustering and the demographic estimation results on each cluster. There are 28 major dusters (numbered 1 to 28), sorted by the cluster size. The common face in a major face cluster can be determined as a main character. Demographic analysis is performed on the cluster level, which aggregates results from each face in a major cluster. The accuracy can be higher than on the individual face level. The error in estimation of the gender can be reduced if the image cluster is larger.",{"@attributes":{"id":"p-0040","num":"0039"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Major face clusters resulting from"},{"entry":"an example family image collection"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"10"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"14pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Cluster No.","1","2","3","4","5","6","7","8","9"]},{"entry":{"@attributes":{"namest":"1","nameend":"10","align":"center","rowsep":"1"}}},{"entry":["Cluster size","447","274","254","224","92","74","69","65","58"]},{"entry":["Est. age","C","C","A","A","C","A","S","S","S"]},{"entry":["Est. gender","F","F","F","M","M","F","M","M","F"]},{"entry":{"@attributes":{"namest":"1","nameend":"10","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"10"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"14pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Cluster No.","10","11","12","13","14","15","16","17","18"]},{"entry":{"@attributes":{"namest":"1","nameend":"10","align":"center","rowsep":"1"}}},{"entry":["Cluster size","43","34","30","18","18","11","9","9","9"]},{"entry":["Est. age","S","A","S","A","C","C","A","A","A"]},{"entry":["Est. gender","M","F","F","M","M","M","M","M","F"]},{"entry":{"@attributes":{"namest":"1","nameend":"10","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"11"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"11","colwidth":"14pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Cluster No.","19","20","21","22","23","24","25","26","27","28"]},{"entry":{"@attributes":{"namest":"1","nameend":"11","align":"center","rowsep":"1"}}},{"entry":["Cluster size","9","8","8","8","8","7","7","7","6","5"]},{"entry":["Est. age","S","S","S","A","S","S","A","A","S","A"]},{"entry":["Est. gender","F","F","F","F","F","F","F","F","F","F"]},{"entry":{"@attributes":{"namest":"1","nameend":"11","align":"center","rowsep":"1"}}},{"entry":"(C\u2014Child; A\u2014Adult; S\u2014Senior; M\u2014Male; F\u2014Female)"}]}}]}}},"Event classification can be performed as follows. Images of the main characters are divided down into events according to time-based clustering. The time-based clustering can be performed based on the time stamp of the images. Each time cluster from the time-based clustering can be centered around the date of an event. Each time duster may correspond to a fixed time, and include images pertaining to an event. As non-limiting examples, an event may be a party within one day, or a Christmas vacation spanning multiple days. The event detection can be based on images captured in burst mode or in an even mode in which the time distance between consecutively taken images follows a certain distribution. In a non-limiting example, the time intervals between consecutive images can be sorted, from large to small, and the Ath and the Bth percentile values (A>B) can be identified. A threshold can be computed according to the following expression:\n\nAth percentile value+\u03b1\u00d7(Ath\u2212Bth)percentile value\u2003\u2003(1)\n\nwhere A, B and \u03b1 are values. The values of A, B and \u03b1 can be empirically determined. According to this example, images are broken down into events at intervals larger than the threshold, and each event is assigned an event identification.\n","Each main character is labeled with the event identification for each event in which he or she appears. Based on the event classification, an event-people graph can be generated.  shows a non-limiting example of an event-people graph. In the graph, the x-axis shows the time of events, and the y-axis shows the main characters identified in the images of the image collection. In , each dot indicates an event a main character is labeled with, and a line indicates that a main character appears in all events across the time span of the line. For example, main characters  appear in all events across the time span from about 2002 to 2008 (by the line), and main characters  appear in select events between about 2002 and 2008 (as indicated by the dots). The example event-people graph of  shows the main characters that appear in which events along the years. In another example, an event-people graph can be generated to show all main characters on the year level, or month level, or event level depending on what length of time the image collection spans.","In the event-people graph, time clustering along the y-axis at any fixed time can be used to show people who appear together in a certain event. The type of an event can be estimated by its length, such as but not limited to a party, a day-trip, a multi-day trip, a multi-day vacation, or a casual image capture, and the time associated with the event, including Valentine's Day, July 4th, Halloween, Thanksgiving, and Christmas. Identification of event-related objects in an image also can be used to identify the event classification, including a baseball game, a kids' party, and a wedding.","The event-people graph may be displayed on an interactive interface so that it can accept user commands. For example, the interactive interface may allow a user to select which main character to show on the event-people graph, and to zoom-in or zoom-out to control the time granularity of the display. In an example event-people graph, the density or thickness of lines may show how much a person appears in one event. A graphing engine can be used to generate the event-people graph. The event-people graph can be displayed to a user.","Based on the results of block , relation determination is performed in block . In an example, a relation determination engine is applied to determine relation circles of the main characters based on the event identifications. An estimation of people's relations is refined and social circles are determined. The relation circles can include the soda circles in an example.","The relation determination engine can be used to identify, as non-limiting examples, multiple people with over-lapping roles but non-overlapping in time. In an example, multiple spouses through different marriages at different times can be identified. A single person with multiple roles can be determined. The relation determination engine also can be used to identify people associated with events in life including child-birth, the passing away of family members, couples getting together, or breaking up. With the event identifications determined in block , social circles involved in the image collection can be identified. The nature and closeness of social circles such as but not limited to relatives, soccer team, colleagues, or classmates, may be estimated.","The main characters who are members of the same social circle can be identified. The image time stamps can provide semantic information. For instance, main characters who tend to appear in the same event may be determined as belonging to the same social circle. Family relations, including nuclear family, extended family, and other relatives or friends, can be determined.","Identification of members of a nuclear family is described. From all major image dusters, which can be defined as dusters including more than M images (e.g., M=4 or 5 or more), the main characters of larger dusters are first selected as candidates for members in the nuclear family including husband, wife and theft kids. In an example, the members of the nuclear family can be determined using a mixture model and associated algorithm, such as but not limited to a Gaussian mixture model (GMM), applied on the kid's dusters (baby, child, junior) and on adult dusters (adult, senior), respectively. Next, among these candidates, a link can be determined between each pair of people based on the number of co-occurrences, and with weights added to cases including exclusive images (i.e., not images with groups of people), closeness of positions of main characters in images with groups of people, face-touching positions, and baby-holding positions. In this example, main characters that are members of a nuclear family appear as those having strong links with one another.","Identification of members of the extended family can be determined as follows. Main characters that are parents and siblings of the husband or wife, as well as siblings' families can be determined as extended family members. They can be identified through facial similarity, co-appearances and positions in images. Main characters that are senior and that have high facial similarity with a husband or wife, and strong links with members of the nuclear family can be identified. The main characters that are senior and appear as a couple can be further identified as the parents of the husband or the wife according to the link between them and theft links with the husband or wife. A sibling of the husband or wife may have strong link not only with the husband or wife, but also with the corresponding parents in facial similarity and co-appearances. The sibling also may have co-occurrences with the children in the nuclear family, with intimate positions. A sibling's family can be identified as those who have strong links with the sibling, as well as co-appearances with nuclear family members and the corresponding parents.","Identification of main characters that are other relatives or friends can be determined as follows. People can be identified as relatives if they co-appear not only with the nuclear family, but also with the extended family members in a large amount of images or in different events. The remainder of the main characters can be determined as friend.","With the results of the operation in block , relation tree construction is performed in block  of  to provide a dynamic relation tree . The dynamic relation tree can be built with an interactive display supported according to a system and method described herein. A construction engine can be used to construct a dynamic relation tree that is representative of relations among the main characters. The dynamic relation tree can provide representations of the positions of the main characters in the relation circles. In an example of the relation tree, the dynamic relation tree can be displayed using an interactive interface so that the views and constituents of the dynamic relation tree change when different time periods are specified during the display. The interactive interface may provide a timeline, and a feature that allows a user to choose to zoom-in or zoom-out of a view of the dynamic relation tree based on the timeline. The interactive interface may provide a feature that allows a user to zoom in to the display of the dynamic relation tree to see main characters associated with specific events in certain years. In an example, the interactive interface provides a selective zoom feature that allows a user to select views of the dynamic relation tree that display images associated with certain branches of the dynamic relation tree. In an example, the interactive interface provides a selective time feature that allows a user to select views of the dynamic relation tree that display images of the dynamic relation tree associated with a specified time period.","A dynamic relation tree can be built that places the main characters in theft corresponding positions in the relation circles. The view of the dynamic relation tree can change when different time periods are specified. The dynamic relation tree is constructed using the identification of the main characters and theft relations from the analysis of the images in the image collection. However, each view of the dynamic relation tree may include only those main characters that appear in the specified time period of the display, such as but not limited to a certain year or a certain event. In an example, the dynamic relation tree can be used to show the evolution of relations among main characters over time. For example, a series of views of the dynamic relation tree can be used to reveal the evolution of relations over time. In an example, the dynamic relation tree can be used to provide a type of life-log with dance views that can be used to remind a user of events and people involved in the events. In an example, the dynamic relation tree can be constructed based on additional heuristic rules. For example, it can be constructed based on an assumption that blood relation estimation is more reliable than significant-other relation estimation.","A non-limiting example implementation of a dynamic relation tree is described. Example displays of views of a dynamic relation tree that can be displayed using an interactive interface are shown in . The views of  show are of a dynamic relation tree of an example image collection including family photos spanning 7 years, from 2001 to 2008. The event classification of the images is performed as described in block , relation determination is performed as described in block . The interactive interface can also be configured to display views showing main characters over different tune spans or at different events.","The relations of the main characters can be depicted in the views of the dynamic relation tree on the interactive interface. In each of the example displays of , the nuclear family members are placed in the center within the solid circle, with the parents (husband and wife) on the upper row and the children on the second row. The extended family members are positioned around the nuclear family within the dashed circle. The parents of the husband and wife, and where available the grandparents of the husband and wife, are positioned one level higher. The siblings and their families are put at about the same level as the nuclear family. Main characters associated with the husband are positioned next to him, and main characters associated with the wife are positioned next to her. Outside the dashed circle are other relatives and friends of the family. A dotted circle indicates either another family (that includes husband, wife and children), or a group of main character who tend to appear together. An arrow shows to what main character in the nuclear family a person or group of people is connected. It may be a single main character in the family or the entire nuclear family.","The displays of branches of the dynamic relation tree provide information on the evolution of relationships of the main characters. For example, the grandparents on both the husband's and wife's side appear together in images with the nuclear family each year except that the wife's parents did not appear in 2006. The nuclear family members appear with the great-grandmother in 2003. In that same year, the husband's sister appears in images with members of the nuclear family. The daughter's favorite teacher, a friend of the family, appears in images in 2004. Two neighbor families, classified as friends, appear in almost every year and the children appear with them several times.","In an example interactive interface, a feature may be included that allows a user to select to zoom-in or zoom-out of a dynamic relation tree view. For example, in the dynamic relation tree of , if a user instructs a command for the interactive interface to display a zoom into specific events in certain years, more details of the dynamic relations may be displayed. For example, a zoom level display in 2004 (such as that of ) indicates that the wife's parents appear in images together with members of the nuclear family during the event of Thanksgiving in 2004, and the husband's parents appear in images at Christmas with members of the nuclear family. The interactive interface can also be configured to display views showing how the physical features of main characters have evolved over time. For example, the interactive interface can be configured to show how the appearance of children changes when they grow from preschoolers to early teenagers. In an example, the interactive interface can be used to display views of the dynamic relation tree that present details about the activities and changes in the main characters and their relations.",{"@attributes":{"id":"p-0057","num":"0056"},"figref":["FIGS. 6A and 6B","FIGS. 5A and 6B","FIGS. 5A and 6B"]},{"@attributes":{"id":"p-0058","num":"0057"},"figref":["FIG. 7","FIG. 7","FIGS. 2 to 6B"],"b":["700","705","710","715","720","725"]},"In an example, the process includes displaying views of the dynamic relation tree, where the views are navigable by selective zooming to display images associated with certain branches of the dynamic relation tree. In another example, the process includes displaying views of the dynamic relation tree, where the views are navigable by selective zooming to display images associated with a specified time period. In another example, the process includes generating an event-people graph based on the dynamic relation tree, and displaying the event-people graph to a user. In another example, where generating an event-people graph comprises clustering images at fixed times to identify people who appear together in certain events, and estimating the nature and closeness of the relation circles based on the characteristics of the events.",{"@attributes":{"id":"p-0060","num":"0059"},"figref":["FIG. 8","FIG. 8","FIGS. 2 to 6B"],"b":["800","805","810","815","820","825","830","825"]},"In an example, the identification and characterization of the main characters includes identifying the main characters using face-based clustering, performing a demographic assessment of the images in the clusters, performing face-based similarity to identify relationships of main characters, and determining the co-appearance and positions of the main characters in the images.","Many modifications and variations of this invention can be made without departing from its spirit and scope, as will be apparent to those skilled in the art. The specific examples described herein are offered by way of example only, and the invention is to be limited only by the terms of the appended dams, along with the full scope of equivalents to which such dams are entitled.","As an illustration of the wide scope of the systems and methods described herein, the systems and methods described herein may be implemented on many different types of processing devices by program code comprising program instructions that are executable by the device processing subsystem. The software program instructions may include source code, object code, machine code, or any other stored data that is operable to cause a processing system to perform the methods and operations described herein. Other implementations may also be used, however, such as firmware or even appropriately designed hardware configured to carry out the methods and systems described herein.","It should be understood that as used in the description herein and throughout the claims that follow, the meaning of \u201ca,\u201d \u201can,\u201d and \u201cthe\u201d includes plural reference unless the context clearly dictates otherwise. Also, as used in the description herein and throughout the claims that follow, the meaning of \u201cin\u201d includes \u201cin\u201d and \u201con\u201d unless the context clearly dictates otherwise. Finally, as used in the description herein and throughout the claims that follow, the meanings of \u201cand\u201d and \u201cor\u201d include both the conjunctive and disjunctive and may be used interchangeably unless the context expressly dictates otherwise; the phrase \u201cexclusive or\u201d may be used to indicate situation where only the disjunctive meaning may apply.","All references cited herein are incorporated herein by reference in theft entirety and for all purposes to the same extent as if each individual publication or patent or patent application was specifically and individually indicated to be incorporated by reference in its entirety herein for all purposes. Discussion or citation of a reference herein will not be construed as an admission that such reference is prior art to the present invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF DRAWINGS","p":[{"@attributes":{"id":"p-0003","num":"0002"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0004","num":"0003"},"figref":["FIG. 1B","FIG. 1A"]},{"@attributes":{"id":"p-0005","num":"0004"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIGS. 5A-5F"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIGS. 6A and 6B"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
