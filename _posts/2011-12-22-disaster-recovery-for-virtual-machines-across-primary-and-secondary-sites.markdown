---
title: Disaster recovery for virtual machines across primary and secondary sites
abstract: A processing system stores a backup of virtual machine data on a storage device in a primary network storage system. A virtual machine running on a primary host machine, which is coupled to the primary network storage system, generates the virtual machine data. A replication software module replicates the backup of the virtual machine data from the primary network storage system to a secondary network storage system. In the event of a failure on the first network storage system, disaster recovery services module restores the virtual machine data from the secondary network storage system to cause the virtual machine to run on a secondary host machine coupled to the secondary network storage system, wherein the disaster recovery services module is coupled to the primary host machine and the secondary host machine over a network.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09020895&OS=09020895&RS=09020895
owner: NetApp, Inc.
number: 09020895
owner_city: Sunnyvale
owner_country: US
publication_date: 20111222
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATION","TECHNICAL FIELD","BACKGROUND","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"p":["This application claims the benefit of U.S. Provisional Patent Application No. 61\/427,439 filed Dec. 27, 2010, which is incorporated herein by reference.","This invention relates to the field of virtualization systems and, in particular, to disaster recovery for virtual machines across primary and secondary sites.","Virtualization is an abstraction that decouples the physical hardware from the operating system in a data processing system to deliver greater resource utilization and flexibility. Virtualization allows multiple virtual machines with heterogeneous operating systems (e.g., Windows\u2122, Linux\u2122, UNIX\u2122, etc.) and applications to run in isolation, side-by-side on the same physical host machine. A virtual machine is the representation of a physical machine by software. A virtual machine has its own set of virtual hardware (e.g., random access memory (RAM), central processing unit (CPU), network interface card (NIC), hard disks, etc.) upon which an operating system and applications are loaded. The operating system sees a consistent, normalized set of hardware regardless of the actual physical hardware components.","A conventional virtualized processing system may include a physical host machine which rims virtualization software such as a hypervisor. The hypervisor software runs on the physical host machine (e.g., a computer) and abstracts physical hardware (e.g., processors, memory, storage and networking resources, etc.) to be provisioned to one or more virtual machines.","A guest operating system (e.g., Windows\u2122, Linux\u2122, UNIX\u2122, etc.) may be installed on each of the virtual machines. The virtualization software presents the physical hardware of the host machine as virtual hardware to the guest operating system and applications running in the guest operating system. A user may access the virtual machine to perform computing tasks as if it were a physical machine. Generally, the virtualization process is completely transparent to the user.","Virtual machines may be backed up on a network storage system attached to the physical host running the virtual machine. For example, a persistent point-in-time image of the virtual machine may be captured and stored in the network storage system. Various forms of network-based storage systems exist today. These forms include network attached storage (NAS), storage area networks (SAN's), and others. Network-based storage systems are commonly used for a variety of purposes, such as providing multiple users with access to shared data, backing up critical data (e.g., by data mirroring), etc. A network-based storage system typically includes at least one storage server, which is a processing system configured to store and retrieve data on behalf of one or more client processing systems (client devices). In the context of NAS, a storage server may be a file server, which operates on behalf of one or more clients to store and manage shared files. The files may be stored in a storage system that includes one or more arrays of mass storage devices, such as magnetic or optical disks or tapes, by using a data storage scheme such as Redundant Array of Inexpensive Disks (RAID). In a SAN context, a storage server provides clients with block-level access to stored data, rather than file-level access. Some storage servers are capable of providing clients with both file-level access and block-level access. In the event of a disaster, the backup snapshot of the virtual machine stored on the storage system may be lost. The disaster may cause the loss of data and\/or functionality on the host machine, on the storage system, or on both. Such a disaster would prevent recovery of data and preclude restoration of the virtual machine because the backup data stored in the storage system may no longer be accessible.","A processing system stores a backup of virtual machine data on a storage device in a primary network storage system. A virtual machine running on a primary host machine, which is coupled to the primary network storage system, generates the virtual machine data. A replication software module replicates the backup of the virtual machine data from the primary network storage system to a secondary network storage system. In the event of a failure on the first network storage system, a disaster recovery services module restores the virtual machine data from the secondary network storage system to cause the virtual machine to run on a secondary host machine coupled to the secondary network storage system, wherein the disaster recovery services module is coupled to the primary host machine and the secondary host machine over a network.","In the following detailed description of embodiments of the invention, reference is made to the accompanying drawings in which like references indicate similar elements, and in which is shown by way of illustration specific embodiments in which the invention may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention, and it is to be understood that other embodiments may be utilized and that logical, mechanical, electrical, functional and other changes may be made without departing from the scope of the present invention. The following detailed description is, therefore, not to be taken in a limiting sense, and the scope of the present invention is defined only by the appended claims.","The present invention enables a disaster recovery solution for virtual machines making use of network storage systems. In a disaster recovery system, a virtual machine runs on a primary host machine, which is coupled to a primary network storage system. A virtual machine disaster recovery services module manages disaster recovery operations in the system. The disaster recovery services may be accessible by the rest of the system over a network, such as the Internet. At the direction of the disaster recovery services, data protection software running on the primary host machine backs up virtual machine data from the virtual machine on the primary network storage system. The disaster recovery system also includes a secondary host machine and a secondary network storage system. The secondary host machine and secondary network storage system may be at different location than the primary site. At the direction of the disaster recovery services, replication software in the network storage systems replicate the backup of the virtual machine data from the primary network storage system to the secondary network storage system. In the event of a disaster causing the primary host machine or the primary network storage system to fail, at the direction of the disaster recovery services data protection software on the secondary host machine restores the virtual machine data. The disaster recovery services identifies the host name of the secondary host machine and uses the host name in a restore to alternate host operation, or calls the data protection software to perform the restore operation, restoring the virtual machine to nm on the secondary host machine. The alternate host name may be found, for example, in a disaster recovery plan provided by the disaster recovery services.","The present invention enables disaster recovery according to application consistent recovery of virtual machines using network storage systems. The data protection software automates the process of disaster recovery, preventing a system administrator from having to manually restore the virtual machines. The disaster recovery solution provides an almost instant recovery of virtual machine data and prevents the loss of mission critical data in a virtualized computing environment. The disaster recovery solution restores a virtual machine on an alternate host (provided by disaster recovery services from the disaster recovery plan), despite metadata stored with the backup of the virtual machine indicating a different host machine. The disaster recovery ignores the identifier of the original host machine and performs a restore operation with an identifier for the alternate host machine.",{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 1","b":["100","130","140","140","140","145","140","145"]},"Host machine  may be a physical computer having a processor to execute instructions stored in memory. Host machine  may run a hypervisor , such as for example, Hyper-V in the Microsoft Windows Server\u00ae 2008 R2 operating system. Hypervisor  enables host machine  to host one or more virtual machines, each running its own operating system. Host machine  also runs data protection software . Data protection software enables backup and recovery of the virtual machines by making use of persistent point-in-time images, sometimes referred to as \u201csnapshots.\u201d In one embodiment, data protection software  includes NetApp\u00ae SnapManager\u00ae for Hyper-V (SMHV), developed by NetApp, Inc. of Sunnyvale, Calif. In one embodiment, host machine  also runs virtualization software  which may enable the provisioning of storage from the network storage system  for the virtual machines. In one embodiment, virtualization software  includes NetApp\u00ae SnapDrive\u00ae for Windows (SDW), developed by NetApp, Inc. One of the virtual machines may be virtual machine . In one embodiment, virtual machine  runs the same operating system as host machine . In other embodiments, virtual machine  may run a different operating system, such as for example, Microsoft Windows Server\u00ae 2003, 2008, or another operating system. Virtual machine  may also be running virtualization software .","In network environment , a user may desire to back-up virtual machine . Similarly, the back-up of the virtual machine may occur automatically at a scheduled interval or at the command of a system administrator. In one embodiment, the data protection software  initiates a request for a backup of virtual machine . Data protection software  generates the request for a backup of virtual machine  and sends the request to a volume shadow copy service (VSS) framework (not shown) in hypervisor . The VSS framework receives the backup request from data protection software  and manages the backup process. The VSS framework communicates with a VSS writer (not shown) to provide an application consistent backup snapshot. For example, when applications and services are running, the VSS writer responds to signals provided by the VSS framework to allow applications to prepare and quiesce their data stores for a backup operation (e.g., creation of a snapshot) and to prevent writes from occurring on the data while the snapshot is being created (e.g., writes are temporarily queued in memory). In one embodiment, preparing and quiescing the data includes completing all open transactions, rolling transaction logs, and flushing caches. In one embodiment, the VSS writer creates an XML description of the backup components and defines the restore process. The VSS framework receives notice from the VSS writer when the data is consistent and directs virtualization software  to create a snapshot. In one embodiment, virtualization software creates an application consistent snapshot on the storage system  using API's provided by the storage server .","In one embodiment, virtualization software  on the host machine  sends the snapshot to the network storage system . An operating system, such as operating system  as shown in , running on the storage server  of network storage system  stores the snapshot in storage, for example in a volume including Logical Unit Number (LUN) .",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 2","b":["200","210","230","240","235","245","200","220","230","290","230","235","290","210","200","210","230","235","270","272","230","235"]},"In one embodiment, each of primary network storage system  and secondary network storage system  include replication software modules ,  running replication software. In one embodiment, the replication software includes NetApp\u00ae SnapMirror\u00ae, developed by NetApp, Inc. Replication software modules ,  create a data protection relationship  between primary network storage system  and secondary network storage system . In one embodiment the data protection relationship  is a mirroring relationship, where data (e.g., volume ) from primary network storage system  is mirrored (i.e., copied or replicated) to secondary network storage system  (e.g., as volume ). In one embodiment, each of volumes ,  may include one or more LUNs. Thus, data stored in primary network storage system  may be replicated in secondary network storage system . In one embodiment, for example, data protection software , as controlled by DR services , initiates an application consistent backup of virtual machine  (as described above with respect to ), and stores the snapshot in primary network storage system . Replication software module , as controlled by DR services , replicates the snapshot to secondary network storage system . Secondary network storage system  may be at the same location as primary network storage system  or may be at a remote location to prevent both storage systems from being lost in the event of a disaster. The replicated data on secondary network storage system  allows for recovery of the data in the event of a disaster, as will be described below.","A disaster recovery policy (e.g., DR plan ) may define the frequency of both backups of virtual machine  (i.e., snapshots stored in primary network storage system ) and replications to secondary network storage system . A user, system administrator, or the system itself may set intervals for the backups and replication. The intervals may be based on a number of factors, including the nature of the data, available network bandwidth, and other factors. DR services  may retrieve this information (e.g., by making API calls) from host machine  and use it to create DR plan .","When data protection software module  initiates a back up of data for virtual machine , it also stores backup metadata with the snapshot. The backup metadata may include various pieces of information including, a virtual machine identifier, an identifier of the host machine on which the virtual machine was running (e.g., a host name), an indication of when the snapshot was taken, and other information. For virtual machine , running on primary host machine , the backup metadata will reference primary host machine . This backup metadata is also replicated to secondary network storage system  along with the virtual machine data.","In the event of a disaster, leading to the loss of data in primary network storage system , it is advantageous to restore the lost data from secondary network storage system . The disaster may include all host machines at the primary site failing, all storage systems at the primary site failing, or complete site failure, attributable, for example, to a natural disaster. In the event of a disaster, DR services  initiates a restore of the virtual machine data replicated to secondary network storage system . DR services  may make an API call for an API published by data protection software module  in order to initiate the restore to alternate host operation. The backup metadata, stored with the data from virtual machine , will indicate that the virtual machine was previously hosted by primary host machine . Due to the failure at the primary site, however, the virtual machine is restored on an alternate host (e.g., secondary host machine ). The alternate host that is the target of the restore operation may be defined by DR services  in DR plan . DR services , thus includes a mechanism to cause the restore operation to ignore the references to primary host machine  in the backup metadata and instead restore the virtual machine on secondary host machine . Additional details of the restore to alternate host operation are provided below.",{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIG. 3A","FIG. 2"],"b":["300","300","210","270","280"]},"Referring to , at block , method  performs a backup of a virtual machine. Data protection software module , running in primary host machine , captures a persistent point-in-time image of the virtual machine . The image may represent a snapshot of the data stored by virtual machine  at the instant the backup is performed. At block , method  stores the snapshot in primary network storage system . In one embodiment, data protection software module  stores the snapshot of virtual machine  in a dedicated volume  in primary network storage system .","At block , method  replicates the snapshot stored in volume  on primary network storage system  to a secondary network storage system . Replication software module  may form a mirroring relationship with replication software module  on the secondary network storage system  to mirror all data stored in primary network storage system  to secondary network storage system . The snapshot stored in volume  may be mirrored to identical volume  on secondary network storage system . Alternatively, replication software module  may use an existing mirroring relationship established with replication software module .","In the event of a disaster causing failure of either the primary host machine  or primary network storage system , at block , method  initiates a failover operation to migrate operations from the primary site to a secondary site. In one embodiment, the secondary site includes secondary host machine  and secondary network storage server . Data protection software module  on secondary host machine  may receive instructions from network administrator or from DR services  to initiate the failover. Data protection software module  executes a series of instructions to cause secondary host machine  and secondary network storage system  to take over the operations from the primary site. In one embodiment, during the failover process, DR services  breaks the mirror relationship with replication software  on primary network storage system . DR services  takes the secondary network storage system offline, brings it back online and scans for disks or volumes containing backup data, such as volume .","Upon identifying backup data, at block , method  restores any virtual machines having backup data on an alternate host. In one embodiment, data protection software  restores the virtual machine on secondary host machine . However, in other embodiments, data protection software  restores the virtual machine on another host machine.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 3B","FIG. 2"],"b":["360","360","210","270"]},"Referring to , at block , method  determines a host name of a secondary host on which the virtual machine is to be restored. In one embodiment, DR services  provides the host name of secondary host machine . The host name may be retrieved from DR plan , where it was input, for example by a system administrator. In another embodiment, DR plan  may include a number of different alternate host names, and DR services  may provide any available host name from DR plan .","At block , method  initiates a restore operation using the host name determined at block  instead of the host name in the backup metadata associated with the snapshot. As discussed above, DR services  initiates a back up of data for virtual machine  through data protection software module , it also stores backup metadata with the snapshot. This backup metadata includes a host name identifying the host machine on which the virtual machine was running (i.e., primary host machine ) when the backup was taken. The restore operation generally accesses a specified field in the backup metadata where the host name is stored. However, during the restore to alternate host operation, DR services , through data protection software module , causes the restore operation to ignore the host name in the backup metadata and instead provides the host name identified at block  as the designated host on which to restore the virtual machine. The restore operation retrieves the necessary data from the snapshot on secondary network storage system  and restores the virtual machine on the designated host machine (e.g., secondary host machine ). In one embodiment, DR services  initiates the restore to alternate host operation by calling a web service API. Data protection software module  provides the host name of the designated host machine and the web service API restores the virtual machine on the designated host. The web service API may be accessible over a wide area network  such as the Internet.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 4","b":["400","490","492","494","496","490","430","430","492","435","435","494","440","440","496","445","445","400","420","420","430","430","430","430","435","435","470","470","472","472"]},"Disaster recovery in clustered network environment  operates similarly to non-clustered network environment  discussed above with reference to . As in the previous embodiment, each of network storage systems A, B, A and B include replication software modules (not shown) running replication software. In this embodiment, the replication software modules create a data protection relationship between each network storage system in first storage cluster  and each network storage system in second storage cluster . For example, disaster recovery services , through replication software forms a mirror relationship  between primary network storage system A and either of secondary network storage systems A, B in first cluster . Replication software may also form a similar mirror relationship between each of secondary network storage systems A, B in second storage cluster  and any network storage system in first storage cluster .","In one embodiment, in the event of a failure wiping out all or part of first host cluster , including primary host machine A or primary host machine B, or first storage cluster , DR services , through data protection software module A or B running on secondary host machine A or B, may initiate a restore of the virtual machine data replicated to secondary network storage system A or B, for example by making an API call. DR services  initiates a restore to alternate host operation, and causes the restore operation to ignore the host name in the backup metadata and instead provides the host name of secondary host machine A or B as the designated host on which to restore the virtual machine. The restore operation retrieves the necessary data from the snapshot on secondary network storage system A or B and restores the virtual machine on the designated host machine (e.g., secondary host machine A or B).",{"@attributes":{"id":"p-0042","num":"0041"},"figref":["FIG. 5A","FIG. 2","FIG. 5A"],"b":["500","500","200","510","510","510","570","570","570","502","530","530","502"]},"Storage of data in storage units  is managed by storage servers  which receive and respond to various read and write requests from clients , directed to data stored in or to be stored in storage units . Storage units  constitute mass storage devices which can include, for example, flash memory, magnetic or optical disks, or tape drives, illustrated as disks  (disks A, B). The storage devices  can further be organized into arrays (not illustrated) implementing a Redundant Array of Inexpensive Disks\/Devices (RAID) scheme, whereby storage servers  access storage units  using one or more RAID protocols known in the art.","Storage servers  can provide file-level service such as used in a network-attached storage (NAS) environment, block-level service such as used in a storage area network (SAN) environment, a service which is capable of providing both file-level and block-level service, or any other service capable of providing other data access services. Although storage servers  are each illustrated as single units in , a storage server can, in other embodiments, constitute a separate network element or module (an \u201cN-module\u201d) and disk element or module (a \u201cD-module\u201d). In one embodiment, the D-module includes storage access components for servicing client requests. In contrast, the N-module includes functionality that enables client access to storage access components (e.g., the D-module) and may include protocol components, such as Common Internet File System (CIFS), Network File System (NFS), or an Internet Protocol (IP) module, for facilitating such connectivity. Details of a distributed architecture environment involving D-modules and N-modules are described further below with respect to  and embodiments of a D-module and an N-module are described further below with respect to .","In yet other embodiments, storage servers  are referred to as network storage subsystems. A network storage subsystem provides networked storage services for a specific application or purpose. Examples of such applications include database applications, web applications, Enterprise Resource Planning (ERP) applications, etc., e.g., implemented in a client. Examples of such purposes include file archiving, backup, mirroring, etc., provided, for example, on archive, backup, or secondary storage server connected to a primary storage server. A network storage subsystem can also be implemented with a collection of networked resources provided across multiple storage servers and\/or storage units","In the embodiment of , one of the storage servers (e.g., storage server A) functions as a primary provider of data storage services to client . Data storage requests from client  are serviced using disks A organized as one or more storage objects. A secondary storage server (e.g., storage server B) takes a standby role in a mirror relationship with the primary storage server, replicating storage objects from the primary storage server to storage objects organized on disks of the secondary storage server (e.g., disks B). In operation, the secondary storage server does not service requests from client  until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server, such event considered a failure at the primary storage server. Upon a failure at the primary storage server, requests from client  intended for the primary storage object are serviced using replicated data (i.e. the secondary storage object) at the secondary storage server.","It will be appreciated that in other embodiments, network storage system  may include more than two storage servers. In these cases, protection relationships may be operative between various storage servers in system  such that one or more primary storage objects from storage server A may be replicated to a storage server other than storage server B (not shown in this figure). Secondary storage objects may further implement protection relationships with other storage objects such that the secondary storage objects are replicated, e.g., to tertiary storage objects, to protect against failures with secondary storage objects. Accordingly, the description of a single-tier protection relationship between primary and secondary storage objects of storage servers  should be taken as illustrative only.",{"@attributes":{"id":"p-0048","num":"0047"},"figref":["FIG. 5B","FIG. 4","FIG. 5B","FIG. 5B"],"b":["520","520","400","520","510","510","510","571","510","525"]},"Nodes  may be operative as multiple functional components that cooperate to provide a distributed architecture of system . To that end, each node  may be organized as a network element or module (N-module A, B), a disk element or module (D-module A, B), and a management element or module (M-host A, B). In one embodiment, each module includes a processor and memory for carrying out respective module operations. For example, N-module  may include functionality that enables node  to connect to client  via network  and may include protocol components such as a media access layer, Internet Protocol (IP) layer, Transport Control Protocol (TCP) layer, User Datagram Protocol (UDP) layer, and other protocols known in the art.","In contrast, D-module  may connect to one or more storage devices  via cluster switching fabric  and may be operative to service access requests on devices . In one embodiment, the D-module  includes storage access components such as a storage abstraction layer supporting multi-protocol data access (e.g., Common Internet File System protocol, the Network File System protocol, and the Hypertext Transfer Protocol), a storage layer implementing storage protocols (e.g., RAID protocol), and a driver layer implementing storage device protocols (e.g., Small Computer Systems Interface protocol) for carrying out operations in support of storage access operations. In the embodiment shown in , a storage abstraction layer (e.g., file system) of the D-module divides the physical storage of devices  into storage objects. Requests received by node  (e.g., via N-module ) may thus include storage object identifiers to indicate a storage object on which to carry out the request.","Also operative in node  is M-host  which provides cluster services for node  by performing operations in support of a distributed storage system image, for instance, across system . M-host  provides cluster services by managing a data structure such as a RDB  (RDB A, B), which contains information used by N-module  to determine which D-module  \u201cowns\u201d (services) each storage object. The various instances of RDB  across respective nodes  may be updated regularly by M-host  using conventional protocols operative between each of the M-hosts (e.g., across network ) to bring them into synchronization with each other. A client request received by N-module  may then be routed to the appropriate D-module  for servicing to provide a distributed storage system image.","It should be noted that while  shows an equal number of N-modules and D-modules constituting a node in the illustrative system, there may be different number of N-modules and D-modules constituting a node in accordance with various embodiments of disaster recovery. For example, there may be a number of N-modules and D-modules of node A that does not reflect a one-to-one correspondence between the N-modules and D-modules of node B. As such, the description of a node comprising one N-module and one D-module for each node should be taken as illustrative only.",{"@attributes":{"id":"p-0053","num":"0052"},"figref":["FIG. 6","FIG. 5A"],"b":["510","510","602","610","612","620","640","650"]},"Memory  includes storage locations addressable by processor , network adapter  and storage adapter  for storing processor-executable instructions and data structures associated with disaster recovery. A storage operating system , portions of which are typically resident in memory  and executed by processor , functionally organizes the storage server by invoking operations in support of the storage services provided by the storage server. It will be apparent to those skilled in the art that other processing means may be used for executing instructions and other memory means, including various computer readable media, may be used for storing program instructions pertaining to the inventive techniques described herein. It will also be apparent that some or all of the functionality of the processor  and executable software can be implemented by hardware, such as integrated currents configured as programmable logic arrays, ASICs, and the like.","Network adapter  comprises one or more ports to couple the storage server to one or more clients over point-to-point links or a network. Thus, network adapter  includes the mechanical, electrical and signaling circuitry needed to couple the storage server to one or more clients over a network. Each client may communicate with the storage server over the network by exchanging discrete frames or packets of data according to pre-defined protocols, such as TCP\/IP.","Storage adapter  includes a plurality of ports having input\/output (I\/O) interface circuitry to couple the storage devices (e.g., disks) to bus  over an I\/O interconnect arrangement, such as a conventional high-performance FC or SAS link topology. Storage adapter  typically includes a device controller (not illustrated) comprising a processor and a memory for controlling the overall operation of the storage units in accordance with read and write commands received from storage operating system . As used herein, data written by a device controller in response to a write command is referred to as \u201cwrite data,\u201d whereas data read by device controller responsive to a read command is referred to as \u201cread data.\u201d","User console  enables an administrator to interface with the storage server to invoke operations and provide inputs to the storage server using a command line interface (CLI) or a graphical user interface (GUI). In one embodiment, user console  is implemented using a monitor and keyboard.","When implemented as a node of a cluster, such as cluster  of , the storage server further includes a cluster access adapter  (shown in phantom) having one or more ports to couple the node to other nodes in a cluster. In one embodiment, Ethernet is used as the clustering protocol and interconnect media, although it will apparent to one of skill in the art that other types of protocols and interconnects can by utilized within the cluster architecture.",{"@attributes":{"id":"p-0059","num":"0058"},"figref":["FIG. 7","FIG. 6","FIG. 6"],"b":["614","700","602","725"]},"Multi-protocol engine  includes a media access layer  of network drivers (e.g., gigabit Ethernet drivers) that interface with network protocol layers, such as the IP layer  and its supporting transport mechanisms, the TCP layer  and the User Datagram Protocol (UDP) layer . A file system protocol layer provides multi-protocol file access and, to that end, includes support for the Direct Access File System (DAFS) protocol , the NFS protocol , the CIFS protocol  and the Hypertext Transfer Protocol (HTTP) protocol . A VI layer  implements the VI architecture to provide direct access transport (DAT) capabilities, such as RDMA, as required by the DAFS protocol . An iSCSI driver layer  provides block protocol access over the TCP\/IP network protocol layers, while a FC driver layer  receives and transmits block access requests and responses to and from the storage server. In certain cases, a Fibre Channel over Ethernet (FCoE) layer (not shown) may also be operative in multi-protocol engine  to receive and transmit requests and responses to and from the storage server. The FC and iSCSI drivers provide respective FC- and iSCSI-specific access control to the blocks and, thus, manage exports of LUNS to either iSCSI or FCP or, alternatively, to both iSCSI and FCP when accessing blocks on the storage server.","The storage operating system also includes a series of software layers organized to form a storage server  that provides data paths for accessing information stored on storage devices. Information may include data received from a client, in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably, client data may be organized as one or more logical storage objects (e.g., volumes) that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one embodiment, the logical arrangement involves logical volume block number (vbn) spaces, wherein each volume is associated with a unique vbn.","File system  implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules (illustrated as a SCSI target module ). SCSI target module  is generally disposed between drivers ,  and file system  to provide a translation layer between the block (LUN) space and the file system space, where LUNs are represented as blocks. In one embodiment, file system  implements a WAFL (write anywhere file layout) file system having an on-disk format representation that is block-based using, e.g., 4 kilobyte (KB) blocks and using a data structure such as index nodes (\u201cinodes\u201d) to identify files and file attributes (such as creation time, access permissions, size and block location). File system  uses files to store metadata describing the layout of its file system, including an inode file, which directly or indirectly references (points to) the underlying data blocks of a file. In one embodiment, replication software module  resides within file system .","Operationally, a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter. A network driver such as layer  or layer  processes the packet and, if appropriate, passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There, file system  generates operations to load (retrieve) the requested data from the disks if it is not resident \u201cin core\u201d. i.e., in memory . If the information is not in memory, file system  accesses the inode file to retrieve a logical vbn and passes a message structure including the logical vbn to the RAID system . There, the logical vbn is mapped to a disk identifier and device block number (dbn) and sent to an appropriate driver of disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block(s) in memory for processing by the storage server. Upon completion of the request, the node (and operating system ) returns a reply to the client over the network.","It should be noted that the software \u201cpath\u201d through the storage operating system layers described above needed to perform data storage access for the client request received at the storage server adaptable to the teachings of the invention may alternatively be implemented in hardware. That is, in an alternate embodiment of the invention, a storage access request data path is implemented as logic circuitry embodied within a field programmable gate array (FPGA) or an application specific integrated circuit (ASIC). This type of hardware implementation increases the performance of the storage service provided by the storage server in response to a request issued by a client. Moreover, in another alternate embodiment of the invention, the processing elements of adapters ,  are configured to offload some or all of the packet processing and storage access operations, respectively, from processor , to thereby increase the performance of the storage service provided by the storage server. It is expressly contemplated that the various processes, architectures and procedures described herein can be implemented in hardware, firmware or software.","When implemented in a cluster, data access components of the storage operating system may be embodied as D-module  for accessing data stored on disk. In contrast, multi-protocol engine  may be embodied as N-module  to perform protocol termination with respect to a client issuing incoming access over the network, as well as to redirect the access requests to any other N-module in the cluster. A cluster services system  may further implement an M-host (e.g., M-host ) to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance, media access layer  may send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.","In addition, a cluster fabric (CF) interface module  (CF interface modules A, B) may facilitate intra-cluster communication between N-module  and D-module  using a CF protocol . For instance, D-module  may expose a CF application programming interface (API) to which N-module  (or another D-module not shown) issues calls. To that end, CF interface module  can be organized as a CF encoder\/decoder using local procedure calls (LPCs) and remote procedure calls (RPCs) to communicate a file system command to between D-modules residing on the same node and remote nodes, respectively.","The above description sets forth numerous specific details such as examples of specific systems, components, methods, and so forth, in order to provide a good understanding of several embodiments of the present invention. It will be apparent to one skilled in the art, however, that at least some embodiments of the present invention may be practiced without these specific details. In other instances, well-known components or methods are not described in detail or are presented in simple block diagram format in order to avoid unnecessarily obscuring the present invention. Thus, the specific details set forth are merely exemplary. Particular implementations may vary from these exemplary details and still be contemplated to be within the scope of the present invention.","Embodiments of the present invention include various operations, which are described above. These operations may be performed by hardware components, software, firmware, or a combination thereof. As used herein, the term \u201ccoupled to\u201d may mean coupled directly or indirectly through one or more intervening components. Any of the signals provided over various buses described herein may be time multiplexed with other signals and provided over one or more common buses. Additionally, the interconnection between circuit components or blocks may be shown as buses or as single signal lines. Each of the buses may alternatively be one or more single signal lines and each of the single signal lines may alternatively be buses.","Certain embodiments may be implemented as a computer program product that may include instructions stored on a machine-readable medium. These instructions may be used to program a general-purpose or special-purpose processor to perform the described operations. A machine-readable medium includes any mechanism for storing or transmitting information in a form (e.g., software, processing application) readable by a machine (e.g., a computer). The machine-readable medium may include, but is not limited to, magnetic storage medium (e.g., floppy diskette); optical storage medium (e.g., CD-ROM); magneto-optical storage medium; read-only memory (ROM); random-access memory (RAM); erasable programmable memory (e.g., EPROM and EEPROM); flash memory; or another type of medium suitable for storing electronic instructions.","Additionally, some embodiments may be practiced in distributed computing environments where the machine-readable medium is stored on and\/or executed by more than one computer system. In addition, the information transferred between computer systems may either be pulled or pushed across the communication medium connecting the computer systems.","The digital processing devices described herein may include one or more general-purpose processing devices such as a microprocessor or central processing unit, a controller, or the like. Alternatively, the digital processing device may include one or more special-purpose processing devices such as a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field programmable gate array (FPGA), or the like. In an alternative embodiment, for example, the digital processing device may be a network processor having multiple processors including a core unit and multiple microengines. Additionally, the digital processing device may include any combination of general-purpose processing devices and special-purpose processing device(s).","Although the operations of the methods herein are shown and described in a particular order, the order of the operations of each method may be altered so that certain operations may be performed in an inverse order or so that certain operation may be performed, at least in part, concurrently with other operations. In another embodiment, instructions or sub-operations of distinct operations may be in an intermittent and\/or alternating manner.","In the above descriptions, embodiments have been described in terms of objects in an object-oriented environment. It should be understood, that the invention is not limited to embodiments in object-oriented environments and that alternative embodiments may be implemented in other programming environments having characteristics similar to object-oriented concepts.","In the foregoing specification, the invention has been described with reference to specific exemplary embodiments thereof. It will, however, be evident that various modifications and changes may be made thereto without departing from the broader scope of the invention as set forth in the appended claims. The specification and drawings are, accordingly, to be regarded in an illustrative sense rather than a restrictive sense."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The present invention is illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings.",{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3A"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3B"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
