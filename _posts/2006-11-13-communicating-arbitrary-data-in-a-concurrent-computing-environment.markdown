---
title: Communicating arbitrary data in a concurrent computing environment
abstract: A communication protocol is provided for processes to send and receive arbitrary data in a concurrent computing environment. The communication protocol enables a process to send or receive arbitrary data without a user or programmer specifying the attributes of the arbitrary data. The communication protocol automatically determines the attributes of the arbitrary data, for example, the type and/or size of the data and sends information on the attributes of the data to a process to which the data is to be sent. Based on the information on the attributes of the data, the receiving process can allocate appropriate memory space for the data to be received.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08745254&OS=08745254&RS=08745254
owner: The MathWorks, Inc.
number: 08745254
owner_city: Natick
owner_country: US
publication_date: 20061113
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["COPYRIGHT","BACKGROUND","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"p":["A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure, as it appears in the Patent and Trademark Office patent file or records, but otherwise reserves all copyright rights whatsoever.","A technical computing environment provides mathematical and graphical tools for mathematical computation, data analysis, visualization and algorithm development. MATLAB\u00ae from The MathWorks, Inc. of Natick, Mass. is one example of technical computing environments. MATLAB\u00ae computing environment integrates numerical analysis, matrix computation, signal processing, and graphics in an easy-to-use environment where problems and solutions are expressed in familiar mathematical notation, without traditional programming. A technical computing environment can be used to solve complex engineering and scientific problems by developing mathematical models that simulate the problem.","A technical computing environment may allow scientists and engineers to interactively perform complex analysis and modeling in their familiar workstation environment. At times, users may want to use more than one computational unit executing a technical computing environment. Multiple computational units may be used to increase the computing power, to decrease computational time, to balance computing loads, or for other reasons as determined by one of skill in the art.","Applications that are traditionally used as desktop applications, may need to be modified to be able to utilize the computing power of concurrent computing. The term \u201cconcurrent computing\u201d can refer to parallel and\/or distributed computing\u2014that is, it encompasses computing performed simultaneously or nearly simultaneously, as well as computing performed on different computing units, regardless of the timing of the computation.","A computation unit may be any unit capable of carrying out processing. Computation units may be, but are not limited to: separate processors, cores on a single processor, hardware computing units, software computing units, software threads, portable devices, biological computing units, quantum computing units, etc.,","An exemplary embodiment provides a computing environment in which a large computational job can be performed concurrently by multiple processes. This concurrent computing environment may include a client that creates the job. The job may include one or more tasks. The client may send the job or tasks to the multiple processes for the concurrent execution of the job. In the embodiment, the processes may establish communication channels with other processes so that the processes can collaborate to execute the job. The processes execute the job and may return execution results to the client directly or indirectly via an intermediary agent, such as a job manager.","The exemplary embodiment provides a communication protocol for processes to send and receive arbitrary data in a concurrent computing environment. The communication protocol enables the processes to send or receive arbitrary data without a user or programmer specifying the attributes of the arbitrary data. In the embodiment, the communication protocol automatically identifies the attributes of the arbitrary data, for example, the type and\/or size of the data and sends information on the attributes of the data to a process to which the data is to be sent. Based on the information on the attributes of the data, the receiving process can allocate appropriate memory space for the data to be received.","In an exemplary communication protocol of the embodiment, a header message can be sent and received first with a fixed size and then payload messages can be sent and received. The header message may include information on the attributes of data to be sent and received, such as information on how to interpret the remainder of the header message and how many payload messages are needed to follow, and how to interpret the payload messages. When the data is small enough to fit in the header message, the data can be sent within the header message. The receiving process knows from the header message information how large the payload messages will be, and can allocate memory space for the payload messages accordingly.","In the exemplary embodiment, the communication protocol may allow the processes to perform error detection, user interrupt detection and\/or deadlock detection while retaining the underlying speed of the communication. When an error, user interrupt request or deadlock occurs and hence it is not possible to send and\/or receive a message, the communication protocol may enable the processes to cancel the communication. The communication protocol may also enable the processes to clear the partially sent message.","In one aspect, a method and a medium holding instructions executable in a computing device are provided for communicating data between processes in a concurrent computing environment. The attributes of data to be sent from a first process to a second process are determined. A header message is sent to the second process for providing information on the attributes of the data the second process.","In another aspect, a method and a medium holding instructions executable in a computing device are provided for communicating data between processes in a concurrent computing environment. A header message is received that includes information on the attributes of data to be received from a first process. Memory space is allocated for the data based on the information on the attributes of the data.","In another aspect, a system is provided for communicating data between processes in a concurrent computing environment. The system includes a first process for identifying attributes of data to be sent. The first process sends a header message for providing information on the attributes of data. The system also includes a second process for receiving the header message and for allocating memory space for the data based on the information on the attributes of the data in the header message.","An exemplary embodiment provides a communication protocol for computation units to send and receive arbitrary data between applications in a concurrent computing environment.","In one embodiment of the invention, a computation unit may need to have its local copy of the application or at least a part of the application. Between different instances of the application or different applications, there may need to be a way to communicate and pass messages and data, so that two or more computing units can collaborate with each other.","In an alternative embodiment of the invention, the application data or executable code may be loaded onto a computing unit prior to execution of the code.","Message passing is a form of communication used in concurrent computing in order to facilitate communication between two or more computing units. The applications execution on the computing units may be different instances of the same application, or they may be different versions of the same or similar application, or they may be entirely different applications and\/or their execution instances.","Communication is accomplished by the sending of messages from one computing unit to another computing unit. Various messages may include function invocation, signals, data packets, a combination of any of the above, etc. One example of a message passing protocol that establishes a communication channel between computing units is Message Passing Interface (MPI). MPI is a message-passing library specification defining a collection of subroutines and arguments used for communication among nodes running a parallel program on a distributed memory system. Message passing protocols are not limited to MPI, and alternative embodiments of the invention may employ different protocols.","In standard message passing programming, a user or programmer is expected to specify how large the data the processes are sending and receiving is so that the processes can allocate memory space for storing the data. However, some programming languages or environments, such as MATLAB\u00ae, control the memory allocation directly and do not allow a user to control the memory allocation. In MATLAB\u00ae, memory allocation is not controlled by the user, and hence it is difficult for the user to know how much memory is being allocated and used for the data type. Therefore, it is desired to provide new communication protocols for sending and receiving an arbitrary data type in a concurrent computing environment.","A communication protocol may enable computing units to send or receive arbitrary data without a user or programmer specifying the attributes of the arbitrary data, for example, the size and\/or type of the data. In an exemplary embodiment, the protocol specifies a fixed maximum size for a header message. In the exemplary embodiment, the communication protocol automatically identifies the attributes of the arbitrary data, and sends a header message with the attributes of the data to the computing units to which the data is to be sent. Based on the information on the attributes of the data in the header message, the receiving unit can allocate appropriate memory space for one or more payload messages to be received.","In the exemplary embodiment, the communication protocol may allow the computing units to perform error detection, user interrupt detection and\/or deadlock detection while retaining the underlying speed of the communication. For example, when a header message indicates that a payload message follows, but a payload message doe not follow, it means that an error occurs. When an error, user interrupt request or deadlock occurs so that it is not possible to send and\/or receive a message, the communication protocol enables the processes to cancel the communication. The communication protocol may also enable the processes to clean the partially sent message.","The exemplary embodiment provides an environment that enables a user to execute one or more computational jobs concurrently using multiple computing units. A job may include activities or tasks that are processed and\/or managed collectively. The job may be a batch processing job or interactive processing job. A task may define a technical computing command, such as a MATLAB\u00ae command, to be executed, and the number of arguments and any input data to the arguments. In the exemplary embodiment, the concurrent computing system may include a client for creating the job. The client may be a user or a software application setting out the computing tasks. One or more clients may be used to control concurrent processing units executing computing tasks. A client may be a stand-alone application, an application of the same type as executing on one or more of the processing units, a client running in a web interface, a client embedded in scientific hardware, an application running on a portable device, etc. The client may send the job to one or more computing units for the execution of the job. The computing units execute the job and return the execution results to the client. The computing units may establish communication channels with other computing units so that the computing units can collaborate to execute the job. In an exemplary embodiment, the job may be executed in batch processing or interactive processing. As such, the exemplary embodiment executes the job using a concurrent computing system.","The exemplary embodiment will be described for illustrative purposes relative to a MATLAB\u00ae-based technical computing environment. Although the exemplary embodiment will be described relative to a MATLAB\u00ae-based application, one of ordinary skill in the art will appreciate that the present invention may be applied to distributing the processing of technical computing tasks with other technical computing environments, such as technical computing environments using software products of LabVIEW\u00ae or MATRIXx from National Instruments, Inc., or Mathematica\u00ae from Wolfram Research, Inc., or Mathcad of Mathsoft Engineering & Education Inc., or Maple\u2122 from Maplesoft, a division of Waterloo Maple Inc., Comsol from Comsol AB of Sweden, GNU Octave.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 1","b":["100","102","106","104","106","102","104","105","12","103","103","116"]},"The memory  may include a computer system memory or random access memory (RAM), such as DRAM, SRAM, EDO RAM, etc. The memory  may include other types of memory as well, or combinations thereof. A human user may interact with the computing device  through a visual display device  such as a computer monitor, which may include a graphical user interface (GUI). The computing device  may include other I\/O devices such a keyboard  and a pointing device , for example a mouse, for receiving input from a user. Optionally, the keyboard  and the pointing device  may be connected to the visual display device . The computing device  may include other suitable conventional I\/O peripherals. Moreover, the computing device  may be any computer system such as a workstation, desktop computer, server, laptop, handheld computer or other form of computing or telecommunications device that is capable of communication and that has sufficient processor power and memory capacity to perform the operations described herein.","Additionally, the computing device  may include a network interface  to interface to a Local Area Network (LAN), Wide Area Network (WAN) or the Internet through a variety of connections including, but not limited to, standard telephone lines, LAN or WAN links (e.g., 802.11, T1, T3, 56 kb, X.25), broadband connections (e.g., ISDN, Frame Relay, ATM), wireless connections, high-speed interconnects (e.g., InfiniBand, gigabit Ethernet, Myrinet) or some combination of any or all of the above. The network interface  may include a built-in network adapter, network interface card, PCMCIA network card, card bus network adapter, wireless network adapter, USB network adapter, modem or any other device suitable for interfacing the computing device  to any type of network capable of communication and performing the operations described herein.","The computing device  may further include a storage device , such as a hard-drive or CD-ROM, for storing an operating system (OS) and for storing application software programs, such as the MATLAB\u00ae-based concurrent computing application or environment . The MATLAB\u00ae-based concurrent computing application or environment  may run on any operating system such as any of the versions of the Microsoft\u00ae Windows operating systems, the different releases of the Unix and Linux operating systems, any version of the MacOS\u00ae for Macintosh computers, any embedded operating system, any real-time operating system, any open source operating system, any proprietary operating system, any operating systems for mobile computing devices, or any other operating system capable of running on the computing device and performing the operations described herein. Furthermore, the operating system and the MATLAB\u00ae-based concurrent computing application or environment  can be run from a bootable CD, such as, for example, KNOPPIX\u00ae, a bootable CD for GNU\/Linux.","The concurrent computing application  may include communication protocols  that enable the concurrent computing application or environment  to communicate with other computing applications or processes in a concurrent computing system. The communication protocols  may use message passing primitives for the concurrent computing application  to establish communication channels with other computing processes, or applications in the concurrent computing system. Message passing systems may permit programs with separate address spaces to synchronize with one another and move data from the address space of one process to that of another by sending and receiving messages. The message passing primitives may be provided as a shared library, such as DLL files on Windows and .so files on UNIX. The shared library enables the application  to dynamically open and close communication channels. The communication protocols  will be described below in more detail with reference to .",{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 2","b":["200","120","200","150","130","170","140","120","150","170"]},"The computing application  may include a concurrent computing client application , or concurrent computing client, running on a client computer  and a concurrent computing application , or concurrent computing unit, running on a workstation . The concurrent computing client  may be in communication through the network communication channel  on the network  with one, some or all of the concurrent computing units . The concurrent computing units  can be hosted on the same workstation, or a single concurrent computing unit  can have a dedicated workstation . Alternatively, one or more of the concurrent computing units  can be hosted on the client .","The concurrent computing client  can be a technical computing software application that provides a technical computing and graphical modeling environment for generating block diagram models and to define mathematical algorithms for simulating models. The concurrent computing client  can be a MATLAB\u00ae-based client, which may include all or a portion of the functionality provided by the standalone desktop application of MATLAB\u00ae. Additionally, the concurrent computing client  can be any of the software programs available in the MATLAB\u00ae product family. Furthermore, the concurrent computing client  can be a custom software program or other software that accesses MATLAB\u00ae functionality via an interface, such as an application programming interface, or by other means. One ordinarily skilled in the art will appreciate the various combinations of client types that may access the functionality of the system.","With an application programming interface and\/or programming language of the concurrent computing client , functions can be defined representing a technical computing task to be executed by either a technical computing environment local to the client computer , or remote on the computing unit . The local technical computing environment may be part of the concurrent computing client , or a concurrent computing unit running on the client computer . The programming language includes mechanisms to define tasks to be distributed to a technical computing environment and to communicate the tasks to the concurrent computing units  on the workstations , or alternatively, on the client . Also, the application programming interface and programming language of the client  includes mechanisms to receive a result from the execution of technical computing of the task from another technical computing environment. Furthermore, the client  may provide a user interface that enables a user to specify the size of the collaboration of the concurrent computing units  for the execution of the job or tasks. The user interface may also enable a user to specify a concurrent computing unit  to be added to the collaboration or to be removed from the collaboration.","The concurrent computing unit  of the system  can be a technical computing software application that provides a technical computing environment for performing technical computing of tasks, such as those tasks defined or created by the concurrent computing client . The concurrent computing unit  can be a MATLAB\u00ae-based computing application, module, service, software component, or a session, which includes support for technical computing of functions defined in the programming language of MATLAB\u00ae. A session is an instance of a running concurrent computing unit  by which a concurrent computing client can connect and access its functionality. The concurrent computing unit  can include all the functionality and software components of the concurrent computing client , or it can just include those software components it may need to perform technical computing of tasks it receives for execution. The concurrent computing unit  may be configured to and capable of running any of the modules, libraries or software components of the MATLAB\u00ae product family. As such, the concurrent computing unit  may have all or a portion of the software components of MATLAB\u00ae installed on the workstation , or alternatively, accessible on another system in the network . The concurrent computing unit  is capable of performing technical computing of the task as if the concurrent computing client  was performing the technical computing in its own technical computing environment. The concurrent computing unit  also has mechanisms, to return a result generated by the technical computing of the task to the concurrent computing client .","A server  may be coupled to the network . The server  may include a job manager . The job manager  can provide control of delegating tasks and obtaining results in the concurrent computing system . The job manager  may provide an interface for managing a group of tasks collectively as a single unit called a job, and on behalf of a concurrent computing client , submitting those tasks making. up the job, and obtaining the results of each of the tasks until the job is completed. This eases the programming and integration burden on the concurrent computing client . For multiple task submissions from the concurrent computing client , the job manager  can manage and handle the delegations of the tasks to the concurrent computing units  and hold the results of the tasks on behalf of the concurrent computing client  for retrieval after the completion of technical computing of all the tasks.","Although the exemplary embodiment is discussed above in terms of the MATLAB\u00ae-based concurrent computing application across the computing devices of a client , server  and workstation , any other system and\/or deployment architecture that combines and\/or distributes one or more of the concurrent computing client , job manager  and concurrent computing units  across any other computing devices and operating systems available in the network  may be used. Alternatively, all or some of the software components of the MATLAB\u00ae-based concurrent computing application can run on a single computing device , such as the client , server  or the workstation .",{"@attributes":{"id":"p-0044","num":"0043"},"figref":["FIG. 3","FIG. 2"],"b":["200","250","302","250","304","260","270","305","270","306","308","310","250","270","312","260","250","313","250","260","314"]},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 4","b":["270","270","270","170","170","170","420","410","270","270","270","122","122","122","122","270","270","270","420","270","270","270","270","270","270","270","270","270"]},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIGS. 5-8","b":["122","122","122","510"]},"In the exemplary embodiment, the arrays may be dynamically typed. In a dynamically typed language, such as MATLAB\u00ae, types are assigned to each data value in memory at runtime, rather than assigning a type to a static, syntactic entity in the program source code. MATLAB\u00ae controls the memory allocation directly and does not allow a user to control the memory allocation. When a user assigns data type (a numeric, string, or structure array, for example) to a variable, MATLAB\u00ae allocates a block of memory and stores data in the block at runtime. The exemplary embodiment enables the process of dynamically typed languages to communicate with other processes using message passing primitives that require a user to specify, for example, the size of the data to be sent. Those of ordinary skill in the art, however, appreciate that although the exemplary embodiment is described relative to MATLAB\u00ae and MPI, other programming language processes and communication interfaces may be used.","Referring back to , depending upon the attributes of the data to be sent (step ), the computing unit may perform separate functions calls for different communication protocols. In the exemplary embodiment, the computing unit may perform three different functional calls, such as SendScalar, SendNumericArray, and SendSerializedData. If the data is a scalar, the computing unit may perform SendScalar (step ), which is described below with reference to . If the data is a numeric array, a character array or a logical array, the computing unit may perform SendNumericArray (step ), which is described below with reference to . If the data is a cell array or a structure array, the computing unit may perform SendSerializedData (step ), which is described below with reference to . Those of ordinary skill in the art will appreciate that the branch algorithm is illustrative and other implementations are possible for the communication protocols  in different embodiments. For example, the data can be sent using a generalized communication protocol (SendSerializedData in the embodiment) regardless of the types of the data. Exemplary code for the branch algorithm is provided as follows.",{"@attributes":{"id":"p-0049","num":"0048"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"#define SCALAR_MESSAGE 1"]},{"entry":[{},"#define NUMERIC_MESSAGE 2"]},{"entry":[{},"#define SERIAL_MESSAGE 3"]},{"entry":[{},"int labSend( mxArray * inData, int dest, int tag ) {"]},{"entry":[{},"\u2003if( mxIsScalarDouble( inData ) ) {"]},{"entry":[{},"\u2003\u2003return labSendScalar( inData, dest, tag );"]},{"entry":[{},"\u2003} else if( mxIsNumericDoubleArray( inData ) ) {"]},{"entry":[{},"\u2003\u2003return labSendNumericArray( inData, dest, tag );"]},{"entry":[{},"\u2003} else {"]},{"entry":[{},"\u2003\u2003return labSendSerialData( inData, dest, tag );"]},{"entry":[{},"\u2003}"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 6","b":["610","620","630"]},{"@attributes":{"id":"p-0051","num":"0050"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"196pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"int labSendScalar( mxArray * inData, int dest, int tag ) {"]},{"entry":[{},"\u2003int header[100];"]},{"entry":[{},"\u2003double scalarValue = mxGetScalarValue( inData );"]},{"entry":[{},"\u2003header[0] = SCALAR_MESSAGE;"]},{"entry":[{},"\u2003memcpy( &header[1], &scalarValue, sizeof( double ) );"]},{"entry":[{},"\u2003return MPI_Send( header, 100, MPI_INT, dest, tag,"]},{"entry":[{},"headerCommunicator );"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"In the exemplary code described above, the fourth line sets the first element of a header message to indicate that the data is a scalar type. From the first element of the header message, the receiver can expect that it will receive a scalar type of data. The fifth line packs the scalar type data into the header message. The sixth line sends the header message and returns MPI status.",{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 7","b":["710","720","730","740","750"]},{"@attributes":{"id":"p-0054","num":"0053"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"int labSendNumericArray( mxArray * inData, int dest, int tag ) {"},{"entry":"\u2003int header[100];"},{"entry":"\u2003int * dims = mxGetDimensions( inData );"},{"entry":"\u2003header[0] = NUMERIC_MESSAGE;"},{"entry":"\u2003header[1] = mxGetNumberOfDimensions( inData );"},{"entry":"\u2003memcpy( &header[2], dims, header[1] * sizeof( int ) );"},{"entry":"\u2003MPI_Send( header, 100, MPI_INT, dest, tag, headerCommunicator );"},{"entry":"\u2003return MPI_Send( mxGetData( inData ),"},{"entry":"mxGetNumberOfElements( data), MPI_DOUBLE,dest,"},{"entry":"tag, payloadCommunicator );"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"In the exemplary code described above, the fourth line sets the first element of a header message to indicate that the data is a numeric array. From the first element of the header message, the receiver can expect that it will receive a numeric array type of data. The fifth line sets the second element of the header message to specify the number of dimensions of the array data. The sixth line copies the number of elements per dimension into the header message. The seventh and eighth lines send the header message and payload messages, respectively,",{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 8","b":["810","820","830","840","850","860"]},{"@attributes":{"id":"p-0057","num":"0056"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"int labSendSerialData( mxArray * inData, int dest, int tag ) {"},{"entry":"\u2003int header[100];"},{"entry":"\u2003unsigned char * serialData;"},{"entry":"\u2003int serialLen;"},{"entry":"\u2003header[0] = SERIAL_MESSAGE;"},{"entry":"\u2003serializeMxData( inData, &serialData, &serialLen );"},{"entry":"\u2003header[1] = serialLen;"},{"entry":"\u2003MPI_Send( header, 100, MPI_INT, dest, tag, headerCommunicator );"},{"entry":"\u2003MPI_Send( serialData, serialLen, MPI_UNSIGNED_CHAR, dest,"},{"entry":"tag, payloadCommunicator );"},{"entry":"\u2003free( serialData );"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"In the exemplary code described above, the fifth line sets the first element of a header message to indicate that the data is serialized data. From the first element of the header message, the receiver can expect that it will receive serialized data. The function in the sixth line converts any arbitrary MATLAB\u00ae data type into a sequence of bytes. The seventh line sets the second element of the header message to specify the length of the serialized data. The eighth and ninth lines send the header message and one or more payload message. The tenth line frees the memory space allocated for the serialized data.",{"@attributes":{"id":"p-0059","num":"0058"},"figref":["FIGS. 9-12","FIG. 10","FIG. 11","FIG. 12"],"b":["122","122","122","910","920","930","940","950","122"]},{"@attributes":{"id":"p-0060","num":"0059"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"int labReceive( mxArray ** outData, int src, int tag ) {"},{"entry":"\u2003int header[100];"},{"entry":"\u2003MPI_Status stat;"},{"entry":"\u2003int act_src, act_tag;"},{"entry":"\u2003MPI_Recv( header, 100, MPI_INT, src, tag, headerCommunicator,"},{"entry":"&stat );"},{"entry":"\u2003act_src = stat.MPI_SOURCE; act_tag = stat.MPI_TAG;"},{"entry":"\u2003switch( header[0] ) {"},{"entry":"\u2003\u2003case SCALAR_MESSAGE:"},{"entry":"\u2003\u2003\u2003return labReceiveScalar( header, outData, act_src, act_tag );"},{"entry":"\u2003\u2003case NUMERIC_MESSAGE:"},{"entry":"\u2003\u2003\u2003return labReceiveNumericArray( header, outData, act_src,"},{"entry":"act_tag );"},{"entry":"\u2003\u2003case SERIAL_MESSAGE:"},{"entry":"\u2003\u2003\u2003return labReceiveSerialData( header, outData, act_src, act_tag"},{"entry":");"},{"entry":"\u2003}"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"In the exemplary code described above, the second line assigns 100 integer elements for a header message, which may be determined by the constraints of protocols as a programmer defined when writing the send and receive portions of the protocol. In the fifth line, a computing unit may receive a header message using an MPI primitive, such as MPI_Recv. The first element of the header message indicates the type of data to be received. From the seventh line, the communication protocol performs separate routines for receiving the data depending on the first element of the header message.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 10","b":["1010","1020"]},{"@attributes":{"id":"p-0063","num":"0062"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"int labReceiveScalar( int [ ] header, mxArray ** outData, int src, int"]},{"entry":[{},"tag ) {"]},{"entry":[{},"\u2003double scalarValue;"]},{"entry":[{},"\u2003memcpy( &scalarValue, &header[1], sizeof( double ) );"]},{"entry":[{},"\u2003*outData = mxCreateDoubleScalar( scalarValue );"]},{"entry":[{},"\u2003return MPI_SUCCESS;"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"In the exemplary code described above, the third line copies the payload value directly from the header message. The fourth line returns the payload value copied from the from the header message.",{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 11","b":["1110","1120","1130"]},{"@attributes":{"id":"p-0066","num":"0065"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"int labReceiveNumericArray( int [ ] header, mxArray ** outData, int"},{"entry":"src, int tag ) {"},{"entry":"\u2003MPI_Status stat;"},{"entry":"\u2003*outData = mxCreateNumericArray( header[1], &header[2],"},{"entry":"mxDOUBLE_CLASS, mxREAL );"},{"entry":"\u2003return MPI_Recv( mxGetData( *outData ), mxGetNumberOfElements("},{"entry":"*outData ), MPI_DOUBLE, src, tag, payloadCommunicator, &stat );"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"In the exemplary code, the third line allocates memory space for the data to be received. Since the first element of the header message indicates that the data is a numeric array, the memory space can be allocated based on the number of dimensions and the number of elements per dimension. The number of dimensions can be obtained from the second element of the header message and the number of elements per dimension can be obtained from the third element and thereafter. In the fourth line, one or more payload messages are received and stored in the allocated memory space.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 12","b":["1210","1220","1230"]},{"@attributes":{"id":"p-0069","num":"0068"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"int labReceiveSerialData( int [ ] header, mxArray ** outData, int src,"},{"entry":"int tag ) {"},{"entry":"\u2003int serialLen = header[1];"},{"entry":"\u2003unsigned char * serialData = (unsigned char*) malloc( serialLen *"},{"entry":"sizeof( unsigned char ) );"},{"entry":"\u2003MPI_Status stat;"},{"entry":"\u2003int rc = MPI_Recv( serrialData, serialLen, MPI_UNSIGNED_CHAR,"},{"entry":"src, tag, payloadCommunicator, &stat );"},{"entry":"\u2003if( rc == MPI_SUCCESS ) {"},{"entry":"\u2003\u2003*outData = deserializeMxData( serialData, serialLen );"},{"entry":"\u2003}"},{"entry":"\u2003return rc;"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"In the exemplary code, the length of serialized data to be received can be obtained from the second element of the header message (the second line). The third line allocates memory space for the serialized data to be received based on the length of the serialized data. In the fifth line, the serialized data is received via one or more payload messages and stored in the allocated memory space. The function in the seventh line de-serializes the received data.","The embodiment described above uses blocking message passing routines, such as MPI_Send and MPI_Receive. Blocking routines return only after the data buffer in the sending task is free for reuse, or block until the requested data is available in the data buffer in the receiving task. Some other embodiments may use a synchronous send to implement the blocking send. In the synchronous blocking communication, a send computing unit may send a message and block until the data buffer in the sending task is free for reuse and the receiving unit has started to receive the message.",{"@attributes":{"id":"p-0072","num":"0071"},"figref":["FIG. 13","FIGS. 6-9"],"b":["122","11","12","1310"]},"The computing unit may then perform error detection, user interrupt detection and\/or deadlock detection while the data is being sent or received (step ). The user may request, for example, by pressing CTRL-C that the program is interrupted. When the request is detected, the process may cancel the communication request. In an embodiment, a checkpoint called barrier synchronization point is used to check for any communication mismatch in a concurrent computing program. The barrier synchronization point can be placed anywhere in a concurrent computing program as desired. Once a node has reached the barrier synchronization point, the node suspends execution and becomes idle to wait for other nodes to reach the barrier synchronization point as well. No node can leave the barrier synchronization point until all nodes have entered the barrier synchronization point. If a node attempts to initiate communication with another node that has already entered the barrier synchronization point, an error is raised immediately. Once all the nodes have reached the barrier synchronization point, any message in the send\/receive buffer is flushed before resuming to normal execution to ensure that any communication mismatch before the barrier synchronization point does not continue past the barrier synchronization point. Each message to be flushed represents a communication mismatch and a warning or an error can be issued. Exemplary code for a communication with error detection, deadlock detection of user interrupt detection is provided as follows.",{"@attributes":{"id":"p-0074","num":"0073"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"int pollForCompletion( MPI_Request req, MPI_Status * pStat ) {"]},{"entry":[{},"\u2003int done = 0;"]},{"entry":[{},"\u2003int rc;"]},{"entry":[{},"\u2003while( !done ) {"]},{"entry":[{},"\u2003\u2003if( deadlockDetected( ) ||"]},{"entry":[{},"\u2003\u2003\u2003errorDetected( ) ||"]},{"entry":[{},"\u2003\u2003\u2003userCancelled( ) ) {"]},{"entry":[{},"\u2003\u2003\u2003MPI_Cancel( &req );"]},{"entry":[{},"\u2003\u2003\u2003return MPI_ERR_OTHER;"]},{"entry":[{},"\u2003\u2003}"]},{"entry":[{},"\u2003\u2003rc = MPI_Test( &req, &done, pStat );"]},{"entry":[{},"\u2003}"]},{"entry":[{},"\u2003return rc;"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"The exemplary code described above checks whether a communication is completed or one of deadlock, communication error or user interrupt (e.g., CTRL-C), occurs during the communication. The fifth to seventh lines check for communication error, deadlock or user interrupt. If communication error, deadlock or user interrupt is detected, the process cancels the communication request at the eighth line. If communication error, deadlock and user interrupt is not detected, the process sets the \u201cdone\u201d flag to 1 when the communication is complete (eleventh line).","To perform the error detection, deadlock detection or user interrupt detection during a communication, MPI_Send and MPI_Recv described above may be replaced by errDetectingSend and errDetectingReceive, which are described below with exemplary code.",{"@attributes":{"id":"p-0077","num":"0076"},"tables":{"@attributes":{"id":"TABLE-US-00010","num":"00010"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"int errDetectingSend( void * data, int count, MPI_Datatype type,"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003int destination, int tag, MPI_Comm comm ) {"]},{"entry":[{},"\u2003MPI_Request req;"]},{"entry":[{},"\u2003MPI_Status stat;"]},{"entry":[{},"\u2003int rc;"]},{"entry":[{},"\u2003rc = MPI_Isend( data, count, type, destination, tag, comm, &req );"]},{"entry":[{},"\u2003if( rc == MPI_SUCCESS ) {"]},{"entry":[{},"\u2003\u2003return pollForCompletion( req, &stat );"]},{"entry":[{},"\u2003} else {"]},{"entry":[{},"\u2003\u2003return rc;"]},{"entry":[{},"\u2003}"]},{"entry":[{},"}"]},{"entry":[{},"int errDetectingReceive( void * data, int count, MPI_Datatype type,"]},{"entry":[{},"\u2003\u2003\u2003\u2003\u2003\u2003int source, int tag, MPI_Comm comm,"]},{"entry":[{},"MPI_Status * pStat ) {"]},{"entry":[{},"\u2003MPI_Request req;"]},{"entry":[{},"\u2003int rc;"]},{"entry":[{},"\u2003rc = MPI_Isend( data, count, type, source, tag, comm, &req );"]},{"entry":[{},"\u2003if( rc == MPI_SUCCESS ) {"]},{"entry":[{},"\u2003\u2003return pollForCompletion( req, pStat );"]},{"entry":[{},"\u2003} else {"]},{"entry":[{},"\u2003\u2003return rc;"]},{"entry":[{},"\u2003}"]},{"entry":[{},"}"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Both methods described above initiate asynchronous communications, and wait for a process to complete using pollForCompletion, which is also described above.","The exemplary code described below shows how a partially transmitted message is flushed. The following method \u201cflushingReceive\u201d may be called repeatedly in a loop until it is determined that no further messages are available to be flushed.",{"@attributes":{"id":"p-0080","num":"0079"},"tables":{"@attributes":{"id":"TABLE-US-00011","num":"00011"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"int flushingReceive( mxArray ** outData, int src, int tag ) {"},{"entry":"\u2003int header[100];"},{"entry":"\u2003MPI_Status stat;"},{"entry":"\u2003int flag;"},{"entry":"\u2003\u2003MPI_Recv( header, 100, MPI_INT, src, tag, headerCommunicator,"},{"entry":"&stat );"},{"entry":"\u2003\u2003MPI_Iprobe( stat.MPI_SOURCE, stat.MPI_TAG,"},{"entry":"payloadCommunicator, &flag );"},{"entry":"\u2003\u2003if( !flag ) {"},{"entry":"\u2003\u2003\u2003return MPI_SUCCESS;"},{"entry":"\u2003\u2003} else {"},{"entry":"\u2003\u2003\u2003switch( header[0] ) {"},{"entry":"\u2003\u2003\u2003\u2003case SCALAR_MESSAGE:"},{"entry":"\u2003\u2003\u2003\u2003\u2003return labReceiveScalar( header, outData,"},{"entry":"stat.MPI_SOURCE, stat.MPI_TAG );"},{"entry":"\u2003\u2003\u2003\u2003case NUMERIC_MESSAGE:"},{"entry":"\u2003\u2003\u2003\u2003\u2003return labReceiveNumericArray( header, outData,"},{"entry":"stat.MPI_SOURCE, stat.MPI_TAG );"},{"entry":"\u2003\u2003\u2003\u2003case SERIAL_MESSAGE:"},{"entry":"\u2003\u2003\u2003\u2003\u2003return labReceiveSerialData( header, outData,"},{"entry":"stat.MPI_SOURCE, stat.MPI_TAG );"},{"entry":"\u2003\u2003\u2003}"},{"entry":"\u2003\u2003}"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"The communication error or deadlock is typically detected during sending of a payload message (the header has already been sent) or during receiving of a payload message (i.e. during the period where the expected sender has not yet started sending a payload message). If there is at least a header message to be received, the fifth line receives the header message. The six line checks if there is a payload present to receive. If there is an error detected, there may be no payload messages. If there is no error detected, the process continues to receive payload messages. Each process can send at most one mismatched header message before a \u201cflush\u201d is performed.","Alternatively, instead of barrier synchronization points, regions can be used to implement the exemplary embodiment. Nodes that use the region-based implementation do not suspend execution when they are leaving one region and entering another. In one embodiment, each message is packaged with information that identifies the region that the sending node is in so that the receiving node can determine if such message can be successfully received without error. The receiving node checks if the region information in the message is compatible with the region that the receiving node is currently in and an error is raised if there is an incompatibility between the regions. In another embodiment, a sending node queries the region that the receiving node is in and compares the region of the receiving node with the region that the sending node is currently in. If the receiving node is in a compatible region with the sending node, then a message is sent from the receiving node to the sending node. However, if the receiving node is in an incompatible region with the sending node, then a communication mismatch is detected. In yet another embodiment, a message is sent by a sending node without information on the region that the sending node is in and the receiving node queries the region that the sending node is in before the receiving node successfully receives the message. If the region of the receiving node is compatible with the region of the sending node, then the receiving node successfully receives the message. If the region of the receiving node is incompatible with the region of the sending node, then a communication mismatch is detected.","The error detection, user interrupt detection and deadlock detection are described in more detail in co-pending U.S. patent application Ser. No. 11\/488,432 filed Jul. 17, 2006 and entitled \u201cRECOVERABLE ERROR DETECTION FOR CONCURRENT COMPUTING PROGRAMS.\u201d The content of the application is incorporated herein by reference in its entirety.","When an error, user interrupt request or deadlock occurs, or when it is not possible to send and\/or receive data anymore, the communication protocols  may enable the computing unit to cancel the pending communication (). The communication protocols  may also enable the computing unit to clean memory space allocated for the partially sent message (step ).","Many alterations and modifications may be made by those having ordinary skill in the art without departing from the spirit and scope of the invention. Therefore, it must be expressly understood that the illustrated embodiments have been shown only for the purposes of example and should not be taken as limiting the invention, which is defined by the following claims. These claims are to be read as including what they set forth literally and also those equivalent elements which are insubstantially different, even though not identical in other respects to what is shown and described in the above illustrations."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing and other objects, aspects, features, and advantages of the invention will become more apparent and may be better understood by referring to the following description taken in conjunction with the accompanying drawings, in which:",{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 3","FIG. 2"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIGS. 5-8"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIGS. 9-12"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 13"}]},"DETDESC":[{},{}]}
