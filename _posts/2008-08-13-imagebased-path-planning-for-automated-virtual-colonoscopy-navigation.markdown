---
title: Image-based path planning for automated virtual colonoscopy navigation
abstract: A method for automatic virtual endoscopy navigation, including: (a) using a fisheye camera to generate an endoscopic image and a depth image from a current position of the camera in lumen computed tomographic (CT) data; (b) segmenting a first region and a second region from the depth image, wherein the first region identifies a view direction of the camera and the second region is an area through which the camera can be moved without touching an inner surface of the lumen; (c) moving the camera from the current position, while pointing the camera in the view direction, to a next position in the second region; and (d) repeating steps (a-c) in sequence using the next position in step (c) as the current position in step (a).
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08514218&OS=08514218&RS=08514218
owner: Siemens Aktiengesellschaft
number: 08514218
owner_city: Munich
owner_country: DE
publication_date: 20080813
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS"],"p":["This application claims the benefit of U.S. Provisional Application No. 60\/955,669, filed Aug. 14, 2007, the disclosure of which is incorporated by reference herein in its entirety.","1. Technical Field","The present invention relates to virtual endoscopy.","2. Discussion of the Related Art","The second leading cause of cancer-related deaths in the United States is colorectal cancer. Unfortunately, it is most often discovered after the patient has developed symptoms. It is recommended that adults should be screened to detect cancer-related polyps. The traditional screening using optical colonoscopy, however, is invasive, expensive, time-consuming, and uncomfortable, and requires an intensive bowel preparation. Because of this, many are not screened. Virtual colonoscopy (VC), also known as computed tomographic colonography (CTC), has been developed to help encourage adults to be regularly screened for polyps. In VC fly-through navigation, it is crucial to generate an optimal camera path for efficient colonic polyp screening. Automatic path planning is required by a VC system because manual planning is difficult and time-consuming due to the complex shape of the human colon. For complete and accurate diagnosis, a planned path should not produce significant blind areas on the colon surface.","There has been a great deal of research on navigation methods for three-dimensional (3D) virtual endoscopy, which can be classified into three categories: manual navigation [M. Gleicher and A. Witkin, \u201cThrough the lens camera control\u201d, in Proc. ACM SIGGRAPH '92, pp. 331-340, 1992 and R. Turner, F. Balaguer, E. Gobbetti and D. Thalmann, \u201cPhysically-based interactive camera motion control using 3D input devices\u201d, in Computer Graphics International '91, pp. 135-145, 1991], planned navigation [L. Hong, A. Kaufman, Y. Wei, A Viswambharan, M. Wax, and Z. Liang, \u201c3D virtual colonoscopy\u201d, in IEEE Symposium on Biomedical Visualization, pp. 26-32, 1995 and G. Rubin, C. Beaulieu, V. Argiro, H. Ringl, A. Norbash, J. Feller, M. Dake, R. Jeffey, and S. Napel, \u201cPerspective volume rendering of CT and MRI images: Applications for endoscopic imaging\u201d, Radiology 99, pp. 321-330, 1996], and guided navigation [L. Hong, S. Muraki, A. Kaufmann, D. Bartz and T. He, \u201cVirtual voyage: Interactive navigation in the human colon\u201d, in Proc. ACM SIGGRAPH '97, pp. 27-34, 1997, M. Wan, Q. Tang, A. Kaufman, Z. Liang, and M. Wax, \u201cVolume rendering based interactive navigation within the human colon\u201d, in Proc. IEEE Visualization '99, pp. 397-400, 1999 and K. Kwon and B. Shin, \u201cAn efficient camera path computation using image-space information in virtual endoscopy\u201d, Lecture Notes in Computer Science 3280, pp. 118-125, 2004]. Manual navigation requires the user to control the camera at every step, which is inefficient and uncomfortable. Moreover, the camera may penetrate through the colon surface when it is incorrectly handled by a physician.","Planned navigation calculates entire camera path and orientations in the preprocessing step, then continuously moves the camera along the pre-calculated path during the navigation. In this method, the physician cannot intuitively change the camera position and orientation. Further, a lot of computation is required in the preprocessing step. The centerline of the colon lumen is usually used as the camera path to obtain a wide view of the colonic surface. Topological thinning methods [L. Hong, A. Kaufman, Y. Wei, A Viswambharan, M. Wax, and Z. Liang, \u201c3D virtual colonoscopy\u201d, in IEEE Symposium on Biomedical Visualization, pp. 26-32, 1995, D. Paik, C. Beaulieu, R. Jeffery, G. Rubin, and S. Napel, \u201cAutomated flight path planning for virtual endoscopy\u201d, Medical Physics 25(5), pp. 629-637, 1998, and R. Sadlier and P. Whelan, \u201cFast colon centerline calculation using optimized 3D topological thinning\u201d, Computerized Medical Imaging and Graphics 29, pp. 251-258, 2005] have been used to eliminate the outermost layer of a segmented colon successively with only the centerline voxels remaining. In the distance mapping method, a distance field is computed, and then the minimum cost spanning tree is built to extract the optimal colonic centerline. Bitter et al. [I. Bitter, M. Sato, M. Bender, K. McDonnel, and A. Kaufman, \u201cCEASAR: A smooth, accurate and robust centerline extraction algorithm\u201d, in Proc. IEEE Visualization '00, pp. 45-52, 2000] have proposed an efficient centerline algorithm using a penalty distance, which is the combination of the distance from the source and the distance from the boundary. Wan et al. [M. Wan, Z. Liang, Q. Ke, L. Hong, I. Bitter, and A. Kaufman, \u201cAutomatic centerline extraction for virtual colonoscopy\u201d, IEEE Transactions on Medical Imaging 21(12), pp. 1450-1460, 2002] have used the exact Euclidian distance from each voxel inside the colon lumen to the nearest colon boundary to extract the colon centerline and its associated branches. Hassouna et al. [M. Hassouna and A. Farag, \u201cRobust centerline extraction framework using level sets\u201d, in IEEE Computer Vision and Pattern Recognition, pp. 458-465, 2005] have proposed a robust centerline extraction method, introducing a new speed function of level sets. However, all of these methods are computationally expensive, especially when they are applied to volumetric data.","Guided navigation provides some guidance for the navigation and allows the physician to control it when desired. The potential field [L. Hong, S. Muraki, A. Kaufmann, D. Bartz and T. He, \u201cVirtual voyage: Interactive navigation in the human colon\u201d, in Proc. ACM SIGGRAPH '97, pp. 27-34, 1997 and M. Wan, Q. Tang, A. Kaufman, Z. Liang, and M. Wax, \u201cVolume rendering based interactive navigation within the human colon\u201d, in Proc. IEEE Visualization '99, pp. 397-400, 1999] has been used to determine the camera position and orientation by considering the attractive force directing to the target point, repulsive force from the colon surface, and the external force. It consists of two distance fields inside the colon lumen: distance from the colonic surface and distance from the target point of the current navigation. This method requires additional storage for distance fields, and the computation of the potential field is time consuming. Kang et al. [D. Kang and J. Ra, \u201cA new path planning algorithm for maximizing visibility in computed tomography colonography\u201d, IEEE Transactions on Medical Imaging 24(8), pp. 957-968, 2005] have proposed a method to determine view positions and their view directions to minimize the blind areas during navigation. However, this algorithm showed poor performance to minimize the blind area between haustral folds although the visible areas at the curved regions are increased. Kwon et al. [K. Kwon and B. Shin, \u201cAn efficient camera path computation using image-space information in virtual endoscopy\u201d, Lecture Notes in Computer Science 3280, pp. 118-125, 2004] have used image space information generated in rendering time to determine the camera position and direction. This technique does not require preprocessing or extra storage, but it is highly likely to converge to local minima in complex regions.","In an exemplary embodiment of the present invention, a method for automatic virtual endoscopy navigation, comprises: (a) using a fisheye camera to generate an endoscopic image and a depth image from a current position of the camera in lumen computed tomographic (CT) data; (b) segmenting a first region and a second region from the depth image, wherein the first region identifies a view direction of the camera and the second region is an area through which the camera can be moved without touching an inner surface of the lumen; (c) moving the camera from the current position, while pointing the camera in the view direction, to a next position in the second region; and (d) repeating steps (a-c) in sequence using the next position in step (c) as the current position in step (a).","The method further comprises displaying the endoscopic image to visualize the movement of the camera. The method further comprises identifying a polyp in the lumen in the displayed image.","The fisheye camera has up to a 360 degree field of view.","The current position of the camera is initially a seed point that is placed in the lumen CT data by a medical practitioner.","The method steps for virtual endoscopy navigation are performed immediately after the lumen CT data is received from a CT scanner.","A region growing method is used to segment the second region and the segmented first region is used as a seed for the region growing.","The method further comprises calculating a centroid of the first region and the second region, respectively, wherein when the camera is moved from the current position to the second position it is moved along a ray toward the centroid of the second region, while being pointed to the centroid of the first region.","The method further comprises calculating a view up vector of the camera and restricting the view direction of the camera by the view up vector to smooth the virtual endoscopy navigation.","The next position is a point between the current position and a center of the segmented second region. Step (d) is performed when the camera is approaching the next position or when the camera reaches the next position.","The lumen is a colon.","In an exemplary embodiment of the present invention, a system for automatic virtual endoscopy navigation, comprises: a memory device for storing a program; a processor in communication with the memory device, the processor operative with the program to: (a) use a fisheye camera to generate an endoscopic image and a depth image from a current position of the camera in lumen CT data; (b) segment a first region and a second region from the depth image, wherein the first region identifies a view direction of the camera and the second region is an area through which the camera can be moved without touching an inner surface of the lumen; (c) move the camera from the current position, while pointing the camera in the view direction, to a next position in the second region; and (d) repeat steps (a-c) in sequence using the next position in step (c) as the current position in step (a).","The processor is further operative with the program to display the endoscopic image to visualize the movement of the camera. The processor is further operative with the program to identify a polyp in the lumen in the displayed image.","The fisheye camera has up to a 360 degree field of view.","The current position of the camera is initially a seed point that is placed in the lumen CT data by a medical practitioner.","The processor is further operative with the program code to execute the virtual endoscopy navigation immediately after the lumen CT data is received from a CT scanner.","A region growing method is used to segment the second region and the segmented first region is used as a seed for the region growing.","The processor is further operative with the program to calculate a centroid of the first region and the second region, respectively, wherein when the camera is moved from the current position to the second position it is moved along a ray toward the centroid of the second region, while being pointed to the centroid of the first region.","The processor is further operative with the program to calculate a view up vector of the camera and restrict the view direction of the camera by the view up vector to smooth the virtual endoscopy navigation.","The next position is a point between the current position and a center of the segmented second region. Step (d) is performed when the camera is approaching the next position or when the camera reaches the next position.","The lumen is a colon.","In an exemplary embodiment of the present invention, a computer readable medium tangibly embodying a program of instructions executable by a processor to perform method steps for automatic virtual endoscopy navigation is provided, the method steps comprising: (a) using a fisheye camera to generate an endoscopic image and a depth image from a current position of the camera in lumen CT data; (b) segmenting a first region and a second region from the depth image, wherein the first region identifies a view direction of the camera and the second region is an area through which the camera can be moved without touching an inner surface of the lumen; (c) moving the camera from the current position, while pointing the camera in the view direction, to a next position in the second region; and (d) repeating steps (a-c) in sequence using the next position in step (c) as the current position in step (a).","The foregoing features are of representative embodiments and are presented to assist in understanding the invention. It should be understood that they are not intended to be considered limitations on the invention as defined by the claims, or limitations on equivalents to the claims. Therefore, this summary of features should not be considered dispositive in determining equivalents. Additional features of the invention will become apparent in the following description, from the drawings and from the claims.","In this disclosure, we present an automatic image-based path planning algorithm for virtual endoscopy (VE) fly-through navigation, in accordance with an exemplary embodiment of the present invention. In our method, preprocessing is not required, and camera position and orientation are calculated on-the-fly using rendered depth images. The only input of our algorithm is lumen computed tomographic (CT) data and a seed point provided by a physician. Therefore, the VE fly-through navigation can be performed immediately after the lumen data are loaded from a computer's hard drive.","In the following description, our VE method\/system will be referred to as a virtual colonoscopy (VC) method\/system, since the lumen referenced hereinafter is a colon. The present invention is not limited thereto. For example, our VE method applies equally well to other lumens such as, bronchi and blood vessels, etc.","1. Methodology","In a VC fly-through navigation system, the path planning algorithm is required to provide camera positions and orientations to a rendering engine. In order to obtain a wide view of the colonic surface, the camera should stay as far away from the surface as possible. The centerline of the colon lumen is not used as the camera path, because its processing is time-consuming and we want to generate the camera path on-the-fly during the navigation. However, we still need to keep the camera away from the colonic surface to achieve better visibility coverage during the navigation. In our method, depth maps are used to determined the camera positions and orientations. Our method consists of three steps: depth image generation (), depth image segmentation (), and camera calculation (). Steps - are repeated for a next position of the camera (). The overview of our image-based navigation method is shown in .","1.1 Depth Image Generation","In Kwon et al.'s method [K. Kwon and B. Shin, \u201cAn efficient camera path computation using image-space information in virtual endoscopy\u201d, Lecture Notes in Computer Science 3280, pp. 118-125, 2004], camera orientation for the next frame is determined using a ray that has maximum distance in the current frame, and camera position in the next frame is calculated using the center of gravity of an organ region on a cross-sectional image. However, the camera is highly likely to converge to local minima in complex regions. In our method, we use a wide angle fisheye camera to generate a depth image at the current frame and then segment the depth image to provide spatial information of the colon lumen for calculating camera position and orientation for the next frame. By using an angular fisheye lens, larger view port angles can be achieved in the final image. This is helpful to solve the local convergence problem and improve the performance of the VC system.","The fisheye lens is a specially designed lens which achieves wider viewing angles. An angular fisheye projection [P. Bourke, \u201cComputer generated angular fisheye projections\u201d, 2001.] is defined so that the distance from the center of the image is proportional to the angle from the camera view direction as shown in ). There are two major differences between angular fisheye projection and perspective projection (see . First, in an angular fisheye image the resolution is approximately equal across the whole image. Second, an angular fisheye projection can be used for angles all the way up to a full 360 degrees. In this way, the physician has more chances to see potential abnormal anatomical structures, such as polyps. Further, since the angular fisheye camera is an ideal lens, it provides images with less distortion than those captured with a real fisheye lens used by an endoscope.","The ray direction corresponding to any pixel on the image plane can be calculated using a special transformation from pixel coordinates to three dimensional (3D) polar coordinates, as described in [P. Bourke, \u201cComputer generated angular fisheye projections\u201d, 2001.]. First, the image coordinates are transformed from pixel coordinates (i,j) into normalized coordinates (x,y) ranging from \u22121 to 1 using the following equation, assuming the resolution of the image plane is (w,h).",{"@attributes":{"id":"p-0044","num":"0043"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mo":"{","mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mi":"x","mo":"=","mrow":{"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mn":"2","mo":"\u2062","mi":"i"},"mo":"+","mn":"1"}},"mo":"\/","mi":"w"},"mo":"-","mn":"1"}},"mo":";"}}},{"mtd":{"mrow":{"mrow":{"mi":"y","mo":"=","mrow":{"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mn":"2","mo":"\u2062","mi":"j"},"mo":"+","mn":"1"}},"mo":"\/","mi":"h"},"mo":"-","mn":"1"}},"mo":";"}}}]}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}}},"Next, the 3D polar coordinates (r,\u03c6,\u03b8) are calculated as:",{"@attributes":{"id":"p-0046","num":"0045"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mo":"{","mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mi":"r","mo":"=","mrow":{"mi":"sqrt","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["x","x"],"mo":"*"},{"mi":["y","y"],"mo":"*"}],"mo":"+"}}}},"mo":";"}}},{"mtd":{"mrow":{"mrow":{"mi":"\u03d5","mo":"=","mrow":{"mi":["a","tan"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mo":["(",")"],"mrow":{"mi":["y","x"],"mo":"\/"}}}},"mo":";"}}},{"mtd":{"mrow":{"mrow":{"mi":"\u03b8","mo":"=","mrow":{"mi":"r","mo":"*","mrow":{"mi":"\u03b4","mo":"\/","mn":"2"}}},"mo":";"}}}]}}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{}},"Our depth image generation algorithm is based on a ray casting volume rendering scheme implemented on a graphics processing unit (GPU). See [J. Kruger and R. Westermann, \u201cAcceleration techniques for gpu-based volume rendering\u201d, in Proc. IEEE Visualization '03, pp. 287-292, 2003] for a description of such a scheme. For each pixel, its 3D ray direction is calculated using the above transformation. Then, a ray is cast into the volume data to do regular volume rendering integral. When the ray is terminated, it returns a depth value instead of color information. A depth image using our algorithm is shown in FIG. B(d).","In order to obtain better visibility coverage and less distortion, a 90 degree perspective projection is usually used in VC systems, see e.g., [L. Hong, S. Muraki, A. Kaufmann, D. Bartz and T. He, \u201cVirtual voyage: Interactive navigation in the human colon\u201d, in Proc. ACM SIGGRAPH '97, pp. 27-34, 1997 and M. Wan, Q. Tang, A. Kaufman, Z. Liang, and M. Wax, \u201cVolume rendering based interactive navigation within the human colon\u201d, in Proc. IEEE Visualization '99, pp. 397-400, 1999]. Compared with a normal perspective projection, more information can be obtained with less distortion when an angular fisheye projection is used. In , comparisons between a 90 degree perspective projection and a 180 degree angular fisheye projection are shown using both endoscopic images and depth images. Comparing the generated endoscopic images, we can see that a small polyp is shown in the fisheye endoscopic image (FIG. A(b)), which is not shown in the perspective endoscopic image (FIG. A(a)). Similarly, the fisheye depth image (FIG. B(d)) provides more information about the colon lumen than the perspective depth image (FIG. B(c)) does.","1.2 Depth Image Segmentation","The depth image generated using angular fisheye projection provides the spatial information about the colon lumen in front of the camera. The colon haustral folds can be detected in the depth image using edge detection algorithms, such as those described in [M. Nixon and A. Aguado, Feature Extraction and Image Processing, ELSEVIER, Amsterdam, The Netherlands, 2002]. The centers of these haustral folds are useful landmarks to guide the camera. It is recommended to move the camera passing through the centers of these curved contours during the navigation. However, it is difficult to accurately detect these haustral folds in several milliseconds even when the latest GPU is used. Thus, in our current implementation we only use the thresholding algorithm to segment the depth image.","In FIG. A(a), a fisheye endoscopic image is displayed to show the complex structure of the human colon, its corresponding depth image is shown in FIG. A(b). In the depth image, the gray level is proportional to the distance from the camera to the colon surface. The brighter region corresponds to the colon lumen which is far away from the current camera location, called target region (see circled region in FIG. B(c)). The center of this region can be used to determine the view direction of the camera. The target region can be efficiently detected in the depth image using a pre-defined distance value. Similarly, we can segment the depth image using a smaller distance value (see circled regions in FIG. B(d)), which provides the spatial information to guide the camera. This region is called safe region, which means moving the camera towards the center of this region is safe. It is noted that sometimes the safe region is separated as shown in FIG. B(d). In this case, we only use the region that contains the target region to guide the camera. Thus, we use a region growing method to segment a safe region using the segmented target region as the seed.","1.3 Camera Calculation","In this section, we describe a method to move the camera and setup the view direction of the camera based on the segmented depth image. Each pixel on the image plane corresponds to a ray direction in the 3D Cartesian coordinates. After the target region and the safe region are segmented from the current depth image, their centroid is calculated respectively, which is used to access its corresponding ray direction. We then move the camera from the current position along the ray direction corresponding to the centroid of the safe region. Moreover, the camera is pointed to the centroid of the segmented target region.","In order to minimize the rotation between the consecutive endoscopic views to provide a user comfortable navigation, the following equation is used to calculate the view up vector of the camera:\n\n\u2212()\u2003\u2003(3)\n\nwhere uis the view up vector and vis the view direction at the current camera position. A detailed description of equation (3) can be found in [D. Kang and J. Ra, \u201cA new path planning algorithm for maximizing visibility in computed tomography colonography\u201d, IEEE Transactions on Medical Imaging 24(8), pp. 957-968, 2005].\n\n2. Implementation and Results\n","We have implemented and tested our method using a workstation with two 2.0 GHz Intel Xeon central processing units (CPUs), 2 GB memory and an NVIDIA Geforce 8800GTX graphics card with 768 MB memory. Our method has been applied to 20 clinical data sets randomly selected from WRAMC VC data at National Cancer Institute at NIH.","An important thing about our implementation is that the depth image should not be read back from the GPU, because reading data back causes Open Graphics Library (OpenGL) pipeline stalls and inhibits parallelism on the current graphics card. NVIDIA's Compute Unified Device Architecture (CUDA) [NVIDIA, \u201cCuda programming guide\u201d, 2007.] is a new hardware and software architecture for issuing and managing computations on the GPU as a data-parallel computing device without the need of mapping them to a graphics application programming interface (API). Our depth image generation algorithm is implemented using the OpenGL shader program. OpenGL buffer objects can be mapped into the address space of CUDA, either to enable CUDA to read data written by OpenGL or to enable CUDA to write data for consumption by OpenGL. Thus, we have implemented our depth image segmentation algorithms using CUDA, which has two obvious advantages:\n\n","Moreover, we do not need to segment the target region and the safe region for every frame during the fly-through navigation. We only perform this operation when the center of the last segmented safe region is approached. In our VC system, the endoscopic view is rendered with a resolution of 512\u00d7512 using a volume ray casting algorithm. Either angular fisheye projection or perspective projection can be used to render the endoscopic view. If the angular fisheye projection is used, the endoscopic image and depth image can be rendered using a single shader program to improve the overall performance. The average timings for each step of our image-based path planning method are listed in Table 1. Because of our CUDA implementation of the depth image segmentation algorithm, our VC system can still guarantee 30 frame per second (FPS) during the fly-through navigation.",{"@attributes":{"id":"p-0056","num":"0057"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Average timings for each step of our VC system."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Step","Execution Time"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"right"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Endoscopic Image Rendering","9.5","ms"]},{"entry":[{},"Depth Image Generation","2.8","ms"]},{"entry":[{},"Depth Image Segmentation","7.4","ms"]},{"entry":[{},"Camera Calculation","<1","ms"]},{"entry":[{},"Total","<20.7","ms"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}}},"Although our image-based path planning algorithm does not require preprocessing, our system still requires the physician to provide a starting point. We recommend that the physician provides a point around the rectum.  shows two camera paths generated using our path planning algorithm starting from the rectum. It is noted that the camera is always located at the center of the colon lumen to obtain a wide view during the fly-through navigation.","3. Conclusions","We have described an efficient image-based path planning method for automated VC fly-through navigation. It does not require preprocessing and extra storage, which allows the physician to start inspection right after the data are ready. A 180 degree angular fisheye lens is used to generate a depth image based on a ray casting volume rendering scheme. It can capture more information than the perspective projection, which is widely used in the rendering of virtual colonoscopy applications. Our method uses an image segmentation algorithm to detect safe regions and target regions from the rendered depth images, which are then used to determine camera positions and orientations on-the-fly. Our method was applied to 20 colon data sets. The experimental results showed that the generated path was located in the center of the colon lumen for an effective polyp screening. It also increased the user comfort during the virtual colonoscopy navigation.","A system in which exemplary embodiments of the present invention may be implemented will now be described with reference to .","In , the system  includes an acquisition device , a computer  and an operator's console  connected over a wired or wireless network . The acquisition device  may be a CT imaging device or any other 3D high-resolution imaging device such as a magnetic resonance (MR) scanner or ultrasound scanner.","The computer , which may be a portable or laptop computer, a medical diagnostic imaging system or a picture archiving communications system (PACS) data management station, includes a CPU , a memory  and a graphics card  which are connected to an input device  and an output device . The CPU  includes a VC navigation module  that includes software for executing methods in accordance with exemplary embodiments of the present invention. Although shown inside the CPU , the VC navigation module  can be located in the graphics card  or external to the CPU , for example.","The memory  includes a random access memory (RAM)  and a read-only memory (ROM) . The memory  can also include a database, disk drive, tape drive, etc., or a combination thereof. The RAM  functions as a data memory that stores data used during execution of a program in the CPU  and is used as a work area. The ROM  functions as a program memory for storing a program executed in the CPU . The input  is constituted by a keyboard, mouse, etc., and the output  is constituted by a liquid crystal display (LCD), cathode ray tube (CRT) display, printer, etc.","The graphics card , which is used to take binary data from the CPU  and turn it into an image, includes a GPU  and a memory . In order to achieve real-time rendering, the depth image segmentation is performed on the GPU . The GPU  determines what to do with each pixel to be displayed, for example, on the output device  or a display  of the operator's console . In operation, the GPU  makes a 3D image by first creating a wire frame out of straight lines, rasterizing the image and adding lighting, texture and color to the 3D image. The memory , which may be a RAM, holds information regarding each pixel and temporarily stores completed images. Although not shown, the graphics card  also includes a connection to a motherboard, which also holds the CPU , for receiving data and power and a connection to the output device  for outputting the picture. The memory  could be included in the GPU  or the GPU  could include its own memory for performing storage tasks.","The operation of the system  can be controlled from the operator's console , which includes a controller , e.g., a keyboard, and a display . The operator's console  communicates with the computer  and the acquisition device  so that image data collected by the acquisition device  can be rendered by the computer  and viewed on the display . The computer  can be configured to operate and display information provided by the acquisition device  absent the operator's console , by using, e.g., the input  and output  devices to execute certain tasks performed by the controller  and display .","The operator's console  may further include any suitable image rendering system\/tool\/application that can process digital image data of an acquired image dataset (or portion thereof) to generate and display images on the display . More specifically, the image rendering system may be an application that provides rendering and visualization of medical image data, and which executes on a general purpose or specific computer workstation. The computer  can also include the above-mentioned image rendering system\/tool\/application.","It is to be understood that the present invention may be implemented in various forms of hardware, software, firmware, special purpose processors, or a combination thereof. In one embodiment, the present invention may be implemented in software as an application program tangibly embodied on a program storage device (e.g., magnetic floppy disk, RAM, CD ROM, DVD, ROM, and flash memory). The application program may be uploaded to, and executed by, a machine comprising any suitable architecture.","It should also be understood that because some of the constituent system components and method steps depicted in the accompanying figures may be implemented in software, the actual connections between the system components (or the process steps) may differ depending on the manner in which the present invention is programmed. Given the teachings of the present invention provided herein, one of ordinary skill in the art will be able to contemplate these and similar implementations or configurations of the present invention.","It is to be further understood that the above description is only representative of illustrative embodiments. For convenience of the reader, the above description has focused on a representative sample of possible embodiments, a sample that is illustrative of the principles of the invention. The description has not attempted to exhaustively enumerate all possible variations. That alternative embodiments may not have been presented for a specific portion of the invention, or that further undescribed alternatives may be available for a portion, is not to be considered a disclaimer of those alternate embodiments. Other applications and embodiments can be implemented without departing from the spirit and scope of the present invention.","It is therefore intended, that the invention not be limited to the specifically described embodiments, because numerous permutations and combinations of the above and implementations involving non-inventive substitutions for the above can be created, but the invention is to be defined in accordance with the claims that follow. It can be appreciated that many of those undescribed embodiments are within the literal scope of the following claims, and that others are equivalent."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIGS. 3A and 3B"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIGS. 4A and 4B"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
