---
title: Techniques for guided access to an external distributed file system from a database management system
abstract: Techniques for guided access to an external distributed file system (DFS) from a database management system (DBMS) are provided. A user access a graphical user interface (GUI) to interactively define a search on an external DFS. The search and any filtering conditions are processed on the external DFS and results are imported to the DBMS and mapped to a table in the DBMS for manipulation and access by the user within the DBMS.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09141251&OS=09141251&RS=09141251
owner: Teradata US, Inc.
number: 09141251
owner_city: Dayton
owner_country: US
publication_date: 20111229
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["After over two-decades of electronic data automation and the improved ability for capturing data from a variety of communication channels and media, even small enterprises find that the enterprise is processing terabytes of data with regularity. Moreover, mining, analysis, and processing of that data have become extremely complex. The average consumer expects electronic transactions to occur flawlessly and with near instant speed. The enterprise that cannot meet expectations of the consumer is quickly out of business in today's highly competitive environment.","Consumers have a plethora of choices for nearly every product and service, and enterprises can be created and up-and-running in the industry in mere days. The competition and the expectations are breathtaking from what existed just a few short years ago.","The industry infrastructure and applications have generally answered the call providing virtualized data centers that give an enterprise an ever-present data center to run and process the enterprise's data. Applications and hardware to support an enterprise can be outsourced and available to the enterprise twenty-four hours a day, seven days a week, and three hundred sixty-five days a year.","As a result, the most important asset of the enterprise has become its data. That is, information gathered about the enterprise's customers, competitors, products, services, financials, business processes, business assets, personnel, service providers, transactions, and the like.","Updating, mining, analyzing, reporting, and accessing the enterprise information can still become problematic because of the sheer volume of this information and because often the information is dispersed over a variety of different file systems, databases, and applications.","In response, the industry has recently embraced a data platform referred to as Apache Hadoop\u2122 (Hadoop\u2122). Hadoop\u2122 is an Open Source software architecture that supports data-intensive distributed applications. It enables applications to work with thousands of network nodes and petabytes (1000 terabytes) of data. Hadoop\u2122 provides interoperability between disparate file systems, fault tolerance, and High Availability (HA) for data processing. The architecture is modular and expandable with the whole database development community supporting, enhancing, and dynamically growing the platform.","However, because of Hadoop's\u2122 success in the industry, enterprises now have or depend on a large volume of their data, which is stored external to their core in-house database management system (DBMS). This data can be in a variety of formats and types, such as: web logs; call details with customers; sensor data, Radio Frequency Identification (RFID) data; historical data maintained for government or industry compliance reasons; and the like. Enterprises have embraced Hadoop\u2122 for data types such as the above referenced because Hadoop\u2122 is scalable, cost efficient, and reliable.","One challenge in integrating Hadoop\u2122 architecture with an enterprise DBMS is selectively acquiring data from Hadoop\u2122 and importing and using that data within the enterprise DBMS.","Recently, a table-based User-Defined Function (UDF) approach was introduced to allow DBMS users to have direct access to Hadoop\u2122 files in Structured Query Language (SQL) queries. The basic idea is that a customized table UDF pulls data from the Hadoop\u2122 distributed file system (HDFS) into the enterprise data warehouse for manipulation. Each table UDF instance runs on a particular Access Module Processor of the parallel DBMS and is responsible for retrieving a portion of the HDFS file defined in the table UDF.","However, there are a few problems with this approach, such as the three problems listed below.","Firstly, the access code to HDFS is hard-coded in the table UDF and the mapping from HDFS files to the DBMS relational data is also hard-coded. This means that if a user needs to access different columns in the same HDFS file or convert the same columns in the HDFS file to different types, a different Table UDF has to be programmed and used. This approach is not productive when the users frequently need to access different HDFS files or need access to the same HDFS file in different ways.","Secondly, although data filtering and transformation can be done by the UDF as the rows are delivered by the UDF to the SQL processing, the HDFS files are first transferred to the parallel DBMS even when later during processing on the DBS some data in the imported file are to be discarded. This can be an unfortunate waste of network bandwidth.","Thirdly, users have to develop and write the customized UDFs, which require knowledge of the Hadoop\u2122 system and its file system's Application Programming Interface (API), and the users also need knowledge of the DMBS's table UDF infrastructure.","In various embodiments, techniques for guided access to an external distributed file system (DFS) from a DBMS are presented. According to an embodiment, a method for customized access to an external DFS from a DBMS is provided.","Specifically, a graphical user interface (GUI) within a parallel database management system (DBMS) is presented to a user. The GUI is then used to interact with the user, who identifies, via the GUI, a file from an external distributed file system (DFS), external to the DBMS. Next, first columns to select from the file are acquired from the user, via the GUI, along with a mapping of the first columns to data types present in the DBMS. Then, filtering conditions for matching data in the first columns of the file are obtained from the user, via the GUI interaction. Finally, a user-defined function (UDF) is generated for selections of the user made via the GUI.","Initially for purposes of illustration and comprehension some context and examples are presented to highlight and illustrate the techniques being presented herein and below.","The techniques herein propose a Graphical User Interface (GUI) guided and interactive approach to help database users with direct and parallel access to Hadoop\u2122 files (any external distributed file system (DFS) files) in their SQL queries via an automated system-generated table UDF. The GUI guided approach presented herein and below solves all the aforementioned problems.","Initially, a GUI is presented to the user within the user's DBMS, this provides processing as follows:","Step 1: the system asks the user which Hadoop\u2122 (or any DFS) file to be operated upon and that is to be transferred to the user's DBMS;","Step 2: the system automatically samples the file, presents a few lines to the user, shows the size of the file, and asks what columns the user is interested in;","Step 3: the user picks a few columns, and for each column the system automatically suggests a few relational data types to which the data is to be converted to; also, a drop-down list is provided so that the user can pick from a complete list of relational data types; and the user can provide data transformation expressions to transform the data values in the DFS file;","Step 4: the system asks which filtering conditions are to be applied so that only certain rows matching the conditions are actually transferred to the DBMS;","Step 5: optionally, the user can provide hints to the system on the desired record boundary so that the system can produce efficient code; hints can be fixed row-width or special record delimiter; and","Step 6: the user can provide a UDF name for the UDF to be generated by the system automatically; or, the system can pick up a default unique UDF name for the user.","Implementation Techniques","Once the GUI system gets all input from the above steps from the user, the system translates the input parameters to a script in a given UDF description language. The UDF description language can be in eXtensible Markup Language (XML) format or in any line delimited syntax. Then, a compiler reads the generated script and automatically generates the table customized UDF and installs it for access and processing within the DBMS.","The above-identified steps solve the two problems described above (Problems 1 and 3). That is, the user does not need to write his\/her own table UDF with the approaches presented herein and it does not need to write a new table UDF for a different data loading job. However, for problem 2 above and when users provide filtering conditions and are not interested in all data in a DFS file to be transferred to the DBMS, other considerations are taken.","In order to reduce unnecessary network data transfer, the techniques presented herein can automatically generate code to be run at the Hadoop\u2122 or external DFS side, which applies user specified filtering conditions and then only sends qualified data back to the DBMS. Data transformation can be performed at the DFS side instead of by the AMPs at the DBMS side. The system can take into account the size of the DFS system and the size of the DBMS system; estimate which system is likely to perform the transformation more quickly and then choose the right system to perform data transformation. Users can optionally give hints or require that a particular system to be used for data transformation. At runtime, when a user invokes an SQL query using the system generated table UDF to access DFS files, the DBMS Table UDF first connects to the DFS system, starts up the client side code, which then will read DFS data, performs data filtering, and sends only qualified data to the table UDF running instances on the DBS AMPs.","Notice that some applications may prefer to directly generate scripts in the DBMS UDF description language to automatically generate Table UDFs for access to DFS data, and not go through the step-by-step GUI approach. This can be useful in command-line tools and extract-transfer and load (ETL) integration scenarios. Notice also that the three problems identified above are also solved in this particular use case.","The approaches herein assist users in automatically generating Table UDFs to directly access external DFS data from SQL queries without the need to learn the external DFS or the DBMS UDF infrastructure and without the need for writing any developer code.","It is within this context that various embodiments are now discussed with reference to the .",{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 1","b":["100","100"]},"At , the data integrator presents a GUI to a user within the DBMS. The GUI is provided as part of the DBMS via an enhancement to an existing parallel DBMS.","At , the data integrator uses the GUI to interact with the user and the user identifies a file within the DFS. That is, the file is not controlled by or located within the DBMS; rather the file is controlled by the DFS. In an embodiment, the file is controlled and located within a HDFS.","According to an embodiment, at , the data integrator samples the file and providing sample lines from the file to the user via the GUI along with a file size for the user to identify all columns of the file including the first columns. Here, the user can also identify delimiters within the sampled data for identifying the first columns in the first instance. This may be needed when the file includes data that is unstructured. Other metadata about the file can also be presented to the user here, such as any that is carried by the DFS and available via an API of the DFS to the data integrator.","At , the data integrator acquires, via the GUI, and from the user who is interacting with the GUI, first columns to select from the file and a mapping. The mapping provides instructions to map the first columns to data types, which are present within the DBMS. This mapping permits data exported according to the instructions for the first columns to be imported into a dynamically created table within the DBMS for the user to subsequently interact with, analyze, and otherwise use.","In an embodiment, at , the data integrator presents a drop-down list within the GUI that shows all relational data types available within the DBMS including the data types selected by the user. The drop-down list can also be searchable within the GUI by the user.","In another instance, at , the data integrator permits the user to access features of the GUI to custom define a new data type within the DBMS to include with the data types selected by the user. In other words, the user may dynamically define a new data type for the DBMS to user.","At , the data integrator obtains, via the GUI, and from the user filtering conditions for matching data in the first columns of the file. The query conditions for selecting data from the user-identified columns can be made with the filtering conditions.","According to an embodiment, at , the data integrator presents a drop-down list within the GUI that shows all filtering conditions available within the DBMS including the filtering conditions selected by the user. It may also be that the GUI allows the list to be user searched.","In another scenario, at , the data integrator permits the user to input hints within the GUI that defines record boundaries or column ranges available within the file and used when generating the UDF. This permits the data integrator to optimize the resulting UDF (discussed below) based on the attributes of the data included within the file.","At , the data integrator generates a user-defined function (UDF) for selections of the user made via the GUI. The data integrator is a code generator that is menu-driven from the selections made by the user interacting with the GUI.","In an embodiment, at , the data integrator assigns a UDF name defined by the user for subsequent recall and execution within the DBMS by the user or other users of the DBMS. The UDF can be referenced by other users within the DBMS or used within automated modules that are implemented within an API of the DBMS using the UDF name.","According to an embodiment, at , the data integrator executes the UDF against the DFS and file and returns results for the matching data to a table within the DBMS for further analysis and manipulation by the user within the DBMS, the matching data is mapped to the selected data types using the mapping.","Continuing with the embodiment of  and at , the data integrator determines whether to execute the UDF on the DBMS or on the DFS. This can be achieved in a number of manners.","For example, at , the data integrator determines based on a selection received from the user that indicates specifically where the UDF is to execute. So, the user can execute the UDF on the DFS or on the DBMS.","In another instance of  and at , the data integrator determines where the UDF is to execute based on dynamically evaluating metrics associated with sizes, loads, and configurations with the UDF and the DBMS. So, dynamic resolution of where to execute the UDF can be achieved by evaluation of metrics for both the DBMS and the DFS.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 2","b":["200","600"]},"The external query builder presents yet another view of the processing discussed above with respect to the  and the data integrator.","At , the external query builder identifies a file located in an external distributed file system (DFS); the identification occurs within a parallel database management system (DBMS). In an embodiment, the DFS is a HDFS having a variety of externally maintained data that is unstructured.","At , the external query builder uses an API of the DFS to access the file on the DFS.","At , the external query builder presents data from the file to a requestor.","At , the external query builder receives from the requestor column definitions, mapping instructions, and filtering conditions for matching selective data within the file as defined by the requestor to a table of the DBMS.","According to an embodiment, at , the external query builder obtains metrics from the requestor that permits the executable code for the UDF to be customized based on attributes of the file.","At , the external query builder generates executable code for a user-defined function (UDF) that acquires the selective data from the file and imports that selective data to the table using the mapping instructions.","In another case, at , the external query builder installs portions of the executable code within the DFS for execution within a processing environment of the DFS.","Continuing with the embodiment of  and at , the external query builder installs other portions of the executable code within the DBMS for execution within a processing environment of the DBMS.","According to an embodiment, at , the external query builder receives an instruction to process the UDF and interacts with the DFS to populate the table with the selective data.","In an embodiment, at , the external query builder is processed based on a requestor that is one or more automated modules defined in an API of the DBMS. In other words, the requestor is an automated module defined in the API of the DBMS and interacts with the external query builder.","In another situation, at , the external builder is processed based on dynamically interaction with the requestor who is a user; the interaction occurring via a GUI interfaced to the external query builder. This embodiment is similar to the processing discussed above with respect to the .",{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 3","b":["400","400","400","400","400"]},"The external distributed file and user-defined function code generator system  implements, inter alia, various aspects of the methods  and  presented above with respect to the , respectively.","The external distributed file and user-defined function code generator system  includes a DFS UDF generator  and a GUI . Each of these and their interactions with one another will now be discussed.","The GUI  is discussed first for purposes of comprehension and illustration.","The GUI  is implemented in a non-transitory computer-readable medium and executes on one or more processors of the DBMS. Various aspects of the GUI  were presented and discussed in detail above with reference to the .","The GUI  is configured to interact with a user of the DBMS and the DFS UDF generator . The GUI  provides a menu-driven interface to the user for the user to define columns, mapping instructions, and filtering conditions for a file of the DFS. That is, the file is external and is not controlled and does not reside within the processing environment of the DBMS; rather, the file is externally controlled and resides within the DFS. In an embodiment, the file is located and controlled within a HDFS.","The DFS UDF generator  resides, is programmed within, and implemented within a non-transitory computer-readable storage medium. The DFS UDF generator  is executed on one or more processors of the DBMS. So, the DFS UDF generator  is not processed, is not controlled, and does not reside within the DFS.","Moreover, the DFS UDF generator  is configured to use the columns, mapping instructions, and the filtering conditions to generate a specific UDF that when executed accesses the file from the DFS and imports selective data matching the filtering conditions for the columns into a table of the DBMS based on the mapping instructions for the selective data to be analyzed and processed within the DBMS. The processing details associated with the DFS UDF generator  were presented above with reference to the .","According to an embodiment, the DFS UDF generator  is also further configured to generate portions of executable code for the specific UDF for execution by the DFS and other portions of the executable code for execution within the DBMS.","The above description is illustrative, and not restrictive. Many other embodiments will be apparent to those of skill in the art upon reviewing the above description. The scope of embodiments should therefore be determined with reference to the appended claims, along with the full scope of equivalents to which such claims are entitled."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 3"}]},"DETDESC":[{},{}]}
