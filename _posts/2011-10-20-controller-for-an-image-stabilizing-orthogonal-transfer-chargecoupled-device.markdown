---
title: Controller for an image stabilizing orthogonal transfer charge-coupled device
abstract: An apparatus includes a video sensing device, a velocity vector estimator (VVE) coupled to the video sensing device, a controller coupled to the velocity vector estimator, and an orthogonal transfer charge-coupled device (OTCCD) coupled to the controller. The video sensing device transmits a plurality of image frames to the velocity vector estimator. The controller receives a location of an object in a current frame, stores locations of the object in one or more previous frames, predicts a motion trajectory and the predicted location of the object on it in a subsequent frame as a function of the locations of the object in the current frame and the one or more previous frames, and transmits the predicted location of the object to the OTCCD. The OTCCD shifts its image array of pixels as a function of the predicted location of the object.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08553937&OS=08553937&RS=08553937
owner: Honeywell International Inc.
number: 08553937
owner_city: Morristown
owner_country: US
publication_date: 20111020
---

{"@attributes":{"id":"description"},"GOVINT":[{},{}],"heading":["GOVERNMENT INTEREST","TECHNICAL FIELD","RELATED APPLICATIONS","BACKGROUND","DETAILED DESCRIPTION","Example Embodiments"],"p":["The present invention was made with support from the Intelligence Advanced Research Project Activity's (IARPA) Biometrics Exploitation Science and Technology (BEST) program under contract W911NF-10-C-0022 issued by U.S. Army RDECOM Acquisition Center on Dec. 17, 2009. The United States government has certain rights in this invention.","The present disclosure relates to image stabilization, and in an embodiment, but not by way of limitation, a controller for an image stabilizing orthogonal transfer charge coupled device.","This application is related to U.S. application Ser. No. 12\/699,368, which was filed on Feb. 3, 2010.","A conventional approach to acquiring high quality iris or face images of moving subjects is to freeze the subject motion by using extremely short exposures. The subject must be brightly illuminated by a flash in order to get a well exposed image. This approach commonly breaks down for larger distances because the flash power that is required in order to obtain an acceptable image often becomes eye unsafe.","Producing a well exposed image without flash illumination typically requires extending the image exposure, which degrades the image quality. One drawback with this approach is that extending the image exposure typically introduces motion blur unless the relative motion between the subject and the camera sensor is eliminated.","The motion of a subject relative to the camera sensor can be real, apparent or both. Real motion is the result of the physical motion of the subject and\/or the sensor. Real motion is described by a velocity vector which gets decomposed into two components. The axial velocity vector points toward the sensor and is aligned with the optical axis of the sensor. The lateral velocity vector is the velocity vector projection into an X-Y plane perpendicular to the optical axis. Axial velocity affects the focusing of camera optics and may introduce magnification blur for long exposures. Lateral velocity causes motion blur.","Moveable lenses are widely used as image stabilizing elements in telephoto objectives for photographic cameras. One known approach is to use a pair of inertial sensors inside the objective to detect camera motion resulting from the user's unsteady hand and then move the stabilization lens during exposure to compensate for it. An alternative approach employs a tip-tilt mirror to compensate for the relative motion. This approach is applicable only to scenarios in which all motion is due to that of the camera, because sensors cannot be installed on the moving subject that is imaged.","One drawback with these types of solutions is that they typically cannot track very fast motions. In addition, these approaches usually utilize delicate mechanical parts that make their use problematic in military and security applications.","In the following description, reference is made to the accompanying drawings that form a part hereof, and in which is shown by way of illustration specific embodiments which may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention, and it is to be understood that other embodiments may be utilized and that structural, electrical, and optical changes may be made without departing from the scope of the present invention. The following description of example embodiments is, therefore, not to be taken in a limited sense, and the scope of the present invention is defined by the appended claims.",{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 1","b":["10","10","12","10","14","12","10"]},"The example image acquisition system  further includes a second image recording device . The second image recording device  includes an orthogonal transfer charge-coupled device (OTCCD) sensing element  which records a target image of the subject. The orthogonal transfer CCD sensing element  includes an array of pixels  that captures the target image.","The image acquisition system  further includes a controller  that shifts the array of pixels  within the orthogonal transfer CCD sensing element . The controller  shifts the array of pixels  based on the lateral velocity vector estimates provided by the lateral velocity estimator .","In some embodiments, the first image recording device  is a camera although it should be noted that the first image recording device  may be any device that is capable of sending a series of images of the subject to the lateral velocity estimator . In other embodiments, the first image recording device  may be a scanning laser\/range finder that is capable of determining the distance from the subject to the image acquisition system  and measuring the velocity vector of the subject relative to the image acquisition system  in real time.","The lateral velocity estimator  may be any device that is capable of receiving a series of images of the subject from the first image recording device . As examples, the lateral velocity estimator  may use range data from a scanning laser range finder, or depth images from a camera whose sensor has pixels capable of real time time-of-flight ranging.","Depending on the application where the image acquisition system  is utilized, the lateral velocity vector estimator  may receive the series of images from the first image recording device  before the second image recording device  starts recording the target image of the subject. In other embodiments, the lateral velocity vector estimator  may receive the series of images from the first image recording device  while the second image recording device  records the target image of the subject. The image acquisition system  may include closed loop control such that estimates are applied immediately after they are computed (i.e., in real time). The rate of updates will be dictated by the frame rate of first image recording device . In addition, the number of updates may be the same as the number of images in the series of images.","Using an Orthogonal Transfer CCD sensor element  as the image stabilizing element improves performance of the image acquisition system  because unlike other known implementations of the image stabilization concept, Orthogonal Transfer CCD sensor element  involves no moveable mechanical parts (e.g., lenses, minors or sensor chips). The orthogonal transfer CCD sensor element  moves the potential wells that correspond to the array of pixels  and accumulate their photoelectrons. Since the wells do not have any inertia, the wells can be moved extremely fast by manipulating in real time the voltages that define the operation of the orthogonal transfer CCD sensor element . With no moveable mechanical parts, the orthogonal transfer CCD sensor element  offers an extremely rugged solution that is well suited for security and military applications.","The orthogonal transfer CCD sensor element  offers an appealing alternative to mechanically delicate tip-tilt mirrors and moveable stabilizing lenses. The orthogonal transfer CCD sensor element  stabilizes the image not by mechanically reconfiguring the optics or moving the sensing chip, but rather by electronically changing the location of the orthogonal transfer CCD sensor element  array of pixels .","Since the image acquisition system  employs an image stabilization concept, the image acquisition system  needs to estimate the relative velocity between the subject and the image acquisition system  in order to properly drive the orthogonal transfer CCD sensor element . The velocity vector can be estimated before or during the image exposure. Estimating the velocity vector before the image exposure is limited to simpler scenarios involving only physical motion of the subject. In the latter case, the series of images provided to the lateral velocity vector estimator  during the image exposure is continuously evaluated as its frames are arriving to determine the velocity vector. The velocity vector updates are then used to drive in real time the potential well movements in the orthogonal transfer CCD sensor element .","The controller  may issue updates to the orthogonal transfer CCD sensor element  at rates on the order of 100 updates\/sec. The update can include calls for shifting the pixels in the array of pixels .","In some embodiments, the orthogonal transfer CCD sensor element  may be capable of executing array shifts at rates on the order of 100,000 updates\/sec. However, the orthogonal transfer CCD sensor element  may only be able to execute single step shifts (i.e., one pixel left, right or none and\/or one pixel up, down or none).","In the example embodiment illustrated in , the first image recording device  and the second image recording device  receive the light radiated by or bouncing off the subject along different optical paths , . The first image recording device  records the series of images of the subject along a first optical path  and the second image recording device  records images along a different second optical path .",{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 2","FIG. 2"],"b":["12","13","23","23","25","26","21","12","22","13"]},"Depending on the hardware that is used in the image acquisition system , the first image recording device  and the second image recording device  may record light at different, the same or overlapping wavelengths. In the example embodiment that is illustrated in , the splitter  directs light having wavelengths in the visible range of the spectrum to the first image recording device  and directs light in the near infrared range of the spectrum to the second image recording device .","An example method of obtaining a target image of a subject will now be described with reference to . The method includes recording a series of images of the subject using an image acquisition system  and estimating lateral velocity vectors of the subject relative to the image acquisition system . The method further includes recording a target image of the subject onto an array of pixels that form part of an orthogonal transfer CCD sensing element within the image acquisition system and adjusting the array of pixels based on the estimated lateral velocity vectors.","In some embodiments, recording a series of images of the subject may include recording a series of images of the subject using a first device , and recording the target image of the subject may include recording the target image of the subject with a second device . In addition, recording a series of images of the subject using a first device  may include recording the series of images along a different (), similar () or identical optical path to recording of the target image of the subject with the second device .","It should be noted that recording a series of images of the subject using a first device  may include recording the series of images in a spectral range that is different, overlapping or identical to the spectral range where the target image of the subject is recorded with the second device .","Recording a series of images of the subject using an image acquisition system  is done while recording the target image of the subject onto an array of pixels . In addition, estimating lateral velocity vectors of the subject relative to the image acquisition system  is done while recording the target image of the subject onto the array of pixels .","The image acquisition system  described herein may be used to consistently produce high-quality iris and\/or face images of moving, uncooperative subjects at larger distances, where the use of a flash to freeze the motion is difficult because of eye safety concerns. The image acquisition system  may be used to produce a well exposed image (without flash illumination) by extending the image exposure without degrading the image quality. The image acquisition system  is able to extend the image exposure without introducing the motion blur by using the Orthogonal Transfer CCD sensor element  as the image stabilizing element.","Consequently, motion blur in images of moving objects can be eliminated or reduced by stabilizing the image projection during exposure. As detailed above, such a stabilization can be implemented by means of an Orthogonal Transfer CCD sensor array, which collects photoelectrons in potential wells that are not firmly attached to individual array pixels by design, but can be moved around in real time underneath the pixel infrastructure implanted on the sensor wafer surface.","For the image stabilization to work well, the array must be properly shifted in real time to make it track the moving object. The remainder of this disclosure addresses embodiments that perform such tracking.","Even controllers processing on the order of a hundred frames per second may not ensure good tracking if they work in a purely reactive manner. Control theory asserts that only a predictive controller can offer small tracking errors at acceptable rates. The accuracy of prediction is principal constraint limiting the attainable accuracy of tracking. The more accurate the tracking, the higher the quality of the image collected by the second device . In some scenarios, the issue is further aggravated by the need to update the predictions hundreds of times per second, which puts an upper bound on their computational complexity.","In an embodiment, a reasonable compromise between accuracy and computational complexity is implemented by assuming the object motion trajectory can be well approximated locally by either a straight line or a circular segment. These two controller designs can be referred to as a 2-frame controller and a 3-frame controller, respectively. The controllers take object locations found in the current and last one or two frames by the velocity vector estimator (VVE), respectively, and use these locations to predict the object motion trajectory and the location on it where the object is predicted to appear in a future frame. The trajectory segment leading to this predicted location is then sub-sampled into shorter segments whose coordinates are sent to the OTCCD camera for execution.","In an embodiment, the straight lines or circular segment trajectory approximations are calculated using object locations observed in the current and any number of past frames. The approximations are fitted to the observed data by a suitable method known in mathematics, for example by a least-mean-square-error (LMSE) fitting method. While this generalization does not change the principle of operation of the 2- and 3-frame controllers, it may make them more robust in the presence of random location measurement errors. Regardless how many observations a controller uses, we still call them 2- and 3-frame controllers to indicate whether they use a line or circle for motion trajectory local approximation.","Referring to , the object depicted as a dot is rotating in the counterclockwise direction. The 2-frame controller embodiment uses object locations that are observed in the current frame  and a previous VVE camera frame () to predict where an object will be in a subsequent frame (). After this determination, the OTCCD sensor is commanded to shift its array from its current location () to the predicted location ().",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIGS. 4A and 4B","FIG. 5"],"b":["410","310","440","340","440","520","510","420"]},"Approximating the actual object motion trajectory locally by a straight line is the first of the two assumptions a controller designer must make. The second one concerns the length of the linear segment. If an object is orbiting at a constant angular velocity, then the distances traveled from frame to frame will be constant, and it can be assumed, as it was above, that the predicted segment length is the same as between the current and preceding frame. In reality, the object (dot in ) can still travel down the same trajectory, but arbitrarily accelerate or decelerate along the way. In one embodiment, the length of the current segment between the current and past observed location is compared with the lengths of one or more preceding segments, and the comparison is used to estimate the acceleration or deceleration trend. The same trend is then assumed to be continuing into the near future and serves as a basis for predicting the length of the segment connecting the current and predicted locations.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 6","FIG. 6"],"b":["600","615","605","610","620","630","605","610","615"]},"In the above examples, the predicted location refers to where the moving target is expected to be found in the first device frame immediately following the current frame. In an embodiment, the predicted location may refer to where the moving target is to be found in a frame that is any number of frames ahead of the current frame. Extending the prediction horizon allows the system to work with hardware whose processing time is longer than one first device frame period.","Using more than three points to define a circle leads to an overdetermined problem. Because a circle is defined by a quadratic equation, solving the problem may be possible only by numerical methods unless the problem is suitably formulated. Due to their iterative nature, such methods are not suitable for real time processing when high frame rates are required. In one embodiment, a least mean square error (LMSE) formulation of the fitting circle problem to any number of observations involves no iterative numerical search for the best fit and thus can execute in real time.","To formulate the problem, it is noted that an object revolves in an orbit with a center and a radius. Because observations of the object are noisy, the object predictions differ from the actual locations by a random error.  illustrates the actual points  and the observed points , and the respective location errors . An ideal best fit would minimize the sum of the lengths of the vectors . However, the actual locations are unknown and thus we cannot compute the vectors . Therefore, what is considered instead is a connector measured along radials from the sample locations to their intersection with the circle that is being fitted. The radials emanate from the center and pass through the observed locations of the samples. They are illustrated at  in . The sum of the squared lengths of these connectors is a function of a yet unknown center and radius of the sought after circle. A solution in this embodiment minimizes this function using closed form formulae.","Alternatively, a circular or another, more complex path may be fitted to observed locations using the Kalman filter. In one embodiment, the filter replaces the Fit circle block C in .",{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 8","b":["800","810","820","840","850","830","820","860","860","861","862","863","864","870","880","885","800"]},"The velocity vector estimator may correct the predicted location of the object so as to account for optical instability of the atmosphere using data obtained from a wavefront sensor. For example, in an embodiment, a laser projected onto a human face can provide a reference for wavefront aberration measurements. A biometric imaging system can include a laser, a wavefront sensor, and an optical system. The laser is configured to project a laser spot onto a skin portion of a human face, and the optical system is configured to collect scattered light from the laser spot and relay the light to the wavefront sensor. The system can further include an adaptive optical element and a controller configured to provide actuation commands to the adaptive optical element based on a wavefront distortion measurement output from the wavefront sensor. The optical system is further configured to relay image light to an image camera of the optical system. The image camera can be an iris camera that is configured for obtaining iris images suitable for biometric identification.",{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 9","b":["100","100","102","100","104","102","100","106","108","102","100","110","112","114","116","118","120","120","110","122","100","124","126","142","100","144"]},"In another embodiment suggested in , the output  from the system  is fed into the VVE through its input . The VVE estimator processes the data communicated on the link  and uses the result to correct the locations observed in the next frame .","Example No. 1 is an apparatus that includes an image sensing device, a velocity vector estimator (VVE) coupled to the image sensing device, a controller coupled to the velocity vector estimator, and an orthogonal transfer charge-coupled device (OTCCD), comprising an image array of pixels, coupled to the controller. The image sensing device is configured to transmit a plurality of image frames to the velocity vector estimator. The controller is configured to receive from the velocity vector estimator a location of an object in a current frame, to store locations of the object in one or more previous frames, to predict a motion path for a predicted location of the object in a subsequent frame as a function of the locations of the object in the current frame and the one or more previous frames, and to transmit the predicted location of the object to the OTCCD. The OTCCD is configured to shift the image array of pixels as a function of the predicted location of the object.","Example No. 2 includes the features of Example No. 1, and optionally includes a controller configured to divide the predicted motion path into a plurality of segments, and to transmit coordinates of the segments to the OTCCD.","Example No. 3 includes the features of Example Nos. 1-2, and optionally includes a controller configured to examine the location of the object in the current frame and two or more previous frames, and to determine if the locations in the current frame and the two or more previous frames are approximately co-linear.","Example No. 4 includes the features of Example Nos. 1-3, and optionally includes a controller configured to predict the location of the object using a linear extrapolation when the locations of the object in the current frame and the two or more previous frames are approximately co-linear.","Example No. 5 includes the features of Example Nos. 1-4, and optionally includes a controller configured to predict the location of the object by fitting a circle to the locations of the object in the current frame and the two or more previous frames when the locations of the object in the current frame and the two or more previous frames are not co-linear.","Example No. 6 includes the features of Example Nos. 1-5, and optionally includes a controller configured to estimate acceleration or deceleration of the object by comparing distances between locations of the object in the current frame and the two or more previous frames.","Example No. 7 includes the features of Example Nos. 1-6, and optionally includes a controller that incorporates the estimated acceleration or deceleration of the object to predict a location of the object in the subsequent frame. It is assumed that the acceleration or deceleration remains constant until the subsequent frame is exposed.","Example No. 8 includes the features of Example Nos. 1-7, and optionally includes a controller that incorporates the estimated acceleration or deceleration of the object to predict a location of the object in the subsequent frame, wherein it is assumed that the acceleration or deceleration trend defined by a selected number of higher order derivatives of motion remains constant until the subsequent frame is exposed.","Example No. 9 includes the features of Example Nos. 1-8, and optionally includes an apparatus that comprises a biometric eye analyzer.","Example No. 10 includes the features of Example Nos. 1-9, and optionally includes an apparatus comprising a scope for a firearm or other device.","Example No. 11 includes the features of Example Nos. 1-10, and optionally includes a velocity vector estimator configured to correct the predicted location of the object so as to account for optical instability of the atmosphere using data obtained from a wavefront sensor.","Example No. 12 is a computer readable storage device comprising instructions that when executed by a processor execute a process. The process includes transmitting a plurality of image frames from an image sensing device to a velocity vector estimator, receiving at a controller a location of an object in a current frame, storing locations of the object in one or more previous frames, predicting a motion path for a predicted location of the object in a subsequent frame as a function of the locations of the object in the current frame and the one or more previous frames, transmitting the predicted location of the object to an orthogonal transfer charge-coupled device (OTCCD), and shifting the image array of pixels as a function of the predicted location of the object.","Example No. 13 includes the features of Example No. 12, and optionally includes instructions for dividing the predicted motion path into a plurality of segments, and transmitting coordinates of the segments to the OTCCD.","Example No. 14 includes the features of Example Nos. 12-13, and optionally includes instructions for examining the location of the object in the current frame and two or more previous frames, and determining if the locations in the current frame and the two or more previous frames are approximately co-linear.","Example No. 15 includes the features of Example Nos. 12-14, and optionally includes instructions for predicting the location of the object using a linear extrapolation when the locations of the object in the current frame and the two or more previous frames are approximately co-linear; and for predicting the location of the object by fitting a circle to the locations of the object in the current frame and the two or more previous frames when the locations of the object in the current frame and the two or more previous frames are not co-linear.","Example No. 16 includes the features of Example Nos. 12-15, and optionally includes instructions for estimating acceleration or deceleration of the object by comparing distances between locations of the object in the current frame and the two or more previous frames; and for incorporating the estimated acceleration or deceleration of the object to predict a location of the object in the subsequent frame. It is assumed that the acceleration or deceleration remains constant until the subsequent frame is exposed.","Example No. 17 is a process including transmitting a plurality of image frames from an image sensing device to a velocity vector estimator, receiving at a controller a location of an object in a current frame, storing locations of the object in one or more previous frames, predicting a motion path for a predicted location of the object in a subsequent frame as a function of the locations of the object in the current frame and the one or more previous frames, transmitting the predicted location of the object to an orthogonal transfer charge-coupled device (OTCCD), and shifting the image array of pixels as a function of the predicted location of the object.","Example No. 18 includes the features of Example No. 17, and optionally includes dividing the predicted motion path into a plurality of segments, and transmitting coordinates of the segments to the OTCCD.","Example No. 19 includes the features of Example Nos. 17-18, and optionally includes examining the location of the object in the current frame and two or more previous frames, and determining if the locations in the current frame and the two or more previous frames are approximately co-linear.","The Abstract is provided to comply with 37 C.F.R. \u00a71.72(b) to allow the reader to quickly ascertain the nature and gist of the technical disclosure. The Abstract is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIGS. 3A and 3B"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIGS. 4A and 4B"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
