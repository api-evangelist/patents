---
title: Synchronization with semaphores in a multi-engine GPU
abstract: A method for performing an operation using more than one resource may include several steps: requesting an operation performed by a resource; populating a ring frame with an indirect buffer command packet corresponding to the operation using a method that may include for the resource requested to perform the operation, creating a semaphore object with a resource identifier and timestamp, in the event that the resource is found to be unavailable; inserting a command packet (wait) into the ring frame, wherein the command packet (wait) corresponds to the semaphore object; and submitting the ring frame to the graphics engine.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08803900&OS=08803900&RS=08803900
owner: ATI Technologies ULC
number: 08803900
owner_city: Markham, Ontario
owner_country: CA
publication_date: 20111222
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application relates to resource management using semaphores in a multi-engine processor.",{"@attributes":{"id":"p-0003","num":"0002"},"figref":["FIG. 1","FIG. 1","FIG. 1"],"b":["100","100","100","105","115","110","120","125"]},"The CPU  and GPU  may be located on the same die (accelerated processing unit, APU). The CPU  may be any commercially available CPU, a digital signal processor (DSP), application specific integrated processor (ASIC), field programmable gate array (FPGA), or a customized processor. The CPU  and\/or GPU  may comprise of one or more processors coupled using a communication infrastructure, such as communication infrastructure . The CPU  and\/or GPU  may also include one or more processors that have more than one processing core on the same die such as a multi-core processor. The memory  may be located on the same die as the CPU  and\/or GPU , or may be located separately from the CPU  and\/or GPU . The memory  may include a volatile or non-volatile memory, for example, random access memory (RAM), dynamic RAM, or a cache.","The CPU  may execute an operating system (not shown) and one or more applications, and is the control processor for the system. The operating system executing on CPU  may control, facilitate access and coordinate the accomplishment of tasks with respect to system.","The graphics driver  may comprise software, firmware, hardware, or any combination thereof. In an embodiment, the graphics driver  may be implemented entirely in software. The graphics driver  may provide an interface and\/or application programming interface (API) for the CPU  and applications executing on the CPU  to access the GPU . As described above and herein, there may be more than one graphics driver , although only one is shown.","The communication infrastructure  may provide coupling between the components of system and may include one or more communication buses such as Peripheral Component Interconnect (PCI), Advanced Graphics Port (AGP), and the like.","The GPU  provides graphics acceleration functionality and other compute functionality to system . The GPU  may include multiple command processors (CP) CP 1 . . . CP n , multiple graphics engines (Engines) Engine 1 . . . Engine n , for example, 3D engines, unified video decoder (UVD) engines, or digital rights management (DRM) direct memory access (DMA) engines. GPU  may include a plurality of processors including processing elements such as arithmetic and logic units (ALU). It is understood that the GPU  may include additional components not shown in .","The CP 1 . . . CP n  may control the processing within GPU  and may be connected to Engine 1 . . . Engine n . Each CP 1 . . . CP n  may be associated with Engine 1 . . . Engine n  and each pair is an engine block (EB) EB 1 . . . EB n . In another embodiment, the CP 1 . . . CP n  may be a single command processor. In general, the CP 1 . . . CP n  receives instructions to be executed from the CPU , and may coordinate the execution of those instructions on Engine 1 . . . Engine n  in GPU . In some instances, the CP 1 . . . CP n  may generate one or more commands to be executed in GPU , that correspond to each command received from CPU . Logic instructions implementing the functionality of the CP 1 . . . CP n  may be implemented in hardware, firmware, or software, or a combination thereof.","The memory  may include a one or more memory devices and may be a dynamic random access memory (DRAM) or a similar memory device used for non-persistent storage of data. The memory  may include a timestamp memory 1-n  (corresponding to driver(s)) and indirect buffers . During execution, memory  may have residing within it, one or more memory buffers  through which CPU  communicates commands to GPU .","The memory buffers  may correspond to the graphics engines  or the engine blocks , as appropriate. Memory buffers  may be ring buffers or other data structures suitable for efficient queuing of work items or command packets. In the instance of a ring buffer, command packets may be placed into and taken away from the memory buffers  in a circular manner. For purposes of illustration, memory buffers  may be referred to as ring buffers  herein.","The indirect buffers  may be used to hold the actual commands, (e.g., instructions and data). For example, when CPU  communicates a command packet to the GPU , the command packet may be stored in an indirect buffer  and a pointer to that indirect buffer  may be inserted in a ring buffer . As described herein below with respect to , the CPU , via driver , as writer of the commands to ring buffers  and GPU  as a reader of such commands may coordinate a write pointer and read pointer indicating the last item added, and last item read, respectively, in ring buffers .","An operation, for example a drawing operation, may require multiple resources. These resources may be associated with more than one operation or graphics engine. When executing such an operation, there are several solutions for buffering the requests for the resources.","When a processor becomes backlogged with the requests, it can store the requests for later execution\u2014or even later overwrite, in a buffer, or more particularly a ring buffer. One advantage of a ring buffer is that it does not need to have its command packets shuffled around when one is consumed. This contrasts with non-ring buffers, where it is necessary to shift all packets when one is consumed. Said another way, the ring buffer is well-suited as a FIFO buffer while a standard, non-ring buffer is well-suited as a LIFO buffer.","Another memory management tool is the semaphore, which controls access to a common resource. It does this by acting as the gatekeeper to the resource, and noting how much of the resource is free after each processor accesses the resource (or frees up a resource when done). If the resource is free, the semaphore permits the next process to access the resource. If not, the semaphore directs the process to wait.","These memory management tools create long wait times if the resource is fully used, and the memory and thread use in the ring buffer may also take up resources. This wait time and memory usage may create performance issues for multiple engines that share the resources.","A method for performing an operation using more than one resource may include several steps, not necessarily in this order. First, requesting an operation performed by a resource. Second, populating a ring frame with an indirect buffer command packet corresponding to the operation using a method that may include for the resource requested to perform the operation, creating a semaphore object with a resource identifier and timestamp, in the event that the resource is found to be unavailable; inserting a command packet (wait) into the ring frame, wherein the command packet (wait) corresponds to the semaphore object; and submitting the ring frame to the graphics engine.",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 2","b":["201","209","215","235","201","202","204","203","208"]},"The registers  include a read pointer  and a write pointer . The engine ring  may include ring frames , , , and free space .  shows an example ring frame  that may include a plurality of command packets , a timestamp command packet , and an indirect buffer (IB) command packet  that points to the indirect buffer . The indirect buffer , as shown in , may include a plurality of command packets  that instruct the GPU  to carry out operations such as drawing an object to memory.","The above architecture may provide a one-way communication from a host processor, (the writer as represented by driver ), to the GPU , (the reader as represented by the command processor ). Initially, the read pointer  and the write pointer  point to the same location indicating that GFX ring  is empty. The GFX ring  has free space  into which the driver  may write a command packet corresponding to a task. The driver  then updates the write pointer  to one position past the last command packet  or the first available space. Following the update, the write pointer  and read pointer  point to different locations. The command processor  may fetch command packets at the read pointer  position and walk the read pointer  until it is equal to the write pointer .","For a GPU  with multiple engines and each engine running concurrently with another, semaphores may be used to control access by multiple engines to a common resource. An example of a scenario where this control is necessary is when there are two drawing operations that use the same piece of memory (resource). For simplicity, the first drawing operation may fill a small area of memory with zero and this drawing operation is submitted to Engine A. The second drawing operation may access the content of the memory and convert zero to one and this operation may be submitted to Engine B. In this case, a semaphore may be used to ensure that Engine B will not start executing the second drawing operation until the first drawing operation is completed by Engine A.",{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 3","b":["200","205","300","200","210","220","230","240","250","260","205","210","210","200","205","200","300","200"],"i":"a"},"Each resource object  may be associated with one or more resources, for example A, C, D, and E. In this example, resources A, C, D, and E are associated with resource objects A , C , D , and E . Resource objects may contain various information but for the sake of , we will focus its semaphore object relationship, engine last use information (in this example GPU Engine Gfx X or Gfx Y), and timestamp information. It should be appreciated that each resource object  has a corresponding semaphore object , with resource object  corresponding to semaphore object ,  with  and so on.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 4","FIG. 5"],"b":["500","515"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 5","FIG. 4"],"b":["110","500","300","200","500","500"]},"For the drawing operation assigned to submit to Gfx Z with the timestamp 88 (the next incremental timestamp of Gfx Z) that needs resources A, D, and E, the graphics driver may follow the process shown in the flow chart in , starting with the operation request itself in step  and the operation assigned to submit to GPU Engine Gfx Z and timestamp 88 in step . Following step , at step  and , the driver may update a free semaphore object (object , item  from ) the GPU Engine (Gfx Z) and timestamp (88) from step . This updated semaphore object is shown in  as reference ","Following step , the driver may determine whether there are resource requested (step ). There should be at minimum one resource requested. If there is no more resource, the process proceeds to step  that will be discussed in more detail below. And if the answer is YES, the driver determines whether the resource requested was used previously (step ).","For a resource requested that was not used previously, a blank resource object  is created with null or blank values for the semaphore object, GPU Engine, and timestamp fields.","If the answer to step  is NO, i.e., the resource requested was not used previously, the process proceeds to step . If the answer to step  is YES, however, a determination is made as to whether this resource (object A, item  from ) has an expired timestamp (step ). For the sake of this example, assume the current timestamps for Gfx X, Gfx Y, and Gfx Z are 212, 87, and 75 respectively, and because this resource object A has a timestamp of 213, the answer is NO. If it had expired, the process proceeds to step . Since it is not expired, the logic moves to step  and checks whether this resource object A GPU Engine is the same as the assigned GPU Engine for submitting this operation (step ). (It can do this by again, checking the resource A). Looking at , the resource A last used GPU Engine Gfx X and the current resource A being considered will use Gfx Z, so again the answer is NO. If YES, the driver would proceed to step  as before and update the resource object.","Since the answer was NO in step , at step , a determination is made whether there exists a semaphore object in the wait bucket that has the same GPU Engine as this resource. This is the first introduction of the wait bucket , which may be a storage area for the semaphore objects  before considering each semaphore object left in the wait bucket  at step  in the flow chart. Returning to the point in the flow chart under consideration, the answer to step  is NO, because the wait bucket  is empty. At step , the semaphore object corresponding to resource A  is added to the wait bucket . At step , the resource object A  is updated with the semaphore object number 4 (from step ), GPU Engine Gfx Z, and Timestamp 88, as shown in . At this point, before starting to examine the next resource, the wait bucket  has one semaphore object, semaphore object , with GPU Engine Gfx X and Timestamp 88.","After step , the driver determines if more resources are requested for the operation at step . Since resources D and E have also been requested, the answer is YES, and the driver proceeds as before until step , where a comparison is made between the semaphore object () in the wait bucket and the semaphore object () for the current resource (D) being considered, and a determination is made regarding whether their GPU engines the same. Looking at the semaphore objects  and , the GPU Engines are both Gfx X, and thus the answer is YES.","Proceeding to step , a comparison is made between the semaphore objects in the wait bucket  and the semaphore object for the current resource being requested , and a determination is made whether the current resource's timestamp greater. Again, the semaphore objects compared are  and , and the semaphore object  has a timestamp of 218, which is larger than the timestamp of 213 for semaphore object . Thus, at step  the wait bucket semaphore object  is removed from the wait bucket  and replaced with semaphore object . At step , the resource object D  is updated with the semaphore object, GPU Engine, and timestamp to create resource object . At this point, the semaphore object  is the only semaphore object in the wait bucket .","Finally, the driver considers the last resource requested: resource E. The flow through FIG. 's flowchart proceeds as before until step , where a determination is make whether there is a semaphore object in the wait bucket  that has the same GPU Engine as this resource E. In this case, the wait bucket  contains semaphore object  with a GPU Engine Gfx X. The current resource E has a semaphore object  with a GPU Engine Gfx Y. Since the GPU Engines are not the same, the answer to step  is NO, and semaphore object  is added to the wait bucket (step ). As before the resource object E  is updated with the semaphore object, GPU Engine, and timestamp to create resource object . At this point, the only semaphore objects in the wait bucket are  and .","Having considered all of the resources, the answer to step  is NO. The driver now determines if there is a semaphore object in the wait bucket (step ). If NO, the procedure skips to step ; if YES, the corresponding semaphore object is removed from the wait bucket and updated at step . In , these updated semaphore objects are and , which now have GPU Engine and timestamp of Gfx Z and 88. For each of these, a command packet (wait) is inserted in the ring frame  by the driver (step ). The command packet (wait)  corresponds to semaphore object and the command packet (wait)  corresponds to semaphore object . These command packet (wait)s, in this instance, direct a wait.","A command packet with a link to an indirect buffer  for the operation may be then inserted (step ). Such an indirect buffer  may instruct a GPU to carry out the operation. Then the command packet (signal)  corresponding to the semaphore object is inserted, indicated completion of the resources (step ), followed by a command packet marking the timestamp 88,  (step ). Finally, the ring frame  may be submitted to the GPU Engine for execution .","Although not shown, at this point, the wait bucket  should then be cleared, as each object in the wait bucket  was removed at step .","From reviewing the logic in , it may be apparent that there are two loops. One resource processing loop comprises steps , , , , , , , and . The other may be considered a wait command loop comprising steps , , and .","It should be understood that many variations are possible based on the disclosure herein. Although features and elements are described above in particular combinations, each feature or element may be used alone without the other features and elements or in various combinations with or without other features and elements","The methods provided may be implemented in a general purpose computer, a processor, or a processor core. Suitable processors include, by way of example, a general purpose processor, a special purpose processor, a conventional processor, a digital signal processor (DSP), a plurality of microprocessors, one or more microprocessors in association with a DSP core, a controller, a microcontroller, Application Specific Integrated Circuits (ASICs), Field Programmable Gate Arrays (FPGAs) circuits, any other type of integrated circuit (IC), and\/or a state machine. Such processors may be manufactured by configuring a manufacturing process using the results of processed hardware description language (HDL) instructions and other intermediary data including netlists (such instructions capable of being stored on a computer readable media). The results of such processing may be maskworks that are then used in a semiconductor manufacturing process to manufacture a processor which implements aspects of the present invention.","The methods or flow charts provided herein may be implemented in a computer program, software, or firmware incorporated in a non-transitory computer-readable storage medium for execution by a general purpose computer or a processor. Examples of computer-readable storage mediums include a read only memory (ROM), a random access memory (RAM), a register, cache memory, semiconductor memory devices, magnetic media such as internal hard disks and removable disks, magneto-optical media, and optical media such as CD-ROM disks, and digital versatile disks (DVDs)."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["A more detailed understanding may be had from the following description, given by way of example in conjunction with the accompanying drawings wherein:",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 2A"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 2B"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 5","FIGS. 5A and 5B"]}]},"DETDESC":[{},{}]}
