---
title: System and method for image sequence processing
abstract: A method for processing a sequence of images. In an embodiment, one or more training datasets are analyzed having a sequence of images showing a first condition and a sequence of images showing a second condition. A multivariate regression model is used to determine a relationship between relative positions of one or more features in the sequence of images showing the first condition and relative positions of the one or more features in the sequence of images showing the second condition. In an embodiment, the determined relationship is used to predict positions of the one or more features in an inquire sequence of images showing the second condition given an inquire sequence of images showing the first condition. The predicted positions can then be refined using various methods. In an embodiment, sequences of images are aligned to a common time scale.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08396268&OS=08396268&RS=08396268
owner: Isis Innovation Limited
number: 08396268
owner_city: Oxford
owner_country: GB
publication_date: 20100331
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF INVENTION","BRIEF SUMMARY","DETAILED DISCLOSURE"],"p":["Various systems and methods for image sequence processing can be used to study sequences of images. In some applications, such systems or methods are used to identify features within an image sequence and\/or to segment an image based on such identified features. In some applications, one or more sequences of images are compared, contrasted, or related. In another application, features of one image in a sequence are used to predict features of another image of the same sequence. Other applications of image sequence processing are known in the art. Such processing can be applied to various image subjects.","One established application of image sequence processing is the processing of echocardiography sequences. Real-time 2D echocardiography (ultrasound imaging of the heart) is a well-established clinical tool for cardiovascular disease diagnosis. One area of interest is stress echocardiography where heart disease is assessed by imaging at rest and then again after a stress stimulus is applied to the subject that makes the heart beat faster and harder. Although a stressed heart moves very differently to when it is at rest, the motion of the two sequences is related and it is this relation that a clinician is trying to infer qualitatively. Currently, the analysis is carried out through visual assessment by expert clinicians but this is very subjective and can result in discrepancies when assessed by different clinicians. In order to overcome these issues, the analysis techniques should be carried out in a more automated (i.e. less subjective) and quantitative manner.","One step toward automating such analysis is to delineate important structures within the heart. For the purpose of stress echocardiography, clinicians often observe the endocardial border of the left ventricle of the heart as it can provide important measures such as left ventricle volume and ejection fraction. Despite the development of segmentation and tracking techniques for the heart at rest, there is little literature on the same for the stressed heart, although some contribution to the field has been made by Shekhar and Leung. E.g., U.S. patent application Ser. No. 12\/371,111, V. Walimbe, M. Garcia, O. Lalude, J. Thomas, and R. Shekhar, \u201cQuantitative real-time three-dimensional stress echocardiography: A preliminary investigation of feasibility and effectiveness,\u201d 20(1), pp. 13-22, 2007; V. Zagrodsky, V. Walimbe, C. R. Castro-Pareja, J. X. Qin, J.-M. Song, and R. Shekhar, \u201cRegistration-assisted segmentation of real-time 3-d echocardiographic data using deformable models,\u201d 24(9), pp. 1089-1099, 2005; K. Y. Leung, M. van Stralen, A. Nemes, M. M. Voormolen, G. van Burken, M. L. Geleijnse, F. J. Ten Cate, J. H. Reiber, N. de Jong, A. F. van der Steen, and J. G. Bosch, \u201cSparse registration for three-dimensional stress echocardiography,\u201d 27(11), pp. 1568-1579, 2008. This limited progress in automating the analysis of stress echocardiography is because the motion of the heart under stress is much more difficult to model because of increased inter-subject variability and more complex dynamics in stress. Tracking algorithms that have been successfully applied to rest datasets cannot necessarily be applied directly to stress datasets as the image quality is poorer and the temporal resolution for the cardiac cycle is lower (around half that at rest). Optical flow based algorithms, for example, assume that the motion between frames is small and this is clearly violated in stress due to lower temporal resolutions. Another factor is that the shape variation throughout the frames from the end-diastolic (ED) to end-systolic (ES) is greater for stress than for rest data. An example of this variation can be seen in , which show rest and stress frames in an echocardiogram from ED to ES, respectively.","Embodiments of the subject invention relate to systems, methods, and apparatus for processing a plurality of images. In an embodiment, a plurality of images depicts a subject at a corresponding plurality of timepoints within a time period. In an embodiment, the plurality of images, called a sequence of images, are sequenced according to the order of their corresponding timepoints within the time period. In an embodiment, one or more training datasets are analyzed having a plurality of images showing a first condition and a plurality of images showing a second condition. In an embodiment, the first and second conditions are dynamic states and the images show movement, velocity, or other changes in the subject over time. In an embodiment, a multivariate regression model is used to determine a relationship between relative positions of one or more features in the plurality of images showing the first condition and relative positions of the one or more features in the plurality of images showing the second condition. In an embodiment, the determined relationship is used to predict positions of the one or more features in an inquire dataset having an inquire plurality of images showing the first condition and an inquire plurality of images showing the second condition. Given positions of the one or more features in the inquire plurality of images showing the first condition and the determined relationship, positions of the one or more features in the inquire plurality of images showing the second condition are predicted. The predicted positions can then be refined using various methods. In an embodiment, one or more pluralities of images are aligned to a common time scale. In a further embodiment, the aligned pluralities are then compared or used to predict positions of features in other pluralities of images.","Echocardiography is described below in detail as an illustrative example of an application of the present invention. As described below, the invention can be applied to compare a plurality of images of a heart at rest with a plurality of images of the heart under stress. Various useful information can be gleaned from the comparison. The systems and methods of the present invention can also be applied to other image sequence processing subjects.","Embodiments of the subject invention relate to systems, methods, and apparatus for processing a plurality of images. In an embodiment, a plurality of images depicts a subject at a corresponding plurality of timepoints within a time period. In an embodiment, the plurality of images, called a sequence of images, are sequenced according to the order of their corresponding timepoints within the time period. In an embodiment, one or more training datasets are analyzed having a first plurality of images showing a first condition and a second plurality of images showing a second condition. In an embodiment, the first plurality of images comprises images of a subject at a corresponding first plurality of timepoints within a first period of time and the second plurality of images of the subject comprises images of the subject at a corresponding second plurality of timepoints within a second period of time. In an embodiment, during the first period of time the subject is in the first condition and during the second period of time the subject is in the second condition. In an embodiment, the first and second conditions are dynamic states and the images show movement, velocity, or other changes in the subject over time. In an embodiment, a relationship is determined between relative positions of one or more features in the images showing the first condition and relative positions of the one or more features in the images showing the second condition. In an embodiment, a multivariate regression model is used to determine such a relationship. In an embodiment, the determined relationship is used to predict positions of the one or more features in an inquire dataset having a first plurality of images showing the first condition and a second plurality of images showing the second condition. Given positions of the one or more features in the first plurality of images and the determined relationship, positions of the one or more features in the second plurality of images are predicted. The predicted positions can then be refined using various methods. In an embodiment, a block-matching scheme is used to refine the predictions. In an embodiment, one or more pluralities of images are aligned to a common time scale. In a further embodiment, the aligned pluralities are then compared or used to predict positions of features in other images.","Embodiments of the subject invention can be applied to various image subjects. In an embodiment, the invention is applied to images of any dynamic subject, such as weather, earthquakes, and other natural forces; machines and other manufactured subjects; organisms or populations of organisms. In an embodiment, the invention is applied to images of exterior features, such as external organs, surfaces, or facial expressions. In an embodiment, the invention is applied to images of interior features, such as internal organs of organisms. In an embodiment, the invention is applied to images of a heart, lung, liver, or other organs. In a particular embodiment, the invention is applied to images of human lungs. In an embodiment, the subject is a liver or other organ wherein the organ's motion is affected by the respiration cycle. Specific embodiments are directed to images of structures that have movement patterns that follow a cycle, such as a beating heart, breathing lungs, or walking animal and\/or human.","Embodiments of the subject invention can be used to relate and\/or predict behavior of the various subjects under various conditions. In an embodiment, the change of a facial expression from smiling to frowning is related or predicted under different conditions. In an embodiment, walking motion, running motion, swimming motion, a golf swing, or other motion is related or predicted under different conditions. In an embodiment, motion at different rates is related. In an embodiment, different types of motion are related. For example, walking can be related to running, or trotting can be related to galloping, a slice can be related to a draw. In an embodiment, organ motion during rest and stress are related. In an embodiment, images of a subject at different levels or magnitudes of a stimulus are related. For example, in an embodiment, organ motion at different levels of rest and stress are related. In an embodiment, organ motion at different respiration rates are related. In an embodiment, heart motion during rest and stress are related. In an embodiment, reactions to a reagent, drug, or other therapy are studied. For example, the motion of a lung can be related both before or after application of therapy, or the motion of the lung at different levels of therapy can be related. In an embodiment, the invention can be used to analyze a subject's response to other stimulus, such as a learning exercise, emotional stimulation, or other stimulus. In an embodiment, fuctional MRI (fMRI) images are used to analyze brain response to different stimuli or levels of stimuli. The examples provided here are merely illustrative. Other subjects can also be studied.","Various principles can be used with the subject invention to relate different states or conditions. In an embodiment, conditions are related by assuming certain forces applied to a subject are balanced. In an embodiment, an additional force is applied to a subject in a second condition related to the first condition. In determining the relationship between the first condition and second condition, the additional force is accounted for. For example, in an embodiment, the equation cx+dy=ax+by+F is used to balance the forces, wherein xand yare components of a position of a feature in an image of the subject in the first condition, xand yare components of a position of the feature in an image of the subject in the second condition, F represents the additional force, and coefficients a, b, c, and d can represent stiffnesses or inertia of the feature in response to the force. The relationship can thus be defined by the values of the coefficients a, b, c, and d. In an embodiment, the positions of a plurality of features or landmarks are studied. For example, in an embodiment, the previous equation is modified as follows, wherein n landmarks are related:",{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["c","n"]},"mo":"\u2062","msubsup":{"mi":["x","n"],"mrow":{"mn":"2","mo":"\u2062","mi":"nd"}}},{"msub":{"mi":["d","n"]},"mo":"\u2062","msubsup":{"mi":["y","n"],"mrow":{"mn":"2","mo":"\u2062","mi":"nd"}}}],"mo":"+"}}},{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["a","n"]},"mo":"\u2062","msubsup":{"mi":["x","n"],"mrow":{"mn":"1","mo":"\u2062","mi":"st"}}},{"msub":{"mi":["b","n"]},"mo":"\u2062","msubsup":{"mi":["y","n"],"mrow":{"mn":"1","mo":"\u2062","mi":"st"}}}],"mo":"+"}}},"mo":"+","mi":"F"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}},"br":{}},{"@attributes":{"id":"p-0062","num":"0061"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["d","n"]},"mo":"\u2062","msubsup":{"mi":["x","n"],"mrow":{"mn":"2","mo":"\u2062","mi":"nd"}}},{"msub":{"mi":["e","n"]},"mo":"\u2062","msubsup":{"mi":["y","n"],"mrow":{"mn":"2","mo":"\u2062","mi":"nd"}}},{"msub":{"mi":["f","n"]},"mo":"\u2062","msubsup":{"mi":["z","n"],"mrow":{"mn":"2","mo":"\u2062","mi":"nd"}}}],"mo":["+","+"]}}},{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["a","n"]},"mo":"\u2062","msubsup":{"mi":["x","n"],"mrow":{"mn":"1","mo":"\u2062","mi":"st"}}},{"msub":{"mi":["b","n"]},"mo":"\u2062","msubsup":{"mi":["y","n"],"mrow":{"mn":"1","mo":"\u2062","mi":"st"}}},{"msub":{"mi":["c","n"]},"mo":"\u2062","msubsup":{"mi":["z","n"],"mrow":{"mn":"1","mo":"\u2062","mi":"st"}}}],"mo":["+","+"]}}},"mo":"+","mi":"F"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{}},"In an embodiment, one-dimensional images are used, such as electrocardiogram (ECG) images. In an embodiment, two-dimensional images are used, such as photographs, sonogram images, X-rays, magnetic resonance images (MRI), or other two-dimensional images. In an embodiment, three-dimensional images are used, such as three-dimensional sonogram images, or X-ray computed tomography images. In other embodiments, higher dimension images can be used.","Embodiments of the subject invention can be applied to compare and predict variation in images based on the presence, absence, or magnitude of various conditions. In an embodiment, movement, velocity, or other changes of a set of indentified features of the images are compared and\/or predicted between pluralities of images showing contrasting conditions. In an embodiment, Doppler effects are used to measure the velocity of tissue in an ultrasound image using a technique called Tissue Doppler. In an embodiment, the velocity of myocardial motion is measured using Tissue Doppler. In an embodiment, the images depict the same subject under contrasting conditions. In an embodiment, a control and an experimental condition are compared and\/or predicted. In a particular embodiment, the invention is applied to pluralities of images showing organ motion at different respiration rates. In another embodiment, the invention is applied to pluralities of images showing rest or stress conditions. In another embodiment, the invention is applied to pluralities of images showing an organism before or after the application of particular therapy. Various therapies can be studied including radiation, pharmacological, psychological, physical, and other types of therapies. In a particular embodiment, the invention is applied to pluralities of images showing drug or reagent uptake. In an embodiment, the invention is applied to pluralities of images showing responses to learning activities, emotional stimulation, or other stimulus. In other embodiments, the invention is applied to other conditions.","In an embodiment, images of a subject at different levels or magnitudes of a stimulus are related. For example, in an embodiment the subject's response to different levels of a therapy are related. In a particular embodiment, organ motion in a subject at different levels of rest and stress are related. For example, in an embodiment, a rest condition is related to a peak stress condition. In an embodiment, a partial stress condition is related to a peak stress condition. In an embodiment, a partial stress condition is related to another partial stress condition.","Various methods of measuring rest and stress in a subject are known in the art. In an embodiment, a rest condition is defined by an absence of activity in the subject. In an embodiment, the magnitude of stress is determined based on a percentage of maximum heart rate (MHR) exhibited by the subject. Various ways of defining maximum heart rate are known in the art. In an embodiment, MHR is defined as varying with age, for example, 220 beats less the age of the subject, 206.3 beats less the age of the subject multiplied by the coefficient 0.711 (MHR=206.3\u2212(0.711\u00d7age)). In other embodiments, MHR varies with sex, level of fitness, or type of activity, for example, some experts have found MHR to be higher when the subject is running on a treadmill than when bicycling. In an embodiment, peak stress is defined as about 85% of MHR. In an embodiment, a first percentage of MHR is determined for the subject under the first condition and a second percentage of MHR is determined for the subject under the second condition. In an embodiment, each percentage of MHR is stored, presented, or output for later use. In a embodiment, the first and second percentage are used to calculate a change in percentage of MHR for the subject between the first and second conditions. In an embodiment, the change in MHR is stored, presented, or output for later use.","Various ways of inducing stress in a subject are known in the art. In an embodiment, stress is induced in a manner that can be controlled. In an embodiment, stress is induced in a manner that can be measured. In an embodiment, a stress stimulus is applied to the subject. In an embodiment, the subject can be imaged during or shortly after the stress stimulus. In an embodiment, the stress stimulus comprises a pharmaceutical drug administered to the subject. For example, in an embodiment, a pharmaceutical drug, such as Dobutamine, Atropine, and\/or Dipyridamole is administered to increase a heart rate of the subject. In an embodiment, Dobutamine is used alone. In an embodiment, Atropine is used with Dobutamine. In an embodiment, Dipyridamole is used alone or in combination with Dobutamine. In an embodiment, the stress stimulus comprises physical exercise performed by the subject, such as running on a treadmill, using an elliptical machine, pedaling on a bicycle, or any other exercise where heart rate can be increased in a controlled manner in a testing environment. In an embodiment, the magnitude of the stress stimulus applied is measured and used in the methods of the subject invention as discussed below.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 1","b":["103","103"]},"In the embodiment shown, the image sequence processing system  includes an input interface , an output interface , memory  for program storage and\/or data storage, and a processor  for processing information. In an embodiment, the input interface  includes an input device  such as a mouse or other pointing device, a keyboard, or a communication device such a modem or other network interface. In a particular embodiment, the input device  is an imaging device such as a sonogram machine. In another embodiment, the image sequence processing system  is incorporated into an imaging device. Other input devices for receiving information are known in the art and can be used with the subject invention. In an embodiment, the input interface  serves to translate data received from the input device  into a format usable by the image sequence processing system . Thus, the input device  and the input interface  can be replaced without modifying the image sequence processing system  as known in the art. In an embodiment, the output interface  includes an output device  such as a monitor, printer, projector, or other display device, a speaker or other audio device, or a communication device such as a modem. Other output devices for presenting information are known in the art and can be used with the subject invention. In an embodiment, the output interface  serves to translate data received from the image sequence processing system  into a format usable by the output device . Thus, the output device  and the output interface  can be replaced without modifying the image sequence processing system  as known in the art. In an embodiment, the image sequence processing system  includes an application interface  for sharing information with other applications. For example, the image sequence processing system  can include an interface for communicating with an electronic medical records system. In an embodiment, the memory  includes computer-readable media embodying a computer-program product as described above. In an embodiment, the memory includes a database or other device for data storage. Other memory devices are known in the art and can be used with the subject invention. In an embodiment, the image sequence processing system  includes multiple input interfaces , output interfaces , memories , processors , input devices , output devices , or APIs .","In an embodiment, the image sequence processing system  includes one or more program modules, such as a temporal alignment module , a landmark identification module , a training module , a prediction module , an interpolation module , a refinement module , and\/or a classification module , as further described below.","In an embodiment, the temporal alignment module  is configured to align one or more pluralities of images to a time scale. In an embodiment, a plurality of images depicts a subject at a corresponding plurality of timepoints within a time period. In an embodiment, the plurality of images, called a sequence of images, are sequenced according to the order of their corresponding timepoints within the time period. In an embodiment, two or more pluralities of images are aligned to a common time scale. In an embodiment, images from two or more pluralities are grouped according to time value. For example, images from two pluralities can be paired when they correspond to, or are proximate to, a common point in time in a physical cycle such as respiration, cardiac movement, or other established physical cycles. Various methods of aligning a plurality of images to a time scale are known in the art and can be used with the present invention. In a particular embodiment, a method is used such as that described in relation to  below.","In an embodiment, the landmark identification module  is configured to identify one or more features of interest within an image. In an embodiment, the features include identifiable shapes or patterns within the image. In an embodiment, the features include variations in intensity, brightness, hue, saturation, luminosity, contrast, color, local phase features, and\/or other properties, within the image. In an embodiment, the features are described by their position within the image. In an embodiment, the position of a feature is described relative to an origin position within the image. For example, the top left corner of the image can be defined as the origin position. In an embodiment, the position of a particular feature defines the origin position. In an embodiment, the position of a feature is described relative to a displacement of the feature from the feature's identified position within a selected image. In an embodiment, a common set of features is identified in each image of a sequence or plurality of images. In an embodiment, a common set of features is identified in each image of multiple sequences or pluralities of images. In an embodiment, the position of a feature in each image is described relative to the identified position of the feature in one image of the plurality. In an embodiment, features include points or pixels of interest in the image. In an embodiment, features include sets of points or pixels in the image. In an embodiment, a feature is a line, a curve, a border, a surface, a volume, a cross-sectional slice, or other shape. In an embodiment, such shapes can be extrapolated from other identified features. In an embodiment, external software is used to identify one or more features and the results are communicated to the image sequence processing system  via the API . In an embodiment, features in images are identified or validated by technicians, radiologists, or other persons trained to analyze such images. In an embodiment, one or more features are identified or validated by visual inspection by such persons and communicated to the image sequence processing system  via the input interface .","In an embodiment, the identified features can define the general shape of an internal or external organ. In an embodiment, the identified features can define the contour or surface of the organ. In a particular embodiment, the subject is a lung and vessel junctions in the lung visible in computer tomography angiography (CTA) are identified. For example, for the right lung the landmarks could be the apex, the four points on the boundary where the horizontal fissure and the oblique fissure begin and end, and the bottom tip of the inferior lobe. In an embodiment, the boundary of the lung can be extrapolated from such points. In another embodiment, Quamus\u2122 software (developed by Mirada Solutions Ltd., Oxford, UK) is used to identify one or more landmarks within a plurality of sonogram images of a heart. In an embodiment, each image of the plurality of sonogram images depicts a particular view of the heart. Various views of the heart are shown in , which was taken from M. D. Cerqueira, N. J. Weissman, et. al, Standardized Myocardial Segmentation and Nomenclature for Tomographic Imaging of the Heart. Circulation, 105(4):539-542, 2002.","In an embodiment, the training module  is configured to develop a model of a relationship between pluralities of images. In an embodiment, a relationship between a plurality of images of a subject under a first condition and a plurality of images of the subject under a second condition is determined by the training module . In an embodiment, such a relationship model is developed according to a method such as the method discussed below in relation to .","In an embodiment, the prediction module  is configured to predict the positions of one or more features in a plurality of images of a second condition given a plurality of images of a first condition. In an embodiment, a relationship model is applied to make such a prediction. In an embodiment, the relationship model is developed according to the systems or methods of the present invention, such as training module  discussed above and\/or the method discussed below in relation to . In an embodiment, such a prediction is made according to a method such as the methods discussed below in relation to .","In an embodiment, the interpolation module  is configured to interpolate and\/or extrapolate one or more features in an image of a plurality of images from other features identified in the plurality of images. In an embodiment, a relative position of a feature in an image of the plurality is interpolated and\/or extrapolated based on the relative position of the feature in another image of the plurality. In an embodiment, a line, curve, border, a surface, a volume, a cross-sectional slice, or other shape in an image is extrapolated from one or more other features of images of the plurality. The one or more other features can be from the same image or different images of the plurality. In a particular embodiment, a plurality of landmarks is used to approximate an epicardial border and\/or an endocardial border in a plurality of echocardiography images, as further described below. In an embodiment, the epicardial border and\/or the endocardial border is a border of the left ventricle (LV) of a heart. Various methods known in the art can be used to interpolate such features, such as a quadratic spline, an nth degree polynomial, a piecewise cubic spline, or a natural cubic spline, among other methods. In an embodiment, the invention is not limited to any one interpolation method. As discussed below, an interpolation module such as the interpolation module  can be used in various steps in the methods discussed below in relation to .","In an embodiment, the refinement module  is configured to refine one or more features identified or predicted within an image of a plurality of images. As discussed above, various features can be identified, predicted, or interpolated within images. In an embodiment, one or more such features can be refined. In an embodiment, the position of one or more points or pixels that comprise the feature are updated or corrected by the refinement module . In an embodiment, the needed refinement is determined by comparing images of a first plurality of images to images of a second plurality of images. In an embodiment, the each of the first plurality of images exhibits a subject in a first condition and each of the second plurality of images exhibits the subject in a second condition. In an embodiment, the needed refinement is determined by comparing images of a first sequence of images to images of a second sequence of images. Various methods known in the art can be used to refine such features. In a particular embodiment, a method is used such as that described in relation to  below. In an embodiment, the invention is not limited to any one refinement method. As discussed below, a refinement module such as the refinement module  can be used in various steps in the methods discussed below in relation to .","In an embodiment, the classification module  is configured to classify the subject of one or more pluralities of images. In an embodiment, the classification module  derives features of interest and\/or performs calculations from one or more pluralities of images. In an embodiment, such features and calculations are used to classify the subject. In an embodiment, such calculations or features compare two or more pluralities of images. Possible configuration and use of the classification module  is further discussed below in relation to .","The components of an image sequence processing system can be housed on a single computer or distributed across a network as is known in the art. In an embodiment, components of the image processing system are distributed on computer-readable media as part of an image sequence analysis package, such as a stress-echocardiography analysis software package. In an embodiment, a user can access the image sequence processing system via a client device. In an embodiment, some of the functions of the image sequence processing system can be stored and\/or executed on such a device. Such devices can take any of a variety of forms. By way of example, a client device may be a desktop or laptop computer, a personal digital assistant (PDA), an MP3 player, a communication device such as a telephone, pager, email reader, or text messaging device, or any combination of these or other devices. In an embodiment, a client device can connect to the image sequence processing system via a network. As discussed below, the client device may communicate with the network using various access technologies, both wireless and wireline. Moreover, the client device may include one or more input and output interfaces that support user access to the image sequence processing system. Such user interfaces may further include various input and output devices which facilitate entry of information by the user or presentation of information to the user. Such input and output devices can include, but are not limited to, a mouse, touch-pad, touch-screen, or other pointing device, a keyboard, a camera, a monitor, a microphone, a speaker, a printer, a scanner, sonogram machine, among other such devices. As further discussed below, the client devices can support various styles and types of client applications.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIG. 2","b":["201","201","121"]},"At a step , a plurality of images of a subject is received showing the subject at a corresponding plurality of points in time in an established physical cycle exhibited by the subject. Various physical cycles can be analyzed with the subject invention. For example, in an embodiment, the subject is a lung and the established physical cycle is a respiration cycle. In an embodiment, the subject is a cell and the established physical cycle is a metabolic process in the cell. In an embodiment, the subject is a lung and the established physical cycle is a respiration cycle. In an embodiment, the subject is a liver or other organ wherein the organ's motion is affected by the respiration cycle. In an embodiment, the subject is a heart and the established physical cycle is a cardiac cycle. In an embodiment, each image shows the subject at a different point in time in the established physical cycle. In an embodiment, the plurality of images is a sequence of images ordered from the earliest point in time to the latest. In an embodiment, the plurality of images includes multiple cycles of the physical cycle. In an embodiment, the plurality of images includes partial cycles of the physical cycle. In an embodiment, the plurality of images depicts a cardiac cycle of a heart from an end-diastolic phase (ED) to an end-systolic phase (ES).","In a particular embodiment, the plurality of images is a plurality of ultrasound images. In an embodiment, the plurality of images is a plurality of echocardiography images. In an embodiment, each image of the plurality of echocardiography images depicts a particular view of the heart. Various views of the heart useable with the subject invention are shown in . In an embodiment, the plurality of images is received via an input interface such as the input interface  and\/or an input device such as the input device . In an embodiment, the plurality of images is received via an API such as the API . In an embodiment, the plurality of images is received from a sonogram machine. In an embodiment, the plurality of images is received in a standard format such as the DICOM format set by the DICOM Standards Committee.","At a step , at least three phase images are identified from the plurality of images corresponding to phases in the established physical cycle. For example, in an embodiment, phase images correspond to the beginning of expiration, the ending of expiration, the beginning of inspiration, and\/or the ending of inspiration in a respiration cycle. In an embodiment, the phases are identified by analyzing movement, velocity, or other changes in the subject exhibited in the plurality of images, such as the expansion and contraction of a lung. In an embodiment, other inputs or measurements are used to identify such phase images. For example, an airflow sensor can be used to identify the beginning and ending of expiration in a respiration cycle. In another embodiment, an ECG signal is used to identify phases in the cardiac cycle. In an embodiment, the images are echocardiography images with corresponding ECG images superimposed thereon. Thus, the ECG signal can be used to identify phase images in the echocardiogram. In another embodiment, a capture time of an image is recorded and used to identify one or more phase images. In an embodiment, the plurality of images does not contain an image that precisely corresponds to a phase of the established cycle. In an embodiment, the closest image is identified as the corresponding phase image. In an embodiment, an image identified if it is determined to be within a certain range of time of the phase. The certain range of time could be a constant, such as 0.1 seconds. Or it could be vary based on the temporal resolution of the studied image sequence, or other factors. In an embodiment, an image is not identified for every established phase in the physical cycle.","At a step , the at least three phase images are assigned time values based on their timing within the established physical cycle. In an embodiment, at least some of the assigned values are to scale in that they are proportional to values assigned to images corresponding to other phases or the length of the entire physical cycle. For example, in a respiration cycle, inspiration normally takes about twice as long as expiration. Thus, the values assigned to the beginning of inspiration and the ending of inspiration can be about twice as close together as the values assigned to the beginning of expiration and the ending of expiration. In an embodiment, the assigned values are not to scale. In an embodiment, the values are assigned based on the order of the phases in the physical cycle. For example, the phase image corresponding to the earliest phase can be assigned time value 1, the phase image corresponding to the next phase can be assigned time value 2, etc.","At a step , time values are assigned to one or more images of the plurality of images which are not identified as corresponding to a phase in the established physical cycle (non-phase images). In an embodiment, time values are assigned to all of the non-phase images of the plurality of images. In an embodiment, the time value assigned to a non-phase image is based on the non-phase image's timing with respect to two or more of the at least three phase images. For example, in an embodiment, when a phase image was captured between two phase images, the phase image is assigned a time value between the time values assigned to the two phase images. In an embodiment, the capture time of one or more images is used to calculate the time value to be assigned. For example, in an embodiment, the capture time of the non-phase image is compared to the capture time of at least one phase image and a proportional time value is calculated and assigned. In an embodiment, it is assumed that images are captured at regular intervals and therefore time values assigned to images captured between two phase images are evenly distributed between the time values of the phase images. For example, in an embodiment, the images are ultrasound images captured at regular intervals. In an embodiment, the interval is defined by the temporal resolution of the sonogram machine used to capture the images. In an embodiment, non-phase images are assigned time values based on at least two piecewise interpolation functions, wherein for each consecutive pair of phase images of the at least three phase images one of the at least two piecewise interpolation functions is applied to the images sequenced between the consecutive pair of phase images to determine the assigned time values of the images based on the time values assigned to the consecutive pair of phase images. In an embodiment, one or more of the at least two piecewise interpolation functions is a linear function.","In an embodiment, the resulting time values are stored, presented, or output for later use. In an embodiment, the resulting time values are stored in a memory such as the memory . In an embodiment, the time values are presented, transmitted, or otherwise output via an output interface such as the output interface  and\/or an output device such as the output device . In an embodiment, the time values are transmitted via an API such as the API . In an embodiment, the time values are stored, presented, or output as a delimited sequence of time values, wherein each time value provides a relative time value for each image in a sequence. In an embodiment, the delimited sequence of time values can be written to a flat file and later read by a suitably programmed file reader.",{"@attributes":{"id":"p-0087","num":"0086"},"figref":"FIG. 3","b":["301","301","123"]},"At a step , a first plurality of images of a subject under a first condition and a second plurality of images of the subject under a second condition is received. As discussed above, embodiments of the subject invention can be used to analyze images of various subjects under various conditions. For example, in an embodiment, the subject is a lung and the first plurality captures images of the lung as a control and the second plurality captures images of the lung after an asthma or other medicament has been administered. In an embodiment, each of the first and the second plurality comprise images of the subject at a plurality of timepoints. In an embodiment, each of the first and the second plurality comprise images of the subject at a plurality of points in time within a physical cycle exhibited by the subject.","In an embodiment, the first plurality and the second plurality comprise a training dataset related to the subject. In an embodiment, additional training datasets are received comprising pluralities of images of additional subjects under the first condition and the second condition. In an embodiment, each plurality is a sequence is ordered from the earliest point in time to the latest. In an embodiment, a plurality includes multiple cycles of the physical cycle. In an embodiment, a plurality includes partial cycles of the physical cycle. As further discussed below in relation to , various types of training data can be used with the method  for various purposes. In an embodiment, the training data used exhibits a specific class of behavior for the subject in order to predict the class of behavior in an inquire subject. For example, in an embodiment, the training data used exhibits normal behavior for the subject in order to predict a normal behavior in the inquire subject. In an embodiment, the training data used exhibits a specific class of abnormal behavior for the subject in order to predict the class of abnormal behavior in the inquire subject.","In a particular embodiment, each plurality of images is a plurality of ultrasound images. In an embodiment, each plurality of images is a plurality of echocardiography images. In an embodiment, a plurality is received via an input interface such as the input interface  and\/or an input device such as the input device . In an embodiment, a plurality is received via an API such as the API . In an embodiment, a plurality is received from a sonogram machine. In an embodiment, a plurality is received in a standard format such as the DICOM format set by the DICOM Standards Committee.","At a step , each image of each plurality is aligned to a common time scale. Various methods can be used to align the images to the common time scale. For example, in an embodiment, the capture times of the images are used. In an embodiment, a temporal baseline image is identified in each plurality, wherein each temporal baseline image corresponds to a common point in time. Each temporal baseline image can thus be assigned the same time value. Then, the other images of each plurality can be assigned values based on the differences between the capture time of the other images and the capture time of the temporal baseline image. In an embodiment, the common time scale is based on an established physical cycle exhibited by the subject as discussed above. In an embodiment, the temporal baseline image can be identified in the same manner as a phase image as discussed above. For example, in an embodiment, an image of a lung when it is maximally inflated or maximally deflated (points in the respiration cycle that are easily identified by clinicians) can be used as the temporal baseline image. In an embodiment, a method such as the method  is used to align each plurality to a common time scale. In an embodiment, the temporal alignment module such as the temporal alignment module  is used to align each plurality to a common time scale.","At a step , a set of common features or landmarks are identified in each image of each plurality. In an embodiment, such features include points or pixels of interest in each image. In an embodiment, such features include sets of points or pixels in each image. In an embodiment, such features include lines, curves, borders, surfaces, volumes, or other shapes. In an embodiment, a single feature is identified in each image. In an embodiment, a plurality of features are identified in each image. In an embodiment, the plurality of features relate to and can define larger features such as lines, shapes, curves, borders, surfaces, or volumes. In an embodiment, the plurality of features can define a border of an object such as an internal or external organ.","Various methods known in the art can be used to identify such features in an image. In an embodiment, a landmark identification module such as the landmark identification module  is used. In an embodiment, after a feature is identified in a first image of a plurality, optical flow based registration\/motion extraction is used to identify the feature in other images of the plurality. In an embodiment, features are identified, validated, or corrected by visual inspection by one or more persons.","At a step , for each plurality a position is determined for each member of the set of common features at a plurality of points in time within the common time scale. In an embodiment, if an image of a plurality is aligned to one of the plurality of points in time, then for that plurality the positions of the features at that time point are determined based on the positions of the features within that image. In an embodiment, if no image of a plurality is aligned to one of the plurality of points in time, the positions of the features for that plurality at that time point are interpolated based on the known positions of the features in that plurality at other time points. Various interpolation methods known in the art can be used to interpolate a position of a feature from the known positions of the feature. In an embodiment, an interpolation module such as the interpolation module  is used.","In an embodiment, the position determined for each feature or landmark is a relative position. In an embodiment, the relative position of a feature or landmark is defined by the displacement of the feature from a defined origin position. In an embodiment, the origin position is defined for all images as an established location within each image, such as the top left corner of the image. In an embodiment, the origin position is defined for each image as the position of another feature or landmark in the image. In an embodiment, the origin position is defined for each plurality for each landmark as a position of the landmark in a positional baseline image of the plurality. In an embodiment, the positional baseline image is a phase image. In an embodiment, the positional baseline image corresponds to the earliest phase of an established physical cycle.","At a step , for each of the plurality of points in time a relationship is determined between:","the positions of the members of the set of common features under the first condition; and","the positions of the members of the set of common features under the second condition. In an embodiment, a multivariate regression model is used to determine such a relationship. In an embodiment, a linear multivariate regression model is used. In an embodiment, a CCA model, PCA model, ICA model, and\/or PLSR model is used. In an embodiment, a plurality of piecewise linear models can be used to introduce some level of non-linearity to the model. For example, in an embodiment, a plurality of piecewise CCA models are used to determine the relationship. In an embodiment, the relationship is modeled using quadratic, polynomial, or other non-linear functions. Such functions can also be applied piecewise. Various other methods are known in the art and can be used in embodiments of the subject invention. In an embodiment, the relationship is determined based on a difference in work energy required under the first condition and the second condition, assuming energy is conserved.","In an embodiment, the determined relationship is used to predict the behavior of a subject under the second condition given the behavior of the subject under the first condition. In an embodiment, a method, such as the method  described next or the method  described thereafter, is used to predict such behavior. In an embodiment, the relationship is stored, presented, or output for later use. In an embodiment, the relationship is stored in a memory such as the memory . In an embodiment, the relationship is presented, transmitted, or otherwise output via an output interface such as the output interface  and\/or an output device such as the output device . In an embodiment, the relationship is transmitted via an API such as the API . In an embodiment, the relationship is stored, presented, or transmitted as a matrix.",{"@attributes":{"id":"p-0100","num":"0099"},"figref":"FIG. 4","b":["401","401","125","131"]},"At a step , a first plurality of images of a subject under a first condition and a second plurality of images of the subject under a second condition is received. As discussed above, embodiments of the subject invention can be used to analyze images of various subjects under various conditions. For example, in an embodiment, the subject is a lung and the first plurality captures images of the lung as a control and the second plurality captures images of the lung after an asthma or other medicament has been administered. In an embodiment, each of the first and the second plurality comprise images of the subject at a plurality of timepoints. In an embodiment, each of the first and the second plurality comprise images of the subject at a plurality of points in time within a physical cycle exhibited by the subject. In an embodiment, each plurality is ordered from the earliest point in time to the latest. In an embodiment, a plurality includes multiple cycles of the physical cycle. In an embodiment, a plurality includes partial cycles of the physical cycle.","In a particular embodiment, each plurality of images is a plurality of ultrasound images. In an embodiment, each plurality of images is a plurality of echocardiography images. In an embodiment, a plurality is received via an input interface such as the input interface  and\/or an input device such as the input device . In an embodiment, a plurality is received via an API such as the API . In an embodiment, a plurality is received from a sonogram machine. In an embodiment, a plurality is received in a standard format such as the DICOM format set by the DICOM Standards Committee.","At a step , each image of each plurality is aligned to a common time scale. As previously discussed in relation to the method  and the step , various methods can be used to align the images to the common time scale.","At a step , a set of common features or landmarks are identified in each image of each plurality. As previously discussed in relation to the step , various methods can be used to identify such features or landmarks. In an embodiment, the same set of common features is also used to analyze a training data set such as by the method  described above.","At a step , for the first plurality a position is determined for each member of the set of common features at a plurality of points in time within the common time scale. In an embodiment, if an image of the plurality is aligned to one of the plurality of points in time, then the positions of the features at that time point are determined based on the positions of the features within that image. In an embodiment, if no image of the plurality is aligned to one of the plurality of points in time, the positions of the features at that time point are interpolated based on the known positions of the features in the plurality at other time points. Various interpolation methods known in the art can be used to interpolate a position of a feature from the known positions of the feature. In an embodiment, an interpolation module such as the interpolation module  is used. In an embodiment, the position determined for each feature or landmark is a relative position as discussed above in relation to the step .","At a step , for the second plurality a position is predicted for each member of the set of common features at the plurality of points in time. In an embodiment, such prediction is made based on outcome of the step . In an embodiment, such prediction is also based on a determined relationship between:","the positions of the members of the set of common features under the first condition; and","the positions of the members of the set of common features under the second condition.","Various methods of determining such a relationship can be used with embodiments of the subject invention. In an embodiment, such a relationship is determined by analyzing empirical evidence. In an embodiment, such a relationship is determined by analyzing one or more training datasets such as by the method  discussed above. Various training data can be used to determine the relationship. In an embodiment, the training data used exhibits a specific class of behavior for the subject in order to predict the class of behavior in response to the second condition given the positions in the first condition. In an embodiment, the training data used exhibits normal behavior for the subject in order to predict a normal response to the second condition given the positions in the first condition. In an embodiment, the training data used exhibits a specific class of abnormal behavior for the subject in order to predict the class of abnormal behavior in response to the second condition given the positions in the first condition. In an embodiment, the relationship is determined by assuming certain forces applied to the subject are balanced. In an embodiment, an additional force is applied to the subject in the second condition. In determining the relationship between the first condition and second condition, the additional force is accounted for. In an embodiment, the relationship is determined based on a difference in work energy required under the first condition and the second condition, assuming energy is conserved. In an embodiment, a difference in the work energy expended by the subject under the first condition and under second condition is used to make such a prediction.","In an embodiment, the predicated positions are stored, presented, or output for later use. In an embodiment, the predicated positions are stored in a memory such as the memory . In an embodiment, the predicated positions are presented, transmitted, or otherwise output via an output interface such as the output interface  and\/or an output device such as the output device . In an embodiment, the predicated positions are transmitted via an API such as the API . In an embodiment, the predicated positions are used to derive one or more features and\/or perform one or more calculations as described below in relation to the step . In an embodiment, the predicted positions can be stored, presented, or output as a delimited sequence of coordinates. In an embodiment, such a delimited sequence can be written to a flat file and later read by a suitably programmed file reader.","In an embodiment, the method  proceeds to a step  wherein for each image in the second plurality a position is predicted for each member of the set of common features. In an embodiment, if an image of the plurality is aligned to one of the plurality of points in time, then the positions of the features in that image are determined based on the positions of the features at that time point. In an embodiment, if an image of the plurality is not aligned to one of the plurality of points in time, the positions of the features in that image are interpolated based on the predicted positions of the features at other time points. Various interpolation methods known in the art can be used to interpolate a position of a feature from the predicted positions of the feature. In an embodiment, an interpolation module such as the interpolation module  is used. In an embodiment, the position predicted for each feature or landmark is a relative position as discussed above in relation to the step . In an embodiment, the predicated positions of the step  are stored, presented, or output for later use as discussed above in relation to the step . In an embodiment, the predicated positions are used to derive one or more features and\/or perform one or more calculations as described below in relation to the step .","In an embodiment, the method  proceeds to a step  wherein a larger feature in the images of the second plurality is approximated based on the predicated positions of the step . In an embodiment, the larger feature is a shape comprising the predicated positions. In an embodiment, the shape is a line, a curve, a surface, a volume, a cross-sectional slice, or other shapes. In an embodiment, the shape is a border. In an embodiment, the border approximates the boundary to an internal or external organ. In an embodiment, the organ is a lung. In an embodiment, the organ is a heart. In an embodiment, the border approximates an endocardial boundary of the heart. Various methods known in the art can be used to interpolate such features, such as a quadratic spline, an nth degree polynomial, a piecewise cubic spline, or a natural cubic spline, among other methods. In an embodiment, the invention is not limited to any one interpolation method. In an embodiment, an interpolation module such as the interpolation module  is used.","In an embodiment, the approximated boundary is stored, presented, or output for later use. In an embodiment, the predicated positions are stored in a memory such as the memory . In an embodiment, the approximated boundary is presented, transmitted, or otherwise output via an output interface such as the output interface  and\/or an output device such as the output device . In an embodiment, the approximated boundary is transmitted via an API such as the API . In an embodiment, the approximated boundary is used to derive one or more features and\/or perform one or more calculations as described below in relation to the step .","In an embodiment, the method  proceeds to a step  wherein a position of a feature of an image of the second plurality is refined. In an embodiment, the refined feature is a point. In an embodiment, the feature is part of a larger feature such as a shape. In an embodiment, the feature is a point on a boundary. In an embodiment, positions of two or more features are refined. In an embodiment, positions of features of two or more images of the second plurality are refined.","Various methods known in the art can be used to refine the position of a feature. In an embodiment, the position of the feature is refined based on information from the first plurality. In an embodiment, the feature is refined based on information from a corresponding image of the first plurality, wherein the corresponding image is aligned to the same point in time in a common time scale as the image of the second plurality. In an embodiment, a method such as the method , described below, is used. In an embodiment, the refined boundary of the step  is stored, presented, or output for later use as discussed above in relation to the step .","In an embodiment, the method  proceeds to a step  wherein one or more features are derived and\/or one or more calculations are performed. In an embodiment, one or more calculations are preformed based on the predicted positions of the set of common features. In an embodiment, the predicted positions are used to approximate at least one feature as described above in relation to the step . In an embodiment, a feature is approximated for each of the plurality of points in time. In an embodiment, the calculations are preformed based on the approximated feature(s). Various calculations of interest will be known to one skilled in the art and familiar with the analysis of the types of images included in the plurality.","For example, in an embodiment, a heart is the subject of the images. In an embodiment, the approximated feature(s) include an endocardial border of the heart. In an embodiment, the border of the heart of a patient after a stress stimulus is approximated. In an embodiment, the border approximates the endocardial border of the left ventricle (LV) of the heart. Various calculations can be made from the endocardial border of the left ventricle. In an embodiment, a left ventricular area and\/or volume is calculated. In an embodiment, such an area and\/or volume is calculated at various time points within the cardiac cycle of the heart. In an embodiment, the images are three-dimensional images that allow a three-dimensional endocardial border to be approximated. A volume can then be calculated from the three-dimensional border. In an embodiment, the images are two-dimensional images that allow a two-dimensional border to be approximated. In an embodiment, a volume is estimated from the two dimensional border. In an embodiment, such a volume is estimated, such as by using perpendicular views of the LV and\/or by assuming an ellipsoidal shape. In an embodiment, the area and\/or volume is calculated at the end diastolic (ED) and\/or the end systolic (ES) phase of the cycle. In an embodiment, the end diastolic volume (EDV) and the end systolic volume (EDV) are used to calculate the stroke volume (SV) and\/or the ejection fraction (EF) as known in the art. Various other calculations can be made, such as radial shortening, circumferential length and\/or surface area, curvatures, velocities, and\/or endocardial motion fields. In an embodiment, features on the epicardial (outer LV) boundary are also tracked and myocardial strain can be calculated from the positions thereof. In an embodiment, myocardial thickening is calculated from tracked features of the heart.","In an embodiment, calculations can be made based on information derived about the behavior of the subject under the first condition and the second condition. In an embodiment, observed data about the behavior of the subject under the first condition is combined with predicted data about the behavior of the subject under the second condition in order to yield such calculations. For example, where the subject is a heart, an embodiment can calculate the portion of the LV in stress that lies outside the boundary of the LV in rest in the ED to ES stage of the cardiac cycle. This measure is often considered by clinicians evaluating the health of the heart. The example calculations presented here are merely illustrative. Various other calculations are also possible.","In an embodiment, one or more results of the step  are stored, presented, or output for later use. In an embodiment, the results are stored in a memory such as the memory . In an embodiment, such results are presented, transmitted, or otherwise output via an output interface such as the output interface  and\/or an output device such as the output device . In an embodiment, such results are transmitted via an API such as the API . In an embodiment, the results can be stored, presented, or output as a delimited sequence of coordinates. In an embodiment, such a delimited sequence can be written to a flat file and later read by a suitably programmed file reader.","In an embodiment, the method  proceeds to a step  wherein the subject of the plurality is classified based one or more results of the step . In an embodiment, the subject is classified as normal or abnormal. In an embodiment, the subject is classified into established categories. Various classification techniques can be used. In an embodiment, a supervised classification technique is used wherein a machine learning technique learns a function for classification from training data, which includes measurements or calculations upon which to base the classification (inputs) paired with their corresponding classes (outputs). In an embodiment, the training data includes one more calculations described in the step  above. Various learning techniques can be used, such as a naive Bayes' classifier and a random forests classifier. The naive Bayes' classifier is based on Bayes' theorem and makes strong independence assumptions about the inputs. A na\u00efve Bayes classifier also assumes that all the inputs are equally powerful in their ability to distinguish between classes. In an embodiment, random forests can produce a very accurate classifier because of its ability to estimate the importance of variables in the classification (i.e., all inputs are not thought to be equally important as in the naive Bayes' classifier). In an embodiment, one or more results from the step  are used as inputs to the trained classifier to produce a classification for the subject.","Various training data can be used with the various learning techniques to create a suitable classifier for the subject. In an embodiment, the training data used exhibits at least one specific class of behavior for the subject. The subject can then be classified as one or more of the at least one specific class when there is good agreement between the behavior of the subject and each of the one or more of the at least one specific class. In an embodiment, similar training data is used in the method  to determine the relationship between the first condition and the second condition. The relationship is then used to predict the relative position of each landmark in the step . Such training can facilitate classification of a specific class of behavior in the step . For example, normal data can be used in the method  so that a normal response to the second condition is predicted in the step  given the positions of each landmark in the first condition. Then, the actual response of the subject can be determined from the plurality of images of the subject under the second condition. In an embodiment, the actual response is determined by identifying the common set of landmarks in the plurality of images of the subject under the second condition. In an embodiment, such identification can be accomplished as described in the step . In the step , the actual response can be compared to the predicted response. Where strong agreement is found between the actual and predicted responses, the subject can be classified as normal. In an embodiment, the classification that results from the step  can be encoded for later use. Various methods to encode such information will be known to one skilled in the art.","In an embodiment, one or more results of the step  are stored, presented, or output for later use. In an embodiment, the results are stored in a memory such as the memory . In an embodiment, such results are presented, transmitted, or otherwise output via an output interface such as the output interface  and\/or an output device such as the output device . In an embodiment, such results are transmitted via an API such as the API .","In an embodiment, after classification of the subject a message is automatically transmitted to a clinician for further action. In an embodiment, such a message is transmitted only when an abnormal result is obtained. In an embodiment, such a message is transmitted to a client device. Various communication techniques can be used as further discussed below. The clinician can then verify the result and take further action as indicated.",{"@attributes":{"id":"p-0124","num":"0123"},"figref":"FIG. 5","b":["501","501","125"]},"At a step , a plurality of images of a subject under a first condition is received. As discussed above, embodiments of the subject invention can be used to analyze images of various subjects under various conditions. In an embodiment, the plurality comprises images of the subject at a plurality of timepoints. In an embodiment, the plurality comprises images of the subject at a plurality of points in time within a physical cycle exhibited by the subject. In an embodiment, the plurality is a sequence ordered from the earliest point in time to the latest. In an embodiment, the plurality includes multiple cycles of the physical cycle. In an embodiment, the plurality includes partial cycles of the physical cycle.","In a particular embodiment, the plurality of images is a plurality of ultrasound images. In an embodiment, the plurality of images is a plurality of echocardiography images. In an embodiment, the plurality is received via an input interface such as the input interface  and\/or an input device such as the input device . In an embodiment, the plurality is received via an API such as the API . In an embodiment, the plurality is received from a sonogram machine. In an embodiment, the plurality is received in a standard format such as the DICOM format set by the DICOM Standards Committee.","At a step , each image of the plurality is aligned to an established time scale. In an embodiment, the established time scale corresponds to an established physical cycle exhibited by the subject. As previously discussed in relation to the method  and the step , various methods can be used to align the images to a time scale.","At a step , a set of common features or landmarks are identified in each image of the plurality. As previously discussed in relation to the step , various methods can be used to identify such features or landmarks. In an embodiment, the same set of common features is also used to analyze a training data set such as by the method  described above.","At a step , based on the images of the plurality a position is determined for each member of the set of common features at a plurality of points in time within the established time scale. In an embodiment, if an image of the plurality is aligned to one of the plurality of points in time, then the positions of the features at that time point are determined based on the positions of the features within that image. In an embodiment, if no image of the plurality is aligned to one of the plurality of points in time, the positions of the features at that time point are interpolated based on the known positions of the features in the plurality at other time points. Various interpolation methods known in the art can be used to interpolate a position of a feature from the known positions of the feature. In an embodiment, an interpolation module such as the interpolation module  is used. In an embodiment, the position determined for each feature or landmark is a relative position as discussed above in relation to the step .","At a step , a position is predicted for each member of the set of common features at the plurality of points in time wherein the subject is under a second condition. As discussed above, embodiments of the subject invention can be used to analyze images of various subjects under various conditions. For example, in an embodiment, under the first condition the subject is unmedicated (a control) and under the second condition a medicament has been administered to the subject. In an embodiment, the invention can be used to analyze drug and\/or reagent uptake in a subject. In an embodiment, the invention can be used to analyze a subject's response to other stimulus, such as a learning exercise, emotional stimulation, or other stimulus. In an embodiment, functional MRI (fMRI) images are used analyze brain response to different stimuli.","In an embodiment, such prediction is made based on the outcome of the step . In an embodiment, such prediction is also based on a determined relationship between:","the positions of the members of the set of common features under the first condition; and","the positions of the members of the set of common features under the second condition. Various methods of determining such a relationship can be used with embodiments of the subject invention. In an embodiment, such a relationship is determined by analyzing empirical evidence. In an embodiment, such a relationship is determined by analyzing one or more training datasets such as by the method  discussed above. In an embodiment, the relationship is determined based on a difference in work energy required under the first condition and the second condition, assuming energy is conserved. In an embodiment, a difference in the work energy expended by the subject under the first condition and under second condition is used to make such a prediction.","In an embodiment, the predicated positions are stored, presented, or output for later use. In an embodiment, the predicated positions are stored in a memory such as the memory . In an embodiment, the predicated positions are presented, transmitted, or otherwise output via an output interface such as the output interface  and\/or an output device such as the output device . In an embodiment, the predicated positions are transmitted via an API such as the API .",{"@attributes":{"id":"p-0135","num":"0134"},"figref":"FIGS. 4 and 5"},{"@attributes":{"id":"p-0136","num":"0135"},"figref":"FIG. 6","b":["601","601","129"]},"At a step , a first plurality of images of a subject under a first condition and a second plurality of images of the subject under a second condition is received. As discussed above, embodiments of the subject invention can be used to analyze images of various subjects under various conditions. In an embodiment, each of the first and the second plurality comprise images of the subject at a plurality of timepoints. In an embodiment, each of the first and the second plurality comprise images of the subject at a plurality of points in time within a physical cycle exhibited by the subject. In an embodiment, each plurality is ordered from the earliest point in time to the latest. In an embodiment, a plurality includes multiple cycles of the physical cycle. In an embodiment, a plurality includes partial cycles of the physical cycle.","In a particular embodiment, each plurality of images is a plurality of ultrasound images. In an embodiment, each plurality of images is a plurality of echocardiography images. In an embodiment a plurality is received via an input interface such as the input interface  and\/or an input device such as the input device . In an embodiment, a plurality is received via an API such as the API . In an embodiment, a plurality is received from a sonogram machine. In an embodiment, a plurality is received in a standard format such as the DICOM format set by the DICOM Standards Committee.","At a step , at least one time-aligned pair of images is identified having a first image from the first plurality of images and a second image from the second plurality of images, wherein the first image comprises an image of the subject at a particular point in time and the second image comprises an image of the subject at a corresponding point in time. In an embodiment, the pairs are identified by comparing movement, velocity, or other changes in the subject exhibited in each plurality of images. In an embodiment, where the movement of the subject is sufficiently similar a pair is identified. In an embodiment, other inputs or measurements are compared to identify such pairs. For example, airflow sensors can be used to identify the beginning and ending of expiration in a respiration cycle. In another embodiment, electrocardiogram (ECG) signals are compared to identify pairs. In another embodiment, a capture time of an image is recorded and used to identify one or more pairs. In an embodiment, the first plurality does not contain an image that precisely corresponds to an image in the second plurality. In an embodiment, the closest image in the first plurality is paired with the image in the second plurality. In an embodiment, the second plurality does not contain an image that precisely corresponds to an image in the first plurality. In an embodiment, the closest image in the second plurality is paired with the image in the first plurality. In an embodiment, images are paired if they are determined to be within a certain range of time of each other. The certain range of time could be a constant, such as 0.1 seconds. Or it could be vary based on the temporal resolution of the studied image sequence, or other factors. In an embodiment, a pair is not identified for every image received.","At a step , a position of at least one landmark or feature in the second image is refined based on a comparison with a corresponding position in the first image. In an embodiment, texture or other image derived information at or near the position in the second image is compared to such information at or near the corresponding position in the first image. In an embodiment, the refined feature is a point. In an embodiment, the feature is part of a larger feature such as a shape. In an embodiment, the feature is a point on a boundary. In an embodiment, positions of two or more features are refined. In an embodiment, positions of features of two or more images of the second plurality are refined. Various image derived information can be for such a comparison. In an embodiment, local phase features are used. Local phase features can be computed from log-Gabor filter banks or wavelets. For example, Log-Gabor filters and wavelets can be used to calculate local amplitude and phase at a particular frequency. Typically, a bank of filters is applied to the image where each filter is created by different scalings and orientations (phase) of the reference filter. At each point in the image, the responses are recorded for each filter from the bank. Each response is greatest when the filter scale\/orientation matches the image point's scale\/orientation. In this way, peak responses from filters give feature information such as where there are edges in a particular orientation for example. In an embodiment, these features can give an indication of the location of a boundary, such as a LV boundary, in a heart with the direction of the edge. In an embodiment, such edge detection can be used on echocardiogram images, as further discussed in M. Mulet-Parada and J. A. Noble, \u201c2D+T Acoustic Boundary Detection in Echocardiography,\u201d Medical Image Analysis, vol. 4, pp. 21-30, 2000. In an embodiment, texture information is used, such as variations in intensity, brightness, hue, saturation, luminosity, contrast, color, and\/or other properties. In an embodiment, speckle distribution is used. Different texture measurements may work better for different types of images. For example, in an embodiment, the images are ultrasound images and Rayleigh distributed speckle is used. In another embodiment, the images are MRI images and Gaussian distributed noise is used.","Various methods of refining a position of a feature or landmark in a second image based a corresponding position in the first image can be used. The steps - discussed below illustrate one method of accomplishing this objective. Other methods are possible.","At a step , a template and a search window are defined, wherein the template is a portion of the first image at or near the corresponding position and the search window is a portion of the second image encompassing a plurality of possible displacements of the template. In an embodiment, the template encompasses a corresponding point in the first image corresponding to the corresponding position. In an embodiment, the template is centered on such a point. In an embodiment, the search window encompasses a point in the second image corresponding to the position. In an embodiment, the search window is centered on such a point.","In an embodiment, the template and the search window are defined by a set of pixels. In an embodiment, the template set includes a pixel corresponding to the corresponding position and one or more additional template pixels. In an embodiment, the search window set includes a pixel corresponding to the position and one or more additional search pixels. In an embodiment, the one or more additional template pixels consist of an area (or volume for three-dimensional images) contiguous to the pixel corresponding to the corresponding position. In an embodiment, the one or more additional search pixels consist of an area (or volume for three-dimensional images) contiguous to the pixel corresponding to the position. In an embodiment, the search window and the template are centered on the pixel corresponding to the position and the pixel corresponding to the corresponding position, respectively. In an embodiment, the search window and the template are not centered on the position and the corresponding position in this manner. For example, in an embodiment, the position and the corresponding position form the upper-left corner of the search window and the template, respectively. In an embodiment, the search window is centered but the template is not (or vice versa) in order to bias the result in a particular direction.","Various template and window sizes and shapes can be used with the subject invention. In an embodiment, the template is square or cubic \u201cblock\u201d of pixels. In an embodiment, the template is a regular triangle, hexagon, octagon, or other regular polygon. In an embodiment, the template is an irregular shape, such as a rectangle or rectangular solid. The search window is larger than and encompasses at least two displacements of the template. For example, a 5\u00d75 block of equally sized pixels, the search window, encompasses nine possible displacements of 3\u00d73 block of equally sized pixels, the template. A 4\u00d75 block of equally sized pixels would encompass six possible displacements.","At a step , the template is compared to each of the possible displacements of the template in the search window. In an embodiment, a superior match is identified between one of the possible displacements and the template. In an embodiment, texture information in the template is compared with texture information in each of the possible displacements. In an embodiment, identifying the superior match comprises computing a similarity measure for each of the possible displacements, wherein the similarity measure represents a degree of textual similarity between the template set and the displacement. In an embodiment, the similarity measure is based on Rayleigh distributed speckle in the template set and the displacement. In an embodiment, identifying the superior match further comprises computing a confidence measure for each of the possible displacements, wherein the confidence measure measures the smoothness and\/or motion coherence of motion leading to the displacement. In an embodiment, the similarity measures and confidence measures are combined to identify the superior match.","At a step , the position of the feature in the second image is updated based on the identified displacement. In an embodiment, the position is updated to a position of a pixel in the identified displacement. In an embodiment, the position is updated to a position in the center of the identified displacement.","In an embodiment, the refined feature is stored, presented, or output for later use as discussed above in relation to the step . In an embodiment, the refined feature is used to derive one or more features and\/or perform one or more calculations as described below in relation to the step .",{"@attributes":{"id":"p-0148","num":"0147"},"figref":"FIG. 7","b":["201","305","307","309","311"]},{"@attributes":{"id":"p-0149","num":"0148"},"figref":["FIG. 8","FIG. 7","FIG. 7","FIG. 7"],"b":["421","423"]},"In an embodiment of the subject invention, a multivariate regression model is used to relate a least one position A corresponding to at least one feature in a sequence A of image frames to a least one position B corresponding to the at least one feature in a sequence B of image frames. The resulting model can be used to predict the at least one position B of the at least one feature in a sequence B for a new dataset, given the at least one position A of the at least one feature in a sequence A for the new data set.","In a further embodiment, information derived from an image A from the sequence A of the new data set is used to refine the prediction of one or more of the at least one position B of the at least one feature in an image B from the sequence B of the new data set. In an embodiment, the image A and the image B depict corresponding times within their respective sequences. For example, if frame  in sequence A (A) corresponds to frame  in sequence B (B), then image derived information from points near the at least one feature in A are compared to the image derived information from a search region around corresponding points near the at least one feature in B. The one or more of the at least one position B of the at least one feature in the image B is then refined using this new information. In an embodiment, the feature is a boundary.","In a particular embodiment, the subject invention is applied to stress echocardiography images. In stress echocardiography, a heart (or typically the left ventricle) is imaged using ultrasound at rest and then after administration of a stress stimulus. Clinical analysis is typically 2D and focuses on visualizing these two sequences of images (rest and stressed) side by side. A sick heart typically moves differently under stress than a healthy one and the clinician will grade this motion by eye and from this decide whether the heart is abnormal or not. This interpretation is subjective. Automating this task is difficult. Methods developed for analyzing rest images do not work well on stress images. Embodiments of the subject invention are directed to automating processing of the stressed images using results from processing the higher quality rest images.","Thus, in an embodiment, the invention involves a prediction of a stress sequence segmentation given a rest sequence segmentation of the left ventricle. In a further embodiment, the prediction is then updated by either using more traditional segmentation algorithms such as level sets or snakes or using image derived information from the rest sequence to inform the segmentation of the stress sequence as described above.","In an embodiment of the subject invention, a multivariate regression model, such as a Canonical Correlation Analysis (CCA) model, is developed using training datasets where each dataset consists of two segmented image sequences; sequence A (rest) and sequence B (stress). The model links the segmentations of the rest sequence to those of the stress sequence by modeling the motion of landmarks in the images and returns the model parameters.","In an embodiment, the model links the rest and stress sequences using the principle of conservation of energy at each corresponding timepoint, t, in the cardiac cycle. In an embodiment, the internal strain energy (in the heart tissue) at stress, S, is assumed to be the same as the internal strain energy at rest, S, plus some additional work energy, W, required to meet the extra demand on the heart when stressed:\n\n.\u2003\u2003(3)\n","Assuming a linear elastic model, the internal strain energy is proportional to the displacement and Equation 3 can be re-written as:",{"@attributes":{"id":"p-0157","num":"0156"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":"c","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["x","stressed"],"mrow":{"mi":["n","t"],"mo":","}}},{"msub":{"mi":"d","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["y","stressed"],"mrow":{"mi":["n","t"],"mo":","}}}],"mo":"+"}}},{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":"a","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["x","rest"],"mrow":{"mi":["n","t"],"mo":","}}},{"msub":{"mi":"b","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["y","rest"],"mrow":{"mi":["n","t"],"mo":","}}}],"mo":"+"}}},{"mrow":{"msub":{"mi":["W","t"]},"mo":"\u00b7","mi":"\u0394"},"mo":["\u2062","\u2062","\u2062"],"mi":["%","MHR"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}],"mo":"+"}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{},"sub":["rest ","stressed "]},{"@attributes":{"id":"p-0158","num":"0157"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":"c","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["x","stressed"],"mrow":{"mi":["n","t"],"mo":","}}},{"msub":{"mi":"d","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["y","stressed"],"mrow":{"mi":["n","t"],"mo":","}}}],"mo":"+"}}},{"mrow":{"msub":{"mi":"W","mrow":{"mi":["stressed","t"],"mo":","}},"mo":"\u00b7","mi":"%"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["MHR","stressed"]}}],"mo":"+"},{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":"a","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["x","rest"],"mrow":{"mi":["n","t"],"mo":","}}},{"msub":{"mi":"b","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["y","rest"],"mrow":{"mi":["n","t"],"mo":","}}}],"mo":"+"}}},{"mrow":[{"msub":{"mi":"W","mrow":{"mi":["rest","t"],"mo":","}},"mo":"\u00b7","mi":"%"},{"msub":{"mi":["MHR","rest"]},"mo":"."}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}],"mo":"+"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}},"br":{},"sub":["rest","stressed"]},"In an embodiment, having derived the model parameters, the model is then applied in a patient-specific way to an inquire dataset. In an embodiment, the model input includes the rest landmark positions derived from the rest sequence of the inquire dataset and the output is a prediction of the stress landmark positions for the stress sequence of the inquire dataset. Hence, the global model is applied in a patient-specific manner by using rest sequence information derived from the inquire dataset to predict stress sequence information for the patient.","In a further embodiment, the predicted stress sequence information is refined using a block-matching scheme as further described below. In an embodiment, the block-matching scheme is used to refine a feature in an image of a stress sequence based on information derived from an image of a rest sequence. In an embodiment, the block-matching scheme can be applied to refine the feature in the stress sequence without first developing or applying the model described above.","Cohen and Dinstein described a similarity measure, suitable for carrying out block-matching between two consecutive frames in a sequence of images, based on a simplified model of image formation whereby they assumed the ultrasound (US) images were contaminated by Rayleigh distributed multiplicative noise. B. Cohen and I. Dinstein, \u201cNew maximum likelihood motion estimation schemes for noisy ultrasound images,\u201d 35, pp. 455-463, 2002. According to this method, a block (or other set) of pixels in one image is matched to another block (or set) in a second image by means of a maximum likelihood (ML) estimation scheme within a pre-defined search area to arrive at motion estimates between the two images. Boukerroui et al. improved on this idea by using normalised image blocks, a probability weighted motion estimate from the search area using Singh's algorithm, and finally optimising all the parameters for a given sequence of images. D. Boukerroui, J. A. Noble, and M. Brady, \u201cVelocity Estimation in Ultrasound Images: A Block Matching Approach,\u201d , pp. 586-598, 2003.","In an embodiment of the subject invention, a similar approach is used to compare a block (or set) of pixels from a rest frame to one or more blocks (or sets) of pixels in a stress frame. In an embodiment, the results of the comparison are then used to refine one or more points comprising a feature (such as a boundary) in the stress frame. In an embodiment, the invention thereby aides segmentation of the stress frame. In an embodiment, the block-matching scheme is applied to paired frames from the rest and stress sequences of a dataset that are at corresponding phases in the cardiac cycle. In an embodiment, the block-matching scheme is used to refine a point along a boundary in a stress frame based on information derived from a corresponding point in the paired rest frame. In a particular embodiment, for any point along the boundary in the rest frame, a template block of size 3\u00d73 pixels is used to search a block of size 5\u00d75 pixels around its corresponding point in the stress frame. In an embodiment, the similarity measure and optimisation are computed as described by Boukerroui et al. but in a multi-resolution approach.","In an embodiment, a Principal Components Analysis (PCA), as discussed in K. Pearson. On lines and planes of closest fit to systems of points in space. 2:559-572, 1901, J. O. Ramsay and B. W. Silverman. . Springer-Verlag, 2002, is used to relate features of the LV wall of a heart. In an embodiment, the features are represented by seven landmarks in images of the heart as shown in . In an embodiment, the landmarks are identified and\/or verified by an expert cardiologist. In an embodiment, a cubic spline curve is used to connect the landmarks. The results from carrying out PCA are shown in  for the first principal component (this accounts for the most variability in the data whilst succeeding components account for reducing variability). The green contour represents the mean cardiac shape in the cardiac cycle from ED to ES, and the red and blue contours are plus and minus one standard deviation of the first principal component respectively. The analysis shows that there is greater shape variation in the LV at stress as the hand between the red and blue contours is wider for the stress case than for the rest. Secondly, the form that the red and blue contours take in stress is quite different to the mean shape whereas for the rest, there are three similarly shaped contours. An embodiment of the subject invention makes no assumptions about the level of motion between frames and allows for this greater variation in shape.","In an embodiment of the subject invention, the objective is to track an epicardial and\/or endocardial boundary or surface of the LV from standard B-mode US images of the cardiac cycle when the heart is stressed. In an embodiment, the invention operates on one or more datasets, each including a plurality of images of a heart at rest and a plurality of images of the heart in a stress condition. In an embodiment, the invention operates on other types of images as described above. As the specific motion of the stressed heart can potentially vary significantly from patient to patient, in an embodiment, a patient-specific model is developed whereby the segmentations for the stress plurality of images are inferred from the segmentations for the rest plurality of images. The processes involved in developing a model and then applying it to segment\/track stress data in one embodiment are shown in the flow chart of . In the embodiment shown, first, the endocardial borders are delineated using a semi-automated tracking software called Quamus\u00ae. As discussed above, other methods can be used to identify and\/or interpolate features in a plurality of images. Next, in the embodiment shown in , the rest and stress datasets are aligned temporarily so that the deformations of the corresponding points in the cardiac cycle can be modelled. Finally, in the embodiment shown in , the rest and stress deformations are linked using a multivariate regression model, such as Canonical Correlation Analysis (CCA), to arrive at priors for use in segmentation of stressed data.","In an embodiment, the Endocardial Border Detection Stage, of the data analysis includes segmenting each image of each dataset. In an embodiment, a software program called Quamus (developed by Mirada Solutions Ltd, Oxford, UK) is used to do this. In an embodiment, for each plurality of images, an ED frame is identified and seven landmarks are marked out in the frame starting from the mitral valve (MV) hinge-point on the left going round to the apex at the top and finishing of at the MV hinge-point on the right (). In other embodiments, other landmarks can be used. In an embodiment, points in between the MV hinge-points and the apex are chosen to represent, as closely as possible, the segment divisions as described by Cerqueira et al. in M. D. Cerqueira, N. J. Weissman, V. Dilsizian, A. K. Jacobs, S. Kaul, W. K. Laskey, D. J. Pennell, J. A. Rumberger, T. Ryan, and M. S. Verani. Standardized myocardial segmentation and nomenclature for tomographic imaging of the heart. 105(4):539-542, 2002 for the two chamber view (2-CV) (see ). Following this step in an embodiment, a quadratic spline is fit through the points. In an embodiment, this first-frame segmentation is then filtered through the remaining frames using an optical flow based registration\/motion extraction algorithm. However, due to speckle, missing boundaries caused by shadowing, and noise in ultrasound images, this generally requires correction in later frames as segmentations become progressively worse as the frame number increases. This is typically the case for stress data where there is greater motion between frames than there is for rest data. In an embodiment, the landmarks and segmentations are separately developed for each frame in each plurality of images. In an embodiment, the segmentations (see ) are validated by one or more experienced cardiologists before being used to build a model for linking the rest and stress images.","In an embodiment, an ECG signal is used to identify particular phases within the cardiac cycle of the heart. As the heart rate is typically significantly higher in stress, the time period of one cardiac cycle is considerably shorter, fewer frames are acquired in stress, and the stress frames are not aligned with the ones in rest. In an embodiment, in order to compare and relate the segmentations of rest and stress, corresponding frames are found according to known phases in the cardiac cycle. In an embodiment, the ED and ES frames are identified in each of the rest and stress pluralities of images and then use a linear interpolation scheme is used to identify corresponding frames in between the ED and ES frames of each plurality. In another embodiment, as further discussed below, three or more phase images are identified in each plurality and two or more piecewise linear functions are used to temporally align each frame sequences between these phase image. In an embodiment, the rest frames that do not correspond to a frame in stress are discarded. In another embodiment, as further discussed below, all frames can be aligned to a common temporal axis and studied.",{"@attributes":{"id":"p-0167","num":"0166"},"figref":["FIG. 15","FIG. 15(","FIG. 15("],"i":["a","b"]},"In an embodiment, the aligned temporal axis provides a basic structure by which the deformations in the rest and stress images can be compared.  shows a plot of rest and stress positions for the left MV hinge-point, relative to the position in the \u2018Q\u2019 phase, against the aligned axis. The landmark positions at each frame (marked as crosses) show that the motion for rest and stress is quite similar for the initial part of the cardiac cycle but then there is greater variation between the two after the \u2018S\u2019 phase (equivalent to 3 on the aligned axis). This is when the blood is being pumped out of the LV and the motion in stress is different and greater in order to meet the increased demand for oxygenated blood from the body. For interpolation at the required timepoints of the plots in , various interpolation schemes can be used.  illustrates the results of a polynomial curve, piecewise cubic splines, and a natural cubic spline, for the rest and stress deformations.","In an embodiment, rest and stress images are temporally aligned to a common axis and mutual information based registration is used to spatially align the ED frame in each plurality to a template as described in V. Walimbe, M. Garcia, O. Lalude, J. Thomas, and R. Shekhar. Quantitative real-time three-dimensional stress echocardiography: A preliminary investigation of feasibility and efectiveness. 20(1):13-22, 2007. In an embodiment, the reverse transformation is then applied to the template wire mesh segmentation to arrive at an ED initial segmentation for each plurality. In an embodiment, this segmentation is refined iteratively towards the endocardial boundary by means of mesh smoothness constraints and gradient-based image forces. In an embodiment, this process is then repeated throughout the cardiac cycle for each of the rest and stress pluralities. In this formulation, there is no direct link between the rest and stress segmentations for each corresponding frame in the cardiac cycle other than the temporal alignment.","In another embodiment, a direct relationship between the rest and stress segmentations is developed and then can be used to predict stress segmentations (which can be used as priors for another segmentation technique) given the rest segmentations for another heart. Using one or more of the interpolation methods described above, landmark positions for rest and stress can be found for each of a plurality of timepoints within the cardiac cycle of the heart. In a particular embodiment, 2D rest and stress landmarks are identified in 2-CV images from the ED to ES part of the cardiac cycle. In an embodiment, as further described below, a multivariate regression model, such as CCA, is used to build a model using training datasets. This model can then be applied to predict stress segmentations, given the rest segmentations for the cardiac cycle from ED to ES, for another stress echocardiography dataset not used in developing the model.","Multiple regression analysis is a technique that allows one to predict the value of a (dependent) output variable given a set of (independent) input variables because the output is described as a linear function of the input variables. CCA however, is an example of a multivariate regression model that allows the formulation of a linear relationship between multiple input and multiple output variables. Unlike multiple regression where one variable is predicted using the model, a multivariate model predicts multiple output variables from multiple input variables.","In an embodiment of the subject invention, CCA or another multivariate regression model is applied to the problem of LV segmentation. In an embodiment, the relationship between rest and stress segmentations is modeled using the principle of conservation of energy at each of a plurality of corresponding timepoints, t, in the cardiac cycle. In an embodiment, the internal strain energy (in the heart tissue) at stress. S, is internal assumed to be the same as the internal strain energy at rest, S, plus some additional work energy, W, required to meet the extra demand on the heart when stressed:\n\n.\u2003\u2003(1.22)\n","The internal strain energy of a body is defined as the internal work done in deforming it by the action of externally applied forces. In the simplest sense, the change in internal energy \u0394S is given by:\n\n\u0394\u2003\u2003(1.23)\n\nwhere F is the external force and \u0394u is the displacement resulting from deformation. However, this assumes a constant force is acting on the endocardial surface which is not typically the case throughout the cardiac cycle. In an embodiment, a linear elastic model is assumed for the myocardium which results in a linearly increasing force\/displacement relationship. Consequently, the internal energy, S, is given by:\n",{"@attributes":{"id":"p-0174","num":"0173"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":["\u0394","S"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mfrac":{"mn":["1","2"]},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mi":["\u0394","F","\u0394"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"u","mo":"."}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"1.24"}}]}}}}},"In either case, the internal strain energy is assumed to be proportional to the deformation displacement resulting from the external force.","In an embodiment, rest and stress displacements for seven landmarks on the endocardial boundary are identified for corresponding timepoints in the cardiac cycle using one or more of the methods discussed above. Equation 1.22 can be rewritten for a timepoint t in terms of these displacements as:",{"@attributes":{"id":"p-0177","num":"0176"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":"c","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["\u03c7","stressed"],"mrow":{"mi":["n","t"],"mo":","}}},{"msub":{"mi":"d","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["y","stressed"],"mrow":{"mi":["n","t"],"mo":","}}}],"mo":"+"}}},{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":"a","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["\u03c7","rest"],"mrow":{"mi":["n","t"],"mo":","}}},{"msub":{"mi":"b","mrow":{"mi":["n","t"],"mo":","}},"mo":["\u2062","\u2062","\u2062"],"mi":"\u0394","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["y","rest"],"mrow":{"mi":["n","t"],"mo":","}}}],"mo":"+"}}},"mo":"+","msub":{"mi":["W","t"]}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"1.25"}}]}}}},"br":{}},{"@attributes":{"id":"p-0178","num":"0177"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":{"mi":["%","MHR"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"mo":"=","mfrac":{"mi":["HR","MHR"]}},{"mrow":[{"mi":["\u0394","%","MHR"],"mo":["\u2062","\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mrow":[{"mi":"%","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["MHR","stress"]}},{"mi":"%","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"msub":{"mi":["MHR","rest"]},"mo":"."}}],"mo":"-"}],"mo":"="}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"1.26"}}]}}}},"br":{}},"In an embodiment, the multivariate regression model is implemented using vector representations of the long axis (y) and short axis (x) positions of the landmarks relative to the position in the \u2018Q\u2019 phase. By considering relative positions, differences in US probe positioning that affect the development of the model, are minimised. In an embodiment, for any timepoint in the cardiac cycle, the rest (input) data matrix, X, is defined as a matrix of dimension m\u00d7n containing m dataset representations of the positions and the level of stress measure: [xx. . . xyy. . . y\u0394% MHR]. Similarly, the stress (output) data matrix, Y, is defined as matrix of dimension m\u00d7(n\u22121) containing m representations of the positions only: [xx. . . xyy. . . y]. In an embodiment, once the model is developed for linking the rest and stress datasets, it is then possible to use equation 1.21 to predict the stress landmark positions given the rest landmark positions for another stress echocardiography dataset which has not been included in the model.","In an embodiment, as shown in ), two or more phases in the cardiac cycle are assigned integer time values when carrying out the temporal alignment. In an embodiment, the relationship between rest and stress is studied at every 0.5 time units on the temporally aligned scale i.e. at each of the \u2018Q\u2019, \u2018R\u2019, \u2018S\u2019, and \u2018T\u2019 phases and also midway in between them. A more sparse or dense temporal model (e.g. at intervals of every 1 or 0.25 time units) can also be used. In an embodiment, landmark positions at each of the required timepoints are interpolated from the rest and stress images, using a natural cubic spline interpolation scheme or other interpolation method as discussed above. In an embodiment, the interval is chosen to ensure that when the stress landmark positions are predicted, there are enough points to plot these against time and infer the positions corresponding to each of the frames (which are not at studied timepoints) in the stress plurality of images. Pseudo-code for one embodiment of this method, using CCA, is given below:",{"@attributes":{"id":"p-0181","num":"0180"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"for timepoint = 0.5 to 5 at 0.5 intervals do"},{"entry":"\u2003for all Corresponding rest\/stress datasets do"},{"entry":"\u2003\u2003Interpolate landmark positions"},{"entry":"\u2003\u2003Get \u0394%MHR values"},{"entry":"\u2003end for"},{"entry":"\u2003Run CCA on the input and output data matrices"},{"entry":"\u2003Interpolate the rest landmark positions for the rest\/stress dataset to "},{"entry":"\u2003be predicted"},{"entry":"\u2003Compute the predicted stress landmark positions from the CCA model"},{"entry":"end for"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"The output from this embodiment is stress landmark positions at every 0.5 time units on the temporally aligned scale. In an embodiment, these timepoints do not correspond exactly to the frames in the stress plurality of images. In an embodiment, therefore, the landmark positions at the frame timepoints are interpolated using a natural cubic spline or other interpolation technique. In embodiment, the positions at the studied time values are plotted against the temporally aligned scale, and then by using the aligned scale values assigned to all the stress frames, the landmark positions at these frames are interpolated (see ). In an embodiment, the predicted landmark positions are connected using periodic cubic spline curves.","As a comparative experiment, rest-derived stress landmark positions are compared to rest landmark positions in . The rest landmark positions at each of the studied time values in the cardiac cycle can be interpolated. If the stress datasets were simply a faster version of the rest datasets, the rest-derived landmark positions should follow the true positions very well. However, if the motion at stress consists of (additional) more complex dynamics than at rest, then the rest-derived landmark positions will not be able to follow the true positions as precisely because they are not derived from modelling the changes between rest and stress. This can already be seen in  where the aligned rest and stress curves look very different after the \u2018S\u2019 phase (corresponding to time value  on the aligned axis). Embodiments of the subject invention are therefore likely to give comparatively better results because they model the additional dynamics present in stress.","A Gradient Vector Flow (GVF) snakes segmentation algorithm, as proposed in C. Xu, D. L. Pham, and J. L. Prince. Chapter 3: image segmentation using deformable models. , Volume III: Medical Image Analysis:129-174, 2000, was applied to show how the final segmentations compare to when the segmentation for frame n\u22121 is used as a prior for frame n and also when the rest-derived priors are used instead. GVF snakes segmentation is an edge-based formulation where an edge map is derived from the image to be segmented. The GVF forces are computed by applying generalised diffusion equations to both components of the gradient of the edge map image. This diffusion means that the even if priors are far from the object boundary, they are still drawn toward the edges defined by the edge map. The governing equations with regards to internal (smoothness) and external GVF (image-driven) forces are as shown by Xu et al. in C. Xu and J. L. Prince. Generalized_gradient vector flow external forces for active contours. 71:131-139, 1998 and the energy functional to be minimised is:",{"@attributes":{"id":"p-0185","num":"0184"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"E","mo":"=","mrow":{"msubsup":{"mo":"\u222b","mn":["0","1"]},"mo":"\u2062","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"mo":"(","mrow":{"mrow":{"mrow":[{"mi":"\u03b1","mo":"\u2062","msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":["x","\u2032"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"s"}}},"mn":"2"}},{"mi":"\u03b2","mo":"\u2062","msup":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"msup":{"mi":["x","\u2033"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"s"}}},"mn":"2"}},{"mrow":[{"msub":{"mi":["E","ext"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"s"}}}},{"mo":"\u2146","mi":"s"}],"mo":"\u2062"}],"mo":["+","+"]},"mo":","}}}}}},{"mrow":{"mo":["(",")"],"mn":"1.27"}}]}}}},"br":{},"sub":"ext"},"Embodiments of the subject invention were tested on a database consisting of 78 normal (no apparent abnormalities in the heart motion) dobutamine stress echocardiography (DSE) studies for which ethics approval had been obtained. The data was acquired by two cardiologists as standard B-mode US rest-stress image sequences from ED to ES in 2D for the 2-CV using Sonos7500 or iE33 echocardiography systems. Typical special resolutions are 3.6 mm\u00d73.6 mm and temporal resolutions of 73 ms. The age distribution for the studies is shown in  which shows that the majority of subjects are quite elderly. Cardiologists generally attempt to acquire data for rest and then at peak stress (with a target HR (THR) of 85% of the MHR). This is not always possible to achieve with the sick and elderly for whom it may be difficult or uncomfortable at such a stress level therefore not all datasets are at rest and peak stress. This is one reason for including the \u0394% MHR term in the model as it allows the model to account for this variability factor. Although the test was limited to 2D datasets, embodiments of the subject invention can be applied to 3D datasets as well.","The results presented here were obtained by developing a CCA model using a leave-one-out approach for training the model then predicting segmentations for the left-out dataset using the model. Other embodiments of the invention are possible as described above.",{"@attributes":{"id":"p-0188","num":"0187"},"figref":"FIG. 21","b":["1","5"]},"It is possible to assess how strong the developed input\/output (or rest\/stress) relationship is by plotting the Right-Hand Side (RHS) against the Left-Hand Side (LHS) of Equation 1.25 for each of the datasets. This can be done for all the eigenvalue\/eigenvector components of the CCA model, but the largest component (i.e. the model coefficients corresponding to the largest eigenvalue) accounts for the most variation in the data. If the coefficients were to exactly define the relationship with no errors, the RHS of the equation would equal the LHS of the equation resulting in all rest\/stress datapoints lying along the y=x line. The correlation coefficient, r, indicates the strength (and direction) of the relationship between two variables. r=\u00b11 represents a perfect positive or negative linear relationship and r=0 represents no linear relationship (but does not rule out the possibility of a non-linear relationship or even clusters of data).","The correlation coefficients for all the components of the CCA model are given by the elements of the diagonal matrix, D), as in Equation 1.21.  shows a plot of the RHS against the LHS of Equation 1.25 for the largest component of the CCA model at each phase of the cardiac cycle. The correlation coefficients are also shown. A significance test was also carried out to decide whether or not to reject the null hypothesis, H: no linear relationship, i.e. r=0. The t-test is commonly performed in assessing the significance of correlation coefficients and the t-statistic is defined as:",{"@attributes":{"id":"p-0191","num":"0190"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"t","mo":"=","mrow":{"mi":"r","mo":"\u2062","msqrt":{"mfrac":{"mrow":[{"mi":"n","mo":"-","mn":"2"},{"mn":"1","mo":"-","msup":{"mi":"r","mn":"2"}}]}}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"1.28"}}]}}}}},{"@attributes":{"id":"p-0192","num":"0191"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1.1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"The t-statistic values for each of the cardiac phases"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Phase","R","S","T","ES"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"49pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["R","0.74973","0.77930","0.77325","0.80149"]},{"entry":["t- statistic","9.8117","10.770","10.561","11.607"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0193","num":"0192"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1.2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"A table showing the percentage points for the "},{"entry":"t-distribution for 28 degrees of freedom"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"196pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"%"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"10"},"colspec":[{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"8","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"9","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"10","colwidth":"21pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"20","15","10","5","2.5","1","0.5","0.1","0.05"]},{"entry":{"@attributes":{"namest":"1","nameend":"10","align":"center","rowsep":"1"}}},{"entry":["% ","0.8546","1.056","1.313","1.701","2.048","2.467","2.763","3.408","3.674"]},{"entry":"Points"},{"entry":{"@attributes":{"namest":"1","nameend":"10","align":"center","rowsep":"1"}}}]}}]}},"br":{},"sub":"0"},"In conclusion, a strong correlation between the rest and stress energy conservation formulation was observed which is why the correlation coefficients were high. The t-statistics show that the correlation coefficients are sufficiently high to disprove the null hypothesis, H. Thus, a strong linear relationship exists between the rest and stress energies as described by Equation 1.25.",{"@attributes":{"id":"p-0195","num":"0194"},"figref":["FIG. 23","FIG. 21"]},"These results show that there is a strong qualitative link between the work energy term and what happens in the cardiac cycle at stress compared to at rest. The extra work energy term is another reason why the prior segmentation results from using the CCA model are better than the rest-derived ones, because it accounts for the energy differences between rest and stress which are not included in the deformation-based energy terms in Equation 1.25.","The CCA priors shown in  match the true segmentation very well, particularly for the first 5 frames of the stress dataset. However, the CCA priors were not as precise for all datasets which is why our method is more useful for computing segmentation priors and not the final LV segmentations.  shows the priors for another dataset where although the CCA priors are still better than the rest-derived priors, they do not match the true segmentations as well as shown previously. The GVF-based snakes segmentation was carried out at each frame of this dataset using CCA priors, rest-derived priors, and the true segmentation from the previous frame as priors. The results for this are shown in . The settings for the GVF-based snakes were {\u03b1=3,\u03b2=0,\u03ba=0.6} which define the snake's tension, rigidity and the weighting, respectively, for the external forces.","The results in  show that the CCA-derived segmentation priors provide a very good starting point for snakes segmentation and the final segmentations are therefore much better than using the rest-derived priors. When compared to using the previous frame's manual segmentation as a prior (see ), the segmentations resulting from CCA-derived priors are poorer in the last two frames because the priors are also not as good in these frames. The previous frame priors are the manual segmentations of the previous frame i.e. the best case scenario is used whereby we assume that for any frame the true segmentation is found before using this as a prior for the next frame. This is often not the case if boundaries are unclear such as in US images so errors tend to accumulate as they are passed along in the frames. In reality therefore, segmentations are likely to get worse further from the starting frame. In terms of consistency, it appears that CCA-derived priors are better as they are independent of previous frames and are based on a motion model that can potentially provide very accurate priors.","The accuracy of the predicted segmentations were analyzed by looking at false positive (Type I) and false negative (Type II) error rates in terms of pixel assignment. False positive and false negative errors were identified by comparing the predicted segmentations to delineations of the LV verified by experts. If a positive instance is defined as a pixel being labeled part of the LV, then a false positive error is when a pixel is incorrectly labeled part of the LV when in fact it is not part of the LV. The converse is true for false negatives. The false positive rate, \u03b1, and the false negative rate, \u03b2, are defined as follows:",{"@attributes":{"id":"p-0200","num":"0199"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"\u03b1","mo":"=","mfrac":{"mrow":[{"mi":["Number","of","false","positives"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},{"mi":["Total","number","of","negative","instances"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}]}},{"mi":"\u03b2","mo":"=","mfrac":{"mrow":[{"mi":["Number","of","false","negatives"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},{"mi":["Total","number","of","positive"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"mi":"instances","mo":"'"}}]}}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"1.29"}}]}}}},"br":{},"sub":["1","2","1","2"]},"There is generally a trade-off between sensitivity and specificity depending on the application. In disease diagnosis for example, where a positive instance is defined as a patient having the disease\/condition, it is acceptable to have a lower specificity provided there is very high sensitivity i.e. it is better to have fewer false negatives than fewer false positives. When comparing the e^ values of different techniques for disease diagnosis, ewould be set to a smaller value than esuch that greater emphasis is placed on minimising false negatives than false positives. In the case of LV segmentation, it is equally important to minimise false positive and false negatives therefore in this study e=e=0.5.  shows a plot of the \u00ea values for all the datasets when the prior segmentations for each were predicted in a leave-one-out approach. These validation results show that the CCA model performs significantly better than the rest-derived model in minimising both Type I and Type II error rates. The results are comparable to the best case previous frame priors (i.e. where one assumes that the true segmentation of a frame is found before being passed as a prior segmentation for the next frame). This makes the CCA model a very useful tool in defining segmentation priors, particularly as it is faster to run and it gives the prior segmentations for all frames in one go, rather than being heavily dependent on the full segmentation of one frame being completed before considering the next.","It is evident from the use of the CCA-derived priors in snakes segmentation that the methodology provides a very good global prior, defined primarily by seven landmarks. It is because of this definition by landmarks that the algorithm takes seconds to apply to a new dataset once the model has been built. Model computation time is dependent on the number of datasets used in developing it and is typically one second per dataset which is also computationally inexpensive. As discussed above, the methodology can be easily extended to 3D stress echocardiography datasets whereby corresponding meshpoints or landmarks that define a 3D surface can be used to build the model.","In an embodiment of the subject invention, a rest image of a subject is linked to a stressed image of the subject. This relationship can be accomplished as described above. In a further embodiment, information derived from the rest image is then used to refine a feature in the stressed image. In a particular embodiment, segmentation of the stress image is refined. In an embodiment, image texture information near a segmentation of the rest image is used to refine the segmentation of the stressed image. In a particular embodiment, the rest image is identified in a plurality of ultrasound images of a heart in a rest condition and the stressed image is identified in a plurality of ultrasound images of the heart in a stressed condition.","In an embodiment, the output from a CCA model, or other methodology described above, provides the prior boundaries for the LV for the sequence of images from ED to ES. In an embodiment, the priors follow the true LV boundaries well but because they are defined solely by the seven landmarks, they cannot follow the more local variations in the boundary. Consequently, in an embodiment, an update step is applied to follow the CCA model predict step. In an embodiment, the update step incorporates the image-derived information from a linked rest image. In an embodiment, the update step is applied to boundaries or other feature priors derived by other means. In an embodiment, the update step is based on the fact that rest data can provide reference parameter estimation for the characteristics of the endocardial boundary for the stress data. In an embodiment, the update step can overcome some of the system dependent effects of the data acquisition when the rest and stress images are acquired in the same session. In an embodiment, texture or other rest image-derived information around the rest endocardial boundary is used as a basis for texture or other stressed image-derived information expected around the stress endocardial boundary. In an embodiment, the best match is found through a maximum likelihood estimation which identifies the most likely correspondence on the stress boundary given the texture or other rest image-derived information on the rest boundary.","The speckle distribution observed for one subject throughout a stress echocardiography session is likely to be similar for rest and stress, therefore, in an embodiment, properties such as grey-level distributions and textures within rest are used as prior information to aid segmentation in the stress case. In an embodiment, a technique called speckle tracking is used, which is typically used between consecutive frames, to link the textures around the endocardial boundary within corresponding rest and stress frames. Generally, texture-based features at corresponding points along the endocardial boundary in rest and stress should be similar because the surrounding tissue is the same, but in a different state.  shows that the appearance of the endocardial boundary in rest closely resembles the appearance in stress in terms of pixel intensities for the same subject. The myocardium is a dark band around the blood pool and regions around the marked boundary near structures such as the mitral valve hinge-points and papillary muscles look similar in rest and stress.","In US imaging, scatter is caused by small imperfections or scatterers within a tissue, which result in reflection and refraction of the sound wave. The result is a random, deterministic, interference pattern which decreases the image quality i.e. signal to noise level, by creating speckle in the tissue and blurring impedance boundaries as shown for the LV in . The scatterer density determines the statistics of the speckle, and in the case of fully developed speckle, it is assumed that the majority of the randomly located scatterers follow a Rayleigh distribution. Different statistical models may be considered in cases where the speckle is not assumed to be fully developed. Speckle tracking is one of the methods applied to B-mode US images when carrying out motion estimation. It works on the basis that the movement of speckle can be tracked through a sequence of images by implementing region-based matching.","Strintzis and Kokkinidis [M. G. Strintzis and I. Kokkinidis. Maximum likelihood motion estimation in ultrasound image sequences. 4(6):156-157, 1997.] implemented speckle tracking as a block-matching motion estimation method where they introduced a maximum likelihood (ML) estimation technique to optimise the block-matching. They used matching probability density functions based on Rayleigh-distributed statistics for motion estimation and used the resulting motion fields to carry out motion compensation. They showed that this produced a greater signal-to-noise ratio when the images were reconstructed than if the probability density function had been Gaussian distributed. Yeung et al. [F. Yeung, S. F. Levinson, and K. J. Parker. Multi-level and motion modelbased ultasonic speckle tracking algorithms. . & 24(3): 427-441, 1998.] used a similar block-based algorithm but through a multi-level approach whereby large matching blocks and search window sizes were used for initialisation, and were iteratively made smaller to converge at a finer resolution for the final motion field.","In an embodiment, the coarse-to-fine technique has the advantage of reducing computation time because of the larger blocks used initially, it benefits from the immunity to noise associated with large matching blocks, and at the same time keeps the higher resolution of the motion field associated with small matching blocks.","The aforementioned classical block-matching algorithms are based on the fact that a noiseless reference image is available. Cohen and Dinstein [B. Cohen and I. Dinstein. New maximum likelihood motion estimation schemes for noisy ultrasound images. 35:455-463, 2002.] have approached the speckle tracking in a similar way as Strintzis and Kokkinidis [M. G. Strintzis and I. Kokkinidis. Maximum likelihood motion estimation in ultrasound image sequences. 4(6):156-157, 1997.] but by assuming that each US image has an independent Rayleigh-distributed noise element, and taking into account that the displayed US images are log-compressed. They found that the resulting probability density function gave ML estimates of the motion that were significantly more accurate than using the probability density function described in [M. G. Strintzis and I. Kokkinidis. Maximum likelihood motion estimation in ultrasound image sequences. 4(6):156-157, 1997.].","To date, speckle tracking has typically been used in motion estimation between consecutive frames. It considers speckle as a feature that is useful, unlike algorithms which may try to smoothe\/filter it out. In embodiments, it is also fast to run computationally which makes speckle tracking based algorithms ideal for use in a clinical environment. In an embodiment of the subject invention, one or more speckle tracking methods described above are used between corresponding rest and stress frames from 2D stress echocardiography. In other embodiments, the concept is applied to 3D images. In an embodiment, a template block (or set) of pixels is centered at a point on the endocardial boundary in the rest frame, and the search window is centered at the corresponding point on the endocardial boundary in the corresponding stress frame (see ). In other embodiments, as discussed above, the template or search window need not be centered on the point and\/or the corresponding point. In an embodiment, the ML displacement estimate gives the location of the best rest-stress match on the stress boundary i.e. the update required to refine the prior endocardial boundary in the stress frame.","Various similarity measures can be used to measure the similarity of the pixel intensities in the template block and the overlapping part of the search window. In an embodiment, a similarity measure that accurately represents US image statistics is required. Standard block-matching for the purposes of motion estimation is often based on the Land Lnorms, where the matching criterion of sum of absolute differences (SAD) [C. Kontogeorgakis. M. G. Strintzis, N. Maglayeras, and I. Kokkinidis. Tumor detection in ultrasound b-mode images through motion estimation using a texture detection algorithm. 1994 , pages 117-120, 1994.; J. Crosby, B. H. Amundsen, T. Hergum, E. W. Remme, S. Langeland, and H. Torp. 3d speckle tracking for assessment of regional left ventricular function. . & 35(3):458-471, 2009.] and the sum of squared differences (SSD) [F. Yeung, S. F. Levinson, and K. J. Parker. Multi-level and motion modelbased ultasonic speckle tracking algorithm. . & 24(3): 427-441, 1998.] between the luminance values of two blocks are used respectively. Normalised Cross Correlation (NCC) is also based on the Lnorm where the normalisation is carried out by subtracting the mean and dividing by the standard deviation of each image. The normalised cross correlation for a template t(x,y) with a subimage i(x,y) is:",{"@attributes":{"id":"p-0212","num":"0211"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mn":"1","mrow":{"mi":"n","mo":"-","mn":"1"}},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":["x","y"],"mo":","}},"mo":"\u2062","mfrac":{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"mrow":{"mi":"t","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},"mo":"-","mover":{"mi":["t","_"]}}},{"mo":["(",")"],"mrow":{"mrow":{"mi":"i","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},"mo":"-","mover":{"mi":["i","_"]}}}],"mo":"\u2062"},{"msub":[{"mi":["\u03c3","t"]},{"mi":["\u03c3","i"]}],"mo":"\u2062"}]}}}},{"mrow":{"mo":["(",")"],"mn":"2.1"}}]}}}},"br":[{},{},{}],"o":"t","sub":["t ","i "],"in-line-formulae":[{},{}],"i":["I","x,y","i","x,y","\u012b","T","x,y","t","x,y",{"o":"t"}]},{"@attributes":{"id":"p-0213","num":"0212"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"mfrac":[{"mi":"I","mrow":{"mo":["\uf605","\uf606"],"mi":"I"}},{"mi":"T","mrow":{"mo":["\uf605","\uf606"],"mi":"T"}}],"mo":","}}},{"mrow":{"mo":["(",")"],"mn":"2.3"}}]}}}},"br":{},"img":[{"@attributes":{"id":"CUSTOM-CHARACTER-00001","he":"2.46mm","wi":"0.68mm","file":"US08396268-20130312-P00001.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00002","he":"2.46mm","wi":"0.68mm","file":"US08396268-20130312-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}],"sub":"2 ","i":["Biomech. Eng., ","Technical Report CSTR"]},"Although Land Lnorm based block-matching similarity measures have been successfully used to estimate motion in many applications, they are more suited for images that are characterised by Laplacian or Guassian statistics, and therefore may not be optimal for US images. Typically, speckle interference patterns are best represented by multiplicative Rayleigh distributed noise, and Strintzis and Kokkinidis [M. G. Strintzis and I. Kokkinidis. Maximum likelihood motion estimation in ultrasound image sequences. 4(6):156-157, 1997.] combined this with signal dependent additive noise to estimate motion fields in US images. They found that the motion-compensated reconstructed images using the Rayleigh distribution model gave better signal-to-noise ratios than using the Lnorm. Cohen and Dinstein [B. Cohen and I. Dinstein. New maximum likelihood motion estimation schemes for noisy ultrasound images. 35:455-463, 2002.] extended this idea for two US images (i.e. one is not a noiseless reference image as implemented by Strintzis and Kokkinidis) and presented two similarity measures; CDand CD. CDis based on a similar model as described by Strintzis and Kokkinidis but with the noise from both images. CDincludes nonlinear natural logarithm compression based on the ultrasound image formation model, and they found that this significantly out-performed both of the similarity measures used by Strintzis and Kokkinidis, and also the CDsimilarity measure. The CDmeasure has also been used by Boukerroui et al. [D. Boukerroui, J. A. Noble, and M. Brady. Velocity estimation in ultrasound images: A block matching approach. 2732: 586-598, 2003.] for the purpose of optimizing various parameters in the ML estimation, depending on the properties of the images for which the motion is to be estimated.","More recently, Myronenko et al. [A. Myronenko, X. Song, and D. J. Sahn. Maximum likelihood motion estimation in 3d echocardiography through non-rigid registration in spherical coordinates. 5528:427-436, 2009.] introduced a similarity measure called MS, which is a more generalised measure than the CD. This is because CDis derived by assuming that the Rayleigh-distributed noise in both images is independent whereas the MS measure is based on assuming there is some correlation between the noise in both images i.e. CDis a special case of MS. For this correlation assumption to hold, there must be a sufficiently high frame rate otherwise the noise in consecutive frames will be independent. They also incorporate a model for blurred speckle which results in a Nakagami distribution; a generalised form of the Rayleigh distribution, to arrive at another similarity measure, MS. In applying MS and MSto raw data in spherical coordinates, their results showed a slight improvement in the accuracy of motion estimation when compared to CD, with MSbeing better than MS.","In an embodiment directed to LV segmentation, the corresponding rest and stress images are paired instead of the current and next frames or the previous and current frames as in standard speckle tracking. In an embodiment, similarity measures based on Rayleigh distributed speckle being present in both images i.e. CD, CD, MS, and MS, are best suited for matching textures between the rest and stress images. In an embodiment, CDis likely to prove just as useful as the MS and MSmeasures because the rest and stress images are not linked in the same way as consecutive frames therefore the Rayleigh distributed noise can be assumed independent.","In an embodiment, block-matching based motion estimation is applied with an ML estimator. In an embodiment, corresponding blocks (or sets) of pixels are sought in each image within a predefined search area. In an embodiment, for each block, the estimated motion vector is one which maximises a likelihood function, which is derived from the similarity measure most suited to the image properties. In an embodiment, in the case of echocardiography, the images are assumed to be contaminated by multiplicative Rayleigh distributed noise.","In an embodiment, Xis defined as a set of coordinates in an image at a time instance t, I(x) as the intensity of a pixel at coordinates x, x\u03b5X, and I={I(x)}as the intensity image at time t. The motion of a pixel between t\u22121 and t is described by the vector v(x), x\u03b5X. If x\u03b5Xcorresponds to the pixel x\u03b5X, then:\n\n()=.\u2003\u2003(2.4)\n","Let v={v(x)}\u03b5represent the vector field describing the motion of pixels Xbetween t\u22121 and t. In an embodiment, the ML estimate of v, {circumflex over (v)}is obtained by the maximization of the conditional probability density function (derived from the maximum likelihood method for parameter estimation [A. Papoulis. Probability, . New York: McGraw-Hill, 1991.]):",{"@attributes":{"id":"p-0220","num":"0219"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":"max","msub":{"mi":["v","t"]}},"mo":"\u2062","mrow":{"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":"I","mrow":{"mi":"t","mo":"-","mn":"1"}},{"mi":["I","t"]}],"mo":"|"},"mo":",","msub":{"mi":["v","t"]}}}},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"at","msub":{"mi":["v","t"]}}},{"msub":{"mover":{"mi":"v","mo":"^"},"mi":"ML"},"mo":"."}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"2.5"}}]}}}}},"In an embodiment, in the case of non-rigid motion, vis not constant therefore local estimation is required. Now assume that Xis composed of non-overlapping sets B, i=1; 2 . . . N, of pixels with uniform motion:",{"@attributes":{"id":"p-0222","num":"0221"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["X","t"]},"mo":"=","mrow":{"munderover":{"mo":"\u22c3","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","msub":{"mi":["B","i"]}}},{"mrow":{"msub":[{"mi":["B","i"]},{"mi":["B","j"]}],"mo":"\u22c2"},"mo":"=","mi":"\u2205"},{"mi":["i","j"],"mo":"\u2260"}],"mo":[",",",",","]}},{"mrow":{"mo":["(",")"],"mn":"2.6"}}]},{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["v","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","t"]}}},{"mrow":[{"msub":[{"mi":["v","l"]},{"mi":["x","t"]}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["for","all"]},{"msub":{"mi":["B","i"]},"mo":"."}],"mo":"\u2208"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"2.7"}}]}]}}},"br":[{},{}],"sub":["i ","t-1 ","i","t-1","t-1","t","1","t","i"],"in-line-formulae":[{},{}],"i":["A","={x",":x","=x","\u2212v",",x","\u03b5B"]},"The sets Acan be overlapping and need not cover the entire Xif the motion is not constant over the whole image. In an embodiment, the pixel sets Aand Brepresent square \u201cblocks\u201d of pixels or more irregularly shaped \u201cobjects\u201d whose criterion for selection is the uniformity of their motion and practical considerations with regards to efficient implementation of the motion estimation algorithm. Let a=[a. . . a]represent the vector for all intensities I(x), x\u03b5A, and =[b. . . b]the corresponding vector of intensities for I(x), where x=x\u2212v. The maximisation in Equation 2.5 is equivalent to the maximisation for each i of the following probability density function:",{"@attributes":{"id":"p-0224","num":"0223"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"munder":{"mi":"max","msub":{"mi":["v","i"]}},"mo":"\u2062","mrow":{"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["a","i"]},{"mi":["b","i"]}],"mo":"|"},"mo":",","msub":{"mi":["v","i"]}}}},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"at","msub":{"mi":["v","i"]}}},{"msub":{"mover":{"mi":"v","mo":"^"},"mi":"ML"},"mo":"."}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"2.9"}}]}}}}},"Deriving the CDand CDmeasures: A commonly used model for ultrasound images where only one image is noisy is [C. Kotropoulos, X. Magnisalis, I. Pitas, and M. G. Strintzis. Nonlinear ultrasonic image processing based on signal-adaptive filters and self-organizing neural networks. 3(1):65-77, 1994.]:\n\n\u2003\u2003(2.10)\n\nwhere nis multiplicative noise with a Rayleigh density:\n",{"@attributes":{"id":"p-0226","num":"0225"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"msub":{"mi":["p","n"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"n"}},{"mfrac":{"mi":"n","msup":{"mi":"\u03c3","mn":"2"}},"mo":["\u2062","\u2062"],"mi":"exp","mrow":{"mo":["{","}"],"mrow":{"mo":"-","mfrac":{"msup":{"mi":"n","mn":"2"},"mrow":{"mn":"2","mo":"\u2062","msup":{"mi":"\u03c3","mn":"2"}}}}}}],"mo":"="},{"mi":"n","mo":">","mn":"0"}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2.11"}}]}}}},"br":{},"sub":"ij","sup":"2","i":"IEEE Signal Processing Letters, "},{"@attributes":{"id":"p-0227","num":"0226"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["a","i"]},{"mi":["b","i"]}],"mo":"|"},"mo":",","msub":{"mi":["v","i"]}}}},{"mrow":[{"mfrac":{"mn":"1","msub":{"mi":["b","ij"]}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u220f","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"k"},"mo":"\u2062","mrow":{"msub":{"mi":["p","n"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"msub":[{"mi":["a","ij"]},{"mi":["b","ij"]}]}}}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"k"},"mo":"\u2062","mrow":{"mfrac":{"msub":{"mi":["a","ij"]},"msubsup":{"mi":["b","ij"],"mn":"2"}},"mo":["\u2062","\u2062"],"mi":"exp","mrow":{"mo":["{","}"],"mrow":{"mo":"-","mfrac":{"msubsup":{"mi":["a","ij"],"mn":"2"},"mrow":{"mn":"2","mo":["\u2062","\u2062"],"msup":{"mi":"\u03c3","mn":"2"},"msubsup":{"mi":["b","ij"],"mn":"2"}}}}}}}],"mo":"="}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"2.12"}}]}}}},"br":{}},"In an embodiment, for the case of two noisy images, and again under the assumption of multiplicative Rayleigh noise with the probability density function given in Equation 2.11, if the noiseless value of the jth pixel in \u201cblock\u201d i at time t is s, then the model for the observed pixel at time t, b, is:\n\n.\u2003\u2003(2.13)\n","Similarly, the model for the observation of the transformed pixel at time t\u22121, a, is:\n\n,\u2003\u2003(2.14)\n\nwhere nj and nare two independent noise elements with the Rayleigh density function as given in Equation 2.11. Isolating sfrom Equations 2.13 and 2.14 and combining gives:\n",{"@attributes":{"id":"p-0230","num":"0229"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["a","ij"]},"mo":"=","mrow":{"msub":[{"mi":["\u03b7","ij"]},{"mi":["b","ij"]}],"mo":"\u2062"}},{"mrow":{"mi":"where","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["\u03b7","ij"]}},"mo":"=","mfrac":{"msubsup":[{"mi":["n","ij"],"mn":"2"},{"mi":["n","ij"],"mn":"1"}]}}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2.15"}}]}}}}},"As \u03b7is a division of two Rayleigh distributed random variables, it has the density function of the following form [A. Papoulis. Probability, . New York: McGraw-Hill, 1991.]:",{"@attributes":{"id":"p-0232","num":"0231"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"msub":{"mi":["p","\u03b7"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b7"}},{"mn":"2","mo":["\u2062","\u2062"],"msup":{"mi":"\u03c3","mn":"2"},"mfrac":{"mi":"\u03b7","msup":{"mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"\u03b7","mn":"2"},"mo":"+","mn":"1"}},"mn":"2"}}}],"mo":"="},{"mi":"\u03b7","mo":">","mn":"0"}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2.16"}}]}}}}},"The conditional probability density function for this density function, using the same properties as for deriving Equation 2.12, is:",{"@attributes":{"id":"p-0234","num":"0233"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["a","i"]},{"mi":["b","i"]}],"mo":"|"},"mo":",","msub":{"mi":["v","i"]}}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"k"},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mfrac":{"mn":"1","msub":{"mi":["b","ij"]}},"mo":"\u2062","mrow":{"msub":{"mi":["p","\u03b7"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"msub":[{"mi":["a","ij"]},{"mi":["b","ij"]}]}}}}}}],"mo":"="}}},{"mtd":{"mrow":{"mo":"=","mrow":{"munderover":{"mo":"\u220f","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"k"},"mo":"\u2062","mrow":{"mo":["{","}"],"mfrac":{"mrow":{"mn":"2","mo":"\u2062","msub":{"mi":["a","ij"]}},"msup":{"mrow":{"msubsup":{"mi":["b","ij"],"mn":"2"},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["a","ij"]},{"mi":["b","ij"]}],"mo":"\/"}},"mn":"2"},"mo":"+","mn":"1"}}},"mn":"2"}}}}}}}]}},{"mrow":{"mo":["(",")"],"mn":"2.17"}}]}}}}},"The maximisation of this equation is equivalent to the maximisation of Equation 2.9 and the motion estimated based on this equation is known as CD[B. Cohen and I. Dinstein. New maximum likelihood motion estimation schemes for noisy ultrasound images. 35:455-463, 2002.].","Taking the natural logarithm of both sides of Equation 2.15, to take into account that the displayed ultrasound images are log-compressed, and defining \u03b1=1n(a) and \u03b2=1n(b) gives:\n\n\u03b1=\u03b2+{circumflex over (\u03b7)}, where {circumflex over (\u03b7)}1(\u03b7).\u2003\u2003(2.18)\n","In this case, the probability density function of the additive noise {circumflex over (\u03b7)}is given by:",{"@attributes":{"id":"p-0238","num":"0237"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"\u03b7","mo":"^"}}},{"mn":"2","mo":["\u2062","\u2062"],"msup":{"mi":"\u03c3","mn":"2"},"mfrac":{"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"2","mo":"\u2062","mover":{"mi":"\u03b7","mo":"^"}}}},"msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"2","mo":"\u2062","mover":{"mi":"\u03b7","mo":"^"}}}},"mo":"+","mn":"1"}},"mn":"2"}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"2.19"}}]}}}}},"The conditional probability density function is therefore given by:",{"@attributes":{"id":"p-0240","num":"0239"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["\u03b1","i"]},{"mi":["\u03b2","i"]}],"mo":"|"},"mo":",","msub":{"mi":["v","i"]}}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"k"},"mo":"\u2062","mrow":{"mn":"2","mo":"\u2062","mfrac":{"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"2","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03b1","ij"]},{"mi":["\u03b2","ij"]}],"mo":"+"}}}}},"msup":{"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"2","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03b1","ij"]},{"mi":["\u03b2","ij"]}],"mo":"-"}}}}},"mo":"+","mn":"1"}},"mn":"2"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"2.20"}}]}}}}},"The maximisation of this equation is equivalent to the maximisation of Equation 2.9 and the motion estimated based on this equation is known as CD[B. Cohen and I. Dinstein. New maximum likelihood motion estimation schemes for noisy ultrasound images. 35:455-463, 2002.]. Alternatively, the following similarity function can be maximised [D. Boukerroui, J. A. Noble, and M. Brady. Velocity estimation in ultrasound images: A block matching approach. 2732: 586-598, 2003.]:",{"@attributes":{"id":"p-0242","num":"0241"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msubsup":{"mi":["E","i"],"msub":{"mi":"CD","mn":"2"}},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["v","i"]}}},{"mrow":[{"munderover":{"mo":"\u220f","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"k"},"mo":"\u2062","msub":{"mi":["\u03b1","ij"]}},{"mi":["\u03b2","ij"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":"ln","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"2","mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["\u03b1","ij"]},{"mi":["\u03b2","ij"]}],"mo":"-"}}}}},"mo":"+","mn":"1"}}}],"mo":["-","-"]}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"2.21"}}]}}}},"br":{}},"The MS and MSmeasures: Myronenko et al. [A. Myronenko, X. Song, and D. J. Sahn. Maximum likelihood motion estimation in 3d echocardiography through non-rigid registration in spherical coordinates. 5528:427-436, 2009.] looked at the more general case of correlated speckle in the two images which is quite likely in the case of high frame rates. The bivariate Rayleigh distribution of two random variables nand nwith equal variance \u03c32 is given by:",{"@attributes":{"id":"p-0244","num":"0243"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["p","n"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"n","mn":"1"},{"mi":"n","mn":"2"}],"mo":","}}},{"mfrac":{"mrow":[{"msub":[{"mi":"n","mn":"1"},{"mi":"n","mn":"2"}],"mo":"\u2062"},{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03c1"}},"mo":"\u2062","msup":{"mi":"\u03c3","mn":"4"}}]},"mo":["\u2062","\u2062"],"msup":{"mi":"exp","mfrac":{"mrow":[{"msubsup":[{"mi":"n","mn":["1","2"]},{"mi":"n","mn":["2","2"]}],"mo":"+"},{"mn":"2","mo":"\u2062","mrow":{"msup":{"mi":"\u03c3","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03c1"}}}}]}},"mrow":{"msub":{"mi":"I","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"msqrt":{"mi":"\u03c1"},"mo":["\u2062","\u2062"],"msub":[{"mi":"n","mn":"1"},{"mi":"n","mn":"2"}]},{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03c1"}},"mo":"\u2062","msup":{"mi":"\u03c3","mn":"2"}}]}}}}],"mo":["=","\u2062"],"mi":{}}},{"mrow":{"mo":["(",")"],"mn":"2.22"}}]},{"mtd":[{"mi":"where"},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]},{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"I","mn":"0"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"z"}},{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"0"},"mi":"\u221e"},"mo":"\u2062","mfrac":{"msup":{"mi":"z","mrow":{"mn":"2","mo":"\u2062","mi":"k"}},"mrow":{"msup":[{"mn":"2","mrow":{"mn":"2","mo":"\u2062","mi":"k"}},{"mrow":{"mo":["(",")"],"mrow":{"mi":"k","mo":"!"}},"mn":"2"}],"mo":"\u2062"}}}],"mo":"="}},{"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]}]}}}},"Iis a modified Bessel function of the fist kind with zero order, 0\u2266\u03c1\u22661 is the correlation coefficient and n\u22670, n\u22670. Using this, they arrive at a similarity measure:",{"@attributes":{"id":"p-0246","num":"0245"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"msubsup":{"mi":["E","i","MS"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["v","i"]}}},{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"k"},"mo":"\u2062","mrow":{"mfrac":{"mn":["3","2"]},"mo":"\u2062","mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msup":{"mi":"cosh","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":{"msub":[{"mi":["a","ij"]},{"mi":["b","ij"]}],"mo":"-"},"mi":"D"}}},"mo":"-","mi":"\u03c1"}}}}},{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"cosh","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":{"msub":[{"mi":["a","ij"]},{"mi":["b","ij"]}],"mo":"-"},"mi":"D"}}}}}}],"mo":["-","+"],"mi":"const"}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2.23"}}]}}}},"br":{},"sub":"2 "},"In an embodiment, by including a model of blurred speckle, Myronenko et al. have also derived a similarity measure based on a bivariate Nakagami distribution, which is a generalized form of the Rayleigh distribution. The similarity measure for blurred and correlated speckle was derived as:",{"@attributes":{"id":"p-0248","num":"0247"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"msubsup":{"mi":["E","i"],"msub":{"mi":"MS","mn":"2"}},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["v","i"]}}},{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"k"},"mo":"\u2062","mrow":{"mfrac":{"mrow":{"mrow":{"mn":"2","mo":"\u2062","mi":"m"},"mo":"+","mn":"1"},"mn":"2"},"mo":"\u2062","mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msup":{"mi":"cosh","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":{"msub":[{"mi":["a","ij"]},{"mi":["b","ij"]}],"mo":"-"},"mi":"D"}}},"mo":"-","mi":"\u03c1"}}}}},{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":"cosh","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":{"msub":[{"mi":["a","ij"]},{"mi":["b","ij"]}],"mo":"-"},"mi":"D"}}}}}}],"mo":["-","+"],"mi":"const"}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2.24"}}]}}}},"br":{},"sub":"2 "},"In an embodiment, speckle tracking is used in an update step for refining a prior segmentation of the LV or an initial placement of the boundary in a stress sequence of images. In an embodiment, speckle tracking is used only along the boundary to search for the best match of a template within a predefined search window. In an embodiment, a set of coordinates, X, along the endocardial boundary in the rest frame and a corresponding set of coordinates, {tilde over (X)}, along the prior endocardial boundary in the stress frame are studied, where the rest and stress frames are at the same point in the cardiac cycle. In an embodiment of the subject invention, the objective of the update step is to get as close as possible to the set of coordinates, X, which define the true endocardial boundary in the stress frame. In an embodiment, the points within Xdefine the true endocardial boundary in rest. In an embodiment, the points within Xare approximations of the true endocardial boundary in rest. In an embodiment, template blocks centered at the points Xare used for matching within search windows around corresponding points in the stress frame (see ). In an embodiment, template blocks centered at the points Xare used for matching within search windows around corresponding points in {tilde over (X)}, under the assumption that structures around the LV boundary have a similar appearance in both the rest and stress states.","In an embodiment, a multiresolution approach is used whereby the search is carried out over three resolution levels to allow for a larger regional search without compromising computational speed; something which is particularly important in clinical applications. In an embodiment, to allow for varying degrees of attenuation between the rest and stress frames, when carrying out matching between the template blocks and the search windows, the log-compressed intensities within them are normalised to have the same mean and variance. In other words, the log-compressed intensities are transformed by subtracting the mean and dividing by the standard deviation of all the pixels in the block\/window. The resulting similarity measure is referred to as CD[D. Boukerroui, J. A. Noble, and M. Brady. Velocity estimation in ultrasound images: A block matching approach. 2732: 586-598, 2003.].","In an embodiment, the ML estimation is not simply carried out in terms of estimating the motion that gives the maximum similarity measure within the search window, but instead we apply Singh's approach [A. Singh. Image-flow computation: An estimation-theoretic framework and a unified perspective. 65(2):152-177, 1992.]. In such a formulation, the similarity measure, E, is computed for each possible displacement, (u, v), of the template block from the rest frame, W, within the search window in the stress frame, W. Then a confidence measure is assigned to each possible displacement by computing a response, R, derived from E, which represents the likelihood of such a displacement. In an embodiment:",{"@attributes":{"id":"p-0252","num":"0251"},"maths":{"@attributes":{"id":"MATH-US-00027","num":"00027"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"msup":{"mi":"R","msub":{"mi":"CD","mrow":{"mn":"2","mo":"\u2062","mi":"bis"}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","v"],"mo":","}}},{"mrow":{"mrow":{"mfrac":{"mn":"1","mi":"Z"},"mo":["\u2062","\u2062"],"mi":"exp","mrow":{"mo":["{","}"],"mrow":{"mo":"-","mrow":{"msup":{"mi":"kE","msub":{"mi":"CD","mrow":{"mn":"2","mo":"\u2062","mi":"bis"}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","v"],"mo":","}}}}}},"mo":"-","mi":"N"},"mo":"\u2264","mi":"u"}],"mo":"="},{"mi":"v","mo":"\u2264","mrow":{"mi":"N","mo":"."}}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2.25"}}]}}}}},"In an embodiment, Z is defined such that all probabilities sum to one, k is chosen to ensure the maximum response from Wis close to unity, and 2N+1 is the search window size. In Singh's approach, the displacement which results in the best texture match is therefore:",{"@attributes":{"id":"p-0254","num":"0253"},"maths":{"@attributes":{"id":"MATH-US-00028","num":"00028"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["u","c"]},"mo":"=","mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"msub":{"mi":["u","c"]}}},{"mtd":{"msub":{"mi":["v","c"]}}}]}},{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"munder":{"mo":"\u2211","mi":"u"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"v"},"mo":"\u2062","mrow":{"mrow":{"msup":{"mi":"R","msub":{"mi":"CD","mrow":{"mn":"2","mo":"\u2062","mi":"bis"}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","v"],"mo":","}}},"mo":"\u2062","mi":"u"}}}}},{"mtd":{"mrow":{"munder":{"mo":"\u2211","mi":"u"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"v"},"mo":"\u2062","mrow":{"mrow":{"msup":{"mi":"R","msub":{"mi":"CD","mrow":{"mn":"2","mo":"\u2062","mi":"bis"}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","v"],"mo":","}}},"mo":"\u2062","mi":"v"}}}}}]}}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"2.26"}}]}}}}},"An embodiment is applied to paired frames from rest and stress sequences that are at corresponding phases in the cardiac cycle. In an embodiment, prior segmentations from a CCA model or other methodology described above are defined as points on the endocardial boundary. In an embodiment, the segmentations are obtained using other methods known in the art. In an embodiment, every point on the boundary in the rest dataset frame, x\u03b5X, has a corresponding point on the boundary in the corresponding stress dataset frame, {tilde over (x)}\u03b5{tilde over (X)}. In an embodiment, there is not a one-to-one correspondence between Xand {tilde over (X)}. In an embodiment, not all frames in {tilde over (X)}, are refined. In an embodiment, by carrying out block matching, the aim is to improve the delineation of the endocardial boundary at a more localized level. In a particular embodiment, for any point along the boundary in rest frame, x, a template block of size 3\u00d73 pixels centered at xis used to search a block of size 5\u00d75 pixels centered at its corresponding point, {tilde over (x)}, in the stress frame. The similarity measure and weighted displacement for the normalised log-compressed intensity blocks are computed as described in Equations 2.21 and 2.26, respectively, in a three level multi-resolution coarse-to-fine approach.","Pseudo-code for carrying out an embodiment of this texture matching between rest and stress frame endocardial boundaries is as follows:",{"@attributes":{"id":"p-0257","num":"0256"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"for all Corresponding rest\/stress frames do"},{"entry":"\u2003for all Coarse to fine resolution levels do"},{"entry":"\u2003\u2003for all Endocardial boundary points do"},{"entry":"\u2003\u2003\u2003Define a template block in the rest image frame"},{"entry":"\u2003\u2003\u2003Define a search window in the stress image frame"},{"entry":"\u2003\u2003\u2003for all Possible displacements of the template in the search "},{"entry":"\u2003\u2003\u2003window do"},{"entry":"\u2003\u2003\u2003\u2003Compute E"},{"entry":"\u2003\u2003\u2003\u2003compute R"},{"entry":"\u2003\u2003\u2003end for"},{"entry":"\u2003\u2003\u2003Compute the probability weighted displacement (Equation 2.26)"},{"entry":"\u2003\u2003\u2003Apply the displacement to the stress boundary point"},{"entry":"\u2003\u2003end for"},{"entry":"\u2003end for"},{"entry":"end for"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"The most closely related work involving segmentation of stress echocardiography sequences is that by Shekhar et al. [R. Shekhar, V. Zagrodsky, M. J. Garcia, and J. D. Thomas. Registration of realtime 3-d ultrasound images of the heart for novel 3-d stress echocardiography. 23(9):1141-1149, 2004.; V. Zagrodsky, V. Walimbe, C. R. Castro-Pareja, J. X. Qin, J.-M. Song, and R. Shekhar. Registration-assisted segmentation of real-time 3-d echocardiographic data using deformable models. 24(9)1089-1099, 2005.; V. Walimbe, M. Garcia, O. Lalude, J. Thomas, and R. Shekhar. Quantitative real-time three-dimensional stress echocardiography: A preliminary investigation of feasibility and effectiveness. 20(1):13-22, 2007.]. Firstly, they make use of the ECG signal to pair up corresponding rest and stress frames i.e. the temporal alignment of the two sequences. For the segmentation, their approach is to have a segmented template 4D dataset for which the segmentations are already defined as a wiremesh. They align the ED frame of the template to the ED frame of the dataset to be segmented, which could be either the rest sequence or the stress sequence, using Mutual Information (MI)-based affine registration. They then transform the template wiremesh (using the transformation derived from the registration step) to arrive at an initial placement of the mesh in the ED frame of the dataset to be segmented. Finally, the transformed mesh is refined using a Sobel edge gradient vector flow deformable model. This process is then repeated using frame n and frame n+1 instead of the template ED frame and the ED frame of the dataset to be segmented, throughout the corresponding frames. In a feasibility study for the clinical application of thus work, they present the corresponding rest and stress segmentations side by side to aid clinical decisions.","In an embodiment, the Shekhar method or other methods are used to refine segmentations derived according to the subject invention. In a comparative study, the Shekhar method was implemented using MI-based affine registration between images and instead of using a wiremesh. The resulting transformation was applied to the boundary points to arrive at an initial boundary placement. This was then updated using the Sobel edge gradient vector flow deformable model in 2D i.e. snakes. The only difference in this implementation and the one in V. Zagrodsky, V. Walimbe, C. R. Castro-Pareja, J. X. Qin, J.-M. Song, and R. Shekhar. Registration-assisted segmentation of real-time 3-d echocardiographic data using deformable models. 24(9):1089-1099, 2005, is that the wiremesh forces in between nodes (which serve as an additional smoothness constraint to the mesh) have not been included.","A comparison of the CDand MSsimilarity measures for the purposes of updating the prior segmentations from the CCA model is presented here. The results for the final segmentations from the predict and update process are then presented before being compared to the segmentations from repeating the MI-based registration\/snakes procedure described by Shekhar et al.","The update step was tested using both the CDsimilarity measure and the MSsimilarity measure (using Singh's approach for smoothing displacement estimates in both). In other embodiments, other similarity measures are used.  shows the results from using CDand  shows the results from using MS, with D=1, \u03c1=0.8 and m=0.5 as given in [A. Myronenko, X. Song, and D. J. Sahn. Maximum likelihood motion estimation in 3d echocardiography through non-rigid registration in spherical coordinates. 5528:427-436, 2009.], when applied to the same stress dataset. Only the stress frames which had corresponding rest frames are shown and the first frame is the ED frame which is initialised to the true boundary for the priors, hence the red\/green boundary overlap in those frames.","In an embodiment, whereas the prior segmentation from the CCA model gives a good general location of the endocardial boundary, the texture matching allows for the finer details at the boundary to be captured because each point is moved individually according to the rest-stress texture matching.  shows corresponding rest and stress frames with their segmentations. It shows that along some areas of the endocardial boundary, the update step corrects the prior boundary very well and moves it closer to the true boundary e.g. close to the mitral valve hinge points at the bottom. This is because the dark regions to the left and right of the mitral valve hinge points act as a contrast against the blood pool inside the LV. In other areas, particularly around the apex at the top, the update step is not as good and this is because the texture is quite similar in the region around the apex and it is therefore more difficult to define the exact location of the boundary. This illustrates the main weakness of the update step; in areas with little detectable features such as a clear boundary, the update step does not have much to base the match on, and therefore can only be as good as the prior.",{"@attributes":{"id":"p-0263","num":"0262"},"figref":"FIG. 35","i":"IEEE Trans. Med. Imag., "},"Next, results of embodiments of the subject invention were validated against the MI-based affine registration\/snakes approach, using the entire database of datasets using the dice coefficient described below. The level of agreement between two segmentations can be (globally) measured by computing the dice coefficient:",{"@attributes":{"id":"p-0265","num":"0264"},"maths":{"@attributes":{"id":"MATH-US-00029","num":"00029"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"D","mo":"=","mfrac":{"mrow":[{"mn":"2","mo":"\u2062","mi":"TP"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":["FP","TP"],"mo":"+"}},{"mo":["(",")"],"mrow":{"mi":["TP","FN"],"mo":"+"}}],"mo":"+"}]}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2.27"}}]}}}}},"Where TP, FP and FN are defined as shown in . The dice coefficient was used to measure the results of embodiments of the subject invention against the \u2018true\u2019 segmentations i.e. those which were marked and verified by two expert clinicians.","In evaluating the results for the segmentation of all the available datasets (n=78 as in the previous chapter), the dice coefficients were computed for the results of embodiments of the subject invention against the truth, and also from the MI-based registration\/snakes scheme described by Shekhar et al. against the truth. The dice coefficients for the subject invention were better for 52 out of 78 datasets when compared to those from the MI-based registration\/snakes scheme.  shows the histograms for the dice coefficients from each method.","In an embodiment, rest and stress segmentations derived using one or more of the methodologies described above are used to classify the behavior of the image subject. In an embodiment, the subject is a heart and the behavior is classified as normal or abnormal. In an embodiment, a particular class of abnormal behavior is identified. In an embodiment, the classifications aide diagnosis of cardiac disease. In an embodiment, the classification highlights subjects for further review by experts or otherwise.","Current analysis by clinicians typically consists of manually looking at rest and stress sequences to link the behaviour between the two and identify any regions that are a cause for concern. The LV wall segments are typically divided as shown in  and a segment scoring scheme is used for describing the motion as 1=normal, 2=kinetic, 3=akinetic and 4=dyskinetic. Automating this process of cardiac analysis is very difficult because of the number of variables that clinicians consider in the evaluation, and even then, only the very experienced clinicians are able to deduce some of the more subtle abnormalities. Consequently, there is very little literature on the subject of automated classification of the LV from stress echocardiography [R. M. Peters. Fuzzy cluster analysis of positive stress tests: Comparison with stress echocardiography and nuclear perfusion imaging in patients with triple vessel and left main coronary disease. 91:66-68, 1999.; S. Mansor, N. P. Hughes, and J. A. Noble. Wall motion classification of stress echocardiography based on combined rest-and-stress data. 5242:139-146, 2008.]. Discussions with clinical collaborators (Dr Becher and Dr Szmigielski of the John Radcliffe Hospital Cardiology Department, Oxford) reveal that even the automated classification into normal\/abnormal for the purpose of highlighting which subjects require further investigation would be of tremendous use as it could significantly reduce the time spent on analysis.","Various classification techniques can be used with embodiments of the subject invention. Here, two illustrative examples (naive Bayes' and random forests) are described. In an embodiment, these or other techniques can be used in determining whether a heart is normal or abnormal. The inputs to these are measures (or features) that have been derived from rest and stress segmentations of images. In an embodiment, the segmentations are produced using one or more of the methods described above. Classifications resulting from embodiments of the subject invention were tested, compared, and validated against recent work carried out by others and also compared to the specificity\/sensitivity values achieved clinically. In embodiments, results are promising and have the advantage that they are much quicker to derive for the purpose of highlighting which patient datasets require further analysis.","In an embodiment, datasets are classified into normal (clinical scoring scheme value of 1) or abnormal (clinical scoring scheme values 2-4) depending on the characteristics measurable from the rest and\/or stress segmentations. Classification can be carried out using a variety of features.","In an embodiment, a supervised classification technique is used wherein a machine learning technique learns a function for classification from training data, which consists of features upon which to base the classification (inputs) paired with their corresponding classes (outputs). In an embodiment, when applying this to a new object (for which the class is unknown), the object features are used as inputs to the function and the most likely class is then assigned to it. In an embodiment, hearts are classified into normal or abnormal. Two possible supervised classification techniques; the naive Bayes' classifier and random forests, are described here. These were found to be among the worst and best classification techniques respectively when their performance was evaluated alongside many other supervised classification techniques [R. Caruana and A N.-Mizil. An empirical comparison of supervised learning algorithms. 23148, 2006.]. The naive Bayes' classifier is based on Bayes' theorem and makes strong independence assumptions about the input features and also assumes that all the features are equally powerful in their ability to distinguish between classes. Random forests can produce a very accurate classifier in many datasets because of its ability to estimate the importance of variables in the classification i.e. all features are not thought to be equally important as in the naive Bayes' classifier. Both of these machine learning classification techniques will now be discussed in more detail.","In an embodiment using the naive Bayes' classifier, the problem is considered as that of computing the class probability of a binary event (normal\/abnormal) given a set of independent features (based on measurements from stress echocardiography segmentations). Let the classifier. Y, be a binary random variable where Y equals 1 for abnormal and 0 for normal and X, . . . , Xbe a set of feature variables. If the features are conditionally independent given Y, the conditional joint probabilities can be written as:",{"@attributes":{"id":"p-0274","num":"0273"},"maths":{"@attributes":{"id":"MATH-US-00030","num":"00030"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"X","mn":"1"},"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"msub":{"mi":["X","p"]},"mo":"\u2758","mi":"Y"}}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"p"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["X","j"]},"mo":"\u2758","mi":"Y"}}},"mo":"."}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"3.1"}}]}}}}},"From Bayes' theorem, the conditional probability for the classifier can be written:",{"@attributes":{"id":"p-0276","num":"0275"},"maths":{"@attributes":{"id":"MATH-US-00031","num":"00031"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"posterior","mo":"=","mfrac":{"mrow":{"mi":["prior","likelihood"],"mo":"\u00d7"},"mi":"evidence"}}},{"mrow":{"mo":["(",")"],"mn":"3.2"}}]},{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"Y","mo":"\u2758","msub":{"mi":"X","mn":"1"}},"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["X","p"]}}}},"mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Y"}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"X","mn":"1"},"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"msub":{"mi":["X","p"]},"mo":"\u2758","mi":"Y"}}}}],"mo":"\u2062"},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"X","mn":"1"},{"mi":["X","p"]}],"mo":[",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}}}]}}}},{"mtd":{"mrow":{"mrow":{"mo":"=","mrow":{"mfrac":{"mn":"1","mi":"Z"},"mo":["\u2062","\u2062"],"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Y"}},{"munderover":{"mo":"\u220f","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"p"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["X","j"]},"mo":"\u2758","mi":"Y"}}}}]}},"mo":","}}}]}},{"mrow":{"mo":["(",")"],"mn":"3.3"}}]}]}}},"br":{},"sub":["1","p"]},"One advantage that the naive Bayes' classifier offers is that it only requires a small number of training datasets to estimate the parameters of the classification model. The decoupling of the class conditional feature distributions (Equation 3.1) means that each distribution can be independently estimated as a one dimensional distribution, so the number of datasets required does not increase exponentially with the number of features used in the model.","In an embodiment using a random forests classifier, classification trees are used to compute the membership of objects in each of the possible classes depending on the values of the set of feature variables. A classification tree is a model with a tree-like structure and contains nodes and edges (see ).","The nodes are one of two types; intermediate nodes and leaf nodes. The intermediate nodes are labeled as feature variables and the edges extending from these nodes are predicates on these features. A leaf node is a node which has one incoming edge and no outgoing edges and is labeled with one class. Once the model is built using training data, the classification for new datasets can be determined by going down the classification tree from root to leaf. Typically, the features that appear at the top of the tree are those that are more important in making the classifications.","In a binary classification tree, there are only two outgoing edges from the intermediate nodes, which we shall refer to as the left (\u2266) and right (>). From the training datasets, D, at any particular node, the classes (1 or 2) have occurrences Nand Nrespectively. After the split, the left side Dhas the class occurrences, Land Land the same analogy can be applied for the right side as shown in .","Ideally, one would like all class 1 occurrences on the left side and all class 2 occurrences on the right side (or vice versa) following the split i.e. no misclassification. More generally, there is misclassification and at a node D, if the ratio of Land Las:",{"@attributes":{"id":"p-0282","num":"0281"},"maths":{"@attributes":{"id":"MATH-US-00032","num":"00032"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"P","mn":"1"},"mo":"=","mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"\u2758","msub":{"mi":["D","L"]}}}},"mo":"=","mfrac":{"msub":{"mi":"L","mn":"1"},"mrow":{"msub":[{"mi":"L","mn":"1"},{"mi":"L","mn":"2"}],"mo":"+"}}}},{"mrow":{"msub":{"mi":"P","mn":"2"},"mo":"=","mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"2","mo":"\u2758","msub":{"mi":["D","L"]}}}},"mo":"=","mfrac":{"msub":{"mi":"L","mn":"2"},"mrow":{"msub":[{"mi":"L","mn":"1"},{"mi":"L","mn":"2"}],"mo":"+"}}}},"mo":","}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"3.4"}}]}}}},"br":[{},{},{}],"in-line-formulae":[{},{}],"i":["P",",P","P","P"],"sub":["1","2","1","2"],"sup":["2","2"]},{"@attributes":{"id":"p-0283","num":"0282"},"maths":{"@attributes":{"id":"MATH-US-00033","num":"00033"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"entropy","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"P","mn":"1"},"mo":[",",","],"mrow":[{"mi":"\u2026","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["P","i"]}},{"mi":"\u2026","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["P","k"]}}]}}},{"mo":"-","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"k"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":["P","i"]},"mo":"\u2062","mrow":{"mrow":{"mi":"ln","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["P","i"]}}},"mo":"."}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"3.6"}}]}}}},"br":{}},"Ensemble learning methods are ones that generate many classification trees i.e. classifiers, and then aggregate their results. The idea behind this is that the ensemble is generally more accurate than any one of the individual classifiers that make up the ensemble. One such method is known as boosting and another is bagging. In boosting methods, the training datasets (typically a feature vector for each dataset) used for each successive classifier are chosen based on the performance of the earlier classifiers. Datasets that are incorrectly classified by the earlier classifiers are chosen more often than those that are correctly classified. In this way, the boosting method attempts to produce new classifiers that are better able to classify the types of datasets for which classification is poor in the current ensemble of classifiers. Adaptive boosting [Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. , pages 148-156, 1996.], otherwise known as Ada-boosting, uses the approach of selecting datasets based on their probabilities, or by using all the datasets but by weighting the error of each dataset by the probability of that dataset. If the first of these approaches is considered; if the number of datasets is N, then for the initialisation, the probability of picking any one dataset is 1\/N (this is later modified after each trained classifier is added to the ensemble). Let Ebe the sum of dataset probabilities of the misclassified instances for the currently trained classifier C. The dataset probabilities for the next trial are modified such that the probabilities for the misclassified instances are multiplied by the factor:",{"@attributes":{"id":"p-0285","num":"0284"},"maths":{"@attributes":{"id":"MATH-US-00034","num":"00034"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["B","k"]},"mo":"=","mfrac":{"mrow":{"mn":"1","mo":"-","msub":{"mi":["E","k"]}},"msub":{"mi":["E","k"]}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"3.7"}}]}}}},"br":{},"sub":["1","2","k","k ","k"]},"In bagging methods [L. Breiman. Bagging predictors. 24:123-140, 1996.], each classifier is trained on a random redistribution of the training datasets. N datasets are randomly drawn, with replacement, from the original N training datasets and consequently, some datasets may be drawn more than once while others are not drawn at all. Each classifier is therefore generated using a different random sampling of the training datasets and the aggregation of these gives the final ensemble of classifiers in the model. The majority vote is then often taken as the aggregated classification. Breiman showed that bagging is effective on unstable learning algorithms where small changes in the training dataset could result in large changes in the final classification, and that decision trees such as the one in  are of this type. Also, bagging methods are generally more resilient to overfitting noise when compared to boosting methods.","Many classification trees are also grown in random forests and for each tree, N training datasets are randomly drawn with replacement from the N available datasets just as in bagging. If there are M input variables for each dataset (or feature vector), a number m<M is specified such that at each node in the classification tree, m variables are selected at random and the variable that gives the best split out of these m is the one used to split the node.","Most features that have been derived for the purpose of assessment of cardiac motion analysis are based on the motion of the endocardial border and myocardial thickening e.g. changes in LV area\/volume, radial shortening, ejection fraction, velocities; from blood flow and endocardial motion, and myocardial thickness around the LV. One reason behind this is that it is easier to track the endocardial border because there is a better contrast between the blood\/tissue boundary than for example a tissue\/tissue boundary, particularly in US images. Consequently, embodiments of the subject invention use features based on measurements derived from the endocardial boundary.","One reason that clinicians carry out stress echocardiography is that the stress state provides information that may not be clearly visible in the rest state e.g. where there are only mild abnormalities in motion in rest, the abnormalities are likely to be exaggerated in stress, therefore the ability to distinguish abnormal from normal behaviour is better from stress. One of the most-used and well-validated quantitative measures that are used clinically is ejection fraction (EF) which is the difference in LV volume between the ED and ES phases in the cardiac cycle. It provides a good global measure of heart function because it measures the amount of blood pumped out from the heart in the systole part of the cardiac cycle. Clearly, if not enough oxygenated blood is being pumped out to the body, then the myocardial muscle is not moving as it should be. The 2D equivalent of the EF is the change in LV area between the ED and ES frames. In an embodiment, this feature is used to classify a heart as described below.","Typically, the combined analysis of rest and stress states is very powerful and through this combined analysis, clinicians can infer even more information on heart function. Discussions with clinical collaborators on this subject reveal that another important aspect that they look out for in corresponding rest\/stress frames is that the endocardial boundary in stress must lie inside the endocardial boundary in rest in the ED to ES stage of the cardiac cycle. In an embodiment, a measure is derived for this feature. In an embodiment, the feature is used for classification of a heart.","In an embodiment, the magnitude of update required in the previously described update step is measures and used to classify a heart. In embodiments, the CCA model, or other method used to derive image segmentations, is trained on normal heart data. Thus, the model's prediction for abnormal hearts is generally poor and would require a larger update to take it to the endocardial boundary. In embodiments, this level of update is included as a feature in the classification of a heart. Each of the features will now be discussed in further detail. These features are illustrative examples. In embodiments, other features can be used in classification.","In an embodiment, feature 1 is measured. Feature 1 is the change in area between the ED and the ES frames relative to the area in the ED frame:",{"@attributes":{"id":"p-0293","num":"0292"},"maths":{"@attributes":{"id":"MATH-US-00035","num":"00035"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"\u0394","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["A","rel"]}},{"mfrac":{"mrow":{"msub":[{"mi":["A","ED"]},{"mi":["A","ES"]}],"mo":"-"},"msub":{"mi":["A","ED"]}},"mo":"."}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"3.8"}}]}}}},"br":{}},"In an embodiment, feature 2 measures the amount of stress LV that lies outside of the rest LV at corresponding points in the cardiac phase. Let R represent a rest frame consisting of binary pixels that are labelled 1 inside the endocardial boundary and 0 outside of the endocardial boundary. Similarly, S represents the corresponding stress frame with binary pixels that are labelled 1 inside the endocardial boundary and 0 outside of the endocardial boundary. Then the stress-outside-rest measure, A, is given by:",{"@attributes":{"id":"p-0295","num":"0294"},"maths":{"@attributes":{"id":"MATH-US-00036","num":"00036"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["A","sor"]},"mo":"=","mfrac":{"mrow":[{"mrow":{"mrow":{"mo":"\u2211","mi":"S"},"mo":"&"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mover":{"mi":["R","_"]}},{"mo":"\u2211","mi":"S"}]}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"3.9"}}]}}}},"br":{},"figref":"FIG. 40"},"In an embodiment, feature 3 measures the level of update made by the block matching technique described above. In an embodiment, a level of update produced via another update technique is measured. In an embodiment, feature 3 is determined by considering the level of agreement between the prior and updated segmentations. One way this can be achieved is by calculating the dice coefficient as described in the previous chapter. An alternative to this method is the Cohen's Kappa coefficient, \u03ba, which is usually considered more robust as it takes into account that there could be agreement resulting from purely random factors [J. Cohen. A coefficient of agreement for nominal scales. 1(37):37-46, 1960.]. If the table of frequencies of pixels classified as inside\/outside the LV is as shown in Table 2.1, for the prior and updated segmentations, then:",{"@attributes":{"id":"p-0297","num":"0296"},"maths":{"@attributes":{"id":"MATH-US-00037","num":"00037"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["p","o"]},"mo":"=","mrow":{"mfrac":{"mrow":{"mn":["25870","64912"],"mo":"+"},"mn":"92160"},"mo":"=","mn":"0.985"}},{"mrow":{"msub":{"mi":["p","c"]},"mo":"=","mrow":{"mrow":{"mrow":[{"mn":["0.285","0.291"],"mo":"\u00d7"},{"mn":["0.175","0.709"],"mo":"\u00d7"}],"mo":"+"},"mo":"=","mn":"0.590"}},"mo":","}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mo":["(",")"],"mn":"3.10"}}]}}}},"br":{},"sub":["o","c "]},{"@attributes":{"id":"p-0298","num":"0297"},"maths":{"@attributes":{"id":"MATH-US-00038","num":"00038"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"\u03ba","mo":"=","mrow":{"mfrac":{"mrow":[{"msub":[{"mi":["p","o"]},{"mi":["p","c"]}],"mo":"-"},{"mn":"1","mo":"-","msub":{"mi":["p","c"]}}]},"mo":"=","mrow":{"mfrac":{"mrow":[{"mn":["0.985","0.590"],"mo":"-"},{"mn":["1","0.590"],"mo":"-"}]},"mo":"=","mrow":{"mn":"0.963","mo":"."}}}}},{"mrow":{"mo":["(",")"],"mn":"3.11"}}]}}}},"br":{}},{"@attributes":{"id":"p-0299","num":"0298"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"259pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2.1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"A table of pixel frequencies according to inside\/outside LV classifactions made by the"},{"entry":"prior and update segmentations."}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"231pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Prior"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"1","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"77pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"77pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"77pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Update","In","Out","Pin\/out Update"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}},{"entry":["In","25870","986",{"maths":{"@attributes":{"id":"MATH-US-00039","num":"00039"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":{"mn":["25870","986"],"mo":"+"},"mn":"92160"},"mo":"=","mn":"0.291"}}}}]},{"entry":{}},{"entry":["Out","392","64912",{"maths":{"@attributes":{"id":"MATH-US-00040","num":"00040"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":{"mn":["392","64912"],"mo":"+"},"mn":"92160"},"mo":"=","mn":"0.709"}}}}]},{"entry":{}},{"entry":["Pin\/out  Update",{"maths":{"@attributes":{"id":"MATH-US-00041","num":"00041"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":{"mn":["25870","392"],"mo":"+"},"mn":"92160"},"mo":"=","mn":"0.285"}}}},{"maths":{"@attributes":{"id":"MATH-US-00042","num":"00042"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":{"mn":["986","64912"],"mo":"+"},"mn":"92160"},"mo":"=","mn":"0.715"}}}},"\u03a3p = 1.000"]},{"entry":{"@attributes":{"namest":"1","nameend":"4","align":"center","rowsep":"1"}}}]}}]}}},"In an embodiment, once the feature values have been calculated for all the datasets a classifier is applied. Here, the naive Bayes' classifier and the random forests classifier is applied using a raw data matrix with one row per dataset, of the form [feature 1; feature 2; feature 3; classification]. The classifier was first trained using all but one dataset i.e. a leave-one-out approach. When this trained classifier is applied, the feature values for the left-out dataset are the inputs to the classifier and the output is the class assigned. For the naive Bayes' classifier, the probabilities are determined from the frequencies in the training dataset. For the random forests classifier, matlab code produced by Abhishek Jaiantilal [A. Jaiantilal. Random forest implementation for matlab. http:\/\/code.google.com\/p\/randomforest-matlab\/, 2009.] which is based on the algorithms described by Breiman et al. [L. Breiman. Random forests. 45(1):5-32, 2001.] was used. This embodiment, makes use of the Gini index for determining optimal node-splitting. Random forests was run with m=1, m=2 and m=3 with the three described features with 500 trees in each ensemble.","The tested database consisted of the aforementioned 78 normal studies plus 92 abnormal studies for which ethics approval had been obtained. The age distribution for the abnormal studies is shown in  which again shows that the majority of subjects are quite elderly.","In order to determine the effectiveness of each of the features in terms of their ability to distinguish normal from abnormal hearts, we considered their values for all the datasets as frequency distributions. This gives a very good overview of the separation of the feature values for normal and abnormal datasets.",{"@attributes":{"id":"p-0303","num":"0302"},"figref":["FIG. 42","FIG. 43"],"sub":["rel ","rel "]},{"@attributes":{"id":"p-0304","num":"0303"},"figref":["FIG. 44","FIG. 45"],"sub":["sor","sor "]},"For the level of update feature, we considered two measures; the dice coefficient and Cohen's Kappa coefficient, \u03ba. The normal distribution probability plots for these are shown in , and the normal density plots for the coefficients' distributions are given in . Both coefficients give very similar results where the separation between the normal and abnormal curves is not very large, especially when one considers that they span a very narrow band of coefficient values.","Naive Bayes' and random forest classification were carried out using each feature individually, and also by using various combinations to show which ones were useful in classification and what the optimal combination was. Table 2.2 outlines the results as the percentage of datasets correctly classified over the total number of datasets, CC (%).",{"@attributes":{"id":"p-0307","num":"0306"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"49pt","align":"center"}}],"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":["Method",{},{},{},{}]},{"entry":["Features Used ","NB","RF (m = 1)","RF (m = 2)","RF (m = 3)"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"49pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["1","59.3","68.3","\u2014","\u2014"]},{"entry":["2","56.3","52.1","\u2014","\u2014"]},{"entry":["1 & 2","58.1","68.3","67.7","\u2014"]},{"entry":["3D","55.7","42.5","\u2014","\u2014"]},{"entry":["1 & 3D","56.9","69.5","70.7","\u2014"]},{"entry":["2 & 3D","55.7","5.03","51.5","\u2014"]},{"entry":["1, 2 & 3D","57.5","74.3","73.7","74.3"]},{"entry":["3\u03ba","55.7","47.9","\u2014","\u2014"]},{"entry":["1 & 3\u03ba","56.9","72.5 ","70.7","\u2014"]},{"entry":["2 & 3\u03ba","55.7","54.5","54.5","\u2014"]},{"entry":["1, 2 & 3\u03ba","57.5","74.3","72.5","71.3"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"The results show that the random forest classifier gives a significantly higher percentage of correct classifications compared to the naive Bayes' classifier for all combinations of the features in the model. This is because the naive Bayes' classifier assumes that all features are equally capable at distinguishing between normal and abnormal hearts, but this assumption is clearly violated because the level of overlap in the normal distributions is not the same for all the features that we have considered. In the random forests implementation, using m=2 or m=3 did not necessarily result in an improvement in the percentage of correct classifications because of the random nature of selection of variables used in node-splitting when building the classification trees. Looking more closely at the results from random forests with m=1 shows that feature 1 (the difference in area between the ED and ES states) is better when it is the sole feature for classification (CC=68.3%) compared to features 2 (CC=52.1%), 3D (CC=42.5%) and 3\u03ba (CC=47.9%). When combining two features, the best combination is features 1 and 3\u03ba which give correct classification 72.5% of the time. Unlike in naive Bayes' where using feature 3D or 3\u03ba gives the same results, 3\u03ba gives better classification than 3D in random forests because these features are not considered equally important. As expected, the more useful information there is available, the better the classifier can be trained and consequently using 3 features gives the best results (CC=74.3%).","The results so far are for testing data where the tested dataset is independent to the training datasets. Next classification results where the tested dataset is one of the training datasets is presented. This can be interesting to consider because the classifier is typically only as good as the data used to train it. If the training data is only for one type of dataset, the classifier's use is limited to datasets of that type as it cannot accurately represent anything outside of this scope. For better general classification, it is best to train the classifier on as large a variety of datasets as possible to encompass as much variation in data as possible. In an embodiment, a clarifier is trained on a specific type of data and then used to classify that type of data. Table 2.3 shows the results for training data and shows that the correct classification percentages are significantly higher than those given in Table 2.2 with the best classifiers using two or more features to get CC=100%.","Clinical accuracies for determining whether a heart is normal or abnormal are not well-established for stress echocardiography because of the many degrees of variation that can be seen in both normal and abnormal dataset groups. Generally, a group consensus is obtained by at least two doctors in determining whether a heart is healthy. Next, sensitivity\/specificity values for classifications produced by embodiments of the subject invention are compared to those by clinicians. Using random forests on testing data with m=1 and features 1, 2, and 3\u03ba gave a specificity of 67.1% and a sensitivity of 74.7% where a positive instance is defined as an abnormal classification. This is close to the 70-80% sensitivity\/specificity ranges achieved clinically (from discussions with our clinical collaborators).",{"@attributes":{"id":"p-0311","num":"0310"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2.3"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"A table of the percentage of datasets correctly classified using various "},{"entry":"combinations of features in random forest (RF) classification. "},{"entry":"3D is feature 3 with the dice coefficient and 3\u03ba is feature 3 with Cohen's"},{"entry":"kappa coefficient as the level of segmentation agreement measure"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"126pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Features Used","RF (m \u2212 1)"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"126pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1","98.8"]},{"entry":[{},"2","99.4"]},{"entry":[{},"1 & 2","100"]},{"entry":[{},"3D","88.0"]},{"entry":[{},"1 & 3D","100"]},{"entry":[{},"2 & 3D","100"]},{"entry":[{},"1, 2 & 3D","100"]},{"entry":[{},"3\u03ba","89.8"]},{"entry":[{},"1 & 3\u03ba","100"]},{"entry":[{},"2 & 3\u03ba","100"]},{"entry":[{},"1, 2 & 3\u03ba","100"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"The most related published work on classification is by Mansor et al. [S. Mansor, N. P. Hughes, and J. A. Noble. Wall motion classification of stress echocardiography based on combined rest-and-stress data. 5242:139-146, 2008.; S. Mansor. Wall motion classification of stress echocardiography. 2009.] where regional wall motion classification is made using Hidden Markov Models (HMMs) and they achieved correct classifications in the range 70-80% for testing data and 80-90% for training data. Embodiments of the subject invention give a similar accuracy for testing data but a significantly higher accuracy for training data.","The results are promising for testing data (74% correct classification) and considerably better for training data (100% correct classification). This shows that provided the training dataset can encompass the variations that are likely to be present in any new (unseen) datasets, very accurate classification results can be obtained. One way to do this is by training the classifier using very large numbers of diverse training datasets which is likely to be a more time-consuming process (in an embodiment, training the classifier using 170 datasets and then applying to one dataset takes about 30 seconds). However, when pre-determined classifiers are applied to a dataset, the process is very fast and therefore ideal for the purpose of clinical use, where only the application step is required. Accurate determination of normal\/abnormal hearts means that clinicians can then focus on the ones that are flagged as abnormal on the system and this can potentially significantly reduce the time spent on analysis.","Aspects of the invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc., that perform particular tasks or implement particular abstract data types. Such program modules can be implemented with hardware components, software components, or a combination thereof. Moreover, those skilled in the art will appreciate that the invention may be practiced with a variety of computer-system configurations, including multiprocessor systems, microprocessor-based or programmable-consumer electronics, minicomputers, mainframe computers, and the like. Any number of computer-systems and computer networks are acceptable for use with the present invention.","Specific hardware devices, programming languages, components, processes, protocols, and numerous details including operating environments and the like are set forth to provide a thorough understanding of the present invention. In other instances, structures, devices, and processes are shown in block-diagram form, rather than in detail, to avoid obscuring the present invention. But an ordinary-skilled artisan would understand that the present invention may be practiced without these specific details. Computer systems, servers, work stations, and other machines may be connected to one another across a communication medium including, for example, a network or networks.","As one skilled in the art will appreciate, embodiments of the present invention may be embodied as, among other things: a method, system, or computer-program product. Accordingly, the embodiments may take the form of a hardware embodiment, a software embodiment, or an embodiment combining software and hardware. In an embodiment, the present invention takes the form of a computer-program product that includes computer-useable instructions embodied on one or more computer-readable media. Methods, data structures, interfaces, and other aspects of the invention described above can be embodied in such a computer-program product.","Computer-readable media include both volatile and nonvolatile media, removable and nonremovable media, and contemplate media readable by a database, a switch, and various other network devices. By way of example, and not limitation, computer-readable media comprise media implemented in any method or technology for storing information. Examples of stored information include computer-useable instructions, data structures, program modules, and other data representations. Media examples include, but are not limited to, information-delivery media, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile discs (DVD), holographic media or other optical disc storage, magnetic cassettes, magnetic tape, magnetic disk storage, and other magnetic storage devices. These technologies can store data momentarily, temporarily, or permanently. In an embodiment, non-transitory media are used.","The invention may be practiced in distributed-computing environments where tasks are performed by remote-processing devices that are linked through a communications network or other communication medium. In a distributed-computing environment, program modules may be located in both local and remote computer-storage media including memory storage devices. The computer-useable instructions form an interface to allow a computer to react according to a source of input. The instructions cooperate with other code segments to initiate a variety of tasks in response to data received in conjunction with the source of the received data.","The present invention may be practiced in a network environment such as a communications network. Such networks are widely used to connect various types of network elements, such as routers, servers, gateways, and so forth. Further, the invention may be practiced in a multi-network environment having various, connected public and\/or private networks.","Communication between network elements may be wireless or wireline (wired). As will be appreciated by those skilled in the art, communication networks may take several different forms and may use several different communication protocols.","All patents, patent applications, provisional applications, and publications referred to or cited herein are incorporated by reference in their entirety, including all figures and tables, to the extent they are not inconsistent with the explicit teachings of this specification.","It should be understood that the examples and embodiments described herein are for illustrative purposes only and that various modifications or changes in light thereof will be suggested to persons skilled in the art and are to be included within the spirit and purview of this application.","Numbered Embodiments",{"@attributes":{"id":"p-0323","num":"0000"},"ul":{"@attributes":{"id":"ul0001","list-style":"none"},"li":["1. A method of determining a plurality of relationships between a first condition and a second condition, comprising:\n    \n    ","2. The method of embodiment 1, wherein the at least one landmark comprises at least two landmarks.","3. The method of embodiment 2, wherein a studied time value is within a certain range of a time value assigned to an image when the studied time value is equal to the time value assigned to the image.","4. The method of embodiment 2, wherein, for each of the at least one training dataset, each relationship of the plurality of relationships is determined based on a force applied to the subject, assuming one or more forces applied to the subject are balanced.","5. The method of embodiment 2, wherein the first period of time corresponds to a first one or more occurrences of a physical cycle exhibited by the subject and the second period of time corresponds to a second one or more occurrences of the physical cycle exhibited by the subject.","6. The method of embodiment 5, wherein:\n    \n    ","7. The method of embodiment 6, wherein, for each of the at least one training dataset, each relationship of the plurality of relationships is determined based on an amount of additional work energy required of the heart because of the stress stimulus, assuming work energy is conserved.","8. The method of embodiment 7, wherein, for each of the at least one training dataset, each relationship of the plurality of relationships is determined based on the equation:"]}},{"@attributes":{"id":"p-0324","num":"0341"},"maths":{"@attributes":{"id":"MATH-US-00043","num":"00043"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["c","n"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["x","n","stressed"]}},{"msub":{"mi":["d","n"]},"mo":"\u2062","msubsup":{"mi":["y","n","stressed"]}}],"mo":"+"}}},{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["a","n"]},"mo":"\u2062","msubsup":{"mi":["x","n","resting"]}},{"msub":{"mi":["b","n"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["y","n","resting"]}}],"mo":"+"}}},"mo":"+","mi":"W"}],"mo":"="},{"mi":"wherein","mo":"\u2062","mstyle":{"mtext":":"}}],"mo":[",","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}}]}}},"ul":{"@attributes":{"id":"ul0004","list-style":"none"},"li":[{"@attributes":{"id":"ul0004-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0005","list-style":"none"},"li":["a, h, c, and d are coefficients to be determined;","xand yare components of the first relative position at the studied time value of each landmark n of the at least one landmark;","xand yare components of the second relative position at the studied time value of each landmark n of the at least one landmark;","W is the amount of additional work energy required of the heart because of the stress stimulus; and","n varies from one to a count of the at least one landmark."]}},"9. The method of embodiment 7, wherein, for each of the at least one training dataset, each relationship of the plurality of relationships is determined based on the equation:"]}},{"@attributes":{"id":"p-0325","num":"0348"},"maths":{"@attributes":{"id":"MATH-US-00044","num":"00044"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["d","n"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["x","n","stressed"]}},{"msub":{"mi":["e","n"]},"mo":"\u2062","msubsup":{"mi":["y","n","stressed"]}},{"msub":{"mi":["f","n"]},"mo":"\u2062","msubsup":{"mi":["z","n","stressed"]}}],"mo":["+","+"]}}},{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["a","n"]},"mo":"\u2062","msubsup":{"mi":["x","n","resting"]}},{"msub":{"mi":["b","n"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["y","n","resting"]}},{"msub":{"mi":["c","n"]},"mo":"\u2062","msubsup":{"mi":["z","n","resting"]}}],"mo":["+","+"]}}},"mo":"+","mi":"W"}],"mo":"="},"mo":","}}},"br":{},"ul":{"@attributes":{"id":"ul0006","list-style":"none"},"li":[{"@attributes":{"id":"ul0006-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0007","list-style":"none"},"li":["a, h, c, d, e, and f are coefficients to be determined;","x, yand zare components of the first relative position at the studied time value of each landmark n of the at least one landmark;","x, yand zare components of the second relative position at the studied time value of each landmark n of the at least one landmark;","W is the amount of additional work energy required of the heart because of the stress stimulus; and","n varies from one to a count of the at least one landmark."]}},"10. The method of embodiment 9, wherein W comprises the difference between:\n    \n    ","11. The method of embodiment 9, wherein an image sequence processing system determines coefficients a, b, c, d, e, and f for each relationship of the plurality of relationships, for each of the at least one training dataset, by applying a multivariate regression model.","12. The method of embodiment 11, wherein the multivariate regression model is a linear multivariate regression model.","13. The method of embodiment 11, wherein the multivariate regression model is selected from the group consisting of a CCA model, a PCA model, a ICA model, and a PLSR model.","14. The method of embodiment 11, wherein each image of the first plurality of images of each of the at least one training dataset and each image of the second plurality of images of each of the at least one training dataset is an ultrasound image of the heart.","15. The method of embodiment 11, wherein one or more experts identify the position of one or more of the at least one landmark in one or more images of the first plurality of images of one or more of the at least one training dataset and\/or one or more images of the second plurality of images of one or more of the at least one training dataset.","16. The method of embodiment 7, further comprising generating a plurality of combined relationships corresponding to the plurality of studied time values, wherein for each studied time value of the plurality of studied time values, the combined relationship is generated based on each relationship corresponding to the studied time value of the plurality of relationships of each of the at least one training dataset.","17. The method of embodiment 16, wherein the method further comprises:\n    \n    "]}},"wherein each at least one predicted stress position represents a position of the corresponding landmark in a hypothetical image of the patient heart at the studied time value when the patient heart is under an amount of stress.\n\n","wherein each at least one predicted second position represents a position of the corresponding landmark in a hypothetical image of the inquire subject at the studied time value when the inquire subject is in a second condition.\n\n",{"@attributes":{"id":"p-0328","num":"0415"},"maths":{"@attributes":{"id":"MATH-US-00045","num":"00045"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["c","n"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["x","n","stressed"]}},{"msub":{"mi":["d","n"]},"mo":"\u2062","msubsup":{"mi":["y","n","stressed"]}}],"mo":"+"}}},{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["a","n"]},"mo":"\u2062","msubsup":{"mi":["x","n","resting"]}},{"msub":{"mi":["b","n"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["y","n","resting"]}}],"mo":"+"}}},"mo":"+","mi":"W"}],"mo":"="},"mo":","}}},"br":{},"ul":{"@attributes":{"id":"ul0022","list-style":"none"},"li":[{"@attributes":{"id":"ul0022-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0023","list-style":"none"},"li":["receiving the plurality of relationships comprises receiving coefficients a, b, c, and d for each studied time value of the plurality of studied time values;","xyare components of the first relative position at the studied time value of each landmark n of the at least one landmark;","W is an amount of additional work energy required of the heart in the second condition;","n varies from one to a count of the at least one landmark; and","xand yare components of the predicated second position at the studied time value for each landmark n of the at least one landmark."]}},"29. The method of embodiment 27, wherein, for each studied time value of the plurality of studied time values, the at least one predicted second position is predicted according to the equation:"]}},{"@attributes":{"id":"p-0329","num":"0422"},"maths":{"@attributes":{"id":"MATH-US-00046","num":"00046"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["d","n"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["x","n","stressed"]}},{"msub":{"mi":["e","n"]},"mo":"\u2062","msubsup":{"mi":["y","n","stressed"]}},{"msub":{"mi":["f","n"]},"mo":"\u2062","msubsup":{"mi":["z","n","stressed"]}}],"mo":["+","+"]}}},{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"n","mo":"=","mn":"1"},"mi":"landmarks"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["a","n"]},"mo":"\u2062","msubsup":{"mi":["x","n","resting"]}},{"msub":{"mi":["b","n"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["y","n","resting"]}},{"msub":{"mi":["c","n"]},"mo":"\u2062","msubsup":{"mi":["z","n","resting"]}}],"mo":["+","+"]}}},"mo":"+","mi":"W"}],"mo":"="},"mo":","}}},"br":{},"ul":{"@attributes":{"id":"ul0024","list-style":"none"},"li":[{"@attributes":{"id":"ul0024-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0025","list-style":"none"},"li":["receiving the plurality of relationships comprises receiving coefficients a, b, c, d, e, and f for each studied time value of the plurality of studied time values;","x, y, and zare components of the first relative position at the studied time value of each landmark n of the at least one landmark;","W is an amount of additional work energy required of the heart in the second condition;","n varies from one to a count of the at least one landmark; and","x, y, and zare components of the predicated second position at the studied time value for each landmark n of the at least one landmark."]}},"30. The method of embodiment 29, wherein W comprises the difference between:\n    \n    ","31. The method of embodiment 29, wherein the aligning each image of the first plurality of images to the common timescale comprises:\n    \n    ","32. The method of embodiment 31, wherein the position of each landmark of the at least one landmark in each image of the first plurality of images is represented by a displacement of the landmark in the image from an origin position.","33. The method of embodiment 32, wherein the origin position is defined for each landmark of the at least one landmark as a location of the landmark in a start phase image selected from the at least three phase images.","34. The method of embodiment 33, wherein the start phase image corresponds to an end diastolic phase of the cardiac cycle of the heart.","35. The method of embodiment 29, the method further comprising:\n    \n    ","37. The method of embodiment 36, the method further comprising:\n    \n    ","38. The method of embodiment 23, wherein the plurality of relationships are received from one or more data structures embodied on one or more computer-readable media.","39. A method of refining a feature associated with an image, comprising:\n    \n    ","40. The method of embodiment 39, wherein the method further comprises identifying the at least one time-aligned pair of images from a plurality of images of the heart at rest and a plurality of images of the heart under the stress stimulus, wherein for each of the at least one time-aligned pair of images:\n    \n    ","41. The method of embodiment 40, wherein the refining the position of each of the one or more points in the feature associated with the stressed image of each of the one or more of the at least one time-aligned pair of images, comprises:\n    \n    ","42. The method of embodiment 41, wherein updating the position based on texture information near the corresponding position in the rest image, comprises:\n    \n    ","43. The method of embodiment 42, wherein the similarity measure is computed by one or more suitably programmed computers.","44. The method of embodiment 42, wherein the position is updated based on the displacement of the plurality of possible displacements with the greatest computed similarity measure.","45. The method of embodiment 42, wherein the position is updated based on two or more displacements of the plurality of possible displacements and the effect of each displacement of the two or more displacements on the position is weighted based on the computed similarity measure of the displacement.","46. The method of embodiment 42, wherein the similarity measure is based on Rayleigh distributed speckle in the template set and the displacement.","47. The method of embodiment 44, wherein:\n    \n    ","48. The method of embodiment 42, wherein:\n    \n    ","49. The method of embodiment 42, wherein the feature approximates an epicardial border and\/or an endocardial border of the heart.","50. A method of temporally aligning a plurality of images of a heart, comprising:\n    \n    ","51. The method of embodiment 50, wherein one or more of the at least two piecewise interpolation functions is a linear function."]}},"In embodiments of the subject invention, computer-usable instructions for performing each of the methods described above are embodied on one or more computer-readable media. With respect to embodiment 38, the computer-usable instructions can be embodied on the one or more computer-readable media recited in the embodiment. In embodiments of the subject invention, at least one of the one or more computer-readable media can be incorporated into an image sequence processing system such as the image sequence processing system  discussed above."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF DRAWINGS","p":["The patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication, with color drawing(s), will be provided by the Office upon request and payment of the necessary fee.",{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 11","sub":["PCA","PCA"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 25"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 29"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 30"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 32","sub":"2bis "},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 33","sub":"2 "},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 34"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 35"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 36"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 37","sub":["CCA","CCA","MIS","MIS"]},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 38"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 39"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 40"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 41"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 42","sub":"rel "},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 43","sub":"rel "},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 44","sub":"sor "},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 45","sub":"sor "},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 46"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 47"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 48"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 49"}]},"DETDESC":[{},{}]}
