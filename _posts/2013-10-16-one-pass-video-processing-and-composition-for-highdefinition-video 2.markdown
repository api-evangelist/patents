---
title: One pass video processing and composition for high-definition video
abstract: A video composition model that provides a set of application programming interfaces (APIs) to set device contexts, and determine capabilities of graphics hardware from a device driver. After the model determines a configuration, the model determines input video stream states applicable to frame rates, color-spaces and alpha indexing of input video streams, interactive graphics, and background images. The model prepares the input video frames and reference frames, as well as a frame format and input/output frame index information. The input video streams, interactive graphics and background images are processed individually and mixed to generate an output video stream.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09584785&OS=09584785&RS=09584785
owner: MICROSOFT TECHNOLOGY LICENSING, LLC
number: 09584785
owner_city: Redmond
owner_country: US
publication_date: 20131016
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["RELATED APPLICATIONS","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This Application is a Continuation of and claims benefit from U.S. patent application Ser. No. 12\/136,771 that was filed on Jun. 11, 2008, and that is incorporated herein by reference in its entirety.","Operating systems provide applications access to graphics hardware for video processing using specified application programming interfaces (APIs), Certain CPU-intensive operations, such as inverse discrete cosine transform, motion compensation, de-interlacing, color correction, video capturing and video processing operations may be performed using the graphics hardware to be accelerated. This video acceleration works in conjunction with a video rendering model used by the graphics hardware (video card).","However, some video acceleration systems provide only limited support for High-Definition video, such as Blu-ray, HD DVD and ISDB-T. Some of the limitations include support for only a single stream of video, support for either multiple YCbCr streams or a single RGB stream, interlacing and de-interlacing of only the main video stream, unsupported composition primitives, limited background support, and others. The limitations make it difficult to provide, for example, a High-Definition video player because of inefficiencies in the 3D graphics pipeline. As such, YCbCr streams and RGB streams cannot be composed in a single pass, and the sub video stream requires a pre-processing stage for de-interlacing. In addition, the streams cannot be different frame rates, and the output frame rate is assumed the same as the input frame rate that is not applicable to telecined film source.","Due to the limitations and the missing functionalities, software vendors and hardware vendors are bypassing the APIs and using specifically designed private interfaces and graphics hardware. Software vendors are using a D3D shader for video processing and composition, which is inefficient and does not support low-end graphics hardware and mobile systems. Hardware vendors are shipping video cards with dedicated video processing hardware. Often the vendors encounter difficulties because of the specifics of their hardware and\/or software implementations.","A video composition model that provides a set of application programming interfaces (APIs) to ascertain device contexts and determine capabilities of graphics hardware from a device driver. After the model determines a configuration, the model determines input video stream states applicable to frame rates, color-spaces and alpha indexing of input video streams, interactive graphics, and background images. The model prepares the input video frames and reference frames, as well as a frame format and input\/output frame index information. The input video streams, interactive graphics and background images are processed individually and mixed to generate an output video stream.","In some implementations, a method of processing input video streams may include receiving input video streams, each video stream having a predefined frame rate and color space; exposing application programming interfaces (APIs) to provide access to graphics hardware; processing each video stream in accordance with characteristics of the video stream, independently of other video streams; mixing processed input video streams by alpha blending the processed video streams and converting the processed input video streams to an output frame rate to generate an output video stream; and outputting the output video stream.","In some implementations, a video composition modeling system may include a graphics kernel that exposes application programming interfaces (APIs) to access graphics hardware, a video input stage that receives input video streams, a processing stage that performs processing on a per-plane and per-pixel alpha basis for each input stream, a mixing stage that mixes and alpha blends the input video streams to generate an output video stream, and an output stage to output the output video stream.","This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the detailed description. This summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","Implementations described below provide a flexible video composition model within an operating system, such as MICROSOFT WINDOWS. The video composition model provides support for High-Definition formats, such as Blu-ray that are controlled by HDMV or BD-J (Java for BD). In addition, digital High-Definition broadcast standards, such as ISBD-T with BML, may be supported.",{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 1","FIG. 6"]},"Video data, such as a video stream or graphics may be input to a computing device for display on the display device . The computing device may include hardware, such as a graphics device  that cooperates with a software device driver  that may include in some implementations, a user-mode device driver  and a kernel-mode device driver  that collectively operate with the graphics device . Thus, the device driver  may be part of the user mode logic or kernel mode logic with respect to the operating environment of a graphics device . The user-mode device driver  and the kernel-mode device driver  are typically written by a hardware vendor such that the graphics device  is operable within an operating system, such as MICROSOFT VISTA.","The operating system may provide user-mode APIs  and a 3D APIs  that provide interfaces to the graphics device . The APIs  may be used to interface with an operating system graphics kernel , which in turn interfaces with the kernel-mode driver  and the graphics device . Exemplary APIs  may be the DIRECTX application programming interface, available from Microsoft Corporation, Redmond, Wash. The 3D APIs  may be the DIRECT3D application programming interface available from Microsoft, and may be use to render three dimensional graphics through exposed interfaces to the graphics device . The 3D APIs  expose the capabilities of 3D graphics hardware, including z-buffering, anti-aliasing, alpha blending, mipmapping, atmospheric effects, and perspective-correct texture mapping.","In the example of , video data may be provided to a graphics processor unit (GPU) , which is configurable to perform processing of the video data, as will be described in detail below. The output from GPU  is provided to the video memory . When the video memory  is read, the resulting data may be then provided to a digital to analog converter (DAC) , which outputs a corresponding analog video signal suitable for display by a display device . In some implementations, the display device  may be configured to process the digital data from video memory  without the need for DAC . As illustrated, the graphics device  may include the GPU , video memory  and DAC . In some implementations, the graphics device  may take the form of a video graphic card that can be configured within a PC or like device.","In some implementations, the video memory  may be virtualized. As such, if there is a demand for memory and the memory is all allocated, then secondary storage source may be used, and the operating system manages all the paging algorithms and mechanics for faulting in the secondary storage into the primary storage when it needs to be operated on. For example, if the primary storage is video memory , the secondary storage may be system memory within a computing device.","In some implementations, the GPU  may be scheduled (preempted). Thus, the usage of the GPU  may be managed giving different computation requirements of applications requesting it.","The configuration shown in  is but one example of a video data processing apparatus. Those skilled in the art will recognize that other possible configurations may also benefit from the methods and apparatuses of the present invention.",{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 2","b":["200","200","200","112","101","200","200"]},"The video composition model  may receive input streams , and in either RGB or YCbCr (YUV) format and compose the streams in a single pass. For example, the video streams , and may be in a YCbCr format where interactive graphics streams are RGB format. Each video stream , and may be either progressive or interlaced stream. For example, a main video stream () may be a progressive film source and a sub video stream () may be an interlaced video source. Each stream , and may have Clear Rectangle (e.g., up to 32 rectangles shared) and Luma Keying. For example, High-Definition content may have bonus features that use these composition primitives. The video composition model  may support the wide gamma standard xvYCC, and the device driver  may optimize the color conversion to the output device context by setting a flag.","If the output has alpha channel data , the video composition model  may provide options to fill out with an opaque value, fill out with an alpha value specified in the background color, copy from an input stream, or remain unchanged. A background color may be specified in either RGB or YCbCr color space. The background color may be color converted to the output color space if it is input in a different color space. Because each input stream , and may be any content, each stream may be either video content or graphics content, and may be in either RGB or YCbCr color space. For example, the video stream at the lowest z-order may be used for a background image. The video content may be optimized to either video playback or video processing.","In some implementations, each stream , and may have an individual frame rate. For example, the main video may be a 24p (progressive) film source, the sub video may be 60i (interlaced) video source, and the interactive graphics may be 10 fps. They may all be composed to the output at either 24 fps or 60 fps. The output frame rate may be configured to be less than the input frame rate. For example, a 24p film source telecined to 60i may be reversed to 24 fps rather than 60 fps with 3:2 repeat frames.","In some implementations, each stream , and may have individual filter settings. For example, the main video stream may be dimmer, the sub video stream may have noise reduction set to \u201con,\u201d and the interactive graphics stream may have all filters \u201coff.\u201d","The video composition model  may provide interfaces such that a video application and the kernel-model driver  may establish a proprietary method that may be used for non-standardized content protection. The processing may have individual proprietary context associated with it as well. For example, the video application may enable proprietary content protection to the main video stream, but not the sub video stream and interactive graphics stream. The video composition model  also preserves extended wide gamut data (xvYCC) through the pipeline to the output.","According to some implementations, interfaces may be provided to split the state parameters and the non-state parameters (see BLT_STATE and STREAM_STATE below) related to video memory. The device driver  may leverage this by only have to know the state change when the state is actually being changed. The video application may leverage this by validating the specific state at the time when state is being changed.","Referring to , a luma keying block  provides a range of upper and lower luma key value limits within which correspondent pixels of an input frame are treated as transparent. In video processing, any distinct attribute of a group of pixels may be the basis for keying. The luma keying block  keys on a brightness value. A pixel with no brightness (black) has a digital luminance value of 0, and a pixel that is fully bright (white) will have a value of 255.","A pre-process filter block  provides for video filtering that is performed prior de-interlacing. For example, a noise reduction filtering process may be performed in this block. Other examples of filtering include, but are not limited to smoothing, blending, motion masking, blurring, de-blocking, enhancing, etc. The video filters may be setup as digital filters, implement through mathematical operations applied to a transformed version of the input video signal. The transformation may be performed using a Fast Fourier or Discrete Cosine transformation on the input video signal, such that the frequency components may be manipulated.","A de-interlace\/inverse telecine block  constructs a progressive frame from one or multiple interlaced frames. Interlaced frames are common in analog transmission. The de-interlace\/inverse telecine block  may also inverse the telecine. Telecine accounts for the frame rate differences between motion pictures captured on film (e.g., 24 frames\/sec) and standard video equipment (e.g., 30 frames\/sec). As noted above, because the input streams may each have the same capabilities, any input stream can be interlaced. In some implementations, the input video streams may use the same de-interlacing method, but the cadence and frame rate may be different from stream to stream.","As noted above, the video application and the device driver  will be able to query the graphics device  to determine the capabilities of the graphics device  before processing begins (see CONTENT_DESC below) and the device usage (see DEVICE_USAGE below). As such, some implementations provide a mechanism to throttle the video application by throttling the reference surfaces provided to the de-interlace\/inverse telecine block  so that the device driver  may fall back to a less complex de-interlacing method (e.g. adaptive de-interlacing falls back to bob de-interlacing). In some implementations, the video application may lower the frame rate dynamically by scheduling intentional frame drops (e.g., 60i to 30p, a half rate with blend de-interlacing).","A post-process filter block  provides various video filtering that is performed after de-interlacing. For example, edge enhancement filtering may be performed in this block.","A frame rate conversion block  may convert the frame rate by either repeating or interpolating the frames. The video composition model  allows the device driver  to expose a frame rate conversion (see CUSTOM_RATE_DATA below), which may be used in a playback scenario to maximize the presentation experience by up-converting the frame rate to higher frame rate. A down-conversion may be used to inverse telecine the video content in video processing scenarios to convert from telecined interlaced frames to the original progressive frames (e.g., convert 60i to 24p with reverse 3:2 pulldown).","A ProcAmp block  may provide various color adjustments and controls. The ProcAmp block  may be used to clean video signal components in standard definition or high definition video signals. ProcAmp controls may include, but are not limited to brightness, contrast, hue and saturation. Common ProcAmp features may include, but are not limited to adjusting sync amplitude, boosting low light level video, reducing video wash out, and chroma clipping.","A color space conversion block  may convert from one color space or color matrix to another if the input stream is different than the output color space. For example, RBG may be converted to YCbCr and vice versa using digital conversion algorithms.","A high quality (HQ) scale block  or scale block  may perform stretching or shrinking of the video and\/or graphics content depending on the input frame size and destination size. The scale blocks  and  may perform an process for converting video signals between one resolution\/aspect-ratio to another resolution\/aspect-ratio. For example, the content may be \u201cupscaled\u201d or \u201cupconverted,\u201d or converted from a lower resolution (Standard Definition) to a higher resolution (High Definition) by the HQ scale block . In some implementations, the scale blocks  and  provide linear scaling and non-linear scaling. The non-linear scaling may use parameters such as the input frame size, output size, input pixel aspect ratio and output pixel aspect ratio.","A clear rectangle block  provides rectangles in which the correspondent pixels of rectangles in the input stream are treated as transparent. A clear rectangle may be a region defined by a height and width.","A mix block  may perform alpha blending of the input streams on top of a background color. Alpha blending may be performed within the mix block, and is the process of combining a translucent foreground color with a background color to produce a new blended color. The degree of the foreground color's translucency may range from completely transparent to completely opaque. If the foreground color is completely transparent, the blended color will be the background color. Conversely, if the foreground color is completely opaque, the blended color will be the foreground color. The translucency may range within the range of transparent to opaque, in which case the blended color is computed as a weighted average of the foreground and background colors.","In some implementations, each input stream may have a per-plane alpha, a per-pixel alpha, and a per-palette index alpha. If an output surface has alpha channel data (e.g., ARGB32), the video composition model  provides an interface for the video application to specify how the alpha channel data to be filled out on the target rectangle. An alpha channel is an additional eight bits used with each pixel in a 32-bit graphics system that may represent 256 levels of translucency. Black and white represent opaque and fully transparent, while various gray levels represent levels of translucency.","A constriction block  performs down-sampling on a composed frame in order to restrict the image resolution. The constriction block  may be used to implement content protection schemes where, e.g., high-definition content is scaled to standard-definition if an output device does not provide authorized protection mechanisms.","A scale block  scales the output to a resolution appropriate for an output device, such as the display device . The scale block  provides an output video stream , which may be formatted in accordance with one of the input streams , and , or in accordance with a device, such as the display device .","API Function","To provide access to the above features of the video composition model , APIs may be provided to enable the video application and\/or device driver  to provide information and data to the video composition model . Below is a non-limiting description of the public interfaces exposed by the video composition model .",{"@attributes":{"id":"p-0047","num":"0046"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"126pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"70pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"HRESULT DXVAHD_CreateDevice(",{}]},{"entry":[{},"\u2003IDirect3Ddevice9Ex*","pD3Ddevice,"]},{"entry":[{},"\u2003const DXVAHD_CONTENT_DESC*","pContentDesc,"]},{"entry":[{},"\u2003DXVAHD_DEVICE_USAGE","Usage,"]},{"entry":[{},"\u2003PDXVAHDSW_Plugin","pPlugin,"]},{"entry":[{},"\u2003IDXVAHD_Device**","ppDevice"]},{"entry":[{},");"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"API Interfaces",{"@attributes":{"id":"p-0049","num":"0048"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"HRESULT IDXVAHD_Device::CreateVideoSurface("}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003UINT","Width,"]},{"entry":["\u2003UINT","Height,"]},{"entry":["\u2003D3DFORMAT","Format,"]},{"entry":["\u2003D3DPOOL","Pool,"]},{"entry":["\u2003DWORD","Usage,"]},{"entry":["\u2003DXVAHD_SURFACE_TYPE","Type,"]},{"entry":["\u2003UINT","NumSurfaces,"]},{"entry":["\u2003IDirect3Dsurface9**","ppSurfaces,"]},{"entry":["\u2003HANDLE*","pSharedHandle"]},{"entry":");"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"HRESULT IDXVAHD_Device::GetVideoProcessorDeviceCaps("}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003DXVAHD_VPDEVCAPS*","pCaps"]},{"entry":");"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"HRESULT IDXVAHD_Device::GetVideoProcessorOutputFormats("}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003UINT","Count,"]},{"entry":["\u2003D3DFORMAT*","pFormats"]},{"entry":");"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"HRESULT IDXVAHD_Device::GetVideoProcessorInputFormats("}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003UINT","Count,"]},{"entry":["\u2003D3DFORMAT*","pFormats"]},{"entry":");"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"HRESULT IDXVAHD_Device::GetVideoProcessorCaps("}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003UINT","Count,"]},{"entry":["\u2003DXVAHD_VPCAPS*","pCaps"]},{"entry":");"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"HRESULT IDXVAHD_Device::GetVideoProcessorCustomRates("}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003const GUID*","pVPGuid,"]},{"entry":["\u2003UINT","Count,"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\u2003DXVAHD_CUSTOM_RATE_DATA* pRates"},{"entry":");"},{"entry":"HRESULT IDXVAHD_Device::GetVideoProcessorFilterRange("}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["\u2003DXVAHD_FILTER","Filter,"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\u2003DXVAHD_FILTER_RANGE_DATA* pRange"},{"entry":");"},{"entry":"HRESULT IDXVAHD_Device::CreateVideoProcessor("}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003const GUID*","pVPGuid,"]},{"entry":["\u2003IDXVAHD_VideoProcessor","ppVideoProcessor"]},{"entry":");"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"HRESULT IDXVAHD_VideoProcessor::SetVideoProcessBltState("},{"entry":"\u2003DXVAHD_BLT_STATE State,"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003UINT","DataSize,"]},{"entry":["\u2003const void*","pData"]},{"entry":");"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"HRESULT IDXVAHD_VideoProcessor::GetVideoProcessBltState("}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003DXVAHD_BLT_STATE","State,"]},{"entry":["\u2003UINT","DataSize,"]},{"entry":["\u2003void*","pData"]},{"entry":");"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"HRESULT IDXVAHD_VideoProcessor::SetVideoProcessStreamState("}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003UINT","StreamNumber,"]},{"entry":["\u2003DXVAHD_STREAM_STATE","State,"]},{"entry":["\u2003UINT","DataSize,"]},{"entry":["\u2003const void*","pData"]},{"entry":");"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"HRESULT IDXVAHD_VideoProcessor::GetVideoProcessStreamState("}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003UINT","StreamNumber,"]},{"entry":["\u2003DXVAHD_STREAM_STATE","State,"]},{"entry":["\u2003UNIT","DataSize,"]},{"entry":["\u2003void*","pData"]},{"entry":");"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"HRESULT IDXVAHD_VideoProcessor::VideoProcessBltHD("}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\u2003const IDirect3Dsurface9*","pOutputSurface,"]},{"entry":["\u2003UINT","OutputFrame,"]},{"entry":["\u2003UINT","StreamCount,"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\u2003const DXVAHD_STREAM_DATA* pStreams"},{"entry":");"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"Thus, implementations of the video composition model , as described above, provide for a flexible mechanism to process a wide range of input video and\/or graphic streams, each possible having differing characteristics in a computing device.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":["FIG. 3","FIG. 2","FIG. 3","FIG. 2"],"b":["301","301","301"],"i":["c ","d ","e "]},{"@attributes":{"id":"p-0052","num":"0051"},"figref":["FIG. 4","FIG. 2","FIG. 4","FIG. 2"],"b":["401","401","400","401","401","401","401","401","202","217","104","106","401","217"],"i":["a ","b ","b ","c","d","e","f ","g "]},"In some implementations of the video composition model , a lowest Z-ordered input stream may be used as the background image input . In some implementations, YCbCr background colors may be specified in addition to an RGB background color with the color space specified on the output using an interface provided by the model.","Table 1 below provides exemplary specifications of the inputs and outputs to the video composition model  for Blu-ray applications.",{"@attributes":{"id":"p-0055","num":"0054"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"175pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}},{"entry":["Plane","Usage"]},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Background","720 \u00d7 480~1920 \u00d7 1080, RGB single color or single image"]},{"entry":[{},"(JPEG or PNG)"]},{"entry":["Main Video","720 \u00d7 480~1920 \u00d7 1080, YCbCr 12-bit\/pixel, progressive"]},{"entry":[{},"or interlaced"]},{"entry":["Closed","720 \u00d7 480~1920 \u00d7 1080, YChCr 8-bit palettized with 8-bit"]},{"entry":["Caption","alpha"]},{"entry":["Sub Video","720 \u00d7 480~1920 \u00d7 1080, YCbCr 12-bit\/pixel, progressive"]},{"entry":[{},"or interlaced, luma keying"]},{"entry":["Presentation","720 \u00d7 480~1920 \u00d7 1080, YCbCr 8-bit palettized (24-bit"]},{"entry":["Graphics","color + 8-bit alpha)"]},{"entry":["Jave","720 \u00d7 480~1920 \u00d7 1080, RGB 32-bit\/pixel"]},{"entry":"Graphics"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}}}},{"@attributes":{"id":"p-0056","num":"0055"},"figref":["FIG. 5","FIG. 2"],"b":["502","200","101","504","202","218"]},"At , per-plane alpha, per-pixel alpha and per-pallete index alpha may be applied to each stream. The video application may specify how the alpha channel data is to be filled out on the target rectangle for application to the output video. At , mixing\/alpha-blending of the input streams may be performed. The mix block  may performed per-plane and per-pixel alpha blending of the input streams on top of a background color. At , it is determined if constriction is to be applied to the output video. The video application may call for a level of protection to be applied to the output video if the graphics hardware does not provide an appropriated level of content protection. If constriction is to be applied, then at , a video quality of the output video may be reduced. If constriction is not to be applied, then the video driver  may be signaled to prepare to output the output video at . Likewise, after constriction at , the video driver  may be signaled at . At , the output video is made available. For example, the output video may be communicated to the display device .","Exemplary Computing Arrangement",{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 6"},"Numerous other general purpose or special purpose computing system environments or configurations may be used. Examples of well known computing systems, environments, and\/or configurations that may be suitable for use include, but are not limited to, personal computers, server computers, handheld or laptop devices, multiprocessor systems, microprocessor-based systems, network personal computers (PCs), minicomputers, mainframe computers, embedded systems, distributed computing environments that include any of the above systems or devices, and the like.","Computer-executable instructions, such as program modules, being executed by a computer may be used. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. Distributed computing environments may be used where tasks are performed by remote processing devices that are linked through a communications network or other data transmission medium. In a distributed computing environment, program modules and other data may be located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing aspects described herein includes a computing device, such as computing device . In its most basic configuration, computing device  typically includes at least one processing unit  and memory . Depending on the exact configuration and type of computing device, memory  may be volatile (such as random access memory (RAM)), non-volatile (such as read-only memory (ROM), flash memory, etc.), or some combination of the two. This most basic configuration is illustrated in  by dashed line .","Computing device  may have additional features\/functionality. For example, computing device  may include additional storage (removable and\/or non-removable) including, but not limited to, magnetic or optical disks or tape. Such additional storage is illustrated in  by removable storage  and non-removable storage .","Computing device  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by device  and includes both volatile and non-volatile media, removable and non-removable media.","Computer storage media include volatile and non-volatile, and removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Memory , removable storage , and non-removable storage  are all examples of computer storage media. Computer storage media include, but are not limited to, RAM, ROM, electrically erasable program read-only memory (EEPROM), flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computing device . Any such computer storage media may be part of computing device .","Computing device  may contain communications connection(s)  that allow the device to communicate with other devices. Computing device  may also have input device(s)  such as a keyboard, mouse, pen, voice input device, touch input device, etc. Output device(s)  such as a display, speakers, printer, etc, may also be included. All these devices are well known in the art and need not be discussed at length here.","It should be understood that the various techniques described herein may be implemented in connection with hardware or software or, where appropriate, with a combination of both. Thus, the methods and apparatus of the presently disclosed subject matter, or certain aspects or portions thereof, may take the form of program code (i.e., instructions) embodied in tangible media, such as floppy diskettes, CD-ROMs, hard drives, or any other machine-readable storage medium where, when the program code is loaded into and executed by a machine, such as a computer, the machine becomes an apparatus for practicing the presently disclosed subject matter.","Although exemplary implementations may refer to utilizing aspects of the presently disclosed subject matter in the context of one or more stand-alone computer systems, the subject matter is not so limited, but rather may be implemented in connection with any computing environment, such as a network or distributed computing environment. Still further, aspects of the presently disclosed subject matter may be implemented in or across a plurality of processing chips or devices, and storage may similarly be affected across a plurality of devices. Such devices might include personal computers, network servers, and handheld devices, for example.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":["FIG. 3","FIG. 2"]},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 4","FIG. 2"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
