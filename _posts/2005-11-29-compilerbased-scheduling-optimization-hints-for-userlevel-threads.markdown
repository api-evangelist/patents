---
title: Compiler-based scheduling optimization hints for user-level threads
abstract: Method, apparatus and system embodiments to schedule user-level OS-independent “shreds” without intervention of an operating system. For at least one embodiment, the shred is scheduled for execution by a scheduler routine rather than the operating system. The scheduler routine may receive compiler-generated hints from a compiler. The compiler hints may be generated by the compiler without user-provided pragmas, and may be passed to the scheduler routine via an API-like interface. The interface may include a scheduling hint data structure that is maintained by the compiler. Other embodiments are also described and claimed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08205200&OS=08205200&RS=08205200
owner: Intel Corporation
number: 08205200
owner_city: Santa Clara
owner_country: US
publication_date: 20051129
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","DETAILED DESCRIPTION"],"p":["1. Technical Field","The present disclosure relates generally to information processing systems and, more specifically, to improved efficiency for scheduling of user-level threads that are not scheduled by an operating system.","2. Background Art","An approach that has been employed to improve processor performance is known as \u201cmultithreading.\u201d In software multithreading, an instruction stream may be divided into multiple instruction streams that can be executed in parallel. Alternatively, multiple independent software streams may be executed in parallel.","In one approach, known as time-slice multithreading or time-multiplex (\u201cTMUX\u201d) multithreading, a single processor switches between threads after a fixed period of time. In still another approach, a single processor switches between threads upon occurrence of a trigger event, such as a long latency cache miss. In this latter approach, known as switch-on-event multithreading (\u201cSoEMT\u201d), only one thread, at most, is active at a given time.","Increasingly, multithreading is supported in hardware. For instance, in one approach, processors in a multi-processor system, such as a chip multiprocessor (\u201cCMP\u201d) system, may each act on one of the multiple software threads concurrently. In another approach, referred to as simultaneous multithreading (\u201cSMT\u201d), a single physical processor is made to appear as multiple logical processors to operating systems and user programs. For SMT, multiple software threads can be active and execute simultaneously on the single physical processor without switching. That is, each logical processor maintains a complete set of the architecture state, but many other resources of the physical processor, such as caches, execution units, branch predictors, control logic and buses are shared. For SMT, the instructions from multiple software threads, each on a distinct logical processor, execute concurrently.","For a system that supports concurrent execution of software threads, such as SMT and\/or CMP systems, an operating system application may control scheduling and execution of the software threads. Typically, however, operating system control does not scale well; the ability of an operating system application to schedule threads without negatively impacting performance is commonly limited to a relatively small number of threads. Accordingly, a system may be implemented such that user-level threads are scheduled by a program in the user space rather than being scheduled by an operating system. One such system is discussed in co-pending application U.S. Ser. No. 11\/235,865, filed Sep. 26, 2005.","The following discussion describes selected embodiments of methods, systems and articles of manufacture to improve efficiency of scheduling for multiple concurrently-executed user-level threads of execution (sometimes referred to herein as \u201cshreds\u201d) that are not created or scheduled by the operating system. The shreds are instead scheduled by a scheduler routine that can dynamically adapt shred scheduling based on information provided, at least in part, by a compiler. The compiler-provided information is in the nature of a hint, which may be disregarded by the scheduler without impacting program correctness. Such information is generated by the compiler independently, without user directives or other pragmatic information.","The shreds may be scheduled to run on one or more OS-sequestered sequencers. The OS-sequestered sequencers are sometimes referred to herein as \u201cOS-invisible\u201d; the operating system does not schedule work on such sequencers. The mechanisms described herein may be utilized with single-core or multi-core multithreading systems. In the following description, numerous specific details such as processor types, multithreading environments, system configurations, and numbers, type and topology of sequencers in a multi-sequencer system have been set forth to provide a more thorough understanding of the present invention. It will be appreciated, however, by one skilled in the art that the invention may be practiced without such specific details. Additionally, some well known structures, circuits, and the like have not been shown in detail to avoid unnecessarily obscuring the present invention.","A shared-memory multiprocessing paradigm may be used in an approach referred to as parallel programming. According to this approach, an application programmer may split a software program, sometimes referred to as an \u201capplication\u201d or \u201cprocess,\u201d into multiple tasks to be run concurrently in order to express parallelism for a software program. All threads of the same software program (\u201cprocess\u201d) share a common logical view of memory.",{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 1","FIG. 1"],"b":["100","103","120","140","100","103","120"]},"The operating system (\u201cOS\u201d)  is commonly responsible for managing the user-defined tasks for a process (e.g., processes  and ). While each process has at least one task (see, e.g., process  and process , bearing reference numerals  and , respectively), others may have more than one (e.g., Process , bearing reference numeral ). The number of processes illustrated in , as well as the number of user-defined tasks for each process, should not be taken to be limiting. Such illustration is for explanatory purposes only.",{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 1","FIG. 1"],"b":["125","126","120","140","140","125","126","127","103","140","124","0"]},"The OS  is commonly responsible for scheduling these threads , ,  for execution on the execution resources. The threads associated with the same process typically have the same virtual memory address space.","Because the OS  is responsible for creating, mapping, and scheduling threads, the threads , ,  are \u201cvisible\u201d to the OS . In addition, embodiments of the present invention comprehend additional user-level threads - that are not visible to the OS . That is, the OS  does not create, manage, or otherwise acknowledge or control these additional user-level threads -. These additional threads, which are neither created nor controlled by the OS , and may be scheduled to execute concurrently with each other, are sometimes referred to herein as \u201cshreds\u201d - in order to distinguish them from OS-visible threads and to further distinguish them from PTHREADS or other user-level threads that may not be executed concurrently with each other for the same OS-visible thread. The shreds are created and managed by user-level programs (referred to as \u201cshredded programs\u201d) and may be scheduled to run on sequencers that are sequestered from the operating system. The OS-sequestered sequencers typically share a common set of ring 0 states as OS-visible sequencers. These shared ring-0 architectural states are typically those responsible for supporting a common shared memory address space between the OS-visible sequencer and OS-sequestered sequencers. For example, for an embodiment based on IA-32 architecture, CR0, CR2, CR3, CR4 are some of these shared ring-0 architectural states. Shreds thus share the same execution environment (virtual address map) that is created for the threads associated with the same process.","As used herein, the terms \u201cthread\u201d and \u201cshred\u201d include, at least, the concept of a set of instructions to be executed concurrently with other threads and\/or shreds of a process. The thread and \u201cshred\u201d terms both encompass the idea, therefore, of a set of software primitives or application programming interfaces (API). As used herein, a distinguishing factor between a thread (which is OS-controlled) and a shred (which is not visible to the operating system and is instead user-controlled), which are both instruction streams, lies in the difference of how scheduling and execution of the respective thread and shred instruction streams are managed. A thread is generated in response to a system call to the OS. The OS generates that thread and allocates resources to run the thread. Such resources allocated for a thread may include data structures that the operating system uses to control and schedule the threads.","In contrast, at least one embodiment of a shred is generated via a user level software \u201cprimitive\u201d that invokes an OS-independent mechanism for generating a shred that the OS is not aware of. A shred may thus be generated in response to a user-level software call. For at least one embodiment, the user-level software primitives may involve user-level (ring-3) instructions that can create a user-level shred in hardware or firmware. The user-level shred thus created may be scheduled by hardware and\/or firmware and\/or user-level software. The OS-independent mechanism may be software code that sits in user space, such as a software library. The techniques for shred scheduling optimizations discussed herein may be used with any user-level thread package.",{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 2","FIG. 1","FIG. 2","FIG. 1"],"b":["100","103","120"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 2","FIG. 1","FIG. 1","FIG. 1","FIG. 1"],"b":["120","124","125","126","130","136","0","100","124"]},"However, other processes ,  may be associated with one or more OS-scheduled threads as illustrated in . Dotted lines and ellipses are used in  to represent optional additional shreds.  illustrates one process  associated with one OS-scheduled thread  and also illustrates another process  associated with two or more threads -. In addition, each process ,  may additionally be associated with one or more shreds -, -, respectively. The representation of two threads ,  and four shreds - for Process   and of one thread  and two shreds ,  for Process   is illustrative only and should not be taken to be limiting. The number of OS-visible threads associated with a process may be limited by the OS program. However, the upper bound for the cumulative number of shreds associated with a process is limited, for at least one embodiment, only by the amount of algorithmic thread level parallelism and the number of shred execution resources (e.g. number of sequencers) available at a particular time during execution.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 2","b":["126","120","125","125","126"]},{"@attributes":{"id":"p-0033","num":"0032"},"figref":["FIG. 2","FIG. 2","FIG. 2"],"b":["200","125","126","120","125","126","202","202","202","125","126","130","136"],"i":["a","b"]},"Accordingly,  illustrates that a system for at least one embodiment of the present invention may support a 1-to-many relationship between an OS-visible thread, such as thread , and the shreds - (which are not visible to the OS) associated with the thread. The shreds are not \u201cvisible\u201d to the OS (see , ) in the sense that a programmer, not the OS, may employ user-level techniques to create, synchronize and otherwise manage and control operation of the shreds. While the OS  is aware of, and manages, one or more threads, the OS  is not aware of, and does not manage or control, shreds.","Thus, instead of relying on the operating system to manage the mapping between thread unit hardware and shreds, scheduler logic in user space may manage the mapping. For at least one embodiment, the scheduler logic may be in a runtime software library.","For at least one embodiment a user may directly control such mapping by utilizing shred control instructions or primitives that are handled by the scheduler or other logic in software, such as in a runtime library. In addition, the user may directly manipulate control and state transfers associated with shred execution. Accordingly, for embodiments of the methods, mechanisms, articles of manufacture, and systems described herein, a user-visible feature of the architecture of the thread units is at least a canonical set of instructions that allow a user direct manipulation and control of thread unit hardware.","As used herein, a thread unit, also interchangeably referred to herein as a \u201csequencer\u201d, may be any physical or logical unit capable of executing a thread or shred. It may include next instruction pointer logic to determine the next instruction to be executed for the given thread or shred. For example, the OS thread  illustrated in  may execute on a sequencer, not shown, as \u201cThread A\u201d  in , while each of the active shreds - may execute on other sequencers, \u201cseq 1\u201d-\u201cseq 4\u201d, respectively. A sequencer may be a logical thread unit or a physical thread unit. Such distinction between logical and physical thread units is illustrated in .",{"@attributes":{"id":"p-0038","num":"0037"},"figref":["FIG. 3","FIG. 3","FIG. 3"],"b":["310","350","310","350"]},"In the single-core multithreading environment , a single physical processor  is made to appear as multiple logical processors (not shown), referred to herein as LPthrough LP, to operating systems and user programs. Each logical processor LPthrough LPmaintains a complete set of the architecture state AS-AS, respectively. The architecture state includes, for at least one embodiment, data registers, segment registers, control registers, debug registers, and most of the model specific registers. The logical processors LP-LPshare most other resources of the physical processor , such as caches, execution units, branch predictors, control logic and buses. Although such features may be shared, each thread context in the multithreading environment  can independently generate the next instruction address (and perform, for instance, a fetch from an instruction cache, an execution instruction cache, or trace cache). Thus, the processor  includes logically independent next-instruction-pointer and fetch logic  to fetch instructions for each thread context, even though the multiple logical sequencers may be implemented in a single physical fetch\/decode unit . For a single-core multithreading embodiment, the term \u201csequencer\u201d encompasses at least the next-instruction-pointer and fetch logic  for a thread context, along with at least some of the associated architecture state, , for that thread context. It should be noted that the sequencers of a single-core multithreading system  need not be symmetric. For example, two single-core multithreading sequencers for the same physical core may differ in the amount of architectural state information that they each maintain.","A single-core multithreading system can implement any of various multithreading schemes, including simultaneous multithreading (SMT), switch-on-event multithreading (SoeMT) and\/or time multiplexing multithreading (TMUX). When instructions from more than one hardware thread contexts (or logical processor) run in the processor concurrently at any particular point in time, it is referred to as SMT. Otherwise, a single-core multithreading system may implement SoeMT, where the processor pipeline is multiplexed between multiple hardware thread contexts, but at any given time, only instructions from one hardware thread context may execute in the pipeline. For SoeMT, if the thread switch event is time based, then it is TMUX.","Thus, for at least one embodiment, the multi-sequencer system  is a single-core processor  that supports concurrent multithreading. For such embodiment, each sequencer is a logical processor having its own instruction next-instruction-pointer and fetch logic and its own architectural state information, although the same physical processor core  executes all thread instructions. For such embodiment, the logical processor maintains its own version of the architecture state, although execution resources of the single processor core may be shared among concurrently-executing threads.",{"@attributes":{"id":"p-0042","num":"0041"},"figref":["FIG. 3","FIG. 3"],"b":["350","350","304","304","304","304","322","304","304","322","320","304","304","322","320","320","350"],"i":["a","n ","a ","n ","a","n ","a","n "]},"For at least one embodiment of the multi-core system  illustrated in , each of the sequencers may be a processor core , with the multiple cores -residing in a single chip package . Each core -may be either a single-threaded or multi-threaded processor core. The chip package  is denoted with a broken line in  to indicate that the illustrated single-chip embodiment of a multi-core system  is illustrative only. For other embodiments, processor cores of a multi-core system may reside on separate chips. That is, the multi-core system may be a multi-socket symmetric multiprocessing system.","For ease of discussion, the following discussion focuses on embodiments of the multi-core system . However, this focus should not be taken to be limiting, in that the mechanisms described below may be performed in either a multi-core or single-core multi-sequencer environment.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 4","FIG. 4"],"b":["400","400","450","450","450","403","404","450"]},"An operating system (\u201cOS\u201d) (see, e.g.,  of ) may operate independently from the scheduler routine  to schedule OS-managed threads. In contrast, the scheduler routine , rather than a scheduling mechanism provided by the OS, schedules the user-level shreds. Each shred is therefore scheduled by the scheduler routine  for execution, independent of OS scheduling logic. Each shred may be scheduled to execute on either an OS-sequestered or OS-visible sequencer.",{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 4","b":["402","440"]},"The compiler  may, when it encounters one of these shred control primitives in the shredded application , generate instead a primitive extension that is placed into the instrumented code  that is produced by the compiler . That is, the API-like primitives defined for interface  may, for at least one embodiment of the present invention, include one or more extensions for passing scheduling hints from the compiler to the scheduler (e.g., shred_create_attr, discussed below in connection with Table 2). The compiler  may insert such primitive extensions into the instrumented code  for each minimal unit of execution (\u201cMUE\u201d) as is described below in the section entitled \u201cGeneration of Hints by the Compiler.\u201d","In addition, the compiler  may also generate in the instrumented code  one or more instructions to update the hint values (see, e.g., values of the attribute table shown in Table 2, below).","The scheduler routine  may receive scheduling hints from instrumented code  that has been generated by a compiler  in order to provide hints to the scheduler routine . The compiler  may generate initial values for the hints based on static analysis or profiling of a shredded user program . (As is mentioned above, the hint values may be updated during runtime in response to instructions placed by the compiler  into the instrumented code .)","As used herein, a \u201cshredded\u201d program is a user-level program that includes one or more shred creation\/control primitives or instructions. The hints are generated independently by the compiler , without user input such as pragmatic information. The hints may be provided from the instrumented code  to the scheduler  via an interface .","The system  illustrated in  may thus receive compiler-generated hints that may be passed to the scheduler  and may be used by the scheduler  to more judiciously perform dynamic shred scheduling in order to improve thread-level parallelism for a shredded program. The compiler  is capable of independently generating the hints, and the scheduler  can utilize the hints to perform more efficient shred scheduling.","One of skill in the art will recognize that there may be one or more levels of abstraction between the programmer's code  (e.g., code that includes an API-like shred creation primitive) and actual architectural instructions that cause a sequencer to execute a shred.","As used herein, an instruction or primitive described as being generated by a programmer or user is intended to encompass not only architectural instructions that may be generated by an assembler or compiler based on user-generated code, or by a programmer working in an assembly language, but also any high-level primitive or instruction that may ultimately be assembled or compiled into architectural shred control instructions. It should also be understood that an architectural shred control instruction may be further decoded into one or more micro-operations.","During analysis of the user application , the compiler  may identify information (hints) that could be beneficial to the scheduler  as the scheduler attempts to dynamically optimize shred scheduling during run-time. At compile-time, the compiler  has access to more semantic information about the program  than the scheduler  is exposed to during run-time of the user program . Based on the threaded algorithm that the application developer employs in the user program , the compiler  may statically capture and highlight (via passing of hints) potential areas where a run-time scheduler can act to dynamically schedule shreds in a manner that enhances performance or reduces power consumption. The compiler  can thus statically generate hints that the run-time scheduler  can use during dynamic scheduling. Because they are \u201chints\u201d that do not affect program correctness, the scheduler  is also free to disregard the hints.","Regarding generation of the hints, a compiler  may, before the application  is executed, perform offline dependence analysis to determine which units of execution in a shred occur often and may be performed as an independent unit of execution. In this manner, the compiler  is able to determine which portions of shred can be performed independently, so that each independent portion of work could be allocated to a different physical sequencer (if available at runtime), in order to increase thread-level parallelism of the program .","The scheduler  may also take into account runtime feedback as well as the compiler hints that were generated before runtime. Some of the run-time characteristics of the system  that the scheduler  may take into account, in addition to (or instead of) the compiler hints, may include, without limitation, sequencer utilization and availability, cache configuration, how many shreds have currently been scheduled, and the like.","It should be noted that the sequencers ,  illustrated in  need not be symmetric, and the number of sequencers illustrated in  should not be taken to be limiting. Regarding the number of sequencers, the scheduling mechanism  may be utilized for any number of sequencers. The illustration of only two sequencers in  is for illustrative purposes only. One of skill in the art will recognize that a system may include more than two sequencers, which may be all of a single sequencer type (symmetric) or may each be one of multiple sequencer types (asymmetric). For example, and without limitation, the scheduling mechanism may be implemented for a multi-sequencer system that includes four, eight, sixteen, thirty-two or more symmetric and\/or asymmetric sequencers.","Regarding symmetry,  illustrates scheduling logic  for a system  that may include at least two types of sequencers\u2014Type A sequencers  and Type B sequencers . Each sequencer ,  may include or run a portion of a distributed scheduler routine . The portions may be identical copies of each other, but need not necessarily be so.","The sequencers ,  may be asymmetric, in that they may differ in any manner, including those aspects that affect quality of computation. The sequencers may differ in terms of power consumption, speed of computational performance, functional features, or the like. By way of example, for one embodiment, the sequencers ,  may differ in terms of functionality. For example, one sequencer may be capable of executing integer and floating point instructions, but cannot execute a single instruction multiple data (\u201cSIMD\u201d) set of instruction extensions, such as Streaming SIMD Extensions 3 (\u201cSSE3\u201d). On the other hand, another sequencer may be capable of performing all the instructions that the first sequencer can execute, and can also execute SSE3 instructions.","As another example of functional asymmetry, one sequencer  may be visible to the OS (see, for example,  of ) and may therefore be capable of performing supervisor mode (e.g., \u201cring 0\u201d for IA-32) operations such as performing system calls, servicing a page fault, and the like. On the other hand, another sequencer  may be sequestered from the OS, and therefore be capable of only user-level (e.g., \u201cring-3\u201d for IA-32) operations and incapable of performing ring 0 operations.","The sequencers of a system on which the scheduling mechanism  is utilized may also differ in any other manner, such as footprint, word width and\/or data path size, topology, memory, power consumption, number of functional units, communication architectures (multi-drop vs. point-to-point interconnect), or any other metric related to functionality, performance, footprint, or the like.","For at least one embodiment, the functionality of type A  and type B  sequencers may be mutually exclusive. That is, for example, one type of sequencer  may support a particular functionality, such as execution of SSE3 instructions, that the other type of sequencer  does not support; while the second type of sequencer  may support a particular functionality, such as ring 0 operations, that the first type of sequencer  does not support.","However, for at least one other embodiment, the functionality of sequencer types A  and B  represent a superset-subset functionality relationship rather than a mutually exclusive functionality relationship. That is, a first set of sequencers (such as type A sequencers ) provide a superset of functionality that includes all functionality of a second set of sequencers (such as type B sequencers ), plus additional functionality that is not provided by the second set of sequencers .","Generally speaking, the system illustrated in  utilizes a hybrid approach for dynamic shred scheduling in order to take advantage of the particular respective strengths of the compiler  and the scheduler . The compiler  has full knowledge of program semantics and is therefore well-suited to perform functional decomposition to uncover for a shred the minimal units of thread execution (\u201cMUE\u201d) that may be performed independently in order to increase thread-level parallelism. Decomposition may involve global dependence analysis and is therefore more easily performed by the compiler than the scheduler. Accordingly, the compiler  may be able to provide more robust scheduling hints than those that could be gleaned by the scheduler  during run-time; the compiler  passes these hints to the scheduler  through the interface .","In contrast, the scheduler  is more suited to using the information regarding MUE, which was gleaned by the compiler, to adaptively perform migration and aggregation of MUE's. The scheduler  has full knowledge of the number of processors of the system, the cache configuration of the system, the interconnect topology of the system, and potential imbalances in resource distribution and functional asymmetry among sequencers. Therefore, the scheduler  is well-suited to adaptively aggregate the MUE's and\/or align MUE's with available resources at run-time for a given target multi-sequencer system.","In other words, fission (breaking computations of the shreds in a user application  into independent units of work and generating the associated hints) is more easily performed by the compiler, while aggregation (that is, aligning MUE's with sequencers in a resource-efficient manner) is better performed by the dynamic shred scheduler  at run-time.","Accordingly,  illustrates a hybrid approach that includes both static and dynamic components in order to adaptively deliver the best performance for various different runtime platforms. Static analysis or off-line profiling is performed by the compiler  to generate application-specific compiler hints, thereby relieving the run-time scheduler from performing such dependence analysis at run-time. Dynamic utilization of the hints by the scheduler  during run-time allows the scheduling to be performed in a manner that efficiently utilizes the run-time resources of the particular system.","Interface for Passing Hints from the Compiler to the Scheduler.","As an initial matter, this section discusses at least one embodiment of the interface  for passing shred scheduling hints from the compiler  to the scheduler . In the following sections, further detail is provided regarding how the compiler  may statically generate (either through static analysis or off-line profiling) the hints and how the scheduler  may utilize the hints during dynamic run-time scheduling of shreds.","Regarding the interface , it may be implemented as an API (\u201cApplication Programmer Interface\u201d) type of interface between the compiler  and the scheduling logic . The API that provides the interface  may include an attribute data structure. Such data structure, referred to herein as an attribute table (ATTR), may be maintained by the compiler  and passed to the scheduler . On creation of a shred, the compiler  is thus responsible for setting up the attribute data structure for the shred and for passing this information to the scheduler logic .","The compiler  may maintain and manage a separate attribute table for each shred in the compiled application program . The interface  includes primitives that explicitly provide for passing of information in the attribute table for a shred from the compiler  to the scheduler . For at least one embodiment, these primitives are extensions to existing shred creation and control primitives. (See discussion of shred_create_attr, below).","The attribute table may include an entry for each type of hint such that it includes all of the optimization hints, for a particular shred, that can exist between the compiler and the scheduler. The data structure is thus responsible for expressing and carrying, for a particular shred, all of the possible optimization hints defined on the interface . Although certain types of hints are described herein, it should be understood that the nature of the attribute table makes it particularly amendable to inclusion of additional, or different, types of hints than those described herein.","A data structure that holds optimization hints allows future amendments to the data structure to be implemented with relative ease so that additional, or different, hints may be added to the data structure. The attribute table may therefore be modified as needed to meet design considerations.","The information in the table, whatever hints it includes, may be passed from the compiler  to the scheduler  via an API primitive. For at least one embodiment, such primitive may be an extension of other shred creation instructions or primitives, the extension indicating that the attribute table is to be passed as a parameter. For example, a \u201cshred_create\u201d primitive may be extended to include the attribute table. An example of such at least one embodiment of such an extension, \u201cshred_create_attr\u201d, may include parameters as shown below in Table 2 (discussed in further detail below).","The attribute table, as indicated above, may contain an entry for each type of hint that may be passed from the compiler to the scheduler. For at least one embodiment, the types of hints included in the attribute table are set forth in Table 1, below.",{"@attributes":{"id":"p-0077","num":"0076"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"133pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"Hint Type","Description"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Imbalance","Indicates the degree of"]},{"entry":[{},{},"computation associated with the"]},{"entry":[{},{},"MUE. In effect, indicates the"]},{"entry":[{},{},"amount of work to be performed"]},{"entry":[{},{},"by the MUE, so that load"]},{"entry":[{},{},"balancing may be performed"]},{"entry":[{},"Fusion","Indicates whether an MUE is"]},{"entry":[{},{},"conformable with other MUE's,"]},{"entry":[{},{},"whether it can be aggregated with"]},{"entry":[{},{},"another MUE, and whether it has"]},{"entry":[{},{},"a dependence with a prior MUE"]},{"entry":[{},"Locality","Indicates degree of data-sharing"]},{"entry":[{},{},"with other shreds"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"One embodiment of the attribute data structure may be represented in pseudocode as set forth in Table 1A:",{"@attributes":{"id":"p-0079","num":"0078"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"140pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 1A"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Typedef struct {"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"int imbalance;"]},{"entry":[{},"int fusion;"]},{"entry":[{},"int locality;"]},{"entry":[{},"int hotspot;"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"140pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"} attr;"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"Upon creation of a shred, the compiler is responsible for setting up and maintaining the attribute data structure and for passing the attribute data to the scheduler. The compiler may do so as follows. If the compiler encounters a shred creation primitive in the application , the compiler generates an instance of the attribute table for that shred. The compiler populates the attribute table with any hints that are appropriate. The compiler may replace the shred creation instruction with a modified shred creation instruction (e.g., \u201cshred_create_attr\u201d, discussed below in connection with Table 2), which includes the attribute table for the shred as a parameter. In this manner, the compiler sets up and populates an instance of the attribute table for each shred.","Regarding how the attribute table information is passed to the scheduler, reference is made to Table 2. Table 2 illustrates that an API that includes shred creation and control instructions or primitives may be modified to provide for extensions that allow passing of the attribute table to the scheduler. In particular, Table 2 illustrates a modification to the API in order to support the new attribute data structure for a shred_create primitive. Table 2 illustrates a modification to pseudocode for the shred_create function that may be performed when a shred_create_attr primitive is executed. The function may be part of a software program in user space, such as a software library.",{"@attributes":{"id":"p-0082","num":"0081"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":[{"entry":[{},"TABLE 2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"2.0 API to support shred creation"]},{"entry":[{},"void shred_create("]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"shred_t* s,","\/\/ return the shred"]},{"entry":[{},"size_t stack_sz,","\/\/ stack size for shred"]},{"entry":[{},"shred_task_t funcptr,","\/\/ pointer to function"]},{"entry":[{},"void* arg);","\/\/ function arguments"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"2.1 API extension to support shred creation AND"]},{"entry":[{},"attribute data structure"]},{"entry":[{},"void shred_create_attr("]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"shred_t* s,","\/\/ return the shred"]},{"entry":[{},"size_t stack_sz,","\/\/ stack size for shred"]},{"entry":[{},"attr_t attr","\/\/ attribute for optimizations"]},{"entry":[{},"shred_task_t funcptr,","\/\/ pointer to function"]},{"entry":[{},"void* arg);","\/\/ function arguments"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}]}}},"Each of the hints passed through the interface  in the ATTR table may be generated statically by the compiler during offline analysis of the user program . As is illustrated in Table 1, above, these hints may include Imbalance\/Asymmetry, Locality, and Fusion hints. These types of hints provide information that the scheduler  may utilize to perform dynamic optimizations that migrate, co-locate, and\/or fuse shreds.","Generation of Hints by the Compiler.","Moving to a discussion of the generation of hints,  is consulted in conjunction with .  illustrates at least one embodiment of a method  that may be performed by the compiler  to generate scheduling hints to be passed to the scheduler  via the interface . The method  may be performed for any compilation unit, such as a program.  illustrates that the method begins at block  and proceeds to block .","At block , the compiler performs dependence analysis to determine which portions of the shreds in the program may be performed independently in order to increase parallelism of the program. The compiler  may perform this \u201ccomputation decomposition\u201d  statically (that is, it may be performed offline before the user program is executed at runtime). During this decomposition, the compiler  may identify one or more MUE's, which are basic units of work that may be scheduled to execute independently.","For the degenerate case, for example, if the entire program is serial, the MUE is the whole program; hence the workload is imbalanced. By breaking up the shreds of the user application into smaller independent units of work (MUE's), the compiler may enable the scheduler to address workload imbalance in applications that include shreds. If the compiler decomposes  the shreds into MUE's aggressively, the scheduler then has larger freedom to adaptively perform run-time workload balancing and increase parallelism.","The decomposition  performed by the compiler  to identify the MUE's of the user program  should also satisfy data dependence constraints. For example, if a unit of work is processed independently in a loop iteration, the compiler may identify loop iteration as the minimal unit of thread execution, using standard data dependence analysis on the loop. In general, if the dependence analysis performed by the compiler  shows that there is no loop-carried dependence among the iterations of a loop, each loop iteration may be viewed as an MUE.","Accordingly, the compiler  may perform computation decomposition at block , in accordance with data dependence constraints, to aggressively identify as many MUE's in the user program  as possible. The more MUE's identified by the compiler  at block , the more freedom the scheduler  has to adaptively perform scheduling to improve performance.","In essence, an MUE identified by the compiler at block  is a virtual shred that may be independently mapped to, and executed on, a physical sequencer of the system based on run-time knowledge. For each MUE that it identifies, the compiler does the following: it inserts a shred creation primitive or instruction into the compiled code, and it generates an attribute structure (see, e.g., Table 1A, above) for each MUE.","Regarding insertion of the shred creation primitive or instruction, reference is made to Table 2, above. A modified shred creation instruction (Table 2.1) that passes the attribute structure as a parameter may be inserted by the compiler for each traditional shred creation instruction (Table 2.0) that the compiler encounters in the program. Thus, each shred as originally programmed is now associated with an attribute table.","However, through dependence analysis, decomposition, and\/or profiling, the compiler may be able to break up the original shreds into smaller independent units of work (MUE's). For each of these MUE's that are identified, the compiler inserts an additional modified shred creation instruction (Table 2.1) and generates an associated attribute structure for each of them.","One of the hints that the compiler may place into the attribute structure for an MUE is an Imbalance hint.  illustrates that the Imbalance hint is calculated by the compiler  at block . The Imbalance hint may be an integer value that represents the degree of computation associated with the MUE. This value indicates to the scheduler  how much \u201cwork\u201d is involved with the MUE, so that the scheduler  can balance the workload. From block , processing proceeds to block .","Allowing a compiler to statically group MUE's into threads, as some other known systems do, may lead to load imbalances at run-time. For example, the compiler  may be unaware of certain cache organization features of the particular run-time platform, and therefore be less able than the scheduler  to adaptively migrate an MUE from an overloaded sequencer to another available sequencer, based on run-time information about available system hardware resources.","Rather than having the compiler  group MUE's into threads, at least one embodiment of the system  allows the scheduler  to aggregate MUE's for execution if it makes sense from a performance-optimization point of view, given the scheduler's full knowledge of the run-time environment. Conversely, the scheduler  may migrate separate MUE's onto separate sequencers. Further discussion of how the scheduler utilizes compiler-generated hint information to perform such optimizations during run-time scheduling is set forth below in the following section.","Rather than, or in addition to, migrating MUE's among sequencers so that a workload is balanced, at least one embodiment of the scheduler  may co-locate MUE's that share data on the same, or nearby, sequencers. For example, shreds that share data may be scheduled on sequencers that are topologically adjacent to each other and\/or on sequencers that share a cache. This type of optimization, referred to herein as co-location, is a type of migration, but it takes into account relationships among MUE's rather than merely considering workload balance.",{"@attributes":{"id":"p-0097","num":"0096"},"figref":"FIG. 5","b":["504","508","402","402","508","442","450"]},{"@attributes":{"id":"p-0098","num":"0097"},"figref":"FIG. 5","b":["402","508","450","442","402","450","450"]},"The graph generated at block  is referred to herein as a \u201clocality graph\u201d, where each node of the graph is an MUE as determined via computation decomposition. The graph may then be subjected to certain optimizations, such as graph reduction. A weight associated with an edge of the locality graph represents the amount of locality between the two connecting nodes (MUE's) of the edge.","For at least one embodiment, pseudocode for logic to generate  a locality graph is set forth in Table 3. The logic of Table 3 may be performed by the compiler  at block . Generally, Table 3 illustrates that the edges of a locality graph may reflect the compiler's computation of spatial locality, temporal locality, near-neighbor (stencil) locality, and reduction locality among MUE's. These values, as well as other intermediate values that the compiler may utilize to generate hints on the interface , may be maintained by the compiler in one or more data structures. At least one embodiment of such data structures is set forth in Table 4. Table 3 illustrates that the generation of the locality graph may take into account one or more of the values maintained in the Table 4 structures (generated based on the compiler's program analysis) as well as the estimated cache line size:",{"@attributes":{"id":"p-0101","num":"0100"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":[{"entry":"TABLE 3"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"build_locality_graph(a_compilation_unit)"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"build_a_node_for_each_MUE( );"]},{"entry":[{},"for each IR (Namely, compiler's intermediate representation)"]},{"entry":[{},"being analyzed {"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"if (reduction found) {"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"add locality weight to the edges connecting the MUE's"]},{"entry":[{},"involved in the reduction"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"operation"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"} else if (stencil found) {"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"add locality weight between the near-neighbor MUE's,"]},{"entry":[{},"based on the stencil"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"pattern"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"} else {"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/* Only current stream element is visible to the kernel *\/"]},{"entry":[{},"if (streams to different kernels are not conformable) {"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"scale the streams to make it conformable"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"}"]},{"entry":[{},"add weight for temporal locality between MUE's accessing"]},{"entry":[{},"the same data"]},{"entry":[{},"add weight for spatial locality between MUE's accessing"]},{"entry":[{},"the same cache line"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"}"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"}"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0102","num":"0101"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":[{"entry":"TABLE 4"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"Locality_t {"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Int","Temporal;","\u2002\/\/degree of temporal locality"]},{"entry":[{},"Int","Spatial;","\/\/degree of spatial locality"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"}"},{"entry":"Fusion_t {"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Int","Conformability;","\u2003\/\/Is it conformable"]},{"entry":[{},"Int","Aggregation;","\u2002\/\/Is there something else to fuse it with?"]},{"entry":[{},"Int","Dependence;","\u2009\/\/Is there a dependence with another MUE"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"}"},{"entry":"Imbalance_t {"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"126pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Int","degree;","\/\/degree of computation"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"The pseudocode shown in Table 3, illustrates that, for at least one embodiment, reduction of the locality graph may be performed at block . A reduction operation indicates that data should be communicated between the MUE's for a parallel reduction between the MUE's. Accordingly, Table 3 indicates that if a reduction is performed, locality weights are added to the edges for the MUE's involved in the reduction.","Table 3 also indicates that the locality graph may take stencils into account. Stencils are near-neighbor dependences such as a[i]=function(b[i\u22121], b[i], b[i+1]). For at least one embodiment, a larger locality weight is added for stencil operations than is added for reductions.","Table 3 illustrates that weights may also be added at block  to the edges of the locality graph to reflect spatial and temporal locality among MUE's. That is, once the compiler has identified the MUE's it can also then identify the type of data that the MUE touches. The compiler  may, through static analysis or profiling, identify locality among MUE's. The compiler  may internally record this locality in the data structure illustrated in Table 4, and then use these values to generate weight values for the edges of the locality graph at block .","The weight on an edge of the locality graph may be modified to reflect spatial locality, which takes into account the likelihood that different MUE's may access the same cache line. Similarly, an edge between two MUE's may be modified to reflect that the two MUE's are likely to access the same data (temporal locality).","For at least one embodiment of the compiler , it is assumed that temporal locality may provide a larger performance benefit than spatial locality, if taken into account during scheduling, because temporal locality addresses use of the exact same data between MUE's. Thus, the compiler  may allocate a higher weight value for temporal locality than spatial locality when generating the locality graph. However, one of skill in the art will recognize that spatial locality can yield the same performance benefit as temporal locality, if taken into account during scheduling, if the runtime cache line size is large enough to hold the adjacent data for both MUE's.","In general, then, at least one embodiment of the compiler  utilizes the following general edge weighting scheme during generation of the locality graph: weight for temporal locality>=weight for spatial locality>=weight for stencil>=weight for reduction.",{"@attributes":{"id":"p-0109","num":"0108"},"figref":"FIG. 5","b":["508","510","510","402","510"]},{"@attributes":{"id":"p-0110","num":"0109"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":"TABLE 5"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Generation of Co-Location Hint"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"co_locate_for_locality(a_compilation_unit, threshold)"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"build_locality_graph(a_compilation_unit);"]},{"entry":[{},"while (there exists an edge whose weight > threshold) {"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/* If multiple edges in the graph have identical weight, one is"]},{"entry":[{},"arbitrarily chosen *\/"]},{"entry":[{},"edge = find_edge_with_heaviest_weight( );"]},{"entry":[{},"\/* Produce co-locate compiler hint for the two nodes"]},{"entry":[{},"corresponding to this edge for"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"scheduler *\/"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"colocate_hint(edge);"]},{"entry":[{},"\/* In the locality graph, the two nodes corresponding to this"]},{"entry":[{},"edge are merged into"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"one and the remaining edges leaving each node are coalesced *\/"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"coalesce(edge);"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"}"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"For the colocate_hint( ) function illustrated in Table 5, the hint may be generated by locality value of the two nodes to the same value. (Table 7, below, sets forth pseudocode for at least one embodiment of a method that the scheduler  my employ to use the co-location hint to guide the co-location decision.) The threshold parameter for the colocate_hint function depends on the cache size, cache line size, and the inter-processor communication cost. For instance, if the machine has larger communication cost, the threshold value will be larger to encourage more co-location. From block , processing proceeds to block .","Another type of hint that may be generated by the compiler  at block  relates to \u201chot spots\u201d. The compiler may obtain, through profiling, information regarding long-latency events such as cache misses. A complier may also obtain profiling information regarding frequently-executed edges of a control flow graph. Each of these types of profiling information may indicate \u201chot spots\u201d of a program\u2014frequently-executed or long-latency portions of a program. The faster execution of these hot spots may lead to improved performance, particularly if the hot spot occurs on a critical thread of the multi-shredded program. As is described in further detail below, a hint about hot spots may also be taken into account by the scheduler  when performing workload re-balancing. From block , processing may proceeds to block .","In addition to the Locality and hot spot hint generated by the compiler, the compiler  may also generate at block  Fusion hints that may be utilized by the scheduler  to perform a fusing optimization. During the fusing optimization, the scheduler  may perform a more aggressive co-location optimization than the co-location of MUE's based on locality. For fusion, shreds are not only migrated so that they are co-located, but the computation order may be changed among dependent shreds. If the compiler can identify two MUE's separated by a synchronization mechanism, the two MUE's can potentially be \u201cfused\u201d, and the compiler  can pass one or more hints to the scheduler  for run-time fusing.","Fusion should satisfy dependence constraints. For example, given two two-deep loop nests, the scheduler  may fuse the loops if: 1) the loops are conformable and 2) there is no dependence vector \u201c<,>\u201d. Accordingly, in order to support the fusing optimization, the compiler  may perform fusion feasibility analysis and, based on this analysis, maintain a conformability value and a dependence value in its internal data structures (see Table 4, above).","Conformability requires that the loop bounds of different MUE's to be the same. The compiler indicates that an MUE representing a loop is conformable with another MUE representing a loop if the loop bounds of the first loop and the second loop are identical. Such information may be recorded in the internal conformability field illustrated in Table 4.","Regarding dependences, the compiler performs dependence analysis at block  to avoid generating a fusion hint for MUE's that would contravene dependence constraints. We say that two accesses to data by different MUE's are dependent if they refer to the same location and at least one of them is a write operation. For at least one embodiment, the compiler may determine a dependence direction vector (see, e.g., dependence field in Table 4). Each vector element corresponds to an enclosing loop. The element value can be \u201c<\u201d, \u201c=\u201d, \u201c>\u201d, or unknown. A value of \u201c=\u201d means that an MUE depends only on itself. A value of \u201c<\u201d indicates that the MUE should be executed in its original order and a value of \u201c>\u201d indicates that the MUE should be executed in reverse order.","For example, consider a sample one-level enclosing loop. The direction vector element from access A[i] to access A[j] can be \u201c<\u201d if i<j. The direction vector element will be \u201c>\u201d if i>j. The direction vector element will be \u201c=\u201d if i=j. If the dependence vector for an MUE is \u201c=\u201d, the MUE may be fused without violating dependence constraints.","The compiler  may, for at least one embodiment, generate a fuse hint at block  for a pair of MUE'S if the two MUE's are conformable and if neither MUE has a dependence vector of (<,>). The fuse hint may be generated by the compiler at block  according to a method illustrated by the pseudocode set forth in Table 6:",{"@attributes":{"id":"p-0119","num":"0118"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":"TABLE 6"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Generation of Fusion Hint"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"fuse(a_compilation_unit)"},{"entry":"{"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"for each pair of MUE's {"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"if (no reduction operator && no stencil operator) {"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/* Only current stream element is visible to the kernel *\/"]},{"entry":[{},"if (streams to both MUE are conformable) {"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"if (no scalar code or pointer code between kernels to"]},{"entry":[{},"induce dependences) {"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"147pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"if (no dependence vector with (<, >)) {"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"133pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"add fuse hint for this pair of MUE's for"]},{"entry":[{},"scheduler"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"147pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"}"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"}"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"From block , processing ends at block . The discussion now turns to the use of hints, generated by the compiler according to the method of , by the scheduler.","Use of Hints by the Scheduler.",{"@attributes":{"id":"p-0122","num":"0121"},"figref":["FIG. 6","FIG. 4","FIG. 6","FIG. 6","FIG. 6","FIG. 6"],"b":["450","450","602","604","606","450","600","600"]},"In addition to the scheduler , the software library  may also include shred creation software  that provides for creation of a shred in response to a \u201ccreate\u201d API-like user instruction such as, for example \u201cshred_create_attr\u201d (discussed above in connection with Table 2). For at least one embodiment, the shred creation software provides for creation of a shred by placing a shred descriptor into a work queue system .","The work queue system  may include one or more queues to maintain, for at least one embodiment, descriptors for user-defined shreds that are in line for scheduling and execution and are therefore \u201cpending\u201d. One or more queues may be utilized to hold descriptors for shreds that are waiting for a shared resource to become available, such as a synchronization object or a sequencer. The work queue system , as well as the scheduler logic , may be implemented as software. In alternative embodiments, however, the queue system  and scheduler logic  may be implemented in hardware or may be implemented as firmware (such as micro-code in a read-only memory).",{"@attributes":{"id":"p-0125","num":"0124"},"figref":"FIG. 6","b":["450","640","650","600","600"]},"The run-time library  may create an intermediate layer of abstraction between a traditional industry standard API, such as a Portable Operating System Interface (\u201cPOSIX\u201d) compliant API, and the hardware of a multi-sequencer system that supports at least a canonical set of shred instructions. The run-time library  may act as an intermediate level of abstraction so that a programmer may utilize a traditional thread API (such as, for instance, PTHREADS API or WINDOWS THREADS API or OPENMP API) with hardware that supports shredding.","The scheduler  may perform various optimizations during runtime scheduling of shreds in an attempt to improve performance of the shredded program. Described herein are three optimizations that the scheduler  may perform based on the compiler-generated hints described above: Migration, Co-location and Fusion. One of skill in the art will recognize, however, that the discussion below should not be taken to be limiting. Various other optimizations may be performed, based on other hints generated by the compiler  and passed to the scheduler  via the interface , without departing from the scope of the claims set forth further below.","Migration. The scheduler  benefits from the compiler's MUE determination to perform this optimization, which is basically a workload balance optimization. The migration optimization may be performed by the migration block  of the scheduler .","Finer granularity in MUE decomposition gives greater flexibility to migrate portions of a program to separate sequencers (increase parallelism). For this optimization, the scheduler  may utilize uses the Imbalance hint, which is an integer value indicating the degree of computation associated with the shred. For at least one embodiment, this is accomplished by associating a \u201cdegree of computation\u201d hint with the MUE. This hint allows the scheduler to know a value for \u201chow much work\u201d is involved with executing the MUE. Using this information, the scheduler  may perform efficient load re-balancing among the available sequencers of the system at run-time. That is, the scheduler  may migrate MUE's of the same original thread or shred to different sequencers in order to more efficiently increase thread level parallelism during execution, and\/or may aggregate MUE's onto a single sequencer to achieve load balancing goals.","The scheduler may utilize the hotspot hint to inform its own runtime monitoring for hotspots. For example, if the scheduler receives a hotspot hint from the compiler, this indicates that compiler has determined that the particular MUE may be executed more often than others or that the compiler has determined, through profiling, that the MUE may include a long latency instruction such as a cache miss. The scheduler  may then add the hotspot to the list of those program addresses that it monitors as potential hotspots. Periodically (e.g., every 500 ms), the scheduler  may sample the program counter (PC) during runtime. If one of the monitored addresses repeatedly appears in the PC during such sampling, the scheduler  may treat the address as a hotspot and may make scheduling decisions accordingly. For at least one embodiment, the scheduler  may allocate a more powerful, faster set of sequencers for hot spot execution, or may schedule hot spots to be executed with a higher scheduling priority.","Co-location. The scheduler  may utilize the co-location hint generated by the compiler  at block  () to perform this optimization, for which the scheduler  may schedule data-sharing MUE's on the same (or nearby) sequencers. The co-location optimization may be performed by the co-location block  of the scheduler .","In order to utilize the Locality hint for an MUE, the co-location block  of the scheduler  may generally perform the following: if the locality hint for a particular MUE is above a certain threshold, the scheduler accesses a locality graph to see which other MUE's the current MUE shares data with.","For at least one embodiment, the scheduler  may have access to the locality graph generated by the compiler  (see, e.g., block  of ). For example, the locality graph may be stored as part of the compiler's output, in the same manner, for example, that the symbol table is stored. The co-location logic  of the scheduler  may, upon receiving the co-location hint for an MUE, determine whether the co-location hint value exceeds a predetermined threshold. If so, the scheduler  may look up the current MUE in the locality graph. The scheduler  may then traverse the locality graph to determine which other MUE's the current MUE is likely to share data with, and may make aggregation decisions accordingly.","Table 7 sets forth sample pseudocode for at least one embodiment of a scheduler routine to utilize the information provided by the compiler  over the interface  in order to guide co-location decisions. That is, once the sequencer  has determined, via the co-location hint and traversal of the locality graph, that shreds share a locality value, the method shown in Table 7 illustrates how the scheduler may utilize this information to guide the aggregation decision.",{"@attributes":{"id":"p-0135","num":"0134"},"figref":["FIG. 7","FIG. 7"],"b":["700","700","450","700","604"]},{"@attributes":{"id":"p-0136","num":"0135"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":[{"entry":"TABLE 7"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Locality schedule algorithm"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"For each available sequencer p"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"For each available shred"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/\/p.locality is a variable maintained by the scheduler to"]},{"entry":[{},"represent sequencer"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"locality"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"If shred.attr.locality == p.locality"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"Schedule shred on processor"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"If p.locality not set"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"161pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Schedule shred on processor"]},{"entry":[{},"p.locality = shred.attr.locality \/\/If no match, then"]},{"entry":[{},"use default scheduling"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"algorithm and set p.locality to be shred.attr.locality"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"For the simple algorithm presented in Table 7 and , the goal is to match threads with sequencers that most likely have data that they can use. For at least one embodiment, the Locality hint received over the interface  is an integer, and the value represents sharing with other shreds. For example, if two shreds have the same locality value, then they most likely have positive locality between them.","Turning to , this concept is further illustrated.  is discussed herein with reference to  as well.  shows that the locality variable  associated with an MUE is an integer. Conceptually, the compiler (see, e.g.,  of ) can divide the MUE's of a compilation unit  (such as a shredded program) into locality-sharing groups , , , and .  illustrates that the sample compilation unit  in  can be divided into four groups of MUE's, where shreds of each group share the same locality value. Thus, sample MUE's (virtual shreds) A, B and Z illustrated in  all have the same locality integer value=0. Sample MUE\/shred C in group  b has a locality value of 1. Sample MUE's\/shreds D and E in group have a locality value of 2. Finally,  shows that sample MUE\/shred F in group has a locality value of 3.",{"@attributes":{"id":"p-0139","num":"0138"},"figref":"FIG. 8","b":["800","850","0","850","804","0"],"i":["n","a "]},"Accordingly, for the method  illustrated in Table 7, the scheduler  may schedule shred A to execute on a particular sequencer () and may then schedule additional shreds (B, Z) with the same locality value to execute on the same sequencer (). It should be noted that the Locality integer merely indicates a possible locality relationship among MUE's and does indicate any particular sequencer or hardware resource. The decision regarding which particular resource to be used for execution of the shreds is best made by the scheduler during runtime. Further detail about this process may be garnered from .","For purposes of illustration, the processing of  is discussed in conjunction with the example illustrated in . For the discussion, it is assumed that prior to performing a current iteration of the method , the work queue system  includes shred descriptors in the FIFO order illustrated in : A, C, D, B, Z, F, E. For at least one embodiment, it is also assumed that the locality value for each sequencer  has been set to a null value at powerup. However, at any particular time that the method  is performed, one or more of the sequencers may have previously been assigned to execute a shred having a particular locality integer value. A locality value for the sequencer, which indicates the locality of shreds that have been scheduled to execute on the sequencer, may thus have been assigned to a particular sequencer before a particular iteration of the method .",{"@attributes":{"id":"p-0142","num":"0141"},"figref":"FIG. 7","b":["700","702","704","704","850","850","708","850","1","706","710","650","710"]},"Processing proceeds to block . For purposes of our example, it is assumed that sequencer () has not been assigned a locality value since its last initialization (at power up, restart, reset, etc). Accordingly, block  evaluates to false, and processing falls through to block . The determination at block  evaluates to \u201ctrue\u201d for our example. Accordingly, the locality value for sequencer () is set to the locality value (integer value of \u201c0\u201d) for shred A at block . Shred A is then scheduled for execution on sequencer () at block , and the shred descriptor for Shred A is removed from the queue system . Processing then proceeds to block .","For our example, several shreds (C, D, B, Z, F, and E) remain in the queue system . Accordingly, the determination at block  evaluates to \u201ctrue\u201d and processing proceeds to block  for a second pass. At the second pass of block  the next shred, shred C, is selected from the work queue .  illustrates that the locality value for Shred C is an integer value of \u201c1\u201d. However, the locality value for the current sequencer (sequencer ()), was set to an integer value of \u201c0\u201d at the first pass of block . Accordingly, the determination at the second pass of blocks  and  evaluate to \u201cfalse\u201d. As a result, Shred C is not scheduled on the current sequencer (sequencer ()), and Shred C therefore remains in the work queue system .","Processing then proceeds to block . Because several shreds (\u2003D, B, Z, F, and E) remain in the queue system , the determination at block  evaluates to \u201ctrue\u201d, and processing proceeds to block  for a third pass. At the third pass of block  the next shred, shred D, is selected from the work queue . Processing for Shred D, whose locality value is an integer value of \u201c2\u201d, proceeds as that described above for Shred C. Because the locality values of Shred D and the current sequencer (()) do not match, Shred D is not scheduled on the sequencer and a descriptor for Shred D remains in the work queue system .","Processing then proceeds to block . Because several shreds (\u2003B, Z, F, and E) remain in the queue system , the determination at block  evaluates to \u201ctrue\u201d, and processing proceeds to block  for a fourth pass. At the fourth pass of block  the next shred, shred B, is selected from the work queue . Processing then proceeds to block .","At block , the locality of Shred B is compared with the locality of the current sequencer, sequencer ().  illustrates that the locality for both is an integer value of \u201c0\u201d. Accordingly, the comparison at block  evaluates to \u201ctrue\u201d and processing proceeds to block . Shred B is scheduled for execution on sequencer (), and the descriptor for Shred B is removed from the queue system . Processing then proceeds to block .","Because several shreds (Z, F, and E) remain in the queue system , the determination at block  evaluates to \u201ctrue\u201d, and processing proceeds to block  for a fifth pass. At the fifth pass of block  the next shred, shred Z, is selected from the work queue .  illustrates that the locality value for Shred Z is also an integer value of \u201c0\u201d. Accordingly, Shred Z is scheduled for execution on sequencer () in the manner discussed above for Shred B. Processing then proceeds to block .",{"@attributes":{"id":"p-0149","num":"0148"},"figref":"FIG. 8","sup":["th ","th "],"b":["700","850","1","650","700","720","650","720","722"]},"At block , it is determined whether any additional sequencers are available for the scheduling of shreds. For our example, assume that sequencers () and () are available. Processing therefore proceeds to block  and the next sequencer is selected as the \u201ccurrent\u201d sequencer. For our example, assume that sequencer () is selected at block . Processing then proceeds to block . At the first pass of block  for sequencer () in our example, the work queue system  includes descriptors for Shreds C, D, B, Z, F, and E.","For our example, assume that shred C is selected at the first pass of block  for sequencer (). Processing then proceeds to block .","Again, it is assumed that the sequencer () has a null locality value. Accordingly, the determination at block  evaluates to \u201cfalse\u201d and processing falls through to block . For our example, the determination at block  evaluates to \u201ctrue\u201d for sequencer (), and processing proceeds to block . At block , the locality value for sequencer () is set to the locality value (integer value \u201c1\u201d) of shred C. Processing then proceeds to block . At this first pass of block  for sequencer (), shred C is scheduled for execution on sequencer (), and the shred descriptor for Shred C is removed from the work queue system . Processing then proceeds to block .","The processing described above in connection with shreds C, D, F and E in relation to sequencer () is performed on the second, third, and fourth passes of the method  for sequencer (). That is, none of Shreds D, F or E are scheduled on sequencer () because none of them have the same locality integer as that which was assigned to sequencer (). That is, the locality integer assigned to block () at block  is an integer value of \u201c1\u201d, while the locality values for Shreds D, F, and E are \u201c2\u201d, \u201c3\u201d, and \u201c2\u201d, respectively.","After all shreds have been considered for current sequencer (), processing proceeds to block , where it is determined that one more sequencer, sequencer () is available for work. Accordingly, for our example sequencer () is selected as the current sequencer at block . Processing then proceeds to block .","For our example, at the first pass of method  for sequencer (), the following shreds remain pending in the work queue system: Shreds D, F and E. As is described above, the method  will cycle through all remaining shreds pending in the work queue  in order to determine if they should be scheduled on sequencer (). For our example (assuming, again, that the locality value for sequencer () is initially a null value), Shred D is scheduled on Sequencer () and is removed from the work queue system  at the first pass of block  for sequencer (). At the second pass of method  for sequencer (), Shred F will not be scheduled, and will remain in the work queue system . This is because the locality value for sequencer () is assigned to the locality value of Shred D, an integer value of \u201c2\u201d, at the first pass of block  for sequencer (), yet the locality value for Shred F is an integer value of \u201c3\u201d.","For our example, only three sequencers ((), (), and ()), were available for work. Accordingly, when the determination at block  evaluates to \u201cfalse\u201d at block , there is still an unscheduled shred, Shred F, in the work queue system .  illustrates that such remaining shred are scheduled at block  according to a default method of the scheduler  (e.g., a FIFO scheduling method), rather than according to the locality-based method  illustrated in ","One of skill in the art will note that the method illustrated in Table 7 and  is simply one implementation of how the scheduler  may use the co-location compiler hint. Other implementations may provide other types of information that the scheduler  may use to exploit the degree of locality and to adjust co-location decisions more aggressively. For example, for one alternative embodiment the co-location hint is not generated by the compiler. Instead, the compiler determines, and places on the interface , the intermediate values such as spatial locality and temporal locality (see Table 4), as well as stencil and reduction information so that the scheduler  may utilize the information to make co-location decisions itself. As with the other hints, of course, the scheduler is free to disregard the Locality hint. Failure to utilize the Locality hint to co-locate MUE's with locality, while it may fail to realize certain performance benefits, does not affect program correctness.","Fusion. The Fusion hint passed to the scheduler  over the interface indicates whether the compiler has determined that the current MUE is fusible with another MUE. As is described above, an MUE that has a non-null value for the Fusion hint has been determined by the compiler to be fusible with another MUE in that the two neighboring loop nests have no dependence vector \u201c<,>\u201d and the 2 loops are conformable. As with the other hints, of course, the scheduler  is free to disregard the fusing hint without affecting program correctness. The fusing hint generated by the compiler  for an MUE indicates that it is safe to fuse the MUE; the scheduler  is free to decide during runtime whether such fusion is desirable from a performance standpoint.","Embodiments of the runtime library discussed herein support user-level shreds for any type of multi-sequencer system. Any user-level runtime software that supports shreds, including fibers, pthreads and the like, may utilize the techniques described herein. In addition, the scheduling mechanism and techniques discussed herein may be implemented on any multi-sequencer system, including a single-core SMT system (see, e.g.,  of ) and a multi-core system (see, e.g.,  of ). Such multi-sequencer system may include both OS-visible and OS-sequestered sequencers.","For at least one embodiment, user-level shreds from the same application may run on all, or any subset, of OS-visible sequencers and\/or OS-sequestered sequencers concurrently. Instead of merely sustaining a one-to-one mapping of application threads to OS threads and relying on the OS to manage the mapping between sequencers and threads, embodiments of the runtime library discussed herein may allow multiple user-level shreds in a single application image to run concurrently in a multi-sequencer system. For a single application program that is both multi-threaded and multi-shredded, embodiments of the present invention may thus support M:N thread-to-shred mapping so that N user-level shreds and M threads may execute concurrently on any or all sequencers in the system, whether OS-visible or OS-sequestered. (M, N\u22671).","Such a runtime library as disclosed herein provides a contrast, for example, to systems which allow, at most, only one user-controlled \u201cfiber\u201d to execute per OS-visible thread. A fiber for such systems is associated with an OS-controlled thread, and two fibers from the same thread cannot be executed concurrently. For such contrasted systems, multiple user-level shreds from the same OS-controlled thread cannot execute concurrently.","For at least one embodiment of a runtime library as disclosed herein, the library (see, e.g.,  of ) may initiate one distinct OS thread as a dedicated service thread for each OS-visible sequencer. The service thread can be associated with one or more OS-sequestered sequencers. These OS-visible service threads may each execute an application-specific copy of the scheduler (see, e.g.,  of ) for its associated OS-visible sequencer. The service thread may schedule one or more shreds for execution on OS-sequestered sequencers associated with the OS-visible sequencer (see, e.g., shreds - and - associated with OS-visible threads  and , respectively, of ). Each of the shreds may run a copy of the scheduler on an OS-sequestered sequencer.",{"@attributes":{"id":"p-0163","num":"0162"},"figref":["FIG. 9","FIG. 6","FIG. 6"],"b":["900","900","904","940","940","902","944","942","902","910","912","904","910","600","450","912","650"]},"Memory system  is intended as a generalized representation of memory and may include a variety of forms of memory, such as a hard drive, CD-ROM, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), flash memory and related circuitry. Memory system  may store instructions  and\/or data  represented by data signals that may be executed by processor . The instructions  and\/or data  may include code and\/or data for performing any or all of the techniques discussed herein. For example, the data  may include one or more queues to form a queue system  capable of storing shred descriptors as described above. Alternatively, the instructions  may include instructions to generate a queue system  for storing shred descriptors.","The processor  may include a front end  that supplies instruction information to an execution core . Fetched instruction information may be buffered in a cache  to await execution by the execution core . The front end  may supply the instruction information to the execution core  in program order. For at least one embodiment, the front end  includes a fetch\/decode unit  that determines the next instruction to be executed. For at least one embodiment of the system , the fetch\/decode unit  may include a single next-instruction-pointer and fetch logic . However, in an embodiment where each processor  supports multiple thread contexts, the fetch\/decode unit  implements distinct next-instruction-pointer and fetch logic  for each supported thread context. The optional nature of additional next-instruction-pointer and fetch logic  in a multiprocessor environment is denoted by dotted lines in .","Embodiments of the methods described herein may be implemented in hardware, hardware emulation software or other software, firmware, or a combination of such implementation approaches. Embodiments of the invention may be implemented for a programmable system comprising at least one processor, a data storage system (including volatile and non-volatile memory and\/or storage elements), at least one input device, and at least one output device. For purposes of this application, a processing system includes any system that has a processor, such as, for example; a digital signal processor (DSP), a microcontroller, an application specific integrated circuit (ASIC), or a microprocessor.","A program may be stored on a storage media or device (e.g., hard disk drive, floppy disk drive, read only memory (ROM), CD-ROM device, flash memory device, digital versatile disk (DVD), or other storage device) readable by a general or special purpose programmable processing system. The instructions, accessible to a processor in a processing system, provide for configuring and operating the processing system when the storage media or device is read by the processing system to perform the procedures described herein. Embodiments of the invention may also be considered to be implemented as a machine-readable storage medium, configured for use with a processing system, where the storage medium so configured causes the processing system to operate in a specific and predefined manner to perform the functions described herein.","Sample system  is representative of processing systems based on the Pentium\u00ae, Pentium\u00ae Pro, Pentium\u00ae II, Pentium\u00ae III, Pentium\u00ae 4, Itanium\u00ae, and Itanium\u00ae 2 microprocessors and the Mobile Intel\u00ae Pentium\u00ae III Processor\u2014M and Mobile Intel\u00ae Pentium\u00ae 4 Processor\u2014M available from Intel Corporation, although other systems (including personal computers (PCs) having other microprocessors, engineering workstations, personal digital assistants and other hand-held devices, set-top boxes and the like) may also be used. For one embodiment, sample system may execute a version of the WINDOWS\u2122 operating system available from Microsoft Corporation, although other operating systems and graphical user interfaces, for example, may also be used.","While particular embodiments of the present invention have been shown and described, it will be obvious to those skilled in the art that changes and modifications can be made without departing from the scope of the appended claims. For example, the static\/off-line analysis described above may instead be implemented in a dynamic compiler, such as a Just-in-Time (JIT compiler).","Accordingly, one of skill in the art will recognize that changes and modifications can be made without departing from the present invention in its broader aspects. The appended claims are to encompass within their scope all such changes and modifications that fall within the true scope of the present invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Embodiments of the present invention may be understood with reference to the following drawings in which like elements are indicated by like numbers. These drawings are not intended to be limiting but are instead provided to illustrate selected embodiments of a computer-accessible medium, system and methods to judiciously schedule user-level threads in a multithreaded system based, at least in part, on scheduling hints from a compiler.",{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
