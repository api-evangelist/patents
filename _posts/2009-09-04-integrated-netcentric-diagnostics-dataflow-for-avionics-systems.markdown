---
title: Integrated net-centric diagnostics dataflow for avionics systems
abstract: A net-centric avionics diagnostics and automated maintenance environment system includes a storage medium on which is stored diagnostic data concerning the operation of systems of an aircraft and diagnostic data of aircraft systems, including BIT data, an organizational level automated maintenance environment server to transport maintenance and diagnostic information throughout the automated maintenance environment. The system also includes an organizational level diagnostics avionics tester that has a processor to execute diagnostics software for gathering, storing, and processing avionics diagnostics information. The tester is linked to an interface device that includes data acquisition hardware, standard interfaces for an avionics unit under test, and instrumentation for troubleshooting the unit under test. The organizational level diagnostics avionics tester is in network communication with the organizational level automated maintenance environment server. The system also includes a common intermediate level tester for testing a plurality of avionics modular assemblies, and an intermediate level maintenance environment server that stores historical maintenance data for use by the common intermediate level tester and by the organizational level diagnostics avionics tester.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08306689&OS=08306689&RS=08306689
owner: The United States of America as represented by the Secretary of the Navy
number: 08306689
owner_city: Washington
owner_country: US
publication_date: 20090904
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","GOVERNMENT INTEREST","BACKGROUND","SUMMARY","DETAILED DESCRIPTION","CONCLUSION"],"p":["This application claims the benefit of priority from U.S. Provisional Patent Application Ser. No. 61\/233,038, filed on Aug. 11, 2009.","The invention described herein may be manufactured, licensed, and used by or for the U.S. Government.","As avionics systems have increased in complexity and function, there have been significant advances in integrated diagnostics. Complex avionics systems that formerly required many hours of troubleshooting to isolate faults are now equipped with built-in test (BIT) systems and other types of automatic test equipment that can diagnose and self-report a wide variety of faults and failures. BIT systems typically run a suite of onboard diagnostic routines that produce fault codes and calibration data which may be used to identify defective modular component assemblies (referred to as Weapons Replaceable Assemblies (WRAs)) in avionics systems. WRA's are sealed modular units that are designed to communicate over common interfaces such as Military data bus standard MIL-STD-1553, and can be replaced quickly on the flightline from inventory in order to rapidly return an avionics system to service. WRAs that have been identified as defective may be removed and rapidly replaced on the flightline with simple hand tools. While such advances have greatly simplified the identification of potentially defective components on the flight line, the systems have not entirely lived up to expectations. The plethora of data generated by BIT systems does not always provide sufficient insight into the subtle and highly complex relationships between failure modes and faults in advanced fighter aircraft avionics to correctly diagnose defective WRAs. Multiple or ambiguous fault scenarios arising from such complex systems interrelationships are also not handled well by BIT systems. Misdiagnosis of component failures has lead to increased repair time, high false return rates (so called \u201cA-799's\u201d) and increased aircraft downtime.","The F\/A-18's electro-optical avionics pods, including the AN\/ASD-10 Advanced Tactical Aerial Reconnaissance System (ATARS), AN\/ASD-12 Shared Reconnaissance Pod (SHARP), and AN\/ASQ-228 Advanced Targeting Forward Looking Infrared (ATFLIR) pod are examples of avionics systems which are difficult to diagnose and repair. A significant amount of diagnostic data is currently captured by BIT systems of such pods. However, diagnostic information is presently not used to full effect. Such deficiencies have wasted resources, manpower and created parts shortages as maintainers struggle to determine whether a component is bad or good.","Problems also arise from the use of specialized maintenance systems that cannot be modified to incorporate new features. An example of one such system is the AN\/USM-681 Electro-Optics Pallet\/Pod Tester (EOPT) System which is currently the main Organizational Level (O-level) support equipment (SE) used on the flightline to test and troubleshoot the ATARS, SHARP, and ATFLIR systems. The EOPT lacks automated diagnostic reasoning in the fault detection\/fault isolation (FD\/FI) test strategies it uses. Moreover, the EOPT lacks the ability to exchange data between the O-level, and I-level (Intermediate Level), D-level (Depot Level) or OEM-level (Original Equipment Manufacturer) maintenance environments. Additionally, network centric warfare (also referred to as \u201cnet-centric\u201d warfare) is a concept that is increasingly embraced by the US Department of Defense (DoD). Part of that concept extends to net-centric maintenance, sometimes referred to as a Net-centric Diagnostic Framework (NCDF). As new diagnostic test strategies and systems are introduced, the technologies used by the EOPT systems are rapidly becoming obsolete, thus, there is an immediate need for upgradeable avionics diagnostic hardware as well as an avionics diagnostic and repair system that has the capability to work in a net-centric environment and is able to seamlessly exchange diagnostic and maintenance data between maintenance levels. Embodiments according to the present invention are directed to addressing these problems.","In general, in one aspect, an embodiment of a net-centric avionics diagnostics and automated maintenance environment system includes a storage medium on which is stored diagnostic data concerning the operation of systems of an aircraft and diagnostic data of aircraft systems, including BIT data, an organizational level automated maintenance environment server to transport maintenance and diagnostic information throughout the automated maintenance environment. The system also includes an O-level diagnostics avionics tester that has a processor to execute diagnostics software for gathering, storing, and processing avionics diagnostics information. The tester is linked to an interface device that includes data acquisition hardware, standard interfaces for an avionics unit under test, and instrumentation for troubleshooting the unit under test. The O-level diagnostics avionics tester is in network communication with the O-level automated maintenance environment server. The system also includes a common I-level tester for testing a plurality of avionics modular assemblies, and an I-level maintenance environment server that stores historical maintenance data for use by the common I-level tester and by the O-level diagnostics avionics tester.","In another aspect, an embodiment of a net-centric process for diagnosing and maintaining an electro-optical avionics pod at multiple aircraft maintenance levels of an automated maintenance environment includes gathering diagnostic data and BIT data from the electro-optical avionics pod, combining the diagnostic data and BIT data with historical Consolidated Automated Support System (CASS) test results for electro-optical avionics pods, providing the combined diagnostic data, BIT data, and historical CASS test results to a first diagnostic reasoner at an O-level, the first diagnostic reasoner identifying one or more O-level test entry points for conducting tests of the electro-optical avionics pod at the O-level, testing the electro-optical avionics pod at the one or more O-level test entry points to derive O-level test results, determining whether I-level repairs will be needed for the electro-optics pod, and, sending the O-level test results to an I-level maintenance server. If I-level repairs are determined to be needed, the process further includes analyzing the O-level test results with a second diagnostic reasoner to determine one or more CASS safe-turn-on entry points for I-level testing of pod modular component assemblies, testing the pod modular component assemblies at the one or more CASS safe-turn-on entry points to obtain I-level test results, identifying one or more modular component assemblies of the electro-optical avionics pod that are likely to be defective from the I-level test results, storing the I-level test results on the I-level maintenance server, and storing the I-level test results on the O-level maintenance server.","In the following detailed description, reference is made to the accompanying drawings which are a part of this patent disclosure, and in which are shown by way of illustration specific embodiments in which the invention, as claimed, may be practiced. This invention may, however, be embodied in many different forms and should not be construed as limited to the embodiments set forth; rather, these embodiments are provided so that this disclosure will be thorough and complete, and will fully convey the scope of the invention to those skilled in the art.",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1","b":["100","100","121","121","120","120","121","100","102","121","108","110","106","102","102","100","114","116","124","116","100"]},"Embodiments according to the present invention build upon the Navy's Smart Test Program Set (Smart TPS). Smart TPS is an initiative designed to limit CASS run times by directing CASS runs based on BIT data that has been gathered from the aircraft at the O-level. Smart TPS has a number of limitations. One serious limitation of Smart TPS is its inability to pass maintenance information from the I-level back down to O-level tools. The data flows in one direction only. Moreover, there is no data flow between the current electro-optics tester (the AN\/USM-681 EOPT) and any other device (except, of course, for the unit under test). In embodiments according to the present invention, OMS server  provides bidirectional network communication to the CASS system . At the most basic level, the interaction proceeds substantially as follows. An aircraft  lands at a location where O-level maintenance is provided. If something is wrong with the aircraft , as identified by BIT data, observables (maintainer observations of maintenance issues), and\/or pilot reports, an O-level maintenance action form (MAF) , also referred to as a \u201cwork order,\u201d is generated against the aircraft. If I-level repair is necessary, the MAF  and BIT test results are sent to the I-level with the work order going from an Optimized Organizational Maintenance Activity (OOMA) server  at the O-level to an Optimized Intermediate Maintenance Activity (OIMA) server  at the I-level. The BIT results are sent from the SAME server  to the OMS server . On OMS server , the Smart TPS reasoner analyzes the BIT results and sends them to CASS  which recommends several safe-turn-on entry points to begin testing along with confidence levels associated with each safe-turn-on entry point.","An embodiment according to the present invention of a net-centric diagnostic process  for diagnosing and repairing a UUT , which, in this example, is an electro-optics pod such as the ATFLIR, SHARP or ATARS, carried by aircraft , will now be described. Referring to , after aircraft  has returned from a mission and been brought to a location where O-level maintenance can be provided, such as the hangar deck or flight deck of the aircraft carrier, a normal Optimized Organizational Maintenance Activity (OOMA) debrief is performed as illustrated in process step . The normal OOMA debrief involves debriefing the pilot to record any maintenance issues that may have manifested during the flight, retrieving MU  from aircraft , recording any observables and uploading data, including aircraft BIT data, from MU  together with the pilot debriefing and any observables to SAME server . MU  is equipped with a standard personal computer interface to facilitate uploading the data to Navy maintenance computers. The currently used interface is known as \u201cPCMCIA,\u201d which is an acronym for Personal Computer Memory Card International Association. In alternative embodiments, other standard interfaces may be used including, for example, Firewire (IEEE 1394), Universal Serial Bus (USB), EIA RS-232 and EIA RS-485 and successor standard interfaces as may be promulgated from time to time. One or more wireless interfaces such as IEEE 802.11 may also be used in alternative embodiments.","Data from MU  is uploaded to AME squadron server . Software on AME squadron server  strips the data from MU , analyzes it, and records the analysis. It also parses the BIT codes and presents to the user a list of the systems that may need to be fixed based on those BIT indications. From analysis of the information gathered in process step  one or more O-level Maintenance Action Forms (MAFs)  (also referred to as Work Orders (WOs)) are created. An O-level MAF  is assigned to a maintainer to fix the problems called out by the form.","Next, in process step , O-level MAFs  are transmitted along with aircraft BIT data, observables, repairs, and job status to SAME server . The data created in response to the aircraft BIT information in the previous step (MAF, BIT data, observables, repairs and job status) is copied from an AME application to an NCDF application both of which reside on the SAME server  (also referred to as \u201csquadron server\u201d). The NCDF application is a part of the NCDF software suite which is resident on SAME Server , OMS server  and PEMA . In step , the \u201cNCDF for AME\u201d application on the SAME server  consolidates and stores the NCDF data (MAF, BIT data, observables, repairs and job status) in a file on a hard drive or other storage media within SAME server . All of this data is already on the SAME server , but stored in different locations.","In step , the NCDF data (MAF, BIT data, observables, repairs, job status and historical test result information (from CASS via the OMS server) is pushed from the SAME server  to PEMA  via an existing F\/A-18 AME Ethernet network. Historical test result information is already resident on the SAME server , having been pushed up from the I-level at the end of the last maintenance action.","In step , the NCDF data (MAF, BIT data, observables, repairs, job status and historical test result information) is stored on the PEMA . The data may be stored on a hard drive or other storage media carried by PEMA , for example, a flash memory, an optical disc, or a random access memory.","In step , the aircraft maintainer, with MAFs and the Diagnostics Avionics Tester (DAT)  (which includes PEMA  and IDA ), goes out to the aircraft and hooks up IDA  to the UUT . The maintainer then connects PEMA  to IDA  via a shielded Ethernet cable and turns on PEMA  and IDA . PEMA  boots up and IDA  does an operational-readiness test (ORT), which is a start-up BIT for self-checking IDA . Once PEMA  has successfully booted and IDA  has passed its ORT, in step , the maintainer launches the DAT Test Executive application (the instance of the NCDF application resident on PEMA ).","In step , the maintainer is queried by the DAT Test Executive application about whether this is a new job or if the maintainer is returning to finish a previously-started maintenance job. If it's an existing job, then in step , the maintainer selects the existing job from a drop down list or another quick reference enumeration of open jobs, provided by DAT Test Executive. If this is a new job, in step  the maintainer enters a job control number (JCN) and a maintenance control number (MCN) associated with the job. The maintainer also enters the part number and serial number for UUT . All of this data is used by the NCDF system to track the job and UUTs across maintenance levels.","Step  captures any pretest repairs completed prior to performing tests. This function reports any previous information already in the \u201cNCDF for DAT\u201d application indicating that\u2014within this maintenance action\u2014some repairs have already been done to the UUT  prior to the test that is about to execute. For example, is the maintainer about to execute a test on a system in which one WRA has already been removed and replaced?","In Step , Directed TPS results are calculated by the NCDF for DAT application and displayed on PEMA . The maintainer selects the appropriate entry point. The \u201cNCDF for DAT\u201d application, which includes a user interface and a Directed TPS Reasoner , makes diagnostic recommendations based on all of the information it has (BIT data, observables, and historical test result information) and presents the maintainer with a list of possible test program entry points and a percentage confidence level for each entry point. The current embodiment of the Directed TPS Reasoner  is based on a weighted decision tree model. However, alternative embodiments may employ other forms of reasoners that are based on any one of a myriad of automated reasoner algorithms, including, but not limited to, data mining, genetic, fuzzy decision tree, correlation-based, and identification of intersecting feature sets algorithms. After examining the list of possible entry points, the maintainer selects the test program entry point at which he\/she wants to start testing.","At step , the maintainer selects whether or not to run the test program at the selected entry point. If the maintainer elects to run the test program, in step , the test program, which has been adapted to interface to the \u201cNCDF for DAT\u201d application, is run at the specified entry point. In step , the test program application tests the UUT . The test program application, running on the PEMA , commands the IDA  over Ethernet, using Local Area Network (LAN) Extensions for Instrumentation (LXI). The IDA  receives those instructions and interfaces with the UUT  appropriately to execute the tests. Test results are compiled from measurement data sent back from the UUT , through the IDA , to the PEMA  (i.e., to the test program application). In step , test results (including TPS callouts) are generated and sent to the NCDF.","If any failures were detected during testing, the test program will generate \u201ccallouts\u201d pertaining to the failures. Step  determines whether any such callouts have been generated.","In step , the test program calculates and displays the callouts on PEMA  and lists the probable causes of failure in order, from the most-likely cause of failure to the least-likely cause of failure. Percentage confidence for each possible cause of failure is displayed as well. The maintainer views these test results and takes action. For example, if the test program indicates that WRA # may be defective with an 80% confidence level, and WRA # may be defective with a 20% confidence level, the maintainer might decide to remove WRA # first. However, there may be other real-world reasons for the maintainer to remove WRA # first, such as WRA # can be removed and replaced much more easily than WRA #. Providing the maintainer with information about the confidence level of failure data at the O-level is a feature of embodiments according to the present invention.","After the maintainer has performed the maintenance actions called for by the test program, i.e., the maintainer has removed and replaced one or more WRAs, at step  the test program queries the maintainer whether the system should be retested to verify that the repair fixed the problem. The maintainer can elect to retest or to skip retesting if it is not warranted at this time.","Step  queries the maintainer to input data to document any post-test repairs. Step  queries the maintainer on the new (i.e., post-repair) status of UUT . In response to the query, the maintainer should, for example, indicate whether the job is complete, or whether it is still open because it needs to be revisited later. Maintainers may leave jobs incomplete because, for example, parts need to be ordered, or it is the end of their shift, or it is a complicated repair that will take multiple days, or a myriad of other reasons.","In step  all NCDF data is transmitted to the I-level. In addition, the PEMA  and IDA  are brought back into maintenance control. PEMA  is plugged into a docking station, which provides it with power and a network connection. PEMA  transmits its NCDF data (i.e., test data from the O-level, BIT codes, JCN, MCN, UUT P\/N and UUT S\/N) to the OMS server  at the I-level via the F\/A-18 AME network. Again, a wireless interface may be employed in alternative embodiments according to the present invention.","In step , open jobs that have been completed are closed out in the Naval Aviation Logistics Command Operating Maintenance Information System (NALCOMIS). The O-level MAF  is closed in the OOMA server  (O-level NALCOMIS). This automatically triggers transmission of the MAF  information to an Optimized Intermediate Maintenance Activity (OIMA) server  (I-level NALCOMIS) to create an I-level MAF. This data is also transmitted over the F\/A-18 network, but it is a separate maintenance control function and is not combined with the other data being sent to I-level.","WRAs that were removed and replaced at the O-level are now sent to the I-level for further trouble-shooting.","At step , NCDF data (test data from O-level, BIT codes, JCN, MCN, UUT P\/N and UUT S\/N) from the O-level via the F\/A-18 AME network is stored from SAME server  and DAT  to the OMS server  for use at the I-level.","At step , OIMA server  sends NALCOMIS data regarding the job to the OMS server  which is the gateway to the CASS .","At step , the OMS server  retrieves the aircraft's Bureau Number (BUNO) from the OIMA NALCOMIS data. The BUNO is used to identify individual airframes (aircraft) within the naval aircraft inventory.","At step , the OMS server  runs the \u201cNCDF for CASS\u201d application and adds the job to this application's data. Next, in step , CASS test program entry points are calculated, along with confidences. The calculations are based on historical test data resident on OMS , O-level test results and BIT results (also sent down from O-level).","At step , OMS server  sends job data and Directed TPS results to CASS . The maintainer then selects a job to run on CASS  in step  and physically hooks up the likely defective WRA to CASS , either directly or via an interface device and any other cable connections that need to be made.","In step , CASS  presents the maintainer with several entry points and percent confidences. The maintainer has the choice at step  of continuing the test or exiting out and not doing any (further) testing. If further testing is selected, at step , the maintainer selects an entry point for the test program and then runs the test program from that entry point. In step , the process determines whether there were any failures identified during testing. If failures were identified, in step , a test within the test program fails and the test program makes a callout. In step , CASS sends its test results back to OMS server . In step , the OMS server  gives the test results to the \u201cNCDF for CASS\u201d application running internally on the OMS server. In step , the \u201cNCDF for CASS\u201d application calculates the appropriate callouts based on the CASS test results and historical test information resident on OMS server . The \u201cNCDF for CASS\u201d application makes the test results available to the software running on OMS server . The reason this is being done again is part of the interactive trouble-shooting process, in case the first entry-points did not find or test the problem. In step , the results of this calculation are sent back to the CASS for the I-level maintainer to see on the screen in CASS. Step  captures the results of CASS testing on the OMS server . In step , the job status is captured (i.e., is the job complete or incomplete?).","In step , the job status and test results are transferred to the OMS server .","In step , the job status and test results are made available to the \u201cNCDF for CASS\u201d (which is running on the OMS server ) and the job status and test results are transmitted to SAME server  at O-level (which is running \u201cNCDF for AME\u201d).","Finally, in step , the I-level MAF in the NALCOMIS system is closed out.","Although the present invention has been described in considerable detail with reference to certain embodiments hereof, it will be clear to one skilled in the art that the above embodiments may be altered in many ways without departing from the invention. Accordingly, the spirit and scope of the appended claims should not be limited to the description of the embodiments contained herein."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Embodiments according to the invention are illustrated in the accompanying drawings in which like reference numerals represent like parts throughout and in which:",{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIGS. 2A","b":["2","2","2"]}]},"DETDESC":[{},{}]}
