---
title: Methods for implementing multi-touch gestures on a single-touch touch surface
abstract: Methods for activating multi-touch functionality by recognizing and processing multi-touch interactions on a touch surface of non-multi-touch computing devices. The computing device may detect a jump from the location of the first touch event to determine that a multiple touch gesture is being traced. Virtual touch events are detected and stored. Using mathematical formulae, parameters are calculated based on initial and subsequent virtual touch event locations. Based on these parameters the multi-touch functionality is determined, such as a zooming or rotating function. A transform factor may be determined and applied to the image display.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08957918&OS=08957918&RS=08957918
owner: QUALCOMM Incorporated
number: 08957918
owner_city: San Diego
owner_country: US
publication_date: 20091103
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The present invention relates generally to user interface systems for computing systems, and more particularly to gesturing using touch sensitive user interface surfaces.","Multi-touch interactions with computing devices have received considerable attention in the recent years. Today, multi-touch touch sensitive devices are found in many portable computing devices such as Personal Digital Assistants (PDA) and laptop computers. These touch sensitive devices allow users to employ multi-touch gestures to perform functions such as zooming or rotating a display image. There are, however, many disadvantages with the currently available multi-touch compatible touch sensitive devices. For instance, touch sensitive devices capable of recognizing multi-touch gestures require special technology and hardware, which is currently expensive to manufacture and implement. Additionally, this special technology is incompatible with many currently available touch surface devices that are used in a majority of computing devices. As a result, because the currently available multi-touch technology is new and costly, the majority of the computing devices already in the market do not possess multi-touch capabilities.","The various aspects include methods for implementing a multi-touch gesture on a computing device having a touch surface user input device that include detecting a first touch event on the touch surface, storing a location of the first touch event on the touch surface, detecting a jump in location of the touch event on the touch surface without detecting a touch-up event, storing a location of the touch event following the detected jump, obtaining a new touch event location, determining a graphical user interface function to be implemented based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location, and implementing the determined graphical user interface function.","In various aspects of the methods, the determined graphical user interface function may be a transform function, an image zoom transform function, and an image rotation transform function, for example. In aspects where the determined graphical user interface function is a transform function, the method further may further include determining a transform factor based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location. In aspects where the determined graphical user interface function is an image zoom transform function, and the method further may further include determining a magnification transform factor based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location, in which implementing the determined graphical user interface function comprises applying a zoom magnification to a displayed image based upon the determined magnification transform factor. In aspects where the determined graphical user interface function is an image rotation transform function, and the method may further include determining a rotation transform factor based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location, in which implementing the determined graphical user interface function comprises rotating a displayed image based upon the determined rotation transform factor.","In various aspects of the methods, determining a graphical user interface function to be implemented based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location may be accomplished in a variety of ways. In an aspect, determining a graphical user interface function to be implemented based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location may include calculating a difference between a distance from the stored location of the first touch event to the stored location of the touch event following the detected jump and a distance from the stored location of the first touch event to the new touch event location. In an aspect, determining a graphical user interface function to be implemented based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location may include calculating an angle between a line from the stored location of the first touch event to the stored location of the touch event following the detected jump and a line from the stored location of the first touch event to the new touch event location. In a further aspect, determining a graphical user interface function to be implemented based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location may include calculating a difference between a distance from the stored location of the first touch event to the stored location of the touch event following the detected jump and a distance from the stored location of the first touch event to the new touch event location, and calculating an angle between a line from the stored location of the first touch event to the stored location of the touch event following the detected jump and a line from the stored location of the first touch event to the new touch event location. In further aspects, determining a graphical user interface function to be implemented based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location may be performed by an operating system or by an application programming interface (API). In a further aspect, determining a graphical user interface function to be implemented based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location may include estimating a touch location of a second touch based on the stored location of the first touch event and the stored location of touch event following the detected jump. In a further aspect, determining a graphical user interface function to be implemented based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location may include calculating a first distance from the stored location of the first touch event to the stored location of touch event following the detected jump, calculating a second distance from the stored location of the first touch event to the new touch event location, determining if the first distance is different from the second distance, determining that the graphical user interface function is a zoom transform function if the first and second distances are different, and calculating a magnification transform factor based on a ratio of the second distance to the first distance, in which implementing the determined graphical user interface function comprises applying a zoom magnification to a displayed image based upon the calculated magnification transform factor. In a further aspect, determining a graphical user interface function to be implemented based on the stored location of the first touch event, the stored location of the touch event following the detected jump, and the new touch event location may include calculating a first distance from the stored location of the first touch event to a second touch location based on the distance from the stored location of the first touch to the stored location of touch event following the detected jump, calculating a second distance from the stored location of the first touch event to a new second touch location based on the distance from the stored location of the first touch to the new touch event location, determining if the first distance is different from the second distance, determining that the graphical user interface function is a zoom transform function if the first and second distances are different, and calculating a magnification transform factor based on a ratio of the second distance to the first distance, in which implementing the determined graphical user interface function comprises applying a zoom magnification to a displayed image based upon the calculated magnification transform factor.","In a further aspect, a computing device includes a processor, a user interface touch surface device coupled to the processor, a memory coupled to the processor, a display coupled to the processor, in which the processor is configured with processor-executable instructions to perform processes of the various aspect methods.","In a further aspect, a computing device includes means for accomplishing the process functions of the various aspect methods.","In a further aspect, a computer program product includes a computer-readable medium having stored thereon at least one instruction for accomplishing the processes of the various aspect methods.","The various aspects will be described in detail with reference to the accompanying drawings. Wherever possible, the same reference numbers will be used throughout the drawings to refer to the same or like parts. References made to particular examples and implementations are for illustrative purposes and are not intended to limit the scope of the invention or the claims.","The word \u201cexemplary\u201d is used herein to mean \u201cserving as an example, instance, or illustration.\u201d Any implementation described herein as \u201cexemplary\u201d is not necessarily to be construed as preferred or advantageous over other implementations.","As used herein, a \u201ctouchscreen\u201d is a touch sensing input device or a touch sensitive input device with an associated image display. As used herein, a \u201ctouchpad\u201d is a touch sensing input device without an associated image display. A touchpad, for example, can be implemented on any surface of an electronic device outside the image display area. Touchscreens and touchpads are generically referred to herein as a \u201ctouch surface.\u201d Touch surfaces may be integral parts of an electronic device, such as a touchscreen display, or may be a separate module, such as a touchpad, that can be coupled to the electronic device by a wired or wireless data link. The terms touchscreen, touchpad, and touch surface may be used interchangeably hereinafter.","As used herein, the terms \u201cpersonal electronic device,\u201d \u201ccomputing device\u201d and \u201cportable computing device\u201d refer to any one or all of cellular telephones, personal data assistants (PDAs), palm-top computers, notebook computers, personal computers, wireless electronic mail receivers, and cellular telephone receivers (e.g., the Blackberry\u00ae and Treo\u00ae devices), multimedia Internet enabled cellular telephones (e.g., the Blackberry Storm\u00ae), and similar electronic devices that include a programmable processor, memory, and a connected or integral touch surface or other pointing device (e.g., a computer mouse). In an example used to illustrate various aspects of the present invention, the electronic device is a cellular telephone including an integral touchscreen display. However, this aspect is present merely as one example implementation of the various aspects, and as such is not intended to exclude other possible implementations of the subject matter recited in the claims.","As used herein a \u201ctouch event\u201d refers to a detected user input on a touch surface that may include information regarding location or relative location of the touch. For example, on a touchscreen or touchpad user interface device, a touch event refers to the detection of a user touching the device and may include information regarding the location on the device being touched.","The currently available multi-touch technology is uniquely available on computing devices which employ a special and costly type of multi-touch capable touch sensitive device. Today, in order to implement multi-touch gestures, users must abandon their non-multi-touch computing devices and purchase new, expensive computing devices equipped with multi-touch capable touch sensitive device.","The various aspect methods and systems provide a user interface system that recognizes multi-touch gestures on any touch sensitive device, thereby enabling non-multi-touch sensitive devices to support multi-touch gestures. The methods of the various aspects may be implemented in many computing device equipped with non-multitouch touch sensitive devices. Further, the various aspects may be implemented in current non-multi-touch computing devices through a simple software upgrade.","The currently available non-multi-touch devices possess certain limitations that preclude conventions multi-touch interactions. When a user touches the surface of a non-multi-touch computing device using two fingers, the two (or more) touch locations are averaged to indicate a single touch location. As a result, all multi-touch events are interpreted as single-touch events with the coordinates of the averaged touch locations.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 1","FIG. 104"],"b":["100","102","104","102","100","1","104","100","2","1","1","2","2","104","100","100","3","1","2","3","1","2","102","1","3","1","3","2"]},"To activate multi-touch graphical user interface functionalities (e.g., zooming or rotating), the processor of a computing device according to the various aspects may be programmed with processor-executable instructions to recognize the touch pattern described above and treat the touch patterns as multi-touch gestures. According to the various aspects, in order to perform multi-touch gestures on non-multi-touch touch surfaces  using two fingers, users are required to touch the touch surface  first with one finger  followed by a touch by a second finger . The computing device may be configured to detect and store the coordinates of the first touch (P), recognize a sudden jump in touch location to an averaged virtual touch coordinates (P) (e.g., a sudden movement without a touchup event) and based on the coordinates of the virtual touch P, calculate the coordinates of the second touch (P). In performing a multi-touch gesture according to the various aspects, the user should keep the first finger  essentially stationary and in contact with the touch surface  while moving the second finger . Keeping the first finger  stationary at all times allows the computing device to calculate changes in the position of the second finger at P based upon the sensed position of the virtual touch at P. By presuming that the first touch location P is not moving, the computing device can identify the performed multi-touch gesture and implement the appropriate multi-touch gesture function.","For example, to perform a pinch gesture for applying a zoom-out (i.e., shrink) function to the display image, a user may first touch the touch surface  with one finger  followed by a second finger  a short distance apart. To perform a graphical user interface zoom-in function, the user may keep the first finger  stationary and move the second finger  away from the first finger  in a pinching gesture that increases the distance between the first and second fingers , . Similarly, to perform a rotation function, the user may keep the first finger  stationary and move the second finger  as though following the circumference of a circle, the center of which is defined by the location of the first finger  at P. Moving the second finger  through an arch with respect to the first finger  in a clockwise direction may be interpreted as multi-touch gesture that the computing device implements to rotate the display image in a clockwise direction. Similarly, moving the second finger  in a counterclockwise direction may cause the computing device to rotate the display image in a counterclockwise direction.","The various aspect methods overcome the technological limitations presented to the non-multi-touch computing devices and enable them to recognize and process multi-touch gestures as graphical user interface commands. Accordingly, the various aspect methods may be implemented on all computing devices equipped with a touch surface  by implementing method  illustrated in . In method  at block , computing devices may detect a first touch by a first finger  and store the coordinates of the first touch location in memory. At block , computing devices may detect a virtual touch location when a second finger  touches the touch surface , and determine that the touch surface touch locations jumped suddenly from the location of the first touch event to the location of the virtual touch event (i.e., from P to P) at determination block . Computing devices may be configured to activate multi-touch functionality based on the parameters of the jump in the touch event locations. The distance of the jump may be calculated based on the distance between the first touch event and the virtual touch event. Mathematical formulae that may be used to calculate the distance of the jump are described in detail below with respect to . To distinguish normal tracing of a finger on the touch surface, the computing device may determine whether the calculated distance of the jump in touch event locations is larger than a predetermined threshold distance \u201cX\u201d at determination block . If the distance of the instantaneous jump is less than the threshold distance (i.e., determination block =\u201cNo\u201d), the multi-touch gesture functionality is not used and normal graphical user interface processing of the touch event may be implemented. If the distance of the instantaneous jump equals or exceeds the threshold distance (i.e., determination block =\u201cYes\u201d), the computing device may activate the multi-touch functionality and store the virtual touch location P in memory at block .","Detecting a multi-touch event based on a sudden jump in position of the touch location is just one example method that may be implemented. In another aspect, the computing device may be configured to detect the presence of two finger touches on the touch surface based upon other measurable parameters, such as the total amount of surface area touched, touch locations in two different zones within the touch surface, total capacitance or resistance measured by the touch surface, or other mechanism or measurement that may be available to the touch surface. Such alternative mechanisms for detecting a multi-touch event may take the place of determination block  in .","When the user moves the second finger  on the touch surface, the coordinates of the averaged virtual touch location P will change correspondingly. At block  the computing device may be configured to obtain the next new virtual touch location (i.e., in the next touch surface refresh), and compare the new virtual touch location to the stored P coordinates at block . At block , the computing device may calculate the change in the location of the current touch event from the stored virtual touch coordinates, and determine an appropriate graphical user interface function that is to be performed at block . For example, the graphical user interface function may be a function to transform a displayed image by a determined transform factor. Examples of graphical user interface transform functions that may be applied include zooming in (i.e., magnifying) an image by a determined amount, zooming out (i.e., de-magnifying) an image by a determined amount, rotating clockwise or counterclockwise by a determined number of degrees, and scanning left, right, up or down by a determined amount. At block , the computing device may then apply the determined graphical user interface function, such as transforming the displayed image or providing a user input to an open application at block . The determined graphical user interface function may vary depending upon the application. For example, the same multi-touch gesture may be related to graphical user interface function that zooms out of an image in a map application, and causes acceleration of a virtual vehicle in a gaming application.","The computing devices of the various aspects may be configured to recognize and process different types of multi-touch gestures as graphical user interface functions. Two commonly used graphical user interface multi-touch gestures include pinch and rotation gestures. The pinch gesture may be associated with a graphical user interface zooming function enabling users to zoom in or out of an image, a file or an application. The rotation gesture may be associated with a graphical user interface rotating function enabling users to rotate images clockwise or counterclockwise, or as a rotational user input as may be understood by game applications.","In some aspects, the multi-touch gesture functionalities may be automatically disabled by an application that employs user interface gestures that might be confused with the multi-touch gestures. In a further aspect, a user may be able to activate and deactivate multi-touch gesture functionalities as part of device user settings based upon a user input (e.g., activation of a virtual button and a configuration menu).","In some aspects, the multi-touch gestures may be manually enabled. To manually enable or activate the multi-touch gestures in an application, a user may select and activate the multi-touch gesture by pressing a button or by activating an icon on a GUI display. For example, the index operation may be assigned to a soft key, which the user may activate (e.g., by pressing or clicking) to launch the multi-touch gesture functionalities. As another example, the multi-touch gesture functionality may be activated by a user command. For example, the user may use a voice command such as \u201cactivate index\u201d to enable the multi-touch gesture functionalities. Once activated, the multi-touch gesture functionalities may be used in the manner described herein.","The multi-touch gesture functionalities may be implemented on any touch surface . A touchscreen display is a particularly useful implementation, given that touchscreens can present a display image that can be manipulated using the multi-touch gestures. In such applications, the user can interact with an image by touching the touchscreen display with two fingers.","When a multi-touch gesture (i.e., the touch of two fingers on the touch surface) is detected, a linked graphical user interface function or gesture function may be activated. The graphical user interface functions linked to, or associated with, the multi-touch gestures may be established in the graphical user interface of the computing device, defined by an application, and\/or set by user preference settings. Some example of such multi-touch gesture graphical user interface functions, and may include zooming or rotating display images as described herein. For example, if the linked function is defined as a zooming function, the computing device processor may zoom in to or out of a display image in response to the user tracing a pinch gesture that satisfies the required parameters.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 3A","b":["100","300","100","102","104","306","102","104","102","302","302","104"]},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 3B","b":["300","300"]},{"@attributes":{"id":"p-0047","num":"0046"},"figref":["FIG. 4A","FIG. 4B"],"b":["100","102","104","102","104","402","104","300"]},"As illustrated in , the computing device  may be configured to recognize and process more than one gesture at a time according to an aspect. For example, a user may use his fingers to trace both a pinch gesture and a rotation gesture by keeping the first finger  stationary on the touch surface , moving the second finger  away from the first finger  in the direction of the dotted line and arrow , and rotating the second finger  clockwise in the direction of the dotted curve and arrow . The dotted line and arrow , and dotted curve and arrow  are shown for illustration purposes only and are not part of the display for according to this aspect. The computing device  may be configured to recognize these gestures and zoom-in to the displayed image while rotating the image in a clockwise direction.","The computing devices  may be configured to perform certain mathematical calculations to recognize and process the multi-touch gestures of the various aspects. Different mathematical formulae may be used in determining the appropriate graphical user interface effects of different multi-touch gestures as they are performed by the user on touch surfaces  of computing devices . Exemplary mathematical formulae are discussed below with reference to .","Some calculations that may be performed when a computing device detects a multi-touch gesture on a non-multi-touch surface are illustrated in . As described above, when a user touches the touch surface , the computing device  may detect the first touch event P and store the coordinates (X, Y) of the first touch in the memory. When the user touches the touch surface a second time at point P (with coordinates X2, Y2), the computing device  will detect that the touch surfaces being touched at a single point P (with coordinates X3, Y3).","The computing device  may be configured to calculate the actual coordinates of P, and the distance between P and P (i.e., distance d), or the distance between P and P (i.e., distance c) using the P and P coordinates. The following mathematical formula may be used to determine the location of P using the known coordinates of P (X1,Y1) and P (X3, Y3):\n\n2=(2*3)\u22121\n\n2=((23\u22121),(23\u22121))\u2003\u2003Formula 1\n","Using the P and P coordinates (i.e., (X1, Y1) and (X3, Y3)), the computing device  may also calculate the distance between each touch location P and P (i.e., distance c), and P and P (i.e., distance d). The following mathematical formula may be used to calculate the distance c:\n\n\u2003\u2003Formula 2\n\n","Another parameter that may be calculated using the P and P coordinates is the angle \u03b8by using the following mathematical formula:\n\n\u03b8=arc Tan()\u2003\u2003Formula 4\n\n","The parameters calculated based on these mathematical formulae may be used to determine the graphical user interface function indicated by a multi-touch gesture, as well as a magnitude to apply in the function (e.g., a zoom factor). For example, depending upon the current application and the nature of the media being displayed, if the distance between P and P increases (i.e., increase in length of c), this may indicate that a pinching gesture is being traced to zoom-in to the displayed image. If the coordinates of P change consistent with a counterclockwise movement of the second finger  with respect to the first finger , this may indicate that a rotating gesture is being traced to rotate the display image in a counterclockwise direction.",{"@attributes":{"id":"p-0055","num":"0063"},"figref":["FIGS. 7A and 7B","FIG. 7A"],"b":["100","2","300","3","300","2","1","1","3","104","502","102","100"]},"The computing device  will detect that the touch surface is being touched at virtual touch location P\u2032, as illustrated in . Using the new P\u2032 coordinates, the computing device may determine the location of the second finger touch P\u2032, and calculate the distance between P and P\u2032 (i.e., c\u2032), or between P and P\u2032 (i.e., d\u2032).","The computing device  may determine that the user traced a pinching gesture by determining whether there has been a change in distance between P stored in memory and P as indicated by the touch surface (i.e., the distance from c to c\u2032), or between P and P (i.e., the distance from d to d\u2032). If the distance between P and P has increased (i.e., c\u2032>c), the computing device  may determine that the user is tracing a pinching-out gesture. However, if the distance between P and P has decreased (i.e., c\u2032<c), the computing device  may determine that the user is tracing a pinching-in gesture. The computing device  may use the mathematical formula 5 to make this determination by calculating the difference between c and c\u2032.","Similarly, if the distance between P stored in memory and P as calculated based upon the current P has increased (i.e., d\u2032>d), the computing device  may determine that the user is tracing a pinching-out gesture. However, if the distance between P and P has decreased (i.e., d\u2032<d), the computing device  may determine that the user is tracing a pinching-in gesture.","It should be noted that while the computing device can calculate the location of the second finger touch (i.e., P), this calculation is not necessary because the multi-touch gestures can be recognized and implemented based solely on the recorded first touch location P and the currently measured touch location P. The change in distance from P to P will be half that of the change in distance from P to P. So, for pinch-type graphical user interface gestures, the transform factor (which is based upon the distance that the two fingers move towards or away from each other) must be increased by a factor of two. However, no adjustment is required for rotational type graphical user interface gestures.","Once the distances c and c\u2032, or d and d\u2032 are calculated, the computing device  may calculate the differences between c and c\u2032, or d and d\u2032 by using the mathematical formula 5. The calculated differences may later be used to determine the transform factors based on which the display image may be transformed. The transform factors are described in more detail below.\n\n\u0394=, or\n\n\u0394=\u2003\u2003Formulae 5\n",{"@attributes":{"id":"p-0061","num":"0069"},"figref":["FIG. 8","FIG. 6"],"b":["300","100","2","300","3","2","104","104","702"],"sub":"1"},"In determining whether the traced movement is a rotating gesture, the computing device  may calculate the angle subtended by the movement from P and P\u2032 by calculating the angle D, by determining the distance between P and P\u2032 (i.e. distance E) or by determining the distance between P and P\u2032 (i.e., distance C\u2032 along the horizontal axis (i.e., the X axis). Any change in the value of D or a change in the distances C or E may signify tracing of a rotating gesture.","The angle D may be calculated using the following mathematical formula:\n\n\u2212\u03b8,\u2003\u2003Formula 6\n\n","The computing device  may also be configured to determine the distance traveled between the virtual touch events P and P\u2032 (i.e., distance C\u2032), and to calculate the change between C and C\u2032. The distance C\u2032 may be calculated using the following mathematical formula:\n\n\u2212(2*cos ),\u2003\u2003Formula 7\n\n","The change in the distance between C and C\u2032 may be calculated using the following mathematical formula:\n\n\u0394=\u2003\u2003Formula 8\n\n","The distance that the second finger  has traveled from P to P\u2032 (i.e., distance E\u2032) along the X axis may also be calculated. The change in the distance between E and E\u2032 may also be calculated using the following mathematical formula:\n\n\u0394=\u2003\u2003Formula 9\n\n","The results of the calculations (i.e., calculating the angle D or the change in the distances C to C\u2032 or E to E\u2032) may be used to determine a transform factor to be applied to the displayed. Transform factors are discussed in more detail below.","The computing device  may also be configured to determine the up and down, and clockwise and counterclockwise directions of a traced gesture using the calculated parameters based on the above formulae. This may be accomplished through simple subtraction of the X and Y coordinate values of the varying P locations. Such values may be of use in different multi-touch gestures that may be developed using the various aspects.","In an aspect, the computing device  may be configured to determine the type of gesture traced by referencing a gestures data table , an example of which is illustrated in . The data table may be organized to include information about the parameter, gesture, and direction that P or P move, and identify a particular display function associated with the parameter values. For example, in row , the computing device may identify a pinch gesture that is used to zoom-in to a display image when the distance c is smaller than distance c\u2032. In another example shown in row , when the angle D is equal to a value greater than zero and the computing device determines that the direction of movement of P or P is counterclockwise the computing device may rotate the display image in a counterclockwise direction. As shown in , other parameters and directions may also be used to determine the type and function of the gesture.","Once the graphical user interface type or transform function of a traced multi-touch gesture is determined, the computing device  may be configured to determine a corresponding transform factor (i.e., \u0394). The transform factor may be a value, such as a number, to be used in the transforming function that the computing device  uses to transform the display image consistent with the user's intent as indicated by the multi-touch gesture. For example, when a user executes a pinch gesture when an image is displayed, the transform function would be magnification and the transform factor would be the degree to which the image is to be magnified. Thus, if the determined transform factor is 10, the computing device would enlarge the displayed image by a factor of 10. Similarly, if the determined transform factor is \u221210, the computing device would reduce the display image by a factor of 10. For rotating transform functions, the transform factor would the number of degrees or radians of rotation to be applied to the image. Thus, in a rotating gesture with a determined transform factor of 10 degrees, the computing device would rotate the display image by 10 degrees in the clockwise direction, for example, whereas if the determined transform factor is \u221210, the computing device would rotate the display image by 10 degrees in the counterclockwise direction.","As mentioned above, the transform factor may be determined based upon the magnitude of changes in touch locations during the multi-touch gesture. Referring now to , in pinch gestures where a zoom function is applied to the display image, the transform factor may be determined based upon the ratio of the distance between the two fingers at the start and at the end of the gesture. For example, the transform factor may be calculated by:\n\n\u0394=\n\nOr, since the change in distance between P and P is half that between P and P, the transform factor may be calculated by:\n\n\u0394=\n","Alternatively, the computing device  may be configured to use a zoom factor look up table to determine the transform factor \u0394 to use based upon a measured or calculated parameter. For example, a zoom factor look-up table may indicate that a change in the P-P separation distance of +1 cm corresponds to a zoom factor of 10%.","Referring now to , rotational transform factors can be simply calculated based on the angle D through which P rotates during the touch gesture or the change in the distances C and\/or E may be used as transform factors. The computing device  may be configured to transform the display image by applying a rotating function that rotates the image by the transform factor in a linear fashion. Thus, if the rotation angle (D) is determined to 30\u00b0 in clockwise direction, the computing device may rotate the display image 30\u00b0 in the clockwise direction. The computing device may also be configured to apply non-linear rotational transform factors, so that the amount of rotation applied to the image per degree of angle distended in the multi-touch gesture increases as the angle of spanned by the gesture increases. For example, a rotational angle D of 5\u00b0 may equate to a 5\u00b0 rotational transformation of the image, while a 15\u00b0 rotation angle D may equate to a 35\u00b0 rotational transformation of the image. Alternatively, the computing device  may be configured to use a table look-up procedure to determine the transform factor to be used based on measured or calculated parameters. For example, change in the distance between C and C\u2032 of 1 cm in the clockwise direction may equate to a 10\u00b0 rotational transformation of an image.","Referring to , in rotating gestures, a transform factor may also be calculated by calculating the \u03b8\/\u03b8, C\/C\u2032 and E\/E\u2032 ratios. The computing device  may use these calculated ratios to determine the transform factor. Such ratio based rotational transform factors may be applied in a linear or non-linear fashion in the rotation transform function applied to an image.","Once the multi-touch gesture is recognized, the computing device  may determine the appropriate graphical user interface function. If the graphical user interface function is a transformation function, the computing device  may apply the appropriate transform function using the determined transform factor, such as to transform the displayed image in ways determined by the current application.","If the multi-touch gesture is determined to be graphical user interface input to an application, the appropriate input may be passed to an open application. For example,  illustrates an aspect in which the multi-touch gestures of the various aspects provide graphical user interface inputs to a driving game application. In this example, the game application may be programmed to translate multi-touch parameters calculated by the computing device  into game function inputs, such as steering and acceleration. For example, in a driving game like that illustrated in , the computing device  may track the change in position of P to determine the steering inputs to apply to a virtual automobile through a windy road. The computing device  may store the location of the first finger  and calculate different parameters (e.g., c and \u03b8) as it detects movements of the virtual touch location caused by movement of the second finger  while the user tries to maneuver the automobile along a road. In this example application, the user may move a second finger  in the direction of the dotted line with arrows  to steer the automobile to the left or right using the virtual steering wheel .","When steering the virtual automobile to the right, for example, the computing device  may detect the movement of virtual touch location P towards the right side of the display  and calculate different parameters (e.g., one or more of the parameters a, b, c, c\u2032, \u03b8, \u03b8, \u03b8, D, C, C\u2032, E, E\u2032), or may the appropriate graphical user interface inputs associated with the movement. The driving application may then use some or all of these parameters, or may use the graphical user interface input derived from these parameters to determine the appropriate game response. For example, the driving application may use the change between \u03b8and \u03b8to determine the direction and amount of steering that is intended by the user. Similarly, the application may calculate the change in distance between fingers (i.e., from c to c\u2032) to determine an acceleration to be applied to the virtual automobile.","Traditionally, touch surfaces  function by transforming a touch event on a touch surface  into an electrical signal that can be interpreted by the computing device  and its application software.  illustrates a hardware\/software architecture of a typical computing device  showing how touch events are communicated to application software. There are different types of touch surfaces  available such as resistive and capacitive touch surfaces. Each of these touch surfaces  may function to detect touch events in a different way. For example, the resistive touch surface panel is composed of several layers, the most important of which are two thin, metallic, electrically conductive layers separated by a narrow gap. When an object, such as a finger, presses down on a point on the panel's outer surface the two metallic layers become connected at that point. The panel then behaves as a pair of voltage dividers with connected outputs. This causes a change in the electrical current which is registered as a touch event and sent to the controller for processing.","A capacitive touch surface panel is a sensor typically made of glass coated with a material such as indium tin oxide. This type of sensor acts like a capacitor in which the plates are the overlapping areas between the horizontal and vertical axes in a grid pattern. Since the human body also conducts electricity, a touch on the surface of the sensor will affect the electric field and create a measurable change in the capacitance of the device. These sensors work on proximity, and do not have to be directly touched to be triggered.","The electrical signals from the touch of the touch surface  can be processed by a hardware driver . The hardware driver  may be circuitry, software, or a mixture of hardware and software, depending upon the particular mobile device . The hardware driver  converts the electrical signal received from the touch surface  into a format that can be interpreted by a software application running on the mobile device . For example, the hardware driver may convert the electrical signals to coordinates of the touch events. This signal may be in the form of an interrupted or stored value in a memory table which is accessible by application software. Such an interrupted or stored value in memory may be received by the operating system (OS) .","In an aspect, the multi-touch functionality described herein may be implemented as part of the operating system , as an auxiliary software application, or as a multi-touch application programming interface (API) . The operating system  or the multi-touch API  may use the data received from the hardware driver  to calculate the parameters used in determining the appropriate multi-touch gesture functionality, such as c, c\u2032, \u03b8, and \u03b8or to transform factors such as c\/c\u2032, \u03b8, \u03b8. Then, using the calculated parameters, the operating system  or the multi-touch API  may pass on to the application layer  appropriate formatting and transforming factors based on detected multi-touch events, such as in the form of a touch event message. Touch events may also be communicated to a user-interface layer , such as to display the value associated with a particular touch event.",{"@attributes":{"id":"p-0082","num":"0107"},"figref":"FIG. 12","b":["1100","1100","1102","300","1104","1106","300","100","1106","300","1124","1102"]},"If the touch event is not a touch up event (i.e., determination =\u201cNo\u201d), the computing device  may determine whether there has been a sudden jump in the touch location greater than a predetermined value \u201cx\u201d at determination block . This may occur when the user touches the screen with a second finger . The distance of the touch location jump may be determined by calculating the distance between the first touch event locations to the location of the next touch event. The predetermined threshold value \u201cx\u201d may be any distance, such as a distance that is greater than the size of an average human finger tip pressed against the touch surface. If the change in position of the touch location is less than the threshold value X (i.e., determination block =\u201cno\u201d), the computing device may perform normal graphical user interface functions at block  before proceeding to receive the next touch event at block .","If the change in position of the touch location is greater than the threshold value X (i.e., determination block =\u201cyes\u201d), the computing device  may store the virtual touch event location at block , and based on the virtual touch event location and the first touch location, calculate and store the location of the second touch event at block . As noted above, the operation of calculating the location of the second touch event is optional, as described below with reference to . The computing device  may obtain the touch event location at block , and determine whether the event is a touch up event at determination block . If it is a touch up event (i.e., determination block =\u201cYes\u201d), the computing device  may perform the normal (i.e., non-multi-touch) graphical user interface functions at block  before proceeding to receive the next touch event at block . If it is not a touch up event (i.e., determination block =\u201cNo\u201d), the computing device  may obtain the new touch event location at block , and calculate the multi-touch gesture parameters as described herein using the stored and new second touch event location data at block . At block  the computing device  may determine a transform function to be applied and a transform factor based on the calculated parameters, and apply the transform function using the transform factor to the displayed image or the currently running application at block . The computing device  may then obtain the next touch event location at block , and repeat the process until a touch up event is detected (i.e., determination block =\u201cYes\u201d).",{"@attributes":{"id":"p-0085","num":"0110"},"figref":"FIG. 13","b":["1200","1200","1102","300","1104","1106","300","100","1106","300","1124","1102"]},"If the touch event is not a touch up event (i.e., determination =\u201cNo\u201d), the computing device  may determine whether there has been a sudden jump in the touch location greater than a predetermined value \u201cx\u201d at determination block . If the change in position of the touch location is less than the threshold value X (i.e., determination block =\u201cno\u201d), the computing device may perform normal graphical user interface functions at block  before proceeding to receive the next touch event at block . If the change in position of the touch location is greater than the threshold value X (i.e., determination block =\u201cyes\u201d), the computing device  may store the averaged virtual touch event location at block . At block  the computing device  may calculate and store various multi-touch gesture parameters described herein related to the virtual touch event location. At block  the computing device  may obtain a new virtual touch event location, and determine whether the event is a touch up event at determination block . If the event is a touch up event (i.e., determination =\u201cYes\u201d), the computing device  may perform the normal graphical user interface functions at block  before proceeding to receive the next touch event at block . If the event is not a touch up event (i.e., determination =\u201cNo\u201d), the computing device  may calculate new multi-touch gesture parameters related to the new touch event at block . At block  the computing device may compare the calculated multi-touch gesture parameters for stored and new virtual touch event locations, and determine a transform function and associated transform factor at block . At block  the computing device  may apply the transform function using the transform factor to the displayed image or an operating application. The computing device  may then obtain the next touch event location at block , and repeat the process until a touch up event is detected (i.e., determination block =\u201cYes\u201d).","The aspects described above may be implemented on any of a variety of computing devices . Typically, such computing devices  will have in common the components illustrated in . For example, the computing devices  may include a processor , coupled to internal memory , and a touch surface input device  or display . The touch surface input device  can be any type of touchscreen display, such as a resistive-sensing touchscreen, capacitive-sensing touchscreen, infrared sensing touchscreen, acoustic\/piezoelectric sensing touchscreen, or the like. The various aspects are not limited to any particular type of touchscreen display or touchpad technology. Additionally, the computing device  may have an antenna  for sending and receiving electromagnetic radiation that is connected to a wireless data link and\/or cellular telephone transceiver  coupled to the processor . Computing devices  which do not include a touch surface input device  (typically including a display ) typically include a key pad  or miniature keyboard, and menu selection keys or rocker switches  which serve as pointing devices. The processor  may further be connected to a wired network interface , such as a universal serial bus (USB) or FireWire\u00ae connector socket, for connecting the processor  to an external touchpad or touch surfaces, or external local area network.","In some implementations, a touch surface can be provided in areas of the computing device  outside of the touch surface  or display . For example, the keypad  can include a touch surface  with buried capacitive touch sensors. In other implementations, the keypad  may be eliminated so the touchscreen display  provides the complete GUI. In yet further implementations, a touch surface  may be an external touchpad that can be connected to the computing device  by means of a cable to a cable connector , or a wireless transceiver (e.g., transceiver ) coupled to the processor .","A number of the aspects described above may also be implemented with any of a variety of computing devices, such as a notebook computer  illustrated in . Such a notebook computer  typically includes a housing  that contains a processor , coupled to volatile memory , and to a large capacity nonvolatile memory, such as a disk drive . The computer  may also include a floppy disc drive  and a compact disc (CD) drive  coupled to the processor . The computer housing  typically also includes a touchpad , keyboard , and the display .","The computing device processor ,  may be any programmable microprocessor, microcomputer or multiple processor chip or chips that can be configured by software instructions (applications) to perform a variety of functions, including the functions of the various aspects described above. In some portable computing devices ,  multiple processors ,  may be provided, such as one processor dedicated to wireless communication functions and one processor dedicated to running other applications. The processor may also be included as part of a communication chipset.","The various aspects may be implemented by a computer processor ,  executing software instructions configured to implement one or more of the described methods or processes. Such software instructions may be stored in memory ,  in hard disc memory , on tangible storage medium or on servers accessible via a network (not shown) as separate applications, or as compiled software implementing an aspect method or process. Further, the software instructions may be stored on any form of tangible processor-readable memory, including: a random access memory , , hard disc memory , a floppy disk (readable in a floppy disc drive ), a compact disc (readable in a CD drive ), electrically erasable\/programmable read only memory (EEPROM), read only memory (such as FLASH memory), and\/or a memory module (not shown) plugged into the computing device , , such as an external memory chip or USB-connectable external memory (e.g., a \u201cflash drive\u201d) plugged into a USB network port. For the purposes of this description, the term memory refers to all memory accessible by the processor ,  including memory within the processor ,  itself.","The foregoing method descriptions and the process flow diagrams are provided merely as illustrative examples and are not intended to require or imply that the processes of the various aspects must be performed in the order presented. As will be appreciated by one of skill in the art the order of blocks and processes in the foregoing aspects may be performed in any order. Words such as \u201cthereafter,\u201d \u201cthen,\u201d \u201cnext,\u201d etc. are not intended to limit the order of the processes; these words are simply used to guide the reader through the description of the methods. Further, any reference to claim elements in the singular, for example, using the articles \u201ca,\u201d \u201can\u201d or \u201cthe\u201d is not to be construed as limiting the element to the singular.","The various illustrative logical blocks, modules, circuits, and algorithm processes described in connection with the aspects disclosed herein may be implemented as electronic hardware, computer software, or combinations of both. To clearly illustrate this interchangeability of hardware and software, various illustrative components, blocks, modules, circuits, and algorithms have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application, but such implementation decisions should not be interpreted as causing a departure from the scope of the present invention.","The hardware used to implement the various illustrative logics, logical blocks, modules, and circuits described in connection with the aspects disclosed herein may be implemented or performed with a general purpose processor, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field programmable gate array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein. A general-purpose processor may be a microprocessor, but, in the alternative, the processor may be any conventional processor, controller, microcontroller, or state machine A processor may also be implemented as a combination of computing devices, e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration. Alternatively, some processes or methods may be performed by circuitry that is specific to a given function.","In one or more exemplary aspects, the functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored on or transmitted over as one or more instructions or code on a computer-readable medium. The processes of a method or algorithm disclosed herein may be embodied in a processor-executable software module executed which may reside on a computer-readable medium. Computer-readable media includes both computer storage media and communication media including any medium that facilitates transfer of a computer program from one place to another. A storage media may be any available media that may be accessed by a computer. By way of example, and not limitation, such computer-readable media may comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, or any other medium that may be used to carry or store desired program code in the form of instructions or data structures and that may be accessed by a computer. Also, any connection is properly termed a computer-readable medium. For example, if the software is transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk, and blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media. Additionally, the operations of a method or algorithm may reside as one or any combination or set of codes and\/or instructions stored on a machine readable medium and\/or computer-readable medium, which may be incorporated into a computer program product.","The foregoing description of the various aspects is provided to enable any person skilled in the art to make or use the present invention. Various modifications to these aspects will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other aspects without departing from the scope of the invention. Thus, the present invention is not intended to be limited to the aspects shown herein, and instead the claims should be accorded the widest scope consistent with the principles and novel features disclosed herein."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The accompanying drawings, which are incorporated herein and constitute part of this specification, illustrate exemplary aspects of the invention. Together with the general description given above and the detailed description given below, the drawings serve to explain features of the invention.",{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIGS. 3A and 3B"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIGS. 4A and 4B"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIGS. 5A and 5B"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIGS. 7A and 7B"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 15"}]},"DETDESC":[{},{}]}
