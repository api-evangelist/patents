---
title: Data clustering method for bayesian data reduction
abstract: This invention is a method of training a mean-field Bayesian data reduction algorithm (BDRA) based classifier which includes using an initial training for determining the best number of levels. The Mean-Field BDRA is then retrained for each point in a target data set and training errors are calculated for each training operation. Cluster candidates are identified as those with multiple points having a common training error. Utilizing these cluster candidates and previously identified clusters as the identified target data, the clusters can be confirmed by comparing a newly calculated training error with the previously calculated common training error for the cluster. The method can be repeated until all cluster candidates are identified and tested.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07587374&OS=07587374&RS=07587374
owner: The United States of America as represented by the Secretary of the Navy
number: 07587374
owner_city: Washington
owner_country: US
publication_date: 20060320
---

{"@attributes":{"id":"description"},"GOVINT":[{},{}],"heading":["STATEMENT OF GOVERNMENT INTEREST","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["The invention described herein may be manufactured and used by or for the Government of the United State of America for governmental purpose without payment of any royalties thereon or therefore.","(1) Field of the Invention","This invention relates to a method for classifying data and more particularly to a training method for a Bayesian Data Reduction Algorithm classifier that enables the identification of data clusters.","Classification systems are a type of artificial intelligence systems that are implemented on digital computers. These systems are implemented using neural networks or statistical measures. Implementation on a neural network involves training the neural network to recognize the given classes. As an example, when given an input XI the classification system decides to which class the input X belongs. If known, measurable characteristics separate classes, the classification decision is straightforward. However, for most applications, such characteristics are unknown, and the classification system must decide which output class does the input X most closely resemble. In such applications, the output classes and their characteristics are modeled (estimated) using statistics for the classes derived from training data belonging to known classes. Thus, the standard classification approach is to first estimate the statistics from the given training data belonging to known classes and then to apply a decision rule using these estimated or modeled statistics.","(2) Description of the Prior Art","In many real world classification problems the domain of the observed data, or features, describing each class can be complicated, obscure, and highly overlapped. The result is that the task of discriminating amongst the classes with standard supervised training techniques can be nearly impossible. However, within these difficult domains, it can often be the case that the target class of interest (e.g., data that produce a desired yield and are thus categorized as the target class) contains isolated unknown clusters (subgroups of data points), where the observations within each cluster have similar statistical properties. In these situations classification performance (or, the average yield) can be significantly improved if one develops a classifier to recognize, or mine, observations within the clusters as the target class, and where all other nonclustered observations (i.e., both with and without a desired yield) are considered the alternative class (the non-target class). A benefit of such a classifier is that subsets of target data points, producing a consistent desired average yield, can be recognized with a minimum probability of error. This is in contrast to a traditional classification approach to this problem (i.e., trained in a completely supervised manner) that has the potential to produce a much higher probability of error and a lower average yield.","Bayesian networks, also known as belief networks are known in the art for use as filtering systems. The belief network is initially learned by the system from data provided by an expert, user data and user preference data. The belief network is relearned when additional attributes are identified having an effect. The belief network can then be accessed to predict the effect.","These benefits can be achieved in diverse fields having multi-dimensional data. Large quantities of data are available in the securities market, and it would be valuable to find groups of securities having predefined characteristics such as a certain yield from the available data. Other fields for using such a classification system are target identification, medical diagnosis, speech recognition, digital communications and quality control systems.","Classification systems are a type of artificial intelligence systems that are implemented on digital computers. These systems are implemented using neural networks or statistical measures. Implementation on a neural network involves training the neural network to recognize the given classes. As an example, when given an input X, the classification system decides to which class the input X belongs. If known, measurable characteristics separate classes, the classification decision is straightforward. However, for most applications, such characteristics are unknown, and the classification system must decide which output class does the input X most closely resemble. In such applications, the output classes and their characteristics are modeled (estimated) using statistics for the classes derived from training data belonging to known classes. Thus, the standard classification approach is to first estimate the statistics from the given training data belonging to known classes and then to apply a decision rule using these estimated or modeled statistics.",{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1A","b":["10","12","14"]},"Bayesian networks, also known as belief networks are known in the art for use as filtering systems. The belief network is initially learned by the system from data provided by an expert, user data and user preference data. The belief network is relearned when additional attributes are identified having an effect. The belief network can then be accessed to predict the effect.","In this case, the ordinate that defines the yield of each data point is plotted versus the domain, where a yield value of 0.5 is used to separate and define the five hundred samples of the target class (i.e., yield>0.5), and the five hundred samples of the non-target class (yield<0.5). It can clearly be seen in this figure that the two classes are highly overlapped with respect to the range of the single feature. In fact, later it will be shown that traditional supervised classification approaches with this data produce nearly a 0.5 probability of error, and an overall average yield of just slightly more than 0.5. However, notice in  that a cluster  of data points also exists in the target class  with an average yield of approximately 0.6. Thus, it would be advantageous to develop a classifier for this data that can essentially mine and recognize the positive yielding cluster  from all other data points contained in . One obvious technique to classify the cluster point in this data would be to visually determine threshold points from ; however, typical problems involve multi-dimensional feature spaces that prevent visual determination of thresholds. Any developed technique should be applicable to multi-dimensional feature spaces.",{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 1B","FIG. 1B","FIG. 1B"],"b":["10","12"]},"It can clearly be seen that the two classes contain many commonly distributed points with respect to the range of the single feature. This case differs from the case shown in  in that three clusters of data points, A, B and C, exist within the target class containing actual respective yields of 0.6, 0.75, and 0.9. In this example, each data cluster was randomly placed to be centered somewhere between the yield values of 0.5 and 1, where, as stated previously, the focus of the general embodiment of the method is on mining each of these clusters.","Prior art methods for classifying data are provided in U.S. Pat. Nos. 6,397,200 and 6,789,070. These are incorporated by reference herein. U.S. Pat. No. 6,397,200 provides a data reduction method for a classification system using quantized feature vectors for each class with a plurality of features and levels. The method utilizes application of a Bayesian data reduction algorithm to the classification system for developing reduced feature vectors. Test data is then quantified into the reduced feature vectors. The reduced classification system is then tested using the quantized test data. A Bayesian data reduction algorithm is further provided by computing an initial probability of error for the classification system. Adjacent levels are merged for each feature in the quantized feature vectors. Level-based probabilities of error are then calculated for these merged levels among the plurality of features. The system then selects and applies the merged adjacent levels having the minimum level based probability of error to create an intermediate classification system. Steps of merging, selecting and applying are performed until either the probability of error stops improving or the features and levels are incapable of further reduction.","U.S. Pat. No. 6,789,070 provides an automatic feature selection system for test data with data (including the test data and\/or the training data) containing missing values in order to improve classifier performance. The missing features for such data are selected in one of two ways: the first approach assumes each missing feature is uniformly distributed over its range of values, and the second approach increases the number of discrete levels for each feature by one for the missing features. These two choices modify the Bayesian Data Reduction Algorithm for automatic feature selection.","This method for solving the problem in  builds upon and utilizes the previously introduced Mean-Field Bayesian Data Reduction Algorithm (Mean-Field BDRA) based classifier. The Mean-Field BDRA classifier was developed to mitigate the effects of the curse of dimensionality by eliminating irrelevant feature information in the training data (i.e., lowering M), while simultaneously dealing with the missing feature information problem. The mean-field BDRA was first introduced in R. S. Lynch, Jr. and P. K. Willett, \u201cAdaptive Classification by Maximizing the Class Separability with Respect to the Unlabeled Data,\u201d Proceedings of the 2003 SPIE Symposium on Security and Defense, Orlando, Fla., April 2003. This paper discloses a method of Bayesian Data Reduction which assigns an assumed uniform Dirichlet (completely non-informative) prior for the symbol probabilities of each class. In other words, the Dirichlet is used to model the situation in which the true probabilistic structure of each class is unknown and has to be inferred from the training data.","The Modified Mean-Field BDRA was developed to better deal with problems in which the class-labeling feature is the primary missing attribute in the training data. In general, this problem greatly complicates the modeling of each class, and to deal with it the mean-field BDRA was created that encourages dissimilar distributions with respect to all missing value data.","The primary aspect of the Mean-Field BDRA (that is, in addition to its data model that incorporates a class-labeling feature) that differentiates it from the original BDRA is its method of dealing with the missing features problem. In the Mean-Field BDRA the missing feature information is adapted by estimating the missing feature from the available training data. The following model provides further detail. Specifically, let z be an N-dimensional vector containing the entire collection of training data for all k classes, and using the Dirichlet distribution based model, this is written as",{"@attributes":{"id":"p-0021","num":"0020"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"f","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mi":"z"}},{"msubsup":{"mo":"\u222b","mi":"p","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mo":["[","]"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"\u2208","msub":{"mi":["w","i"]}},"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["p","l"]}}},{"mo":["(",")"],"mi":"p"},{"mo":"\u2146","mi":"p"}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mi":"f","mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}}]}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}},"br":{},"sub":"1 ","sup":"th "},{"@attributes":{"id":"p-0022","num":"0021"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mi":"M","mo":"-","mn":"1"}},"mo":"!"},"mo":"\u2062","msub":{"mi":"I","mrow":{"mrow":{"mo":["{","}"],"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","msub":{"mi":["p","l"]}},"mo":"=","mn":"1"}},"mo":","}}}},{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"\u2062","mi":"A"}}}]}}}},"br":{},"sub":["i ","i ","{x}"]},"Equation (1) represents the optimal approach to solving this problem. However, when expanded, and after integration, Equation (1) results in a sum of products whose number of terms depends upon the number of missing features in the data. That is, there are",{"@attributes":{"id":"p-0024","num":"0023"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["w","i"]}}}}},"br":{},"sub":"i","sup":["th ","20"]},"As an alternative to Equation (1), the distribution contained in it, f(z|p), and given by",{"@attributes":{"id":"p-0026","num":"0025"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"f","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mrow":{"mi":["z","p"],"mo":"|"}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["\u230a","\u230b"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"l","mo":"\u2208","msub":{"mi":["w","i"]}},"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["p","l"]}}}}],"mo":"="}}},"br":{}},{"@attributes":{"id":"p-0027","num":"0026"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"f","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mrow":{"mi":["z","p"],"mo":"|"}}},{"munderover":{"mo":"\u220f","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"M"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":["p","j"],"msub":{"mi":["x","j"]}}}],"mo":"="}}},"br":{}},{"@attributes":{"id":"p-0028","num":"0027"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["x","j"]},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":"\u03b2","mrow":{"mi":["i","j"],"mo":","}},{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mi":"j","mo":"\u2208","msub":{"mi":["w","i"]}}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},{"mrow":{"mrow":{"mo":["(",")"],"mi":"i"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"msub":{"mi":"\u03b2","mrow":{"mi":["i","j"],"mo":","}},"mi":["iff","j"]},"mo":"\u2209","msub":{"mi":["w","i"]}},{"mrow":{"mi":"and","mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":[{"mo":["(",")"],"mi":"ii"},{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"M"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"\u03b2","mrow":{"mi":["i","j"],"mo":","}}}]},"mo":"=","mn":"1."}],"mo":[",",","]}}},"br":{}},"In general, under mean-field theory the expectation E(f(x)) is replaced by f(E(x)). Thus, identifying \u201cf(x)\u201d as a particular term in the sum of products in Equation (1), meaning a particular configuration of the actual symbols of the symbol-uncertain data, the expected value of this data is added to the appropriate symbol's total number of observations. To accomplish this, the following iterative steps are used (these steps will be referred to as the mean-field recursion):","(i) Begin with",{"@attributes":{"id":"p-0030","num":"0029"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"n","mo":"=","mn":"1"},{"msubsup":{"mi":"\u03b2","mrow":[{"mi":["i","j"],"mo":","},{"mo":["(",")"],"mn":"1"}]},"mo":"=","mrow":{"mn":"0","mo":"\u2062","mrow":{"mo":"\u2200","mrow":{"mi":"j","mo":"\u2209","msub":{"mi":["w","i"]}}}}},{"mrow":[{"mi":"and","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msubsup":{"mi":"\u03b2","mrow":[{"mi":["i","j"],"mo":","},{"mo":["(",")"],"mn":"1"}]}},{"msub":{"mi":"\u03c0","mrow":{"mi":["i","j"],"mo":","}},"mo":"\u2062","mrow":{"mo":"\u2200","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":["w","i"]}}}}],"mo":"="}],"mo":[",",","]}}},"br":{},"sup":"th "},{"@attributes":{"id":"p-0031","num":"0030"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"\u03c0","mrow":{"mi":["i","j"],"mo":","}},"mo":"=","mrow":{"mfrac":{"mn":"1","mrow":{"mo":["\uf603","\uf604"],"msub":{"mi":["w","i"]}}},"mo":"."}}}},"br":{}},{"@attributes":{"id":"p-0032","num":"0031"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mi":"\u03b2","mrow":[{"mi":["i","j"],"mo":","},{"mo":["(",")"],"mrow":{"mi":"n","mo":"+","mn":"1"}}]},"mo":"=","mrow":{"mn":"0","mo":"\u2062","mrow":{"mo":"\u2200","mrow":{"mi":"j","mo":"\u2209","msub":{"mi":["w","i"]}}}}}}},"br":{}},{"@attributes":{"id":"p-0033","num":"0032"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mi":"\u03b2","mrow":[{"mi":["i","j"],"mo":","},{"mo":["(",")"],"mrow":{"mi":"n","mo":"+","mn":"1"}}]},"mo":"=","mrow":{"mfrac":{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","mrow":{"munderover":{"mo":"\u2211","mrow":{"mrow":[{"mi":"l","mo":"=","mn":"1"},{"mi":["l","i"],"mo":"\u2260"}],"mo":","},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":"\u03b2","mrow":[{"mi":["i","j"],"mo":","},{"mo":["(",")"],"mi":"n"}]}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"\u03c0","mrow":{"mi":["i","j"],"mo":","}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":["w","i"]}},"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"+","mrow":{"munderover":{"mo":"\u2211","mrow":{"mrow":[{"mi":"l","mo":"=","mn":"1"},{"mi":["l","i"],"mo":"\u2260"}],"mo":","},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msubsup":{"mi":"\u03b2","mrow":[{"mi":["i","j"],"mo":","},{"mo":["(",")"],"mi":"n"}]}}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"\u03c0","mrow":{"mi":["i","j"],"mo":","}}}}}]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":"\u2200","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":["w","i"]}}}}}}},"br":{}},{"@attributes":{"id":"p-0034","num":"0033"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"msubsup":[{"mi":"\u03b2","mrow":[{"mi":["i","j"],"mo":","},{"mo":["(",")"],"mrow":{"mi":"n","mo":"+","mn":"1"}}]},{"mi":"\u03b2","mrow":[{"mi":["i","j"],"mo":","},{"mo":["(",")"],"mi":"n"}]}],"mo":"-"}},"mn":"2"}}},{"mo":["(",")"],"mi":"Tolerance"}],"mo":">"}}},"br":{}},"At convergence,",{"@attributes":{"id":"p-0036","num":"0035"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["x","j"]},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","msubsup":{"mi":"\u03b2","mrow":[{"mi":["i","j"],"mo":","},{"mo":["(",")"],"mi":"n"}]}}}}},"br":{},"sup":["th ","th ","th "],"sub":"i,j"},"Notice that steps (i) through (iii) shown above are similar to the recursive steps utilized in the Expectation Maximization (EM) algorithm. A typical implementation of EM involves using the available data to estimate, or \u201cplug-in,\u201d the components of a Gaussian mixture density. However, the recursive steps, above, involve estimation of the \u03b2's for an algorithm that is approximately Bayesian. In any case, as the EM algorithm has been shown to converge to a solution, it is expected that due to its similar form, the Mean-Field BDRA will also converge.","In seeking best performance for a given data set the dimensionality reduction steps of the BDRA are used after each application of the mean-field recursion described above. That is, the Mean-Field BDRA alternates between reducing irrelevant feature information and \u201cfilling-in\u201d missing feature values. The steps of the basic BDRA have been modified to include a class-labeling feature in augmentation to each datum. Recall, the algorithm reduces the quantization complexity to the level that minimizes the average conditional probability of error, P(e|X), and in its modified form it appears as",{"@attributes":{"id":"p-0039","num":"0038"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["e","X"],"mo":"\u2758"}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","mrow":{"munder":{"mo":"\u2211","mi":"y"},"mo":"\u2062","mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["H","k"]}}},"mo":["\u2062","\u2062"],"msub":[{"mi":"I","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"msub":[{"mi":["f","k"]},{"mi":["f","l"]}],"mo":"\u2264"},{"mrow":{"mi":["for","all","k"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]},"mo":"\u2260","mi":"l"}],"mo":","}}},{"mi":["f","k"]}]}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{}},{"@attributes":{"id":"p-0040","num":"0039"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["f","k"]},"mo":"=","mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"y","mo":"\u2758","msub":{"mi":["x","k"]}},"mo":",","msub":{"mi":["H","k"]}}}},{"mfrac":{"mrow":[{"mrow":[{"msub":{"mi":["N","y"]},"mo":"!"},{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["N","k"]},"mo":["+","-"],"mi":"M","mn":"1"}},"mo":"!"}],"mo":"\u2062"},{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["N","k"]},{"mi":["N","y"]}],"mo":["+","+","-"],"mi":"M","mn":"1"}},"mo":"!"}]},"mo":"\u2062","mrow":{"munder":{"mo":"\u220f","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":["H","k"]}}},"mo":"\u2062","mfrac":{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["x","j"]},{"mi":["y","j"]}],"mo":"+"}},"mo":"!"},{"mrow":[{"msub":{"mi":["x","j"]},"mo":"!"},{"msub":{"mi":["y","j"]},"mo":"!"}],"mo":"\u2062"}]}}}],"mo":"="}},"mo":";"}}},"br":[{},{},{},{}],"sub":["k ","k ","y","all j\u03b5H",{"sub2":"k"}]},{"@attributes":{"id":"p-0041","num":"0040"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mo":["{","}"],"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":["H","k"]}},"mi":"M"},"mo":"\u2062","msub":{"mi":["p","j"]}}},"mo":"=","mn":"1"}},"mo":";"}}},"br":[{},{}],"sub":["j","k ","k"],"sup":"th "},{"@attributes":{"id":"p-0042","num":"0041"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"N","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mi":"N","mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","msub":{"mi":["x","j"]}}}}}}},"br":{},"sup":"th "},{"@attributes":{"id":"p-0043","num":"0042"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mo":["{","}"],"mrow":{"msub":{"mi":["N","k"]},"mo":"=","mrow":{"munder":{"mo":"\u2211","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":["H","k"]}}},"mo":"\u2062","msub":{"mi":["x","j"]}}}},"mo":";"}}},"br":{},"sub":"j ","sup":"th "},{"@attributes":{"id":"p-0044","num":"0043"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["N","y"]},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"msub":{"mi":["N","y"]},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","msub":{"mi":["y","j"]}}}}}}},"br":[{},{},{}],"sub":["{x}","{x}","{x}","y","k"]},{"@attributes":{"id":"p-0045","num":"0044"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":"=","mrow":{"mn":"1","mo":"|","mi":"x"}},"mo":",","msub":{"mi":["H","k"]}}}},{"mfrac":{"mrow":[{"msub":{"mi":"x","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":["H","k"]}}},"mo":"+","mn":"1"},{"mi":["N","M"],"mo":"+"}]},"mo":"."}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}}},"Given the above equations, dimensionality reduction (i.e., feature selection) is implemented on the training data using the following iterative steps, which are analogous to backward sequential feature selection.","(i) Apply mean-field recursive steps to the data.","(ii) Using the initial training data with quantization complexity M (e.g., in the case of all binary valued features M=2, where Nis the number of features), Equation (2) is used to compute P(e|X;M).","(iii) Beginning with the first feature (selection is arbitrary), and excluding the class labeling feature, reduce this feature by summing or merging (i.e., marginalizing) the numbers of occurrences of those quantized symbols that correspond to joining adjacent discrete levels of that feature.\n\n(iv) Re-apply mean-field recursive steps to the data.\n\n(v) Use the newly merged training data (it is referred to as X\u2032) and the new quantization complexity (e.g., M\u2032=2in the binary feature case), and use Equation (2) to compute P(e|X\u2032;M\u2032).\n\n(vi) Repeat items (iii), (iv) and (v) for all Nfeatures.\n\n(vii) From item (vi) select the minimum of all computed P(e|X\u2032;M\u2032) (in the event of a tie use an arbitrary selection), and choose this as the new training data configuration. (This corresponds to permanently reducing, or removing, the associated feature.)\n\n(viii) Repeat items (iii) through (vii) until the probability of error does not decrease any further, or until M\u2032=2, at which point the final quantization complexity has been found.\n","The Mean-Field BDRA is modified in this section to improve its performance. Its performance is particularly improved when the adapted training data is missing the class labeling feature. The idea behind the method of the current invention is based on developing a model that encourages dissimilar distributions amongst the classes with respect to all missing feature information. Therefore, given the missing feature values, the new method is designed to give more likelihood to those feature vectors that have dissimilar values.","The modified Mean-Field BDRA is based on the assumptions that the distribution of the true discrete symbol probabilities, (p), for the idiscrete symbol of the kclass, are uniformly Dirichlet distributed, and that the form of the underlying new distributional model is given by,",{"@attributes":{"id":"p-0050","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":"p","mrow":{"mn":"1","mo":",","mi":"i"}},{"mi":"p","mrow":{"mn":"2","mo":",","mi":"i"}}],"mo":[",",",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"msub":[{"mi":"p","mrow":{"mi":["c","i"],"mo":","}},{"mi":["p","i"]}],"mo":"|"}}}},{"mfrac":{"mi":"K","msub":{"mi":["p","i"]}},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"msup":[{"mrow":[{"mo":["(",")"],"mfrac":{"msub":[{"mi":"p","mrow":{"mn":"1","mo":",","mi":"i"}},{"mi":["p","i"]}]}},{"mi":"\u03b1","mo":"-","mn":"1"}]},{"mrow":[{"mo":["(",")"],"mfrac":{"msub":[{"mi":"p","mrow":{"mn":"2","mo":",","mi":"i"}},{"mi":["p","i"]}]}},{"mi":"\u03b1","mo":"-","mn":"1"}]},{"mrow":[{"mo":["(",")"],"mfrac":{"msub":[{"mi":"p","mrow":{"mi":["c","i"],"mo":","}},{"mi":["p","i"]}]}},{"mi":"\u03b1","mo":"-","mn":"1"}]}],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{}},{"@attributes":{"id":"p-0051","num":"0050"},"maths":{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","msub":{"mi":"p","mrow":{"mi":["k","i"],"mo":","}}},"mo":"=","msub":{"mi":["p","i"]}},"mo":","}}},"br":{}},"Given Equation (4), Equation (3) is now redeveloped by writing it as,",{"@attributes":{"id":"p-0053","num":"0052"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":"=","mrow":{"mn":"1","mo":"|","mi":"x"}},"mo":",","msub":{"mi":["H","k"]}}}},{"mrow":[{"msubsup":{"mo":"\u222b","mn":["0","1"]},"mo":"\u2062","mrow":{"msubsup":{"mo":"\u222b","mn":"0","msub":{"mi":["P","i"]}},"mo":"\u2062","mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["y","i"]},"mo":"=","mn":"1"},{"mrow":{"msub":[{"mi":["p","i"]},{"mi":"p","mrow":{"mi":["k","i"],"mo":","}}],"mo":"\u2062"},"mo":"|","mi":"x"}],"mo":[",",","],"msub":{"mi":["H","k"]}}}},{"mo":"\u2146","msub":{"mi":["p","i"]}},{"mo":"\u2146","msub":{"mi":"p","mrow":{"mi":["k","i"],"mo":","}}}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}},{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}}]}}},{"msubsup":{"mo":"\u222b","mn":["0","1"]},"mo":"\u2062","mrow":{"msubsup":{"mo":"\u222b","mn":"0","msub":{"mi":["P","i"]}},"mo":"\u2062","mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":"=","mrow":{"mn":"1","mo":"|","mrow":{"msub":[{"mi":["p","i"]},{"mi":"p","mrow":{"mi":["k","i"],"mo":","}}],"mo":"\u2062"}}},"mo":[",",","],"mi":"x","msub":{"mi":["H","k"]}}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"p","mrow":{"mi":["k","i"],"mo":","}},"mo":[",",","],"mrow":[{"mo":"|","msub":{"mi":["p","i"]}},{"mi":"x","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["H","k"]}}]}}},{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["p","i"]},"mo":"|","mi":"x"},"mo":",","msub":{"mi":["H","k"]}}}},{"mo":"\u2146","msub":{"mi":["p","i"]}},{"mo":"\u2146","msub":{"mi":"p","mrow":{"mi":["k","i"],"mo":","}}}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}},{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}}]}}}],"mo":"="}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}},"br":[{},{},{}],"in-line-formulae":[{},{}],"i":["f","y","|x,H","f","y","|p",",H","f","x|p",",H","f","p","|p","H","f","p","|x,H","dp","dp"],"sub":["i","k","0","0","i","k,i","k","k,i","k","k,i","i","k","i","k","i","k,i"],"sup":["1","p",{"sub2":"i"}]},{"@attributes":{"id":"p-0054","num":"0053"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mrow":{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":"=","mrow":{"mn":"1","mo":"\u2758","msub":{"mi":"p","mrow":{"mi":["k","i"],"mo":","}}}},"mo":",","msub":{"mi":["H","k"]}}}},"mo":"=","msub":{"mi":"p","mrow":{"mi":["k","i"],"mo":","}}},{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"x","mo":"|","msub":{"mi":"p","mrow":{"mi":["k","i"],"mo":","}}},"mo":",","msub":{"mi":["H","k"]}}}},{"mrow":{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","msub":{"mi":"x","mrow":{"mrow":{"mi":"j","mo":"\u2208","msub":{"mi":["H","j"]}},"mo":",","mi":"i"}}}}},{"mtd":{"msub":{"mi":"x","mrow":{"mrow":{"mi":["j","\u03b5"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":"H","mrow":{"mi":["k","i"],"mo":","}}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}}}}}]}},"mo":["\u2062","\u2062"],"msup":[{"mrow":{"mo":["(",")"],"mfrac":{"msub":[{"mi":"p","mrow":{"mi":["k","i"],"mo":","}},{"mi":["p","i"]}]}},"msub":{"mi":"x","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":"H","mrow":{"mi":["k","i"],"mo":","}}}}},{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mfrac":{"msub":[{"mi":"p","mrow":{"mi":["k","i"],"mo":","}},{"mi":["p","i"]}]}}},{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","msub":{"mi":"x","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":"H","mrow":{"mi":["k","i"],"mo":","}}}}},"mo":"\u2062","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}]}]}],"mo":"="},{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":"p","mrow":{"mi":["k","i"],"mo":","}},{"mi":["p","i"]}],"mo":"|"},"mo":",","msub":{"mi":["H","k"]}}}},{"mfrac":{"mn":"1","msub":{"mi":["p","i"]}},"mo":["\u2062","\u2062"],"msup":[{"mrow":[{"mo":["(",")"],"mfrac":{"msub":[{"mi":"p","mrow":{"mi":["k","i"],"mo":","}},{"mi":["p","i"]}]}},{"mi":"\u03b1","mo":"-","mn":"1"}]},{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mfrac":{"msub":[{"mi":"p","mrow":{"mi":["k","i"],"mo":","}},{"mi":["p","i"]}]}}},{"mi":"\u03b1","mo":"-","mn":"1"}]}]}],"mo":"="},{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["p","i"]},"mo":"|","mi":"x"},"mo":",","msub":{"mi":["H","k"]}}}},{"mfrac":{"mrow":[{"mrow":{"mi":"\u0393","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["N","M"],"mo":"+"}}},"mo":"\u2062","msup":{"mrow":[{"msubsup":{"mi":["p","i"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","msub":{"mi":"x","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":"H","mrow":{"mi":["j","i"],"mo":","}}}}}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":["p","i"]}}}},{"mi":["N","M"],"mo":["-","+","-"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"C"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"x","mrow":{"mi":"j","mo":"\u2208","msub":{"mi":"H","mrow":{"mi":["j","i"],"mo":","}}}}},"mn":"2"}]}},{"mrow":[{"mi":"\u0393","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"c"},"mo":"\u2062","msub":{"mi":"x","mrow":{"mi":["j","\u03b5"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":"H","mrow":{"mi":["j","i"],"mo":","}}}}},"mo":"+","mn":"1"}}},{"mi":"\u0393","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["N","M"],"mo":["-","+","-"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","msub":{"mi":"x","mrow":{"mi":["j","\u03b5"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":"H","mrow":{"mi":["j","i"],"mo":","}}}}},"mn":"1"}}}],"mo":"\u2062"}]},"mo":"."}],"mo":"="}],"mo":[";",";",";"]}}}},"Using these equations, Equation (6) can now be solved, which produces the result,",{"@attributes":{"id":"p-0056","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","i"]},"mo":"=","mrow":{"mn":"1","mo":"|","mi":"x"}},"mo":",","msub":{"mi":["H","k"]}}}},{"mfrac":{"mrow":[{"mrow":[{"mi":"\u0393","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"x","mrow":{"mi":["j","\u03b5"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":"H","mrow":{"mi":["k","i"],"mo":","}}}},"mo":["+","+"],"mi":"\u03b1","mn":"1"}}},{"mi":"\u0393","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","msub":{"mi":"x","mrow":{"mi":"j","mo":["\u2062","\u2209","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":"H","mrow":{"mi":["k","i"],"mo":","}}}}},"mo":"+","mi":"\u03b1"}}},{"mi":"\u0393","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","msub":{"mi":"x","mrow":{"mi":["j","\u03b5"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":"H","mrow":{"mi":["j","i"],"mo":","}}}}},"mo":"+","mn":"2"}}}],"mo":["\u2062","\u2062"]},{"mrow":[{"mi":"\u0393","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["N","M"],"mo":"+"}}},{"mi":"\u0393","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"x","mrow":{"mi":["j","\u03b5"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":"H","mrow":{"mi":["k","i"],"mo":","}}}},"mo":"+","mn":"1"}}},{"mi":"\u0393","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","msub":{"mi":"x","mrow":{"mi":"j","mo":["\u2209","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":"H","mrow":{"mi":["k","i"],"mo":","}}}}},"mo":"+","mn":"1"}}},{"mi":"\u0393","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","msub":{"mi":"x","mrow":{"mi":["j","\u03b5"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":"H","mrow":{"mi":["j","i"],"mo":","}}}}},{"mn":"2","mo":"\u2062","mi":"\u03b1"}],"mo":["+","+"],"mn":"1"}}}],"mo":["\u2062","\u2062","\u2062"]}]},"mo":"."}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":{}},{"@attributes":{"id":"p-0057","num":"0056"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["y","i"]},"mo":"=","mrow":{"mn":"1","mo":"|","mi":"x"}},{"msub":{"mi":["H","k"]},"mo":";","mrow":{"mi":"\u03b1","mo":"=","mn":"1"}}],"mo":","}}},{"mfrac":{"mrow":[{"msub":{"mi":"x","mrow":{"mi":["j","\u03b5"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":"H","mrow":{"mi":["k","i"],"mo":","}}}},"mo":"+","mn":"1"},{"mrow":[{"mo":["(",")"],"mrow":{"mi":["N","M"],"mo":"+"}},{"mo":["(",")"],"mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"C"},"mo":"\u2062","msub":{"mi":"x","mrow":{"mi":["j","\u03b5"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":{"mi":"H","mrow":{"mi":["j","i"],"mo":","}}}}},"mo":"+","mn":"2"}}],"mo":"\u2062"}]},"mo":"."}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}},"br":{}},"The prior art does not disclose a method for training a Mean-Field Bayesian Reduction classifier for detecting clusters in unknown data.","Accordingly, this invention is a method of training a mean-field Bayesian data reduction algorithm (BDRA) which includes using an initial training for determining the best number of levels. The Mean-Field BDRA is then retrained for each point in a target data set and training errors are calculated for each training operation. Cluster candidates are identified as those with multiple points having a common training error. Utilizing these cluster candidates and previously identified clusters as the identified target data, the clusters can be confirmed by comparing a newly calculated training error with the previously calculated common training error for the cluster. The method can be repeated until all cluster candidates are identified and tested.","These and other features, aspects and advantages of the present invention will become better understood with reference to the following drawings, description and claims.","The modified version of the Mean-Field BDRA disclosed in the prior art is used as the basis for a new method to solve the problem shown in  because of its superior performance with difficult unsupervised training situations. To further develop the new technique, a new training method is developed for the modified algorithm that enables it to mine the domain of an unlabeled set of data points for clusters. The new training method utilizes a combination of unsupervised and supervised training, and a sequential data search to localize all points within the cluster. In general, all results shown for the methods developed here with the Mean-Field BDRA will be based on simulated data like that shown in . However, this approach is equally applicable to real-life data sets.","Typical domain data can have any number of features so that a given cluster may exist across all dimensions of the feature space, or across some subset of features within that space. Thus, the built-in dimensionality reduction aspects of the Mean-Field BDRA are useful for isolating the data cluster. Further, as the Mean-Field BDRA is a discrete classifier, it naturally defines threshold points in multi-dimensional space that isolate the relative location of the cluster.","The automatic algorithm developed here to locate data clusters strongly relies on the Mean-Field BDRA's training metric, P(e). This is given above as equation (2), above. Using this, quantization complexity is reduced to the level that minimizes the average conditional probability of error, P(e|X).","The idea is that because the Mean-Field BDRA discretizes all multi-dimensional feature data into quantized cells (or levels), any data points that are common to a cluster  will share the same discrete cell, which also assumes that appropriately defined quantization thresholds have been determined by the Mean-Field BDRA. These are shown as dashed lines  in . Therefore, given all, or most, cluster data points can be quantized to share a common discretized cell they will all also share a common probability of error metric, P(e).","In other words, locating cluster data points can be based on developing a searching method that looks for data points sharing a common P(e). In this case, it is expected that this common P(e) value, for all points within the cluster , will be relatively small with respect to that computed for most other data points outside of the cluster. This latter requirement should be satisfied in most situations as data clusters should tend to be distributed differently with respect to data outside of the cluster. As a final step in training, the validity of each cluster can be checked by computing the overall average yield for all points within the cluster (i.e., any grouped data points producing the largest average yield are chosen as appropriately mined data clusters).","To improve results, the steps shown in  and described below have been developed for training the Mean-Field BDRA, that is, in such a way that the data cluster is identified with a minimum probability of error. For each of these steps training will proceed in a semi-unsupervised manner in that all target data (yield>0.5) identified as  is utilized without class labels (i.e., no class information at all), and all non-target data (yield<0.5) identified as  is utilized with class labels (full class information). The motivation for training in this way is to force the Mean-Field BDRA to readily recognize the contrast between target cluster data points  and all other data points in both classes  and  that are not like the cluster. In this way, when adapting class labels for the target class the Mean-Field BDRA is more likely to label cluster data points as target, while grouping most other non-cluster target data points with the non-target. The new method of training proceeds with the following steps.","Initially, a maximum number of levels should be set as in step . A user can provide this maximum based on the available amount of computing resources, the time required for completion or by some characteristic of the data. This maximum can also be computed from these attributes. In step , using all available training data (i.e., with all target points unlabeled and all non-target points labeled), the Mean-Field BDRA is separately trained for each level. The levels are shown for illustrative purposes in  by dashed lines . After training the Mean-Field BDRA, the training error is computed in step . The number of levels is incremented in step . Step  continues the process until the maximum number of levels is reached. From the separate training runs, the method chooses the initial number of discrete levels to use for each feature as that producing the least training error (see Equation (2), above) in step . In the next stage of training, the Mean-Field BDRA is trained for each target data point. In step , a target data point is labeled with the correct target label. The remaining data points are unlabeled in step . The Mean-Field BDRA is trained in step  with this labeling, and training error is computed for each point in step . Step  proceeds to the next point, while step  loops through the routine until the Mean-Field BDRA has been separately trained for each training data point. Notice that these steps produce a set of Ncomputed training errors equal to the number of target training data points.","The next group of steps is utilized to identify the clusters from the computed training errors. Target data points are sorted by training error and grouped in step . Step  chooses all data points that have both the smallest common training error and the most number of data points from the set of Ncomputed training errors. These data points are candidate cluster data points and are accepted or rejected, for example, with the problem of , by checking the commonality of associated yield values in step .","Notice that it is possible that in some problems multiple data clusters can be found in this way. That is, if more than one candidate cluster appears to have points with more than one minimum error probability value. In this case, data points common to each cluster can be grouped according to accepted yield values.","As a final step, in step , the training is finished by refining the computed number of levels. In this step, all cluster data points found in step  are labeled as target, and all other target data points are unlabeled. The Mean-Field BDRA is then retrained to recognize this data. This step fine-tunes the best initial number of discrete levels to use for each feature by the Mean-Field BDRA.","To extend the idea described above to finding multiple unknown clusters, it is required for the new method to have the ability to intelligently sort through and separate data points having common error probabilities. In this case, both the total number of clusters and the number of samples per cluster are assumed to be unknown to the classifier. Therefore, with multiple data clusters, each error probability value must now be thought of as an indicator to each point within each cluster. Restated, it is expected that with multiple clusters all data points within each separate thresholded cluster region will share common error probability values. These common error probability values will be relatively small with respect to those computed for most other data points outside of any clusters. In general, the degree to which this latter requirement is satisfied depends on how differently the clusters tend to be distributed with respect to the non-clustered data. As data within a cluster becomes distributed more like the data outside of the cluster, it becomes less distinguishable. Unknown clusters within a data set will be distinguishable by being distributed differently with respect to all other data points outside of the clusters. Notice that these methods exploit this important assumption.","Therefore, a proper data mining algorithm of multiple clusters, and one that is based on the Mean-Field BDRA, will have a higher likelihood of finding leading cluster candidates by focusing on the largest groups of data points that cluster around smaller common error probability values. As the sorting, or mining, continues in this way any data points associated with small error probabilities and that have few common data points are rejected as cluster members. The algorithm will be designed to automatically stop when all unknown data clusters have been found, or when the training error begins to increase. Finally, and as in the single cluster case, the validity of each cluster with respect to the training data can be checked by computing the overall average yield for all points within the cluster.","The steps shown below have been developed for training the new multiple cluster classifier using the Mean-Field BDRA, that is, in such a way that all unknown data clusters are identified with a minimum probability of error. These steps are detailed in the flow chart given as . For each of these steps training proceeds in a semi-unsupervised manner in that all target data (yield>0.5) identified as  in  is utilized without class labels (i.e., no class information at all), and all non-target data (yield<0.5) identified as  is utilized with class labels (full class information). The motivation for training in this way is to force the Mean-Field BDRA to readily recognize the contrast between target cluster data points such as A, B and C and all other data points in both classes that are not like the cluster. Therefore, when adapting class labels for the target class the Mean-Field BDRA is more likely to label any cluster data points as target, while grouping most other non-cluster\/target data points with the non-target. The new method of training proceeds with the following steps for each feature of interest.","Initially, a user selects a maximum number of levels for the algorithm in step . This selection depends on type and amount of data and the available computing resources. For the example shown here, the maximum level is set as twenty. Typically, it is desired to train with as many initial levels as the data will support for best results. As above, this can be set by a user or calculated based on preferences. Using all available training data (i.e., with all target points unlabeled and all non-target points labeled), the Mean-Field BDRA is trained, separately for each level, step . For the results shown here, \u201call available\u201d training data means 50% of the entire data set. After training, step  computes the training error for that level. The number of levels is incremented in step  until the preset maximum level is exceeded, step .","From the iterated training runs for each level, the initial number of discrete levels to use for each feature is chosen as the number of levels that produces the least training error, step . (See Equation (2)). Notice that the idea of steps - is to find the best initial number of discrete levels to use for each feature prior to looking for individual clusters.","The next steps of the method train the Mean-Field BDRA to identify clusters in the data. A first target data point is labeled in step  and the remaining points are left unlabeled in step . The Mean-Field BDRA, is retrained in this manner in step . A cluster-training error is computed after training for each target data point in step . This error is computed based on counting the number of wrong decisions made under each hypothesis. The method then proceeds to the next point in step . Step  returns back to step  until processing of all target data points is complete. Thus, step  produces a set of Ncomputed training errors equal to the number of target training data points.","In step , the set of Ncomputed cluster-training errors in steps - are sorted and grouped according to those having common error values. The final list of separate cluster-training errors should proceed from the smallest error to the largest error. All data points that share each error should be identified. This step helps to reveal those data points that are sharing a similar region in quantized feature space.","Step  conducts a cluster search and looks for the first data cluster candidate using the list obtained in step , above. In step , the first data cluster candidate is chosen as the one having simultaneously the smallest cluster-training error and the largest number of common data points. Typically, the first error value on the list has both the absolute smallest error and the largest number of common points. However, because the algorithm is suboptimal, this does not have to always be the case. Optionally, the user can set a minimum number of data points for each cluster. Once the cluster is selected, the error associated with all points of this first cluster candidate are identified as P(e|0).","After selecting the first cluster candidate in step , pre-existing cluster candidates and current cluster points are all labeled in step . All points not associated with the current or previous cluster candidates are unlabeled in step . The Mean-Field BDRA is then retrained in step . A new cluster-training error is computed in step . This error is identified as P(e|1). Steps - determine how statistically similar the selected group of training data points are with each other, or, on the other hand, how different this group is with respect to the non-target class (which now includes all other target data points outside of the cluster).","In step , P(e|1) and P(e|0) from steps  and  are compared. If P(e|1)\u2266P(e|0), as it should be in most cases containing data clusters, one can conclude that the current cluster is a valid data cluster and proceed to process for additional clusters. Otherwise, one can conclude that no substantial data clusters exist in step , and terminate the algorithm.","When the current cluster is valid, this is indicated in step . A search is conducted for the next cluster candidate in step  according to the previously stated criteria excluding all points in the first cluster. This new group of points will have simultaneously the next smallest cluster-training error and the largest number of common data points. Steps - and - are then repeated until the current error is greater than the initially computed error as found in step . It is important to note that these steps always utilize and train with all previously determined clusters from the previous steps marked. Upon terminating the algorithm, the average yield for each cluster is computed in step  and, if applicable, step  is performed selecting those clusters producing the largest overall yield. The training method results in a trained mean-field BDRA classifier that is capable of recognizing data clusters in the target region.","Table 1, below, shows classification performance results for the Mean-Field BDRA (i.e., w\/o a cluster mining algorithm applied) with supervised training (i.e., data with yields greater than 0.5 are called target and those with yields less than 0.5 are called non-target), for single cluster data of the type shown in . Appearing in this table is the average probability of error computed on an independent test set (50% training\/50% test), and the average associated yield (shown in parentheses) obtained from data classified as the target class. Each entry in the table is shown as a function of the true mean yield value, c, per dimension, of the data cluster (where the two entries in braces, f g, shows the initial number of discrete levels used for each feature by the Mean-Field BDRA), respectively, for one, land four, l, dimensional data spaces. Also appearing in this table is the total number of features, n, in the data space, where the true number of those features relevant to the data cluster, nis shown in brackets, [ ]",{"@attributes":{"id":"p-0087","num":"0086"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"105pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"14pt","align":"left"}}],"thead":{"row":{"entry":[{},"TABLE 1"]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"n[n]",{}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"98pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"77pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["c{l, l}","1[1]","4[1]"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["0.6 {9, 8}\u2002","0.478(0.495)","0.500(0.476)"]},{"entry":["0.9 {10, 9}","0.480(0.528)","0.494(0.512)"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"It can be seen in Table 1 that average classification results are poor when all of the training data are labeled correctly and training proceeds in a supervised manner. This is significant as the results in this table were obtained by partitioning the available data into 50% training and 50% test sets, which highlights the difficulty of the classification problem shown in . Observe that the exact location of the cluster seems to make very little difference to the overall average probability of error, and average yield, no matter how many features are contained in the data. Even in the case when three additional irrelevant features are added to the data, n=4, the results are very similar for both actual cluster locations.","As a final observation in Table 1, notice that the initial number of discrete levels per feature was chosen to be either eight, nine, or ten by the Mean-Field BDRA for either the one or four dimensional cases. For the supervised training case shown in this table, the initial number of discrete levels used for each feature was chosen to be consistent with that used below in obtaining the modified results of Table 2. In all cases, when obtaining these results the actual number of initial discrete levels per feature was incrementally varied between two and ten by the Mean-Field BDRA. The final values shown were determined by the Mean-Field BDRA to be those that produced the smallest training error with the clustering algorithm applied.","Table 2 shows classification performance results for the Mean-Field BDRA (with the cluster mining algorithm applied), and semi-supervised training (i.e., all cluster data points are labeled as target, and all unclustered target data points and all non-target data points are unlabeled), for single cluster data of the type shown in . Appearing in this table is the average probability of error computed on an independent test set (50% training\/50% test), and the average associated yield (shown in parentheses) obtained from data classified as the target class. Each entry in the table is shown as a function of the true mean yield value, c, per dimension, of the data cluster (where the two entries in braces, f g, shows the initial number of discrete levels used for each feature by the Mean-Field BDRA), respectively, for one, l, and four, l, dimensional data spaces. Also appearing in this table is the total number of features, n, in the data space, where the true number of those features relevant to the data cluster, nis shown in brackets, [ ].",{"@attributes":{"id":"p-0091","num":"0090"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"105pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"14pt","align":"left"}}],"thead":{"row":{"entry":[{},"TABLE 2"]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]},{"entry":[{},"n[n]",{}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"98pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"77pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["c{l, l}","1[1]","4[1]"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["0.6 {9, 8}\u2002","0.030(0.582)","0.046(0.547)"]},{"entry":["0.9 {10, 9}","0.019(0.749)","0.022(0.746)"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"In Table 2, it can be seen that when the cluster mining method is applied, average classification results have dramatically improved over that shown in Table 1. That is, not only have error probabilities been substantially reduced but average yields have also been significantly increased. For example, notice in Table 1 that with a true cluster location of 0.6, and for both the one and four dimensional cases, the average yield is less than 0.5. However, in Table 2, and after the cluster algorithm is applied, it can be seen that the average yield has been increased to be much closer to the true value of 0.6. Notice that a similar significant performance improvement occurs for a true cluster location of 0.9.","It is interesting to note that in obtaining the yield results of Table 2, the Mean-Field BDRA classifier labeled an average of fifty four data points as target (i.e., belonging to the cluster). In Table 1, the Mean-Field BDRA called an average of two hundred forty eight points the target. In other words, the clustering algorithm was able to significantly increase the average yield of the data with only slightly more than twenty percent of the number of data points. Thus, the clustering algorithm is utilizing the data much more efficiently to predict a gain in yield in unlabeled data. However, there still are some \u201cfalse alerts\u201d with the clustering method as other data points share the exact same feature space as those within the cluster. The fine tuning of threshold locations shown as the last step in the clustering algorithm above helps to reduce these false declarations by more precisely locating the best initial discrete levels to use by the Mean-Field BDRA.","In Table 3, classification performance results are illustrated for the Mean-Field BDRA (i.e., w\/o a cluster mining algorithm applied) with supervised training (i.e., data with yields greater than 0.5 are called target and those with yields less than 0.5 are called non-target), for two and three cluster data of the type shown in . Appearing in this table is the average probability of error computed on an independent test set (50% training\/50% test), for the respective number of unknown clusters shown. In this case, supervised training results appear for both unclustered (i.e., the classifier has no knowledge about the data clusters), and clustered (i.e., the classifier knows all data points in each cluster, and these are the only points labeled as target). In producing these results the Mean-Field BDRA trains with twenty initial discrete levels of quantization.",{"@attributes":{"id":"p-0095","num":"0094"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"84pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"91pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 3"},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Number of","Supervised","Supervised"]},{"entry":["clusters","Unclustered","Clustered"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["2","0.400","0.104"]},{"entry":["3","0.388","0.126"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"Table 3 illustrates the interesting aspects of this data with regard to classifying data that contains isolated clusters. Observe in this table that average classification results are poor when all of the training data are labeled correctly, and training proceeds in a supervised manner (see the unclustered results column), given the classifier has no knowledge about any data clusters. However, it can also be seen (see the clustered results column) that performance improves dramatically when the classifier is given precise knowledge about the location of all points within the data clusters.","The error probabilities in Table 3 indicate that there is only a slight difference in the results if the data contains either two or three clusters, such as the data shown in . For example, with the unclustered results the three cluster case is slightly better as more clusters are providing information to help discriminate the classes (as a comparison to this, in Table 1 single cluster results using supervised training produced an error probability of near 0.5). When the classifier is given knowledge about the points within each cluster, the two cluster case appears to perform slightly better. In this situation, with three clusters an increasing number of isolated quantized cells also causes more false positive classifications to occur in the regions containing all clusters.","As a final observation in Table 3, notice that the initial number of discrete levels per feature was chosen to be twenty by the Mean-Field BDRA. For the supervised training case shown in this table, the initial number of discrete levels used for each feature was chosen to be consistent with that used below in obtaining the modified results of Table 4. In all cases, when obtaining these results the actual number of initial discrete levels per feature was incrementally varied between two and twenty by the Mean-Field BDRA. The final value of ten shown was determined by the Mean-Field BDRA to be those that produced the smallest training error with the clustering algorithm applied.",{"@attributes":{"id":"p-0099","num":"0098"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"56pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 4"},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Number of",{},"Supervised"]},{"entry":["clusters","Unsupervised Mean-Field BDRA","Clustered"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["2","0.110","0.104"]},{"entry":["3","0.134","0.126"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"In Table 4, classification performance results appear for the Mean-Field BDRA (i.e., with a cluster mining algorithm applied) and unsupervised training (i.e., using the algorithmic steps described above), for two and three cluster data of the type shown in . Appearing in this table is the average probability of error computed on an independent test set (50% training\/50% test), for the respective number of unknown clusters shown. Notice, that for comparison the error probabilities are repeated for the supervised clustered case of Table 3. Observe that the utility of the data clustering algorithm developed here can clearly be seen in the results of Table 4. Observe for both the two and three cluster cases, that the error probability of the cluster mining algorithm is only about one percent higher than it is for the clustered supervised classifier that knows everything. This is significant because the cluster mining algorithm used here has no prior information at all about the clusters.",{"@attributes":{"id":"p-0101","num":"0100"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 5"},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":[{},{},"Supervised"]},{"entry":["Number of clusters","Unsupervised Mean-Field BDRA","Unclustered"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["1","0.666","0.512"]},{"entry":["2","0.608","0.555"]},{"entry":["3","0.622","0.588"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"Table 5 shows average yield results for the multiple cluster cases of Tables 3 and 4, and for comparison previously obtained single cluster results are also shown. In each of these cases, the actual average yield for all data clusters is 0.75. Appearing for two and three clusters are computed average yields for the unsupervised Mean-Field BDRA based classifier of Table 3, and the supervised unclustered classifier of Table 1. For the single cluster case, yield values are based on averaging the one-dimensional results for actual cluster yields of 0.6 and 0.9. From this table, it can be seen that the cluster mining algorithm developed here is improving the overall average yield for all numbers of clusters over that of the supervised classifier. This implies that the new algorithm is improving the quality of the decisions in that it is declaring a proportionately larger ratio of high yielding data points as the target. However, notice also that as the number of clusters increases yield performance of the supervised classifier improves with respect to that of the unsupervised Mean-Field BDRA. Intuitively, as more clusters appear in the data classification performance with supervised training should improve as each cluster provides additional information. This implies that in some cases it might be best for an algorithm such as the Unsupervised Mean-Field BDRA to mine for clusters individually, as opposed to collectively as a group.","In summary, this invention provides a new cluster mining algorithm which has been developed for the Mean-Field Bayesian Data Reduction Algorithm (BDRA). The new method works by utilizing a semi-unsupervised method (only non-target training data points were completely labeled), and an iterative sequential search through the target data, to locate features that are clustered relative to all other target and non-target features within the data set. For the simulated data generated here, clustering was typically based on two defined goodness metrics. In particular, the clustering was based both on reducing the relative training error and on improving the overall predicted yield of the data (clusters were located within target locations where yield values were greater than 0.5). In all cases, classification results revealed that the new clustering algorithm improved performance (over the conventional supervised training method) by significantly reducing the average predicted probability of error, and average predicted yield, for independent evaluation data. (Typically, data outside of a cluster will have a more random distribution of error probability values that will not necessarily associate with a common yield value.)","It should be understood, of course, that the foregoing relates to preferred embodiments of the invention and that modifications may be made without departing from the spirit and scope of the invention as set forth in the following claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 1B"},{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIGS. 3A and 3B"}]},"DETDESC":[{},{}]}
