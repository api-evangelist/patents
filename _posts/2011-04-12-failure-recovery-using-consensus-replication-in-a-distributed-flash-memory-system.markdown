---
title: Failure recovery using consensus replication in a distributed flash memory system
abstract: Data replication in a distributed node system including one or more nodes. A consensus protocol for failure recovery is implemented. Data items and information relating to consensus protocol roles of participant nodes are stored in at least some of the plurality of nodes. Logical logs stored in at least some of the plurality of nodes are created. The logical logs contain additional consensus protocol information including container metadata and replicated data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08856593&OS=08856593&RS=08856593
owner: Sandisk Enterprise IP LLC
number: 08856593
owner_city: Milpitas
owner_country: US
publication_date: 20110412
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATION","RELATED APPLICATION DATA","FIELD OF THE INVENTION","BACKGROUND","DETAILED DESCRIPTION"],"p":["This application claims the benefit under 35 USC 119(e) of the co-pending and commonly owned U.S. Provisional Application No. 61\/323,306 entitled \u201cFailure Recovery Using Consensus Replication In A Shared Distributed Flash Memory System\u201d filed on Apr. 12, 2010, which is incorporated herein by reference.","The present application is related to co-pending U.S. patent application Ser. No. 12\/276,540, entitled \u201cScalable Database Management Software on a Cluster of Nodes Using a Shared-Distributed Flash Memory, filed on Nov. 24, 2008, the disclosure of which is hereby incorporated by reference for all purposes as if fully set forth herein.","The disclosure herein relates to providing failure recovery using consensus replication in a distributed node system.","In distributed computer systems including multiple computer nodes, data may be replicated across computer nodes and storage units to decrease the chance of data loss and or to increase the percentage of time that the systems are available as compared to non-replicated systems. When replicating, many applications desire single copy consistency semantics where all clients see the same version of data and data writes, which may have been observed, do not revert to a prior state. For example, consider a single register with replicas A and B with an initial value 1. A client changes the register value to 2. Once the value 2 is observed, no reader is allowed to observe the value 1 regardless of which replica is read, even if the observation occurs indirectly, such as by knowing that the write completed. A split brain scenario where some clients read the value 1 and others read the value 2 is avoided.","This is sometimes solved by designating one replica as the \u201cmaster\u201d and additional replicas as \u201cslaves,\u201d with a more reliable hardware and software component storing the replica which is the current master and slaves which may become masters. When a slave fails, the current master uses the component (i.e., the more reliable hardware and software component) to designate the failed slave non-authoritative before completing additional data writes. However, when the master fails, an authoritative slave is made master and the old master is marked as non-authoritative by the component before input-output (IO) requests are satisfied. This scheme may be undesirable because some embodiments of the component can still be single points of failure. The scheme may also be intolerant of sequential failures which are common due to correlated causes causing simultaneous failures to manifest sequentially. For example, consider three replicas A, B, and C with A acting as master. Correlated failures such as overheating may cause abnormal shutdowns of all three nodes far enough apart in time for B to replace A and then C to replace B before C fails. When the fault causes a permanent failure to C all data is lost because neither A nor B is authoritative.","Consensus protocols such as Paxos can be applied to solve the problem, exploiting the mathematical property of every majority (>n\/2 in an n-replica system) sharing at least one member in common with every other majority. The system remains available through any sequence of failures leaving a majority reachable and reliable as long as a complete data set exists regardless of what sequential failures occurred. When replication is implemented with a consensus protocol, reads and writes complete when a majority agree on the current value. Additional meta-data in the form of sequence numbers or time stamps are included to identify which disagreeing replica is correct when a different quorum participates in a read. The replication is often implemented as distributed state machine with an instance of the consensus protocol determining the Nth command, which may be \u201cwrite key A=value B\u201d where the current value of A is the latest of its writes, \u201creplica 1 is no longer authoritative\u201d, or \u201cadd node 23 to the cluster\u201d. Naive implementations explicitly store sequence numbers for each command, use separate storage for undecided commands, and always store at least three copies of data. Due to these space and time overheads, consensus is often applied only to determining which replicas are authoritative. While this avoids replica authority determination as a single point of failure, the system may still be vulnerable to sequential failures.","A reallocate-on-write policy may be implemented with a scheme that implies the temporal order of writes, such as a log ordering the writes, or sequence numbers on written blocks. The reallocate-on-write policy may be used to provide low-latency IO to storages requiring a separate erase phase and\/or to accommodate storages that may have bad blocks, such as flash memories. The reallocate-on-write policy implicitly retains old copies of data. The mechanism used for reallocate-on-write may imply ordering which can be used for consensus processing without requiring that additional consensus sequence numbers be stored for the consensus protocol. Time stamps or sequence numbers stored with blocks of data could be used for consensus ordering. The order of blocks in a log implemented as a linked list could be used. Offset into a block or region could be used alone or with one of these other methods. However, there is a need for techniques that allow consensus-based replication tolerant of more sequential failure modes to be implemented with the same time and space overhead as simpler master-slave replication schemes.","Approaches for using data replication in a distributed node system are disclosed. In the following description, numerous specific details are set forth such as examples of specific components, circuits, and processes to provide a thorough understanding of the present disclosure. Also, in the following description and for purposes of explanation, specific nomenclature is set forth to provide a thorough understanding of the present embodiments. However, it will be apparent to one skilled in the art that these specific details may not be required to practice the present embodiments. In other instances, well-known components are shown in block diagram form to avoid obscuring the present disclosure.","A method in accordance with the present embodiments includes using data replication in a distributed node system including a plurality of nodes. The method includes implementing a consensus protocol for failure recovery. Data items and Information relating to consensus protocol roles of participant nodes is stored in at least some of the plurality of nodes. Logical logs stored in at least some of the plurality of nodes are created. The logical logs contain additional consensus protocol information including container metadata and replicated data.","Embodiments described herein uses \u201cwrite-once\u201d persistent storage, such as flash memory, to simplify implementation of the consensus protocol. The write-once storage maintains multiple old versions of the data items, thereby making implementation of the consensus protocol more efficient in terms of the number of write operations and the used storage space as described in more detail herein. The embodiments need not explicitly store a consensus protocol state. Instead the techniques can exploit relationships between the expected consensus protocol state in an error-free operation mode and reallocate-on-write meta-data so that consensus state does not need to be explicitly stored. Moreover, the fact that a not-yet-consistent local copy of data written this way is not visible to readers allows consensus-based replication tolerant of more sequential failure modes to be implemented with reduced time and space overheads.",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1","b":["100","100","110","120","110","120"]},"Present embodiments implement a consensus protocol such as Paxos for failure recovery in data replication among nodes  of distributed node system . In implementing the protocol, each node  may assume one or more roles such as Proposer, Acceptor, or Learner as will be discussed in more detail below with respect to . Data items and information relating to consensus protocol roles of each node  may be stored in one or more of the nodes . The information relating to consensus protocol roles includes, for example, a Propose and an Accept message (e.g., ballot) communicated between various nodes . Also, during a conflict or a crash recovery, message numbers will be stored in one or more of nodes . In an embodiment, logical logs containing additional consensus protocol information including container metadata and replicated data are created and stored in some of the nodes .  is a block diagram of an embodiment of each node  of . Node  includes a processor , a network interface , a disk unit , non-volatile (NV) memory , and memory  all communicating with each other via a bus . Processor  may include one or more processors working separately or in parallel to execute programs including instructions  stored in memory . Network interface  provides support for connecting node  to network . Disk unit  may support one or more hard drive disks (HDDs), other magnetic or optical disks, or state drives (SSDs). NV-memory  may include read only memory (ROM) and flash memory. In present embodiment, NV-memory  is considered to be flash memory (i.e. write-once storage). Flash memory  includes cache\/store . Flash memory  has the property that when a data item (such as a block in a block device) is overwritten, it does not literally put a new data item in the same place as the old data item. Instead, it writes the new data item to a new location in flash memory , leaving the old data item in place. The flash memory  uses one or more mapping tables  to indicate where the most current version of a piece of data item resides. In some embodiments the mapping tables may be stored in memory . Later on, flash memory  may erase the region that contains the old data when it needs to reuse the space. Erasure may be deferred because it can be a high latency operation, and may only be done on large contiguous regions of storage at a time. Flash memory controller (not shown in  for simplicity) must therefore do \u201cgarbage collection\u201d to reclaim unused space within an erasure region. Any current data item within a region that is selected for erasure is copied and compacted into a different, unused region before the old region can be erased. The fact that flash memory  naturally maintains multiple old versions of a data item can be exploited to implement a consensus protocol that is efficient in terms of the number of writes it uses to maintain state, and in terms of the amount of space it requires to hold that state.","Memory  is the main memory of node  and may include random access memory (RAM) or dynamic RAM (DRAM). Memory  may include programs and instructions  which upon executed by processor  implement the methods and techniques of the present embodiments. For example, codes used to implement consensus protocol algorithms such as Paxos may be stored in memory . In some embodiments, such codes are stored non-volatile memory . Memory  may also store copies of consensus protocol information  and logical logs , which are stored in nonvolatile memory . Copies of the mapping table  may also be stored in memory . Consensus protocol information  includes information relating to consensus protocol roles of participant nodes. Logical logs  contain additional consensus protocol information including container metadata and replicated data.","In embodiments, processor  is configured to make multiple versions of the data items stored in flash memory  visible to an application (e.g., a key-value store application), such that the application can use the multiple versions to maintain a consensus protocol state. Processor  may implement the consensus protocol for a distributed key value store with master-slave replications. Processor  may also be configured to apply the consensus protocol on a per-fragment basis at each node. The consensus protocol may be applied via processor  to clustered rational databases, by replicating each row of the rational database as the row is written and applying the consensus protocol to each row write operation. Processor  may be further configured to use erasure coding for each row of the rational database as discussed in more detail herein. Processor  may apply the consensus protocol to cluster configurations other than a master-slave configuration, including a configuration of the plurality of nodes, in which more than one node are allowed to perform write operations.  is a block diagram illustrating a globally-shared flash memory accessible to multiple nodes of  using a Sharing Data Fabric (SDF) . Central Processing Units (CPUs) or processors -\u2032 of nodes  of  can execute programs such as server or other applications to process requests that arrive over network  of . Each of processors  has a cache of DRAM  that contain local copies of data objects associated with the corresponding node . These local copies in DRAM  are accessed by processors  in response to requests from external users.","While DRAM , \u2032 stores transient copies of data objects, the data objects are more permanently stored in flash memory -\u2032. Data objects remain in flash memory ,-\u2032 and are copied to caches in DRAM -\u2032 in response to access requests by programs running on processors , \u2032. SDF  is a middleware layer that includes SDF threads running on processors , \u2032, and Application Programming Interfaces (APIs) and tables of data. A physical interconnect such as an Ethernet or InfiniBand\u00ae fabric connect physical nodes together. Object copies are transferred across the physical interconnect by SDF  from flash memory -\u2032 to cache DRAM -\u2032, and among DRAM -\u2032 caches as needed to ensure coherency of object copies.","Flash memory -\u2032 can be physically located on many nodes such as nodes  having one flash memory  for each processor , or in other arrangements. SDF  makes all the objects stored in flash memory -\u2032 appear to be stored in a global address space, even though the global address spaced is shared among many processors -\u2032. Thus flash memory -\u2032 together appear to be one globally-shared flash memory  via SDF .",{"@attributes":{"id":"p-0025","num":"0024"},"figref":["FIG. 4","FIG. 1","FIG. 4","FIG. 4"],"b":["110","36"]},"Processor  executes application programs, threads, and other routines and accesses a local memory that stores program code and data, such as DRAM . DRAM  also acts as a DRAM cache of objects in the globally-shared flash memory. Processor  also connects to Peripheral Component Interconnect Express (PCIe) switch . PCIe switch  allows processor  to communicate with other nodes through NIC  to send and receive object copies and coherency commands. Flash modules  contain arrays of flash memory that store permanent objects. Flash modules  are accessed by processor  through PCIe switch .",{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 5A","FIG. 5E","FIG. 1","FIG. 3"],"b":["110","24"]},"Paxos may be split into three roles; each node such as nodes  of  may play one or more roles. First role is a Proposer that makes requests and suggests which of multiple started requests will be chosen. Progress is guaranteed when only one live Proposer is allowed to make requests, where this Proposer is the Leader. Second role is an Acceptor that persists protocol state. Third role is a Learner that learns of decisions when they receive Accepted responses from a quorum of nodes .","Paxos is split into two phases. The first phase (phase ) establishes a lower bound for the system's current logical time. This is done so that proposed values are newer than all previously proposed values. Furthermore, all nodes  can agree on the newest value thus allowing it to be consistently chosen by subsequent executions of the protocol instance. Proposers send Prepare messages to a majority of Acceptors with their current ballot number N, where ballots issued by different nodes  come from disjoint sets. When N is larger than any seen by an Acceptor it persists N and sends a Promise message with the value seen from the Accept! command with the highest N. In the normal case without conflicts the value is null. When acknowledgements are received from a majority of replicas, Paxos proceeds to the second phase (phase ).","The second phase decides on the newest value. In phase , Proposers send Accept! commands with their current ballot number N. Where Promise messages contained values, the value with the highest ballot number N must be chosen. When N is at least as high as that seen in prior Prepare and Accept! messages the value is stored and Accepted responses sent to the Learners. Otherwise the Acceptor denies and the Proposer reverts to phase . In an embodiment, phase  can be eliminated on later instances of the protocol thus saving two message delays, unless conflicts are detected in phase . This may be called Multi-Paxos.","For example, consider a 3-node system with nodes N, N, and N each issuing ballots with numbers derived from 3*i+node_number (i.e., 0, 1, and 2 for nodes N, N, and N) for all positive integer values of i. For instance, for i=1, ballot numbers for nodes N, N, and N will be 3, 4, and 5, respectively, as shown in  as an initial state. First, node N sends Propose () to nodes N and N. These nodes (N and N), in response, send Promise (, null) as represented by numbers \u201c3\u201d in the second row of the table under nodes N and N in the state shown in . Next, node N chooses a value A and sends Accept! (A,) to nodes N and N. In response, nodes N and N send Accepted (A,), which is reflected in corresponding third and fourth rows of table of . Subsequent iterations of the protocol will receive a Promise from nodes N and N. Node N then sends Propose () to N and N and, in response, N sends Promise (, {A,}) and N sends Promise (, null) as depicted in the table shown in . N must send Accept! (A,) to N and N producing Accepted (A,) sent by N and N, as represented by table shown in .","Paxos is a distributed write-once register implementation, where only one of multiple simultaneous writes completes and all observers see the same write completing. This becomes interesting when the register is the Nth command to a distributed state machine. The commands, for example, may include \u201ckey A=value B,\u201d \u201creplica 1 is no longer authoritative,\u201d or \u201cadd node 23 to the cluster.\u201d Multiple nodes  of  assuming that they are the Leader may prevent progress but does not pose any correctness problems.","Acceptor protocol states including Propose and Accept! ballot numbers are only logged during conflicts and crash recovery. Replicated payload-write state-machine commands are logged to the same storage which will make up their log checkpoints. Paxos instance numbers are implied by ordering in payload logs. Limited length of window of Paxos instances, in which commands are Accepted based on other Acceptor's Accepted messages, implies that all commands preceding the window have their Accepted values stored in a local stable storage. Because present embodiments only encodes the deviation from expected steady state behavior and implies other protocol states, replication for far greater reliability than a non-replicated system is possible. This can be achieved without decreased storage performance, increased wear, or bus traffic as compared to the single copy configuration. Extremely short time to recovery is possible with Paxos execution over the small implied window of commands that may be unaccepted by a majority. Without contention, additional recovery activity is limited to log head read, a Proposer ballot number determination from Acceptor or local state, and Acceptor ballot number write. The same technique can be applied to other consensus protocols such as Implied Consensus Protocol State, as described below. The embodiment described below uses the property of flash memory to hold multiple versions of data items for a distributed key-value store with master-slave replication. Paxos type consensus is used to reconstruct authoritative state whenever a failure occurs or a new node is brought on-line.","Consider an example scenario with three nodes A, B, and C. Assume that logical times are unique regardless of which node coordinates a write operation. Nodes A, B and C use logical times 3I, 3I+1, and 3I+2, respectively, where I is a non-negative integer. The alphabetically first live node acts as coordinator with storage nodes and their clients determining this via some mechanism which provides eventual consistency when liveness is stable. Each storage node, such as node , provides container meta-data storage and data storage. The data storage system associates sequence numbers with each write operation. In one embodiment the sequence number is explicitly stored for the first write operation in a block and implied based on position for subsequent write operations.","Herein, Paxos type consensus is used to reconstruct authoritative state whenever a failure occurs or a new node is brought on-line. A state on a node is composed of two parts. The first part lists the key-value writes as propagated by the master node. Each entry in the list includes a key, value and global sequence number. In the second part includes meta-data that is used to compute Paxos consensus state for all key-value entries in the first part. \u201cN=0\u201d: defines current global time, and \u201cI  local=0\u201d defines mappings that allow a node to locate the key-value entry with current data. This would be the key-value data that would be retrieved for any subsequent read operations. Also, \u201clocal (,) N=3\u201d defines mappings that are used to compute the global time-stamp for each key-value entry in the first part. All of this state must be persisted for the consensus protocol to work correctly when the state of the cluster changes (nodes fail or new nodes are brought on-line). The first part holds all of the key-value data for the key-value store. A naive replication scheme would keep the same collection of key-value entries, except that it would only keep the most recent value for a particular key, and would not include global sequence numbers. The key-values would be written to persistent storage such as flash memory  of  with some mapping scheme from the mapping table  of  to locate particular keys. If a storage medium with the \u201cwrite-once\u201d property is used in the naive scheme, the storage devices would actually maintain multiple versions of overwritten data, even though the application would only have visibility to the most recent version. One aspect of the present embodiments is a way to make these multiple versions at the storage level visible to the key-value store application so that it can use the multiple versions to maintain consensus protocol state.","Continuing with the above example. Initially A acts as coordinator. A new container is created with writes going to nodes A and B with C as a standby.","Nodes A and B get meta-data","N=0","I  local=0","local [,infinity) N=0","implying that the current logical time N is 0 for all protocol instances, distributed state machine command I maps to local key, value +I, and any proposed or estimated values stored for local key  onwards have logical time N=0. In one embodiment N would be the Paxos ballot number set in a Prepare phase. The absence of data implies that no command has been proposed for any state machine instance.","Node A write","I=0 Key A=value A locally and to node B","I=1 Key B=value B locally and to node B","I=2 Key A=value A locally and crashes before B receives the data.","In one embodiment, these write operations would be performed as Paxos Accept! Commands. B learns that A is no longer live, becomes coordinator, and issues a write request Key B=value B.","Node B gets the logical time locally","N=0","and from C","null","and requests that it be 1 thus creating meta-data","Node B state","N=1","I  local=0","local [, infinity) N=0","Node C state","N=1","Node B iterates over the first of the state machine commands","N=0 I=0 Key A=value A","logically writing","N=1 I=0 Key A=value A","to itself and \u2018C\u2019. This may be optimized to a meta-data mapping change locally","Node B state","N=1","I  local=0","local [, ) N=1","local [, infinity) N=0","local  Key A=value A","local  Key B=value B","Node C state","N=1","I  local=0","local [, infinity) N=1","local  key A=value A","The process repeats for state I=1 which is determined to be the limit, thus leaving the state on both B and C as","N=1","I  local=0","local [, infinity) N=1","local  Key A=value A","local  Key B=value B","and B assigns the new write I=2, which completes, leaving","local  Key B=value B","on C with the write stamped Itime N=1.","B crashes. A restarts, assumes the coordinator role, and operates on itself and \u2018C\u2019 yielding","Node A state","N=3","local=0","local [, ) N=3","local [, infinity) N=0","I=0 Key A=value A implied N=3","I=1 Key B=value B implied N=3","I=2 Key A=value A implied N=0","and node C to","N=3","I  local=0","local [, ) N=3","local [, infinity) N=1","I=0 Key A=value A implied N=3","I=1 Key B=value B implied N=3","I=2 Key A=value A implied N=1","Node A must logically write","I=2 Key A=value A","to both nodes.","To do this, node A must use local key  as an undo record for local key  so the meta-data and data become","N=3","I  local=0","I  local=4","local [, ) N=3","local [, infinity) N=3","I=0 Key A=value A implied N=3","I=1 Key B=value B implied N=3","Key A=value A not visible to the consensus protocol","Key A=value A will undo Key A=value A on local recovery","I=2 Key B=value B implied N=3","The ranges","local [, ) N=3","local [, infinity) N=3","can be collapsed into","local [, infinity) N=3","As an optimization, retention of individual state machine commands is limited to a set that are potentially undecided. As an optimization, only one end point of each half-open interval needs to be stored since they are contiguous. The present embodiments may be implemented by allowing the key-value store application to interact with the low-level storage controller as follows: (1) The low-level storage controller would allow application software to indicate when particular versions of a data item (object or block) are no longer required and can be garbage collected; (2) The low-level storage controller would provide a mechanism for application software to retrieve older versions of a data item (object or block). These new mechanisms could be used to implement the consensus protocol in the above example as follows: (A) Global state machine commands (e.g., \u201cI=0 Key A=value A\u201d, \u201cI=1 Key B=value B\u201d, etc.) would be processed by simply performing the write operation to storage, persisting the global sequence number, key and value. The low-level storage controller would write the data to a new location in storage without destroying the older version(s) of the key-value pair, and would maintain metadata that would allow the older versions to be retrieved for a particular key. (B) On the second part state described above: The application would be responsible for persisting the current global time (e.g., \u201cN=3\u201d) and the intervals (e.g., local[,) N=3\u2033) used to determine the global timestamps for each object entry written in Part . This is a small amount of state that would only be modified when the state of the cluster changed (e.g., a node fails or a new node is added).","The low-level storage controller would maintain and persist the portion of the second part meta-data that determines which version of an object is current (e.g., \u201cI  local=0\u201d). Controllers for persistent media with the \u201cwrite-once\u201d property would typically already maintain such state. Whenever the state of the cluster changes with the failure or addition of a node, the application would follow the algorithm shown by the above example to determine the authoritative values for any key the implied global timestamp of which is not current. For example, when a node that crashed comes back online, it will retrieve new key-values from all survivor nodes (any key-value writes that occurred at global timestamps after the node crashed). These new key-values will supersede any older writes that occurred before the failed node crashed, including those that were performed on the failed node but not replicated to one or more other nodes. This can be done efficiently because the recovering failed node can identify the particular key-values that might not be current by examining the intervals it persisted in the \u201cPart \u201d application meta-data.","In an embodiment, the flash memory controller may not provide direct access to the multiple versions that it naturally crates. Instead, the application software may create and track multiple versions (part  of persisted consensus data) itself by keeping track of the most recent consensus data and providing mechanisms to access and\/or purge older versions as the consensus protocol establishes the authoritative version. The benefit resulting from this is that the writes that are replicated (Part  of the persisted consensus data) can be written directly into the data storage structure, without being staged in special buffers used only for consensus processing. This would avoid multiple copies and reduces the space requirements of the application.","An embodiment may be implemented for flash-memory storage. When multiple copies of data are stored on a flash system, such globally-shared flash memory  of , either in flash memory or in caches\/store  and\/or in flash memory , both of . Consensus may be used to determine which copies have valid data and which copy or copies of the data are faulty. A consensus protocol like Paxos may be used to determine what writes went into a replica across a cluster of nodes  or what replicas are authoritative in part or whole. A Paxos-Flash implementation for consensus replication covers majority voting for replica copies and erasure-coded data-replica segments, with consensus applied to the actual data or meta-data for determining which replicas are valid.","The example described above was for a master-slave cluster with replication, in which writes are sent to a single master, which replicates the writes to one or more slave nodes. In this example, all nodes in the cluster maintain complete replicas of the key-value data. An alternative for brute-force replication is to spread the data across multiple nodes using erasure coding. Erasure coding may provide redundancy without the overheads from strict replication of data items. Erasure codes may divide an object into \u201cn\u201d fragments and recode first fragments into \u201cm\u201d fragments, where \u201cn\u201d>\u201cm\u201d. The parameter r=m\/n<1 may be called the rate of encoding. A rate r code increases the storage cost by a factor of 1\/r. The key property of erasure codes may be that the original object can be reconstructed from any m s fragments. For example, using an r=\u00bc encoding on a block divides the block into m=16 fragments and encodes the original m fragments into n=64 fragments; thereby, increasing the storage cost by a factor of four.","Erasure codes are a superset of the well-known replicated and RAID systems. For example, a system that creates four replicas for each block can be described by an (m=1, n=4) erasure code. RAID level 1, 4, 5 can be described by an (m=1, n=2), (m=4,n=5) and (m=4, n=5) erasure code, respectively.","Erasure coding is a subset of error correction which sub-divides a data set (e.g., a sub-object, a single-object, or multiple objects) into shards and tolerates the erasure\/loss of a subset of the shards. This method may be applied to simple replication (with copies) or erasure coding. When replicating a data container, the container is subdivided into segments where the segments are individually placed and replicated. The number of segments may be substantially larger than the number of nodes, with different segments having replicas (copies) or erasure-coded shards spread across subsets of the nodes  selected to tolerate certain numbers and patterns of failures. The segment replicas may be allocated across nodes using consistent hashing, chained de-clustering, dynamic load balancing, or other schemes. The two main types of replication are \u201coperation transfer\u201d and \u201cstate transfer\u201d. Operation transfer replication duplicates individual operations such as specific writes, e.g., \u201cObject A byte offset 42=\u2018a\u2019,\u201d with the replicated state being the cumulative effect of such operations. State transfer replication copies state, such as Object A offsets 0 through 511 inclusive.","One implementation of operation transfer replication applies a consensus protocol like Paxos (with or without the above optimizations) to determine the set of data and meta-data writes to each of these segments. In this case, there are not authoritative and non-authoritative container segment replicas. Instead, the correct data is determined by agreed upon state plus additional states, determined via consensus protocol execution for potentially undecided state machine commands, on at least a quorum of nodes . Write operations succeed when a quorum of replicas agree that a given (key, value) combination is the Nth update to the system. For classic Paxos, successful write operations to N+1 out of 2N+1 segment replicas would be required. Other variations and protocols can be used to provide different performance or fault tolerance characteristics with larger quorum sizes. For example, Castro-Liskov's protocol could be applied to tolerate Byzantine failures with a quorum size of 2N from 3N segment replicas. As an optimization, only a set of replicas on primary nodes constituting a quorum may be written during normal operation, with the others only accessed when a primary node has failed.","Erasure coding may be implemented with an additional write phase where the coordinator persists portions of a tentative value x[consensus protocol Instance][coordinator] to at least j of k shards. This value may be stored optimistically in a similar manner to replicated writes. Then a subset or superset of the nodes run a consensus protocol such as Paxos to agree on the written value, with the set of nodes chosen to satisfy reliability and availability requirements.","An (m,n) erasure code would encode a data item into n fragments that would be sent to n separate nodes in a cluster. If one or more nodes fail, the data item could still be recovered if at least m nodes remain. Erasure codes are attractive because they can provide similar or better fault tolerance than brute-force replication with less storage overhead. Present embodiments could be applied to erasure coded data by simply applying the consensus algorithm on a per-fragment basis at each node. Key-value stores\/caches have been used as examples. Present embodiments could also be used in more complex data storage systems, including relational databases. A straightforward way in which the present embodiments could be used in a clustered relational database would be to replicate database rows as they are written and apply consensus to each row write. Alternatively, erasure coding could be used for each row write. Other similar applications would be apparent to someone skilled in the art. This method can also be applied to cluster configurations other than master slave. For example, it can also be applied in a cluster in which more than one node is allowed to perform writes.","Consider a five-node system consisting of nodes A through E that tolerates the loss of any two nodes through a 2 of 4 erasure code and Paxos execution across 5 nodes. One container segment may have erasure coded data on nodes A+B+C+D with Paxos executed on nodes C+D+E during normal operation and nodes A or B when one of the primary Paxos nodes has failed. A five node system which survives the loss of one node may combine a 4 of 5 erasure code with Paxos execution on 2 nodes during normal operation and a third during failures. Other implementations are possible.","One variation on this scheme would be to have a proxy which agrees with the logically newest value from a subset of nodes . Given nodes A, B, and C with fast links between B and C and a slow link to A, A could act as a proxy where usually it agrees with the newest value from B or C but changes to only agreeing with the node from B when C fails.","While the simplest implementation requires executing the consensus protocol for read operations, this can be avoided by granting a lease to the node acting as the consensus protocol Leader or coordinator that guarantees write operations will not be completed for any other node. The lease can be implicitly extended each time the coordinator has a state machine command accepted by a quorum of nodes, where the command may be a null-operation during idle periods. The lease implies that the holder has seen all write operations and can therefore directly serve authoritative data to readers without executing the consensus protocol thus significantly reducing read operation cost.","Another embodiment only applies the consensus protocol to selected meta-data, including replicas or shards (for erasure-coded replica fragments) which are authoritative with a different mechanism used for the actual replication. In this embodiment, consensus protocol processing is not applied to the data write and read operations, thus, reducing overhead. Storage used and recovery time may be decreased by reverting to state transfer plus a set of additional operations in some situations. For example, write operations which have been superseded by newer data may be garbage collected once all replicas have processed them, thus leaving a baseline state plus newer deltas. Following a long outage, a new replica may be initialized with state transfer from this baseline and stale replica replaced. While Classic Paxos voting is described, other embodiments may use different variants or other voting schemes, such as a simple majority vote, a vote over a threshold amount (such as >66%), or some other method to ensure data integrity when data is replicated, especially onto different kinds of memory, such as DRAM or SRAM caches and flash memory. Weaker consistency may be exchanged for greater availability by allowing replicas to diverge and eventually be reconciled, via mechanisms including but not limited to newest-write-wins or causal versioning with automatic conflict resolution where possible and a fallback to client application resolution.","Data redundancy may be provided by mirroring content. Some or all of the contents of a memory sub-system at a particular one of nodes  is mirrored to one or more other nodes . Some or all of the contents of cache of the particular node, such as cache  (see ) of flash memory  (see ) of the particular node, are mirrored in caches  of one or more other nodes . In some embodiments and\/or usage scenarios, mirroring of cache  provides copies in more than one of nodes  of any data whose most recent version is not yet stored in flash memory . In a second example, some or all of the contents of flash memory  of the particular node, are mirrored in the respective flash memories  of the one or more other nodes .",{"@attributes":{"id":"p-0120","num":"0119"},"figref":["FIG. 6","FIG. 1","FIG. 1","FIG. 2","FIG. 2"],"b":["600","600","100","610","110","254","110","620","256","110","256","630"]},"One or more embodiments described herein provide that methods, techniques, and actions performed by a computing device are performed programmatically, or as a computer-implemented method. Programmatically means through the use of code or computer-executable instructions. A programmatically performed step may or may not be automatic.","One or more embodiments described herein may be implemented using programmatic modules or components. A programmatic module or component may include a program, a subroutine, a portion of a program, or a software component or a hardware component capable of performing one or more stated tasks or functions. As used herein, a module or component can exist on a hardware component independently of other modules or components. Alternatively, a module or component can be a shared element or process of other modules, programs or machines.","Furthermore, one or more embodiments described herein may be implemented through the use of instructions that are executable by one or more processors. These instructions may be carried on a computer-readable medium, such as disk unit  of . Numerous machines, such as node  of , may provide examples of processing resources and computer-readable mediums, on which instructions for implementing present embodiments can be carried and\/or executed. In particular, the numerous machines include processor(s) and various forms of memory for holding data and instructions. Computers, terminals, network enabled devices (e.g., mobile devices, such as cell phones) are all examples of machines and devices that utilize processors, memory, and instructions stored on computer-readable mediums. Additionally, embodiments may be implemented in the form of computer-programs or a computer usable carrier medium capable of carrying such a program.","While the present embodiments have been described with reference to specific embodiments thereof, it will be evident that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention. For example, features or aspects of any of the embodiments may be applied, at least where practicable, in combination with any other of the embodiments or in place of counterpart features or aspects thereof. Accordingly, the specification and drawings are to be regarded in an illustrative rather than a restrictive sense."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["Embodiments of the present embodiments are described herein by way of example, and not by way of limitation, in the figures of the accompanying drawings and in which like reference numerals refer to similar elements, as follows.",{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0012","num":"0011"},"figref":["FIG. 3","FIG. 1"]},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 4","FIG. 1"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 5A","FIG. 5E"]},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
