---
title: System for managing distributed assets and metadata
abstract: A system for managing distributed digital assets and related metadata in a network, including several interconnected nodes, each of the interconnected nodes providing accessibility to the distributed digital assets via user controlled stores and caching subsystems such that the management of the assets and related metadata is based on predetermined criteria. Each node includes a caching subsystem, a metadata store, a controller, a user-controlled asset component, and a networking subsystem for communication between each of the nodes. The interconnected nodes provide support for platform operation of client applications.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08832023&OS=08832023&RS=08832023
owner: Apple Inc.
number: 08832023
owner_city: Cupertino
owner_country: US
publication_date: 20090130
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION","PARTS LIST"],"p":["The present invention relates to the architecture, services, and methods for managing data among multiple devices, servers and systems. Specifically, the present invention relates to providing a logically unified and aggregated view of a user's data-related assets, including metadata from any system node or device.","Data-related assets include images, videos, and music files which are created and downloaded to personal computer (PC) storage for personal enjoyment. In general, today's data-related assets are often in a digital format (i.e., \u201cdigital assets\u201d) even when they don't originate as such. Typically, these digital assets are accessed only when needed for viewing, listening or playing. Various devices and internet services provide and utilize these assets, including Personal Digital Assistants (PDAs), digital cameras, personal computers (PCs), media servers, terminals and web sites. Collections of assets stored on these devices or service providers are generally loosely coupled or not coupled at all and current synchronization processes occur typically between two devices, for instance a media player and a PC.","Many existing solutions provide an aggregated view of a distributed digital asset collection by replicating each asset (i.e., by duplicating every data file associated with the asset) on every node. Syncplicity, Sharpcast's SugarSync and Microsoft's Live Mesh are examples of current market offerings using this replicating methodology. One shortcoming of such systems is their low storage efficiency. Additional storage capacity is required on every node as new assets are added to other nodes, regardless of the node's need for the stored asset.","Another shortcoming of the above systems, which replicate new assets on every node, is a pronounced user experience impact while the user waits for an asset to be uploaded. Since many common workflows require the asset interest to be uploaded (e.g. sharing or printing), the user experience is substantially delayed while the upload operation is completed.","Yet another shortcoming of conventional systems relates to the ability to restrict access to a previously shared asset. Once a shared asset has been shared with another user the asset is also replicated on the recipient's node. Most existing solutions do not allow the initial sharer to delete the replica when a new, identical replica of the asset exists on the recipient's node or computer.","Other conventional solutions provide an aggregated view of a distributed digital asset collection by enabling each node that stores assets to broadcast to a requesting node a version of the requested asset. One example is Orb Systems. A client application, installed on every point of access node, streams the requested assets on to other nodes per requested requirements (e.g., resolution criteria). A shortcoming of this type of system is the inability to access when a point of access is not connected to the system (i.e., offline status). Furthermore, an offline node is virtually inoperable providing only value related to the assets stored locally. Yet, another limitation is that if the node that is storing an asset is offline, none of the other nodes in the other system will be aware of the existence of the stored asset.","Another shortcoming of existing systems is their limited ability to recognize that a newly introduced asset to the collection may already be present; therefore, the system commonly produces an unnecessary duplicate in the collection, which impacts the system by requiring additional resources to operate (e.g., storage space or processing time). It also impacts the user, who might not be aware that the system contains multiple copies of the same asset.","Finally, another shortcoming of conventional aggregated views of a distributed digital asset collection is that the information created by the owner while organizing her assets locally (e.g. Windows folder system to organize pictures) may be either destroyed or ignored.","Accordingly, there remains a need in the art to overcome the deficiencies and limitations associated with conventional distributed digital asset collection systems that exist today.","The aforementioned need is met by the present invention that describes a system for managing distributed digital assets and related metadata in a network, including several interconnected nodes, each of the interconnected nodes providing accessibility to the distributed digital assets via user controlled stores and caching subsystems, such that the management of the assets and related metadata is based on predetermined criteria Each interconnected node includes a caching subsystem, a metadata store, a controller, a user-controlled asset storage component, and a networking subsystem for communication between each of the nodes. The interconnected nodes provide support for platform operation of client applications.","The present invention further provides a method for determining duplicate data assets by determining an added asset at a monitored disconnected node, and evaluating whether the added asset is a duplicate of an existing asset. Where the added asset is immediately evaluated as a duplicate, it is categorized as an additional replica. Where no duplicate asset is found at the disconnected node, then, when the node reconnects, all other known user nodes are contacted to determine if a duplicate asset exists at those nodes. If so, that duplicate asset is categorized as an additional replica. Where there is no existence of the added asset at the known user nodes, the added asset is categorized as a new asset.","Yet one other aspect of this invention includes the ability for one user of this system to share an asset with a second user in such a way that the second user is able to see the asset and see changes to the asset and its metadata, and to be able to make changes to the asset and its metadata, if given permission to do so by the owner of the asset.","Accordingly, a method of providing access to data assets between multiple users is disclosed that, includes a step of providing a network for sharing the data assets among at least a first user and a second user. A second step enables an invitation to share the data assets. The invitation is sent from the first user to the second user. A third step enables an acceptance of the invitation by the second user; hence, causing, thereafter, production of sharing metadata that corresponds to shared data assets. Another step sends a copy of the metadata to the second user to enable sharing of the data assets between the first and second users. The final step synchronizes the shared metadata, where the shared metadata has been changed, such that, at a minimum, recent metadata changes are reflected.","A distributed digital asset management system in accordance with the present invention is described herein. Distributed digital asset management systems are sometimes referred to as a federated asset management system. The distributed digital asset management system, within the context of the present invention, combines data from different sources in a way which makes it seem to a user of the system as if the data were all stored together. These sources include, for example, personal computers, servers, mobile phones, gaming devices, etc. The data includes digital data, audio data, imaging data, etc. Consequently, within the context of the present invention, \u2018assets\u2019 relate to images, video, music, other audio data and files that are originally digitized or that will be converted to a digital format.","One of several aspects of this invention provides for an aggregated view (across one or many nodes) and access of all media assets owned and shared. All of the digital\/media assets owned or shared by a user are aggregated and termed a user's virtual collection. This invention describes a distributed system for managing either all or a selected amount of the user's digital assets and metadata associated with this virtual collection that includes a set of software services that run on various nodes that the user might wish to use or where the user's assets may reside, and which communicate with other nodes.","An exemplary distributed digital asset management system (hereinafter frequently referenced as \u201cthe system\u201d), according to the present invention, includes a set of nodes (, , , & ) as shown in . Nodes , , , &  include a software service or set of services running on a computer. This computer can be a workstation belonging to a user, but the nodes might also be a personal digital assistant (PDA) or other networked-enabled device, such as a mobile phone or gaming device. The nodes , , , &  communicate with each other over a network using a peer-to-peer link . This network will generally be the Internet, but could also contain ad-hoc sub-networks based on other protocols such as Bluetooth\u2122. Some nodes will communicate with application . Typical applications allow users (also referred to as members) to view, share, and otherwise manipulate their virtual collection and all of the assets and metadata belonging to the collection, including assets in their virtual collection residing on other nodes. Each node on a given computer supports all of the users who use that computer. Implementations of this distributed system will preferably include a central node . Central node  resides on a set of network servers. The central node  helps different users on the distributed system make initial contact with each other, supports web applications, and supports external services, such as photo-finishing. Central node  also can provide caching storage for assets that need guaranteed availability. For most users of this distributed system, interaction and communication is primarily with and between non-central (edge) nodes.","The internal structure of an exemplary node is shown in . A typical node includes a number of components that communicate by passing messages  to each other. On some systems the messages pass over asynchronous queues implemented by Microsoft's MSMQ or similar software, but the components can also pass messages directly. An Application Program Interface (API) component  gives user applications access to the user's virtual collection and corresponding information about it. A network interface  allows each node to communicate with all the other nodes by sending and receiving messages. A user controlled memory\/storage component  manages user-controlled assets. Typically, the assets managed by the user-controlled memory\/storage component  include files residing on the computer where the node is located. However, it is also possible for a user-controlled memory\/storage component  to manage assets at remote network sites such as Flickr\u2122. The heavy asset cache  includes assets and other files being stored on this node temporarily. The controller  runs the workflows which perform the tasks done by the node. These workflows are controlled by workflow descriptions and policies stored in a node data base . The nodes can be independently configured by changing these workflow descriptions and policies. For example a PDA would have very different workflows than a workstation. The workflows and policies used by the controller include the policies which govern the management of the heavy asset cache.","The metadata store  contains metadata corresponding to the assets in the user's collection. In general, the metadata store  for a given user will contain metadata for all of the assets in that user's collection. Thumbnails (e.g., images and audio clips) corresponding to assets are also considered to be metadata within this distributed system. The thumbnails are stored in a thumbnail cache . Other embodiments of this invention could store the thumbnails in the metadata store . The amount of metadata stored on a given node will be sufficient so that a user can browse and do some activities with the entire collection even if the computer is isolated from the network. The exact specification of which asset metadata is stored on each node is determined by criteria stored in the node data store .","The network interface  is responsible for managing the peer-to-peer connection between nodes. It transfers messages and files between nodes and discovers which nodes are online. The network interface  is composed of two parts. One part, called the Network Talker\/Listener receives messages from the rest of the node. Those messages are specific to the system and are in a format which is independent of transport. Those messages are addressed to a specific user on a specific node. The identity of the node is specified in a way which does not make reference to specific network addresses. The second part, are interfaces used by the Talker\/Listener to send and receive messages. Transport interfaces used by the Network Talker\/Listener encapsulate all information about various transports available to the system. The most preferable mechanism for communicating between nodes transmits data between home systems which are separated by routers and firewalls. Ideally, the network interface  should do this without requiring users to open holes in their routers or firewalls. For this reason, the method used for transporting data and metadata between nodes used most generally in one embodiment of this system uses a software package developed by Google called libjingle. This package combines a chat mechanism for sending messages between computers with a peer-to-peer file transmission mechanism which can pass data through consumer network routers. Other well-known methods for establishing peer-to-peer connections between computers can also be used with this system. In addition, nodes located within a single home network can communicate using simple TCP\/IP based protocols which do not require the use of an external chat server.","A user accesses this system through applications, which provide a graphical user interface (i.e., GUI) to allow the user to view\/edit the collection. These applications use the services provided by a local asset management node to get access to a distributed collection belonging to a user without requiring the application to know any details about the location of the asset or its metadata. The application uses a library that is distributed and loaded as part of the system's application. This library communicates with the system's application program interface (API) component  on the local node. The library communicates by sending messages to the API component  using standard inter-process communications methods. The system API component  responds to messages from the application by communicating with the other components of the system node.","An application can use the system API to read and write system metadata (e.g., member information, collection information, and asset information). The application can also subscribe to notifications for both status and error information and for notification when changes are made to the collection outside the application. The application uses system services to access the heavy assets (i.e. the fill resolution asset files) in heavy asset cache . Accessing the heavy assets can be done by requesting a local path for the asset or by opening a stream to the asset. System Services will provide a usable local path or stream, regardless of the actual storage location of the heavy asset.","On the central node  the API needs to be appropriate to a network environment. The most commonly used protocols for internet API's are the simple object access protocol (SOAP) and representational state transfer (REST) protocols, each of which involve transmitting the API parameters and responses inside of HTTP packets. The API processor on the central node  responds to REST commands. A request for a heavy asset through this API moves a copy of the asset to the central heavy asset cache , and returns a proxy URL which can be used to retrieve the asset from the central heavy asset cache .","System metadata, in accordance with the present invention, includes information about the assets in the collection as well as user information (e.g., passwords, file names, video previews, asset creation dates, and user-provided tags) and the state\/status of system operations (e.g., file copy, file sharing). The asset information includes information extracted from the file such as Exif tags, information added by users such as captions, and information extracted from the asset content using image intelligence operations. The metadata for an asset also includes a thumbnail for the asset, for example, a small image extracted from the asset which allows a user to visually identify the asset easily. The metadata for an asset also contains references to all known copies of that asset as well as references to all other assets which were made from that asset. If the system knows which asset a given asset was derived from, the asset metadata also includes a reference to the original asset from which the asset was derived. Access control information for an asset is also part of the metadata for the asset. System metadata is organized as subjects with properties, with both the subjects and properties being themselves pieces of metadata.","Metadata generated on one node is communicated to other nodes through the process of node synchronization. In the context of the present invention, a given system node will not have each and every piece of metadata associated with the collections of the users who use that node. Each node will, however, store the metadata it considers to be \u201caccess-critical\u201d. Access-critical metadata is the metadata needed to allow a local application to function when the node is off-line. Each node can have a different notion of what is \u201cAccess-Critical Metadata\u201d, for example, a cell phone may only keep a list of asset IDs. A PC may have a significantly larger set including asset ID, dates, captions, descriptions, etc. As part of the node synchronization process, each node communicates the specific set of metadata it considers \u201caccess-critical\u201d to other nodes. There can also be a separate metadata store  that holds metadata that only relates to the local node. The separate metadata store  holds items that relate to the local state of things, for example.","Collection metadata can include references as well as values. For example, a piece of asset metadata is a property of the asset. The various copies of an asset located on different nodes are also properties of an asset and references to those copies are stored in the metadata store. The system stores these references as system Universal Resource Identifiers (URI's). In addition to identifying a system resource, a system URI provides a means of locating the resource by describing its primary access mechanism (e.g., its network \u201clocation\u201d), so it is also a system Universal Resource Locator (URL). The URL can be used in this way, because it contains a system network Node ID, which is a globally unique identifier (GUID) that uniquely identifies each node and can be interpreted by the network interface component. If a node is asked for metadata or assets that it does not store locally, it can use the system URL to find out which nodes that do have the required data and send messages to those nodes asking for the desired data.","The metadata store  can include a variety of database technologies. Much of the collection metadata is stored in databases implementing a Resource Description Framework (RDF). An RDF is the means by which a semantic web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries. Additionally, much past and ongoing semantic web work involves \u201creasoning\u201d about data that is represented using RDF. Sharing and reuse of, as well as reasoning on, metadata are important to the system, hence the use of open interfaces and image intelligence. An additional advantage of RDF over traditional relational databases is the ease with which new kinds of information can be added to the design. However, there are also costs associated with RDF and for certain kinds of data, such as an Exif file, a relational database is a better choice. Therefore, one embodiment of this invention uses a combination of databases. Each type of metadata is associated with a storage type. When the metadata store  needs to access a particular type of metadata it looks at its storage mode for that type of metadata to determine which database to go to.","The node controller  on a node controls access to system metadata and assets. Its operations are controlled by information about the capabilities of the node stored in the node data store . The node controller  uses a workflow manager to control much of its processing. There are many workflows that require a series of sending and receiving messages from other system components before the message processing can be considered complete. The workflow manager is responsible for managing all of these workflows. The workflow manager is also responsible for maintaining the state of a workflow between messages by saving parameters describing that state into the node data store .","The workflow manager is policy driven. It is the component that has knowledge about the type of node it is operating on and uses this information in setting up and executing workflows consistent with a node's capabilities and requirements. The workflow manager also uses some of the metadata to find assets and metadata that are not stored locally. The workflows themselves are specified by statements in the Business Process Execution Language (BPEL). The node controller  stores the BPEL workflow descriptions in the node data store  and executes them using an execution engine which is part of the node controller . Because the workflows for the node are stored on the node, the workflows can vary from one node to another and can be different on nodes having different levels of capability.","System storage on each node includes all of the assets on that node. The asset storage is managed by asset managers. These asset managers communicate with other components of the system and provide the distributed system with a way of accessing and managing the assets. Each asset manager accepts messages to control or request information on the data it's responsible for. They provide a standard interface between the system and the various types of asset storage which the system can support.","There are three basic types of asset managers corresponding to the types of asset storage:\n\n","More specifically, User Asset Managers are responsible for feeding asset data into the user's asset management virtual collection. A UAM typically includes a file sniffer or some other way of detecting adds, deletes, and changes to assets in the directories it manages. When a UAM detects a change to an asset it sends a message to the node controller. The node controller reacts to the message by starting a workflow. For a newly discovered asset this workflow determines whether the asset is new or is a duplicate of an existing asset. For a new asset the workflow extracts whatever metadata is contained within the asset file, creates a thumbnail of the asset, and stores all metadata about the asset in the metadata store. The workflow which causes the creation of the thumbnail can also cause the creation of several thumbnails at different resolutions. Whether the workflow does this depends on the workflow policies implemented on the respective node.","Thumbnail Asset Managers manage the thumbnails stored by a node. Some of these thumbnails may have been generated locally and moved into the thumbnail cache by the workflow governing the discovery of new assets; others are moved into the thumbnail cache by the workflows controlling metadata synchronization with other nodes. When a user removes an asset, the workflow controlling asset removal removes the thumbnail.","Cache asset managers control temporary storage on the node. The Cache Asset Manager manages assets in the local Heavy Asset Cache . This temporary storage includes assets from other nodes that have been copied to the node and are being stored there temporarily. The Cache Asset Manager keeps track of the asset copies in the local cache and will remove them, as needed, to manage disk space quotas for the cache. Workflow policies control removal of assets from the cache. For example, assets that have been modified are not removed until the changes have been applied to at least one other replica of the asset via synchronization or via a local workflow if a local replica exists. For another example, \u201cpopular\u201d assets (i.e., assets which are shared with many other users) may be retained in cache preferentially over less popular assets.","Similarly, assets that are in high demand (i.e., assets which users access frequently) can be preferentially retained. In another example, assets determined to be at higher risk of being lost (e.g., fewer backup copies, stored on a device with lower storage reliability like a cell phone) can be retained preferentially over assets which are determined to be at a lower risk (e.g. backup copies stored on a reliable online service). The workflows which remove assets from the cache can vary from one node to another. If the CAM detects a change that has been made to an asset in its cache, it notifies the workflow manager. The workflow manager then initiates the appropriate steps to deal with this modification.","The heavy asset cache  is initially empty. Heavy assets are added to it when a workflow requires that a copy of an asset stored on another node also be stored locally. There are a number of reasons why a copy of an asset might be stored in the cache. One reason is that an application on some node wants to access an asset whose replicas are all on other nodes. In that case, the node controller  will move the asset to the local cache and then give the application access to the new local replica. More interesting reasons for using the heavy asset cache  involve considerations of convenience, performance, safety, or availability. Some assets are more important to users than others. A person might mark the asset as important. Alternatively, its importance can be derived from a record of user action involving this asset or from an analysis of the asset's content. In either case, the importance of the asset, for example, a particular picture becomes a metadata property of that picture. If a user has indicated that the asset should be made safe from accidental deletion, the distributed system can make sure that there are replicas of the asset in more than one place. If the only copy of an important asset is on a machine that is only online intermittently, the distributed system herein can move the important asset to a node that is online more reliably, or to the central node , which is online all the time. In these cases, the selected picture would be moved to heavy asset caches on other nodes. It is even possible that workflow policies on a node would direct that a copy of the asset be moved into user controlled storage on another node. Again, workflows control the movement of these assets. The workflows can be initiated by user activity communicated through the system's API, and can also be initiated by synchronization commands, since the completion of a synchronization cycle indicates that the status of a user's collection has changed. In order to facilitate these workflows, there is node availability metadata associated with each node. This node availability metadata is shared with other nodes the node is in contact with. Node availability metadata allows a node to know whether assets should be pushed to another node for availability reasons.","The system stores copies of system member data (replicas) on system nodes, which may be on- or off-line at any particular time. The system gives each node a means to reconcile its data with the data on other nodes, so that the distributed system as a whole is in a self-consistent state. The system enables members to work with their collections whether or not they are currently on-line. This means that data can change on nodes at times when it is not possible to update the other nodes. It is also possible for different copies of the same data to have different changes applied. Synchronization is the process the system uses to bring member data among all the system nodes to a coherent state by reconciling differences between copies of the same data that could exist at different nodes.","System data is different from many conventional replication systems in that different nodes may store more or less information about each asset. (Conventional synchronization mechanisms are designed for full replicas where all data is replicated on all nodes.) As used here, a \u201cless capable node\u201d implies a node that does not store all metadata. While all nodes store at least some metadata, any number of heavy assets may or may not be present on any node. In the present invention, the implementation of system synchronization supports both metadata and heavy asset synchronization with a single mechanism.","When two nodes have different versions of the same data, it might be true for three different reasons. The first node might have a more recent version of the data; the second node might have a more recent version of the data; or both nodes might have equally recent, but different, versions of the data, because users on each node changed the same data, since the last time the nodes synchronized. The last possibility is particularly likely on the system, since it allows nodes to operate offline so that they cannot be always be synchronized. In order to allow the system to determine efficiently which possibility has occurred, the system uses version vectors. Version vectors and change lists are the means by which the system tracks revisions of replicas and system data on system nodes and are the primary mechanism for determining whether or not synchronization is needed.","A system change is a set of one or more changes made to system data on a single node to which the system assigns a sequential version number. Examples of system changes include changing the title of a photograph, incorporating multiple edits from an application, and adding multiple assets discovered by a system user or asset manager. More than one system change can occur between sync operations. Multiple system changes are kept in a system Change List. In addition to storing its own changes in a system Change list, each node stores copies of the changes it has received from other nodes during synchronization. The highest version number from all system changes for a specific node is stored in the node's element of its respective version vector. System version vectors explicitly track changes to metadata and implicitly track changes to heavy asset replicas. A version vector  (shown in ) is an array that contains an element  for each node. In one exemplary implementation, each system node that stores metadata or assets will have only one version vector for each system member who has data on the node. Therefore, the version vector will contain one element for each node on which the system member stores data.","In an exemplary embodiment the version vectors only describe nodes which have ever made changes to their associated metadata. The system uses a top-level version vector, along with system change lists to track and synchronize system member data across all system member nodes. Each version vector element  contains a synchronization ID  (for example, \u201cA\u201d in ) corresponding to a user for a specific node along with a counter  (for example, \u201c\u201d in ) (i.e., the \u201clogical revision) that the software increments each time the data at that node for that user changes. A given node keeps the same synchronization ID for a user's systemdata, unless it loses its data base and has to be updated completely, or unless it can't support normal synchronization, because other nodes no longer have copies of changes it needs in their change list. The version vector for a particular member\/node includes current revision information for that member\/node and the revision information for other nodes used by that member as of the last time synchronization occurred. There is an additional version vector for each shared part of a collection on each node.","Synchronization takes place between at least a pair of nodes. A synchronization cycle (\u201csync cycle\u201d) happens when a predetermined number of changes have happened on a node, since its last synchronization cycle, or when a predetermined amount of time has passed since the last sync cycle. When that happens on a node, the node compares its version vector for a user to copies of the version vectors for that user it had previously received from the user's other nodes; and determines which of those other nodes are at a lower revision level than it is and, therefore, need to be updated with its data. The node sends those other nodes a \u201csync cycle required\u201d message. Nodes which have received this message begin a sync cycle with the node that sent the \u201csync cycle required\u201d message. The sync cycle begins with one node, the \u201crequestor\u201d, sending its version vector to another node, the \u201csource\u201d. The source compares its version numbers in the requestor's and its version vectors. The goal is for the source to send to the requestor all changes (that the requester wants, based on its capability level) that have occurred on the source since it last synced with the requestor. If a comparison of version vectors shows that the source should send changes to the destination, the source looks at its change list to see which changes need to be sent to the requestor given the values of the revision levels in the version vectors. Once a sync cycle is completed, the requester updates its version vector to reflect its new change level, and then responds to other sync cycle required messages it has received.",{"@attributes":{"id":"p-0052","num":"0054"},"figref":"FIGS. 5","b":["6","7","8"]},"In each case, a sync cycle starts with an exchange of version vectors. In , the version vectors are identical, and so no further processing occurs. Specifically, in step , describing an interchange between node  and node , node  sends a Start Sync message to node  with it version vector_N(VV_N). In step , node  compares VV_N with its own version vector. As a result of the comparison, node  detects changes and compiles a list of changes it has that aren't on node . Step  has node  sending Start Sync response message with VV_N plus the change list.","In step , node  compares node  with its own version vector. As a result of the comparison, node  detects changes and sees the change list sent by node . Node , subsequently, checks if there are any conflicts with its change list and node 's change list. In this example, there are no conflicts found. Step  has node  sending its change list to node . During step , node  incorporates node 's changes in its data store and updates its change lists to reflect the additional changes. Additionally, VV_N gets updated. Likewise, in step , node  incorporates node 's changes in its data store and updates its change lists to reflect the additional changes; and VV_N gets updated. In this example, in the final step , VV_N and VV_N are identical.","In , one node (node ) detects from the version vector that two nodes (node , node ) need to be synchronized. The nodes exchange change lists, use the change lists to determine which metadata needs to be updated, and exchange whatever metadata is required to bring the two nodes into synchronization. Notably, in step  node  sends Start Sync message to node  with its version vector_N (VV_N). Step  is where node  compares VV_N with its own version vector. Accordingly, node  sees that node  has changes that node  will need, but node  has no changes that node  needs. In step , node  sends Start Sync response message with VV_N. Thereafter, in step , node  compares node  with its own version vector. Node  sees or detects that there are changes that will need to be sent to node . Accordingly, in step , node  sends its change list to node .","During step , node  examines and evaluates each change from node  and determines if it affects anything stored by node . If there is an impact upon a stored asset, node  updates its store and change list. If there is no impact upon a stored asset, node  discards the change and adds a null as a placeholder within the change list. In the final step, step , VV_N and VV_N are identical, but node  has flagged its version vector with a partial update flag.","In  and , one node is less capable than the other, i.e., the node does not contain all metadata. The node located on the cell phone is less capable than the node located on the desktop PC, because the cell phone is not able to store as much metadata for each asset as the desktop PC. The capability data for the cell phone contains a description of which types of metadata the cell phone node does not store. The figures differ in whether the more capable or the less capable node has initiated the sync cycle. The process differs from the process for equally capable nodes in that the less capable node does not update all of its data. The figure also shows the version vectors before and after the sync cycle has taken place. These version vectors are in parentheses. The notation indicates all other nodes known to the node in question, which version level each of those nodes are at, and whether any of those nodes have been updated partially.","Specifically, for the exemplary embodiment shown in , step  has node  sending Start Sync message to node  with its VV_N. Step  has node  comparing VV_N with its own version vector. Node  detects that it has changes that node  needs. Node  has no changes that it needs. In step  node  sends Start Sync response message with VV_N and its change lists for placement at N: and N: of its version vector. In step  node  compares node  with its own version vector. Node  sees there are changes it needs to send node .","In step , node  reviews the change list from node . Node  sees that place N: has a null indicating that node  didn't store a change. Additionally, node  sees a change in N: so the database for node  is updated. Notably, node 's version vector has a \u201cp\u201d for N, therefore, node  also sets a \u201cI\u201d for node . In the step , VV_N and VV_N are identical and both have node  version vector, VV with a partial update flag.","In the exemplary embodiment shown in , step  illustrates node  sending a Start Sync message to node  with its VV_N. In step , node  compares VV_N with its own version vector to detect changes that node  needs. In this example, node  doesn't have any changes that it needs. Subsequently, in step  node  compares node  with its own version vector to detect if there are any changes it needs to send to node . Step  has node  reviewing the change list from node . At N: there is a null indicating that node  didn't store a change. A change is seen at N:, therefore, the corresponding database is updated. Since node's version vector has a \u201cp\u201d for N, node  also sets a \u201cp\u201d for node . The final step of  has version vectors VV_N and VV_N, as identical and both have node  version vector, VV with a partial update flag.","A sync cycle for shared data is implemented in a slightly different fashion. If a node on which a first user has an account includes data that has been shared with another user, it will have a version vector corresponding to the shared data. If the shared data changes on a specific node for the first user, the node will initiate a sync cycle with another node that has an account for the second user. After the second user's node is synchronized, it will synchronize itself with other nodes containing accounts for the second user using the normal synchronization process.","Because change lists grow over time, they sometimes need to be pruned. If all nodes were online all the time pruning would be easy\u2014as soon as a given change had been propagated to all other nodes its change list item would no longer be needed and could be deleted. In fact, as pairs of nodes become synchronized, the system as a whole does become substantially synchronized, since a substantial proportion of the nodes are synchronized with each other. The exception is the offline nodes, which cannot be synchronized until they come online. If a node is offline for a predetermined long amount of time, other nodes will not wait for it to receive their changes, but will delete change list items for changes which have been propagated to all other, non-missing nodes. This deletion process is termed pruning. Once the missing node comes back online, it will once again participate in sync cycles with other nodes. Those sync cycles will not be able to proceed normally, because the other nodes will no longer have change list items corresponding to changes that occurred on those nodes after the node had gone offline, because of the pruning that had occurred. Therefore, the formerly missing node subsequently participates in a modified sync cycle. First, it sends the changes it made while offline to a second node. All these offline changes are assumed to be potentially in conflict and a user is asked to determine if these changes should be accepted. Then after the conflicts have been resolved, the formerly-offline node updates its entire metadata store and change list from the same second node it sent its changes to. A simpler example of this is when a new node comes on-line and all other nodes have already pruned their change lists. The new node will update its entire metadata store from another node. Notably, in this case the new node would very likely not have any changes.","Sharing in digital asset management means that other users (sometimes called, \u201cshare recipients\u201d) are able to access digital assets within a user's collection. The amount of access the other users have is under the control of the user who owns the asset. A share recipient who is only partly trusted might be granted read access to some of the metadata for the asset, while a fully trusted share recipient can be given the right to modify the asset itself. Shares are initiated or offered by one user and directed to another user. The second user is messaged with a share invitation and allowed to accept or reject the share invitation, which generates a response message back. This messaging uses the same peer-to-peer network as other inter-node communications. If the share invitation was accepted, it also initiates a workflow that adds information about the shared asset to the metadata and the tracking of the shared asset by the offering user. Again, the owner of the shared asset controls how much access is granted to the share recipient with respect to the shared asset. In an exemplary embodiment of this system, the sharer assigns a role to each share recipient. This assigned role, along with the identity of the share recipient, is stored in the metadata store  for the sharer. The node controller  uses this assigned role to determine whether to allow the share recipient to read or alter the shared asset.","In order to share an asset with a second user the offering user first needs to put the second user on his contact list. Any node in the systemnetwork can find out whether other users are available by asking the central node , since all users are in contact with it. If the central node  is not available, a node can at least ask the nodes it is in contact with for their users. Once a systemmember has obtained a list of potential contacts, the member can select a user to be sent a message inviting that user to be a contact. The message, like other user messages is sent to at least one node known to contain an account for the addressee of the message and is propagated via the synchronization process to other nodes containing that user Once the second user has been made a contact, the offering user can share assets with it. The offering user sends a user message to the potential share recipient inviting the share recipient to share the selected assets. Once the share recipient accepts the invitation to share, the node that the share recipient is working at will ask one of the offering user's node for metadata describing the asset and add that metadata to its collection metadata with additional attributes indicating that it is shared.",{"@attributes":{"id":"p-0065","num":"0067"},"figref":"FIG. 3","b":"300"},"Initial step , Bob shares asset \u201cA\u201d with Ted. Step , the system copies shared asset A's metadata into Ted's collection and identifies Bob as the owner. Step , Bob modifies the metadata for assets A and B. Step  system synchronizes the changes for assets A and B to all of Bob's nodes. In addition, the system synchronizes (i.e., \u2018synchs\u2019) the changes for asset A to all of Ted's nodes. Step  Ted modifies the metadata for assets A and C.","In step , system synchs A and C changes to all of Ted's nodes, while also synching changes involving asset A to all of Bob's nodes. Step  Ted disconnects his node from the distributed system, (i.e., goes off-line). In step , Ted modifies asset A's metadata while being disconnected from Bob. Subsequently, in step , Ted reconnects his node to the distributed system. Step , the system synchs A's changes to all of Ted's nodes and all of Bob's nodes.","In step , Ted decides to edit asset A's image data (which Bob has granted Ted permission to do so). Step , the systemuses asset A's metadata in Ted's virtual collection to find a replica and copies it to create a new replica in Ted's system cache for Ted to edit. The system updates asset A's metadata to include the newly created replica. Step , the system syncs A's metadata changes to all of Ted's nodes and all of Bob's nodes.","In step , Ted edits asset A's image data. Step , the system syncs image changes to all of asset A's replicas. In step , Ted, again, disconnects from the distributed system. In a final step, step , Ted prints the image data from asset A on his own printer (system uses the cached copy of the asset file).","In a further feature of the present invention, when a UAM on a node detects a new asset, the system needs to determine if it is an entirely new asset or a copy of an asset which is already known to the system. If the asset is a copy of an asset already known to the system, the distributed system can categorize the asset file as a replica of an existing asset, rather than as a new asset. There are known means for detecting whether the asset is unique in the case where the node already contains metadata for that asset. However, because the distributed system described herein can be used by nodes that are offline, it is possible that two nodes, not in contact with each other, could both add the same asset, so that the duplication cannot be detected, until after both duplicates have been independently added to the collection databases. When the nodes come into contact, there needs to be an efficient means to determine if any assets need to be identified as duplicates among the nodes. The distributed system of the present invention solves the identification of duplicates problem by recording new assets discovered by an offline node as \u201cprovisional new assets\u201d. The distributed system does this by assigning a type of asset metadata termed the \u201cprovisional new asset property\u201d to newly discovered assets on offline nodes. The provisional new asset property provides an unambiguous indicator that duplicate checking has not been completed for the asset and still needs to occur.  shows illustrative steps in assigning and using the provisional new asset property within exemplary flowchart  and a file labeled, \u201cMy Picture\u201d.","These steps are preferably performed by a processor, computer, or application specific integrated circuits (ASICs). The steps can comprise a computer program that can be stored in a computer readable storage medium, which may comprise, for example; magnetic storage media such as a magnetic disk (such as a hard drive or a floppy disk) or magnetic tape; optical storage media such as an optical disc, optical tape, or machine readable bar code; solid state electronic storage devices such as random access memory (RAM), or read only memory (ROM); or any other physical device or medium employed to store a computer program.","In Step  a user adds file, \u201cMy Picture\u201d to their system virtual collection. Step , the system creates asset (ID, type, thumbnail, etc.) and replica (ID, type, thumbnail, etc.) metadata and adds it to the user's virtual collection metadata. Step , the system adds \u201cprovisional new asset\u201d property to newly added asset's metadata in user's collection. Step , the system applies duplicate detection method by consulting all user nodes that are currently connected.","An inquiry is conducted in step  to determine whether any duplicates have been found. If a duplicate has been found, step  labels \u201cMy Picture\u201d as a replica of an existing asset. Step  causes system to move replica metadata for newly added asset to existing asset. In other words, the existing asset gains an additional replica. In step , system removes remaining metadata for newly added asset from user's collection before completing the workflow.","Otherwise, if no duplicate was found, when the node reconnects, an additional inquiry in step  consults all the user's nodes for duplicates. If all the user's nodes have been consulted, then the asset is considered new and the provisional property designation is removed in step  before completing the workflow.","When all the user nodes have not been consulted, an additional inquiry in step  determines whether the particular node is on-line and continues to poll for on-line status. Step  causes system to apply duplicate detection method when consulting newly connected nodes for duplicates during a repetitive cycle.","An alternative method for detecting duplicate assets when nodes which had been offline come into contact is as follows. When nodes come into contact, they will synchronize their metadata. This process will cause assets added on one node to be added to the metadata of the other node. Because the synchronization of two nodes involves an exchange of asset change lists between those nodes, and because the change list for a node can indicate which assets are new, the need to perform duplicate checking for a particular asset could be inferred from the asset change lists exchanged during synchronization.","The invention has been described in detail with particular reference to certain preferred embodiments thereof, but it will be understood that variations and modifications can be effected within the spirit and scope of the invention.",{"@attributes":{"id":"p-0078","num":"0000"},"ul":{"@attributes":{"id":"ul0003","list-style":"none"},"li":[{"@attributes":{"id":"ul0003-0001","num":"0080"},"b":"1"},{"@attributes":{"id":"ul0003-0002","num":"0081"},"b":"2"},{"@attributes":{"id":"ul0003-0003","num":"0082"},"b":"3"},{"@attributes":{"id":"ul0003-0004","num":"0083"},"b":"5"},{"@attributes":{"id":"ul0003-0005","num":"0084"},"b":"10"},{"@attributes":{"id":"ul0003-0006","num":"0085"},"b":"11"},{"@attributes":{"id":"ul0003-0007","num":"0086"},"b":"12"},{"@attributes":{"id":"ul0003-0008","num":"0087"},"b":"13"},{"@attributes":{"id":"ul0003-0009","num":"0088"},"b":"14"},{"@attributes":{"id":"ul0003-0010","num":"0089"},"b":"15"},{"@attributes":{"id":"ul0003-0011","num":"0090"},"b":"16"},{"@attributes":{"id":"ul0003-0012","num":"0091"},"b":"17"},{"@attributes":{"id":"ul0003-0013","num":"0092"},"b":"18"},{"@attributes":{"id":"ul0003-0014","num":"0093"},"b":"30"},{"@attributes":{"id":"ul0003-0015","num":"0094"},"b":"31"},{"@attributes":{"id":"ul0003-0016","num":"0095"},"b":"32"},{"@attributes":{"id":"ul0003-0017","num":"0096"},"b":"33"},{"@attributes":{"id":"ul0003-0018","num":"0097"},"b":"300"},{"@attributes":{"id":"ul0003-0019","num":"0098"},"b":"310"},{"@attributes":{"id":"ul0003-0020","num":"0099"},"b":"315"},{"@attributes":{"id":"ul0003-0021","num":"0100"},"b":"320"},{"@attributes":{"id":"ul0003-0022","num":"0101"},"b":"325"},{"@attributes":{"id":"ul0003-0023","num":"0102"},"b":"330"},{"@attributes":{"id":"ul0003-0024","num":"0103"},"b":"335"},{"@attributes":{"id":"ul0003-0025","num":"0104"},"b":"330"},{"@attributes":{"id":"ul0003-0026","num":"0105"},"b":"335"},{"@attributes":{"id":"ul0003-0027","num":"0106"},"b":"340"},{"@attributes":{"id":"ul0003-0028","num":"0107"},"b":"345"},{"@attributes":{"id":"ul0003-0029","num":"0108"},"b":"350"},{"@attributes":{"id":"ul0003-0030","num":"0109"},"b":"355"},{"@attributes":{"id":"ul0003-0031","num":"0110"},"b":"360"},{"@attributes":{"id":"ul0003-0032","num":"0111"},"b":"365"},{"@attributes":{"id":"ul0003-0033","num":"0112"},"b":"370"},{"@attributes":{"id":"ul0003-0034","num":"0113"},"b":"375"},{"@attributes":{"id":"ul0003-0035","num":"0114"},"b":"380"},{"@attributes":{"id":"ul0003-0036","num":"0115"},"b":"385"},{"@attributes":{"id":"ul0003-0037","num":"0116"},"b":"390"},{"@attributes":{"id":"ul0003-0038","num":"0117"},"b":"501"},{"@attributes":{"id":"ul0003-0039","num":"0118"},"b":"502"},{"@attributes":{"id":"ul0003-0040","num":"0119"},"b":"503"},{"@attributes":{"id":"ul0003-0041","num":"0120"},"b":"504"},{"@attributes":{"id":"ul0003-0042","num":"0121"},"b":"505"},{"@attributes":{"id":"ul0003-0043","num":"0122"},"b":"506"},{"@attributes":{"id":"ul0003-0044","num":"0123"},"b":"507"},{"@attributes":{"id":"ul0003-0045","num":"0124"},"b":"508"},{"@attributes":{"id":"ul0003-0046","num":"0125"},"b":"601"},{"@attributes":{"id":"ul0003-0047","num":"0126"},"b":"602"},{"@attributes":{"id":"ul0003-0048","num":"0127"},"b":"603"},{"@attributes":{"id":"ul0003-0049","num":"0128"},"b":"604"},{"@attributes":{"id":"ul0003-0050","num":"0129"},"b":"605"},{"@attributes":{"id":"ul0003-0051","num":"0130"},"b":"606"},{"@attributes":{"id":"ul0003-0052","num":"0131"},"b":"607"},{"@attributes":{"id":"ul0003-0053","num":"0132"},"b":"701"},{"@attributes":{"id":"ul0003-0054","num":"0133"},"b":"702"},{"@attributes":{"id":"ul0003-0055","num":"0134"},"b":"703"},{"@attributes":{"id":"ul0003-0056","num":"0135"},"b":"704"},{"@attributes":{"id":"ul0003-0057","num":"0136"},"b":"705"},{"@attributes":{"id":"ul0003-0058","num":"0137"},"b":"706"},{"@attributes":{"id":"ul0003-0059","num":"0138"},"b":"801"},{"@attributes":{"id":"ul0003-0060","num":"0139"},"b":"802"},{"@attributes":{"id":"ul0003-0061","num":"0140"},"b":"803"},{"@attributes":{"id":"ul0003-0062","num":"0141"},"b":"804"},{"@attributes":{"id":"ul0003-0063","num":"0142"},"b":"805"},{"@attributes":{"id":"ul0003-0064","num":"0143"},"b":"806"}]}}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 3","i":"a "},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
