---
title: Tuned feedforward LMS filter with feedback control
abstract: A method to automatically and adaptively tune a leaky, normalized least-mean-square (LNLMS) algorithm so as to maximize the stability and noise reduction performance in feedforward adaptive noise cancellation systems. The automatic tuning method provides for time-varying tuning parameters λand μthat are functions of the instantaneous measured acoustic noise signal, weight vector length, and measurement noise variance. The method addresses situations in which signal-to-noise ratio varies substantially due to nonstationary noise fields, affecting stability, convergence, and steady-state noise cancellation performance of LMS algorithms. The method has been embodied in the particular context of active noise cancellation in communication headsets. However, the method is generic, in that it is applicable to a wide range of systems subject to nonstationary, i.e., time-varying, noise fields, including sonar, radar, echo cancellation, and telephony. Further, the hybridization of the disclosed Lyapunov-tuned feedforward LMS filter with a feedback controller as also disclosed herein enhances stability margins, robustness, and further enhances performance.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=06996241&OS=06996241&RS=06996241
owner: Trustees of Dartmouth College
number: 06996241
owner_city: Hanover
owner_country: US
publication_date: 20040510
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["STATEMENT OF RELATED APPLICATION","STATEMENT OF GOVERNMENT INTEREST","FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["This application is a continuation-in-part of U.S. patent application Ser. No. 09\/887,942 filed Jun. 22, 2001 now U.S. Pat. No. 6,741,707 and incorporated herein by reference Force. The Government has certain rights in this invention.","The invention was made with the Government support under Grant No. F41624-99-C-606 awarded by the United States Air Force. The Government has certain rights in this invention.","The present invention relates to a method for automatically and adaptively tuning a leaky, normalized least-mean-square (LMS) algorithm so as to maximize the stability and noise reduction performance of feedforward adaptive noise cancellation systems and to eliminate the need for ad-hoc, empirical tuning and more specifically, to the hybridization of a Lyapunov-tuned feedforward LMS filter with a feedback controller so as to enhance stability margins, robustness, and further enhance performance.","Noise cancellation systems are used in various applications ranging from telephony to acoustic noise cancellation in communication headsets. There are, however, significant difficulties in implementing such stable, high performance noise cancellation systems.","In the majority of adaptive systems, the well-known LMS algorithm is used to perform the noise cancellation. This algorithm, however, lacks stability in the presence of inadequate excitation, non-stationary noise fields, low signal-to-noise ratio, or finite precision effects due to numerical computations. This has resulted in many variations to the standard LMS algorithm, none of which provide satisfactory performance over a range of noise parameters.","Among the variations, the leaky LMS algorithm has received significant attention. The leaky LMS algorithm, first proposed by Gitlin et al. introduces a fixed leakage parameter that improves stability and robustness. However, the leakage parameter improves stability at a significant expense to noise reduction performance.","Thus, the current state-of-the-art LMS algorithms must tradeoff stability and performance through manual selection of tuning parameters, such as the leakage parameter. In such noise cancellation systems, a constant, manually selected tuning parameter cannot provide optimized stability and performance for a wide range of different types of noise sources such as deterministic, tonal noise, stationary random noise, and highly nonstationary noise with impulsive content, nor adapt to highly variable and large differences in the dynamic ranges evident in real-world noise environments. Hence, \u201cworst case\u201d, i.e., highly variable, nonstationary noise environment scenarios must be used to select tuning parameters, resulting in substantial degradation of noise reduction performance over a full range of noise fields.","Presently, commercial active noise reduction (ANR) technology uses feedback control to reduce unwanted sound. A feedback topology is shown in . Here, the measured error signal eis minimized through an infinite impulse response feedback compensator designed using traditional frequency-domain methods. The feedback controller seeks to force the phase between the output signal and the error signal equal to \u2212180 degrees for as much as the ANR frequency band as possible. In active noise control, a high-gain control law is required to achieve this objective and to maximum ANR performance. However, a high-gain control law leaves inadequate stability margins, and such systems destabilize easily in practice, as the transfer function of the system can vary substantially with environmental conditions. In order to provide adequate stability margins, ANR performance is sacrificed, thus present feedback technology exhibits narrowband performance and \u201cspillover\u201d or creation of noise outside of the ANR band. Present commercial technology implements feedback control using analog circuitry.","The present invention discloses a method to automatically and adaptively tune a leaky, normalized least-mean-square (LNLMS) algorithm so as to maximize the stability and noise reduction performance in feedforward adaptive noise cancellation systems. The automatic tuning method provides for time-varying tuning parameters \u03bband \u03bcthat are functions of the instantaneous measured acoustic noise signal, weight vector length, and measurement noise variance. The method addresses situations in which signal-to-noise ratio varies substantially due to nonstationary noise fields, affecting stability, convergence, and steady-state noise cancellation performance of LMS algorithms. The method has been embodied in the particular context of active noise cancellation in communication headsets. However, the method is generic, in that it is applicable to a wide range of systems subject to nonstationary, i.e., time-varying, noise fields, including sonar, radar, echo cancellation, and telephony. Further, the hybridization of the disclosed Lyapunov-tuned feedforward LMS filter with a feedback controller as also disclosed herein enhances stability margins, robustness, and further enhances performance.","It is important to note that the present invention is not intended to be limited to a device or method which must satisfy one or more of any stated or implied objects or features of the invention. It is also important to note that the present invention is not limited to the preferred, exemplary, or primary embodiment(s) described herein. Modifications and substitutions by one of ordinary skill in the art are considered to be within the scope of the present invention, which is not to be limited except by the following claims.","Operation of the adaptive feedforward LMS algorithm of the present invention is described in conjunction with the block diagram of , which is an embodiment of an adaptive LMS filter  in the context of active noise reduction in a communication headset. In a feedforward noise reduction system, the external acoustic noise signal , X, is measured by a microphone . The external acoustic noise signal is naturally attenuated passively , as it passes through damping material, for example, a headset shell structure, and is absorbed by foam liners within the ear cup of the headset, as defined on [0061].","The attenuated noise signal  is then cancelled by an equal and opposite acoustic noise cancellation signal , y, generated using a speaker  inside the ear cup of the communication headset. The algorithm  that computes yis the focus of the present invention. Termed an adaptive feedforward noise cancellation algorithm in the block diagram, it provides the cancellation signal as a function of the measured acoustic noise signal X(\u2032), and the error signal e(), which is a measure of the residual noise after cancellation.","In real-world applications, each of these measured signals contains measurement noise due to microphones and associated electronics and digital quantization. Current embodiments of the adaptive feedforward noise canceling algorithm include two parameters\u2014an adaptive step size \u03bcthat governs convergence of the estimated noise cancellation signal, and a leakage parameter \u03bb. The traditional normalized, leaky feedforward LMS algorithm is given by the following two equations:\n\ny=WX\n\n\u2003\u2003(1, 2)\n\nwherein Wis a weight vector, or set of coefficients of a finite-impulse response filter. \u03bb=1 for ideal conditions: no measurement noise; no quantization noise; deterministic and statistically stationary acoustic inputs; discrete frequency components in X; and infinite precision arithmetic. Under these ideal conditions, the filter coefficients converge to those required to minimize the mean-squared e.\n","Algorithms for selecting parameter \u03bcappear in the literature and modifications or embodiments of published \u03bcselection algorithms appear in various prior art. However, the choice of parameters \u03bb and \u03bcas presented in the prior art does not guarantee stability of the traditional LMS algorithm under non-ideal real-world conditions, in which measurement noise in the microphone signals is present, finite precision effects reduce the accuracy of numerical computations, and noise fields are highly nonstationary.","Furthermore, in current algorithms, the leakage parameter must be selected so as to maintain stability for worst case, i.e., nonstationary noise fields with impulsive noise content, resulting in significant noise cancellation degradation. Parameter \u03bb is a constant between zero and one. Choosing \u03bb=1 results in aggressive performance, with compromised stability under real-world conditions. Choosing \u03bb<1 enhances stability at the expense of performance, as the algorithm operates far away from the optimal solution.","The invention disclosed here is a computational method, based on a Lyapunov tuning approach, and its embodiment that automatically tunes time varying parameters \u03bband \u03bcso as to maximize stability with minimal reduction in performance under noise conditions with persistent or periodic low signal-to-noise ratio, low excitation levels, and nonstationary noise fields. The automatic tuning method provides for time-varying tuning parameters \u03bband \u03bcthat are functions of the instantaneous measured acoustic noise signal X, weight vector length, and measurement noise variance.","The adaptive tuning law that arises from the Lyapunov tuning approach that has been tested experimentally is as follows: \n\n\nwherein X+Qis the measured reference signal, which contains measurement noise Qdue to electronic noise and quantization. The measurement noise is of known variance \u03c3. L is the length of weight vector W. This choice of tuning parameters provides maximal stability and performance of the leaky LMS algorithm, causing it to operate at small leakage factors only when necessary to preserve stability, while providing mean leakage factors near unity to maximize performance. Through application of these adaptive tuning parameters developed using the Lyapunov tuning approach, continual updating of the tuning parameters preserves stability and performance in non-ideal, real world noise fields described in [0005].\n\nSummary of Experimental Results\n","Three candidate tuning laws that result from the Lyapunov tuning approach of the invention have been implemented and tested experimentally for low frequency noise cancellation in a prototype communication headset. The prototype headset consists of a shell from a commercial headset, which has been modified to include ANR hardware components, i.e., an internal error sensing microphone, a cancellation speaker, and an external reference noise sensing microphone. For experimental evaluation of the ANR prototype headset, the tuning method of the present invention is embodied as software within a commercial DSP system, the dSPACE DS 1103.","A block diagram , , shows one implementation of the present invention. The preferred embodiment of the \u2018Adaptive Leaky LMS\u2019  contains a c-program that embodies the tuning method of the present invention, although a software implementation is not specific to nor a limitation of the present invention, but is applicable to all feedforward adaptive noise cancellation system embodiments. The three inputs to the Adaptive Leaky LMS block are the reference noise \u2032, the error microphone , and a \u2018reset\u2019 trigger  that is implemented for experimental analysis. The output signals are the acoustic noise cancellation signal , the tuned parameters \u03bb() and \u03bc(), and the filter coefficients .","The stability and performance of the resulting Active Noise Reduction (ANR) system has been investigated for a variety of noise sources ranging from deterministic discrete frequency components (pure tones) and stationary white noise to highly nonstationary measured F-16 aircraft noise over a 20 dB dynamic range. Results demonstrate significant improvements in stability of the adaptive leaky LMS algorithm disclosed (Eq. 3\u20134) over traditional leaky or non-leaky normalized algorithms, while providing noise reduction performance equivalent to that of a traditional NLMS algorithm for idealized noise fields. Performance comparisons have been made as a function of signal-to-noise ratio (SNR) as well, showing a substantial improvement in ANR performance at low SNR.","Performance of the prototype communication headset ANR system , , employing the disclosed tuning method has been experimentally compared with a commercial electronic noise cancellation headset that uses a traditional feedback ANR algorithm. Both headsets were evaluated within a low frequency test cell  specifically designed to provide a highly controlled and uniform acoustic environment.","To perform the evaluation, a calibrated B&K microphone  was placed in the base of the test cell . A Larson-Davis calibrated microphone  with a wind boot was placed in the side  of the test cell , approximately 0.25 inches from the external reference noise microphone  of the headset  under evaluation. The Larson Davis microphone  measured the sound pressure level of the external noise when the headset  is in the test cell . The B&K microphone , which was mounted approximately at the location of a user's ear, was used to record sound pressure level (SPL) attenuation performance. With this test setup, each headset was subject to a sum of pure tones at 50, 63, 80, 100, 125, 160, and 200 Hz and 100 dB SPL. Both the passive attenuation and total attenuation were measured.","The active and passive attenuation of each headset, as measured by the power spectrum of the difference between the external Larson-Davis microphone  and internal B&K microphone  is recorded in  respectively. The ANR prototype headset that uses the disclosed automatic tuning algorithm achieves superior active SPL attenuation at all frequencies in the 50\u2013200 Hz band as measured at the B&K microphone . Passive noise attenuation of the commercial headset  is superior to the prototype headset , which being a prototype, was not optimized for passive performance.","These measured results demonstrate that a headset with the combination of current technology in passive performance, and the superior active performance provided by the disclosed tuning method can achieve 30\u201335 dB SPL attenuation of low frequency stationary noise at the ear over the 50 to 200 Hz frequency band. This is a significant improvement over commercially available electronic feedback noise cancellation technology. There is both a theoretical and experimental basis for extending this performance over a wider frequency range. Additional test results are discussed below.","Review of the Leaky Least Mean Square (LMS) Algorithm","A review of the LMS algorithm and its leaky variant follows. Denoting X\u03b5Ras the reference input at time tand d\u03b5Ras the output of the unknown process, the LMS algorithm recursively selects a weight vector W\u03b5Rto minimize the squared error between dand the adaptive filter output WX.","The cost function is \n\n\nwhere\n\n.\u2003\u2003(6)\n\nThe well-known Wiener solution, or optimum weight vector is\n\nW=E[XX]E[Xd]\u2003\u2003(7)\n\nwhere E[XX] is the autocorrelation of the input signal and E[Xd] is the cross correlation between the input vector and process output. The Wiener solution reproduces the unknown process, such that d=WX.\n","By following the stochastic gradient of the cost surface, the well-known unbiased, recursive LMS solution is obtained:\n\n\u2003\u2003(8)\n\nStability, convergence, and random noise in the weight vector at convergence are governed by the step size \u03bc. Fastest convergence to the Wiener solution is obtained for \n\n\nwhere \u03bbis the largest eigenvalue of the autocorrelation matrix E[XX].\n","As an adaptive noise cancellation method, LMS has some drawbacks. First, high input power leads to large weight updates and large excess mean-square error at convergence. Operating at the largest possible step size enhances convergence, but also causes large excess mean-square error, or noise in the weight vector, at convergence. A nonstationary input dictates a large adaptive step size for enhanced tracking, thus the LMS algorithm is not guaranteed to converge for nonstationary inputs.","In addition, real world applications necessitate the use of finite precision components, and under such conditions, the LMS algorithm does not always converge in the traditional form of eq. 4, even with an appropriate adaptive step size. Finally, nonpersistent excitation due to a constant or nearly constant reference input, such as can be the case during \u2018quiet periods\u2019 in adaptive noise cancellation systems with nonstationary inputs, can also cause weight drift.","In response to such issues, the leaky LMS (LLMS) algorithm or step-size normalized versions of the leaky LMS algorithm \u201cleak off\u201d excess energy associated with weight drift by including a constraint on output power in the cost function to be minimized. Minimizing the resulting cost function, \n\n\nresults in the recursive weight update equation\n\n\u2003\u2003(10)\n\nwhere \u03bb=1\u2212\u03b3\u03bc is the leakage factor. Under conditions of constant tuning parameters \u03bb and \u03bc, no measurement noise or finite-precision effects, and bounded signals Xand e, eq. 6 converges to: \n\n\nas k\u2192\u221e. Thus, for stability 0\u2266\u03bb\u22661 is required. The lower bound on \u03bb assures that the sign of the weight vector does not change with each iteration.\n","The traditional constant leakage factor leaky LMS results in a biased weight vector that does not converge to the Wiener solution and hence results in reduced performance over the traditional LMS algorithm and its step size normalized variants.","The prior art documents a 60 dB decrease in performance for a simulated a leaky LMS over a standard LMS algorithm when operating under persistently exciting conditions. Hence, the need is to find time varying tuning parameters that maintain stability and retain maximum performance of the leaky LMS algorithm in the presence of quantifiable measurement noise and bounded dynamic range.","Lyapunov Tuning of the Leakage Factor","In the presence of measurement noise Q\u03b5Rcorrupting the reference signal X, and with time varying leakage and step size parameters, \u03bband \u03bc, the LLMS weight update equation becomes\n\n=\u03bb+\u03bc(())()\u2003\u2003(12)\n\nThe stability analysis objective is to find operating bounds on the variable leakage parameter \u03bband the adaptive step size \u03bcto maintain stability in the presence of noise vector Qwhose elements have known variance, given the dynamic range or a lower bound on the signal-to-noise ratio.\n","For stability at maximal performance, the present invention seeks time-varying parameters \u03bband \u03bcsuch that certain stability conditions on a candidate Lyapunov function Vare satisfied for all k in the presence of quantifiable noise on reference input X. Moreover, the choice of \u03bband \u03bcshould be dependent on measurable quantities, such that a parameter selection algorithm can be implemented in real-time. Finally, the selection algorithm should be computationally efficient. For uniform asymptotic stability, the Lyapunov stability conditions are:\n\ni) V\u22670\u2003\u2003(13)\n\nii) <0\u2003\u2003(14)\n\nand a decrescent Lyapunov function is required, i.e., V=0 at W=0, and V<V* for all k\u22670, where V* is a time-invariant scalar function of W. Finally, for global uniform asymptotic stability, the scalar function V* must be radially unbounded, such that \n\n\nDevelopment of the candidate Lyapunov function proceeds by first defining {tilde over (W)}=W\u2212W. Eq. 12 becomes\n\n=(\u03bb()())+(\u03bb())\u2003\u2003(16)\n\nSince scalar tuning parameters \u03bband \u03bcare required, {tilde over (W)}and {tilde over (W)}are projected in the direction of X+Q, as shown in : \n\n\nCombining Eq. 16 through 18 and simplifying the expression gives \n\n\nA candidate Lyapunov function satisfying stability condition i) above (Eq. 13), is\n\nV={tilde over (w)}{tilde over (w)}\u2003\u2003(20)\n\nthus the Lyapunov function difference is\n\n\u2003\u2003(21)\n\nThe expression for the projected weight update in Eq. 19 can be simplified as\n\n=(\u03c6+\u03b3)+\u03b3\u03b1\u2003\u2003(22)\n\nwhere \n\n\nis the unit vector in the direction of X+Q, and\n\n\u03c6=\u03bb\u2212\u03bc()()\u2003\u2003(24)\n\n\u03b3=\u03bb\u22121\u2003\u2003(25)\n",{"@attributes":{"id":"p-0056","num":"0055"},"br":{},"in-line-formulae":[{},{}],"sub":["2",{"sub2":"k"},"k","k","k","k","k"],"i":["X","+Q","X","+Q"],"sup":"T","maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["\u03b1","k"]},"mo":"=","mfrac":{"msub":{"mi":["Q","k"]},"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mi":["X","k"]},{"mi":["Q","k"]}],"mo":"+"}}}}},{"mrow":{"mo":["(",")"],"mn":"27"}}]}}}}},"With these definitions, the Lyapunov function difference becomes, \n\n\nNote that the projected weight vector of Eq. 17 and 18 and the resulting Lyapunov function candidate of Eq. 20 do not satisfy condition Lyapunov stability condition iii) (Eq. 15), which is required for global uniform asymptotic stability. However, it is possible to find a time-invariant scalar function V* such that the Lyapunov candidate V<V* for all k>0.\n","Since the scalar projection is always in the direction of the unit vector defined by eq. 16, an example of such a function is V*=10{tilde over (W)}{tilde over (W)}. Hence, the Lyapunov function can be used to assess uniform asymptotic stability.","Note also that there are two conditions that may be considered problematic with the projected weight vector. These occur if (a) X=\u2212Qor (b) {tilde over (W)}is orthogonal to uor some component of {tilde over (W)}is orthogonal to u. Condition (a) is highly unlikely, especially at realistic tap lengths and signal-to-noise ratios (SNR). In fact, if this condition does occur, then, intuitively, it must be the case that SNR is so low that noise cancellation is futile, since the noise floor effectively dictates the maximum performance that can be achieved.","If {tilde over (W)}is orthogonal to uunder reasonable SNR conditions, then it is likely that the filter output eis very close to zero, i.e., the LMS algorithm is simply unnecessary if such a condition persists. Thus, though it is possible, but unlikely, that one or more of the weight vector components could become unbounded, in considering such unlikely occurrences it is impossible to avoid serious performance degradation.","The goal of the Lyapunov analysis is to enable quantitative comparison of stability and performance tradeoffs for candidate tuning rules. Since uniform asymptotic stability suffices to make such comparisons, and since the Lyapunov function of Eq. 20 enhances the ability to make such comparisons, it was selected for the analysis that follows.","Several approaches to examining Lyapunov stability condition ii) V\u2212V<0 for Eq. 28 exist. The usual approach to determining stability is to examine V\u2212Vterm by term to determine whether the two parameters \u03bband \u03bccan be chosen to make each term negative thereby guaranteeing uniform asymptotic stability. Since there are several terms that are clearly positive in Eq. 28, there is no guarantee that each individual term will be negative. Furthermore, it is clear from an analysis of Eq. 28 that the solution is nearly always biased away from zero. At {tilde over (W)}=W\u2212W=0, Eq. 28 becomes:\n\n=\u03b3+\u03b3\u03b1\u03b1+2\u03b3\u03b3\u03b1\u2003\u2003(29)\n\nFor 0<\u03bb<1, all coefficients of terms in Eq. 29 are positive, and it is clear that a negative definite V\u2212Vresults only if \u03b3WuuW+\u03b3W\u03b1\u03b1W<\u22122\u03b3\u03b3Wu\u03b1Wwith \u03b3\u03b3>0. That the leaky LMS algorithm, as examined using the Lyapunov candidate of Eq. 20, is biased away from Wis in agreement with the prior art. It is possible, but difficult, to examine the remaining space of {tilde over (W)}=W\u2212W(i.e., the space that excludes the origin) to determine whether time varying tuning parameters can be found to guarantee stability of some or all other points in the space or a maximal region of the space.\n","Time varying tuning parameters are required since constant tuning parameters found in such a manner will retain stability of points in the space at the expense of performance. However, since we seek time varying leakage and step size parameters that are uniquely related to measurable quantities and since the Wiener solution is generally not known a priori, the value of such a direct analysis of the remaining space of {tilde over (W)}=W\u2212Wis limited.","Thus, the approach taken in the present invention is to define the region of stability around the Wiener solution in terms of parameters: \n\n\nand to parameterize the resulting Lyapunov function difference such that the remaining scalar parameter(s) can be chosen by optimization.\n","The parameters A and B physically represent the output error ratio between the actual output and ideal output for a system converged to the Wiener solution, and the output noise ratio, or portion of the ideal output that is due to noise vector Q. Physically, these parameters are inherently statistically bounded based on i) the maximum output that a real system is capable of producing, ii) signal-to-noise ratio in the system, and iii) the convergence behavior of the system. Such bounds can be approximated using computer simulation. These parameters provide convenient means for visualizing the region of stability around the Wiener solution and thus for comparing candidate tuning rules.","In a persistently excited system with high signal-to-noise ratio, B approaches zero, while the Wiener solution corresponds to A=0, i.e., W=W. Thus, high performance and high SNR operating conditions imply both A and B are near zero in the leaky LMS algorithm, though the leaky solution will always be biased away from A=0. In a system with low excitation and\/or low signal-to-noise ratio, larger instantaneous magnitudes of A and B are possible, but it is improbable that the magnitude of either A or B is >>1 in practice. Note that B depends only on the reference and noise vectors, and thus it cannot be influenced by the choice of tuning parameters. B can, however, affect system stability.","Using parameters A and B, Eq. 28 becomes \n\n\nBy choosing an adaptive step size and\/or leakage parameter that simplifies analysis of Eq. 32, one can parameterize and subsequently determine conditions on remaining scalar parameters such that V\u2212V<0 for the largest region possible around the Wiener solution. Such a region is now defined by parameters A and B, providing a means to graphically display the stable region and to visualize performance\/stability tradeoffs introduced for candidate leakage and step size parameters.\n\nComparison of Candidate Tuning Laws Using Lyapunov Analysis\n","To demonstrate the use of the parameterized Lyapunov difference of Eq. 32, consider three candidate leakage parameter and adaptive step size combinations.","The first candidate uses a traditional choice for leakage parameter in combination with a traditional choice for adaptive step size to provide:",{"@attributes":{"id":"p-0070","num":"0069"},"br":{},"in-line-formulae":[{},{}],"sub":["k","k","q"],"sup":"2","maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["\u03bc","k"]},"mo":"=","mfrac":{"msub":{"mi":"\u03bc","mn":"0"},"mrow":{"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["X","k"]},{"mi":["Q","k"]}],"mo":"+"}},"mi":"T"},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["X","k"]},{"mi":["Q","k"]}],"mo":"+"}}}}}},{"mrow":{"mo":["(",")"],"mn":"34"}}]}}}}},"wherein \u03c3is the variance of quantifiable noise corrupting each component of vector X. This choice results in a simple relationship for the constants in Eq. 32\n\n\u03c6=\u03bb\u2212\u03bc\u2003\u2003(35)\n\n\u03b3=\u2212\u03bc\u2003\u2003(36)\n\nThus, the combined candidate step size and leakage factor parameterize Eq. 32 in terms of \u03bc.\n","To determine the optimal \u03bc, one can perform a scalar optimization of V\u2212Vwith respect to \u03bcand evaluate the result for worst-case constants A and B. In essence, one seeks the value of \u03bcthat makes V\u2212Vmost negative for worst-case deviations of weight vector Wfrom the Wiener solution and for worst-case effects of measurement noise Q. Worst case A and B are chosen to be that combination in the range A\u2266A<0 and 0<A\u2266A, B\u2266B\u2266Bthat provides the smallest (i.e., most conservative) step size parameter \u03bc.","For example, for A=B=\u22121 and A=B=1, and the traditional adaptive leakage parameter and step size combination of Eq. 33 and 34, this optimization procedure results in \u03bc=\u2153, which is consistent with the choice for \u03bc.","The second candidate also retains the traditional leakage factor of Eq. 34, and finds an expression for \u03bcas a function of the measured reference input and noise covariance directly by performing a scalar optimization of V\u2212Vwith respect to \u03bc. Again, the results are evaluated for worst-case conditions on A and B, as described above. This scalar optimization results in \n\n","The final candidate appeals to the structure of Eq. 32 to determine an alternate parameterization as a function of \u03bc. Selecting \n\n\nresults in\n\n\u03c6=(1\u2212\u03bc)\u03bb\u2003\u2003(40)\n\n\u03b3=\u2212\u03bc\u03bb\u2003\u2003(41)\n\n\u03b3=\u03bb\u22121\u2003\u2003(42)\n\nThe expression for \u03bbin Eq. 39 is not measurable, but it can be approximated as \n\n\nwherein L is the filter length.\n","Equation 43 is a function of statistical and measurable quantities, and is a good approximation of Eq. 39 when \u2225X\u2225>>\u2225Q\u2225. The corresponding definitions of \u03c6, \u03b3, \u03b3, and \u03bc, Eq. 32 becomes \n\n\nThe optimum \u03bcfor this candidate, which is again found by scalar optimization subject to worst case conditions on A and B is \u03bc=\u00bd.\n","In summary, the three candidate adaptive leakage factor and step size solutions are Candidate : Eq. 33 and 34, Candidate : Eq. 33 and 37, and Candidate : Eq. 38 and 43. All are computationally efficient, requiring little additional computation over a fixed leakage, normalized LMS algorithm, and all three candidate tuning laws can be implemented based on knowledge of the measured, noise corrupted reference input, the variance of the measurement noise, and the filter length.","To evaluate stability and performance tradeoffs, one examines V\u2212Vfor various instantaneous signal-to-noise ratios |X|\/|Q| (SNR), and 1>A>\u22121, 1>B>\u22121.",{"@attributes":{"id":"p-0078","num":"0077"},"figref":["FIG. 6","FIGS. 6A\u20136C","FIGS. 6D\u20136F","FIGS. 6G\u20136I","FIG. 6","FIG. 7","FIG. 6"],"sub":["k+1","k "]},"Note again, that A=0 corresponds to the LMS Wiener solution. At sufficiently high SNR, for all candidates, V\u2212V=0 for A=B=0, i.e., operation at the Wiener solution with Q=0. A notable exception to this is candidate , for which V\u2212V>0 for A=0 and B=0 and SNR=2, due to the breakdown of the approximation of the leakage factor in Eq. 43 for low SNR.","For A=0 and B>0, the Wiener solution is unstable, which is consistent with the bias of leaky LMS algorithms away from the Wiener solution. The uniform asymptotic stability region in  is the region for which V\u2212V<0. At sufficiently high SNR, this stability region is largest for candidate , followed by candidate . Candidate  provides the smallest overall stability region.","For example, if one takes a slice of each  at B=\u22121, the resulting range of A for which V\u2212V>0 is largest for candidate . However, the likelihood of obtaining such combinations of A and B in practice is remote for sufficiently high SNR and a stationary or slowly time varying Wiener solution. Near the origin, which is the most likely operating point, the stability region for all three candidates is similar for sufficiently high SNR.","Performance of each candidate tuning law is assessed by examining both the size of the stability region and the gradient of V\u2212Vwith respect to parameters A and B. Note from Eq. 32 that the gradient of V\u2212Vapproaches zero as \u03bbapproaches one and \u03bcapproaches zero (i.e., stability, but no convergence). In the stable region of , the gradient of the Lyapunov difference is larger for tuning that provides an aggressive step size.","Thus, a tuning law providing a more negative V\u2212Vin the stable region should provide the best performance, while the tuning law providing the largest region in which V\u2212V<0 provides the best stability.  records the maximum and minimum values of V\u2212Vfor the range of A and B examined, showing candidate  should provide the best performance (and least stability), while candidate  provides the best overall stability\/performance tradeoff for high SNR, followed by candidates  and .","For all three candidates, leakage factor approaches one as signal-to-noise ratio increases, as expected, and candidate  provides the most aggressive step size, which relates to the larger gradient of V\u2212Vand thus the best predicted performance. An alternate view of V\u2212Vas it relates to performance is to consider V\u2212Vas the rate of change of energy of the system. The faster the energy decreases, the faster convergence, and hence the better performance.","The results of this stability analysis do not require a stationary Wiener solution, and thus these results can be applied to reduction of both stationary and nonstationary X. The actual value of the Wiener solution, which is embedded in the parameters A and B does affect the stability region, and it is possible, that any of the three candidates can be instantaneously unstable given an inappropriate combination of A and B.","Nevertheless, it is appropriate to use the graphical representation of  to determine how close to the Wiener solution one can operate as a measure of performance and to use the size of the stability region as a measure of stability. In cases where the Wiener solution is significantly time variant, the possibility of operating far from the Wiener solution increases, requiring more attention to developing candidate tuning laws that enhance the stability region for larger magnitudes of parameters A and B.","Experimental Results","The three candidate Lyapunov tuned leaky LMS algorithm are evaluated and compared to i) an empirically tuned, fixed leakage parameter leaky, normalized LMS algorithms (LNLMS), and ii) an empirically tuned normalized LMS algorithm with no leakage parameter (NLMS). The comparisons are made for a low-frequency single-source, single-point noise cancellation system in an acoustic test chamber (, ) designed to provide a highly controlled and repeatable acoustic environment with a flat frequency response over the range of 0 to 200 Hz for sound pressure levels up to 140 dB.","The system under study is a prototype communication headset earcup. The earcup contains an external microphone to measure the reference signal, an internal microphone to measure the error signal, and an internal noise cancellation speaker to generate y. Details regarding the prototype are given above in connection with .","The reference noise is from an F-16, a representative high-performance aircraft that exhibits highly nonstationary characteristics and substantial impulsive noise content. The noise source is band limited at 50 Hz to maintain a low level of low frequency distortion in the headset speaker and 200 Hz, the upper limit for a uniform sound field in the low frequency test cell.",{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 8"},"The noise floor of the test chamber  is 50 dB. Without active noise cancellation, the earmuff provides approximately 5 dB of passive noise reduction over the 50 to 200 Hz frequency band. The amplitude of the reference noise source is established to evaluate algorithm performance over a 20 dB dynamic range, i.e., sound pressure levels of 80 dB and 100 dB, as measured inside the earcup after passive attenuation. The difference in sound pressure levels tests the ability of the tuned leaky LMS algorithms to adapt to different signal-to-noise ratios.","The two noise amplitudes represent signal-to-noise ratio (SNR) conditions for the reference microphone measurements of 35 dB and 55 dB, respectively. For the F-16 noise source and 100 dB SPL (55 dB SNR), analysis of V\u2212Vof Eq. 32 for Lyapunov tuned candidates shows statistically determined bounds on B of \u22120.6<B<0.6, while for the 80 dB SPL (35 dB SNR), statistically determined bounds on B are \u22123<B<3. Thus, , which gives the V\u2212Vsurface for each candidate algorithm, shows that by lowering SNR to 35 dB, instability is possible for all three candidates, as the fixed step size is chosen for worst case conditions on B of \u22121<B<1.","Thus, in addition to eliciting stability and performance tradeoffs, the 80 dB SPL noise source tests the limits of stability for the three candidate algorithms. The quantization noise magnitude is 610e-6 V, based on a 16-bit round-off A\/D converter with a \u00b110 V range and one sign bit. The candidate LMS algorithms are implemented experimentally using a dSPACE DS1103 DSP board. A filter length of 250 and weight update frequency of 5 kHz are used. The starting point for the noise segments used in the experiments is nearly identical for each test, so that noise samples between different tests overlap.","In the first part of this comparative study, the empirically tuned NLMS and LNLMS filters with constant leakage parameter and the traditional adaptive step size of Eq. 34 are tuned for the 100 dB SPL and subsequently applied without change to the system for the 80 dB SPL. On the other hand, the constant leakage parameter LNLMS filter is empirically tuned for 80 dB and subsequently applied to the 100 dB SPL test condition.","These two empirically tuned algorithms are denoted LNLMS(100) and LNLMS(80), respectively. For both filters, \u03bc=\u2153, and the respective leakage parameter is given in . Application of the algorithm tuned for a specific SPL to cancellation of noise not matching the tuning conditions demonstrates the loss of performance that results under constant tuning parameters that would be required for a noise cancellation system subject to this 20 dB dynamic range. In all experiments, the weight vector elements are initialized as zero.",{"@attributes":{"id":"p-0096","num":"0095"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0097","num":"0096"},"figref":["FIG. 11","FIGS. 12A and 12B","FIGS. 10 through 12"]},"The Lyapunov based tuning approach provides a candidate algorithm that retains stability and satisfactory performance in the presence of the nonstationary noise source over the 20 dB dynamic range, i.e., at both 80 and 100 dB SPL.  shows performance at 100 dB SPL, and  shows performance at 80 dB SPL.","At 100 dB SPL (), all three candidate algorithms retain stability, and at steady-state, noise reduction performance of all three candidate algorithms exceeds that of empirically tuned leaky LMS algorithms. In fact, performance closely approximates that of the NLMS algorithm, which represents the best possible performance for a stable system, as it includes no performance degradation due to a leakage bias.","At 80 dB SPL (), candidates  and  are unstable at 80 dB SPL, reflecting the fact that candidate algorithms do not necessarily guarantee uniform asymptotic stability when assumptions regarding bounds on measurement noise are exceeded. Candidate , which was predicted by Lyapunov analysis to provide the best stability characteristics of the three candidates retains stability and provides a steady-state SPL attenuation exceeding that of the LNLMS(80) by 5 dB.","Since the LNLMS(80) is the best performing stable fixed leakage parameter algorithm available, the performance improvement is significant. Note that comparison of performance at 80 dB SPL to the NLMS algorithm cannot be made, because the NLMS algorithm is unstable for the 80 dB SPL (35 dB SNR).",{"@attributes":{"id":"p-0102","num":"0101"},"figref":"FIG. 15","b":"3"},"Performance gains of Lyapunov tuned candidates over the fixed leakage parameter LMS algorithms are confirmed by the mean and variance of the leakage factor for each candidate, as shown in . For all three candidates, the variance of the leakage factor is larger for the 80 dB test condition that for the 100 dB condition, as expected, since the measured reference signal at 80 dB represents lower average and instantaneous signal-to-noise ratios. Moreover, with the exception of candidate  at 80 dB, the mean leakage factor is larger than that provided by empirical tuning.","Hence, on average, the Lyapunov tuned LMS algorithms are more aggressively tuned and operate closer to the Wiener solution, providing better performance over a large dynamic range than constant leakage factor algorithms.","Finally, relative performance, which is predicted to be most aggressive for candidate , followed by candidates  and , respectively, is seen in . Candidate  provides the fastest convergence and the largest SPL attenuation of the three candidates.","The experimental results provide evidence that the method of tuning an adaptive Leaky LMS Filter according to the algorithm of the present invention provides stability and performance gains which result in the reduction of highly nonstationary noise for an optimized combination of both adaptive step size and adaptive leakage factor without requiring empirical tuning, with candidate  providing the best overall stability and performance tradeoffs.","In accordance with another aspect of the present invention, hybridization of a traditional feedback control law with a feedforward control law improves ANR performance and stability margins. The Lyapunov-tuned feedforward controller described herein has excellent response in systems with time-varying signal-to-noise ratio. Acting alone, the algorithm(s) disclosed above substantially improves ANR performance over traditional LMS filters and exhibits excellent performance for non-stationary noise sources, and good performance for non-stationary noise sources.",{"@attributes":{"id":"p-0108","num":"0107"},"figref":"FIG. 17","b":["100","102","104","106","108"],"sub":"k"},"In experimental evaluation of the architecture, a broadband, feedback controller providing 5\u201310 dB of attenuation in the 40 Hz to 1600 Hz frequency band is paired with the feedforward controller, which is tuned according to one aspect of the present invention. Both the feedback and feedforward components are implemented digitally. Because of this, no additional hardware components are required to add the feedback component beyond those used for the feedforward controller.  shows sample experimental results. At low frequencies (<100 Hz), the feedback component provides 7\u20138 dB of active attenuation, and the feedforward component, which is tuned according to method disclosed herein provides 15\u201327 dB of attenuation. However, the hybrid system demonstrates overall performance that is greater than the sum of the individual components at frequencies below 80 Hz. The exceptional performance of the hybrid system is achieved by pairing the feedforward controller tuned in accordance with the method disclosed herein with the traditional infinite impulse response feedback controller.","It is known that feedback controllers exhibit less sensitivity than feedforward controllers to noise source characteristics. Thus, for non-stationary noise sources, the hybrid system exhibits the positive characteristics of the Lyapunov-tuned feedforward system combined with the positive characteristics of a feedback controller in exhibiting less sensitivity to noise source characteristics.",{"@attributes":{"id":"p-0111","num":"0110"},"figref":"FIG. 19","sub":"ff"},"It is important to note that the present invention is not intended to be limited to a device or method which must satisfy one or more of any stated or implied objects or features of the invention. It is also important to note that the present invention is not limited to the preferred, exemplary, or primary embodiment(s) described herein. Modifications and substitutions by one of ordinary skill in the art are considered to be within the scope of the present invention, which is not to be limited except by the following claims."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["These and other features and advantages of the present invention will be better understood by reading the following detailed description, taken together with the drawings wherein:",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIGS. 4A and 4B"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIGS. 6A\u20136I","sub":["k+1","k"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 7","FIG. 6"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIGS. 12A and 12B"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 13","b":["1","2","3"]},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 14","b":["1","2","3"]},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 19"}]},"DETDESC":[{},{}]}
