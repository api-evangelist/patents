---
title: Data transfer optimization
abstract: A dataset structure that is suitable for transferring data between service components may include a table having one or more rows and a plurality of columns. The table may be defined by a schema for an entity type. The schema may include a corresponding column definition for each column. Each of the rows may store a tuple of information that conforms to the entity type defined by the schema. A data transfer component may use the dataset structure to transfer data between the service components.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08874620&OS=08874620&RS=08874620
owner: Amazon Technologies, Inc.
number: 08874620
owner_city: Seattle
owner_country: US
publication_date: 20110615
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","DETAILED DESCRIPTION","Overview","CONCLUSION"],"p":["The manipulation of data in an efficient and predictable manner is a desired operational goal of applications that include service oriented architectures (SOAs). SOAs may include both interactive and background components that perform many different computing services. For example, an e-commerce application that serves millions of customers may include many thousands of servers located in numerous data center across the world. Such servers may be running hundreds of computing services that facilitate product recommendations, product ordering, product fulfillment, transaction processing, fraud detection, product shipping, and so forth. The SOAs are designed to manipulate data in an efficient and predictable manner, which is important when transferring data between different computing services.","When the computing services are used to fulfill a customer's order, data may be serialized, transmitted, and then deserialized across many different computing services, which may occur multiple times. Serialization is a transformation of a data structure or a data object into data bytes of a different format for transmission across a network and\/or storage in a storage node. Deserialization is the extraction of a data structure and data objects from the series of data bytes. However, the serialization, transfer, and deserialization of data across multiple services may result in latency and bottlenecks. Further, the transfer of data may be inefficient as each computing service often only consumes or produces a small portion of the data.","This disclosure is directed, in part, to a data transfer component that implements a light weight in-memory dataset structure that may facilitate the transfer of data between services. The data transfer component may be a library that is accessible to one or more applications that desire to transfer data between the services. In some embodiments, the data transfer component may be implemented in conjunction with a data transfer middleware framework. The data transfer middleware framework may connect the applications to one or more data stores that are implemented in a cloud computing environment. The one or more applications may include an application that stores and retrieves data, such as a web-based commerce application, an online banking application, an inventory tracking application, and\/or any other type of application. The data stores may store the data for the one or more applications and provide the data to each application after a corresponding application request.","In various embodiments, a dataset structure that is suitable for transferring data between service components may include a table having one or more rows and a plurality of columns. The table may be defined by a schema for an entity type. The schema may include a corresponding column definition for each column. Each of the rows may store a tuple of information that conforms to the entity type defined by the schema. A data transfer component may use the dataset structure to transfer data between the service components.","Further, a variable row size table or a fixed row size table may be implemented as the type of table in the dataset structure. Accordingly, the selection of a type of table for implementation may include testing each of a variable row size table or a fixed row size table for an ability to support the transfer of data between a sending service component and a receiving service component. Depending on the results of the testing, data may be stored using one of the variable row size table or the fixed row size table that supports the fastest data transfer between the sending service component and the receiving service components.","The techniques and systems described herein may be implemented in a number of ways. Example implementations are provided below with reference to the following figures.","Illustrative System Architecture",{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 1","b":["100","100","102","104","104"]},"Each of the client devices  may be an electronic device that is capable of receiving, processing and transmitting data to another device. In various embodiments, each of the client devices  may be any one of a laptop computer, a desktop computer, a server, or another electronic device that is equipped with network communication components, data processing components, and at least one electronic display for displaying data. The client devices  may include one or more applications ()-(N) that transact data to the servers  via a network . For example, the application ()-(N) may include a web browser that sends data to and receives data from one or more applications  that are executing on the servers . The network  may be a local area network (LAN), a larger network such as a wide area network (WAN), or a collection of networks, such as the Internet. Protocols for network communication, such as TCP\/IP, may be used to implement the network . Although embodiments are described herein as using a network such as the Internet, other distribution techniques may be implemented.","The servers  may include processor(s)  and a memory . An operating system  may be stored in the memory . The operating system  may include components that enable the servers  to receive data via various inputs (e.g., user controls, network interfaces, and\/or memory devices) and process the data using the processors  to generate output. The operating system  may further include one or more components that present the output (e.g., display an image on an electronic display, store data in memory, transmit data to another electronic device, etc.). Additionally, the operating system  may include other components that perform various other functions generally associated with an operating system.","The processors  and memory  may implement the applications , service components , a data transfer component , one or more serializers , and a middleware framework . The processors  may also access data store(s) that are implemented in the memory . Each of the applications  may receive data from the applications ()-(N), process the data, and send process data back to the application ()-(N). For example, in the context of e-commerce, the applications  may include a web transaction application that receives and fulfills online purchase requests from users, an online banking application that provide users with web access to financial information, a corporate inventory application that keeps track of inventory in real time, and\/or the like.","The one or more service component  may be components that are called upon by the applications  to perform particular tasks. For example, in the context of e-commerce, the services components  may include a component that provides product recommendations, a component that processes product orders, a component that calculates a payment, a component that detects fraud, a components that processes shipments, and the like.","The data transfer component  may transfer data between the service components , the applications , as well as between other entities. The transaction of data between the applications  and the data stores may be facilitated by the middleware framework . For instance, the middleware framework  may serve to transform a data store request from the applications  into multiple data writes to the multiple data stores. Similarly, in another instances, the middleware framework  may serve to transform a data retrieval request from the applications  into multiple data reads from the multiple data stores in the middleware framework . However, in both of these instances, the goal of the middleware framework  is to handle the store requests and\/or retrieval requests seamlessly for the applications , regardless of the data store architecture of the data stores. Accordingly, the data transfer component  may work in conjunction with the middleware framework . For example, the data transfer component  may call the middleware frame  to store to the data stores and\/or retrieve data from the data stores.","In some instances, the data stores may have some usage constraints. For example, there may be a limit on the sizes of atomic data collections that are stored in the data stores so that a certain level of data access performance may be maintained. In another example, the data stores may use the principle of eventual consistency to store and retrieve data. Accordingly, data that may have been recently stored in the data stores may be unavailable for immediate retrieval as the data may need to proliferate to the different data stores to maintain eventual consistency.","The data transfer component  may use a light weight in-memory dataset structure  that resembles a database to manage the data that is transferred between the service components , the applications , and\/or the data stores. As shown, the dataset structure  may include a collection of tables , in which each table is associated with one of a collection of table types, also known as a schema, such as schema . The tables  may further include one or more rows, such as the row , as well as one or more columns, such as the column . The data transfer component  may include a plurality of application programming interfaces (APIs)  that interact with the dataset structure . In various embodiments, the APIs may be similar in nature to those used for interacting with databases. The APIs  may include APIs for, but are not limited to, APIs for schema creation, APIs for data modeling, APIs for create, read, update and\/or delete (CRUD) data with respect to the dataset structure , and\/or APIs to one or more serializers  that perform serialization\/deserialization. An example set of application programming interfaces (API) for interacting with the overall dataset structure  is illustrated in Table I.",{"@attributes":{"id":"p-0032","num":"0031"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE I"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Example APIs for Interaction with a Dataset"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"112pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Class","APIs","Description"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Dataset","setTables","A collection of tables and schemas"]},{"entry":[{},"getTables",{}]},{"entry":[{},"addTable",{}]},{"entry":[{},"getTable",{}]},{"entry":[{},"deleteTable",{}]},{"entry":[{},"size",{}]},{"entry":[{},"addTableType",{}]},{"entry":[{},"getTableType",{}]},{"entry":[{},"getTableTypes",{}]},{"entry":[{},"deleteTableType",{}]},{"entry":[{},"getTablesByType",{}]},{"entry":[{},"isDirty",{}]},{"entry":[{},"setClean",{}]},{"entry":[{},"iterator"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"The tables  may be a dataset structure class that includes one or more tables. However, the tables  may be empty and contain no data in some instances. Each table may stand alone, which may make it easy to create a set of temporary tables  that are not part of the dataset structure . Additionally, the table's class may serve as a convenient way to iterate over all the tables  in a database set. Each of the tables  is associated with a schema, such as one of the schemas .","A row of data, such as the row , may represent a tuple of information that is of the type defined in a schema, such as the schema . In other words, a row is a logical construct. When a row of data is added to a table, the data may be added as a collection of objects. Accordingly, when a row of data is extracted from a table, the data is extracted as a row from the table.","In various embodiments, a row of data may be related to one or more other rows of data in a relational database manner. Accordingly, the data transfer component  may manage such relationships using a custom code. For example, queries may be written in code rather than relying on a pre-established query language. Further, the data transfer component  may use custom code to perform constraint checking rather than rely on the use of the pre-established query language or pre-defined constraints for values in a particular row. In this way, the use of custom codes may free the dataset structure  from the operation limitations imposed by pre-established query languages.","A column definition for a column, such as the column , may define the column as containing the names of attributes, types of attributes, or whether each attribute is a piece of identifying information (e.g., primary key). An example set of APIs for interacting with the tables  is illustration in Table II.",{"@attributes":{"id":"p-0037","num":"0036"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE II"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Example APIs for interacting with Tables"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"84pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Class ","APIs","Description"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"Tables","addTable","Manages a collection of"]},{"entry":[{},{},"getTable","tables."]},{"entry":[{},{},"deleteTable",{}]},{"entry":[{},{},"getTables",{}]},{"entry":[{},{},"size",{}]},{"entry":[{},{},"isDirty",{}]},{"entry":[{},{},"setClean",{}]},{"entry":[{},{},"iterator",{}]},{"entry":[{},"Table","getSchema","Manages rows of data."]},{"entry":[{},{},"getTypeName",{}]},{"entry":[{},{},"addColumn",{}]},{"entry":[{},{},"addRow",{}]},{"entry":[{},{},"getRow",{}]},{"entry":[{},{},"getRowByKey",{}]},{"entry":[{},{},"deleteRowId",{}]},{"entry":[{},{},"deleteRowByKey",{}]},{"entry":[{},{},"deleteUsingBinaryRow",{}]},{"entry":[{},{},"updateRow",{}]},{"entry":[{},{},"length",{}]},{"entry":[{},{},"isDirty",{}]},{"entry":[{},{},"setClean",{}]},{"entry":[{},{},"iterator",{}]},{"entry":[{},"Column","getName","Manages columns of data."]},{"entry":[{},{},"getType",{}]},{"entry":[{},{},"isFixedLength",{}]},{"entry":[{},{},"isPrimaryKey"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}}},"Each of the schemas  may contain a collection of column definitions. The entirety of the schema defines a type. Accordingly, a plurality of tables  that share the same schema may be of the same type. A schema may be modified so long as there is no data in the dataset structure  that is using the schema. Conversely, once a piece of data is using the schema, then the schema may not be modified. A schema may be created in several ways. For instance, the schema may be directly shared across multiple tables of the tables  as a table type. In another instance, the schema may be implemented directly on a table, such as one of the tables . An example set of APIs for interacting with one or more schemas is illustrated below in Table III.",{"@attributes":{"id":"p-0039","num":"0038"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE III"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Example APIs for Interacting with Schemas"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"84pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"84pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Class","APIs","Description"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["TableSchema","add","A collection of columns that"]},{"entry":[{},"getColumnCount","define what a row (tuple) of "]},{"entry":[{},"getPkColumnCount","data looks like"]},{"entry":[{},"getColumns",{}]},{"entry":[{},"getIndexBlockSize",{}]},{"entry":[{},"isDirty",{}]},{"entry":[{},"setClean",{}]},{"entry":[{},"isValid",{}]},{"entry":[{},"getColumn",{}]},{"entry":[{},"isColumnPrimaryKey",{}]},{"entry":[{},"isFixedLength",{}]},{"entry":[{},"getFixedSize",{}]},{"entry":[{},"getFixedSchema",{}]},{"entry":[{},"hasPrimaryKey",{}]},{"entry":[{},"inUse",{}]},{"entry":[{},"getOverHeadColumnCount"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},"In various embodiments, the data transfer component  may use data types that consist of both reference types and value types. A reference type may be a value that is referenced with some form of pointer, while a value type is something that exists with a specific scope. Accordingly, reference types may reference non-existing values, such as when a reference is null. On the other hand, value types generally exist within their scope and are associated with values. The example types of data that are supported by the data transfer component  are illustrated below in Table IV.",{"@attributes":{"id":"p-0041","num":"0040"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE IV"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Example Data types Supported by the "},{"entry":"Data transfer component for a Dataset"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"119pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Type","Bytes","Description"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"Boolean","\u20021","True or False"]},{"entry":[{},"Byte","\u20021","Single byte integer"]},{"entry":[{},"Short","\u20022","Two byte integer"]},{"entry":[{},"Int","\u20024","Four byte integer"]},{"entry":[{},"Long","\u20028","Eight byte integer"]},{"entry":[{},"Float","\u20024","Four byte floating point number"]},{"entry":[{},"Double","\u20028","Eight byte floating point number"]},{"entry":[{},"Uuid","16","Universally unique identifier"]},{"entry":[{},"DateTime","\u20028","Date and time"]},{"entry":[{},"String","2 + size","String data in UTF8 encoding"]},{"entry":[{},"ByteArray","2 + size","Binary data"]},{"entry":[{},"Decimal","2 + size","Accurate numbers, no rounding errors "]},{"entry":[{},{},{},"(as is needed for financial transactions)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}}},"It will be appreciated that data of the variable length data type, such as String, ByteArray, and Decimal, may be set to a length of zero. However, such a length is not equivalent to null. Further, other data types, such as integers that may be 1, 2, 3, or 4 bytes in length, may be implemented by the data transfer component  in other embodiments.","The data transfer component  may use the one or more serializers  to convert data into different formats as the data is transferred between different service components. Such formats may include a custom binary format, Extensible Markup Language (XML) format, JavaScript Object Notation (JSON), and the like.","In various embodiments, in order to transfer data from a sending service component to a receiving service component, a serializer associated with the sending service component may perform one or more data conversions to serialize at least a portion of the formatted data into data bytes that are capable of being transmitted over a network to the receiving service component. The receiving service component may perform one or more data conversions to deserialize the received byte data and reconstitute the data into the original format or a new format for additional processing. Such serialization and deserialization may occur multiple times as an application  uses various service components  to process data, as the data may be sequentially passed through the service components  during the data processing.","For example, the serializers  may include a custom binary formatter. In various embodiments, the custom binary formatter may perform two type of serialization: full serialization and differential serialization. In such cases, the initial information produced by the custom binary formatter may include four bytes of data. The first two bytes of the data may be the characters \u201cd\u201d and \u201cs\u201d. The first two bytes may be followed by two single unsigned byte integers that represent the major version and minor version numbers. For instance, for version \u201c1.0\u201d, the bytes may be \u201c0100\u201d.","Additionally, the custom binary formatter may serialize the data in a dataset, such as the dataset structure , according to dataset, tables, or each table. In the serialized data, each of these may be signified with a different value. For example, a dataset may be signified by \u201c1\u201d, tables may be signified by \u201c2\u201d, and each table may be signified by \u201c3\u201d. Thus, if the serialized data starts off with \u201cd, s, 1, 0, 1\u201d, then the serialized data may be a serialized dataset that is serialized using a 1.0 version of the custom binary formatter. Further, all data that is variable in size, such as strings, arrays, and lists, may be preceded with a corresponding size designation. For instance, an array of six items may be preceded with six in the serialized data.","During a full serialization, the custom binary formatter may visit each class in the dataset structure to generate a complete copy of all the data in serialized form. Differential serialization is similar to full serialization, except that only pages of data that are marked as \u201cdirty\u201d are serialized. Thus, during transfer of data between a source service and a destination service using differential serialization, the data transfer component  may merge serialized \u201cdirty data\u201d from a source service with an unchanged data portion that is already present in a destination service to reconstitute the copy of the data at the destination service.",{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 2","b":["120","128","202","1","202","202","1","202"]},"When a page, such as the page (), is created, the data transfer component  may provide the page with a predetermined size. Subsequently, data may be added to a page until the size is reached. Once this occurs, the data transfer component  may mark the page as full so that no additional rows of data are added to the page. The data transfer component  may then start and populate a new page in the same manner. In this way, the data transfer component  may achieve minimal unused space, i.e., fragmentation. In at least one embodiment, the data transfer component  may generally constrain the size of the each page to 8 kilobytes (8K). In alternative embodiments, the data transfer component  may use other page size constraints. However, as further described below, such page constraints are intended to be non-absolute. Each of the pages ()-(N) may include a universally unique identifier (UUID), a revision number (RVN), and a value flag that is set to a value that represents \u201cdirty\u201d when the data in the page has changed. For example, the page () may include a UUID , a RVN , and a value flag .","Further, once a row, such as one of the rows , is created on a page, the data transfer component  may maintain that row of data on the page rather than allow the row of data to change pages. Each of the rows that are created may be assigned a row identifier by the data transfer component  that is permanently associated with the corresponding row. The row identifier may specify the designation (e.g., page UUID) of the page that a row resides in, as well as the row designation (e.g., row number) within the page of the row. Accordingly, the row identifiers that are assigned to rows in a page may be monotonically increasing values that are managed on a table-by-table level. This means that when a row is updated or deleted, the row nevertheless logically remains on the same page, and the data transfer component  may still check the page for the row of data if another request to interact with the row occurs after the row has already been deleted. Thus, by using such an implementation, the data transfer component  may track data changes at a page level, rather than at a row level. Further, a page may be labeled as either changed (i.e., \u201cdirty\u201d) or not changed, in which a dirty page includes at least one row that has been modified.","The tracking of changes at a page level may be more efficient in some scenarios in which an application, such as one of the applications , changes only a few data rows stored in the data stores, or when the data rows being changed are of the same type. Such selective updating is sparse in nature and rarely results in sweeping changes across the entire set of data at once. Thus, the data transfer component  may take advantage of this fact by tracking changes at a page level. In doing so, the data transfer component  may track such data changes without resorting to tracking changes at an object-by-object level, or at an attribute-by-attribute level as is the case with conventional database frameworks.","When data on a page is to be updated, the data transfer component  may either update the page in place or the page is rebuilt. In various embodiments, the data transfer component  may update a page in place when the page is a fixed row size page, and rebuild a page when the page is a variable row size page. The fixed row size page and the variable row size page are described flow in . The rebuilding of a page may involve several operations. First, the page is rebuilt. For example, as shown in , the page () with the UUID  may store data in three original sequential data rows , , and , which have the row identifiers , , and , respectively. The data row  is to be updated to include additional data bytes. Accordingly, the data transfer component  may first copy the data row  to a replacement page . The data transfer component  may then insert a replacement data row  for the data row  that includes the new data bytes into the replacement page . Subsequently, the data transfer component  may copy one or more data rows that follow the updated data row, such as the data row , into the replacement page . The data rows  and  may retain their row identifiers, i.e., row identifiers  and  respectively, while the data transfer component  may provide the replacement data row  with the row identifier  from the data row . The page () is then deleted, and the replacement page  may be provided with the UUID  of the page (), an updated RVN , and a value flag  that indicates the page is \u201cdirty\u201d.","Second, since the logical position of each row in a page is tracked by a corresponding absolute offset (row pointer) and one or more relative offsets (column offsets) in a corresponding page index, an absolute offset for each data row in the page that follows the updated data row  (e.g., the data row ) may be updated by having a delta value added to them. For example, if the new updated data row  is five bytes bigger after the update, then an absolute offset for the data row  may have five added to it. However, all the relative offsets may remain the same. Conversely, in another example, if the replacement data row  that replaced the data row  is five bytes smaller after the update, then an absolute offset to the data row  may have five subtracted from it, while the column offsets may remain the same. The use of absolute offsets and relative offsets to track the rows in a page is further described in detail below with respect to .","Alternatively, in the update in place of a page, the data transfer component  may initially modify a data row in the page to include modified data that has the same data byte length as the original data in the data row. The RVN of the page may then be updated, and the value flag of the page may be set to \u201cdirty\u201d. Further, since the modified data row includes data with the same data byte length, the absolute offset (row pointer) associated with each of the one or more data rows in the page that follows the modified data row as well as their corresponding relative offsets (column pointers) may remain the same.","In some embodiments, because the page size constraints are intended to be non-absolute, a data update to a page that results in the page exceeding a page size constraint is not problematic. For example, when a page has a page size constraint of 8K, as described above, and a string value that is 32K in size is added to a row in the page, the data transfer component  may grow the page in size to support the storage of this new string value. However, since updates that cause a page to exceed a predetermined page size constraint may happen infrequently, the impact of such occasional oversize pages on the overall operating efficiency of the dataset structure  may be minimal.","The tables  of the dataset structure  that are used by the data transfer component  may come in two forms: fixed sized row tables and variable row size tables. In a fixed sized row table, all of the columns of data in the table are of a fixed size. Conversely, if any column of data in a table is a variable size, then the table is a variable row size table. The use of the fixed row size tables or the variable row size tables as part of the dataset structure  are described below in .",{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 3","b":["302","304","306","308","310","312","314","316","318","320","322","310","320","310","316","318","120","310","310","120","310"]},"Likewise, by using the second value \u201c04\u201d in the column index , the data transfer component  may determine that the second column of data in \u201crow 3\u201d  starts at byte 40 in \u201crow 3\u201d  (as 36+4=40). Accordingly, the data transfer component  may determine that the second column of data in \u201crow 3\u201d  is \u201c00 00 00 BB\u201d. Further, by the using the third value \u201c08\u201d in the column index , the data transfer component may determine that the third column of data in \u201crow 3\u201d starts at byte 44 in \u201crow 3\u201d  (as 36+8=44). Accordingly, the data transfer component  may determine that the third column of data in \u201crow 3\u201d  is \u201c00 00 00 AA\u201d.",{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 4","b":["402","404","406","402","410","404","404","406","120","408","408"]},"Further, a 4-byte data block  that follows the data block  may be a relative offset (e.g., 00 00 00 04) for the second column  of data in the second row. Thus, by using the data block , the data transfer component  may locate the beginning of the second column . Accordingly, the data transfer component  may further determined that the second column  holds the UTF-8 value \u201c00 02 42 42\u201d, which translates into text \u201cBB\u201d. Another 4-byte data block  that follows the data block  may be the relative offset (e.g., 00 00 00 08) for a third column  in the second row. Once again, by using the data block , the data transfer component  may locate the beginning of the third column . Accordingly, the data transfer component  may determine that the third column  holds the hexadecimal value \u201c00 00 00 0A\u201d, which translates into the decimal value \u201c10\u201d. Thus, unlike in a fixed row size page, each row of data in a page may be provided with its own relative offsets for its columns in addition to an absolute offset, and such relative offsets cannot be shared across different rows.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 5","b":["500","500"]},"At block , a determination may be made as to whether a data processing scenario includes the use of at least one variable size data tuple. For example, the data processing scenario may be an e-commerce situation in which online orders for merchandise are received and merchandise are billed and shipped according to the online orders. At decision block , if it is determined that the data processing scenario may include the use of at least one variable size data tuple (\u201cyes\u201d at decision block ), the process  may proceed to block .","At block , the variable row size table may be selected for use with the data processing scenario. Each of the one or more data tuples used in the data processing scenario may be stored in a corresponding row of the variable row size table. In various embodiments, the transaction of data using the variable row size table may involve data transfers between entities, as well as create, read, update and\/or delete (CRUD) data with respect to a dataset structure (e.g., the dataset structure ) that implements the variable row size table. However, if it is determined that the data scenario does not include the use of any variable size data tuples (\u201cno\u201d at decision block ), the process  may proceed to block .","At block , the fixed row size table may be selected for use with the data processing scenario. Each of the one or more data tuples used in the data processing scenario may be stored in a corresponding row of the fixed row size table. In various embodiments, the transaction of data using the variable row size table may involve data transfers between entities, as well as create, read, update and\/or delete (CRUD) data with respect to a dataset structure (e.g., the dataset structure ) that implements the fixed row size table.","The performance of each type of table in the data processing scenario may be dependent upon factors such as the size of the data that is stored in each row of the tables in the dataset structure , network bandwidth, network speed, and\/or available processing resources (e.g., processing power, memory, etc.) of the computing devices that manipulate the data in the tables. A fixed row size table may take more bandwidth to transfer across a network than a variable row size table since each row in the fixed row size table has the same byte size regardless of the size of the actual data that is stored in each row. However, in some scenarios, any transfer efficiency gained by using a variable row size table over a fixed sized row table may be offset to various degrees by the transfer of additional index data across a network. This is because the index data in a variable row size table, which includes the relative offsets for the columns associated with every row of data, may be bigger in size than the index data for a fixed row size table.","Further, less processing power may be used to perform transactions on data in a fixed row size table than a variable row size table since the column offsets in the table are identical for each row. On the other hand, the performance of transactions on data in a variable row size table may consume more processing power as the columns in the rows of data cannot be systematically determined as in the case of a fixed row size table. Rather, the columns in each data row may be calculated using the corresponding absolute offset and column offsets before the data in each row may be transacted. For example, when network bandwidth is limited but processing power for transacting the data is ample at computing devices that are parties to a data transfer, one or more variable row size tables may provide faster performance when used with the data transfer component . However, when network bandwidth is abundant but processing power at the computing devices is limited, one or more fixed sized row tables may provide faster performance when used with the data transfer component .","The performance of each type of table in the data processing scenario may be measured using various criteria. In some embodiments, the performance of each type of table for the data processing scenario may be tested using the transfer and processing of actual data. In other embodiments, the performance of each type of table for the data processing scenario may be tested using simulated data transfer and processing transactions. The performance of each type of table may be gauged using one or more quantitative measurements (e.g., latency, overall data transfer speed, data read speed, data write speed, data error rate, data loss rate, and\/or the like), and\/or one or more qualitative measurements (e.g., accuracy of the data that is retrieved, load handling ability, and\/or the like).","In additional embodiments, the data transfer component  may implement a data streaming scheme that includes service components having internal pipelines. Such service components may further reduce the latency associated with using multiple service components  to process a piece of data. The use of such a data streaming scheme may enable the deserialization of a piece of data, processing of the piece of data, and the serialization of the piece data by the various service components to at least partially overlap so that overall latency may be reduced.",{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 6","b":["600","602","602","604","606","608"]},"The order service component  may be a service that receives an order identifier from the application , and further calls upon the storage service component  and the calculator service component  to process the order associated with the order identifier so that item and charge data associated with the order may be returned to the application .","Accordingly, in order for the application  to display charges for an online purchase order, an order identifier may be passed from the application  to the order service component  at step 1. At step 2, the order identifier is passed by the order service component  to the storage service component , which loads the appropriate order information from a data store, such as the data stores. At step 3, the storage service component  returns the order information to the order service component . The order service component  may then pass the order information to the calculator service component  at step 4. The calculator service component  may calculate the charges for the items listed in the order information, and may provide the charge information back to the order service component  at step 5. At step 6, the order service component  may then pass the item and charge information back to the application  for display.","In a conventional arrangement, a piece of data that is passed between two different services is initially serialized by a sending service component for transmission over a network to a receiving service component. The data is then deserialized by the receiving service component prior to being processed at the receiving service component. Thus, in the context of service components described in , serialization and deserialization of the entire data occurs at each of the steps 2-5, which may cause increased latency.","However, services components having internal pipelines may enable at least some overlapping of data serialization, data processing, and data deserialization. Such service components may reduce the latency experienced by a client application, such as the application . For example, as shown in , the calculator service component  may include an example internal streaming pipeline  that reduces the overall latency experienced by the application . The example streaming pipeline  may include a deserializer , a stream split module , a query module , a map module , a query module , a join module , a calculation module , a stream join module , and a serializer . Additionally, as described below in , a scaling component  may further instantiate modules for the streaming pipeline .","In operation, incoming data  arriving at the calculator service component  from the order service component  may enter the stream pipeline  to be deserialized by the deserializer . The incoming data  may include a set of data entities , such as items, item descriptions, purchase descriptions, and purchase charges, discounts, promotion, tax charges, and\/or the like. Following deserialization, the data entities  may be passed to the stream split module . The stream split module  may generate a duplicate set of data entities  based on the set of data entities , and send the duplicate set of data entities  to the stream joining module  to be serialized and sent out of the streaming pipeline  to the order service component  for further processing. The duplicate set of data entities  may be serialized by the serializer  and sent out of the calculator service at any time after duplication. For example, the duplicate set of data entities  may be serialized while one or more other modules in the streaming pipeline are simultaneously processing the data entities .","For example, the set of data entities  may be sent by the stream split module  to the query module . The query module  may parse out entities from the set of entities  that are purchase charges, i.e., belonging to the type \u201ccharge group\u201d. For example, purchase charges may include item cost, item shipping cost, item tax, and the like, for each purchased item. Subsequently, the set of entities may be further sent to the map module  and the query module  for processing. The query module  may parse out entities from the set of entities  that are line items, i.e., items that are purchased. The map module  and the join module  may work in conjunction to group each line item with its associated purchase charges. Once such grouping has been accomplished, the entities may be sent to the calculation module  so that the purchase charges for each line item may be totaled up. The stream joining module  may then combine the one or more purchase charges with the data entities  for serialization and transmission to the order service component .","In various embodiments, the grouping performed by the map module  may be a blocking step that holds up some processing as the grouping may not occur until all the entities in the incoming data have been received by the map module . However, because of the multi-pathway nature of the streaming pipeline , the query module  may continue to process the entities even when the map module  is blocked (i.e., busy processing data). Likewise, the sending of the duplicate set of data entities  is also unhindered by the blocking of the map module . Thus, the multi-path way nature of the streaming pipeline  may reduce latency.","Moreover, since each of the service components may call upon the data transfer component  to handle serialization and deserialization of the data, the nature of the in-memory dataset structure  that is used by the data transfer component  may further decrease latency. In various embodiments, the entities in the data that is processed by the streaming pipeline  may be stored in rows in the dataset structure , such as the row . Accordingly, the rows of data may be sent individually through the various modules of the streaming pipeline .","For example, as shown in , the data  may be propagated through the streaming pipeline such that portions (e.g., rows) of data are being process on a row-by-row basis. For example, the data  may be propagated through the streaming pipeline  such that as a first row  of the data may have been propagated through the streaming pipeline  and is being processed by the query module . Simultaneously, a second row  of the data  may have been propagated through the streaming pipeline  and is being processed by the query module , while a third row  of the data  may be at the point in the streaming pipeline  where it is being duplicated by the stream split module . Further, a fourth row  of the data  may be simultaneously being deserialized by the deserializer .","In this way, the configuration of one or more service components as streaming pipelines, when combined with the use of the in-memory dataset structure , may further reduce latency. In other words, the combination may enable data to be consumed as soon as it is produced at various stages in a streaming pipeline. In this way, a service component with a streaming pipeline, such as the streaming pipeline , may serialize and output processed data derived from a portion of the data  while still simultaneously receiving another portion of the data . In contrast, in conventional data processing, a service component may only output processed data when incoming data  has been processed in its entirety by the service component. It will be appreciated that the streaming pipeline  is an example pipeline, and a pipeline that includes different modules may be constructed for different service components.","Thus, in scenarios in which each service component that is called upon by an application may include an internal streaming pipeline, the application may in some instances receive processed data that is derived from some original data as the application is still sending the original data to the service components for processing. The use of the in-memory dataset structure  to reduce latency during streaming of data between two entities is further illustrated in .","In various embodiments, the streaming of data using the dataset structure  may involve the grouping of one or more rows in the dataset structure  into at least one row pool, as well as the grouping of the index data (i.e., row pointers, column offsets) that identifies the locations of the one or more rows into at least one corresponding index pool. Accordingly, a row pool is an organizational construct that may include as few as a single row, a page of rows, as many as multiple pages of rows, or any variation in between. Each of the rows may be a variable row size or a fixed row size. Further, each of the row pools may be associated with a corresponding index pool that is an organization construct that includes the index data for the one or more pages. The organization of rows of the dataset structure  and their corresponding index data into row pools and index pools is further illustrated in .",{"@attributes":{"id":"p-0082","num":"0081"},"figref":"FIG. 7","b":["702","704","706","704","708","702","710","702","712","712","702","704","704","712","706","120","712","702","710","702","712"]},"In another instance, an example row pool  may include a plurality of data rows ()-(N) that resides in a page . Each of the data rows ()-(N) may have a row identifier, such as the respective row identifiers ()-(N). The row pool  may be assigned a pool identifier . The row pool  may be additionally associated with an index pool  that includes the index data for each of the data rows in the row pool , i.e., row pointers and one or more column offsets for each row. The index data in the index pool  may be from the index of the page . The data transfer component  may associate index pool  the row pool  by storing the pool identifier  of the row pool  in a portion of the index pool .","In an additional instance, an example row pool  may include data rows that reside in a plurality of pages, such as pages ()-(N). The page () may include one or more data rows ()-(N). Likewise, the page (N) may include one or more data rows ()-(N). Each of the data rows ()-(N) and data rows ()-(N) may have a plurality of row identifiers, such as the respective row identifiers ()-(N) and row identifiers ()-(N). Further, the row pool  may be assigned a pool identifier . The row pool  may be associated with an index pool  that includes the index data for each of the data rows in the row pool , i.e., row pointers and one or more column offsets for each row. The index data in the index pool  may be from the data indices of the pages ()-(N). The data transfer component  may associate the index pool  with the row pool  by storing the pool identifier  of the row pool  in a portion of the index pool .","In order to implement the concept of row pools and index pools as organizational constructs for the dataset structure , each of the row identifiers for the rows in the dataset structure  may be further modified to include an additional piece of information that identifies the row pool that the row belongs. Thus, recall that as described, a row identifier for a data row (e.g., row ) may contain information such as the UUID of the page that the data row reside in, as well as the row designation (e.g., row number) of the data row. Each of the row identifiers may be further modified to include a pool designation (e.g., pool identifier) of the row pool in which the data row resides. For example, a row identifier may include bytes that represent a UUID , a row number , and the pool identifier . In this way, the data transfer component  may use the row identifier of each row to locate and distribute the rows among different entities.","Further, the division of rows in a dataset structure into row pools may decrease latency during data transfer between entities. For example, the division of data rows in a page into row pools may enable a service component to receive and process a row pool without waiting for the rest of the row pools to arrive from another service component.",{"@attributes":{"id":"p-0087","num":"0086"},"figref":"FIGS. 8-11","b":["120","604","606","602","616","618"]},{"@attributes":{"id":"p-0088","num":"0087"},"figref":"FIG. 8","b":["802","804","806","1","806","808","1","808","802","120","808","1","808","802","804","810","802","120","806","1","806","802","804","812","804","814","802","804","804","806","1","804","806","1","804","806","2","806"]},{"@attributes":{"id":"p-0089","num":"0088"},"figref":"FIG. 9","b":["902","904","906","902","120","908","904","902","906","910","902","120","904","906","912","912","906","904","904","906","904","914","1","914","906","914","1","914","2","906","914","1","914","2","914","3","914"]},"Further, the data transfer component , on behalf of the entity , may create a new row pool , and then stores data  derived from the one or more data rows in the row pool  (e.g., the total retail value) as one or more new data rows  in the row pool . In this way, by storing the data  in the row pool  rather than appending the data  into a data row in the row pool , latency may be further reduced. Otherwise, latency may have resulted because the entity  is generally not permitted to modify any row pool, including the row pool , until streaming of remaining the data rows ()-(N) to the entity  is completed.","Lastly, the data transfer component  may create a new index pool  for the row pool . In some embodiments, the entity  may further use the data transfer component  may to consolidate the row pool  and the row pool  into a single row pool, and also consolidate the index pool  and the index pool  into a single index pool.","In contrast, in a scenario in which the entity  derives the data  after the streaming of the row pool  to the entity  is completed, the data transfer component  may store the data  in the row pool  rather than create the row pool  to store the data .",{"@attributes":{"id":"p-0093","num":"0092"},"figref":"FIG. 10","b":["1002","1004","1002","1006","1004","1002","1008","1004","1002"]},"However, rather than completing the modification before sending the row pool , the data transfer component  may instead perform the transfer of row pool . The data transfer component  may transfer the row pool  by sending an index pool  that corresponds to the row pool  to the entity  via transfer , and then sending the row pool  to the entity  via transfer .","However, during the sending of the index pool  or the row pool , the data transfer component  may also create a new row pool . The data transfer component  may then store the one or more data rows of the data  into the row pool . Subsequently, the data transfer component  may also create an index pool  that corresponds to the row pool . Following their creation, the data transfer component  may use respective data transfers  and  to send the index pool  and the row pool  to the entity . The data transfers  and  may be implemented in such a way that the index pool  and the index pool  are consolidated into a combined index pool  at the entity , and the row pool  and the row pool  are consolidated together into a combined row pool . For example, the pool identifier used by the row pool  and index pool  may be modified by the data transfer component  to match the pool identifier used by the row pool  and the index pool . After the transfers are completed, the entity  may retrieve data  from the row pool  for additional processing or modification.","Thus, by at least partially overlapping the transfer of the row pool  and the creation of the row pool  to store the new data , the implementation described in  may reduce latency as compared to conventional data transfer approaches. In contrast, conventional data transfer approaches may dictate that a piece of data is to be modified prior to being transferred from one entity to another entity, which may result in increased data transfer latency.",{"@attributes":{"id":"p-0097","num":"0096"},"figref":["FIG. 11","FIG. 11","FIGS. 9 and 10"],"b":["1102","1104","1102","1106","1104","1102","1188","1104","1102"]},"However, rather than completing the modification before sending the row pool , the data transfer component  may instead perform the transfer of row pool . The data transfer component  may transfer the row pool  by sending an index pool  that corresponds to the row pool  to the entity  via transfer , and then sending the row pool  to the entity  via transfer .","However, during the sending of the index pool  or the row pool , the data transfer component  may also create a new row pool . The data transfer component  may then store the one or more data rows of the data  into the row pool . Subsequently, the data transfer component  may also create an index pool  that corresponds to the row pool . Following their creation, the data transfer component  may use respective transfers  and  to send the index pool  and the row pool  to the entity . The transfers  and  may be implemented in such a way that the index pool  and the index pool  are consolidated into a combined index pool  at the entity , and the row pool  and the row pool  are consolidated together into a combined row pool . For example, the pool identifier used by the row pool  and index pool  may be modified by the data transfer component  to match the pool identifier used by the row pool  and the index pool .","In some embodiments, the data transfer component  may modify the row numbers of the data rows in the row pool  during the consolidation of the row pools so that such row numbers numerically follow the row number of the last data row in the row pool . For example, as shown in , the row pool  may include data rows  ()-() with respective row numbers \u201c1\u201d, \u201c2\u201d, and \u201c3\u201d prior to consolidation. Further, the last data row  in the row pool  may have a row number of \u201c10\u201d. Accordingly, during the consolidation of the row pool  and the row pool  into the combined pool , the data transfer component  may change the row number belonging to each of the data rows ()-() to numerically follow the row number \u201c10\u201d of the last data row . In other words, the row numbers of the data rows ()-() may be changed to \u201c11\u201d, \u201c12\u201d, and \u201c13\u201d, respectively, during the consolidation of the row pools  and  into the combined row pool .","However, during one or more of the transfers , ,  and , the entity  may receive a request to modify the data in the combined row pool . The modification may involve operations that create, delete, or change the data in the combined row pool . As part of the modification, the entity  may determine that new data  (e.g., one or more additional data rows) is to be added to the combined row pool . Accordingly, the data transfer component , on behalf of the entity , may create a new row pool  that includes a table, and then stored the data  as one or more new data rows  in the table of the row pool . Lastly, the data transfer component  may create a new index pool  for the row pool . In contrast, in a scenario in which the entity  derives the data  after the consolidation of the row pools  and  into row pool , the data transfer component  may store the data  in the row pool  rather than create the row pool  to store the data .",{"@attributes":{"id":"p-0102","num":"0101"},"figref":"FIGS. 12-14","b":["1200","1300","1400","120","1200","1300","1400"]},{"@attributes":{"id":"p-0103","num":"0102"},"figref":"FIG. 12","b":"1200"},"At block , a receiving entity may receive one or more index pools from a sending entity. The one or more index pools may be sequentially transferred from the sending entity to the receiving entity by the data transfer component . Each index pool may include index data for one or more rows in a corresponding row pool, such index data may include row pointers and one or more column offsets for each data row in the corresponding row pool.","At block , the receiving entity may receive one or more row pools that correspond to the one or more index pools from the sending party. Each row pool may include one or more data rows that store information for a table, such as one of the tables , which is part of the dataset structure . In some alternative embodiments, the transfer of the one or more row pools may be performed prior to the transfer of the one or more corresponding index pools.","At block , the receiving entity may retrieve data from at least one data row in the one or more row pools. The receiving party may then process or modify the data. However, in some embodiments, the receiving entity may process or modify the data in a received row pool without waiting for the arrival of remaining row pools. For example, the receiving entity may have received a request to process data that is in a first row pool of a group of row pools. Accordingly, the receiving entity may fulfill such a request as soon as the first row pool arrives at the receiving entity without having to wait for the arrival of the remaining row pools in the group, which may reduce latency.","At block , the receiving entity may retrieve data from the one or more index pools. For example, the receiving entity may run on a search query on the index data in a particular index pool to determine if certain data is stored in the row pool that corresponds to the particular index pool. Since an index pool is generally smaller in size than its corresponding row pool, performing a search query on the index pool may result in faster return of query results than a search query on the corresponding row pool. In various embodiments, the retrieval of data from the one or more row pools in the block  may occur prior to or concurrently with the retrieval of data from the one or more row pools in the block .",{"@attributes":{"id":"p-0108","num":"0107"},"figref":"FIG. 13","b":"1300"},"At block , a receiving entity may receive an existing row pool or an existing index pool for the existing row pool from a sending entity. The existing row pool may include one or more data rows that store information for a table, such as one of the tables , which is part of the dataset structure . The existing index pool may include index data for one or more rows in a corresponding row pool, such index data may include row pointers and one or more column offsets for each data row in the corresponding row pool. The data transfer component  may be used by the sending entity to transfer the existing row pool and the corresponding index from the sending party to the receiving party. In various embodiments, the existing index pool may be transferred before the existing row pool, or the existing row pool may be transferred before the existing index pool.","At block , the receiving entity may determine that at least a portion of the existing row pool is to be modified while it is receiving the existing row pool or the existing index pool. The modification may involve operations that create, delete, or change the data in the existing row pool. In various embodiments, the receiving entity may decide to make the modification due to its processing of data or based on a request from another entity (e.g., an application). The modification may include the addition of one or more data rows to the existing row pool that is being received.","At block , the data transfer component  may create a new row pool for the storage of data that modifies the existing row pool, e.g., one or more additional data rows, while the receiving of the existing row pool or the existing index pool is still in progress.","At block , the data transfer component  may store the modified data in the new row pool for the receiving entity. The modified data may include the one or more additional data rows.","At block , the data transfer component  may generate a new index pool for the new row pool. The existing index pool may include index data for one or more additional data rows in the new row pool. Subsequently, the receiving entity may further modify data in at least one data row in the rows pools and\/or transfer the row pools to another entity. Further, the receiving entity may also alternatively or concurrently retrieve the index data from the corresponding index pools or transfer the index data in the corresponding index pools to another entity.",{"@attributes":{"id":"p-0114","num":"0113"},"figref":"FIG. 14","b":"1400"},"At block , a sending entity may send an existing row pool or an existing index pool for the existing row pool from a sending entity to a receiving entity. The existing row pool may include one or more data rows that store information for a table, such as one of the tables , which is part of the dataset structure . The existing index pool may include index data for one or more rows in a corresponding row pool, such index data may include row pointers and one or more column offsets for each data row in the corresponding existing row pool. The data transfer component  may be used by the sending entity to transfer the existing row pool and the corresponding index from the sending party to the receiving party. In various embodiments, the existing index pool may be transferred before the existing row pool, or the existing row pool may be transferred before the existing index pool.","At block , the sending entity may determine that at least a portion of the existing row pool is to be modified while it is sending the existing row pool or the existing index pool. The modification may involve operations that create, delete, or change the data in the existing row pool. In various embodiments, the sending entity may make the modification due to its processing of data or based on a request from another entity (e.g., an application). The modification may include the addition of one or more data rows to the existing row pool that is being sent.","At block , the data transfer component  may create a new row pool for the storage of data that modifies the existing row pool, e.g., one or more additional data rows, while the sending is still in progress.","At block , the data transfer component  may store the modified data in the new row pool for the receiving entity. The modified data may include the one or more additional data rows.","At block , the data transfer component  may generate a new index pool for the new row pool. The new index pool may include index data for one or more additional data rows in the new row pool.","At block , the data transfer component  may, on behalf of the sending entity, send the new index pool to the receiving entity. The transfer may be implemented in such a way that the new index pool integrates into the existing index pool that was previously sent to the receiving party.","At block , the data transfer component  may, on behalf of the sending entity, send the new row pool to the receiving entity. The transfer may be implemented in such a way that the new row pool integrates into the existing row pool that was previously sent to the receiving party. Subsequently, the receiving entity may further modify data in at least one data row in the integrated row pool and\/or transfer the integrated row pool to another entity. Further, the receiving entity may also alternatively or concurrently retrieve the index data from the corresponding integrated index pool or transfer the index data in the corresponding integrated index pool to another entity.","It will be appreciated that while the transfer of one or more row pools is described in each of the scenarios in  as occurring prior to the transfer of the one or more corresponding index pools, the order of the transfers may be reversed in additional embodiments. In other words, the transfer of the one or more index pools may occur prior to the transfer of the one or more corresponding row pools in the additional embodiments.","Data latency may be additionally reduced by scaling the number of modules that are in the streaming pipeline of a service component. In various embodiments, a scaling component , as shown in , may monitor the performance of the modules in a streaming pipeline, such as the streaming pipeline , and scale the number of modules depending on the data production rates and the data consumption rates of the modules that are transferring data over a channel, such as a channel  between the query modules  and . The operation of the scaling component  is further illustrated in .",{"@attributes":{"id":"p-0124","num":"0123"},"figref":"FIG. 15","b":["1500","1500"]},"At block , the scaling component  may monitor the production and consumption of data across a channel by a pair of modules in a service component. For example, the scaling component  may monitor a rate at which the query module  produces processed data and a rate at which the query module  consumes the processed data from the query module .","At decision block , the scaling component  may determine whether a discrepancy between the data production rate and the data consumption rate exists. For example, a discrepancy may exist if the data production rate is higher than the data consumption rate. If the scaling component  determines that no discrepancy exists (\u201cno\u201d at decision block ), the process  may loop back to block  so that the data production rate and the data consumption rate may be continuously monitored.","However, if the scaling component  determines at decision block  that a discrepancy exists (\u201cyes\u201d at decision block ), the process  may proceed to block . At block , the scaling component  may determine whether to instantiate additional consumer modules. For example, if the stream split module  is producing more data then the query module  is capable of processing, then the scaling component  may determine whether one or more additional query modules  are to be produced. In various embodiments, the determination may be made based on a difference between the data production rate and the data consumption rate. For example, whether the data production rate exceeds the data consumption rate by a predetermined threshold.","In some embodiments, the scaling component  may take into account one or more resource availability factors in addition to the difference between the two rates. These factors may include available processing power of the servers , available memory of the servers , the number of requests being processed by the servers , and\/or other factors that affect the performance of the servers . For example, in some instances, no additional consumer services component modules may be instantiated despite a discrepancy because the servers  may lack the available resources. The amount of resources available may be impacted by the instantiation of additional consumer modules for one or more additional pairs of service component modules in the same streaming pipeline or at least one other streaming pipeline. In other words, the scaling component  may be simultaneously managing the instantiation of multiple consumer modules with respect to different streaming pipelines.","Further, the number of additional consumer modules may be dependent on a magnitude of the difference between data production rate and the data consumption rate (e.g., the greater the difference, the more numerous the additional consumer service components that are instantiated). Alternatively, the number of additional consumer modules may be further dependent upon an amount of resources available (e.g., processing power, memory, etc.) in combination with the magnitude of the difference between data production rate and the data consumption rate.","At decision block , if the scaling component  determines that no additional consumer modules are to be instantiated (\u201cno at decision block ), the process  is to loop back to block  so that the data production rate and the data consumption rate may be continuously monitored.","However, if the scaling component  determines that at least one additional consumer modules is to be instantiated (\u201cyes\u201d at decision block ), the process  may proceed to block . At block , the scaling component  may instantiate one or more additional consumer modules based on the magnitude of the discrepancy and\/or the amount of resources available on the servers . In various embodiments, the actual number of additional consumer modules that are instantiated by the scaling component  may be proportional to the magnitude of the discrepancy and\/or the amount of resources available on the servers .","At decision block , the scaling component  may determine whether at least one additional consumer modules is to be terminated. In various embodiments, the scaling component  may terminate the one or more additional consumer modules when the difference between the data production rate and the data consumption rate falls below the predetermined threshold and\/or when the amount of one or more resources available (e.g., processing power and\/or memory) falls below a certain designated level.","In some embodiments, the number of the additional consumer modules that are terminated may be in proportion to a change in the difference between the data production rate and the data consumption rate and\/or a decrease in the amount of one or more resources that is available. For example, the scaling component  may terminate one additional consumer module but maintain another additional consumer module in place when the data production rate decreases from being three times the data consumption rate to being two times the data consumption rate. In another example, the scaling component  may terminate one additional consumer module but maintain another additional consumer module when the amount of one or more resources available falls from being able to sustain two additional consumer modules to sustain one additional consumer module.","Accordingly, if the scaling component  determines that no additional consumer modules are to be terminated (\u201cno\u201d at decision block ), the process  may loop back to block , so that the scaling component  may continuously determine whether to instantiate additional consumer modules.","However, if the scaling component  determines that at least one additional consumer modules is to be terminated (\u201cyes\u201d at decision block ), the process  may proceed to block . At block , the scaling component  may terminate one or more additional consumer modules. The number of the additional consumer modules that are terminated may be in proportion to a decrease in the magnitude of the difference between the data production rate and the data consumption rate and\/or a decrease in the amount of one or more resources that is available. Subsequently, the process  may loop back to block  so that the data production rate and the data consumption rate may be continuously monitored.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather, the specific features and acts are disclosed as illustrative forms of implementing the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The same reference numbers in different figures indicate similar or identical items.",{"@attributes":{"id":"p-0005","num":"0004"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 15"}]},"DETDESC":[{},{}]}
