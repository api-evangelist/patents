---
title: Memory sharing in graphics processing unit
abstract: Aspects of this disclosure are directed to a method of processing data with a graphics processing unit (GPU). According to some aspects of the disclosure, the method comprises receiving input defining execution orders for a shader processor, wherein the execution orders comprise a plurality of kernel designations and a plurality of workgroup designations. The method may also include assigning workgroups of kernels identified in the plurality of workgroup designations and the plurality of kernel designations to the shader processor. The method may also include executing, by the shader processor, the workgroups of kernels identified in the plurality of workgroup designations and the plurality of kernel designations to process input data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09092267&OS=09092267&RS=09092267
owner: QUALCOMM Incorporated
number: 09092267
owner_city: San Diego
owner_country: US
publication_date: 20110620
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This disclosure relates to processing data using a graphics processing unit (GPU).","Graphics processing devices may be implemented to carry out a variety of image processing or other general purpose processing applications. For example, a graphics processing unit (GPU, sometimes referred to as a general purpose graphics processing unit (GPGPU), may execute applications that benefit from a high degree of parallelism, such as color correction algorithms, face detection algorithms, pattern recognition algorithms, augmented reality applications, a variety of algorithm applications (e.g., wavelet transforms, Fourier transforms, and the like), or a variety of other applications.","In general, GPUs are designed to process a series of instructions, which may be referred to as shader instructions, using one or more shader processors residing in the GPU. In an example image processing application, shader instructions may define one or more mathematical operations to be performed by the shader processors on the pixels that make up the image. By applying a shader instruction to a pixel, the pixel value is changed or evaluated according to the mathematical operation defined by the shader instruction.","Shader instructions may be organized into shader program code known as a kernel. A kernel may define a function or task that is performed by the GPU. In order to execute a kernel, the program code is divided into work items (e.g., a basic unit of work in a GPU), which are organized into one or more workgroups (e.g., a set of work items).","In general, aspects of this disclosure are related to generation and processing of kernel and workgroup execution orders for graphics processing. Kernel and workgroup execution orders may provide management of memory resources associated with a shader processor (SP) of a graphics processing unit (GPU). For example, kernel and workgroup execution orders allow data stored in local memory resources of an SP to be shared by workgroups of different kernels. In one example, aspects of this disclosure are directed to a method of processing data with a graphics processing unit (GPU). The method includes receiving input defining execution orders for a shader processor, wherein the execution orders comprise a plurality of kernel designations and a plurality of workgroup designations. The method also includes assigning workgroups of kernels identified in the plurality of workgroup designations and the plurality of kernel designations to the shader processor. The method also includes executing, by the shader processor, the workgroups of kernels identified in the plurality of workgroup designations and the plurality of kernel designations to process input data.","In another example, aspects of this disclosure are directed to a graphics processing unit (GPU) comprising a sequencer module. The sequencer module is configured to receive input defining execution orders for a shader processor, wherein the execution orders comprise a plurality of kernel designations and a plurality of workgroup designations. The sequencer module is also configured to assign workgroups of kernels identified in the plurality of workgroup designations and the plurality of kernel designations to the shader processor. The shader processor is configured to execute the workgroups of kernels identified in the plurality of workgroup designations and the plurality of kernel designations to process input data.","In another example, aspects of this disclosure are directed to a computer-readable storage medium encoded with instructions for causing one or more programmable processors of a computing device to receive input defining execution orders for a shader processor, wherein the execution orders comprise a plurality of kernel designations and a plurality of workgroup designations. The instructions also cause one or more programmable processors of a computing device to assign workgroups of kernels identified in the plurality of workgroup designations and the plurality of kernel designations to the shader processor. The instructions also cause one or more programmable processors of a computing device to execute, by the shader processor, the workgroups of kernels identified in the plurality of workgroup designations and the plurality of kernel designations to process input data.","In another example, aspects of this disclosure are directed to a graphics processing unit (GPU) comprising a means for receiving input defining execution orders for a shader processor, wherein the execution orders comprise a plurality of kernel designations and a plurality of workgroup designations. The GPU also comprises a means for assigning workgroups of kernels identified in the plurality of workgroup designations and the plurality of kernel designations to the shader processor. The GPU also comprises a means for executing, by the shader processor, the workgroups of kernels identified in the plurality of workgroup designations and the plurality of kernel designations to process input data.","The details of one or more examples are set forth in the accompanying drawings and the description below. Other features, objects, and advantages will be apparent from the description and drawings, and from the claims.","Aspects of this disclosure generally relate to defining and generating streams of shader instructions for execution by a shader processor (SP) of a GPU, which may be implemented as a GPGPU. Typically, GPUs include a plurality of SPs that are designed to execute one or more shader programs (referred to herein as \u201ckernels\u201d). Kernels define functions that can be implemented to analyze or modify a variety of input data. Examples include functions for processing relatively large numerical data sets in parallel. In an image processing context, functions may include, for example, color correction algorithms, face detection algorithms, or functions for carrying out augmented reality applications. Other examples include transform functions, functions for ray tracing, or a variety of other functions.","Kernels comprise individual work items (e.g., a basic unit of work in a GPU) that are grouped into workgroups. In an example in which a GPU is implemented to process an image (e.g., a frame of video data, computer-generated graphics image, still image, and the like), the input data is the image and the work items are configured to analyze or modify pixels of the image. A plurality of work items can be organized into workgroups (e.g., a set of work items). Accordingly, in the image processing example, workgroups may include instructions related to a particular group of pixels in an image.","When executing the kernels, a GPU loads the input data associated with a workgroup into local memory of an SP before the SP can execute the workgroup. Generally, when a GPU executes a kernel, the GPU assigns workgroups of the kernel to SPs without identifying or controlling which SP executes a particular workgroup. For example, a GPU may include hardware that distributes workgroups to SPs in a fixed pattern that is not configurable by a GPU application developer (e.g., computer programmer). In such an example, the GPU sequentially executes an application having multiple kernels by evenly distributing and executing all workgroups associated with a particular kernel prior to moving on to the next kernel.","Workgroup data typically cannot be shared between SPs. For example, SPs are typically discrete and physically separate units within a GPU, and a GPU application developer does not control which SP executes a particular workgroup. Accordingly, in an application having multiple kernels that process the same or substantially the same input data, the input data associated with a particular workgroup may need to be loaded into local memory of different SPs. For example, without the ability to control which SP of the GPU executes a particular workgroup of a particular kernel, a workgroup of a first kernel having the same input data as a workgroup of a second kernel may be processed by different SPs of the GPU.","Aspects of the disclosure relate to generating and processing kernel and workgroup execution orders. Kernel and workgroup execution orders support management of local memory resources associated with an SP of a GPU. For example, kernel and workgroup execution orders executed by a GPU allow data stored in SP local memory to be shared by workgroups of different kernels. Kernel and workgroup execution orders may be referred to as \u201cinstruction streams.\u201d An instruction stream ties, or virtually links, workgroups and kernels together such that the input data associated with one workgroup of one kernel can be shared and serially executed by multiple other kernels using a single SP. By generating the instruction streams, input data is able to remain in local SP memory and can be available to workgroups of multiple kernels. Creating instruction streams reduces memory bandwidth consumption as well as SP arithmetic logic unit (ALU) operations, because ALU operations are not necessary for fetching the same data multiple times.","In some examples, kernel and workgroup execution orders are useful when a GPU is executing an application having multiple kernels that process the same, or substantially the same, input data. In one example implementation, a GPU may be implemented to process an image (e.g., a frame of video data, computer-generated graphics image, still image, and the like). In this example, a work item may correspond to an instruction related to a particular pixel of an image. A plurality of work items can be organized into workgroups that include instructions related to a particular group of pixels of the image. When processing a workgroup associated with a group of pixels, the GPU loads the image data associated with the group of pixels into local memory of an SP.","Without the ability to control which SP of the GPU executes a particular workgroup, in the image processing example, executing multiple kernels in succession may cause the same input image area to be processed by different SPs. For example, to execute a first kernel, the GPU loads the data associated with the entire image, one workgroup at a time, into local memory of the SPs of the GPU for execution. To execute a second kernel after executing the first kernel, the GPU reloads the same image data into local memory of the SPs of the GPU for execution. Accordingly, the input image data is loaded into local SP memory multiple times, once for each kernel. The SP local memory input bandwidth consumption for an entire image is approximately equal to the image data size multiplied by the number of kernels (e.g., a program for analyzing a 64 MB image having 3 kernels results in 3\u00d764 MB or 192 MB of bandwidth consumed). Without any input data sharing between kernels and their executed workgroups, a relatively large amount of memory bandwidth is consumed.","In the image processing example, generating and executing instruction streams allows data associated with a particular portion of an image to be loaded into local memory resources of a particular SP once and processed with multiple kernels. An example program having three kernels (e.g., a first kernel, a second kernel, and a third kernel) is provided. Data associated with a first workgroup of a first kernel is loaded into a particular SP's local memory and the first workgroup is executed by the SP. In addition, an instruction stream is provided that includes kernel and workgroup execution orders, which direct the same SP to subsequently execute the first workgroup of a second kernel, followed by the first workgroup of the third kernel. Accordingly, image data associated with the first workgroup need not be loaded into the particular SP's local memory prior to executing the second kernel and the third kernel. Instead, the second kernel and the third kernel use the input data previously loaded for the first kernel. In this way, memory bandwidth consumption may be reduced, because the data associated with a particular area of an input image need only be loaded into local SP memory once and can be processed with multiple kernels. In the three-kernel example provided above, the memory bandwidth consumption is reduced by two thirds.","Instruction streams can be defined in a variety of ways. According to some aspects, a user identifies candidate kernels that would benefit from utilizing instruction streams. For example, a user may identify kernels that utilize the same input data multiple times. Utilizing instruction streams may help to manage local memory resources of SPs by reducing the number of times input data needs to be loaded into the local memory resources.","After identifying candidates, the user can define instruction streams in a program that is executed by a GPU. For example, GPU application programming is typically performed by an application developer (e.g., a computer programmer) with an application program interface (API) that provides a standard software interface that can run on multiple platforms, operating systems, and hardware. Examples of APIs include Open Graphics Library (\u201cOpenGL,\u201d version 4.1 released Jul. 26, 2010 and publically available), Compute Unified Device Architecture (\u201cCUDA\u201d developed by NVIDIA Corporation, version 3.2 released Sep. 17, 2010), and DirectX (developed by Microsoft, Inc., version 11 released Oct. 27, 2009). In general, an API includes a predetermined, standardized set of commands that are executed by associated hardware. API commands allow a user to instruct hardware components of a GPU to execute commands without user knowledge as to the specifics of the hardware components.","Aspects of the disclosure relate to one or more API commands that allow a user to define instruction streams. For example, one or more API commands may be developed and created in design environment. The API commands may then be included in an API, such as the APIs described above, as a preconfigured option for users (e.g., computer programmers) of the API.","A user can implement the preconfigured instruction stream API commands to designate instruction streams in an application that will be executed by a GPU during development and coding of an application. For example, the instruction stream API commands allow the user to designate instructions (e.g., one or more workgroups) from different kernels of a multiple-kernel application to be processed sequentially by a particular SP. Upon executing an application that includes instruction stream designations, a GPU routes instructions (e.g., workgroups of different kernels) to an SP of the GPU in accordance with the received designations.","In another example, an automated system may be implemented to generate instruction streams. For example, a program, such as a compiler program, may automatically generate instruction streams for an application that repetitively processes the same input data with multiple kernels. In this example, the program may partition instructions of each kernel so that each group of instructions corresponds to a predetermined amount of input data (e.g., a workgroup of instructions). The program may then generate instruction streams by linking the groups of instructions from different kernels such that the input data associated with one group of instructions of one kernel can be shared and serially executed by multiple other kernels using a single SP.","In a non-limiting image processing example, a GPU application developer may provide the instruction stream generating program with an input image and an application having three kernels for processing the image. The instruction stream generating program can automatically generate instruction streams based on predefined spatial areas of the image. For example, the instruction stream generating program may partition instructions of each of the three kernels into groups of instructions, with each group of instructions corresponding to a predefined area of the input image. The instruction stream generating program can then generate instruction streams by linking the groups of instructions of each of the kernels that are associated with the same input image area.","For example, a program, such as a compiler program or other development\/analysis program, may identify candidate kernels that would benefit from implementing instruction streams. For example, a program may monitor memory access patterns and identify data that is used by more than one kernel. In this example, the program may monitor and log the read\/write access patterns associated with workgroups of an application having multiple kernels. After logging, the program can detect input\/output dependency of respective workgroups of each kernel. That is, the program can determine which workgroups of the multiple kernel application utilize the same input data. Based on this data sharing information, instruction stream designations can be inserted into the application that facilitate workgroups from different kernels of a multiple-kernel application being processed sequentially by a particular SP. For example, code can be inserted into the application that instructs a GPU to execute workgroups from different kernels that share the same input data to be executed sequentially by the same SP.","The program for identifying candidate kernels and designating workgroups of different kernels to an instruction stream may be executed during application development, or \u201con the fly\u201d during GPU application execution. For example, according to some aspects, a GPU application developer may implement the program for identifying candidate kernels and designating workgroups of different kernels to an instruction stream. The developed GPU application may then include the instruction stream designations that will be executed by a GPU. In another example, a host processor or GPU of a computing device may implement the program for identifying candidate kernels and designating workgroups of different kernels to an instruction stream \u201con the fly\u201d while executing a GPU application.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":["FIG. 1","FIG. 1"],"b":["20","20","24","28","32","36","40","44","20","48"]},"Computing device  may, in some examples, include or be a part of a portable computing device (e.g. mobile phone, netbook, laptop, tablet device, digital media player, gaming device, or other portable computing device). Alternatively, computing device  may be configured as a desktop computer or other stationary computing device. Computing device  may include additional components not shown in  for purposes of clarity. For example, computing device  may include one or more communication bridges for transferring data between components of the computing device . Moreover, the components of computing device  shown in  may not be necessary in every example of computing device . For example, user interface  and display  may be external to computing device , e.g., in examples where computing device  is a desktop computer.","Host processor  may include any one or more of a microprocessor, a controller, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field-programmable gate array (FPGA), or equivalent discrete or integrated logic circuitry. Additionally, the functions attributed to host processor , in this disclosure, may be embodied as software, firmware, hardware or any combination thereof.","Host processor  processes instructions for execution within computing device . Host processor  may be capable of processing instructions stored on storage device  or instructions stored in memory . Example applications include applications for processing viewable images (e.g., filtering images, analyzing images for predefined features, and the like). Host processor  may execute the one or more applications based on a selection by a user via user interface . In some examples, host processor  may execute the one or more applications without user interaction.","According to some aspects of the disclosure, and as described in greater detail below with respect to GPU , host processor  may collaborate with GPU  to execute various tasks associated with one or more applications. For example, host processor  may initialize execution of an application and offload or delegate certain processing functions associated with the application to GPU . In an example, host processor  may initialize execution of an image processing application, and offload certain processing functions associated with the application to GPU .","Storage device  may include one or more computer-readable storage media. Storage device  may be configured for long-term storage of information. In some examples, storage device  may include non-volatile storage elements. Examples of such non-volatile storage elements may include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of electrically programmable memories (EPROM) or electrically erasable and programmable (EEPROM) memories. Storage device  may, in some examples, be considered a non-transitory storage medium. The term \u201cnon-transitory\u201d may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. However, the term \u201cnon-transitory\u201d should not be interpreted to mean that storage device  is non-movable. As one example, storage device  may be removed from computing device , and moved to another device. As another example, a storage device, substantially similar to storage device , may be inserted into computing device .","Storage device  may store instructions for execution of one or more applications by host processor  or GPU . Storage device  may also store data for use by host processor  or GPU . For example, storage device  may store image data for processing by host processor  or GPU .","Memory  may be configured to store information within computing device  during operation. In some examples, memory  is a temporary memory, meaning that a primary purpose of memory  is not long-term storage. Memory  may, in some examples, be described as a computer-readable storage medium. Accordingly, memory  may also be considered \u201cnon-transitory,\u201d despite storing data that can change over time. Memory  may also, in some examples, be described as a volatile memory, meaning that memory  does not maintain stored contents when the computer is turned off. Examples of volatile memories include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of volatile memories known in the art.","In some examples, memory  may be used to store program instructions for execution by host processor  or GPU . Memory  may be used by software or applications running on computing device  to temporarily store information during program execution. As such, memory  may be accessed by other components of computing device  such as host processor  and GPU .","Computing device  may utilize network module  to communicate with external devices via one or more networks, such as one or more wireless networks. Network module  may be a network interface card, such as an Ethernet card, an optical transceiver, a radio frequency transceiver, or any other type of device that can send and receive information. In some examples, computing device  may utilize network module  to wirelessly communicate with an external device such as a server, mobile phone, or other networked computing device.","Computing device  also includes user interface . Examples of user interface  include, but are not limited to, a trackball, a mouse, a keyboard, and other types of input devices. User interface  may also include a touch-sensitive screen that is incorporated as a part of display . Display  may comprise a liquid crystal display (LCD), an organic light emitting diode (OLED) display, a plasma display, or another type of display device.","GPU  of computing device  may be a dedicated hardware unit having fixed function and programmable components for executing GPU applications. GPU  may also include a DSP, a general purpose microprocessor, an ASIC, an FPGA, or other equivalent integrated or discrete logic circuitry. GPU  may also include other components, such as dedicated memory, as described in greater detail with respect to . Furthermore, although shown as separate components in , in some examples, GPU  may be formed as part of host processor . GPU  may be configured to utilize processing techniques in accordance with a variety of application programming interfaces (APIs). For example, a user may program an application to be executed by GPU  using a standard software interface that can run on multiple platforms, operating systems, and hardware. In some examples, GPU  may be configured to utilize applications generated using OpenCL, CUDA, or the DirectX collection of APIs (as described above).","According to some examples, GPU  can be implemented as a general purpose graphics processing unit (GPGPU). For example, GPU  may carry out a variety of general purpose computing functions traditionally carried out by host processor . Examples include a variety of image processing functions, including video decoding and post processing (e.g., de-blocking, noise reduction, color correction, and the like) and other application specific image processing functions (e.g., facial detection\/recognition, pattern recognition, wavelet transforms, and the like). In some examples, GPU  may collaborate with host processor  to execute applications. For example, host processor  may offload certain functions to GPU  by providing GPU  with instructions for execution by GPU .","When implemented as a GPGPU, GPU  executes shader programs, referred to herein as kernels. Kernels can be defined by a user using an API, such as the example APIs described above. Kernels may comprise individual work items (e.g., a basic unit of work in a GPU) that are grouped into workgroups.","According to some aspects of the disclosure, GPU  receives and executes kernel and workgroup execution orders, referred to herein as instruction streams. GPU  can use kernel and workgroup execution orders to manage local memory resources associated with an SP (e.g., as shown and described, for example, with respect to ) of GPU . For example, GPU  may use the kernel and workgroup execution orders to share data stored in SP local memory with workgroups of different kernels.","Certain examples provided in the following figures may refer to a GPU executing work items and workgroups to perform an image processing application. For example, the work items and workgroups may be described below as being associated with pixels of an image (e.g., a frame of video data). It should be understood, however, that a GPU may be implemented to carry out a variety of functions other than image processing functions on a variety of input data (e.g., any functions and data sets that benefit from parallel processing). Accordingly, the examples and aspects described below regarding instruction streams and memory sharing between workgroups, for example, can be carried out by a GPU performing a variety of other functions on a variety of other input data sets.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":["FIG. 2","FIG. 1","FIG. 2"],"b":["49","49","50","50","50","52","48","49","16","50","50","52"]},"In the example shown in , image  is a square, approximately 16 megabyte (MB) image that includes 1024 pixels. Each of the work items  represents a basic unit of work that can be executed by GPU . In some examples, each work item  includes instructions that may be related to a particular pixel of image . Accordingly, when GPU  executes a work item , the corresponding pixel of image  may be processed (e.g., analyzed or changed according to the instructions). The work items  may be organized into workgroups  that include instructions related to a particular group of pixels of image . When processing a workgroup , image data related to the particular group of pixels associated with the workgroup  may be loaded into local memory resources of an SP (as shown and described, for example, with respect to  below).","The relationships between pixel data, work items, and workgroups described with respect to  are merely one example of possible instruction structures. In other examples, a work item may relate to more or less than one pixel of image .",{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIG. 3","FIG. 1","FIG. 3","FIG. 1","FIG. 2"],"b":["56","57","58","48","56","58","48","49"]},"GPU  may execute kernels - to carry out to carry out a specific task on an image, such as image  shown in . For example, GPU  may be implemented as a GPGPU to carry out a variety of functions such as face detection\/recognition, pattern recognition, and many other functions suited for parallel processing (e.g., processing more than one instruction concurrently). Provided as a simplified, non-limiting example, kernels - may be implemented in a face detection application. In this example, GPU  can implement kernels - to detect one or more faces in image . Each of the kernels - may be configured to perform a specific face detection related function. Such kernels - may be referred to as \u201cclassifiers.\u201d That is, the kernels - classify pixels as having (or not having) a specific, predefined feature. The kernels - may include mathematical formulas that have been created using a number of training images. For example, the kernels - may include mathematical formulas that have been developed in a testing environment with a number of predefined images.","In the example shown in , GPU  may execute the kernels - consecutively to determine whether each pixel includes the predefined properties set forth in the kernels -. That is, when executed by GPU , each kernel - may return a Boolean value which can be used to identify a predefined property that is associated with a face. If a certain pixel exhibits all of the predefined properties set forth in kernels - (e.g., the Boolean results associated with the pixels satisfy some predefined criteria), the pixel is considered a candidate face pixel. If a certain pixel does not exhibit the predefined properties set forth in kernels - (e.g., the Boolean results associated with the pixel do not satisfy some predefined criteria), the pixel is excluded from being considered a face pixel.","In the image processing example of , the data associated with the image  is processed three times, one for each kernel -. For example, workgroups of kernels - may correspond to the same input image area of image . Similarly numbered workgroups of each of the kernels - may include a set of instructions that are to be carried out on the same input image area of image .","Aspects of the disclosure relate to generation of instructions that tie similarly numbered workgroups of kernels - into instruction streams for processing by GPU . For example, a user (e.g., computer or application programmer) or program can create an instruction stream that instructs GPU  to execute Workgroup  of kernel , followed by Workgroup  of kernel , and followed by Workgroup  of kernel  using the same SP. In this way, GPU  can load the input area of image  that corresponds to Workgroup  into local memory resources of a shader processor (SP) of GPU  (e.g., as shown and described, for example, with respect to ) and process that input image area sequentially using kernels -.","In some examples, a user (e.g., computer or application programmer) can define the instruction streams that include workgroup designations of kernels - using a preconfigured API command while developing the kernels -. For example, a user can implement the preconfigured instruction stream API commands to designate workgroups of kernels - to instruction streams that will be executed by GPU . Upon executing the instruction stream designations associated with kernels -, a GPU  routes the workgroups of kernels - to a certain SP of GPU .","In another example, an automated system may be implemented to generate instruction streams that include workgroup designations of kernels -. For example, a complier program or other program (e.g., a program that traces memory access patterns from complied low level machine assembler code) may monitor or analyze memory access patterns and identify that data associated with a workgroup, such as Workgroup , is accessed multiple times by kernels -. The program may then designate the workgroups to an instruction stream so that the workgroups are processed sequentially by an SP of GPU . Upon executing the instruction stream designations associated with kernels -, a GPU  routes the workgroups of kernels - to a certain SP of GPU .",{"@attributes":{"id":"p-0061","num":"0060"},"figref":["FIG. 4","FIG. 1","FIG. 4"],"b":["60","60","48","60","72","76","76","76","78","78","78","82"]},"In other examples, GPU  may include other components not shown in  for purposes of clarity. For example, GPU  may also include a variety of other modules related to analyzing and rendering images, such as a rasterizer, texture units, one or more buffers, or other GPU components. In addition, GPU  may include more or fewer components than those shown in . For example, GPU  is shown in  as including three SPs . In other examples, however, GPU  may include more or fewer SPs than those shown in .","In some examples, GPU memory  may be similar to memory  shown in . For example, GPU memory  may be a temporary computer-readable storage medium. Examples of GPU memory  include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of memories known in the art. In examples where GPU  is formed as part of another processor, such as host processor  shown in , GPU memory  may be accessed by components other than GPU .","GPU memory  may be configured as a global memory for GPU . For example, GPU memory  may be configured to store instructions and information within GPU  during operation (e.g., image data and instructions for processing by GPU ). GPU memory  may also be configured to store results of data that has been processed by GPU . In some examples, GPU memory  interfaces with computing device components that are external to GPU . For example, a component of a computing device that incorporates GPU  may initially pass data to GPU memory  (e.g., one or more frames of video data) for processing by GPU . GPU  then processes the data and stores the results to GPU memory . The results may subsequently be read from GPU memory  to another component of the computing device.","SPs  may be configured as a programmable pipeline of processing components. In some examples, SPs  may be referred to as \u201cunified shaders,\u201d in that the SPs  can perform geometry, vertex, or pixel shading operations to render graphics. SPs  can also be used in GPGPU applications for performing general purpose calculations. For example, SPs  may be implemented to analyze or otherwise process an image, such as image  shown in . SPs  may include a one or more components not specifically shown in , such as components for fetching and decoding instructions and one or more arithmetic logic units (\u201cALUs\u201d) for carrying out arithmetic calculations. SPs  also include one or more memories, caches, or registers, such as SP memories .","SP memories  may be configured as registers or data caches for storing data that is processed by SPs . In some examples, SP memories  are local memories of the SPs . For example, SP memories  may be relatively smaller than global GPU memory , and store the data associated with one or more workgroups prior to execution. The SP memories  may have relatively lower latency than GPU memory . For example, SP memories  can be accessed by SPs  relatively quickly. Latency associated with data transfer from global memory  to SP memories , however, is typically much greater. For example, data transfer from global memory  to SP memories  may consume multiple clock cycles, thereby creating a bottleneck and slowing overall performance of GPU .","SP memories  may exchange data with GPU memory  when GPU  is operating. For example, GPU  sends data associated with one or more workgroups from GPU memory  to SP memories . Once stored in SP memories , SPs  operate in parallel to access and process the data stored in the separate SP memories . Upon executing the data, SPs  return the results to GPU memory . In general, memory bandwidth between SP memories  and SPs  is greater than the memory bandwidth between GPU memory  and SPs . Accordingly, an SP  can generally read data from an associated SP memory  more quickly than the SP  can read data from GPU memory . That is, GPU memory  typically exhibits higher latency than that associated with SP memories . Accordingly, it may be beneficial for data to be transferred to SP memories  prior to being executed by SPs .","Sequencer module  controls instruction and data flow within GPU . Sequencer module  may comprise a combination of fixed function and programmable components for distributing work items, workgroups and associated data to SP memories  for execution by the SPs . Accordingly, sequencer module  manages data transfers between GPU memory  and SPs . For purposes of example only, workgroup distribution of sequencer module  is described with respect to the application shown and described with respect to .","Sequencer module  may distribute workgroups in a fixed distribution pattern without regard to which workgroups are executed by a particular SP of SPs . For example, to process the example application  (shown in ) having multiple kernels -, sequencer module  may distribute workgroups evenly to all SPs  of GPU . In addition, as described in greater detail with respect to  below, sequencer module  may distribute all workgroups of a kernel to the SPs  before moving on to the next kernel. For example, sequencer module  may distribute Workgroup  of kernel  to SP A, Workgroup  of kernel  to SP B, Workgroup  of kernel  to SP C and so on until kernel  has been processed by the SPs .","In other examples, according to some aspects of the disclosure, sequencer module  may receive and execute kernel and workgroup execution orders. For example, sequencer module  may receive the instructions defining instruction streams that direct sequencer module  to distribute workgroups of kernels to a specific SP of SPs . The instruction streams tie workgroups of different kernels together so that they are processed by the same SP of SPs . The instruction streams provide a way to manage resources of SP memories . For example, by carrying out the instructions that define the instruction streams, sequencer module  allows the input data associated with one workgroup to be shared and serially executed by workgroups of multiple other kernels.","Sequencer module  can be implemented to carry out instructions that define instruction streams when GPU  is executing an application having multiple kernels that process the same, or substantially the same, input data. For example, as described with respect to , application  includes three kernels -, each kernel having a plurality of associated workgroups. Workgroup  of kernel  corresponds to the same input data as Workgroup  of kernel  and Workgroup  of kernel . Accordingly, sequencer module  may sequentially distribute Workgroup  of kernels - to SP A. In addition, sequencer module  may distribute Workgroup  of kernels - to SP B, and so on until all workgroups of all kernels have been executed by SPs .","In this way, sequencer module  can manage local memory resources of SP memories . For example, prior to executing Workgroup  of kernel , GPU  transfers the input data associated with Workgroup  of kernel  from GPU memory  to SP memory A. After executing Workgroup  of kernel , and instead of fetching new data for SP memory A, sequencer module  directs SP A to execute Workgroup  of kernel , followed by Workgroup  of kernel . The input data of Workgroup  is the same among kernels -, so the data associated with Workgroup  can remain in SP memory A and be shared by Workgroup  of all three kernels -.","Executing instruction streams and sharing data between workgroups of different kernels may provide a local memory bandwidth savings, as compared to a system that distributes workgroups in a fixed distribution pattern. For example, sharing data between workgroups of different kernels allows for less data to be transferred between GPU memory  and SP memories . In the three kernel example shown in , memory bandwidth consumption between GPU memory  and SP memories  is reduced by two thirds. Rather than transferring the data associated with a workgroup, such as Workgroup , to local SP memory three times (e.g., one transfer for each kernel), GPU  can transfer the data associated with the workgroup to local SP memory once and share the data between all three kernels -.","According to some examples of the disclosure, the local memory bandwidth savings associated with executing instruction streams may also provide a time savings. For example, while the SPs may perform the same number calculations associated with the workgroups to execute a given program, such as program , time savings may be achieved because less data may be transferred between GPU memory  and SP memories . As described above, data transfer between GPU memory  and SP memories  may be a relatively time intensive process that introduces a bottleneck into the process of executing the program . Accordingly, reducing the amount of data that is required to be transferred between GPU memory  and SP memories  also may reduce the bottleneck associated with data transfer between GPU memory  and SP memories .","The instructions received by sequencer module  defining instruction streams may be generated by a user or may be automatically generated (e.g., by a compiler program). For example, a user (e.g., a software developer) may define and implement instruction streams using an API that includes one or more instruction stream commands. Upon receiving the application having the instruction stream commands, sequencer module  executes the instruction stream commands to manage local memory resources associated with the SPs .","The instructions defining instructions streams may be transmitted to sequencer module  by a host processor of a computing device, such as host processor  shown in . In examples in which GPU  is a distinct device (e.g., not included in a computing device having a host processor), another processing component may be responsible for transmitting instructions containing instruction streams to sequencer module .",{"@attributes":{"id":"p-0077","num":"0076"},"figref":["FIG. 5","FIG. 1","FIG. 4"],"b":["140","142","144","146","152","156","152","156","152","156","152","156","140","152","48","60"]},"Sequencer module  and SPs  may be configured similarly to, or the same as sequencer module  and SPs  shown and described with respect to . For example, sequencer module  may be responsible for controlling instruction and data flow within a GPU. Sequencer module  may include a combination of fixed function and programmable components for distributing work items and workgroups to SPs  and associated SP memories .","Sequencer module  distributes workgroups of kernels - in a fixed distribution pattern, without controlling the specific destination of the workgroups of different kernels. For example, sequencer module  distributes workgroups of the first kernel  by sequentially distributing a first workgroup WG to SP A (as represented by line ), a second workgroup WG to SP B (as represented by line ), a third workgroup WG to SP C (as represented by line ), and so on until the first kernel  has been distributed and executed. Sequencer module  then moves on to the second kernel  and the third kernel  and distributes those workgroups to the SPs . For example, the sequencer module  may continue in the fixed distribution pattern and may distribute all of the workgroups of the second kernel  among the SPs . Sequencer module  then may proceed to the third kernel  and distribute all of the workgroups of the third kernel  among the SPs .","Without the ability to control which SP  executes a particular workgroup, the data associated with workgroups may be required to be loaded into more than one of the SP memories . As shown in the example of , in following the fixed distribution pattern, after processing the entire first kernel ,sequencer module  distributes workgroup WG of the second kernel  to SP B (line ). Accordingly, the input data associated with WG must be loaded into SP memory B. In addition, after processing the entire second kernel , due to the fixed nature of the distribution pattern, sequencer module  distributes workgroup WG of the third kernel  to SP C (line ). Accordingly, the input data associated with WG is loaded into SP memory C.","As described above with respect to , the data associated with a particular workgroup typically must be loaded into local memory of an SP before the SP can execute the workgroup. Without the ability to control which SP  executes a particular workgroup, data associated with the workgroups cannot be shared between kernels. In the example shown in , the data associated with workgroup WG must be loaded into each of the SP memories A, B, and C at different times prior to processing by the SPs A-C. Accordingly, the memory bandwidth for the SPs  is equal to three times the input data for each kernel.",{"@attributes":{"id":"p-0082","num":"0081"},"figref":["FIG. 6","FIG. 1","FIG. 4"],"b":["180","184","186","188","200","204","200","204","200","204","200","204","180","200","48","60"]},"Sequencer module  and SPs  may be configured similarly to, or the same as sequencer module  and SPs  shown and described with respect to . For example, sequencer module  may be responsible for controlling instruction and data flow within a GPU. Sequencer module  may include a combination of fixed function and programmable components for distributing work items and workgroups to SP memories  for execution by the SPs .","According to some aspects of the disclosure, sequencer module  distributes workgroups of kernels - according to predefined instructions that designate workgroups to instruction streams. For example, sequencer module  may be configured to receive instruction streams that tie workgroups of different kernels together so that they are executed by the same SP. Accordingly, rather than distributing workgroups of kernels - in a fixed pattern (as shown, for example, in ), sequencer module  may be configured to distribute workgroups based on the instruction streams that tie workgroups of the kernels together.","In the example shown in , sequencer module  executes an instruction stream that ties workgroup WG of kernel  to workgroup WG of kernel , and to workgroup WG of kernel . The workgroups WG of the kernels - are all associated with the same input data. By executing the instruction stream, workgroup WG of the kernels - are sequentially processed using SP A. Accordingly, the input data associated with WG, which may be the same among kernels -, can be shared among the workgroups WG of kernels -. For example, the data associated with workgroup WG can be loaded into SP memory A, when processing WG of kernel , and shared by WG of kernel  and WG of kernel .","Retaining data in SP memory A and sharing that data among multiple workgroups provides efficient management of SP memory A. For example, rather than having to fetch and transfer new data into SP memory A after every execution of a workgroup, the data can remain in SP memory A and be shared by multiple workgroups of multiple kernels. Accordingly, local memory bandwidth consumption may be reduced. In the three-kernel example shown in , local memory bandwidth consumption is reduced by two thirds, compared to the three-kernel example shown in .","According to some examples of the disclosure, the local memory bandwidth savings associated with executing instruction streams also provides a time savings. For example, while the SPs  may perform the same number of calculations associated with the workgroups as a system that does not utilize instruction streams to execute a given program, time savings may be achieved because less data must be transferred between a GPU global memory and SP memories . Data transfer between GPU global memory and SP memories  may be a relatively time intensive process that introduces a bottleneck into the process of executing the kernels -. Accordingly, reducing the amount of data that is required to be transferred between GPU global memory and SP memories  also reduces the bottleneck associated with data transfer between GPU global memory and SP memories .",{"@attributes":{"id":"p-0088","num":"0087"},"figref":["FIG. 7","FIG. 1","FIG. 4"],"b":["240","244","248","252","256","252","256","252","256","252","256","252","48","60"]},"The example shown in  includes three SPs  executing workgroups associated with three kernels. It should be appreciated, however, that streams of execution orders may be implemented in systems having more or fewer SPs than those shown in  (e.g., 2 SPs, 10 SPs, 100s of SPs). In addition, streams of execution orders may link more or fewer workgroups and kernels together than the three shown in .","The streams of execution orders, or instruction streams -, may be assigned to SPs  by a sequencer module, such as the sequencer module  shown in . The instruction streams - virtually ties workgroups of different kernels together so that they are processed by the same SP. For example, as shown in , instruction stream  links Workgroup  of kernel  to Workgroup  of kernel  and Workgroup  of kernel . Likewise, instruction stream  links Workgroup  of kernel  to Workgroup  of kernel  and Workgroup  of kernel , and instruction stream  links Workgroup  of kernel  to Workgroup  of kernel  and Workgroup  of kernel ","In the example shown in , at least some of the input data associated with the like-numbered workgroups is consistent. For example, the input data associated with Workgroup  of kernel  is the same as, or has at least some overlap with, the input data associated with Workgroup  of kernel  and Workgroup  of kernel . Accordingly, SP A can execute instruction stream  by loading the input data associated with Workgroup  into SP memory A and sharing that input data when executing Workgroup  of kernel , Workgroup  of kernel , and Workgroup  of kernel . In this way, executing instruction stream  reduces the amount of data that is required to be transferred into and out of SP memory A during execution of kernels , , and . Similar operations may be carried out for SP B and SP memory B, and for SP C and SP memory C.",{"@attributes":{"id":"p-0092","num":"0091"},"figref":["FIG. 8","FIG. 7","FIG. 1","FIG. 4","FIG. 4"],"b":["300","240","248","300","48","60","300","60"]},"As shown in  candidates for execution order streams are initially identified (). According to some aspects, a user identifies candidate kernels that would benefit from utilizing instruction streams. For example, a user may identify kernels that utilize the same input data multiple times. In another example, a program, such as a compiler program, may identify candidate kernels that would benefit from implementing instruction streams. For example, a complier program may monitor memory access patterns and identify input data that is used by more than one kernel. When input data is used by more than one kernel of a program, an instruction stream may be implemented to tie the workgroups that utilize the same data such that those workgroups are executed by the same SP. Utilizing instruction streams in this way may help to manage local memory resources of SPs by reducing the number of times input data needs to be loaded into the local memory resources. For example, input data can be loaded into local memory of an SP once and shared among multiple workgroups of multiple kernels.","After candidates have been identified, execution order stream designations are generated (). A user may define instruction streams using an API adapted to include instruction stream commands. For example, APIs such as OpenGL, CUDA, DirectX, or any other API for creating GPU programs can be adapted to include one or more commands that allow a user to designate workgroups and their associated kernels to instruction streams. In another example, a program, such as a complier program, may automatically generate instruction streams after identifying repeated memory access patterns.","After the instruction streams are generated, the execution order designations are transmitted to and received by a GPU, such as GPU  (). In some examples, sequencer module  may receive input defining execution order streams that include one or more kernel designations and one or more workgroup designations. The instruction streams may be transmitted to sequencer module  by a host processor of a computing device, such as host processor  shown in . In examples in which GPU  is a distinct device (e.g., not included in a computing device having a host processor) another processing component may be responsible for receiving the instruction streams and transmitting them to sequencer module .","Sequencer module  may implement the execution orders by assigning the streams to SPs, such as SPs  (). For example, sequencer module  may assign workgroups designated in an instruction stream to be executed by the same SP of GPU . SPs execute the instruction streams by executing the instructions designated in the in the execution orders (). For example, an SP sequentially executes the workgroups designated in the instruction stream. By doing so, input data associated with the workgroups designated in the instruction stream can be shared among the workgroups designated in the instruction stream. Executing the instruction streams may reduce the amount of data that is required to be transferred between GPU memory  and SP memories , and reduce the overall time required to execute a particular program.","In the examples provided above, the instruction streams are described as tying workgroups of different kernels together so that the workgroups of the different kernels are executed consecutively by the same SP. Tying the workgroups of different kernels together in this way aids in managing the memory resources associated with the SPs because the data associated with the workgroups can be shared by multiple kernels. It should be understood, however, that the term \u201cworkgroup\u201d refers generically to a group of instructions. For example, a \u201cworkgroup\u201d may be referred to in Compute Unified Device Architecture (\u201cCUDA\u201d developed by NVIDIA Corporation, version . released Sep. 17, 2010) as a \u201cthread block.\u201d","It should also be understood that the workgroup and kernel designations are provided as an example only. The memory management aspects of the disclosure may be applied to other configurations of GPU applications. For example, other GPU applications may include a single relatively larger \u201ckernel\u201d that includes instructions that use the same input data more than once during execution. In such an example, aspects of the disclosure may still be applied to manage memory resources. Instruction streams may be created that tie the instructions together the use the same input data, despite the instructions belonging to the same kernel.","In one or more examples, the functions described may be implemented in hardware, software executed on hardware, firmware executed on hardware, or any combination thereof. In some examples, instructions stored on a computer-readable media may cause the hardware components to perform their respective functions described above. The computer-readable media may include computer data storage media. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and\/or data structures for implementation of the techniques described in this disclosure. By way of example, and not limitation, such computer-readable media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other medium that can be used to carry or store desired program code in the form of instructions or data structures and that can be accessed by a computer. Combinations of the above should also be included within the scope of computer-readable media.","The code may be executed by one or more processors, such as one or more DSPs, general purpose microprocessors, ASICs, FPGAs, or other equivalent integrated or discrete logic circuitry. Accordingly, the term \u201cprocessor,\u201d as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition, in some aspects, the functionality described herein may be provided within dedicated hardware and\/or software modules configured for encoding and decoding, or incorporated in a combined codec. Also, the techniques could be fully implemented in one or more circuits or logic elements.","The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a wireless handset, an integrated circuit (IC) or a set of ICs (e.g., a chip set). Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units. Rather, as described above, various units may be combined by a collection of interoperative hardware units, including one or more processors as described above, in conjunction with suitable software and\/or firmware.","Various examples have been described. These and other examples are within the scope of the following claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5","b":"146"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
