---
title: Sequential classification recognition of gesture primitives and window-based parameter smoothing for high dimensional touchpad (HDTP) user interfaces
abstract: A method for classification recognition of gestures and gesture primitives in a touch-based user interface. In an implementation the method comprises receiving tactile image data responsive to data generated from user touch of a user touch interface comprising a sensor array. The tactile image data is processed to create a plurality of numerical values responsive to data generated from the user touch interface. These numerical values are applied to a principle component analysis operation to produce a reduced-dimensionality data vector which is applied to a classifier having a plurality of classifier outputs interpretable as probabilities. The classifier outputs provide likelihoods that an execution gesture is from a collection of pre-defined gestures, and a decision test is used to produce a decision output indicating a gesture outcome useful in user interface applications. The arrangement can recognize single finger “6D” actions of roll, pitch, yaw, left-right, forward-back, and variations in applied pressure.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08754862&OS=08754862&RS=08754862
owner: 
number: 08754862
owner_city: San Antonio
owner_country: US
publication_date: 20110711
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","COPYRIGHT & TRADEMARK NOTICES","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS","CLOSING REMARKS"],"p":["Pursuant to 35 U.S.C. \u00a7119(e), this application claims benefit of priority from Provisional U.S. Patent application Ser. No. 61\/363,272, filed Jul. 11, 2010, and Provisional U.S. Patent Application 61\/506,096, filed Jul. 9, 2011, the contents of both of which are incorporated by reference.","A portion of the disclosure of this patent document may contain material, which is subject to copyright protection. Certain marks referenced herein may be common law or registered trademarks of the applicant, the assignee or third parties affiliated or unaffiliated with the applicant or the assignee. Use of these marks is for providing an enabling disclosure by way of example and shall not be construed to exclusively limit the scope of the disclosed subject matter to material associated with such marks.","The invention relates to user interfaces providing an additional number of simultaneously-adjustable interactively-controlled discrete (clicks, taps, discrete gestures) and pseudo-continuous (downward pressure, roll, pitch, yaw, multi-touch geometric measurements, continuous gestures, etc.) user-adjustable settings and parameters, and in particular to the sequential selective tracking of subsets of parameters, and further how these can be used in applications.","By way of general introduction, touch screens implementing tactile sensor arrays have recently received tremendous attention with the addition multi-touch sensing, metaphors, and gestures. After an initial commercial appearance in the products of FingerWorks, such advanced touch screen technologies have received great commercial success from their defining role in the iPhone and subsequent adaptations in PDAs and other types of cell phones and hand-held devices. Despite this popular notoriety and the many associated patent filings, tactile array sensors implemented as transparent touchscreens were taught in the 1999 filings of issued U.S. Pat. No. 6,570,078 and pending U.S. patent application Ser. No. 11\/761,978.","Despite the many popular touch interfaces and gestures, there remains a wide range of additional control capabilities that can yet be provided by further enhanced user interface technologies. A number of enhanced touch user interface features are described in U.S. Pat. No. 6,570,078, pending U.S. patent application Ser. Nos. 11\/761,978, 12\/418,605, 12\/502,230, 12\/541,948, and related pending U.S. patent applications. These patents and patent applications also address popular contemporary gesture and touch features. The enhanced user interface features taught in these patents and patent applications, together with popular contemporary gesture and touch features, can be rendered by the \u201cHigh Definition Touch Pad\u201d (HDTP) technology taught in those patents and patent applications. Implementations of the HDTP provide advanced multi-touch capabilities far more sophisticated that those popularized by FingerWorks, Apple, NYU, Microsoft, Gesturetek, and others.","For purposes of summarizing, certain aspects, advantages, and novel features are described herein. Not all such advantages may be achieved in accordance with any one particular embodiment. Thus, the disclosed subject matter may be embodied or carried out in a manner that achieves or optimizes one advantage or group of advantages without achieving all advantages as may be taught or suggested herein.","The invention provides, among other things, sequential selective tracking of subsets of parameters, the sequence of selections being made automatically by classifications derived from information calculated from data measured by the touchpad sensor.","In one aspect of the invention, the parameters can comprise left-right geometric center (\u201cx\u201d), forward-back geometric center (\u201cy\u201d), average downward pressure (\u201cp\u201d), clockwise-counterclockwise pivoting yaw angular rotation (\u201c\u03c8\u201d), tilting roll angular rotation (\u201c\u03c6\u201d), and tilting pitch angular rotation (\u201c\u03b8\u201d) parameters calculated in real time from sensor measurement data.","In one aspect of the invention, the information calculated from data measured by the touchpad sensor can comprise raw parameter values including three or more of left-right geometric center (\u201cx\u201d), forward-back geometric center (\u201cy\u201d), average downward pressure (\u201cp\u201d), clockwise-counterclockwise pivoting yaw angular rotation (\u201c\u03c8\u201d), tilting roll angular rotation (\u201c\u03c6\u201d), and tilting pitch angular rotation (\u201c\u03b8\u201d) parameters calculated in real time from sensor measurement data.","In another aspect of the invention, a method is provided for classification recognition of gesture primitives in a touch-based user interface using at least one computational processor, the method comprising:\n\n","In another aspect of the invention, the classifications are made from among a collection of predefined gestures or gesture primitives.","In another aspect of the invention, the classifications are made by heuristics.","In another aspect of the invention, the classifications are made by an artificial neural network.","In another aspect of the invention, the classifications are made by a genetic algorithm.","In another aspect of the invention, the classifications are made by combinations of two or more of heuristics, an artificial neural network, and a genetic algorithm,","In another aspect of the invention, a window-based arrangement is used to smooth the numerical values of measured gesture parameters.","In another aspect of the invention, a Kalman filter is used to smooth the numerical values of measured gesture parameters.","In the following, numerous specific details are set forth to provide a thorough description of various embodiments. Certain embodiments may be practiced without these specific details or with some variations in detail. In some instances, certain features are described in less detail so as not to obscure other aspects. The level of detail associated with each of the elements or features should not be construed to qualify the novelty or importance of one feature over the others.","In the following description, reference is made to the accompanying drawing figures which form a part hereof, and which show by way of illustration specific embodiments of the invention. It is to be understood by those of ordinary skill in this technological field that other embodiments may be utilized, and structural, electrical, as well as procedural changes may be made without departing from the scope of the present invention.","Despite the many popular touch interfaces and gestures in contemporary information appliances and computers, there remains a wide range of additional control capabilities that can yet be provided by further enhanced user interface technologies. A number of enhanced touch user interface features are described in U.S. Pat. No. 6,570,078, pending U.S. patent application Ser. Nos. 11\/761,978, 12\/418,605, 12\/502,230, 12\/541,948, and related pending U.S. patent applications. These patents and patent applications also address popular contemporary gesture and touch features. The enhanced user interface features taught in these patents and patent applications, together with popular contemporary gesture and touch features, can be rendered by the \u201cHigh Definition Touch Pad\u201d (HDTP) technology taught in those patents and patent applications.","The present patent application addresses additional technologies for feature and performance improvements of HDTP technologies. Specifically, this patent application addresses a curve-fitting approach to HDTP parameter extraction.","Overview of HDTP User Interface Technology","Before providing details specific to the present invention, some embodiments of HDTP technology is provided. This will be followed by a summarizing overview of HDTP technology. With the exception of a few minor variations and examples, the material presented in this overview section is draw from U.S. Pat. No. 6,570,078, pending U.S. patent application Ser. Nos. 11\/761,978, 12\/418,605, 12\/502,230, 12\/541,948, 12\/724,413, 13\/026,248, and related pending U.S. patent applications and is accordingly attributed to the associated inventors.","Embodiments Employing a Touchpad and Touchscreen Form of a HDTP",{"@attributes":{"id":"p-0072","num":"0078"},"figref":["FIGS. 1","FIG. 1","FIG. 1","FIGS. 1","FIG. 1","FIG. 1"],"i":["a","g ","a","e ","a ","b ","a","b ","c ","d "],"b":["1","2","2","1"]},{"@attributes":{"id":"p-0073","num":"0079"},"figref":["FIG. 1","FIG. 1","FIGS. 1"],"i":["e ","f ","e","f "],"b":"1"},{"@attributes":{"id":"p-0074","num":"0080"},"figref":"FIG. 1","i":"g "},"In at least the arrangements of , , , and , or other sufficiently large tactile sensor implementation of the HDTP, more than one hand can be used an individually recognized as such.","Embodiments Incorporating the HDTP into a Traditional or Contemporary Generation Mouse",{"@attributes":{"id":"p-0077","num":"0083"},"figref":["FIGS. 2","FIGS. 3"],"i":["a","e ","a","b "],"b":["2","3"]},"In the integrations depicted in -the HDTP tactile sensor can be a stand-alone component or can be integrated over a display so as to form a touchscreen. Such configurations have very recently become popularized by the product release of Apple \u201cMagic Mouse\u2122\u201d although such combinations of a mouse with a tactile sensor array on its back responsive to multitouch and gestures were taught earlier in pending U.S. patent application Ser. No. 12\/619,678 (priority date Feb. 12, 2004) entitled \u201cUser Interface Mouse with Touchpad Responsive to Gestures and Multi-Touch.\u201d","In another embodiment taught in the specification of issued U.S. Pat. No. 7,557,797 and associated pending continuation applications more than two touchpads can be included in the advance mouse embodiment, for example as suggested in the arrangement of . As with the arrangements of -, one or more of the plurality of HDTP tactile sensors or exposed sensor areas of arrangements such as that of can be integrated over a display so as to form a touchscreen. Other advance mouse arrangements include the integrated trackball\/touchpad\/mouse combinations of -taught in U.S. Pat. No. 7,557,797.","Overview of HDTP User Interface Technology","The information in this section provides an overview of HDTP user interface technology as described in U.S. Pat. No. 6,570,078, pending U.S. patent application Ser. Nos. 11\/761,978, 12\/418,605, 12\/502,230, 12\/541,948, and related pending U.S. patent applications.","In an embodiment, a touchpad used as a pointing and data entry device can comprise an array of sensors. The array of sensors is used to create a tactile image of a type associated with the type of sensor and method of contact by the human hand.","In one embodiment, the individual sensors in the sensor array are pressure sensors and a direct pressure-sensing tactile image is generated by the sensor array.","In another embodiment, the individual sensors in the sensor array are proximity sensors and a direct proximity tactile image is generated by the sensor array. Since the contacting surfaces of the finger or hand tissue contacting a surface typically increasingly deforms as pressure is applied, the sensor array comprised of proximity sensors also provides an indirect pressure-sensing tactile image.","In another embodiment, the individual sensors in the sensor array can be optical sensors. In one variation of this, an optical image is generated and an indirect proximity tactile image is generated by the sensor array. In another variation, the optical image can be observed through a transparent or translucent rigid material and, as the contacting surfaces of the finger or hand tissue contacting a surface typically increasingly deforms as pressure is applied, the optical sensor array also provides an indirect pressure-sensing tactile image.","In some embodiments, the array of sensors can be transparent or translucent and can be provided with an underlying visual display element such as an alphanumeric, graphics, or image display. The underlying visual display can comprise, for example, an LED array display, a backlit LCD, etc. Such an underlying display can be used to render geometric boundaries or labels for soft-key functionality implemented with the tactile sensor array, to display status information, etc. Tactile array sensors implemented as transparent touchscreens are taught in the 1999 filings of issued U.S. Pat. No. 6,570,078 and pending U.S. patent application Ser. No. 11\/761,978.","In an embodiment, the touchpad or touchscreen can comprise a tactile sensor array obtains or provides individual measurements in every enabled cell in the sensor array that provides these as numerical values. The numerical values can be communicated in a numerical data array, as a sequential data stream, or in other ways. When regarded as a numerical data array with row and column ordering that can be associated with the geometric layout of the individual cells of the sensor array, the numerical data array can be regarded as representing a tactile image. The only tactile sensor array requirement to obtain the full functionality of the HDTP is that the tactile sensor array produce a multi-level gradient measurement image as a finger, part of hand, or other pliable object varies is proximity in the immediate area of the sensor surface.","Such a tactile sensor array should not be confused with the \u201cnull\/contact\u201d touchpad which, in normal operation, acts as a pair of orthogonally responsive potentiometers. These \u201cnull\/contact\u201d touchpads do not produce pressure images, proximity images, or other image data but rather, in normal operation, two voltages linearly corresponding to the location of a left-right edge and forward-back edge of a single area of contact. Such \u201cnull\/contact\u201d touchpads, which are universally found in existing laptop computers, are discussed and differentiated from tactile sensor arrays in issued U.S. Pat. No. 6,570,078 and pending U.S. patent application Ser. No. 11\/761,978. Before leaving this topic, it is pointed out that these the \u201cnull\/contact\u201d touchpads nonetheless can be inexpensively adapted with simple analog electronics to provide at least primitive multi-touch capabilities as taught in issued U.S. Pat. No. 6,570,078 and pending U.S. patent application Ser. No. 11\/761,978 (pre-grant publication U.S. 2007\/0229477 and therein, paragraphs [0022]-[0029], for example).","More specifically,  (adapted from U.S. patent application Ser. No. 12\/418,605) illustrates the side view of a finger  lightly touching the surface  of a tactile sensor array. In this example, the finger  contacts the tactile sensor surface in a relatively small area . In this situation, on either side the finger curves away from the region of contact , where the non-contacting yet proximate portions of the finger grow increasingly far , , , from the surface of the sensor . These variations in physical proximity of portions of the finger with respect to the sensor surface should cause each sensor element in the tactile proximity sensor array to provide a corresponding proximity measurement varying responsively to the proximity, separation distance, etc. The tactile proximity sensor array advantageously comprises enough spatial resolution to provide a plurality of sensors within the area occupied by the finger (for example, the area comprising width ). In this case, as the finger is pressed down, the region of contact  grows as the more and more of the pliable surface of the finger conforms to the tactile sensor array surface , and the distances , , , contract. If the finger is tilted, for example by rolling in the user viewpoint counterclockwise (which in the depicted end-of-finger viewpoint clockwise ) the separation distances on one side of the finger , will contract while the separation distances on one side of the finger , will lengthen. Similarly if the finger is tilted, for example by rolling in the user viewpoint clockwise (which in the depicted end-of-finger viewpoint counterclockwise ) the separation distances on the side of the finger , will contract while the separation distances on the side of the finger , will lengthen.","In many various embodiments, the tactile sensor array can be connected to interface hardware that sends numerical data responsive to tactile information captured by the tactile sensor array to a processor. In various embodiments, this processor will process the data captured by the tactile sensor array and transform it various ways, for example into a collection of simplified data, or into a sequence of tactile image \u201cframes\u201d (this sequence akin to a video stream), or into highly refined information responsive to the position and movement of one or more fingers and other parts of the hand.","As to further detail of the latter example, a \u201cframe\u201d can refer to a 2-dimensional list, number of rows by number of columns, of tactile measurement value of every pixel in a tactile sensor array at a given instance. The time interval between one frame and the next one depends on the frame rate of the system and the number of frames in a unit time (usually frames per second). However, these features are and are not firmly required. For example, in some embodiments a tactile sensor array can not be structured as a 2-dimensional array but rather as row-aggregate and column-aggregate measurements (for example row sums and columns sums as in the tactile sensor of year 2003-2006 Apple Powerbooks, row and column interference measurement data as can be provided by a surface acoustic wave or optical transmission modulation sensor as discussed later in the context of , etc.). Additionally, the frame rate can be adaptively-variable rather than fixed, or the frame can be segregated into a plurality regions each of which are scanned in parallel or conditionally (as taught in U.S. Pat. No. 6,570,078 and pending U.S. patent application Ser. No. 12\/418,605), etc.",{"@attributes":{"id":"p-0091","num":"0097"},"figref":["FIG. 5","FIG. 5"],"i":["a ","b "]},{"@attributes":{"id":"p-0092","num":"0098"},"figref":"FIG. 6"},"Types of Tactile Sensor Arrays","The tactile sensor array employed by HDTP technology can be implemented by a wide variety of means, for example:\n\n","Below a few specific examples of the above are provided by way of illustration; however these are by no means limiting. The examples include:\n\n","An example implementation of a tactile sensor array is a pressure sensor array. Pressure sensor arrays discussed in U.S. Pat. No. 6,570,078 and pending U.S. patent application Ser. No. 11\/761,978.  depicts a pressure sensor array arrangement comprising a rectangular array of isolated individual two-terminal pressure sensor elements. Such two-terminal pressure sensor elements typically operate by measuring changes in electrical (resistive, capacitive) or optical properties of an elastic material as the material is compressed. In typical embodiment, each sensor element in the sensor array can be individually accessed via multiplexing arrangement, for example as shown in , although other arrangements are possible and provided for by the invention. Examples of prominent manufacturers and suppliers of pressure sensor arrays include Tekscan, Inc. (307 West First Street, South Boston, Mass., 02127), Pressure Profile Systems (5757 Century Boulevard, Suite 600, Los Angeles, Calif. 90045), Sensor Products, Inc. (300 Madison Avenue, Madison, N.J. 07940 USA), and Xsensor Technology Corporation (Suite 111, 319-2nd Ave SW, Calgary, Alberta T2P 0C5, Canada).","Capacitive proximity sensors can be used in various handheld devices with touch interfaces. Prominent manufacturers and suppliers of such sensors, both in the form of opaque touchpads and transparent touch screens, include Balda AG (Bergkirchener Str. 228, 32549 Bad Oeynhausen, DE), Cypress (198 Champion Ct., San Jose, Calif. 95134), and Synaptics (2381 Bering Dr., San Jose, Calif. 95131). In such sensors, the region of finger contact is detected by variations in localized capacitance resulting from capacitive proximity effects induced by an overlapping or otherwise nearly-adjacent finger. More specifically, the electrical field at the intersection of orthogonally-aligned conductive buses is influenced by the vertical distance or gap between the surface of the sensor array and the skin surface of the finger. Such capacitive proximity sensor technology is low-cost, reliable, long-life, stable, and can readily be made transparent.  shows a popularly accepted view of a typical cell phone or PDA capacitive proximity sensor implementation. Capacitive sensor arrays of this type can be highly susceptible to noise and various shielding and noise-suppression electronics and systems techniques can need to be employed for adequate stability, reliability, and performance in various electric field and electromagnetically-noisy environments. In some embodiments of an HDTP, the present invention can use the same spatial resolution as current capacitive proximity touchscreen sensor arrays. In other embodiments of the present invention, a higher spatial resolution is advantageous.","Forrest M. Mims is credited as showing that an LED can be used as a light detector as well as a light emitter. Recently, light-emitting diodes have been used as a tactile proximity sensor array. Such tactile proximity array implementations typically need to be operated in a darkened environment (as seen in the video in the above web link). In one embodiment provided for by the invention, each LED in an array of LEDs can be used as a photodetector as well as a light emitter, although a single LED can either transmit or receive information at one time. Each LED in the array can sequentially be selected to be set to be in receiving mode while others adjacent to it are placed in light emitting mode. A particular LED in receiving mode can pick up reflected light from the finger, provided by said neighboring illuminating-mode LEDs.  depicts an implementation. The invention provides for additional systems and methods for not requiring darkness in the user environment in order to operate the LED array as a tactile proximity sensor. In one embodiment, potential interference from ambient light in the surrounding user environment can be limited by using an opaque pliable or elastically deformable surface covering the LED array that is appropriately reflective (directionally, amorphously, etc. as can be advantageous in a particular design) on the side facing the LED array. Such a system and method can be readily implemented in a wide variety of ways as is clear to one skilled in the art. In another embodiment, potential interference from ambient light in the surrounding user environment can be limited by employing amplitude, phase, or pulse width modulated circuitry or software to control the underlying light emission and receiving process. For example, in an implementation the LED array can be configured to emit modulated light modulated at a particular carrier frequency or variational waveform and respond to only modulated light signal components extracted from the received light signals comprising that same carrier frequency or variational waveform. Such a system and method can be readily implemented in a wide variety of ways as is clear to one skilled in the art.","Use of video cameras for gathering control information from the human hand in various ways is discussed in U.S. Pat. No. 6,570,078 and Pending U.S. patent application Ser. No. 10\/683,915. Here the camera image array is employed as an HDTP tactile sensor array. Images of the human hand as captured by video cameras can be used as an enhanced multiple-parameter interface responsive to hand positions and gestures, for example as taught in U.S. patent application Ser. No. 10\/683,915 Pre-Grant-Publication 2004\/0118268 (paragraphs [314], [321]-[332], [411], [653], both stand-alone and in view of [325], as well as [241]-[263]). and depict single camera implementations, while depicts a two camera implementation. As taught in the aforementioned references, a wide range of relative camera sizes and positions with respect to the hand are provided for, considerably generalizing the arrangements shown in -","In another video camera tactile controller embodiment, a flat or curved transparent or translucent surface or panel can be used as sensor surface. When a finger is placed on the transparent or translucent surface or panel, light applied to the opposite side of the surface or panel reflects light in a distinctly different manner than in other regions where there is no finger or other tactile contact. The image captured by an associated video camera will provide gradient information responsive to the contact and proximity of the finger with respect to the surface of the translucent panel. For example, the parts of the finger that are in contact with the surface will provide the greatest degree of reflection while parts of the finger that curve away from the surface of the sensor provide less reflection of the light. Gradients of the reflected light captured by the video camera can be arranged to produce a gradient image that appears similar to the multilevel quantized image captured by a pressure sensor. By comparing changes in gradient, changes in the position of the finger and pressure applied by the finger can be detected.  depicts an implementation.",{"@attributes":{"id":"p-0101","num":"0118"},"figref":["FIGS. 12","FIG. 12","FIG. 12"],"i":["a","b ","a","b "],"b":"12"},{"@attributes":{"id":"p-0102","num":"0119"},"figref":"FIG. 13"},"Compensation for Non-Ideal Behavior of Tactile Sensor Arrays","Individual sensor elements in a tactile sensor array produce measurements that vary sensor-by-sensor when presented with the same stimulus. Inherent statistical averaging of the algorithmic mathematics can damp out much of this, but for small image sizes (for example, as rendered by a small finger or light contact), as well as in cases where there are extremely large variances in sensor element behavior from sensor to sensor, the invention provides for each sensor to be individually calibrated in implementations where that can be advantageous. Sensor-by-sensor measurement value scaling, offset, and nonlinear warpings can be invoked for all or selected sensor elements during data acquisition scans. Similarly, the invention provides for individual noisy or defective sensors can be tagged for omission during data acquisition scans.",{"@attributes":{"id":"p-0105","num":"0122"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0106","num":"0123"},"figref":"FIG. 15"},"Additionally, the macroscopic arrangement of sensor elements can introduce nonlinear spatial warping effects. As an example, various manufacturer implementations of capacitive proximity sensor arrays and associated interface electronics are known to comprise often dramatic nonlinear spatial warping effects.  depicts the comparative performance of a group of contemporary handheld devices wherein straight lines were entered using the surface of the respective touchscreens. A common drawing program was used on each device, with widely-varying type and degrees of nonlinear spatial warping effects clearly resulting. For simple gestures such as selections, finger-flicks, drags, spreads, etc., such nonlinear spatial warping effects introduce little consequence. For more precision applications, such nonlinear spatial warping effects introduce unacceptable performance. Close study of  shows different types of responses to tactile stimulus in the direct neighborhood of the relatively widely-spaced capacitive sensing nodes versus tactile stimulus in the boundary regions between capacitive sensing nodes. Increasing the number of capacitive sensing nodes per unit area can reduce this, as can adjustments to the geometry of the capacitive sensing node conductors. In many cases improved performance can be obtained by introducing or more carefully implementing interpolation mathematics.","Types of Hand Contact Measurements and Features Provided by HDTP Technology",{"@attributes":{"id":"p-0109","num":"0126"},"figref":["FIGS. 17","FIGS. 17","FIG. 17","FIGS. 17"],"i":["a","f ","a","c ","c","d","f "],"b":["17","17","17"]},"Each of the six parameters listed above can be obtained from operations on a collection of sums involving the geometric location and tactile measurement value of each tactile measurement sensor. Of the six parameters, the left-right geometric center, forward-back geometric center, and clockwise-counterclockwise yaw rotation can be obtained from binary threshold image data. The average downward pressure, roll, and pitch parameters are in some embodiments beneficially calculated from gradient (multi-level) image data. One remark is that because binary threshold image data is sufficient for the left-right geometric center, forward-back geometric center, and clockwise-counterclockwise yaw rotation parameters, these also can be discerned for flat regions of rigid non-pliable objects, and thus the HDTP technology thus can be adapted to discern these three parameters from flat regions with striations or indentations of rigid non-pliable objects.","These \u2018Position Displacement\u2019 parameters -can be realized by various types of unweighted averages computed across the blob of one or more of each the geometric location and tactile measurement value of each above-threshold measurement in the tactile sensor image. The pivoting rotation can be calculated from a least-squares slope which in turn involves sums taken across the blob of one or more of each the geometric location and the tactile measurement value of each active cell in the image; alternatively a high-performance adapted eigenvector method taught in co-pending provisional patent application U.S. Ser. No. 12\/724,413 \u201cHigh-Performance Closed-Form Single-Scan Calculation of Oblong-Shape Rotation Angles from Binary Images of Arbitrary Size Using Running Sums,\u201d filed Mar. 14, 2009, can be used. The last two angle (\u201ctilt\u201d) parameters, pitch and roll, can be realized by performing calculations on various types of weighted averages as well as a number of other methods.","Each of the six parameters portrayed in -can be measured separately and simultaneously in parallel.  (adapted from U.S. Pat. No. 6,570,078) suggests general ways in which two or more of these independently adjustable degrees of freedom adjusted at once.","The HDTP technology provides for multiple points of contact, these days referred to as \u201cmulti-touch.\u201d  (adapted from U.S. patent application Ser. No. 12\/418,605 and described in U.S. Pat. No. 6,570,078) demonstrates a few two-finger multi-touch postures or gestures from the hundreds that can be readily recognized by HDTP technology. HDTP technology can also be configured to recognize and measure postures and gestures involving three or more fingers, various parts of the hand, the entire hand, multiple hands, etc. Accordingly, the HDTP technology can be configured to measure areas of contact separately, recognize shapes, fuse measures or pre-measurement data so as to create aggregated measurements, and other operations.","By way of example,  (adapted from U.S. Pat. No. 6,570,078) illustrates the pressure profiles for a number of example hand contacts with a pressure-sensor array. In the case  of a finger's end, pressure on the touch pad pressure-sensor array can be limited to the finger tip, resulting in a spatial pressure distribution profile ; this shape does not change much as a function of pressure. Alternatively, the finger can contact the pad with its flat region, resulting in light pressure profiles  which are smaller in size than heavier pressure profiles . In the case  where the entire finger touches the pad, a three-segment pattern (, , ) will result under many conditions; under light pressure a two segment pattern (or missing) could result. In all but the lightest pressures the thumb makes a somewhat discernible shape  as do the wrist , edge-of-hand \u201ccuff\u201d , and palm ; at light pressures these patterns thin and can also break into disconnected regions. Whole hand patterns such the first  and flat hand  have more complex shapes. In the case of the first , a degree of curl can be discerned from the relative geometry and separation of sub-regions (here depicted, as an example, as , , and ). In the case of the whole flat hand , there can be two or more sub-regions which can be in fact joined (as within ) or disconnected (as an example, as and are); the whole hand also affords individual measurement of separation \u201cangles\u201d among the digits and thumb (, , , ) which can easily be varied by the user.","HDTP technology robustly provides feature-rich capability for tactile sensor array contact with two or more fingers, with other parts of the hand, or with other pliable (and for some parameters, non-pliable) objects. In one embodiment, one finger on each of two different hands can be used together to at least double number of parameters that can be provided. Additionally, new parameters particular to specific hand contact configurations and postures can also be obtained. By way of example,  (adapted from U.S. patent application Ser. No. 12\/418,605 and described in U.S. Pat. No. 6,570,078) depicts one of a wide range of tactile sensor images that can be measured by using more of the human hand. U.S. Pat. No. 6,570,078 and pending U.S. patent application Ser. No. 11\/761,978 provide additional detail on use of other parts of hand. Within the context of the example of :\n\n","Other HDTP Processing, Signal Flows, and Operations","In order to accomplish this range of capabilities, HDTP technologies must be able to parse tactile images and perform operations based on the parsing. In general, contact between the tactile-sensor array and multiple parts of the same hand forfeits some degrees of freedom but introduces others. For example, if the end joints of two fingers are pressed against the sensor array as in , it will be difficult or impossible to induce variations in the image of one of the end joints in six different dimensions while keeping the image of the other end joints fixed. However, there are other parameters that can be varied, such as the angle between two fingers, the difference in coordinates of the finger tips, and the differences in pressure applied by each finger.","In general, compound images can be adapted to provide control over many more parameters than a single contiguous image can. For example, the two-finger postures considered above can readily pro-vide a nine-parameter set relating to the pair of fingers as a separate composite object adjustable within an ergonomically comfortable range. One example nine-parameter set the two-finger postures consider above is:\n\n","As another example, by using the whole hand pressed flat against the sensor array including the palm and wrist, it is readily possible to vary as many as sixteen or more parameters independently of one another. A single hand held in any of a variety of arched or partially-arched postures provides a very wide range of postures that can be recognized and parameters that can be calculated.","When interpreted as a compound image, extracted parameters such as geometric center, average downward pressure, tilt (pitch and roll), and pivot (yaw) can be calculated for the entirety of the asterism or constellation of smaller blobs. Additionally, other parameters associated with the asterism or constellation can be calculated as well, such as the aforementioned angle of separation between the fingers. Other examples include the difference in downward pressure applied by the two fingers, the difference between the left-right (\u201cx\u201d) centers of the two fingertips, and the difference between the two forward-back (\u201cy\u201d) centers of the two fingertips. Other compound image parameters are possible and are provided by HDTP technology.","There are number of ways for implementing the handling of compound posture data images. Two contrasting examples are depicted in -(adapted from U.S. patent application Ser. No. 12\/418,605) although many other possibilities exist and are provided for by the invention. In the embodiment of , tactile image data is examined for the number \u201cM\u201d of isolated blobs (\u201cregions\u201d) and the primitive running sums are calculated for each blob. This can be done, for example, with the algorithms described earlier. Post-scan calculations can then be performed for each blob, each of these producing an extracted parameter set (for example, x position, y position, average pressure, roll, pitch, yaw) uniquely associated with each of the M blobs (\u201cregions\u201d). The total number of blobs and the extracted parameter sets are directed to a compound image parameter mapping function to produce various types of outputs, including:\n\n",{"@attributes":{"id":"p-0122","num":"0158"},"figref":["FIG. 22","FIG. 22"],"i":["b ","a. "]},"Additionally, embodiments of the invention can be set up to recognize one or more of the following possibilities:\n\n","Embodiments that recognize two or more of these possibilities can further be able to discern and process combinations of two more of the possibilities.",{"@attributes":{"id":"p-0125","num":"0165"},"figref":"FIG. 22","i":"c "},"Refining of the HDTP User Experience","As an example of user-experience correction of calculated parameters, it is noted that placement of hand and wrist at a sufficiently large yaw angle can affect the range of motion of tilting. As the rotation angle increases in magnitude, the range of tilting motion decreases as mobile range of human wrists gets restricted. The invention provides for compensation for the expected tilt range variation as a function of measured yaw rotation angle. An embodiment is depicted in the middle portion of  (adapted from U.S. patent application Ser. No. 12\/418,605). As another example of user-experience correction of calculated parameters, the user and application can interpret the tilt measurement in a variety of ways. In one variation for this example, tilting the finger can be interpreted as changing an angle of an object, control dial, etc. in an application. In another variation for this example, tilting the finger can be interpreted by an application as changing the position of an object within a plane, shifting the position of one or more control sliders, etc. Typically each of these interpretations would require the application of at least linear, and typically nonlinear, mathematical transformations so as to obtain a matched user experience for the selected metaphor interpretation of tilt. In one embodiment, these mathematical transformations can be performed as illustrated in the lower portion of . The invention provides for embodiments with no, one, or a plurality of such metaphor interpretation of tilt.","As the finger is tilted to the left or right, the shape of the area of contact becomes narrower and shifts away from the center to the left or right. Similarly as the finger is tilted forward or backward, the shape of the area of contact becomes shorter and shifts away from the center forward or backward. For a better user experience, the invention provides for embodiments to include systems and methods to compensate for these effects (i.e. for shifts in blob size, shape, and center) as part of the tilt measurement portions of the implementation. Additionally, the raw tilt measures can also typically be improved by additional processing. (adapted from U.S. patent application Ser. No. 12\/418,605) depicts an embodiment wherein the raw tilt measurement is used to make corrections to the geometric center measurement under at least conditions of varying the tilt of the finger. Additionally, the invention provides for yaw angle compensation for systems and situations wherein the yaw measurement is sufficiently affected by tilting of the finger. An embodiment of this correction in the data flow is shown in (adapted from U.S. patent application Ser. No. 12\/418,605).","Additional HDTP Processing, Signal Flows, and Operations",{"@attributes":{"id":"p-0130","num":"0170"},"figref":["FIG. 25","FIGS. 17"],"i":["a","f"],"b":"17"},{"@attributes":{"id":"p-0131","num":"0171"},"figref":"FIG. 26"},"The HDTP affords and provides for yet further capabilities. For example, sequence of symbols can be directed to a state machine, as shown in (adapted from U.S. patent application Ser. No. 12\/418,605 and described in U.S. Pat. No. 6,570,078), to produce other symbols that serve as interpretations of one or more possible symbol sequences. In an embodiment, one or more symbols can be designated the meaning of an \u201cEnter\u201d key, permitting for sampling one or more varying parameter, rate, and symbol values and holding the value(s) until, for example, another \u201cEnter\u201d event, thus producing sustained values as illustrated in (adapted from U.S. patent application Ser. No. 12\/418,605 and described in U.S. Pat. No. 6,570,078). In an embodiment, one or more symbols can be designated as setting a context for interpretation or operation and thus control mapping or assignment operations on parameter, rate, and symbol values as shown in (adapted from U.S. patent application Ser. No. 12\/418,605 and described in U.S. Pat. No. 6,570,078). The operations associated with -can be combined to provide yet other capabilities. For example, the arrangement of shows mapping or assignment operations that feed an interpretation state machine which in turn controls mapping or assignment operations. In implementations where context is involved, such as in arrangements such as those depicted in -, the invention provides for both context-oriented and context-free production of parameter, rate, and symbol values. The parallel production of context-oriented and context-free values can be useful to drive multiple applications simultaneously, for data recording, diagnostics, user feedback, and a wide range of other uses.",{"@attributes":{"id":"p-0133","num":"0173"},"figref":["FIG. 28","FIGS. 25","FIG. 27","FIG. 28"],"b":["26","27","27"],"i":["a","b"]},"In an arrangement such as the one of , or in other implementations, at least two parameters are used for navigation of the cursor when the overall interactive user interface system is in a mode recognizing input from cursor control. These can be, for example, the left-right (\u201cx\u201d) parameter and forward\/back (\u201cy\u201d) parameter provided by the touchpad. The arrangement of  includes an implementation of this.","Alternatively, these two cursor-control parameters can be provided by another user interface device, for example another touchpad or a separate or attached mouse.","In some situations, control of the cursor location can be implemented by more complex means. One example of this would be the control of location of a 3D cursor wherein a third parameter must be employed to specify the depth coordinate of the cursor location. For these situations, the arrangement of  would be modified to include a third parameter (for use in specifying this depth coordinate) in addition to the left-right (\u201cx\u201d) parameter and forward\/back (\u201cy\u201d) parameter described earlier.","Focus control is used to interactively routing user interface signals among applications. In most current systems, there is at least some modality wherein the focus is determined by either the current cursor location or a previous cursor location when a selection event was made. In the user experience, this selection event typically involves the user interface providing an event symbol of some type (for example a mouse click, mouse double-click touchpad tap, touchpad double-tap, etc). The arrangement of  includes an implementation wherein a select event generated by the touchpad system is directed to the focus control element. The focus control element in this arrangement in turn controls a focus selection element that directs all or some of the broader information stream from the HDTP system to the currently selected application. (In , \u201cApplication K\u201d has been selected as indicated by the thick-lined box and information-flow arrows.)","In some embodiments, each application that is a candidate for focus selection provides a window displayed at least in part on the screen, or provides a window that can be deiconified from an icon tray or retrieved from beneath other windows that can be obfuscating it. In some embodiments, if the background window is selected, focus selection element that directs all or some of the broader information stream from the HDTP system to the operating system, window system, and features of the background window. In some embodiments, the background window can be in fact regarded as merely one of the applications shown in the right portion of the arrangement of . In other embodiments, the background window can be in fact regarded as being separate from the applications shown in the right portion of the arrangement of . In this case the routing of the broader information stream from the HDTP system to the operating system, window system, and features of the background window is not explicitly shown in .","Use of the Additional HDTP Parameters by Applications","The types of human-machine geometric interaction between the hand and the HDTP facilitate many useful applications within a visualization environment. A few of these include control of visualization observation viewpoint location, orientation of the visualization, and controlling fixed or selectable ensembles of one or more of viewing parameters, visualization rendering parameters, pre-visualization operations parameters, data selection parameters, simulation control parameters, etc. As one example, the 6D orientation of a finger can be naturally associated with visualization observation viewpoint location and orientation, location and orientation of the visualization graphics, etc. As another example, the 6D orientation of a finger can be naturally associated with a vector field orientation for introducing synthetic measurements in a numerical simulation.","As another example, at least some aspects of the 6D orientation of a finger can be naturally associated with the orientation of a robotically positioned sensor providing actual measurement data. As another example, the 6D orientation of a finger can be naturally associated with an object location and orientation in a numerical simulation. As another example, the large number of interactive parameters can be abstractly associated with viewing parameters, visualization rendering parameters, pre-visualization operations parameters, data selection parameters, numeric simulation control parameters, etc.","In yet another example, the x and y parameters provided by the HDTP can be used for focus selection and the remaining parameters can be used to control parameters within a selected GUI.","In still another example, x and y parameters provided by the HDTP can be regarded as a specifying a position within an underlying base plane and the roll and pitch angles can be regarded as a specifying a position within a superimposed parallel plane. In a first extension of the previous two-plane example, the yaw angle can be regarded as the rotational angle between the base and superimposed planes. In a second extension of the previous two-plane example, the finger pressure can be employed to determine the distance between the base and superimposed planes. In a variation of the previous two-plane example, the base and superimposed plane are not fixed parallel but rather intersect in an angle responsive to the finger yaw angle. In each example, either or both of the two planes can represent an index or indexed data, a position, a pair of parameters, etc. of a viewing aspect, visualization rendering aspect, pre-visualization operations, data selection, numeric simulation control, etc.","A large number of additional approaches are possible as is appreciated by one skilled in the art. These are provided for by the invention.","Support for Additional Parameters Via Browser Plug-Ins","The additional interactively-controlled parameters provided by the HDTP provide more than the usual number supported by conventional browser systems and browser networking environments. This can be addressed in a number of ways. The following examples of HDTP arrangements for use with browsers and servers are taught in pending U.S. patent application Ser. No. 12\/875,119 entitled \u201cData Visualization Environment with Dataflow Processing, Web, Collaboration, High-Dimensional User Interfaces, Spreadsheet Visualization, and Data Sonification Capabilities.\u201d","In a first approach, an HDTP interfaces with a browser both in a traditional way and additionally via a browser plug-in. Such an arrangement can be used to capture the additional user interface input parameters and pass these on to an application interfacing to the browser. An example of such an arrangement is depicted in ","In a second approach, an HDTP interfaces with a browser in a traditional way and directs additional GUI parameters though other network channels. Such an arrangement can be used to capture the additional user interface input parameters and pass these on to an application interfacing to the browser. An example of such an arrangement is depicted in ","In a third approach, an HDTP interfaces all parameters to the browser directly. Such an arrangement can be used to capture the additional user interface input parameters and pass these on to an application interfacing to the browser. An example of such an arrangement is depicted in ","The browser can interface with local or web-based applications that drive the visualization and control the data source(s), process the data, etc. The browser can be provided with client-side software such as JAVA Script or other alternatives. The browser can provide also be configured advanced graphics to be rendered within the browser display environment, allowing the browser to be used as a viewer for data visualizations, advanced animations, etc., leveraging the additional multiple parameter capabilities of the HDTP. The browser can interface with local or web-based applications that drive the advanced graphics. In an embodiment, the browser can be provided with Simple Vector Graphics (\u201cSVG\u201d) utilities (natively or via an SVG plug-in) so as to render basic 2D vector and raster graphics. In another embodiment, the browser can be provided with a 3D graphics capability, for example via the Cortona 3D browser plug-in.","Multiple Parameter Extensions to Traditional Hypermedia Objects","As taught in pending U.S. patent application Ser. No. 13\/026,248 entitled \u201cEnhanced Roll-Over, Button, Menu, Slider, and Hyperlink Environments for High Dimensional Touchpad (HTPD), other Advanced Touch User Interfaces, and Advanced Mice\u201d, the HDTP can be used to provide extensions to the traditional and contemporary hyperlink, roll-over, button, menu, and slider functions found in web browsers and hypermedia documents leveraging additional user interface parameter signals provided by an HTPD. Such extensions can include, for example:\n\n","Potential uses of the MHOS and more generally extensions provided for by the invention include:\n\n","A number of user interface metaphors can be employed in the invention and its use, including one or more of:\n\n","These extensions, features, and other aspects of the present invention permit far faster browsing, shopping, information gleaning through the enhanced features of these extended functionality roll-over and hyperlink objects.","In addition to MHOS that are additional-parameter extensions of traditional hypermedia objects, new types of MHOS unlike traditional or contemporary hypermedia objects can be implemented leveraging the additional user interface parameter signals and user interface metaphors that can be associated with them. Illustrative examples include:\n\n","Yet other types of MHOS are possible and provided for by the invention. For example:\n\n","In any of these, the invention provides for the MHO to be activated or selected by various means, for example by clicking or tapping when the cursor is displayed within the area, simply having the cursor displayed in the area (i.e., without clicking or tapping, as in rollover), etc. Further, it is anticipated that variations on any of these and as well as other new types of MHOS can similarly be crafted by those skilled in the art and these are provided for by the invention.","User Training","Since there is a great deal of variation from person to person, it is useful to include a way to train the invention to the particulars of an individual's hand and hand motions. For example, in a computer-based application, a measurement training procedure will prompt a user to move their finger around within a number of different positions while it records the shapes, patterns, or data derived from it for later use specifically for that user.","Typically most finger postures make a distinctive pattern. In one embodiment, a user-measurement training procedure could involve having the user prompted to touch the tactile sensor array in a number of different positions, for example as depicted in (adapted from U.S. patent application Ser. No. 12\/418,605). In some embodiments only representative extreme positions are recorded, such as the nine postures -. In yet other embodiments, or cases wherein a particular user does not provide sufficient variation in image shape, additional postures can be included in the measurement training procedure, for example as depicted in (adapted from U.S. patent application Ser. No. 12\/418,605). In some embodiments, trajectories of hand motion as hand contact postures are changed can be recorded as part of the measurement training procedure, for example the eight radial trajectories as depicted in -, the boundary-tracing trajectories of (adapted from U.S. patent application Ser. No. 12\/418,605), as well as others that would be clear to one skilled in the art. All these are provided for by the invention.","The range in motion of the finger that can be measured by the sensor can subsequently be re-corded in at least two ways. It can either be done with a timer, where the computer will prompt user to move his finger from position  to position , and the tactile image imprinted by the finger will be recorded at points ., . and .. Another way would be for the computer to query user to tilt their finger a portion of the way, for example \u201cTilt your finger \u2154 of the full range\u201d and record that imprint. Other methods are clear to one skilled in the art and are provided for by the invention.","Additionally, this training procedure allows other types of shapes and hand postures to be trained into the system as well. This capability expands the range of contact possibilities and applications considerably. For example, people with physical handicaps can more readily adapt the system to their particular abilities and needs.","Data Flow and Parameter Refinement",{"@attributes":{"id":"p-0164","num":"0221"},"figref":["FIG. 31","FIG. 31","FIG. 31"]},"For example, a blob allocation step can assign a data record for each contiguous blob found in a scan or other processing of the pressure, proximity, or optical image data obtained in a scan, frame, or snapshot of pressure, proximity, or optical data measured by a pressure, proximity, or optical tactile sensor array or other form of sensor. This data can be previously preprocessed (for example, using one or more of compensation, filtering, thresholding, and other operations) as shown in the figure, or can be presented directly from the sensor array or other form of sensor. In some implementations, operations such as compensation, thresholding, and filtering can be implemented as part of such a blob allocation step. In some implementations, the blob allocation step provides one or more of a data record for each blob comprising a plurality of running sum quantities derived from blob measurements, the number of blobs, a list of blob indices, shape information about blobs, the list of sensor element addresses in the blob, actual measurement values for the relevant sensor elements, and other information. A blob classification step can include for example shape information and can also include information regarding individual noncontiguous blobs that can or should be merged (for example, blobs representing separate segments of a finger, blobs representing two or more fingers or parts of the hand that are in at least a particular instance are to be treated as a common blob or otherwise to be associated with one another, blobs representing separate portions of a hand, etc.). A blob aggregation step can include any resultant aggregation operations including, for example, the association or merging of blob records, associated calculations, etc. Ultimately a final collection of blob records are produced and applied to calculation and refinement steps used to produce user interface parameter vectors. The elements of such user interface parameter vectors can comprise values responsive to one or more of forward-back position, left-right position, downward pressure, roll angle, pitch angle, yaw angle, etc from the associated region of hand input and can also comprise other parameters including rates of change of there or other parameters, spread of fingers, pressure differences or proximity differences among fingers, etc. Additionally there can be interactions between refinement stages and calculation stages, reflecting, for example, the kinds of operations described earlier in conjunction with , , and ","The resulting parameter vectors can be provided to applications, mappings to applications, window systems, operating systems, as well as to further HDTP processing. For example, the resulting parameter vectors can be further processed to obtain symbols, provide additional mappings, etc. In this arrangement, depending on the number of points of contact and how they are interpreted and grouped, one or more shapes and constellations can be identified, counted, and listed, and one or more associated parameter vectors can be produced. The parameter vectors can comprise, for example, one or more of forward-back, left-right, downward pressure, roll, pitch, and yaw associated with a point of contact. In the case of a constellation, for example, other types of data can be in the parameter vector, for example inter-fingertip separation differences, differential pressures, etc.","Example First-Level Measurement Calculation Chain","Attention is now directed to particulars of roll and pitch measurements of postures and gestures. depicts a side view of an exemplary finger and illustrating the variations in the pitch angle. -depict exemplary tactile image measurements (proximity sensing, pressure sensing, contact sensing, etc.) as a finger in contact with the touch sensor array is positioned at various pitch angles with respect to the surface of the sensor. In these, the small black dot denotes the geometric center corresponding to the finger pitch angle associated with . As the finger pitch angle is varied, it can be seen that:\n\n","From the user experience viewpoint, however, the user would not feel that a change in the front-back component of the finger's contact with the touch sensor array has changed. This implies the front-back component (\u201cy\u201d) of the geometric center of contact shape as measured by the touch sensor array should be corrected responsive to the measured pitch angle. This suggests a final or near-final measured pitch angle value should be calculated first and used to correct the final value of the measured front-back component (\u201cy\u201d) of the geometric center of contact shape.","Additionally, -depict the effect of increased downward pressure on the respective contact shapes of -. More specifically, the top row of -are the respective contact shapes of -, and the bottom row show the effect of increased downward pressure. In each case the oval shape expands in area (via an observable expansion in at least one dimension of the oval) which could thus shift the final value of the measured front-back component (\u201cy\u201d). (It is noted that for the case of a pressure sensor array, the measured pressure values measured by most or all of the sensors in the contact area would also increase accordingly.)","These and previous considerations imply:\n\n",{"@attributes":{"id":"p-0171","num":"0232"},"figref":["FIG. 34","FIGS. 34","FIG. 34"],"i":["a ","b","f ","d"],"b":"34","ul":{"@attributes":{"id":"ul0030","list-style":"none"},"li":{"@attributes":{"id":"ul0030-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0031","list-style":"none"},"li":["The eccentricity of the oval shape changes;","The position of the oval shape migrates and in the cases of -and -have a geometric center shifted from that of , and in the cases of -the oval shape migrates enough to no longer even overlap the geometric center of \n\nFrom the user experience, however, the user would not feel that the left-right component of the finger's contact with the touch sensor array has changed. This implies the left-right component (\u201cx\u201d) of the geometric center of contact shape as measured by the touch sensor array should be corrected responsive to the measured roll angle. This suggests a final or near-final measured roll angle value should be calculated first and used to correct the final value of the measured left-right component (\u201cx\u201d) of the geometric center of contact shape.\n"]}}}},"As with measurement of the finger pitch angle, increasing downward pressure applied by the finger can also invoke variations in contact shape involved in roll angle measurement, but typically these variations are minor and less significant for roll measurements than they are for pitch measurements. Accordingly, at least to a first level of approximation, effects of increasing the downward pressure can be neglected in calculation of roll angle.","Depending on the method used in calculating the pitch and roll angles, it is typically advantageous to first correct for yaw angle before calculating the pitch and roll angles. One source reason for this is that (dictated by hand and wrist physiology) from the user experience a finger at some non-zero yaw angle with respect to the natural rest-alignment of the finger would impart intended roll and pitch postures or gestures from the vantage point of the yawed finger position. Without a yaw-angle correction somewhere, the roll and pitch postures and movements of the finger would resolve into rotated components. As an extreme example of this, if the finger were yawed at a 90-degree angle with respect to a natural rest-alignment, roll postures and movements would measure as pitch postures and movements while pitch postures and movements would measure as roll postures and movements. As a second example of this, if the finger were yawed at a 45-degree angle, each roll and pitch posture and movement would case both roll and pitch measurement components. Additionally, some methods for calculating the pitch and roll angles (such as curve fitting and polynomial regression methods as taught in pending U.S. patent application Ser. No. 13\/038,372) work better if the blob data on which they operate is not rotated by a yaw angle. This suggests that a final or near-final measured yaw angle value should be calculated first and used in a yaw-angle rotation correction to the blob data applied to calculation of roll and pitch angles.","Regarding other calculations, at least to a first level of approximation downward pressure measurement in principle should not be affected by yaw angle. Also at least to a first level of approximation, for geometric center calculations sufficiently corrected for roll and pitch effects in principle should not be affected by yaw angle. (In practice there can be at least minor effects, to be considered and addressed later).","The example working first level of approximation conclusions together suggest a causal chain of calculation such as that depicted in .  depicts a utilization of this causal chain as a sequence flow of calculation blocks.  does not, however, represent a data flow since calculations in subsequent blocks depend on blob data in ways other than as calculated in preceding blocks. More specifically as to this,  depicts an example implementation of a real-time calculation chain for the left-right (\u201cx\u201d), front-back (\u201cy\u201d), downward pressure (\u201cp\u201d), roll (\u201c\u03c6\u201d), pitch (\u201c\u03b8\u201d), and yaw (\u201c\u03c8\u201d) measurements that can be calculated from blob data such as that produced in the exemplary arrangement of . Examples of methods, systems, and approaches to downward pressure calculations from tactile image data in a multi-touch context are provided in pending U.S. patent application Ser. No. 12\/418,605 and U.S. Pat. No. 6,570,078. Examples methods, systems, and approaches to yaw angle calculations from tactile image data are provided in pending U.S. patent application Ser. No. 12\/724,413; these can be applied to a multi-touch context via arrangements such as the depicted in . Examples methods, systems, and approaches to roll angle and pitch angle calculations from tactile image data in a multi-touch context are provided in pending U.S. patent application Ser. No. 12\/418,605 and Ser. No. 13\/038,372 as well as in U.S. Pat. No. 6,570,078 and include yaw correction considerations. Examples methods, systems, and approaches to front-back geometric center and left-right geometric center calculations from tactile image data in a multi-touch context are provided in pending U.S. patent application Ser. No. 12\/418,605 and U.S. Pat. No. 6,570,078.","The yaw rotation correction operation depicted in  operates on blob data as a preprocessing step prior to calculations of roll angle and pitch angle calculations from blob data (and more generally from tactile image data). The yaw rotation correction operation can, for example, comprise a rotation matrix or related operation which internally comprises sine and cosine functions as is appreciated by one skilled in the art. Approximations of the full needed range of yaw angle values (for example from nearly \u221290 degrees through zero to nearly +90 degrees, or in a more restricted system from nearly \u221245 degrees through zero to nearly +45 degrees) can therefore not be realistically approximated by a linear function. The need range of yaw angles can be adequately approximated by piecewise-affine functions such as those to be described in the next section. In some implementations it will be advantageous to implement the rotation operation with sine and cosine functions in the instruction set or library of a computational processor. In other implementations it will be advantageous to implement the rotation operation with piecewise-affine functions (such as those to be described in the next section) on a computational processor.",{"@attributes":{"id":"p-0177","num":"0240"},"figref":["FIG. 37","FIG. 37"]},{"@attributes":{"id":"p-0178","num":"0241"},"figref":["FIG. 37","FIG. 37"]},{"@attributes":{"id":"p-0179","num":"0242"},"figref":["FIG. 37","FIG. 37"]},{"@attributes":{"id":"p-0180","num":"0243"},"figref":["FIG. 37","FIG. 37","FIG. 37"]},"Additionally,  does not depict optional data flow support for the tilt refinements described in conjunction with , the tilt-influent correction to measured yaw angle described in conjunction with , the range-of-rotation correction described in conjunction with , the correction of left-right geometric center measurement using downward pressure measurement (as discussed just a bit earlier), the correction of roll angle using downward pressure measurement (as discussed just a bit earlier), or the direct correction of front-back geometric center measurement using downward pressure measurement. There are many further possible corrections and user experience improvements that can be added in similar fashion. In one embodiment any one or more such additional corrections are not performed in the context of  and either no such correction is provided, or such corrections are provided in a later stage after an arrangement such as that depicted in . In another embodiment one or more such corrections are implemented in the example arrangement of , for example through the addition of relevant data flow support to the relevant calculation step and additional calculations performed therein. In either case, any one or more such corrections can be implemented in various ways depending on approximations chosen and other considerations. The various ways include use of a linear function, a piecewise-linear function, an affine function, a piecewise-affine function, a nonlinear function, or combinations of two or more of these.","In one approach, one or more shared environments for linear function, a piecewise-linear function, an affine function, a piecewise-affine function, or combinations of two or more of these can be provided. In an embodiment of such an approach, one or more of these one or more shared environments can be incorporated into the calculation chain depicted in .","In another or related embodiment of such an approach, one or more of these one or more shared environments can be implemented in a processing stage subsequent to the calculation chain depicted in . In these circumstances, the output values from the calculation chain depicted in  can be regarded as \u201cfirst-order\u201d or \u201cunrefined\u201d output values which, upon further processing by these one or more shared environments produce \u201csecond-order\u201d or refined\u201d output values.","Additional Parameter Refinement","Additional refinement of the parameters can be obtained by additional processing. As an example,  shows an arrangement of  wherein each raw parameter vector is provided to additional parameter refinement processing to produce a corresponding refined parameter vector. The additional parameter refinement can comprise a single stage, or can internally comprise two or more internal parameter refinement stages as suggested in . The internal parameter refinement stages can be interconnected in various ways, including a simple chain, feedback and\/or control paths (as suggested by the dash-line arrows within the Parameter Refinement box), as well as parallel paths (not explicitly suggested in ), combinations, or other topologies as may be advantageous. The individual parameter refinement stages can comprise various approaches systems and methods, for example Kalman and\/or other types of statistical filters, matched filters, artificial neural networks (such as but not limited to those taught in pending U.S. provisional patent application 61\/309,421), linear or piecewise-linear transformations (such as but not limited to those taught in pending U.S. Provisional Patent Application 61\/327,458), nonlinear transformations, pattern recognition operations, dynamical systems, etc. In an embodiment, the parameter refinement can be provided with other information, such as the measured area of the associated blob, external shape classification of the associated blob, etc.","Segmented Classification of Parameters and Parameter Subsets",{"@attributes":{"id":"p-0185","num":"0248"},"figref":"FIG. 39","i":"a "},{"@attributes":{"id":"p-0186","num":"0249"},"figref":"FIG. 39","i":"b ","ul":{"@attributes":{"id":"ul0032","list-style":"none"},"li":{"@attributes":{"id":"ul0032-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0033","list-style":"none"},"li":["suppression of minor unintended variations in parameters the user does not intend to adjust within a particular interval of time;","suppression of minor unintended variations in parameters the user effectively does not adjust within a particular interval of time;","utilization of minor unintended variations in some parameters within a particular interval of time to aid in the refinement of parameters that are being adjusted within that interval of time;","reduction of real-time computational load in real-time processing."]}}}},"Accordingly, the invention provides, among other things, sequential selective tracking of subsets of parameters, the sequence of selections being made automatically by classifications derived from information calculated from data measured by the touchpad sensor. In one aspect of the invention, the parameters tracked at any particular moment include one or more of left-right geometric center (\u201cx\u201d), forward-back geometric center (\u201cy\u201d), average downward pressure (\u201cp\u201d), clockwise-counterclockwise pivoting yaw angular rotation (\u201c\u03c8\u201d), tilting roll angular rotation (\u201c\u03c6\u201d), and tilting pitch angular rotation (\u201c\u03b8\u201d) parameters calculated in real time from sensor measurement data.","Typically the left-right geometric center (\u201cx\u201d), forward-back geometric center (\u201cy\u201d) measurements are essentially independent and these can be tracked together if none of the other parameters only undergo minor spurious variation. An exemplary classification under such conditions could be {x,y}. For example, depicts two exemplary intervals of time wherein the {x,y} classification is an estimated outcome.","Other motions of the finger or parts of the hand can invoke variations of not only the intended parameter but also variation in one or more other \u201ccollateral\u201d parameters as well. One example of this is tilting roll angular rotation (\u201c\u03c6\u201d), where rolling the finger from a fixed left-right position nonetheless causes a correlated shift in the measured and calculated left-right geometric center (\u201cx\u201d). In an embodiment, the classification system discerns between a pure tilting roll angular rotation (\u201c\u03c6\u201d) with no intended change in left-right position (classified for example as {\u03c6}) from a mixed tilting roll angular rotation with an intended change in left-right position (classified for example as {\u03c6,x}). A similar example is the tilting pitch angular rotation (\u201c\u03b8\u201d), where pitching the finger from a fixed forward-back position nonetheless causes a correlated shift in the measured and calculated forward-back geometric center (\u201cy\u201d). In an embodiment, the classification system discerns between a pure tilting pitch angular rotation (\u201c\u03b8\u201d) with no intended change in forward-back position (classified for example as {\u03b8}) from a mixed tilting roll angular rotation with an intended change in forward-back position (classified for example as {\u03b8,y}). depicts an exemplary interval of time wherein the {\u03b8} classification is an estimated outcome and an exemplary interval of time wherein the {y} classification is an estimated outcome.","In a similar fashion, the invention provides for embodiments to include classifications for isolated changes in pressure {p} and isolated changes in yaw angle {\u03c8}. (Should it be useful, the invention also provides for embodiments to include classifications pertaining to isolated changes in left-right position {x} and\/or isolated changes in forward-back position {y}.)","Also in a similar fashion, the invention provides for embodiments to include classifications pertaining to other pairs of simultaneous parameter variations, for example such as but not limited to {x,p}, {y,p}, {\u03b8,\u03c8}, {\u03b8,p}, {\u03b8 x}, {\u03c6, \u03b8}, {\u03c6, \u03c8}, {\u03b8, \u03c8}, {\u03c6,p}, {\u03c6, y}, {\u03c8, x}, {\u03c8, y}, etc.",{"@attributes":{"id":"p-0192","num":"0259"},"figref":["FIG. 40","FIG. 40"],"i":["a ","b "],"b":["1","2","1","2","1","2"]},"Also in a similar fashion, the invention provides for embodiments to include classifications pertaining to other triples of parameter variations, for example such as but not limited to {x,y,p}, {\u03c6,\u03b8,\u03c8}, {\u03b8,p}, {\u03b8,x,p}, {\u03c6,\u03b8,p}, {\u03c6,\u03c8 p}, {\u03b8,\u03c8}, {\u03c6,p}, {\u03c6, y, p}, {\u03c8, x, p}, {\u03c8, y, p}, etc. depicts an exemplary simplified arrangement wherein degrees of change per unit time of a triple of parameters can be classified for:\n\n","Here the three orthogonal axes represent the magnitude (absolute value) of the change per unit time each of the pair of parameters. In this simplified example, the resulting state-space is partitioned into the eight aforementioned regions according to straight-line boundaries. However, this is merely exemplary; the invention provides for straight-line boundaries, curved boundaries, a mix of these, static boundary locations, conditional or controllable boundary locations, adaptive control of boundary locations, hysteretic boundary locations, etc. In an embodiment, the boundary for moving from classification A to classification B within the state-space can be different from boundary for moving from classification B to classification A within the state-space. In an embodiment, one or more boundary locations can be changed according to values and or variations in parameters other than parameter , parameter , and parameter . In an embodiment, the boundary locations can be changed according to values and or variations in other quantities, such as the measured area of the associated blob, external shape classification of the associated blob, etc.","In an embodiment the invention provides for including classifications pertaining to four parameter variations. The four dimensional present\/nonpresent state-space would comprise 2=16 classification regions.","The boundaries between such classification regions can comprise straight-line boundaries, curved boundaries, a mix of these, static boundary locations, conditional or controllable boundary locations, adaptive control of boundary locations, hysteretic boundary locations, etc. In an embodiment, the boundary for moving from classification A to classification B within the state-space can be different from boundary for moving from classification B to classification A within the state-space. In an embodiment, one or more boundary locations can be changed according to values and or variations in parameters other than the four associated with the state-space. In an embodiment, the boundary locations can be changed according to values and or variations in other quantities, such as the measured area of the associated blob, external shape classification of the associated blob, etc. In various approaches to this provided for by the invention, the classifications can be made by one or more of heuristics, an artificial neural network, a genetic algorithm.","In an embodiment the invention provides for including classifications pertaining to five parameter variations. The five dimensional state-space would comprise 32 classification regions. The boundaries between classification regions can comprise straight-line boundaries, curved boundaries, a mix of these, static boundary locations, conditional or controllable boundary locations, adaptive control of boundary locations, hysteretic boundary locations, etc. In an embodiment, the boundary for moving from classification A to classification B within the state-space can be different from boundary for moving from classification B to classification A within the state-space. In an embodiment, one or more boundary locations can be changed according to values and or variations in parameters other than the four associated with the state-space. In an embodiment, the boundary locations can be changed according to values and or variations in other quantities, such as the measured area of the associated blob, external shape classification of the associated blob, etc. In an aspect of the invention, the classifications are made by an artificial neural network.","In an embodiment the invention provides for including classifications pertaining to all six parameter variations. The five dimensional state-space would comprise 64 classification regions. The boundaries between classification regions can comprise straight-line boundaries, curved boundaries, a mix of these, static boundary locations, conditional or controllable boundary locations, adaptive control of boundary locations, hysteretic boundary locations, etc. In an embodiment, the boundary for moving from classification A to classification B within the state-space can be different from boundary for moving from classification B to classification A within the state-space. In an embodiment, the boundary locations can be changed according to values and or variations in other quantities, such as the measured area of the associated blob, external shape classification of the associated blob, etc. In an aspect of the invention, the classifications are made by an artificial neural network.","More generally, design consideration can include the number of gestures and parts of gestures (or \u201cgesture primitives\u201d) to be discerned. For example:\n\n",{"@attributes":{"id":"p-0200","num":"0277"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mn":"6"}},{"mtd":{"mn":"2"}}]}},{"mn":"15","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mi":"possibilities"}],"mo":"="},"mo":";"}}},"ul":{"@attributes":{"id":"ul0038","list-style":"none"},"li":{"@attributes":{"id":"ul0038-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0039","list-style":"none"},"li":"For recognition of triples of the individual 6D actions of a single finger, there are"}}}},{"@attributes":{"id":"p-0201","num":"0279"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mn":"6"}},{"mtd":{"mn":"3"}}]}},{"mn":"20","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mi":"possibilities"}],"mo":"="},"mo":";"}}},"ul":{"@attributes":{"id":"ul0040","list-style":"none"},"li":{"@attributes":{"id":"ul0040-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0041","list-style":"none"},"li":"For recognition of groups of 4 of the individual 6D actions of a single finger, there are"}}}},{"@attributes":{"id":"p-0202","num":"0281"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mn":"6"}},{"mtd":{"mn":"4"}}]}},{"mn":"15","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mi":"possibilities"}],"mo":"="},"mo":";"}}},"ul":{"@attributes":{"id":"ul0042","list-style":"none"},"li":{"@attributes":{"id":"ul0042-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0043","list-style":"none"},"li":"For recognition of groups of 4 of the individual 6D actions of a single finger, there are"}}}},{"@attributes":{"id":"p-0203","num":"0283"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mn":"6"}},{"mtd":{"mn":"5"}}]}},{"mn":"6","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mi":"possibilities"}],"mo":"="},"mo":";"}}},"ul":{"@attributes":{"id":"ul0044","list-style":"none"},"li":{"@attributes":{"id":"ul0044-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0045","list-style":"none"},"li":"For recognition of groups of 4 of the individual 6D actions of a single finger at once, there is 1 possibility.\n\nIn general there can be up to 6+15+20+15+6+1=63 types of potentially discernable motions, and if these are to be further distinguished by direction, there can be considerably more.\n"}}}},{"@attributes":{"id":"p-0204","num":"0285"},"figref":["FIG. 41","FIG. 31","FIG. 38"],"i":"a "},{"@attributes":{"id":"p-0205","num":"0286"},"figref":["FIG. 41","FIG. 41"],"i":["b ","b "]},{"@attributes":{"id":"p-0206","num":"0287"},"figref":["FIG. 42","FIG. 42","FIG. 41","FIG. 41"],"i":["a ","b ","a ","b "]},"The arrangements discussed thus far can be used for a variety of purposes in a HDTP system, for example:\n\n","In an example implementation, the detection of entire gestures and\/or components of gestures (\u201cgesture primitives\u201d) from a sequence of frames of sequential tactile image data is treated as a real-time sequential pattern recognition problem. This type of pattern recognition problem is formally referred as \u201csequence labeling\u201d [1] and in this context is often employed in speech recognition. For purposes here, a formalization of real-time sequence labeling relevant to the present invention is:\n\n","Note in an alternate formulation the output label sequence need not have the as many elements in it as the input observation sequence. For example, the output label sequence need not be produced with the same frequency as the input sequence. Thus, more generally\n\n","Although there are a wide variety of possible positions and gestures a user can perform on the HDTP, it is instructive to consider a few of the most basic gesture involving variation over a (typically short) duration of time of single individual finger orientations from the list below:\n\n","As a simple example, if each of these six gestures is allowed two states of existence, one for each direction of variation:\n\n","Conditions that are candidates for the null symbol can include the following three conditions:\n\n","Using the raw parameter calculations for each of the six parameters (as well as other possible derived quantities such as contact image area, yaw eigensystem eigenvectors, etc.) on the tactile image data comprised by each sensor system provided tactile image frame, at a given time t each tactile image frame could be represented in the next portion of the HDTP system by a vector of signals. As example of such a vector of signal:\n\n,avgp,bcx,bcy,cx,cy,eigv1,eigv2,\u03c6,\u03c8,\u03b8}\n\nwherein the discrete Cartesian moments (sometimes referred as image moments [3]) for a gray scale image of size M by N with pixel intensities I(x,y) are defined as:\n",{"@attributes":{"id":"p-0214","num":"0320"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mover":{"mi":"M","mo":"~"},"mrow":{"mi":["p","q"],"mo":","}},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"x","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"y","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"msup":[{"mi":["x","p"]},{"mi":["y","q"]}],"mo":["\u2062","\u2062"],"mrow":{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}}}}}}},"br":{}},{"@attributes":{"id":"p-0215","num":"0321"},"maths":[{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["I","step"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},{"mo":["\uf603","\u2003"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mrow":{"mrow":{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}},"mo":"\u2265","mi":"threshold"}}]},{"mtd":[{"mn":"0"},{"mrow":{"mrow":{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}},"mo":"<","mi":"threshold"}}]}]}}],"mo":"="}}},{"@attributes":{"id":"MATH-US-00006-2","num":"00006.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"M","mrow":{"mi":["p","q"],"mo":","}},"mo":"=","mrow":{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"x","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"y","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"msup":[{"mi":["x","p"]},{"mi":["y","q"]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mrow":{"msub":{"mi":["I","step"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},"mstyle":{"mtext":{}},"mi":"avgp"}}},"mo":"=","mfrac":{"msub":[{"mover":{"mi":"M","mo":"~"},"mrow":{"mn":["0","0"],"mo":","}},{"mi":"M","mrow":{"mn":["0","0"],"mo":","}}]}}}}}],"br":{},"sub":["p,q ","p,q ","step","0,0"]},{"@attributes":{"id":"p-0216","num":"0322"},"maths":[{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"bcx","mo":"=","mfrac":{"msub":[{"mi":"M","mrow":{"mn":["1","0"],"mo":","}},{"mi":"M","mrow":{"mn":["0","0"],"mo":","}}]}}}},{"@attributes":{"id":"MATH-US-00007-2","num":"00007.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"bcy","mo":"=","mfrac":{"msub":[{"mi":"M","mrow":{"mn":["0","1"],"mo":","}},{"mi":"M","mrow":{"mn":["0","0"],"mo":","}}]}}}}],"br":{}},{"@attributes":{"id":"p-0217","num":"0323"},"maths":[{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"cx","mo":"=","mfrac":{"msub":[{"mover":{"mi":"M","mo":"~"},"mrow":{"mn":["1","0"],"mo":","}},{"mi":"M","mrow":{"mn":["0","0"],"mo":","}}]}}}},{"@attributes":{"id":"MATH-US-00008-2","num":"00008.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"cy","mo":"=","mfrac":{"msub":[{"mover":{"mi":"M","mo":"~"},"mrow":{"mn":["0","1"],"mo":","}},{"mi":"M","mrow":{"mn":["0","0"],"mo":","}}]}}}}],"br":{}},"As an example, consider a \u201csliding window\u201d implementation wherein the classification of a sample at time t is made based on a set comprising w current and previous observations s:\n\n{}.\n\nThe number w will be called the \u201cwindow size.\u201d In an implementation, choice of a specific value for the window size w can be based on one or more factors such as sampling rate, average gesture duration, shortest gesture duration, longest allowed gesture duration, etc. The value for the window size w can selected by a priori design or experimentally.\n","To make the HDTP more responsive and limit computational complexity and real-time computational loading it can be desirable to keep the window smaller. Note that for example, in responding to transients in touch the system can include a delay of up to w observation times. Note that the classifications can be structured to include more responsive initial classifications at the beginning of a variation. Such an arrangement, however, can in many circumstances create erratic behavior. In a typical implementation it is reasonable to consider a window size value in the range of 10 to 30, depending on attributes of the sensor, sampling times, and aspects of the desired user experience.","In an example implementation, at every associated time step in the system the input to a \u201cclassifier\u201d (performing multiclass classification operations) is the concatenation xcomprising the w most recent signal vectors:\n\n}.\n\nAt each time step t this \u201csliding window\u201d concatenation of observations xis presented to a classifier.\n","In an example implementation, an Artificial Neural Network (\u201cANN\u201d) can be used as a classifier. Alternatively, a heuristic-based classifying implementation can be used. As another alternative, a genetic algorithm classifying implementation can be used. As another example, a classifying implementation can comprise two or more of an ANN, heuristic-based, and genetic algorithm aspects or components. Yet other approaches are possible and are provided for by the invention.","Example Use of Artificial Neural Network as a Classifier","Here an example implementation is presented employing an Artificial Neural Network (\u201cANN\u201d) as a classifier. As mentioned above, other approaches are possible and are provided for by the invention, and thus this illustrative is example is in no way limiting.","In one approach to an implementation, the input of the classifier at time step t is the \u201csliding window\u201d of observations x. The associated output label sequence produced by a classifier could be interpreted as a vector of probabilities for each label from set C.","In an example ANN implementation, a simple back-propagation ANN with two hidden layers can be used as a classifier (for example the FANN library [4]. Hidden layers can be implemented for example using a hyperbolic tangent (tan h) activation function. The ANN output layer can be implemented for example using a logistic activation function.","In an example ANN implementation, a separate output is associated with each of the labels in the set C. Use of logistic activation functions to produce the ANN outputs is advantageous as it produces outputs in [0,1] interval which is naturally applicable for probabilistic interpretation of the likelihood of each of these labels. All of the above can be readily implemented in other ways with a wide range of variation and these are provided for by the invention.","In one approach to an implementation, the ANN outputs are used to select a label by next applying accept and reject thresholds to each of the ANN outputs. For example, a \u201cOne-of-N with Confidence Thresholds\u201d [5] can be used wherein the label whose associated ANN output is maximal is chosen if (a) its probability is above the acceptance threshold and (b) all other label probabilities are below the rejection threshold. In an implementation, if no single label passes this composite \u201cOne-of-N with Confidence Thresholds\u201d test, a null or tracking label can be applied. Alternatively, a \u201cWinner-Takes-All\u201d test can be used in place of the \u201cOne-of-N with Confidence Thresholds\u201d test. Other types of tests can be used to select a best candidate outcome and these are provided for by the invention. All of the above can be readily implemented in other ways with a wide range of variation and these are provided for by the invention.","For training of such an ANN, a variation [6] of the Rprop learning algorithm can be used (although other training methods and ANN configuration approaches can also be used and are provided for by the invention). As an example training procedure, the ANN can be trained by providing recorded a datasets of representative users performing various representative renderings of selected gestures accompanied by manually transcribed indications of the intended gesture label for each frame in the recorded dataset. As an example cross-validation process for ANN training, the ANN can first be provided a first collection of recorded datasets for use in training and later be provided a second collection of recorded datasets for use in validation of ANN performance.","In a robust system design, provisions are included for the handling of situations where of the individual touch parameter signals (for example here, sway, surge, heave, yaw, roll, and pitch) cannot be reliably calculated. For example, in such a case, a special value outside the range of the permitted signal variation can be placed of the signal at that point. In some implementations should this special value be presented to the ANN, the ANN could be come confused, so provisions can be included to block such special values from being used as inputs to the ANN portion of the classifier.","Further as to provisions included for handling of situations where of the individual touch parameter signals cannot be reliably calculated, an implementation can be configured to distinguished two missing value cases:\n\n","Various implementations can be configured to act on these two missing-value cases in various ways as can be advantageous for desired performance. For example:\n\n","All the signals discussed thus far have a physical meaning and it is clear, intuitively, how they can contribute to gesture classification. Alternatively, other forms of moments, as well as some higher order moments, can also be used in gesture detection. Since it is difficult to guess which items of information will become useful, it is tempting to feed as much information as possible to the ANN classifier and let it decide (brute force approach). Unfortunately this brute force approach quickly is encumbered by limitations as feeding large numbers of inputs and irrelevant inputs to ANN makes it more difficult to converge on correct weights during training. Also, the overall number of ANN inputs have a significant impact on ANN size, ANN training time, ANN training CPU usage, and real-time CPU resource usage.","In an implementation, Principal Component Analysis (PCA) can be used to reduce the dimensionality of the information applied as inputs to the ANN. PCA performs a linear orthogonal projection of vector data into a lower-dimensional linear space, known as principal subspace, such that the variance of the projected data is maximized [7]. In an implementation, a PCA projection can be applied to a vector of signal and\/or moment information; for example in addition to those signals defined in s, additional moments can be added. Also since some signals and moments are linearly related to each other and PCA is a linear transformation, some signals defined in sand moments identified earlier can be excluded to a priori reduce the task of the PCA operation. In an implementation, some of the moment signals can be mean-centered, for example as in the central moments [8] defined as:",{"@attributes":{"id":"p-0233","num":"0349"},"maths":[{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mover":{"mi":"\u03bc","mo":"~"},"mrow":{"mi":["p","q"],"mo":","}},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"x","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"y","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"msup":[{"mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"-","mover":{"mi":["x","_"]}}},"mi":"p"},{"mrow":{"mo":["(",")"],"mrow":{"mi":"y","mo":"-","mover":{"mi":["y","_"]}}},"mi":"q"}],"mo":["\u2062","\u2062"],"mrow":{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}}}}}}},{"@attributes":{"id":"MATH-US-00009-2","num":"00009.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"\u03bc","mrow":{"mi":["p","q"],"mo":","}},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"x","mo":"=","mn":"1"},"mi":"M"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"y","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","mrow":{"msup":[{"mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"-","mover":{"mi":["x","_"]}}},"mi":"p"},{"mrow":{"mo":["(",")"],"mrow":{"mi":"y","mo":"-","mover":{"mi":["y","_"]}}},"mi":"q"}],"mo":["\u2062","\u2062"],"mrow":{"msub":{"mi":["I","step"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}}}}}}}}],"br":{}},"As one example, the signal set operated on by PCA could comprise:\n\n{,{tilde over (\u03bc)},{tilde over (\u03bc)},{tilde over (\u03bc)},{tilde over (\u03bc)},{tilde over (\u03bc)},{tilde over (\u03bc)},eigv1,eigv2,\u03c6,\u03c8,\u03b8}\n\nAs another example, the signal set operated on by PCA could alternatively comprise:\n\n{,{tilde over (\u03bc)},{tilde over (\u03bc)},{tilde over (\u03bc)},{tilde over (\u03bc)},{tilde over (\u03bc)},{tilde over (\u03bc)},eigv1,eigv2,\u03c6,\u03c8,\u03b8avgp,bcx,bcy,cx,cy}\n\nAlternatively, a shorter vector can be used.\n","The PCA operation produces a dimensionally-reduced vector comprising only principal components, each corresponding to dimension in a new vector space. In an implementation, the various principle components can be ordered by decreasing variance.","In an implementation, the dimensionally can be reduced yet further by omitting those principle components which have standard deviations significantly lower than the first component. In one approach, the threshold for omission can be determined manually based on high-level design. In another approach, the threshold for omission can be determined manually based on analysis of empirical data. In yet another approach, the threshold for omission can be","automatically determined after ANN training and measuring how choice of various threshold values affect miss rate.","Based on training data the PCA procedure calculates a vector of scaling factors P, a vector of offsets Pand transformation matrix P(a matrix of basis vectors, each column corresponding to an eigenvector). These are later used to convert the vector of signals corresponding to PCA inputs to a vector of principal components:\n\n=(())\n\nIn an implementation, the PCA operation is performed as a first step to produce inputs to the ANN, and the input and training of the ANN employs only PCA data (rather than the earliear-described vector of signals as ANN classifier input, using the same sliding window approach as described earlier, i.e., replacing vector swith vector of principal components, calculated based on s.\n\nExample Gesture Recognition and Gesture Parameter Value Smoothing Architecture\n","An example architecture for gesture recognition employing the techniques described and other aspects provided for by the invention is shown in . It is noted that variations of this and alternative implementations are possible, and these are anticipated by the invention. Accordingly, the arrangement depicted in  is not limiting.","In an example implementation, a PCA operation, trained ANN, Kalman filter, \u201cOne-of-N with Confidence Thresholds\u201d test, vector partition, vector merge, and two moving window vector \u201cshift register\u201d accumulators are arranged in a \u201cgesture recognition and gesture value smoothing module.\u201d For example, a vector of signals s() is provided by earlier signal extraction stages and serves as the input of the module . A PCA operation is applied to this vector of signals sand produces the svector (). A window of last sare retained for future reference in a \u201cdetection window\u201d. The content of this detection window is then concatenated into a vector () which is provided as the input to the ANN. The output of the ANN () is a vector of label probabilities. This output is interpreted by the label assigning module which, using a \u201cOne-of-N with Confidence Thresholds\u201d test, decides what label to assign to the current frame. This sequence of labels () provides a \u201csymbol\u201d (or label\u201d) output of the \u201cgesture recognition and gesture value smoothing module.\u201d","Example Gesture Recognition and Gesture Parameter Value Smoothing Architecture","Parallel to the \u201csymbol\u201d or \u201clabel\u201d data flow depicted in the upper portion of , the original signal inputs () can also be used to obtain smoothed numerical values responsive to the amount of variation of finger orientation. In an example embodiment, the original signal input is partitioned into two smaller vectors: a first vector of centroid signals () and a second vector of remaining signals from s. The centroid signals in the first vector is smoothed using a Kalman filter, resulting in a vector of smoothed centroid coordinates (). The remaining input signal values are smoothed using LOWESS [9] based on several previous remaining input signal vectors that are accumulated in the \u201csmoothing window.\u201d These smoothed remaining input signals are reunited back with the smoothed centroid signals to produce a composite vector () which serves as a smoothed version of s() and which is the \u201cnumerical values\u201d output of the \u201cgesture recognition and gesture value smoothing module.\u201d In a prototype implementation, the performance of this arrangement is sufficient to perform gesture recognition in real-time on a regular consumer-level PC at a frame rate of 100 frames per second with gesture recognition accuracy above 99%.","The terms \u201ccertain embodiments\u201d, \u201can embodiment\u201d, \u201cembodiment\u201d, \u201cembodiments\u201d, \u201cthe embodiment\u201d, \u201cthe embodiments\u201d, \u201cone or more embodiments\u201d, \u201csome embodiments\u201d, and \u201cone embodiment\u201d mean one or more (but not all) embodiments unless expressly specified otherwise. The terms \u201cincluding\u201d, \u201ccomprising\u201d, \u201chaving\u201d and variations thereof mean \u201cincluding but not limited to\u201d, unless expressly specified otherwise. The enumerated listing of items does not imply that any or all of the items are mutually exclusive, unless expressly specified otherwise. The terms \u201ca\u201d, \u201can\u201d and \u201cthe\u201d mean \u201cone or more\u201d, unless expressly specified otherwise.","While the invention has been described in detail with reference to disclosed embodiments, various modifications within the scope of the invention will be apparent to those of ordinary skill in this technological field. It is to be appreciated that features described with respect to one embodiment typically can be applied to other embodiments.","The invention can be embodied in other specific forms without departing from the spirit or essential characteristics thereof. The present embodiments are therefore to be considered in all respects as illustrative and not restrictive, the scope of the invention being indicated by the appended claims rather than by the foregoing description, and all changes which come within the meaning and range of equivalency of the claims are therefore intended to be embraced therein.","Although exemplary embodiments have been provided in detail, various changes, substitutions and alternations could be made thereto without departing from spirit and scope of the disclosed subject matter as defined by the appended claims. Variations described for the embodiments may be realized in any combination desirable for each particular application. Thus particular limitations and embodiment enhancements described herein, which may have particular advantages to a particular application, need not be used for all applications. Also, not all limitations need be implemented in methods, systems, and apparatuses including one or more concepts described with relation to the provided embodiments. Therefore, the invention properly is to be construed with reference to the claims.\n\n"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The above and other aspects, features and advantages of the present invention will become more apparent upon consideration of the following description of preferred embodiments taken in conjunction with the accompanying drawing figures.",{"@attributes":{"id":"p-0020","num":"0026"},"figref":"FIGS. 1","i":["a","g "],"b":"1"},{"@attributes":{"id":"p-0021","num":"0027"},"figref":["FIGS. 2","FIGS. 3"],"i":["a","e ","a","b "],"b":["2","3"]},{"@attributes":{"id":"p-0022","num":"0028"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0023","num":"0029"},"figref":["FIG. 5","FIG. 5"],"i":["a ","b "]},{"@attributes":{"id":"p-0024","num":"0030"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0025","num":"0031"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0026","num":"0032"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0027","num":"0033"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0028","num":"0034"},"figref":"FIGS. 10","i":["a","c "],"b":"10"},{"@attributes":{"id":"p-0029","num":"0035"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0030","num":"0036"},"figref":"FIGS. 12","i":["a","b "],"b":"12"},{"@attributes":{"id":"p-0031","num":"0037"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0032","num":"0038"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0033","num":"0039"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0034","num":"0040"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0035","num":"0041"},"figref":"FIGS. 17","i":["a","f "],"b":"17"},{"@attributes":{"id":"p-0036","num":"0042"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0037","num":"0043"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0038","num":"0044"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0039","num":"0045"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0040","num":"0046"},"figref":"FIGS. 22","i":["a","c "],"b":"22"},{"@attributes":{"id":"p-0041","num":"0047"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0042","num":"0048"},"figref":["FIG. 24","FIG. 24"],"i":["a ","b "]},{"@attributes":{"id":"p-0043","num":"0049"},"figref":["FIG. 25","FIGS. 17"],"i":["a","f"],"b":"17"},{"@attributes":{"id":"p-0044","num":"0050"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0045","num":"0051"},"figref":"FIGS. 27","i":["a","d "],"b":"27"},{"@attributes":{"id":"p-0046","num":"0052"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0047","num":"0053"},"figref":"FIGS. 29","i":["a","c "],"b":"29"},{"@attributes":{"id":"p-0048","num":"0054"},"figref":["FIG. 30","FIG. 30","FIG. 30"],"i":["a ","b ","c "]},{"@attributes":{"id":"p-0049","num":"0055"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0050","num":"0056"},"figref":["FIG. 32","FIGS. 32"],"i":["a ","b","f "],"b":"32"},{"@attributes":{"id":"p-0051","num":"0057"},"figref":["FIGS. 33","FIGS. 32"],"i":["a","e ","b","f. "],"b":["33","32"]},{"@attributes":{"id":"p-0052","num":"0058"},"figref":["FIG. 34","FIGS. 34"],"i":["a ","b","f "],"b":"34"},{"@attributes":{"id":"p-0053","num":"0059"},"figref":"FIG. 35"},{"@attributes":{"id":"p-0054","num":"0060"},"figref":"FIG. 36"},{"@attributes":{"id":"p-0055","num":"0061"},"figref":"FIG. 37"},{"@attributes":{"id":"p-0056","num":"0062"},"figref":"FIG. 38"},{"@attributes":{"id":"p-0057","num":"0063"},"figref":"FIG. 39","i":"a "},{"@attributes":{"id":"p-0058","num":"0064"},"figref":"FIG. 39","i":"b "},{"@attributes":{"id":"p-0059","num":"0065"},"figref":"FIG. 40","i":"a ","b":["1","2","1","2"]},{"@attributes":{"id":"p-0060","num":"0066"},"figref":"FIG. 40","i":"b "},{"@attributes":{"id":"p-0061","num":"0067"},"figref":"FIG. 40","i":"c "},{"@attributes":{"id":"p-0062","num":"0068"},"figref":["FIG. 41","FIG. 31","FIG. 38"],"i":"a "},{"@attributes":{"id":"p-0063","num":"0069"},"figref":["FIG. 41","FIG. 41"],"i":["b ","b "]},{"@attributes":{"id":"p-0064","num":"0070"},"figref":["FIG. 42","FIG. 42","FIG. 41","FIG. 41"],"i":["a ","b ","a ","b "]},{"@attributes":{"id":"p-0065","num":"0071"},"figref":"FIG. 43"}]},"DETDESC":[{},{}]}
