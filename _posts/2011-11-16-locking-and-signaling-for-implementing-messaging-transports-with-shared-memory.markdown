---
title: Locking and signaling for implementing messaging transports with shared memory
abstract: Disclosed are systems and methods for transporting data using shared memory comprising allocating, by one of a plurality of sender application, one or more pages, wherein the one or more pages are stored in a shared memory, wherein the shared memory is partitioned into one or more pages, and writing data, by the sender application, to the allocated one or more pages, wherein a page is either available for use or allocated to the sender applications, wherein the one or more pages become available after the sender application has completed writing the data. The systems and methods further disclose sending a signal, by the sender application, to a receiver application, wherein the signal notifies the receiver application that writing the data to a particular page is complete, reading, by the receiver application, the data from the one or more pages, and de-allocating, by the receiver application, the one or more pages.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08898399&OS=08898399&RS=08898399
owner: TIBCO Software Inc.
number: 08898399
owner_city: Palo Alto
owner_country: US
publication_date: 20111116
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application relates and claims priority to U.S. Provisional Patent Application No. 61\/414,331 entitled \u201cLocking and signaling for implementing messaging transports with shared memory,\u201d filed on Nov. 16, 2010, which is hereby incorporated by reference in its entirety.","The disclosed embodiments relate generally to data transports through the use of shared memory and, more specifically, relate to substantially lossless allocation of pages of the shared memory of messaging for many senders to one receiver and proactive flow control of the shared memory of messaging for one sender to many receivers.","Typical servers and communication systems do not use shared memory for many different applications or processes. Shared memory is typically used with one application writing to memory segments and another application reading the memory segments. The need has arisen to provide a method and system for using shared memory to implement highly efficient data transports in both the scenario of many senders to one receiver and one sender to many receivers.","Disclosed are systems and methods for transporting data using shared memory comprising allocating, by one of a plurality of sender application, one or more pages, wherein the one or more pages are stored in a shared memory, wherein the shared memory is partitioned into one or more pages, and writing data, by the sender application, to the allocated one or more pages, wherein a page is either available for use or allocated to the sender applications, wherein the one or more pages become available after the sender application has completed writing the data. The systems and methods further disclose sending a signal, by the sender application, to a receiver application, wherein the signal notifies the receiver application that writing the data to a particular page is complete, reading, by the receiver application, the data from the one or more pages, and de-allocating, by the receiver application, the one or more pages. Senders and receivers residing on a single host can implement highly efficient data transports through the use of shared memory. Disclosed also are systems and methods for flow control for many receiver applications and one sender application using shared memory in a multicast scenario.",{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 1","FIG. 1"],"b":["100","101","103","107","100","109","109","101","107","103","111","101","103","109","109","101","103","107","101","103","121","131","101","103","123","133","101","103","107"]},"In an embodiment, the host machine  provides a shared memory engine (not shown). A host machine processor  may execute the shared memory engine's routines and the shared memory element  may store and provide data structures, variable values, etc. as the shared memory engine operates. The shared memory engine is operable to assist the sender application  and the receiver application  in utilizing shared memory  for data transports. Thus, the shared memory  may be operable to assist the sender application  and the receiver application  with partitioning shared memory, page allocation and de-allocation, page locking and unlocking, writing data to a page, signaling each other, determining whether space is available in the shared memory , performing fault and failure prevention schemes, determining receiver interest, etc.","In another embodiment, sender application  and receiver application  provide routines and some processing to utilize shared memory  for data transports. Thus, the applications ,  may also be operable to partition shared memory, allocate and de-allocate pages, lock and unlock pages, write data to a page, signal each other, determine whether space is available in the shared memory , perform fault and failure prevention schemes, determine receiver interest, interact host machine , etc.","In the disclosed embodiments, sender application  may write data to memory segments (at shared memory ) that are shared with receiver applications  and signal the receiver applications  when a write is complete. Receiver applications , in turn, read data from these shared memory  segments when they are signaled.","The patterns of sender applications  and receiver applications  relevant to this invention can be broken down into two general cases. A first includes many sender applications  and one receiver application  (i.e., \u201cmany-to-one\u201d). A second case includes one sender application  and many receiver applications  (i.e., \u201cone-to-many\u201d). In some embodiments, sender application  may write data to memory segments in shared memory  that are shared with receiver applications , and receiver applications  may read data from these shared memory segments in shared memory  when the receiver applications  are signaled by the sender application .","While illustrated as a single sender machine  in , system  may comprise more than one sender application . Although described as sender application  in , sender application  may receive messages and\/or read data from shared memory  in some embodiments. While illustrated as a single receiver application  in , system  may comprise more than one receiver application . Although described as receiver application  in , receiver application  may send messages and write data to shared memory  in some embodiments.","System  may comprise middleware server , which may be any suitable computing device comprising a processor and a memory to perform the described functionality. Middleware server  may comprise one or more machines, workstations, laptops, blade servers, server farms, and\/or stand-alone servers. Middleware server  may include any hardware and\/or controlling logic used to communicate information to and from one or more elements illustrated in . For example, middleware server  may be operable to receive and process data of different types that may be transmitted via different protocols or formats. Other elements in  may also comprise hardware and\/or controlling logic to communicate information to and from one or more elements illustrated in . Shared memory  may store any suitable information. Shared memory  may comprise any collection and arrangement of volatile and\/or non-volatile components suitable for storing data. For example, shared memory  may comprise random access memory (RAM) devices. In particular embodiments, shared memory  may represent, in part, computer-readable storage media on which computer instructions and\/or logic are encoded. Shared memory  may represent any number of memory components within, local to, and\/or accessible by processor. Processor  may represent and\/or include any form of processing component, including general purpose computers, dedicated microprocessors, or other processing devices capable of processing electronic information. Examples of processor  include digital signal processors (DSPs), application-specific integrated circuits (ASICs), field-programmable gate arrays (FPGAs), and any other suitable specific or general purpose processors.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 2","b":["200","202","204"]},"Lossless Allocation of Panes for Many-to-One Messaging",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 3","FIGS. 10A and 10B"],"b":["300","300","301","303","307","300","309","311","301","311"]},"This process  also allows for senders to allocate multiple pages at action  when their data cannot fit into a single page. Senders may also make multiple writes at action  over time, signaling the receiver at action  after each write is completed.","In an embodiment, the process  is executed such that pages are allocated and de-allocated in a substantially lossless fashion when the following conditions are met: (1) a page is either on the free list or allocated to one sender at a time, and (2) pages are returned to the free list upon completion of their use. In an embodiment, these conditions may be met whether or not the sending process exits naturally or unexpectedly and without the need for an expensive \u201cgarbage collection\u201d process to periodically scan the shared memory. In some embodiments, it is assumed that a receiver process is substantially faultless, or, should it exit unexpectedly, the entire shared memory segment is de-allocated. Thus, in these embodiments, the processes are resilient to sender faults.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 4","b":["400","400","404","406","401","403","401","401","403"]},"Because the shared memory segment may be located at a different virtual memory address in each of the mounting processes, pointer first free  and pointer last free  are offsets into an array rather than virtual memory addresses. Further, the system may keep track of each iteration that pointer first free  makes through the circular buffer free list . Thus, the value of pointer first free  is a monotonically increasing integer whose raw value is taken modulo the length of the free list  to determine the actual offset.","As discussed above, the circular buffer free list  contains links to free pages, e.g., pages  and , in a shared memory segment. Each free page  and  includes a next field  pointing to a slot in the circular buffer free list  from which it is linked. The actual value used is the raw (un-modulo) value of pointer first free  when it will next point to the cell. In an example embodiment, if free list  has a size of 100 and the first free page value is 205, meaning that it points at slot  and this is the third time through this buffer (204 free pages have been consumed from the free list), then the second page on the free list would have next field having value of 206. This value of 206 would then be what the value of first free page would be when it is adjusted to point to slot  after the preceding page is removed from the list. Referring back to , by using the raw values, the system may detect a page that has been allocated and de-allocated, even if it falls into the same pointer first free  slot each time.","In addition to the global circular buffer free list , in an embodiment, each sender application may maintain a linked list of pages it has been allocated.",{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 5","b":["500","500","501","502","503","501","502","503","505","500","503","505","503","502","509","500","501","502","503"]},"In an embodiment, senders add pages to the end of their linked lists , and a receiver de-allocates pages from the front of the senders' linked lists as it completes reading them and begins reading the next page. Allocation and de-allocation is discussed in further detail below.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 6","b":["600","601","603","605","607","605","607","603","605"]},"Failure at any step in the process  may disturb the system and, thus, processes are used to minimize the risk of failure at each step. Should a failure occur, the processes would leave the system in a consistent or recoverable state, such that a page is either left on (or returned to) a circular buffer free list or allocated to exactly one sender. Multiple senders may be simultaneously attempting to allocate a page from the circular buffer free list, and any one page should be allocated to one sender at a time.","To accomplish only allocating one page to one sender at a time, referring back to , the next field  of the sender's last page  in the sender's linked list  is set to point at the free page being allocated (not shown). Referring back to , the next field of the free page being allocated is set to point at the sender's last page in the sender's linked list. Then the pointer first free is advanced and the sender may write data into the page at action . The sender's available count is incremented and the receiver is signaled at action .","Multiple senders may attempt to allocate the same free page. This is resolved by using an atomic compare and swap instruction. As discussed above and referring back to , each free page ,  has its next field  set to point logically back into the free list . As discussed, the pointer first free  is a monotonically increasing integer whose value is reduced modulo the length of the free list to create an offset into the list. The combination of these two techniques substantially prevents free pages from accidentally being allocated from the middle of the circular buffer free list . This may accidentally happen if a first sender reads the value of pointer first free  and then sleeps before completing a compare-and-swap process. A second sender may successfully allocate the page, write to it, and signal the receiver. The receiver may read the data and de-allocate the page, linking it to the end of the circular buffer free list . When the first sender wakes up it will find that the value in its free page's next field no longer matches the pointer first free  value it read before sleeping. Even if the de-allocated page happened to be linked from the same cell in the circular buffer free list, the monotonically increasing value of pointer first free  can distinguish between iterative uses of the same cell.","Using a compare-and-swap thus results in one sender at a time being able to set the free page's next field. This winning sender will know that it has been allocated the page, and losing senders will know that they failed and can then restart the allocation sequence.","The winning sender may also fail after the next field of the free page being allocated is set to point at the sender's last page in the sender's linked list. In this case the page has already been successfully allocated, but the pointer first free has not been advanced. This may be resolved by noting that pages on the circular buffer free list have next fields indicating whether or not they are free. If the next fields point to the free list, then they are free; if they point to another page, they are allocated, and therefore not free. Thus, any sender initiating the allocation sequence can check the free page that pointer first free points to and determine whether it has already been allocated. If the page has already been allocated, this sender may advance the pointer first free.",{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 7","b":["700","701","703","705","707","709","711","701","711","713","715","717","719"]},"Thus, pointer first free does not fail in the case that an allocating sender fails before completing action .","In an embodiment, a receiver may correctly reset a page's next field to point at the circular buffer free list before the page is returned to the circular buffer free list.","Actions  and  in process  advance pointer first free. Given that multiple senders may be executing this sequence in parallel, the process may also ensure that the pointer first free is not advanced multiple times when only one advance is called for. In particular, should a sender decide in action  that it should advance the pointer first free, it should ensure that another sender making the same decision in parallel has not already advanced the pointer. When a sender is ready to advance the pointer in action , it should ensure that another sender that is at action  has not advanced the pointer first. To accomplish this, the sender may use compare-and-swap so that it only advances the pointer first free when the value remains consistent from checking its value to updating its value. If the compare-and-swap fails while executing action , the sender repeats decision . Should the compare-and-swap fail while executing action , the sender continues without incrementing the pointer first free.","In an embodiment, before advancing pointer first free, the sender may determine whether the next page is free or not free. If not, the circular buffer free list is currently empty (that is, there are no free pages to be allocated), and the sender should leave pointer first free as-is. Should a page eventually be de-allocated by a receiver, the pointer first free will subsequently be advanced by the next sender attempting to allocate a page.","Because the free list is a circular buffer, it is possible pointer first free advances all the way through the list to its starting point. If pointer first free is a simple offset into the circular buffer free list, this circular advancement may go unnoticed, and a sender at action  or  might advance the pointer first free when it has already been advanced by others. As noted above, pointer first free is stored as a monotonically increasing integer for purposes of compare-and-swap, and its value is reduced modulo the length of the free list to create an offset into the circular buffer free list itself. This allows a sender to notice if pointer first free has changed between the time its value was recorded and the time the compare-and-swap is executed, even if pointer first free has circled the entire list.","As described above, each sender maintains a linked list of the pages that have been allocated for its use.  is a schematic diagram illustrating a sender's linked list  of allocated pages , , . A sender's linked list available count  specifies the number of bytes that are available for reading. In this case, assume each page holds 100 bytes and the available count  of 230 indicates that pages 1 and 2 are full, and that page 3 contains 30 bytes.","Each page is de-allocated as the receiver finishes reading its data, returning the page to the slot after the pointer last free on the circular buffer free list.  is a flow diagram illustrating a page de-allocation . The pointer last free is advanced at action , the next field of the de-allocated page is set to point to the pointer last free at action  (the pointer last free is also a monotonically increasing integer and is handled substantially identical to the first free pointer in that regard), and the last free slot in the circular buffer free list is set to point to the de-allocated page at action  (as discussed above, the raw value of the pointer last free is reduced modulo the length of the circular buffer free list to find the actual offset into the list).","Referring back to , reading and de-allocating continues until the last page  is reached. The last page  is recognized via the available count  indicating that the written data does not stretch past the end of the page.",{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 16A"},"Referring back to , the receiver may read the bytes on the last remaining page , keeping track of the number of bytes read so that an additional read, prompted by an additional write from the sender, will start at the appropriate offset into the page. Once the final page  is read, the receiver may leave the last page  in place if the sender is still active or may de-allocate the last page  if the sender has exited. In an embodiment, the receiver is notified out-of-band when a sender exits.","Regarding de-allocating the last page  if the sender has exited, in an embodiment in which the sender is known to have exited, the receiver handles situations in which the sender exited at various points in the allocation process. As discussed above in relation to , senders allocate pages in accordance with process .","At action , a page is considered allocated if the compare-and-swap is successful. And at action , the data on the page is considered live for receiver consumption. If the sender exits before completing action , the page is not considered allocated to this sender, and thus of no concern to the receiver in processing this sender's pages. If the sender exits after action  is completed, the page is considered allocated and the available count signals to the receiver that the remaining page is actually the last page. This page may be de-allocated. If the sender exits after completing action , but before completing action , then the page will be allocated to the sender, but the available count will not correctly signal this to the receiver.",{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 9B","b":["950","951","951","952"]},{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 16B"},"Referring back to , if a sender failed after decision , then a page will have been allocated from the circular buffer free list, and this page will have been linked into the sender's list, as in . If the sender failed before action  or after action , then no additional page needs to be de-allocated by receiver. In either case, the receiver correctly returns all of the sender's pages back to the circular buffer free list.","As discussed above, each sender notifies a receiver when it has successfully written additional data to its pages. The mechanism for notification might be implemented as an operating system socket, semaphore, or via polling of a sender's available count by the receiver. The former can be very expensive, particularly when there are a large number of senders; the latter uses careful balance between the polling period and the overhead it creates. Further, in implementing low-latency messaging, the signals should be received in-line for maximum performance.","Thus, an alternative is to create a linked list of nodes representing senders that have content (that is, a non-zero available count), and have each sender add itself to the list as it successfully writes data, using a similar compare-and-swap tie-breaking strategy to ensure the correctness of the list.",{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 10A","b":["1000","1000","1001","1","1000","1002","1000"]},{"@attributes":{"id":"p-0070","num":"0069"},"figref":"FIG. 17","b":["2","2","1","2"]},{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 10B","b":["1050","1051","2","1000"]},"Senders add themselves to the list when their writing of data causes them to increment their available count from zero to non-zero (when the available count is already >0, the sender is already represented on the list and the receiver will automatically read additional data). When a sender detects that it is inserting the first page in the linked lists  and  (i.e., by detecting that its page's next pointer is NIL), it may wake the receiver upon completion of the insertion.","Proactive Flow Control for One-to-Many Messaging",{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 11","b":["1100","1102","1104","1106"]},"The methods and systems disclosed herein implement sender-based flow control. In an embodiment, should the shared memory segment become full (that is, contain data in each page that has not been read by every registered receiver), the sender is paused until memory becomes available so that the sender does not write over data being read by a receiver. The methods and systems correctly survive receiver failures. In an embodiment, in the case of a sender failure, the shared memory segment is scrapped. The methods and systems disclosed herein are implemented without use of in- or out-of-band ACKs or NACKs, or a separate communication back-channel.",{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 12","b":["1200","1202","1202","1206","1206","1204","1208","1202","1202"]},"A global variable, e.g., global write page, tracks the current page to which the sender is writing. The value of global variable may be stored in the host machine and is accessible to both the sender and receivers. An array of values (e.g., current read page array) tracks pages that the receivers are currently reading.","In an embodiment, receivers explicitly signal their interest to the sender through a receiver registration process. The sender assigns the receiver an ID that correlates to a specific bit position in the bit arrays. In an embodiment, a receiver is assigned the lowest order bit that is currently unassigned.",{"@attributes":{"id":"p-0079","num":"0078"},"figref":"FIG. 13","b":["1300","1302","1304","1306"]},{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIG. 14","FIG. 12"],"b":["1400","1402","1204","1206","1202","1406","1404"]},"Referring back to , receivers begin reading at page  and at an offset provided by the sender upon initial registration. When a receiver has read all of the bytes indicated as having been written (e.g., by the page header 's byte count ), the receiver decides whether to wait for the sender to write more to this page or to advance to the next page. This decision is guided by whether the global variable, e.g., global write page, has advanced to a subsequent page. If the global variable has advanced from the current page, the receiver will advance to the next page.",{"@attributes":{"id":"p-0082","num":"0081"},"figref":"FIG. 15"},"Should a receiver fail, its page locks should be cleared. When a receiver failure is detected by the sender, the sender may clean up the receiver's state by unlocking the receiver's current read page array and the following page. The locking protocol discussed above in  results in a receiver not locking more than two consecutive pages, whereby the first locked page is the page referenced by the receiver's current read page array.","In an embodiment, the bit arrays (e.g.,  in ) are actually read and written as bytes or words, rather than as individual bits. With the sender and multiple receivers each writing to the bit arrays, writes use compare-and-swap to allow one of multiple contending writers to write at a given time. This prevents one writer from incorrectly setting or resetting another writer's bit value.","For example, for a bit array in which bits  and  are set to 10100000, if a first receiver (e.g., receiver) and a third receiver (receiver) each attempt to clear their bit, they may incorrectly set the bit of their competitor.",{"@attributes":{"id":"p-0086","num":"0085"},"figref":"FIG. 18","b":["3","1"]},"Receiver and receiver act in parallel. Both receivers may read 10100000 and both receivers would like to unlock the page by clearing their bit. Without compare-and-swap, receiver would want to change 10100000 into 00100000 and receiver would want to change 10100000 into 10000000. If receiver writes back to shared memory first and receiver writes second, the resulting value is 10000000. This value would be incorrect in that it signifies that receiver still has the page locked. With compare-and-swap, if receiver writes first and receiver writes second, receiver will state that it wants to change the contents of shared memory from 10100000 to 10000000 and will get a failure, since by then the content of the shared memory is 00100000 (after receiver's change). At this point receiver will re-read, obtain 00100000, and will use compare-and-swap to change shared memory contents to 00100000.","Thus, when using a compare-and-swap, the third receiver's attempt would fail because it holds a stale value that fails the compare. In such a case, a receiver re-reads the value and reattempts the compare-and-swap to set or clear the appropriate bit.","While various embodiments have been described above, it should be understood that they have been presented by way of example only, and not limitation. Thus, the breadth and scope of a preferred embodiment should not be limited by any of the above described exemplary embodiments, but should be defined only in accordance with the claims and their equivalents for any patent that issues claiming priority from the present provisional patent application.","For example, as referred to herein, a machine or engine may be a virtual machine, computer, node, instance, host, or machine in a networked computing environment. Also as referred to herein, a networked computing environment is a collection of machines connected by communication channels that facilitate communications between machines and allow for machines to share resources. Also as referred to herein, a server is a machine deployed to execute a program operating as a socket listener and may include software instances.","Resources may encompass any types of resources for running instances including hardware (such as servers, clients, mainframe computers, networks, network storage, data sources, memory, central processing unit time, scientific instruments, and other computing devices), as well as software, software licenses, available network services, and other non-hardware resources, or a combination thereof.","A networked computing environment may include, but is not limited to, computing grid systems, distributed computing environments, cloud computing environment, etc. Such networked computing environments include hardware and software infrastructures configured to form a virtual organization comprised of multiple resources which may be in geographically disperse locations.","While communication protocols may be described herein, the coverage of the present application and any patents issuing there from may extend to other local-area network, wide-area network, or other network operating using other communications protocols.","Services and applications are described in this application using those alternative terms. Services can be java services or other instances of operating code. A service\/application is a program running on a machine or a cluster of machines in a networked computing environment. Services may be transportable and may be run on multiple machines and\/or migrated from one machine to another.","Various terms used herein have special meanings within the present technical field. Whether a particular term should be construed as such a \u201cterm of art,\u201d depends on the context in which that term is used. \u201cConnected to,\u201d \u201cin communication with,\u201d or other similar terms should generally be construed broadly to include situations both where communications and connections are direct between referenced elements or through one or more intermediaries between the referenced elements, including through the Internet or some other communicating network. \u201cNetwork,\u201d \u201csystem,\u201d \u201cenvironment,\u201d and other similar terms generally refer to networked computing systems that embody one or more aspects of the present disclosure. These and other terms are to be construed in light of the context in which they are used in the present disclosure and as those terms would be understood by one of ordinary skill in the art would understand those terms in the disclosed context. The above definitions are not exclusive of other meanings that might be imparted to those terms based on the disclosed context.","Words of comparison, measurement, and timing such as \u201cat the time,\u201d \u201cequivalent,\u201d \u201cduring,\u201d \u201ccomplete,\u201d and the like should be understood to mean \u201csubstantially at the time,\u201d \u201csubstantially equivalent,\u201d \u201csubstantially during,\u201d \u201csubstantially complete,\u201d etc., where \u201csubstantially\u201d means that such comparisons, measurements, and timings are practicable to accomplish the implicitly or expressly stated desired result.","Additionally, the section headings herein are provided for consistency with the suggestions under 37 CFR 1.77 or otherwise to provide organizational cues. These headings shall not limit or characterize the invention(s) set out in any claims that may issue from this disclosure. Specifically and by way of example, although the headings refer to a \u201cTechnical Field,\u201d such claims should not be limited by the language chosen under this heading to describe the so-called technical field. Further, a description of a technology in the \u201cBackground\u201d is not to be construed as an admission that technology is prior art to any invention(s) in this disclosure. Neither is the \u201cBrief Summary\u201d to be considered as a characterization of the invention(s) set forth in issued claims. Furthermore, any reference in this disclosure to \u201cinvention\u201d in the singular should not be used to argue that there is only a single point of novelty in this disclosure. Multiple inventions may be set forth according to the limitations of the multiple claims issuing from this disclosure, and such claims accordingly define the invention(s), and their equivalents, that are protected thereby. In all instances, the scope of such claims shall be considered on their own merits in light of this disclosure, but should not be constrained by the headings set forth herein."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 9A"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 9B"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 10A"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 10B"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 16A"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 16B"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 18"}]},"DETDESC":[{},{}]}
