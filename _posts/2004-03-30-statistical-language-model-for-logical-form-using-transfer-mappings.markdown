---
title: Statistical language model for logical form using transfer mappings
abstract: A method of decoding an input semantic structure to generate an output semantic structure. A set of transfer mappings are provided. A score is calculated for at least one transfer mapping in the set of transfer mappings using a statistical model. At least one transfer mapping is selected based on the score and used to construct the output semantic structure.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07593843&OS=07593843&RS=07593843
owner: Microsoft Corporation
number: 07593843
owner_city: Redmond
owner_country: US
publication_date: 20040330
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS"],"p":["The present invention relates to automated language systems. More specifically, the present invention relates to language models in statistical language systems.","Automated language systems include speech recognition, handwriting recognition, speech production, grammar checking and machine translation.","Machine translation (MT) systems are systems that receive an input in one language (a \u201csource\u201d language), translate the input to a second language (a \u201ctarget\u201d language), and provide an output in the second language.","One example of a MT system uses logical forms (LFs), which are dependency graphs that describe labeled dependencies among content words in a string as an intermediate step in translation. Under this system, a string in the source language is first analyzed with a natural language parser to produce a source LF. The source LF must then be converted into a target language LF. A database of mappings from source language LF pieces to target language LF pieces (along with other metadata, such as sizes of mappings and frequencies of mappings in some training sets) is used for this conversion. All mappings whose source language LF pieces are a sub-graph of the source LF are first retrieved. Typically, the source language LF piece of a single mapping does not cover the entire source LF. As a result, a set of mappings (possibly overlapping) must be selected and their target language LF pieces must be combined to form a complete target LF.","To identify the set of target logical forms, an MT system uses a greedy search algorithm to select a combination of mappings from the possible mappings whose source language LF pieces match the source LF. This greedy search begins by sorting the mappings by size, frequency, and other features that measure how well the source language LF pieces of the mapping match the source LF. The sorted list is then traversed in a top-down manner and the first set of compatible mappings found that covers the source logical form is chosen. This heuristic system, however, does not test all possible combinations of input mappings, but simply selects the first set of mappings that completely cover the source LF.","After the set of mappings is selected, the target language LF pieces of the mappings are combined in a manner consistent with the source LF to produce a target LF. Finally, running a natural language generation system on the target LF produces the target language output.","However, MT systems do not always employ logical forms or other parsed structures as intermediate representations. Nor do they necessarily use heuristic methods to resolve translation ambiguities. Some other MT systems try to predict the most likely target language string given an input string in the source language using statistical models. Such MT systems use traditional statistical frameworks and models, such as the noisy-channel framework, to decode and find the target sentence T that is the most probable translation for a given source sentence S. Maximizing this probability is represented by:",{"@attributes":{"id":"p-0009","num":"0008"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"T","mo":"=","mrow":{"munder":{"mrow":{"mi":["arg","max"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}},"msup":{"mi":["T","\u2032"]}},"mo":"\u2062","mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msup":{"mi":["T","\u2032"]},"mo":"\u2758","mi":"S"}}}}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"1"}}]}}}},"br":{}},{"@attributes":{"id":"p-0010","num":"0009"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"T","mo":"=","mrow":{"munder":{"mrow":{"mi":["arg","max"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}},"msup":{"mi":["T","\u2032"]}},"mo":["\u2062","\u00d7"],"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":"\u2758","msup":{"mi":["T","\u2032"]}}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["T","\u2032"]}}}]}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"2"}}]}}}},"br":{}},"There are a number of problems associated with conventional, string-based statistical MT systems. In particular, the search space (all possible strings in the target language) is quite large. Without restricting this search space, a practical MT system cannot be built because it takes too long to consider all possible translation strings. To address this, many systems use a simplifying assumption that the probabilities of the channel model and the target language model for an entire string can be determined as the product of probabilities of sub-strings within the string. This assumption is only valid as long as the dependencies in the strings and between the strings are limited to the local areas defined by the sub-strings. However, sometimes the best translation for a chunk of source language text is conditioned on elements of the source and target language strings that are relatively far away from the element to be predicted. Since the simplifying assumptions made in string-based statistical MT models are based in large part on string locality, sometimes the conditioning elements are far enough from the element to be predicted that they cannot be taken into account by the models.","For example, some string-based statistical MT systems use string n-gram models for their language model (LM). These n-gram models are simple to train, use and optimize. However, n-gram models have some limitations. Although a word can be accurately predicted from one or two of its immediate predecessors, a number of linguistic constructions place highly predictive words sufficiently far from the words they predict that they are excluded from the scope of the string n-gram model. Consider the following active and passive sentences:","1. John hit the ball.","2. The balls were hit by Lucy.","The following trigrams occur in these sentences with the indicated frequencies:",{"@attributes":{"id":"p-0015","num":"0014"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"<P>\u2003<P>\u2003John 1","<P >\u2003<P>\u2003The 1"]},{"entry":[{},"<P>\u2003John hit 1","<P >The balls 1"]},{"entry":[{},"John hit the 1","the balls were 1"]},{"entry":[{},"hit the ball 1","balls were hit 1"]},{"entry":[{},"the ball <POST>\u20031","were hit by 1"]},{"entry":[{},{},"hit by Lucy 1"]},{"entry":[{},{},"by Lucy <POST>\u20031"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}},"br":{}},"In another statistical MT system, a syntax structure in the source language is mapped to a string in the target language. Syntax-based models have several advantages over string-based models. In one aspect, syntax-based models can reduce the magnitude of the sparse data problem by normalizing lemmas. In another aspect, syntax-based models can take the syntactic structure of the language into account. Therefore, events that depend on each other are often closer together in a syntax tree than they are in the surface string because the distance to a common parent can be shorter than the distance in the string.","However, even in a syntax-based model, drawbacks remain: the distance between interdependent words can still be too large to be captured by a local model; also, similar concepts are expressed by different structures (e.g., active vs. passive voice) and are, therefore, not modeled together. These result in poor training of the model and poor translation performance.","The present invention includes a method for decoding an input semantic structure to produce an output semantic structure. The technique employs a set of transfer mappings for portions of the input semantic structure. A score is calculated for at least one transfer mapping in the set of transfer mappings using a statistical model. At least one transfer mapping is selected based on the score and used to construct the output semantic structure. The present invention can also be embodied as a computer-implemented method and as a machine translation system. A further aspect of the present invention is a language model constructed from semantic structures.","It should be noted that to the extent that the present invention is described in the context of machine translation systems the present invention is also applicable to other systems that produce words or that require a language model. For example, systems can include speech recognition, optical character recognition (OCR), grammar checking and etc. Prior to describing the present invention in detail, embodiments of illustrative computing environments within which the present invention can be applied will be described.",{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 1","b":["100","100","100","100"]},"The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well-known computing systems, environments, and\/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, telephony systems, distributed computing environments that include any of the above systems or devices, and the like.","The invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The invention is designed to be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules are located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing the invention includes a general-purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit. System bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way of example, and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard , a microphone , and a pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  is operated in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 2","b":["200","200","202","204","206","208","210"]},"Memory  is implemented as non-volatile electronic memory such as random access memory (RAM) with a battery back-up module (not shown) such that information stored in memory  is not lost when the general power to mobile device  is shut down. A portion of memory  is preferably allocated as addressable memory for program execution, while another portion of memory  is preferably used for storage, such as to simulate storage on a disk drive.","Memory  includes an operating system , application programs  as well as an object store . During operation, operating system  is preferably executed by processor  from memory . Operating system , in one preferred embodiment, is a WINDOWS\u00ae CE brand operating system commercially available from Microsoft Corporation. Operating system  is preferably designed for mobile devices, and implements database features that can be utilized by applications  through a set of exposed application programming interfaces and methods. The objects in object store  are maintained by applications  and operating system , at least partially in response to calls to the exposed application programming interfaces and methods.","Communication interface  represents numerous devices and technologies that allow mobile device  to send and receive information. The devices include wired and wireless modems, satellite receivers and broadcast tuners to name a few. Mobile device  can also be directly connected to a computer to exchange data therewith. In such cases, communication interface  can be an infrared transceiver or a serial or parallel communication connection, all of which are capable of transmitting streaming information.","Input\/output components  include a variety of input devices such as a touch-sensitive screen, buttons, rollers, and a microphone as well as a variety of output devices including an audio generator, a vibrating device, and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition, other input\/output devices may be attached to or found with mobile device  within the scope of the present invention.","Logical Forms","Prior to discussing the present invention in greater detail, a brief discussion of a logical form may be helpful. A full and detailed discussion of logical forms and systems and methods for generating them can be found in U.S. Pat. No. 5,966,686 to Heidorn et al., issued Oct. 12, 1999 and entitled METHOD AND SYSTEM FOR COMPUTING SEMANTIC LOGICAL FORMS FROM SYNTAX TREES. Briefly, however, logical forms are generated by performing a morphological analysis on an input text to produce conventional phrase structure analyses augmented with grammatical relations. Syntactic analyses undergo further processing in order to obtain logical forms, which are data structures that describe labeled dependencies among content words in the textual input.","In general, a logical form is a data structure of connected logical relations representing a single input, such as a sentence or portion thereof. The logical form minimally consists of one logical relation and portrays structural relationships (i.e., syntactic and semantic relationships), particularly argument and\/or adjunct relation(s) between important words in an input string.","Logical forms can normalize certain syntactical alternations, (e.g., active\/passive) and resolve both intrasentential anaphora and long distance dependencies. For example,  illustrate logical forms or dependency graphs  and  for the active and passive sentences given as examples in the Background section to help in understanding the elements of logical forms. However, as appreciated by those skilled in the art, when stored on a computer readable medium, the logical forms may not readily be understood as representing a graph.  illustrate important generalizations that the surface string and the syntax models, as described in the Background section, can not capture.","To see why dependency graphs might provide a better language model than string-based n-gram models or syntax trees, consider the following sentences:","1. John hit the ball.","2. The balls were hit by Lucy.","A surface-string-based 3-gram model would generate the following counts:",{"@attributes":{"id":"p-0056","num":"0055"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},{},"<P>\u2003<P>\u2003The 1"]},{"entry":[{},"<P><P>\u2003John 1","<P>\u2003The balls 1"]},{"entry":[{},"<P> John hit 1","the balls were 1"]},{"entry":[{},"John hit the 1","balls were hit 1"]},{"entry":[{},"hit the ball 1","were hit by 1"]},{"entry":[{},"the ball <POST>\u20031","hit by Lucy 1"]},{"entry":[{},{},"by Lucy <POST>\u20031"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"2","align":"center","rowsep":"1"}}]}]}}}}},"Note that each of these trigrams occurs only once, even though the event (the hitting of a ball) is the same in both cases.","If we look at syntax trees, we get a slightly different picture. Specifically, for the sentences above, the following syntax trees would be produced:",{"@attributes":{"id":"p-0059","num":"0058"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"John hit the ball."]},{"entry":[{},"Decl1"]},{"entry":[{}," |NP1 NOUN1 \u2003\u2003\u201cJohn\u201d"]},{"entry":[{}," |VERB1 \u2003\u2003\u201chit\u201d"]},{"entry":[{}," |NP2 DETP1 ADJ1* \u2003\u2003\u201cthe\u201d"]},{"entry":[{}," |\u2003\u2003\u2003|NOUN2 \u2003\u2003\u201cball\u201d"]},{"entry":[{}," |CHAR1 \u2003\u2003\u201c.\u201d"]},{"entry":[{},"The balls were hit by Lucy."]},{"entry":[{},"Decl1"]},{"entry":[{}," |NP1DETP1ADJ1* \u2003\u201cThe\u201d"]},{"entry":[{}," |\u2003\u2003|NOUN1 \u2003\u2003\u201cballs\u201d"]},{"entry":[{}," |AUX1 VERB1 \u2003\u2003\u201cwere\u201d"]},{"entry":[{}," |VERB2* \u2003\u2003\u201chit\u201d"]},{"entry":[{}," |PP1PP2PREP1* \u2003\u201cby\u201d"]},{"entry":[{}," |\u2003\u2003|NOUN2* \u2003\u2003\u201cLucy\u201d"]},{"entry":[{}," |CHAR1\u2003\u2003\u201c.\u201d"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}}}},"Note that here too, the hitting of the ball is spit into two separate buckets (one set of rules for the active voice and another for the passive voice), and so the system would fail to learn a useful generalization.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIGS. 3 and 4","b":["300","400","300","400","302","402","304","308","404","408","306","307","406","407","306","406","304","308","404","408","302","402"]},"Parent nodes  and  contain word forms or lemmas. For example, the lemma in parent nodes  and  is the word \u201chit\u201d. Child nodes , , , and  also contain word forms or lemmas. The semantic relationship nodes  and  illustrate that child nodes  and  are deep subjects and semantic relationship nodes  and  indicate that child nodes  and  are deep objects of parent nodes  and . In addition, LFs  and  also include binary features (or \u201cbits\u201d) attached to each lemma in each node. For example, the binary features are attached to each lemma of LFs  and  and are illustrated in parentheticals. Binary features describe the syntactic properties of a lemma. For example, the word form in node  includes bits that describe the word \u201chit\u201d as past tense and as a proposition.","In contrast to strings and syntax trees, logical forms  and  include both the active voice construction and the passive voice construction in the same graph structure. LF structures can be degraphed to produce a tree, such that no node can have two parents. Logical forms  and  are hierarchical logical representations of the corresponding sentences (or sentence fragments). Each node depends on all of its ancestors. For the purpose of building a (target) language model, the local structure is modeled based on the approximation that each child node depends only on its parent (or on n\u22121 ancestors, for an n-gram LF model).","In one illustrative embodiment, the particular code that builds logical forms from syntactic analyses is shared across the various source and target languages that the machine translation system operates on. The shared architecture greatly simplifies the task of aligning logical form segments from different languages since superficially distinct constructions in two languages frequently collapse onto similar or identical logical form representations.","Machine Translation",{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 5","b":["500","500","504","506","508","510","512","514","520","518","500","522","524","554","526","528"]},"In one illustrative embodiment, a bilingual corpus is used to train the system. The bilingual corpus (or \u201cbitext\u201d) includes aligned translated sentences (e.g., sentences in a source or target language, such as English, in 1-to-1 correspondence with their human-created translations in the other of the source or target language, such as Spanish). During training, sentences are provided from the aligned bilingual corpus into system  as source sentences  (the sentences to be translated), and as target sentences  (the translation of the source sentences). Parsing components  and  parse the source sentences and the target sentences from the aligned bilingual corpus to produce source logical forms  and target logical forms .","During parsing, the words in the sentences are converted to normalized word forms or lemmas and can be provided to statistical word association learning component . Both single word and multi-word associations are iteratively hypothesized and scored by learning component  until a reliable set of each is obtained. Statistical word association learning component  outputs learned word translation pairs .","Word pairs are added to an association list , which acts as an updated bilingual dictionary.","The word pairs , along with source logical forms  and target logical forms  are provided to logical form alignment component . Briefly, component  first establishes tentative correspondences between nodes in the source and target logical forms  and , respectively. This is done using translation pairs from a bilingual lexicon (e.g. bilingual dictionary) , which can be augmented with word pairs  from statistical word association learning component . After establishing possible correspondences, alignment component  aligns logical form nodes according to both lexical and structural considerations and creates word and\/or logical form transfer mappings .","Basically, alignment component  draws links between logical forms using the bilingual dictionary information  and word pairs . The transfer mappings are optionally filtered based on a frequency with which they are found in the source and target logical forms  and  and are provided to a lexical knowledge base building component .","While filtering is optional, in one example, if the transfer mapping is not seen at least twice in the training data, it is not used to build transfer mapping database , although any other desired frequency can be used as a filter as well. It should also be noted that other filtering techniques can be used, other than frequency of appearance. For example, transfer mappings can be filtered based upon whether they are formed from complete parses of the source sentences and based upon whether the logical forms used to create the transfer mappings are completely aligned.","Component  builds transfer mapping database , which contains transfer mappings that basically link words and\/or logical forms in one language, to words and\/or logical forms in the second language. With transfer mapping database  thus created, system  is now configured for runtime translations.","During translation runtime, a source text , to be translated, is provided to parse component . Parse component  receives source text  and creates a source logical form  based upon the source text input.","The source logical form  is provided to search component . Search component  attempts to search the transfer mapping database  in order to obtain transfer mappings that cover all or portions of source logical form . Multiple mappings may be found for one or more of the nodes in source logical form .","After a set of possible transfer mappings and a set of possible combinations of transfer mappings are found, decoding component  scores each combination of transfer mappings using a plurality of models. Under one embodiment, the individual transfer mappings and combinations of transfer mappings are scored with a linearly interpolated score that will be explained in greater detail below. After scores are generated, decoding component  picks and stores the best combination of transfer mappings.","Transfer component  receives the best combination of candidate mappings from decoding component  and builds a target logical form  that will form the basis of the target translation. This is done by combining the nodes of the selected transfer mappings.","In cases where no applicable transfer mappings are found by search component , the nodes in source logical form  and their relations are simply copied into the target logical form . Default single word translations may still be found in transfer mapping database  for these nodes and inserted in target logical form . However, if none are found, translations can illustratively be obtained from association list , which was used during alignment.","Generation component  is illustratively a rule-based, application-independent generation component that maps from target logical form  to the target text (or target string) . Generation component  can alternatively utilize a machine learned approach to sentence realization. Generation component  may illustratively have no information regarding the source language of the input logical forms, and works exclusively with information passed to it by transfer component . Generation component  also illustratively uses this information in conjunction with a monolingual (e.g., for the target language) dictionary to produce target text . One generic generation component  is thus sufficient for each language.","Statistical Machine Translation of Logical Forms","With respect to the following discussion, those skilled in the art should recognize that dependency graphs, logical forms, semantic structures, semantic relationships and semantic representations all relate to and describe the input logical form as provided during runtime. In addition, those skilled in the art should recognize that transfer mappings or mappings refer to those mappings formed during training.","The following equation, reminiscent of equation 2, is a high-level view of an embodiment of the present invention:",{"@attributes":{"id":"p-0081","num":"0080"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"T","mo":"=","mrow":{"munder":{"mrow":{"mi":["arg","max"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}},"msup":{"mi":["T","\u2032"]}},"mo":["\u2062","\u00d7"],"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":"\u2758","msup":{"mi":["T","\u2032"]}}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["T","\u2032"]}}}]}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"3"}}]}}}},"br":{},"sub":["\u03bc",{"sub2":"T"},"T "]},{"@attributes":{"id":"p-0082","num":"0081"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"T","mo":"=","mrow":{"munder":{"mrow":{"mi":["arg","max"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"msup":{"mi":["T","\u2032"]}},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":"\u2758","msup":{"mi":["T","\u2032"]}}}}},{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"P","msub":{"mi":["\u03bc","T"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["T","\u2032"]}}}}],"mo":"+"}}}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"4"}}]}}}},"br":{},"sub":["\u03bc",{"sub2":"C"},"\u03bc",{"sub2":"F"},"\u03bc",{"sub2":"S"},"\u03bc",{"sub2":"B"}]},{"@attributes":{"id":"p-0083","num":"0082"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":"\u2758","msup":{"mi":["T","\u2032"]}}}}},{"mrow":[{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"P","msub":{"mi":["\u03bc","C"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}}},{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"P","msub":{"mi":["\u03bc","F"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}}},{"msub":{"mi":"Score","msub":{"mi":["\u03bc","S"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}},{"msub":{"mi":"Score","msub":{"mi":["\u03bc","B"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}}],"mo":["+","+","+"]}],"mo":"\u2248"},{"mi":"Hence","mo":","}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"5"}}]},{"mtd":[{"mrow":{"mi":"T","mo":"\u2248","mrow":{"munder":{"mrow":{"mi":["arg","max"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},"msup":{"mi":["T","\u2032"]}},"mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"P","msub":{"mi":["\u03bc","C"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}}},{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"P","msub":{"mi":["\u03bc","F"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}}}],"mo":["+","+"]}}},{"mtd":{"mrow":{"mrow":[{"msub":{"mi":"Score","msub":{"mi":["\u03bc","S"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}},{"msub":{"mi":"Score","msub":{"mi":["\u03bc","B"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}},{"mi":"log","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"P","msub":{"mi":["\u03bc","T"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["T","\u2032"]}}}}],"mo":["+","+"]}}}]}}}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"6"}}]}]}}}},"The relative contribution of each score or log-probability is weighted by a factor (\u03bb, \u03bb, \u03bb, \u03bb, and \u03bb), and the resulting linear interpolated approximation is:",{"@attributes":{"id":"p-0085","num":"0084"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"T","mo":"\u2248","mrow":{"munder":{"mrow":{"mi":["arg","max"],"mo":["\u2062","\u2062","\u2062","\u2003"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},"msup":{"mi":["T","\u2032"]}},"mo":"\u2062","mrow":{"mo":"\u2003","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mrow":[{"msub":{"mi":["\u03bb","C"]},"mo":"\u00b7","mi":"log"},{"msub":{"mi":"P","msub":{"mi":["\u03bc","C"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mrow":[{"msub":{"mi":["\u03bb","F"]},"mo":"\u00b7","mi":"log"},{"msub":{"mi":"P","msub":{"mi":["\u03bc","F"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":["+","+"]}}},{"mtd":{"mrow":{"mrow":[{"msub":{"mi":["\u03bb","S"]},"mo":"\u00b7","mrow":{"msub":{"mi":"Score","msub":{"mi":["\u03bc","S"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}}},{"msub":{"mi":["\u03bb","B"]},"mo":"\u00b7","mrow":{"msub":{"mi":"Score","msub":{"mi":["\u03bc","B"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"S","mo":",","msup":{"mi":["T","\u2032"]}}}}}],"mo":["+","+"]}}},{"mtd":{"mrow":{"mrow":[{"msub":{"mi":["\u03bb","T"]},"mo":"\u00b7","mi":"log"},{"msub":{"mi":"P","msub":{"mi":["\u03bc","T"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["T","\u2032"]}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}]}}}}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"7"}}]}}}}},"In practice, this embodiment does not score an entire source logical form and target language logical form all at once. Rather, the search (represented in the above equations by the \u201cargmax\u201d) builds a target language logical form, one translation mapping at a time. In doing so, it employs a score for each mapping. The total linearly interpolated score for a transfer mapping m is represented by:\n\nSCORE()=log ()=\u03bb\u00b7log ()+\u03bb\u00b7log ()+\u03bb\u00b7log ()+\u03bb\u00b7Score()+\u03bb\u00b7Score() \u2003\u2003Equation 8\n","As in the full approximation and as indicated above, each information source score or probability is weighted by a factor. These factors (\u03bb, \u03bb, \u03bb, \u03bb, and \u03bb) or weights are trained by using Powell's algorithm to maximize the BLEU score on the output of the system. Powell's algorithm is known in the art. An example of this algorithm is described in an article by Powell entitled \u201cAn Efficient Method of Finding the Minimum of a Function of Several Variables without Calculating Derivates.\u201d (7:155-162). The BLEU score is also known in the art and is described in an article by Papineni, Roukos, Ward, and Zhu titled\u2014\u201cBleu: a method for automatic evaluation of machine translation.\u201d 2001. IBM Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center.","Models","Under one embodiment, the target language model is an n-gram model that provides the probability of a node in a target dependency graph given a sequence of n-1 preceding nodes and relationships.  illustrates an example target dependency graph , which can be found on the target side in transfer mapping database . In , nodes A, B, C, and D contain word forms. Nodes B, C and D are children nodes and node A is a parent node or root node. Nodes R and R are semantic relationship nodes.","Using this n-gram model, the probability of the entire target dependency graph \u03c4  given the target language model is equal to the product of the n-gram probabilities of each of the nodes. Thus, the probability of target dependency graph  given the target language model is represented by the following formula:",{"@attributes":{"id":"p-0090","num":"0089"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"P","msub":{"mi":["\u03bc","T"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}},{"munder":{"mo":"\u220f","mi":"i"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"P","msub":{"mi":["\u03bc","T"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["c","i"]},"mo":"\u2758","mrow":{"msub":[{"mi":"c","mrow":{"mi":"i","mo":"-","mn":"1"}},{"mi":"c","mrow":{"mi":"i","mo":"-","mrow":{"mo":["(",")"],"mrow":{"mi":"n","mo":"-","mn":"1"}}}}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}],"mi":"\u2026"}}}}}],"mo":"="}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"9"}}]}}}},"br":{},"sub":["i","i ","i\u22121 ","i-(n\u22121)"],"b":"600"},{"@attributes":{"id":"p-0091","num":"0090"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"P","msub":{"mi":["\u03bc","T"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03c4"}},{"mrow":[{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["A","ROOT"],"mo":"\u2758"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mi":"R","mo":"\u2062","mn":"1"},"mo":"\u2758","mi":"ROOT"},"mo":",","mi":"A"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mrow":{"mi":"R","mo":"\u2062","mn":"2"},"mo":"\u2758","mi":"ROOT"},"mo":",","mi":"A"}}}],"mo":["\u00b7","\u00b7","\u00b7"],"mi":"P"},{"mrow":[{"mo":["(",")"],"mrow":{"mrow":[{"mi":["B","A"],"mo":"\u2758"},{"mi":"R","mo":"\u2062","mn":"1"}],"mo":","}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["C","A"],"mo":"\u2758"},{"mi":"R","mo":"\u2062","mn":"2"}],"mo":","}}}],"mo":["\u00b7","\u00b7"],"mi":"P"},{"mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["D","A"],"mo":"\u2758"},{"mi":"R","mo":"\u2062","mn":"2"}],"mo":","}},"mo":"\u00b7","mi":"P"},{"mrow":[{"mo":["(",")"],"mrow":{"mrow":{"mi":"LEAF","mo":"\u2758","mrow":{"mi":"R","mo":"\u2062","mn":"1"}},"mo":",","mi":"B"}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"LEAF","mo":"\u2758","mrow":{"mi":"R","mo":"\u2062","mn":"2"}},"mo":",","mi":"C"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":"LEAF","mo":"\u2758","mrow":{"mi":"R","mo":"\u2062","mn":"2"}},"mo":",","mi":"D"}}}],"mo":["\u00b7","\u00b7"]}],"mo":["\u2062","\u2062","\u2062"]}],"mo":"="}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"10"}}]}}}},"br":{},"b":["1","2"],"i":"Computer Speech and Language, "},"The channel model predicts the probability of the source logical form given the target logical form P(S|T). In one embodiment, we define a transfer mapping cover M for a given source logical form S and a target logical form T as a set of transfer mappings from S to T (denoted as M:S\u2192T). The probability of the source logical form S given the target logical form T is estimated by:",{"@attributes":{"id":"p-0093","num":"0092"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["S","T"],"mo":"\u2758"}}},{"munder":{"mo":"\u2211","mrow":{"mrow":{"msub":{"mi":["M","i"]},"mo":":","mi":"S"},"mo":"->","mi":"T"}},"mo":"\u2062","mrow":{"munder":{"mo":"\u220f","mrow":{"mi":"m","mo":"\u2208","msub":{"mi":["M","i"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"P","msub":{"mi":["\u03bc","C"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}}}}],"mo":"="}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"11"}}]}}}},"br":{},"sub":"i"},{"@attributes":{"id":"p-0094","num":"0093"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":"P","msub":{"mi":["\u03bc","C"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}},"mo":"=","mfrac":{"mrow":[{"mi":"count","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["m","S"]},{"mi":["m","T"]}],"mo":","}}},{"mi":"count","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["m","T"]}}}]}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"12"}}]}}}},"br":{},"sub":["c ","s","T","T"]},"In other words, the probability, according to the channel model \u03bcof a transfer mapping m, is estimated by dividing how many times the source side of a transfer mapping was encountered with the target side of a transfer mapping count(m,m) (in a logical form bitext) by how many times the target side of that transfer mapping was encountered count(m).","The channel model also uses overlapping transfer mappings. Thus, the probability calculated in Equation 12 is the unnormalized probability. The unnormalized probability can be normalized such that the channel model does not favor certain mappings that have more overlap than others.",{"@attributes":{"id":"p-0097","num":"0096"},"figref":["FIG. 7","FIG. 8"],"b":["700","800","518","800","802","803","804","806","808"]},"Mappings  may be combined in a number of different ways to cover all of the nodes of source LF . For example, mapping  may be combined with mapping , mapping  may be combined with mapping , and mappings ,  and  may be combined together. Each of these combinations is non-overlapping because each node in the source LF  is only covered by a single transfer mapping. However, another way to combine mappings  to cover all of source LF  is to combine mapping  with mapping . This forms an overlapping mapping because source node A is covered by both mapping  and mapping .","To prevent the channel model from favoring overlapping mappings, the channel model is normalized. The normalized probability P(m)Pfor a mapping m given the channel model \u03bcis computed by:",{"@attributes":{"id":"p-0100","num":"0099"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msubsup":{"mi":["P","N"],"msub":{"mi":["\u03bc","c"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}},"mo":"=","msup":{"mrow":{"msub":{"mi":"P","msub":{"mi":["\u03bc","c"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}},"mfrac":{"mi":["new","total"]}}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"13"}}]}}}},"br":{},"sub":["\u03bc",{"sub2":"C"}]},"As previously discussed, in some instances, a node in the input LF is not found in the training data because it could not be parsed, it simply didn't show up, or it was filtered out of transfer mapping database  because its frequency was below a predetermined threshold (usually one). In these cases, a default node is inserted, which is formed using a single-word translation from a dictionary. The probability for this default mapping can be determined using IBM Model 1 trained on a bilingual text. Because this probability is obtained in a different manner from the other channel probabilities, the default mapping probability is adjusted by a weight (\u03bb) before being combined with the other channel probabilities. In addition, if no probability can be determined for the default mapping, a default value of (\u03bb) is used. The weights \u03bband \u03bb, like the rest of the weights used in the translation system, are trained using Powell's algorithm to maximize the BLEU score for a set of training sentences. It should be noted that these parameters are separate from the weight (\u03bb) associated with the channel model in computing the final score for a transfer mapping as illustrated in Equation 8.","The channel model operates differently from the target language model. Specifically, the channel model promotes accuracy in translation, while the target language model promotes fluency in the target language without regard to the source language.","The target language model suffers from a liability in that it prefers smaller graphs to larger ones. As a result, the target language model favors mappings that delete a node in the target structure over mappings that keep the same number of nodes in the source and target structures or that add a node in the target structure. For example,  illustrate that training database  contains transfer mappings  and . Transfer mapping  illustrates that there is one less node on the target side than on the source side. Transfer mapping  illustrates that there are the same number of nodes on the source side and the target side of the mapping. The target language model will score mapping  higher than mapping  because there are fewer probabilities in the product for the target LF fragment resulting from mapping  than in the fragment resulting from mapping .","The fertility model helps to overcome this problem by providing a score based on the number of times nodes are deleted in mappings in the training data. If nodes are rarely deleted in the training data, the fertility model will provide a higher score for mappings that do not have deletions.","The fertility model is calculated by reviewing the training data and counting, for each node on the source side of the transfer mapping, how often there is a corresponding node in the target side of the transfer mappings. To avoid sparse data problems, counts for the lemmas are grouped together by parts of speech while counts for the semantic relationships (the number of which is approximately equal to the number of parts of speech) are maintained separately. These frequencies are then used to estimate the probability of a deletion occurring.","For each part of speech or semantic relationship label x, an entry in a fertility table is represented by:",{"@attributes":{"id":"p-0107","num":"0106"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["[","]"],"mi":"x"}},"mo":"=","mfrac":{"mrow":[{"mi":"count","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"x","mo":"\u2208","msub":{"mi":["m","t"]}},{"mi":"x","mo":"\u2208","msub":{"mi":["m","s"]}}],"mo":","}}},{"mi":"count","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"\u2208","msub":{"mi":["m","s"]}}}}]}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"14"}}]}}}},"br":{},"sub":"F "},{"@attributes":{"id":"p-0108","num":"0107"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":"P","msub":{"mi":["\u03bc","F"]}},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"m"}},{"munder":{"mo":"\u220f","mrow":{"mi":"c","mo":"\u2208","msub":{"mi":["m","s"]}}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"c"}}}],"mo":"="}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"15"}}]}}}},"br":{}},{"@attributes":{"id":"p-0109","num":"0108"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"f","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"c"}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["[","]"],"mi":"x"}}},{"mrow":{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mo":"\u2203","mrow":{"msub":{"mi":["c","t"]},"mo":"\u2208","mrow":{"msub":[{"mi":["m","t"]},{"mi":["c","t"]},{"mi":["c","s"]}],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["corresponds","to"]}}}}}]},{"mtd":[{"mrow":{"mn":"1","mo":"-","mrow":{"mi":"F","mo":"\u2061","mrow":{"mo":["[","]"],"mi":"x"}}}},{"mi":"otherwise"}]}]}}],"mo":"="}}},"br":{}},"The next information source is the mapping size score, which takes into account the number of nodes on the source side of the transfer mappings. This information source assigns a score computed by:\n\nScore()=|\u2003\u2003Equation 16\n\nIn effect, the size score gives preference to larger mappings on the assumption that mappings with more context information are likely to be better than mappings with less context information. With reference to , transfer mapping  would receive a score of two because there are two nodes on the source side. Transfer mapping  also would receive a score of two because there are two nodes on the source side.\n","The binary features (or bits) information source takes into account the number of binary features (bits) that match between the input dependency graph and the source side of the transfer mapping. The binary features source provides a rank score that is the sum of the input bits in the source dependency graph that match the bits on the source side of the transfer mapping.  illustrates an input LF  and  illustrates a transfer mapping  stored in transfer mapping database . Node A in input LF  specifies that the lemma of node A has a passive bit and a singular bit. Node A of the source side of transfer mapping  specifies that the lemma of node A has a singular bit. Therefore the rank score of transfer mapping  is one because both node A of the input LF  and node A of the source side of mapping  have a matching singular bit.",{"@attributes":{"id":"p-0112","num":"0111"},"figref":["FIG. 13","FIG. 5","FIG. 14","FIGS. 15-21"],"b":["1300","554","554","554","524","1400","524","1400"]},"Decoding component  of  begins by selecting the top node of the source LF  as shown in block . In , the top node is node A. After selecting node A, decoding component  passes to block  and determines if the best mapping for this node in this context has been identified before. In this example, no mappings for node A have been scored.","The process then continues at step  where a transfer mapping is selected from the set of mappings found by search component  that has a source side that covers the selected node. For example, decoding component  selects transfer mapping  of .","At block , decoding component  determines if there are any nodes in source LF  that are not covered by the selected mapping and that extend directly from the selected mapping. In the example above, mapping  only covers nodes A and B. As such, child node C is not covered by the selected mapping but extends directly from node A, which is covered by the selected mapping. If there is an uncovered child node at step , the process continues at block .","At block , decoding component  selects child node C and passes back to block . At block , decoding component  determines if a best mapping has already been identified for the selected node in the selected context. In particular for a 3-gram target model, \u201cthe selected context\u201d consists of the n\u22121 target side ancestors of node C (in this case, <PRE_ROOT, PRE_ROOT, A\u2032>). For node C, the best mapping has not been identified so the process continues at step  where decoding component  selects a transfer mapping from the set of transfer mappings that covers child node C. For example, decoding component  may select transfer mapping  illustrated in . After step , decoding component  passes to block  and decides if there are any uncovered child nodes that extend from a node covered by the mapping. In the example above, nodes E and F are uncovered child nodes that extend from nodes covered by mapping . Based on the discovery of uncovered child nodes, decoding component  passes to block .","At block , decoding component  selects one of the uncovered child nodes, for example node E, and passes back to block . At block , decoding component  determines that the best mapping for node E in the currently-active target context (in this case, <PRE_ROOT, A\u2032, C\u2032>) has not been determined. The process then continues at block , where decoding component  selects a transfer mapping from the set of transfer mappings that covers node E. For example, decoding component  selects transfer mapping  illustrated in . Decoding component  then passes to block  and decides if the transfer mapping leaves any uncovered child nodes.","According to source LF , node E has no children. Thus, decoding component  proceeds to block  to compute a score for the selected transfer mapping. This score is computed using Equation 8 as described above by incorporating all of the above-described models. Note that one reason for adopting the top-down approach of  is to ensure that the context of the nodes (in the mapping being scored) is known, so that the target model (which requires the context) can be used to calculate the target model score.","After scoring the mapping, decoding component  passes to block  and determines whether there are any more transfer mappings that cover the selected node. In this example,  illustrates another transfer mapping  for selected node E. If there is another transfer mapping, decoding component  passes back to block  and selects the additional transfer mapping. For example, mapping  would be selected. In this example, mapping  does not have uncovered child nodes. Thus, decoding component  passes through block  to block  where decoding component  computes the score of transfer mapping  using Equation 3.","Decoding component  then passes to block  to determine if there are more transfer mappings for the selected node. In this example,  illustrates transfer mapping  that covers node E. Again, decoding component passes back to block . In this example, transfer mapping  does not have any uncovered child nodes. Thus, decoding component  computes a score for transfer mapping  using Equation 3. After the score is computed, decoding component  passes to block .","If decoding component  determines that there are no more mappings at step , the process continues at step  where it compares and selects the highest scoring transfer mapping from the transfer mappings that cover the selected node. In the example above, the scores for mappings ,  and  are compared and the highest scoring mapping is selected. In this example, the highest scoring transfer mapping is assumed to be transfer mapping . Decoding component  stores the node at the head of the mapping, context of the highest scoring mapping (the node that the mapping extends from in the source LF), the score for the highest scoring mapping and each individual model probability or information source scores for the highest scoring mapping. Thus, the target model probability, the channel model probability, the fertility model probability, the size score, and the rank score for the selected mapping are all stored. Although each probability or score for each model is stored, one skilled in the art should recognize that the score determined in Equation 8 is the score that is most important to store.","After the scores for the selected mapping have been stored, decoding component  passes to block  where it determines if more levels exist above the selected node. In the example above, the node C is above node E. If there is another level of nodes above the current selected node, decoding component  returns to the last mapping that was under consideration for that node at block . In the example above, this involves returning to mapping  of .","At block , decoding component  determines if this mapping has any other uncovered child nodes that have not been explored. If there are additional uncovered child nodes to explore, decoding component  continues at block , where decoding component  selects the uncovered child node. In the example above, this would involve selecting child node F. Decoding component  then passes to block  to determine if a best mapping has been identified for this node given its context. If a best mapping has not been determined, decoding component  selects a transfer mapping that covers the selected child node at step  (e.g., mapping  of ). In this example, transfer mapping  has not previously been scored. At block , decoding component  determines that node F has no uncovered child nodes. Thus, decoding component  passes to block  and computes a score for node F using Equation 3.","Decoding component  passes to block  to determine if node F has more transfer mappings. In this example, no other transfer mappings cover node F. Thus, decoding component  stores the score for transfer mapping  and stores each individual model probability or information source score for mapping , the n-a nodes of target-side context in which transfer mapping  was evaluated, the input node corresponding to the head node of transfer mapping  and the total score for transfer mapping .","In this example, more levels of source LF  exist above node F. So, decoding component  passes to block  and returns to the last mapping for node C that was under consideration. At block , decoding component  determines that the selected mapping for node C has no more uncovered children. So, decoding component  passes to block  and computes a total score for transfer mapping .","If the selected mapping had uncovered children, the score for the mapping is determined by combining the scores for the highest scoring mappings for the uncovered child nodes with the score for the selected mapping. For example, the scores for mapping  and  would be combined with the score for mapping  to provide a total score for the entire mapping below node C in the source LF.","Under one embodiment, each component of the mapping scores are combined separately. Thus, the total target model probability of mapping  is:\n\n=log ()+log ()+log () \u2003\u2003Equation 17\n\nwhere P(m) is the target model probability for mapping , P(m) is the target model probability for mapping  and log P(m) is the target model probability for mapping .\n","Similarly, the total channel model probability of mapping  is:\n\n=log ()+log ()+log () \u2003\u2003Equation 18\n\nand the total fertility model probability of mapping  is:\n\n=log ()+log ()+log () \u2003\u2003Equation 19\n\nThe total mapping size score for mapping  is the average of the child mapping size scores and mapping size score for mapping  alone such that:\n\n=[Score()+Score()+Score()]\/3 \u2003\u2003Equation 20\n\nLike the total mapping size score, the total rank score of mapping  is the average of the child rank scores and the rank score for mapping  alone and is described as:\n\n=[Score()+Score()+Score()]\/3 \u2003\u2003Equation 21\n","Once total scores for each component have been determined, they are combined into a single score for the selected mapping using Equation 8 above.","Decoding component  then passes to block  and decides whether more transfer mappings exist for node C. In this example, no other transfer mappings exist for node C, so decoding component  selects transfer mapping  as having the highest scoring mapping and stores the total score for mapping , the context (node A) of mapping , the head node of mapping  (node C) and the individual total component scores for mapping .","At block , decoding component  decides whether more levels exist above node C in source LF . In this example, node A is above node C. Thus, decoding component  returns to the next level up in the mappings as illustrated in block . In the example above, this involves returning to mapping  of . At block , decoding component  determines if the selected mapping has any other uncovered child nodes that need to be explored. In this example, there are no other uncovered child nodes, so decoding component  passes to block  and computes a total score for transfer mapping . Like mapping , the total score for transfer mapping  is formed by combining the scores for mapping  with the scores for mapping .","Decoding component  then passes to block  and determines if more transfer mappings exist for node A. In this example, transfer mapping  also covers node A. As a result, the process returns to step  to select transfer mapping .","At step , the process determines that mapping  has an uncovered child node. Specifically, nodes E and F are not covered by mapping . At step , node E is selected and the process returns to step  to determine if a best mapping has been selected for node E in the current context given our previous choice of mapping , which in this case would be <PRE_ROOT, A\u2032, C\u2032>. Such a best mapping was selected (mapping ). This best mapping and its scores are then selected and the process returns to mapping  at step .","The process then determines if there are more uncovered child nodes to consider. For mapping , child node F has not been considered and is selected at step . At step , it is determined that a best mapping for node F in the context of node D has been determined (mapping ). This best mapping is then selected and the process returns to mapping  at step .","Upon returning to step , there are no further uncovered child nodes to consider and a score for mapping  is computed using the stored scores for mappings  and  and the individual mapping scores for mapping . As above, the individual components of the mapping score are combined separately.","At step , no other transfer mappings exist, so decoding component  passes to block  and selects between the transfer mapping structure headed by transfer mapping  and the transfer mapping structure headed by transfer mapping  based on the total scores for these two mapping structures. Decoding component  stores the total score for the highest scoring transfer mapping structure and passes to block . At block , decoding component determines whether more levels exist above node A. In this example, node A is the top node in source LF , so decoding component  ends the decode and returns the highest scoring transfer mapping structure determined for node A.","Based on the highest stored scoring transfer mapping illustrated in , transfer component  can build a target LF. For example, if transfer mappings , ,  and  were selected as the highest scoring transfer mappings and, therefore, have the highest probability of a target translation, they are combined to form a target LF.","Information sources, such as statistical models and other scoring techniques are used in the present invention to determine the best translation for a semantic structure. Input semantic structures have been used to generate output semantic structures by using a greedy search algorithm (in one embodiment) against a set of transfer mappings. However, the greedy search algorithm does not test all possible combinations of transfer mappings but simply selects the first set of transfer mappings that completely cover the input semantic structure. Statistical models have been used to predict the most likely output string given an input string. However, statistical models used in string-based systems assume that an element can be predicted based on another adjacent or almost adjacent element. Thus, the present invention uses statistical models to predict the best translation for a semantic structure.","Although the target language model, as applied to semantic structures of the present invention can be used in building output word strings, the target language model as applied to semantic structures can be used in other language programs. For example, other systems include speech recognition, optical character recognition, handwriting recognition, information extraction, and grammar checking.","Although the present invention has been described with reference to particular embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIGS. 3 and 4"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIGS. 9 and 10"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":["FIG. 14","FIG. 13"]},{"@attributes":{"id":"p-0031","num":"0030"},"figref":["FIGS. 15-21","FIG. 13"]}]},"DETDESC":[{},{}]}
