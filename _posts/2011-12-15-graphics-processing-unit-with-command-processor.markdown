---
title: Graphics processing unit with command processor
abstract: Aspects of the disclosure relate to a method of controlling a graphics processing unit. In an example, the method includes receiving one or more tasks from a host processor, and scheduling, independently from the host processor, the one or more tasks to be selectively executed by a shader processor and one or more fixed function hardware units, wherein the shader processor is configured to execute a plurality of instructions in parallel, and the one or more fixed function hardware units are configured to render graphics data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08842122&OS=08842122&RS=08842122
owner: QUALCOMM Incorporated
number: 08842122
owner_city: San Diego
owner_country: US
publication_date: 20111215
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["TECHNICAL FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Aspects of the disclosure relate to graphics processing.","Graphics processing units (GPUs) are commonly implemented to render three-dimensional (3D) graphics content for presentation on a two-dimensional visual display. For example, a GPU may be implemented to render pixels that are representative of 3D content on a display. The GPU may perform graphics processing to generate pixel values for each pixel of the display when rendering a scene.","Some GPUs may render an entire scene at one time. Alternatively, a GPU may render graphics in smaller portions of a scene, often referred to as \u201ctiles\u201d of a scene. For example, a GPU may subdivide a scene into a plurality of tiles, and individually render each of the tiles. The GPU may then reconstruct the scene by combining each of the rendered tiles. Tiles may be sized so that the data associated with a particular tile can be stored in local GPU memory resources during rendering.","In general, this disclosure describes techniques for controlling a graphics processing unit (GPU). More specifically, this disclosure describes techniques for managing resources of a GPU using an integrated processing unit. That is, for example, rather than receiving a command stream from a CPU that dictates how the resources of a GPU are to be utilized, the integrated processing unit of the GPU may receive computational tasks that generally define work that is to be performed by the GPU. The integrated processing unit of the GPU may then determine how the tasks are executed by the resources of the GPU. For example, the integrated processing unit of the GPU may receive a task and independently schedule the task to a specific GPU resource, thereby controlling the resource of the GPU that executes the task.","In an example, a graphics processing unit comprises a shader processor, one or more fixed function hardware units, and a command processor. The shader processor is configured to execute a plurality of instructions in parallel. The one or more fixed function hardware units are configured to render graphics data. The command processor unit is configured to receive one or more tasks from a host processor and independently schedule the one or more tasks to be selectively executed by the shader processor and the one or more fixed function hardware units.","In another example, a method for controlling a graphics processing unit comprises receiving one or more tasks from a host processor, and scheduling, independently from the host processor, the one or more tasks to be selectively executed by a shader processor and one or more fixed function hardware units, wherein the shader processor is configured to execute a plurality of instructions in parallel, and the one or more fixed function hardware units are configured to render graphics data.","In another example, an apparatus for controlling a graphics processing unit comprises means for receiving one or more tasks from a host processor, and means for scheduling, independently from the host processor, the one or more tasks to be selectively executed by a shader processor and one or more fixed function hardware units, wherein the shader processor is configured to execute a plurality of instructions in parallel, and the one or more fixed function hardware units are configured to render graphics data.","In another example, a computer program product comprising a computer-readable medium comprising stored thereon instructions that, when executed, cause one or more processors to receive one or more tasks from a host processor, and schedule, independently from the host processor, the one or more tasks to be selectively executed by a shader processor and one or more fixed function hardware units, wherein the shader processor is configured to execute a plurality of instructions in parallel, and the one or more fixed function hardware units are configured to render graphics data.","The details of one or more aspects of the disclosure are set forth in the accompanying drawings and the description below. Other features, objects, and advantages of the disclosure will be apparent from the description and drawings, and from the claims.","In general, this disclosure describes techniques for controlling a graphics processing unit (GPU). More specifically, this disclosure describes techniques for managing resources of a GPU using an integrated processing unit. That is, for example, rather than receiving a command stream from a CPU that dictates how the resources of a GPU are to be utilized, the integrated processing unit of the GPU may receive computational tasks that generally define work that is to be performed by the GPU. The integrated processing unit of the GPU may then determine how the tasks are executed by the resources of the GPU. For example, the integrated processing unit of the GPU may receive a task and independently schedule the task to a specific GPU resource, thereby controlling the resource of the GPU that executes the task.",{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 1","FIG. 1"],"b":["20","20","24","28","32","36","40","44","20","48","52","54","56","20","20"]},"The illustrated computing device  of  is merely one example. Techniques for managing resources of a GPU, such as GPU , may be carried out by a variety of other computing devices having other components. In some examples, computing device  may include additional components not shown in  for purposes of clarity. For example, computing device  may include one or more communication bridges for transferring data between components of the computing device . Moreover, the components of computing device  shown in  may not be necessary in every example of computing device . For example, user interface  and display  may be external to computing device  in examples where computing device  is a desktop computer.","Host processor  may include any one or more of a microprocessor, a controller, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field-programmable gate array (FPGA), or equivalent discrete or integrated logic circuitry. Additionally, the functions attributed to host processor , in this disclosure, may be embodied as software, firmware, hardware or any combination thereof.","Host processor  processes instructions for execution within computing device . Host processor  may be capable of processing instructions stored on storage device  or instructions stored in memory . The host processor  may generate a command stream using a driver (e.g., which may be implemented in software executed by the host processor ) for execution by GPU . That is, host processor  may generate a command stream that defines operations to be performed by GPU .","Host processor  may generate a command stream to be executed by GPU  that causes viewable content to be displayed on display . For example, host processor  may generate a command stream that provides instructions for GPU  to render graphics data. In this example, host processor  may generate a command stream that is executed by a graphics rendering pipeline, such as the pipeline shown and described with respect to .","Additionally or alternatively, host processor  may generate a command stream to be executed by GPU  that causes GPU  to perform other operations. For example, in some instances, host processor  may generate a command stream for using GPU  as a general purpose graphics processing unit (GPGPU). For example, GPU  may carry out a variety of general purpose computing functions traditionally carried out by host processor . Examples include a variety of image processing functions, including video decoding and post processing (e.g., de-blocking, noise reduction, color correction, and the like) and other application specific image processing functions (e.g., facial detection\/recognition, pattern recognition, wavelet transforms, and the like). In some examples, GPU  may collaborate with host processor  to execute such GPGPU applications. For example, host processor  may offload certain functions to GPU  by providing GPU  with a command stream for execution by GPU .","Storage device  may include one or more computer-readable storage media. Storage device  may be configured for long-term storage of information. In some examples, storage device  may include non-volatile storage elements. Examples of such non-volatile storage elements may include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of electrically programmable memories (EPROM) or electrically erasable and programmable (EEPROM) memories. Storage device  may, in some examples, be considered a non-transitory storage medium. The term \u201cnon-transitory\u201d may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. However, the term \u201cnon-transitory\u201d should not be interpreted to mean that storage device  is non-movable. As one example, storage device  may be removed from computing device , and moved to another device. As another example, a storage device, substantially similar to storage device , may be inserted into computing device . Storage device  may store instructions for execution of one or more applications by host processor  or GPU .","Memory  may be configured to store information within computing device  during operation. In some examples, memory  is a temporary memory, meaning that a primary purpose of memory  is not long-term storage. Memory  may, in some examples, be described as a computer-readable storage medium. Accordingly, memory  may also be considered \u201cnon-transitory,\u201d despite storing data that can change over time. Memory  may also, in some examples, be described as a volatile memory, meaning that memory  does not maintain stored contents when the computer is turned off. Examples of volatile memories include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of volatile memories known in the art.","In some examples, memory  may be used to store program instructions for execution by host processor  or GPU . Memory  may be used by software or applications running on computing device  to temporarily store information during program execution. As such, memory  may be accessed by other components of computing device  such as host processor  and GPU .","According to some aspects of the disclosure, memory  may be implemented as a display buffer that stores pixel data (e.g., a frame of video data, a computer-generated graphics image, a still image, and the like) before the data is displayed by display . For example, GPU  may resolve pixel data to memory  before the pixel data is displayed by display .","Computing device  may utilize network module  to communicate with external devices via one or more networks, such as one or more wireless networks. Network module  may be a network interface card, such as an Ethernet card, an optical transceiver, a radio frequency transceiver, or any other type of device that can send and receive information. In some examples, computing device  may utilize network module  to wirelessly communicate with an external device such as a server, mobile phone, or other networked computing device.","Computing device  also includes user interface . Examples of user interface  include, but are not limited to, a trackball, a mouse, a keyboard, and other types of input devices. User interface  may also include a touch-sensitive screen that is incorporated as a part of display . Display  may comprise a liquid crystal display (LCD), an organic light emitting diode (OLED) display, a plasma display, or another type of display device.","GPU  of computing device  may be a dedicated hardware unit having fixed function and programmable components for rendering graphics and executing GPGPU applications. For example, according to the example shown in , GPU  includes a programmable shader processor , fixed function units  (which may include fixed function hardware components), and command processor . GPU  may also include a DSP, a general purpose microprocessor, an ASIC, an FPGA, or other equivalent integrated or discrete logic circuitry. GPU  may also include other components, such as a dedicated GPU memory, as described in greater detail with respect to .","In addition, although shown as separate components in , in some examples, GPU  may be formed as part of host processor . GPU  may be configured to utilize processing techniques in accordance with a variety of application programming interfaces (APIs). For example, a user may program an application to be executed by GPU  using a standard software interface that can run on multiple platforms, operating systems, and hardware.","In some examples, GPU  may be configured to utilize applications generated using the OpenGL platform, including OpenGL for Embedded Systems (\u201cOpenGL ES,\u201d released March, 2007 and publicly available). Other example APIs include Compute Unified Device Architecture (\u201cCUDA\u201d developed by NVIDIA Corporation, version 3.2 released Sep. 17, 2010) and DirectX (developed by Microsoft, Inc., version 11 released Oct. 27, 2009). In general, an API includes a predetermined, standardized set of commands that are executed by associated hardware. API commands allow a user to instruct hardware components of a GPU to execute commands without user knowledge as to the specifics of the hardware components.","While example of  shows shader processor  as a single block, shader processor  may include one or more shader processing units, and may generally be referred to as a \u201cunified shader processor.\u201d That is, for example, shader processor  may perform geometry, vertex, pixel, or other shading operations (such as those described with respect to the shader stages shown in the example of ) to render graphics. In another example, shader processor  may perform general purpose calculations. That is, shader processor  may execute instructions associated with a GPGPU application. A GPGPU application typically includes one or more kernels, which define functions that can be used to analyze or modify a variety of input data. Examples include functions for processing relatively large numerical data sets in parallel. In an image processing context, functions may include, for example, color correction algorithms, face detection algorithms, or functions for carrying out augmented reality applications. Other examples include transform functions, functions for ray tracing, or a variety of other functions.","Shader processor  may generally be configured to execute a plurality of instructions in parallel. For example, shader processor  may include an array of processing units (e.g., such as arithmetic logic units (\u201cALUs\u201d)) that execute instructions in parallel. Accordingly, shader processor  may include a one or more components not specifically shown in , such as components for fetching and decoding instructions, components for load balancing, and one or more ALUs or other computational units for carrying out calculations. Shader processor  may also include one or more memories, caches, or registers (e.g., such as shader processor memory  shown and described with respect to the example of ).","Fixed function units  may include one or more units, such as fixed function hardware components, for rendering graphics data. For example, fixed function units  may include units for performing an initial input assembly of graphics data to prepare the graphics data (triangles, lines and points) for rendering. In another example, fixed function units  may include units for performing rasterization. That is, fixed function units  may prepare primitives for shading during rasterization. Fixed function units  may also perform a variety of other operations associated with rendering graphics data and\/or performing other operations.","As noted above, GPU  may render graphics for display on display . For example, GPU  may use shader processor  and\/or fixed function units  to perform the operations associated with the stages of a graphics rendering pipeline, such as the example pipeline shown in . According to some aspects of the disclosure, GPU  may implement a tile-based rendering architecture. For example, rather than rendering an entire scene of pixel data (e.g., a frame of video data, a computer-generated graphics image, a still image, and the like), GPU  may break a scene into multiple pieces (e.g., \u201ctiles\u201d), and individually render the tiles. The GPU  may write each of the rendered tiles to a buffer, such as a display buffer included in memory . Upon writing all of the tiles of the scene to memory , computing device  may assemble the rendered tiles and display the scene on display .","When implemented as a GPGPU, GPU  may execute shader programs, referred to herein as kernels. For example, as described in greater detail with respect to , shader processor  may execute kernels to perform a variety of general purpose computing functions, such as image processing functions and other functions. A kernel can be defined by a user using an API, such as the example APIs described above. Kernels may comprise individual work items (e.g., a basic unit of work in a GPU) that are grouped into workgroups. In some examples, GPU  may execute kernels in a particular sequence when executing a GPGPU command stream. That is, shader processor  may execute multiple instances of the same kernel in parallel before moving on to the next kernel. In other examples, GPU  may execute multiple kernels simultaneously.","In general, as noted above, GPU  receives a command stream from a host processor, such as host processor . The command stream is typically generated by a software driver being executed by host processor , and controls the operation of shader processor  and fixed function units . For example, the command stream typically controls which components of GPU  perform the operations defined in the command stream.","Due to constraints imposed by the configuration of some GPUs and the limitations of a command stream, a GPU, such as GPU , may typically execute a single task at a time. For example, GPU  may serially execute each operation included in a command stream. Thus, certain resources of GPU  may be idle while others are processing instructions according to the command stream. That is, shader processor  of GPU  may execute an operation according to a command stream while the fixed function units  sit idle.","In some examples, multitasking may be emulated by host processor  by switching between more than one command stream at suitable times. Command stream switching, however, may be relatively time intensive and may make it difficult to efficiently share GPU resources to carry out multiple tasks. For example, there may be relatively high latency associated with switching from one command stream to another. Accordingly, it may not be efficient to switch between various rendering operations and other operations, such as general purpose operations (e.g., image processing, ray tracing, or the like). In addition, it may not be efficient to assign high-priority, time-critical tasks (e.g., multimedia processing tasks such as audio processing) to GPU  via a command stream, due to uncertainty regarding when the task will be executed. For example, the time of execution of operations included in a command stream may be difficult to determine and may vary significantly depending on existing commands being executed by GPU .","According to aspects of the disclosure, command processor  may locally control the GPU resources without intervention by a host processor, such as host processor  or another host processing unit (e.g., a central processing unit (CPU)). For example, according to aspects of this disclosure, command processor  of GPU  may receive one or more \u201ctasks\u201d from host processor . Command processor  may independently schedule the tasks to be executed by the resources of GPU , including, for example, shader processor  and fixed function units . That is, rather than receiving a command stream from host processor  that dictates how GPU resources are utilized, command processor  may receive one or more higher level tasks that generally define work that is to be performed by the GPU. Such tasks may define operations that are to be performed by GPU  without dictating which resources of GPU  are to be used to perform the operations.","Command processor  may independently determine when to execute the tasks and\/or which resources to execute the tasks. That is, aspects of this disclosure may refer to command processor  independently determining when to execute the tasks and\/or which resources to execute the tasks, which may generally refer to the ability of command processor  to control the execution of tasks without intervention from host processor  (e.g., without intervention provided from, for example, a command stream generated by host processor ). As described in greater detail below, such determinations may be made based on resource availability and\/or task priority, among other potential considerations. Command processor  may also independently control the data flow between the various resources of GPU .","Command processor  may simultaneously control multiple tasks, including independently and selectively distributing tasks to different resources of GPU , such as shader processor  and\/or fixed function units . That is, command processor  may control the execution of tasks without intervention from host processor , and select which resources of GPU  execute each task without all resources of GPU  necessarily dedicated to executing a single task. In example, command processor  may schedule a graphics rendering task to be executed by fixed function units  of GPU , while also selectively scheduling a computational task to be simultaneously executed by shader processor  of GPU . In this way, command processor  may enable GPU  to execute multiple tasks in parallel, without having to wait for host processor  to switch command streams. Rather, host processor  can send a plurality of tasks to command processor , and command processor  can control the execution of the tasks.","The ability of GPU  to locally control GPU resources without intervention by host processor  may provide increased flexibility and promote efficient management of GPU resources. For example, as described above, host processor  traditionally transmits a command stream to GPU , which GPU  sequentially executes using the resources of GPU  in a pipeline fashion. A single operation is typically performed by GPU  at a given time, such that some resources of the GPU may be idle while waiting for other resources to finish executing a particular command stream. That is, in an example for purposes of illustration, certain fixed function units  may be performing graphics rendering operations while shader processor  sits idle.","According to aspects of this disclosure, command processor  may increase efficiency by reducing an amount of time that resources of GPU  are idle. For example, rather than treating GPU resources as an interconnected series of components that may only process one command stream at a time, command processor  may individually and selectively control the resources of GPU . In the example provided above, command processor  may schedule a graphics rendering task to fixed function units  of GPU , while also scheduling a different, computational task to shader processor  of GPU . Accordingly, command processor  potentially reduces the amount of time that GPU resources are idle by selectively scheduling tasks to different resources of GPU , rather than having all of the resources of GPU  execute one task at a time.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 2","FIG. 1"],"b":["80","80","48","80","80"]},"Graphics rendering pipeline  generally includes programmable stages (e.g., illustrated with rounded corners) and fixed function stages (e.g., illustrated with squared corners). For example, graphics rendering operations associated with certain stages of graphics rendering pipeline  are generally performed by a programmable shader processor, such as shader processor , while other graphics rendering operations associated with other stages of graphics rendering pipeline  are generally preformed by non-programmable, fixed function hardware units, such as fixed function units . Graphics rendering stages performed by shader processor  may generally be referred to as \u201cprogrammable\u201d stages, while stages performed by fixed function units  may generally be referred to as fixed function stages.","Input assembler stage  is shown in the example of  as a fixed function stage and is generally responsible for supplying graphics data (triangles, lines and points) to the graphics rendering pipeline . For example, input assembler stage  may collect vertex data for high order surfaces, primitives, and the like, and output vertex data and attributes to vertex shader stage .","The vertex shader stage  may process the received vertex data and attributes. For example, vertex shader stage  may perform per-vertex processing such as transformations, skinning, vertex displacement, and calculating per-vertex material attributes. In some examples, vertex shader stage  may generate texture coordinates, vertex color, vertex lighting, fog factors, and the like. Vertex shader stage  generally takes a single input vertex and outputs a single, processed output vertex.","The process of tessellation may generally be performed by the hull shader stage , the tessellator stage , and the domain shader stage . For example, the hull shader stage  may generate tessellation factors to pass to the tessellator stage . In an example, the hull shader stage  may transform input data that defines a low-order surface into control points that make up a patch. A patch may include data for each of a plurality of nodes that together specify certain properties (e.g., such as geometry) for a relatively small portion of a surface of an object. The tessellator stage  may be a fixed-function stage that uses the tessellation factors from the hull shader stage  to tessellate (or subdivide) a patch into multiple triangle or quad primitives. Each vertex resulting from the tessellator stage  may be output to the domain shader stage . The domain shader stage  may evaluate the surface representation at each vertex. The domain shader stage  may send complete data for each vertex (e.g., position, texture coordinates, etc.) to the geometry shader .","The geometry shader stage  may receive a primitive defined by the vertex data (e.g., three vertices for a triangle, two vertices for a line, or a single vertex for a point) and further process the primitive. For example, the geometry shader stage  may perform per-primitive processing such as silhouette-edge detection and shadow volume extrusion, among other possible processing operations.","The rasterizer stage  is typically a fixed function stage that is responsible for clipping primitives and preparing primitives for the pixel shader stage . For example, the rasterizer stage  may generate a number of fragments for shading by pixel shader . The pixel shader stage  receives fragments from the rasterizer stage  and generates per-pixel data such as color. The pixel shader stage  may also perform per-pixel processing such as texture blending and lighting model computation. The output merger stage  is generally responsible for combining various types of output data (such as pixel shader values, depth and stencil information) to generate a final result.","As noted above, graphics rendering pipeline  generally includes programmable stages (e.g., illustrated with rounded corners) and fixed function stages (e.g., illustrated with squared corners). Accordingly, some of the stages of graphics rendering pipeline  are typically performed by programmable components, such as a shader processor (e.g., shader processor  shown in the example of ), while other stages of graphics rendering pipeline  are typically performed by non-programmable, fixed function hardware units (e.g., fixed function units  shown in the example of ).","According to aspects of this disclosure, a command processor (e.g., command processor  shown in the example of ) may be used to independently control each of the stages of graphics rendering pipeline . For example, rather than data being sequentially processed by each stage of pipeline , according to aspects of this disclosure, command processor  may independently control the stages of pipeline . That is, command processor  may distribute a first computational task to input assembler , and distribute a second computational task to one of the shader stages (which uses shader processor ). In this example, the second computational task is routed through or executed by input assembler , as would normally occur in pipeline processing. Accordingly, input assembler  may be performing a graphics rendering function, while shader processor  is executing another task (such as a GPGPU operation).",{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIG. 3","FIG. 3","FIG. 3","FIG. 3"],"b":["48","48","52","54","56","60","52","62"]},"It should also be understood that  is provided as merely one example of a GPU that can utilize techniques the described in this disclosure. The techniques described with respect to this disclosure may be carried out by a variety of other GPUs having other components. That is, in other examples, GPU  may also include a variety of other components and modules related to rendering images, analyzing images, and\/or performing other calculations. For example, GPU  may include texture units, scheduling units, arithmetic logic units (ALUs), or other GPU components not shown in  for purposes of clarity.","As noted with respect to the example of  above, shader processor  may include one or more shader processing units that may perform graphics rendering and\/or other general purpose operations. That is, for example, shader processor  may perform geometry, vertex, pixel, or other shading operations to render graphics. In other examples, shader processor  may perform general purpose calculations.","Fixed function units  may generally include one or more units, such as fixed function hardware components, for rendering graphics data. For example, fixed function units  may include units for performing an initial input assembly of graphics data to prepare the graphics data (triangles, lines and points) for rendering. In another example, fixed function units  may include units for performing rasterization. That is, fixed function units  may prepare primitives for shading during rasterization. Fixed function units  may also perform a variety of other operations associated with rendering graphics data and\/or performing other operations.","Command processor  may include any one or more of a microprocessor, a controller, a digital signal processor (DSP), an application specific integrated circuit (ASIC), a field-programmable gate array (FPGA), or equivalent discrete or integrated logic circuitry. Additionally, the functions attributed to command processor  may, in some examples, be embodied as software, firmware, hardware or any combination thereof. While shown command processor  is shown as a separate unit from shader processor , in some examples, command processor  may be integrated with shader processor .","As noted above with respect to , command processor  may locally control the GPU resources without intervention by host processor  or other processing unit. For example, according to aspects of this disclosure, command processor  of GPU  may receive one or more \u201ctasks\u201d from host processor . Command processor  may independently schedule the tasks to be executed by the resources of GPU , including, for example, shader processor  and fixed function units . That is, rather than receiving a command stream from host processor  that dictates how GPU resources are utilized, command processor  may receive one or more higher level tasks that generally define work that is to be performed by the GPU. Such tasks may define operations that are to be performed by GPU  without dictating which resources of GPU  are to be used to perform the operations. GPU  may independently determine when to execute the tasks and\/or which resources to execute the tasks.","GPU memory  may be is similar to memory  shown in . That is, in some examples, GPU memory  may be a temporary computer-readable storage medium. Examples of GPU memory  include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of registers and memories known in the art. In examples where GPU  is formed as part of another processor, such as host processor , GPU memory  may be accessed by components other than GPU . Typically, GPU memory  stores data that is used in operations performed by GPU . In some examples, GPU memory  may receive data from another memory unit, such as memory  of computing device . That is, computing device  may transfer data from memory  to GPU memory  so that the data is accessible to GPU .","In the example shown in , shader processor  includes shader processor memory  (\u201cSP memory\u201d). As described in greater detail below with respect to memory copy engine , data may be exchanged between SP memory  and GPU memory . For example, SP memory  may receive and store unprocessed data for execution by shader processor  from GPU memory . That is, SP memory  may store data prior to the data being processed, for example, by ALUs of shader processor . In addition, SP memory may store processed data prior to transferring the processed data to GPU memory . SP memory  may be accessed relatively quickly by units within shader processor . However, data transfer between SP memory  and GPU memory  may consume one or more clock cycles during which shader processor  may not process other data.","According to aspects of this disclosure, GPU  also includes memory copy engine , which is in communication with one or more memory units of GPU . For example, as shown in , memory copy engine  may be in communication with memory , GPU memory , and\/SP memory . In some examples, memory copy engine  may facilitate data transfer between memory  (e.g., a system memory) and SP memory . For example, according to aspects of this disclosure, memory copy engine  may receive instructions from command processor  that identify data to transfer between memory  and SP memory  (and vice versa). In addition, memory copy engine  may receive instructions from command processor  that identify when to transfer data between memory  and SP memory  (and vice versa). Upon receiving the instructions from command processor , memory copy engine  may carry out the data transfer between memory  and SP memory . In other examples, memory copy engine  may also be responsible for data transfer between other memory units of GPU , such as between GPU memory  and SP memory .","In some examples, according to aspects of this disclosure, certain units of GPU  (e.g., such as shader processor , SP memory , memory , and\/or GPU memory ) may be \u201cunaware\u201d of the operations of memory copy engine . That is, memory copy engine  may operate independently of the units to which data is being copied to and copied from. In an example for purposes of illustration, rather than shader processor  copying data from memory  to SP memory  (and using the associated resources and clock cycles to facilitate the data transfer), such a data transfer may be handled by memory copy engine . That is, memory copy engine  may copy the data from memory  to SP memory  for use by shader processor . Accordingly, shader processor  may utilize the data stored in SP memory  without waiting for data to be transferred between SP memory  and memory . In this way, memory copy engine  may provide shader processor  with a data as needed, and shader processor  can continue to perform operations on the data without utilizing clock cycles for data transfer. For example, command processor  may synchronize memory copy engine  with the tasks being scheduled by command processor , such that SP memory  is supplied with the appropriate data when executing a particular task. As noted above, memory copy engine  may also be responsible for transferring processed data between GPU memory  and SP memory . Additionally or alternatively, memory copy engine  may transfer data to and from fixed function units  and\/or other components of GPU .","In operation, according to aspects of this disclosure, host processor  may generate a command stream that includes one or more tasks to be carried out by GPU . The tasks may set forth a variety of work that is to be done by GPU , such as performing graphics rendering and\/or other general purpose operations. Command processor  of GPU  may receive the command stream and facilitate the execution of each of the tasks included in the command stream.","According to aspects of this disclosure, command processor  may initially parse the received command stream and identify each task that is to be performed by GPU . In addition to parsing the tasks from the command stream, command processor  may maintain a command queue for organizing each of the tasks to be executed by the components of GPU . For example, command processor  may schedule tasks to be executed by the components of GPU  (such as shader processor  and\/or fixed function units ) using the command queue. In some examples, the command queues may be fixed function hardware units (e.g., first in first out (FIFO) registers, or the like). In other examples, the command queues may be general memory or register units.","Command processor  may also maintain command queues for controlling other functions associated with GPU . For example, command processor  may maintain a command queue for organizing data exchange with a host processor (e.g., a central processing unit (CPU)). In other examples, command processor  may maintain a command queue for organizing data exchange with a digital signal processor (DSP) or other computing components, such as multimedia units.","In some examples, command processor  may schedule tasks based on an availability of GPU resources . For example, command processor  may schedule tasks based on input\/output interfaces being available. In another example, command processor  may schedule tasks based on whether the data being executed is ready for processing. In another example, command processor  may schedule tasks based on whether space is available in memory (e.g., GPU memory ) to store the result of the tasks.","According to some aspects of this disclosure, command processor  may prioritize the tasks in the command queues. In some examples, the component responsible for generating the task (e.g., host processor ) may be responsible for indicating a priority with the task. In other examples, command processor  may determine priority based on the tasks that are included in the command queue. For example, command processor  may identify a task that is more time sensitive than other tasks in the command queue, and may prioritize the high priority task in the command queue such that the task is scheduled and executed prior to the other tasks.","In some examples, command processor  may also maintain a list of active tasks that are being executed by the components of GPU , as well as track the component responsible for executing the tasks. The active task list may be used, for example, to track the status of tasks. By tracking the status of tasks, command processor  may determine which resources of GPU  are available for scheduling tasks. That is, if a task is currently being executed by a GPU resource (e.g., the task is \u201cactive\u201d), that resource may not be available until the active task is completed. In some examples, command processor  may be responsible for identifying which components of GPU  are actively executing tasks (e.g., by polling or otherwise determining that a component is busy). In other examples, command processor  may receive an indication that a particular task has been completed by the component responsible for executing the task, or by monitoring data being written to GPU memory .","In an example, command processor  may initiate the execution of a task by shader processor . Command processor  may then add the task to the list of active tasks that are being currently being executed by shader processor . Using the active task list, command processor  may track the resources that are currently executing tasks, while also identifying the available resources that are not currently executing tasks.","Using priority information and\/or the list of active tasks, command processor  may preempt a task being executed by a component of GPU  if a higher priority task is added to the command queue and that higher priority task is ready to be executed (e.g., the data associated with the task is ready for execution). That is, command processor  may stop execution of a lower priority task in order to execute a higher priority task if the higher priority task is ready to be executed. Command processor  may resume the lower priority task after completing execution of the higher priority task.","In this way, command processor  may independently distribute tasks to different resources of GPU , such as shader processor  and\/or fixed function units  without intervention by a host processor. In example for purposes of illustration, computing device  including GPU  may be a digital camera (e.g., a mobile device that includes a digital camera). In this example, GPU  of the digital camera may be rendering images for display in a viewfinder. The user of the digital camera may select an image processing function to perform on the displayed images (e.g., image sharpening, noise reduction, and the like). GPU  may switch from rendering the images to performing the (general purpose) image processing functions. In this example, the switching between functions of GPU  may be independently facilitated by command processor .","The ability of GPU  to locally control GPU resources without intervention by host processor  may provide increased flexibility and promote efficient management of GPU resources. For example, according to aspects of this disclosure, command processor  may reduce an amount of time that resources of GPU  are idle. In the example provided above, command processor  may schedule a graphics rendering task to fixed function units  of GPU , while also scheduling a computational task to shader processor  of GPU . That is, for example, with respect to the digital camera example described above, command processor  may schedule graphics rendering task (e.g., rendering images for display in the viewfinder) while also scheduling a computational task (e.g., image processing). Accordingly, command processor  potentially reduces the amount of time that GPU resources are idle.","According to aspects of this disclosure, the \u201ctasks\u201d from host processor  may be defined by a user (e.g., an application developer) using an API. For example, as noted above, APIs such as DirectX and OpenGL allow a user to develop an application for rendering graphics or performing other operations with GPU . Traditional APIs, however, may be restricted to a fixed (e.g., pipelined) data flow, which may be suitable for one type of graphics rendering but may not efficiently accommodate other tasks (e.g., such as raytracing or raycasting). That is, applications developed using traditional APIs may route data sequentially through each stage of a rendering pipeline (such as graphics rendering pipeline  shown in the example of ), regardless of whether each stage is needed to carry out a particular operation.","Aspects of this disclosure relate to an API that a user may apply to generate an application having relatively high level computational tasks. For example, the API may allow a user to define tasks that generally describe work that is to be performed by the GPU. That is, tasks may define operations that are to be performed by GPU  without dictating which resources of GPU  are to be used to perform the operations. In some examples, a variety of tasks may be included in an API and exposed to an application developer. Using the API, the application developer may generate an application that, when executed, utilizes a custom pipeline (e.g., relative to graphics pipelines of traditional APIs). In an example, an application developer may generate an application that uses a custom graphics pipeline that is optimized for performing particular rendering operations, such as raytracing or raycasting.","An API may not be needed, however, to perform the techniques of this disclosure. For example, according to some aspects, a complier (e.g., a compiler program, such as a C\/C++ compiler) may map portions of program code to be executed by components of GPU . That is, the compiler may identify the appropriate components of GPU  to execute a given portion of program code, as well as specify a task that includes the given portion of code. The command processor  may then receive the mapped portions of code and schedule the tasks accordingly.",{"@attributes":{"id":"p-0077","num":"0076"},"figref":["FIG. 4","FIG. 4","FIGS. 1 and 3","FIG. 4"],"b":["100","56","56","56"]},"In the example method  of , command processor  initially receives one or more tasks, for example, from a host processor such as host processor  (). For example, command processor  may receive a command processor that includes one or more tasks defining work to be performed by components of GPU . Command processor  may parse the command stream to identify each task included in the command stream ().","Command processor  may also determine task priority (). For example, according to some aspects of this disclosure, command processor  may maintain one or more command queues that may determine when each of the received tasks are to be executed by the components of GPU . Command processor  may organize the one or more command queues based on task priority, such that more critical and\/or time sensitive tasks are executed prior to other tasks. In some examples, the component responsible for generating the task (e.g., host processor ) may be responsible for indicating a priority with the task. In other examples, command processor  may determine priority based on the tasks that are included in the command queue.","Command processor  may also determine resource availability (). For example, command processor  may determine whether the component(s) responsible for executing the task is available (e.g., not executing another task). Moreover, command processor  may determine whether the data associated with the task is available.","In addition to or instead of organizing the one or more command queues based on task priority, command processor  may organize the one or more command queues based on resource availability. That is, for example, command processor  may not schedule a task to be executed by a component of GPU  unless the component is available to execute the task. Moreover, command processor  may not schedule a task to be executed by a component of GPU  unless the data associated with the task is available. That is, command processor  may wait until the data associated with a particular task has been moved by memory copy engine  to SP memory  prior to scheduling the task for execution. Moreover, command processor  may ensure that the data associated with a particular task is not being modified by any other components of GPU  prior to scheduling the task.","Command processor  may then schedule a task for execution by one or more of the components of GPU  (). In some examples, according to aspects of this disclosure, command processor  may independently manage tasks such that components of GPU  may be executing different tasks in parallel. For example, command processor  may schedule a task to be executed by shader processor  in parallel with a task to be executed by fixed function units .","Upon scheduling a task, command processor  may update a list of active tasks (), i.e., an active task list. For example, command processor  may maintain a list of active tasks that are being executed by the components of GPU , as well as track the component responsible for executing the task. The active task list may be used, for example, to track the status of tasks. By tracking the status of tasks, command processor  may determine which resources of GPU  are available for scheduling tasks, and which resources of GPU  are busy executing tasks. That is, if a task is currently being executed by a GPU resource (e.g., the task is \u201cactive\u201d), that resource may not be available until the active task is completed. In an example, after initiating execution of a task by shader processor , command processor  may add the task to the list of active tasks that are being currently being executed by shader processor . In some examples, command processor  may be responsible for identifying which components of GPU  are actively executing tasks (e.g., by polling or otherwise determining that a component is busy). In other examples, command processor  may receive an indication that a particular task has been completed by the component responsible for executing the task, or by monitoring data being written to GPU memory .","It should also be understood that the steps shown and described with respect to  are provided as merely one example. That is, the steps of the method of  need not necessarily be performed in the order shown in , and fewer, additional, or alternative steps may be performed. For example,  shows task priority being determined prior to determining resource availability. In another example, determining resource availability may be performed prior to determining task priority.",{"@attributes":{"id":"p-0085","num":"0084"},"figref":["FIG. 5","FIG. 5","FIGS. 1 and 3","FIG. 5"],"b":["120","56","56","56","56"]},"In some examples, the method  may be performed instead of, or in conjunction with, the method  shown in the example of . For example, the method shown in the example of  may be performed when scheduling a task for execution (e.g., step  shown in ).","Command processor  initially determines whether one or more resources (e.g., including data, input\/output interfaces, memory, and\/or processing units) required to execute a current task are available (). If the resources are available (the yes branch of step ), command processor  may instruct data copy engine  to copy the appropriate data from GPU memory  to SP memory , if necessary (). In addition, command processor  may initiate task execution (). That is, command processor  may schedule the task to be executed by the appropriate component of GPU .","If the resources for executing the current task are not available (the no branch of step ), command processor  may determine whether the current task is the highest priority task for the resources required to execute the task (). If the current task is the highest priority task (the yes branch of step ), command processor  may preempt the task currently being executed by the resources (). That is, command processor  may interrupt operation of the resources required to execute the task so that the current task can be executed (e.g., GPU resources not required to execute the task may continue executing other tasks). After preempting the task currently being executed, command processor  may perform steps  and  to execute the current task, as described above. In some examples, the state of the preempted task may be stored (e.g., stored to GPU memory ) to allow the task to be resumed after the current task has been executed.","If the current task is not the highest priority task for the resources required to execute the task (the no branch of step ), command processor  may wait for the task currently being executed by the resources to finish executing (). Upon the resources completing the task (the yes branch of step ), command processor may execute the current task, for example, by performing steps  and  as described above.","It should also be understood that the steps shown and described with respect to  are provided as merely one example. That is, the steps of the method of  need not necessarily be performed in the order shown in , and fewer, additional, or alternative steps may be performed.","In addition, it should be understood that, depending on the example, certain acts or events of any of the methods described herein can be performed in a different sequence, may be added, merged, or left out all together (e.g., not all described acts or events are necessary for the practice of the method). Moreover, in certain examples, acts or events may be performed concurrently, e.g., through multi-threaded processing, interrupt processing, or multiple processors, rather than sequentially.","Moreover, in one or more examples, the functions described herein may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored on or transmitted over as one or more instructions or code on a computer-readable medium and executed by a hardware-based processing unit. Computer-readable media may include computer-readable storage media, which corresponds to a tangible medium such as data storage media, or communication media including any medium that facilitates transfer of a computer program from one place to another, e.g., according to a communication protocol.","In this manner, computer-readable media generally may correspond to (1) tangible computer-readable storage media which is non-transitory or (2) a communication medium such as a signal or carrier wave. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and\/or data structures for implementation of the techniques described in this disclosure. A computer program product may include a computer-readable medium.","By way of example, and not limitation, such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also, any connection is properly termed a computer-readable medium. For example, if instructions are transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium.","It should be understood, however, that computer-readable storage media and data storage media do not include connections, carrier waves, signals, or other transient media, but are instead directed to non-transient, tangible storage media. Disk and disc, as used herein, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and blu-ray disc where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.","Instructions may be executed by one or more processors, such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Accordingly, the term \u201cprocessor,\u201d as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition, in some aspects, the functionality described herein may be provided within dedicated hardware and\/or software modules configured for encoding and decoding, or incorporated in a combined codec. Also, the techniques could be fully implemented in one or more circuits or logic elements.","The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a wireless handset, an integrated circuit (IC) or a set of ICs (e.g., a chip set). Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units. Rather, as described above, various units may be combined in a codec hardware unit or provided by a collection of interoperative hardware units, including one or more processors as described above, in conjunction with suitable software and\/or firmware.","Various examples have been described. These and other examples are within the scope of the following claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 3","FIG. 1"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
