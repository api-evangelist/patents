---
title: Interaction models for indirect interaction devices
abstract: One or more techniques and/or systems are provided for utilizing input data received from an indirect interaction device (e.g., mouse, touchpad, etc.) as if the data was received from a direct interaction device (e.g., touchscreen). Interaction models are described for handling input data received from an indirect interaction device. For example, the interaction models may provide for the presentation of two or more targets (e.g., cursors) on a display when two or more contacts (e.g., fingers) are detected by indirect interaction device. Moreover, based upon a number of contacts detected and/or a pressured applied by respective contacts, the presented target(s) may be respectively transitioned between a hover visualization and an engage visualization. Targets in an engage visualization may manipulate a size of an object presented in a user interface, pan the object, drag the object, rotate the object, and/or otherwise engage the object, for example.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09367230&OS=09367230&RS=09367230
owner: Microsoft Technology Licensing, LLC
number: 09367230
owner_city: Redmond
owner_country: US
publication_date: 20111108
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Conventionally, users interact with elements of a user interface using a variety of input devices. Generally, these input devices can be classified as direct interaction devices and\/or indirect interaction devices based upon the type of interaction the device has with the user interface. For example, a direct interaction device, such as a touchscreen, is configured to enable a user to interact directly with what is displayed on a monitor, whereas an indirect interaction device, such as a mouse or keyboard, for example, is configured to enable a user to indirectly interact with what is displayed on a monitor.","A mouse and devices that emulate a mouse, such as a touchpad, for example, are some of the more popular types of indirect interaction devices. The user interacts with the device, and the interaction is mapped to a position on the display. For example, a mouse may sense movement, which is mapped to a position based upon a presumed starting position and the sensed interaction with the device. A touchpad is commonly used in a manner similar to a mouse. The motion of a contact (e.g., finger, stylus, etc.) on the touchpad is sensed, and the sensed motion is treated in a manner similar to a mouse input. Conventionally, mice and devices that emulate mice, such as touchpads, have been configured to control merely one target on the user interface (e.g., one mouse pointer, cursor, etc.). Thus, they may be referred to as single-point devices.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key factors or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","Among other things, one or more systems and\/or techniques for interacting with objects of a user interface via an indirect interaction device are provided. More particularly, the systems and\/or techniques describe various interaction models that are configured to substantially emulate a response that may occur if a user were to interact via a direct interaction device (e.g., via a touchscreen) when the user (instead) interacts with the indirect interaction device.","The interaction models are configured to provide for the visualization of one or more contacts (e.g., fingers) that interact with the indirect interaction device. For example, when two contacts interact with the indirect interaction device substantially concurrently, two targets (e.g., representations of the contacts on a display, such as mouse pointers, cursors, etc.) may be presented on a display of the computer system substantially concurrently. In this way, interaction with an indirect interaction device is visualized in a manner similar to the visualization of a direct interaction device (e.g., where a user could interact directly with the display such as where his\/her fingers overlay objects on the display).","Numerous interaction models are devised herein for substantially emulating a response that may occur if a user were to interact via a direct interaction device when the user actually interacts with the indirect interaction device. The particular interaction model that is used by a computer system may depend upon, among other things, user preferences, a size of the display in relation to a size of the indirect interaction device, developer preferences, etc. For example, in one embodiment, a user and\/or operating system may dynamically change the interaction model that is utilized depending upon, among other things, a user interface that is presented on the display.","The interaction models may generally fall into two categories, basic and advanced. The basic interaction models are generally designed to utilize at least two contacts (e.g., fingers, stylus, etc.) that can interact with (e.g., touch, hover over, etc.) the indirect interaction device. For example, the detection of a first contact by the indirect interaction device may be configured to cause a computer system (e.g., or a portion thereof) to enter a hover mode, where a target (e.g., a representation of the first contact that is presented on a display, such as a mouse pointer on a monitor) may be maneuvered on the display (e.g., via movement by the first contact on the indirect interaction device) until the target hovers overs over a desired object of the user interface (e.g., a particular file, menu, etc.). The detection of a second contact by the indirect interaction device may be configured to cause the computer system (e.g., or a portion thereof) to enter into an engage mode configured to engage an object on the display that the target is hovering over.","The advanced interaction models are generally designed to utilize merely one contact for determining when to enter hover mode and\/or engage mode. For example, in one embodiment, the indirect interaction device comprises one or more pressure sensors configured to determine an amount of pressure the contact is applying to the indirect interaction device. When the amount of pressure is within a specified range, the computer system (e.g., or a portion thereof) may be configured to enter a hover mode (e.g., to allow a mouse pointer to be moved around on a display). When the amount of pressure exceeds the specified range, the computer system (e.g., or a portion thereof) may be configured to enter an engage mode, for example (e.g., to allow a hovered-over button to be \u201cclicked\u201d).","In one embodiment of an advanced interaction model, a plurality of contacts may interact with the indirect interaction device. In such an embodiment, respective contacts may be configured to cause the computer system (e.g., or a portion thereof) to enter a hover and\/or engage mode. For example, a first contact associated with a first target on the display may be utilized to engage a first object (e.g., proximate the first target on the display), and a second contact associated with a second target on the display may be utilized to engage a second object (e.g., proximate the second target on the display). Thus, in advanced mode, contacts may control whether respective targets merely hover over an object or engage the object.","To the accomplishment of the foregoing and related ends, the following description and annexed drawings set forth certain illustrative aspects and implementations. These are indicative of but a few of the various ways in which one or more aspects may be employed. Other aspects, advantages, and novel features of the disclosure will become apparent from the following detailed description when considered in conjunction with the annexed drawings.","The claimed subject matter is now described with reference to the drawings, wherein like reference numerals are generally used to refer to like elements throughout. In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the claimed subject matter. It may be evident, however, that the claimed subject matter may be practiced without these specific details. In other instances, structures and devices are illustrated in block diagram form in order to facilitate describing the claimed subject matter.","It may be appreciated that in some applications direct interaction devices (e.g., utilizing touch screen technology) may be more advantageous than indirect interaction devices (e.g., utilizing cursor and key interfaces). For example, direct interaction devices may be well suited for point-of-sale devices, kiosk applications, GPS navigation devices, and\/or industrial and home automation systems, for example, (e.g., because transactions are relatively brief, data input\/user interaction is limited, etc.). In other applications, however, indirect interaction devices may be more advantageous. For example, graphic design, audio and video editing, spreadsheet manipulation, and word processing applications, etc. may be well served by indirect interaction devices (e.g., where substantial user interaction is sustained for extended periods of time). Moreover, direct touch devices may be somewhat imprecise as hands and fingers may occlude relatively small elements of a user interface, and may leave residue, smudges, etc. on a presentation surface of a display.","Among other things, one or more systems and\/or techniques are provided for utilizing an indirect interaction device (e.g., a touchpad or other input apparatus) in one or more manners that provide for at least some of the benefits of direct touch devices while also mitigating at least some of the deficiencies of direct touch devices. For example, interaction with an indirect interaction device is visualized in a manner similar to the visualization of a direct interaction device (e.g., where a user could interact directly with a display via a direct touch system such as where his\/her fingers overlay objects on the display). It will be appreciated that \u201cindirect interaction\u201d and\/or the like are used in a broad sense herein to describe a form of interaction where an entity (e.g., user) interacts with a device other than a display to cause a user interface on the display to change. For example, a touchpad of a laptop computer may be an example of an indirect interaction device because a user may interact with the touchpad to control a pointer that is presented on the display\/monitor of the laptop.","It will be appreciated that in some applications, a direct interaction device may be utilized as an indirect interaction device. As an example, where a mobile device comprising a touchscreen is utilized as a remote control for controlling a television, the touchscreen of the mobile device may be utilized as an indirect interaction device to manipulate a user interface (e.g., a target, cursor, etc. thereon) that is being displayed on the television, for example.",{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 1","b":["100","100","100"]},"The example method  begins at , and an indirect interaction device that has been coupled to a computer system is detected at . By way of example, a touchpad may be coupled to the computer system via a wired connection (e.g., universal serial bus (USB), etc.) and\/or a wireless connection (e.g., Bluetooth connection, infrared connection, etc.). Moreover, it will be appreciated that the indirect interaction device may be coupled to the computer system at the time the computer system is manufactured, and thus the indirect interaction device may be detected upon the initial boot-up of the computer system, for example. Alternatively, the indirect interaction device may be coupled to the computer system by the end-user, for example, and may be detected upon the indirect interaction device being coupled and\/or upon a subsequent reboot, for example.","Upon detection of the indirect interaction device, the computer system and\/or a component thereof may be configured to install drivers that are configured to provide for the transference of data between the indirect interaction device and the computer system and\/or otherwise prepare the computer system for interaction with the indirect interaction device and\/or vice-versa, for example.","The example method  also comprises mapping the indirect interaction device to the computer system at  based at least in part upon a selected interaction method. That is, stated differently touch sensors, heat sensors, or other types of sensors comprised on a sensory surface of the indirect interaction device are mapped to the display based upon a mapping technique specified by the selected interaction method (e.g., which may be selected by an operating system manufacturer, computer manufacturer, user, etc.). As an example, the interaction method may provide for mapping the indirect interaction device to the computer system using an absolute, relative, and\/or heterogeneous (e.g., a combination of absolute and relative) mapping technique(s).","Absolute mapping techniques are generally configured to scale an axis of the indirect interaction device to a corresponding axis of the display. For example, an x-axis of the indirect interaction device may be scaled with a corresponding x-axis of the display such that, in the x-direction, respective points on a sensory surface of the indirect interaction device map to merely one point (e.g., or a specific number of points) on the display. Similarly, a y-axis of the indirect interaction device may be scaled with a corresponding y-axis of the display such that, in the y-direction, respective points on the sensory surface of the indirect interaction device map to merely one point (e.g., or a specific number of points) on the display. In this way, points on the sensory surface generally correspond to respective points on the display. That is, for example, the edges of the sensory surface of the indirect interaction device correspond to the respective edges of the display, such that the sensory surface is a (miniature) representation of the display. For example, if a contact (e.g., finger) touches the top right corner of the sensory surface of an indirect interaction device, a pointer representing the contact on the display may be positioned in the top right corner because the top right corner of the sensory surface is mapped to the top right corner of the display. Conventionally, touchscreens and\/or pen digitizers have utilized absolute mapping techniques.","Relative mapping techniques are different than absolute mapping techniques in that the sensory surface is generally mapped to a moveable subregion of a display (e.g., which is not shown to a user). That is, the mapping is a function of a present location of a target (e.g., pointer) on the display at the time a contact (e.g., finger) is detected by the sensory surface and may change based upon a change in a location of the target. For example, where the target is centered in an upper right quadrant of the display when a contact is detected, the sensory surface may be mapped such that a center point of the sensory surface corresponds to a location of the target (e.g., causing less than all of the display to be mapped to a point on the sensory surface) (e.g., merely the upper right quadrant may be mapped, for example). If no contact is detected by the sensory surface for a period of time, when a contact is again detected, the mapping may be altered such that the position of the target is mapped to a center point of the sensory surface, for example. Thus, where the sensory surface is mapped using a relative mapping technique, the mapping may change from time-to-time (e.g., as opposed to absolute mapping, where the mapping typically does not change unless there is a change to the display and\/or to the sensory surface of the indirect interaction device). Conventionally, mice and\/or mice-emulating trackpads have utilized relative mapping techniques.","Heterogeneous mapping techniques combine the notion of absolute mapping and relative mapping. For example, respective axes of a sensory surface of the indirect interaction device may be mapped independently, with a first axis being mapped using an absolute mapping technique and a second axis being mapped using a relative mapping technique. For example, a relative mapping technique may be applied to the y-axis of the sensory surface and an absolute mapping technique may be applied to the x-axis of the sensory surface or vice-versa.","While absolute, relative, and heterogeneous mapping techniques are described herein, it will be appreciated that other mapping techniques are also contemplated and are not to be excluded from the scope of the appended claims, for example.","At  in the example method , input data is received from the indirect interaction device or from the sensory surface of the indirect interaction device. For example, a user may place one or more fingers on the sensory surface, causing input data to be generated. Generally speaking, the input data is indicative of a location(s) on the sensory surface that is being contacted by a contact(s). The input data may also be indicative of other information that may be acquired from the sensory surface.","By way of example, the input data may describe a location and\/or locations of one or more fingers in contact with the sensory surface so that actions of the fingers (e.g., movement about the sensory surface) can be mapped to the display based at least in part upon the map(s) created at  in the example method . Moreover, in one embodiment, the input data may further comprise information on an amount of pressure respectively applied to the sensory surface by the one or more fingers. As will be described in more detail below, in one embodiment, such pressure information may be used to determine whether to hover over and\/or engage an object of a user interface that is being displayed, for example.","At  in the example method , the input data is processed based at least in part upon a selected interaction model. The interaction models provide guidance and\/or instruction on how to handle the input data and\/or how to translate the input data into information that is more useful for the computer system. For example, as described above, the interaction model may provide for mapping the position and\/or movement of a contact(s) (e.g., an event) that are applied relative to the sensory surface of the indirect interaction device. Thus, if one or more contacts are moved to the right on the sensory surface, a target or targets (e.g., mouse pointer(s)) respectively corresponding the contacts, may be moved to the right on the user interface of the display.","As will be described in more detail below with respect to , the input data may be used to determine whether the computer system (e.g., or a portion thereof) is to enter a hover mode and\/or an engage mode according to specifications of the interaction model. That is, the interaction models may be configured to specify whether a target is to merely hover over an element(s) of a user interface of the computer system and\/or is to engage an element(s) of the user interface. Thus, the selection interaction mode may be configured to specify which actions of a contact(s) are configured to cause the computer system and\/or a portion thereof (e.g., such as a pointer that is presented on the display) to enter a hover mode and\/or which actions are configured to cause the computer system and\/or a portion thereof to enter into an engage mode (e.g., to engage an object of the user interface).","At  in the example method , a target(s) is positioned and\/or moved on a display of the computer system as provided for based upon the processed input data. Moreover, where the processed input data indicates that the computer system and\/or portion thereof is to enter an engage mode, the target that is displayed on the computer system may be configured to engage an object of a user interface presented on the display. Such engagement may include, but it not limited to manipulating a scale of an object presented on the display, panning an object, dragging an object, rotating an object, and\/or selecting an object (e.g., \u201cclick\u201d a button), for example.","It will be appreciated that more than one target may be presented on the display of the computer system at  in the example method . As an example, a target may be presented for respective contacts detected by the sensory surface of the indirect interaction device. Thus, if two or more fingers, for example, are applied relative to the sensory surface (e.g., such that two or more fingers are detected by the sensory surface), two or more targets may be presented on the display. Targets may be controlled by respective contacts, such that a first contact controls the position\/movement of the first target (e.g., and\/or causes the first target to switch between a hover mode and\/or an engage mode), a second contact controls the position\/movement of the second target (e.g., and\/or causes the second target to switch between a hover mode and\/or an engage mode), etc. Thus, there may be a one-to-one ratio between the number of contacts detected and the number of targets presented, although other ratios are also contemplated. For example, in another one target may be presented for every two contacts (e.g., fingers, styli, etc.) that are detected.","It will also be appreciated that in some embodiments, the number of targets presented on the display may change as the number of contacts that are detected change. As an example, in one embodiment (e.g., when an indirect interaction device is mapped absolutely), zero targets may be displayed when no contacts are detected, one target may be displayed when one contact is detected, two targets may be displayed when two contacts are detected, etc. In another embodiment (e.g., when the indirect interaction device is mapped relatively), one or more contacts may be displayed even when no contacts are detected. For example, one target may be displayed when zero or one contact is detected, two targets may be displayed when two contacts are detected, etc.","Thus, in one embodiment, the computer system may be configured to present a target on the display for respective contacts detected by the indirect interaction device. Moreover, the position\/movement of the targets may be controlled by a respective contact. That is, a first contact may control the position\/movement of a first target, a second contact may control the position\/movement of a second target, etc. In one embodiment, respective targets may be positioned\/moved independent of the other targets. For example, if a user were to move a first finger on a touchpad while keeping another finger stationary, a target corresponding to the first finger may move while a target corresponding to the second finger remains stationary. Similarly, if a user picks up and\/or otherwise ceases to \u201ccontact\u201d the sensory surface with the second finger the second target may disappear and\/or be presented in a different manner on the display.","It will be appreciated that by displaying a target for respective contacts that are detected by the sensory surface of the indirect interaction device, movements of the user may be visualized in a manner similar to the visualization that occurs on a direct interaction device, for example. That is, respective targets may respond in a manner similar to fingers touching a touchscreen, for example. Just as a user could place two fingers on a direct interaction device (e.g., interacting with and\/or covering up multiple objects on the user interface), the user may visualize his\/her fingers on the display (e.g., where his\/her fingers are represented by the targets) even though the user is not in physical contact with the display (e.g., but is rather controlling the targets via an indirect interaction device), for example.","The example method  ends at .",{"@attributes":{"id":"p-0041","num":"0040"},"figref":["FIG. 2","FIG. 1","FIG. 2","FIGS. 3-4"],"b":["200","200","110","200"]},"The example method  begins at , and first input data indicative of a first event detected by an indirect interaction device is received at . The event is generally performed by a contact(s), and may comprise, for example, touching a contact (e.g., finger, stylus, etc.) to a sensory surface of the indirect interaction device, moving the contact on the sensory surface, etc. As an example, the first event may comprise detecting a first finger on and\/or adjacent to the sensory surface (e.g., touch sensitive surface, heat sensitive surface, etc.) of the indirect interaction device and\/or detecting movement of the first finger on sensory surface. Such an event may then be recorded and\/or translated into input data (e.g., by the indirect interaction device) that is configured to be received by the computer system.","At  in the example method  a first target is provided to be presented on a display of the computer system data based upon the first input. That is, stated differently, the first event is mapped to the display according to mapping technique (e.g., which may be specified in the selected interaction model). For example, the mapping technique may comprise an absolute mapping technique, a relative mapping technique, and\/or a heterogeneous mapping technique. As an example, if the first event comprises sliding a finger that is contacting the sensory surface to the right, mapping the first event to the display may comprise moving a target (e.g., pointer) to the right.","Generally speaking, at , the target is presented as a hover visualization. That is, the target may maneuver about the display and hover over objects of the user interface without engaging the objects. For example, the target may maneuver over applications without selecting the applications, initializing the applications, and\/or manipulating the applications (e.g., or rather representations of the applications presented on the display). In this way, the target behaves similar to a traditional pointer (e.g., where the pointer can hover over the user interface until a user taps or clicks a button on a mouse, for example).","At  in the example method , second input data indicative of a second event that is detected by the indirect interaction device is received at . The second event may comprise, among other things, detecting a change in an amount of pressure applied by the contact (e.g., relative to the pressure applied when the first event was detected), detecting a second contact on and\/or near the sensory surface, and\/or detecting the engagement of a button or other engagement mechanism (e.g., that is part of the indirect interaction device) (e.g., configured to cause a transition of the computer system from a hover mode to an engage mode).","At  in the example method , one or more objects presented on the display are engaged based at least in part upon a position of the first target relative to the object and the received second data. That is, upon receipt of the second input data, one or more targets may transition from a hovering visualization (e.g., where the one or more targets merely hover over an object) to an engage visualization (e.g., where the one or more targets engage the object that the respective targets were hovering over).","For example, suppose a picture is displayed within the user interface that is presented on a computer system, and a user desires to interact with the picture using an indirect interaction device (e.g., a touchpad). The user may begin by placing one finger on a sensory surface of the touchpad and moving the finger on the sensory surface. This event (e.g., the placement of the finger on the sensory surface and\/or the movement of the finger) may cause first input data to be generated at  by the indirect interaction device, for example, and transmitted to the computer system. The computer system may in turn, analyze the first input data at  (e.g., mapping the event to the display) and provide information to the display that causes the target to be positioned and\/or moved in a manner that substantially corresponds with the placement and\/or movement of the finger. Once the target is located at a position that is desirable to the user (e.g., is hovering over an object (e.g., the picture) the user desires to engage), the user may apply more pressure to the sensory surface via the first finger, place a second finger on the sensory surface, and\/or select an engage element (e.g., button on the indirect interaction device). In response to such an event, second input data may be generated by the indirect interaction device and\/or transmitted to the computer system. Based upon second input data (e.g., indicative of a second event), and the position of the target at the time the second event occurs, an object (e.g., the picture) of the user interface that is proximate to the target (e.g., at least partially overlapped by the target) may be engaged (e.g., the picture may be \u201cgrabbed\u201d so that it can be dragged and dropped, for example).","It may be appreciated that engage and\/or the like is used herein in a broad sense to describe one or more actions\/manipulations that may occur to an object. For example, the scale of an object may be manipulated, an object may be panned, an object may be dragged to a different location, an object may be rotated, and\/or an object may be otherwise selected (e.g., causing an application associated with the object to be initialized, launched, etc. and\/or the user interface to change based upon the engagement with the object (e.g., a menu associated with the object may open)), etc.","It will also be appreciated that particular action(s) that are configured to transition the computer system and\/or a portion thereof (e.g., a particular target) from a hover visualization to an engage visualization may be provided for in the specified interaction model. For example, as will be described in more detail below, in one embodiment, the detection of a second contact may transition a target associated with the first contact from a hover visualization to an engage visualization. However, in another example the detection of a second contact may merely cause targets to be displayed, where respective targets are in a hover visualization (e.g., and thus the mere detection of a second contact does not automatically cause one or more targets to transition from a hover visualization to an engage visualization).","Moreover, it will be appreciated that there may be intervening events between the first and second events which do not cause an object to be engaged. For example, a third event may occur between the first and second events, such as a movement of a contact relative to the sensory surface, that causes the target to move but does not necessarily cause an object to be engaged (e.g., because the third event was not a type of event specified in the interaction model to cause a target to transition from a hover mode to an engagement mode).",{"@attributes":{"id":"p-0051","num":"0050"},"figref":["FIG. 3","FIG. 3"],"b":["300","302","304","306"]},"Typically, the computer system and\/or portion thereof is in idle mode  when a sensory surface of an indirect interaction device detects zero contacts (e.g., fingers) being applied relative to the sensory surface of the indirect interaction device. The number of targets that are displayed when the computer system and\/or portion thereof is in idle mode may depend upon, among other things, the type of interaction model that is specified to be used by the computer system. For example, where the events are to be mapped relatively, at least one target may be presented on the display while the computer system (e.g., or a portion thereof) is in idle mode to provide some context (e.g., to a user) of where the target will move if the user interacts with the sensory surface to move the contact to the left and\/or right of center, for example.","When events are mapped absolutely (e.g., when the sensory surface is mapped to the display absolutely) (e.g., such that a point on the sensory surface is mapped to a corresponding point on the display that is substantially fixed), zero or more targets may be presented on the display while the computer system and\/or a portion thereof is in idle mode . That is, stated differently, a user may not need to be aware of the current placement of the target when the computer system is in idle mode  because the user may be aware of where the target will appear when a contact is applied relative the sensory surface without such a priori information (e.g., the target will appear in an area of the display that substantially corresponds to the area of the sensory surface that is touched).","When a first contact is applied relative to the sensory surface of the indirect interaction device (e.g., when a first finger touches a touchpad), the computer system and\/or a portion thereof may transition from idle mode  to hover mode  as illustrated by the arrow  and\/or a target may be displayed in a hover visualization. Hover mode  is configured to provide a user with a visualization of a target hovering over an object(s) of a user interface that is presented on the display. As an example, the user may maneuver the contact on the sensory surface to cause the target to move about the screen, causing it to appear as though the target is hovering over (e.g., but not engaging) objects. That is, stated differently, a target may overlap one or more objects of the presented user interface without engaging the objects. It will be appreciated that hover mode  may be similar to using a mouse, where the pointer is free to move about the user interface without engaging aspects of the user interface (e.g., unless a button is selected, at which time it is no longer hovering over objects). It may be appreciated, however, that merely hovering over an element may cause the element \u201creact\u201d (e.g., luminesce, change color, etc.) even though the element is not engaged.","When a second contact is applied relative to the sensory surface (e.g., when a second finger touches the touchpad) substantially currently with the first contact (e.g., such that two fingers are in contact with the touchpad), the computer system and\/or a portion thereof may transition from hover mode  to engage mode  as illustrated by arrow  and\/or the target may be displayed in an engage visualization. It will be appreciated that in one embodiment, the computer system (e.g., or a portion thereof) may be configured to display a target for respective contacts that are applied relative to the sensory surface. For example, where two contacts are applied relative to the sensory surface, two targets may be presented on the display; where three contacts are applied relative to the sensory surface, three targets may be presented on the display, etc.","Engage mode  is configured to cause the targets to engage objects that are proximate to the target(s) (e.g., to engage objects that are respectively at least partially overlapped by the targets). Engaging an object may involve, among other things, manipulating the scale of an object, panning an object, dragging an object, rotating an object, and\/or otherwise selecting an object to cause some change in the object (e.g., initializing an application associated with the object and\/or providing for the presentation of a menu associated with the object, etc.).","By way of example and not limitation, moving the two or more contacts towards one another on the sensory surface may cause the scale of the object relative to the size of the display to decrease. Similarly, moving the two or more contacts apart from one another on the sensory surface may cause the scale of the object relative to the size of the display to increase. Moving two or more contacts that are adjacent to one other may result in a pan manipulation if the object is a pannable object and\/or may result in a drag manipulation if the object is draggable. Moving the two or more fingers in a rotating fashion may cause an object to be rotated in a manner similar to the rotation of the contacts. Thus, in a basic model , two or more contacts may be used to engage an object and\/or provide for the manipulation of the object (e.g., if input data is indicative of two or more contacts being detected by the indirect interaction device, the target(s) are transitioned from a hover mode to an engage mode).","As illustrated by arrow  in the example model , it will be appreciated that in one embodiment, the computer system and\/or a portion thereof may transition directly from idle mode  to engage mode  (e.g., bypassing hover mode ). As an example, the input data may be indicative of the sensory surface transitioning from detecting zero contacts to detecting two or more contacts, thus causing the computer system to bypass hover mode .","Moreover, it will be appreciated that the computer system and\/or portion thereof can be transitioned back to hover mode  if it is in engage mode  and\/or back to idle mode  if it is in hover mode  and\/or engage mode  (e.g., using a reverse technique). For example, when the sensory surface transitions from detecting two or more contacts to merely detecting one contact, the computer system and\/or portion thereof may be transitioned from engage mode  to hover mode  as illustrated by arrow . Similarly, when the sensory surface transitions from detecting one contact to detecting zero contacts, the computer system and\/or portion thereof may be transitioned from hover mode  to idle mode  as illustrated by arrow . If the sensory surface transitions from detecting two or more contracts to detecting zero contacts, the computer system and\/or portion thereof may be transitioned from engage mode  to idle mode  (e.g., bypassing hover mode ) as illustrated by arrow .",{"@attributes":{"id":"p-0060","num":"0059"},"figref":["FIG. 4","FIG. 3","FIG. 3","FIG. 3","FIG. 4"],"b":["400","402","302","404","304","406","306"]},"Typically, the computer system and\/or portion thereof is in idle mode  when a sensory surface of an indirect interaction device detects zero contacts (e.g., fingers) being applied relative to the sensory surface of the indirect interaction device. The number of targets that are displayed when the computer system and\/or portion thereof is in idle mode may depend upon, among other things, the type of interaction model that is specified to be used by the computer system. For example, where the events are to be mapped relatively, at least one target may be presented on the display while the computer system (e.g., or a portion thereof) is in idle mode to provide some context (e.g., to a user) of where the target will move if the user interacts with the sensory surface to move the contact to the left and\/or right of center, for example.","When events are mapped absolutely (e.g., when the sensory surface is mapped to the display absolutely) (e.g., such that a point on the sensory surface is mapped to a corresponding point on the display that is substantially fixed), zero or more targets may be presented on the display while the computer system and\/or a portion thereof is in idle mode . That is, stated differently, a user may not need to be aware of the current placement of the target when the computer system is in idle mode  because the user may be aware of where the target will appear when a contact is applied relative the sensory surface without such a priori information (e.g., the target will appear in an area of the display that substantially corresponds to the area of the sensory surface that is touched).","When a first contact is applied relative to the sensory surface of the indirect interaction device (e.g., when a first finger touches a touchpad) an amount of pressure that is applied by the first contact may be determined. If the amount of pressure is within a specified range, the computer system (e.g., or a portion thereof) may transition from idle mode  to hover mode  as illustrated by the arrow  and\/or a target may be displayed in a hover visualization. If the amount of pressure that is applied relative to the sensory surface is greater than and\/or less than the specified range, the computer system (e.g., or a portion thereof) may transition from idle mode  to engage mode  (e.g., bypassing the hover mode ) as illustrated by the arrow .","When there is a change in a manner through which the first contact is being applied relative to the sensory surface (e.g., when the pressure increases and\/or decreases by a specified threshold), the computer system and\/or a portion thereof) may transition from hover mode  to engage mode  as illustrated by arrow .","Engage mode  is configured to cause the targets to engage objects that are proximate to the target(s) (e.g., to engage objects that are respectively at least partially overlapped by the targets). Engaging an object may involve, among other things, manipulating the scale of an object, panning an object, dragging an object, rotating an object, and\/or otherwise selecting an object to cause some change in the object (e.g., initializing an application associated with the object and\/or providing for the presentation of a menu associated with the object).","It will be appreciated that while the basic model  illustrated in  generally utilized two contacts to enter engage mode  and\/or manipulate an object, the advanced model  may utilized merely one contact (e.g., finger) to enter engage mode . By way of example and not limitation, moving a contact in one direction and\/or pattern while in the engage mode may cause the scale of the object relative to the size of the display to decrease while moving the contact in an opposite direction and\/or pattern may cause the scale of the object relative to the size of the display to increase. Similarly, moving the contact in yet another direction may result in a pan manipulation and\/or a drag manipulation of the object. Rotating the contact on the sensory surface may cause the object to be rotated in a manner similar to the rotation of the contact. Thus, in an advanced model , merely one contact may be used to engage an object and\/or provide for the manipulation of the object.","The computer system and\/or portion thereof can be transitioned back to hover mode  if it is in engage mode  and\/or back to idle mode  if it is in hover mode  and\/or engage mode  (e.g., using a reverse technique). For example, when the pressure applied by the contact is brought back within the specified range, the computer system or portion thereof may be transitioned from engage mode  to hover mode  as illustrated by arrow . If the pressure applied drops below a specified threshold (e.g., drops to substantially zero) while the computer system and\/or portion thereof is in hover mode , the computer system and\/or portion thereof may transition to idle mode  as illustrated by arrow . Similarly, the computer system and\/or a portion thereof can transition from engage mode  to idle mode  (e.g., bypassing hover mode ) if the pressure drops below a specified threshold (e.g., drops to substantially zero) while the system is in engage mode  as illustrated by arrow .","It will be appreciated that while the basic model  described with respect to  and the advanced model  described with respect to  provide for making the transition between modes based upon particular events (e.g., the detection of multiple contacts and\/or a change in pressure applied by a contact), other techniques for determining when to transition between various modes are also contemplated. For example, in one embodiment, the transition between modes in the basic model and\/or advanced model can be initiated based upon input data indicating that an engage button and\/or switch, for example, has been activated (e.g., on the indirect interaction device). Thus, for example, a transition from hover mode to engage mode may occur when the received input data is indicative of a single contact being detected on the sensory surface and is indicative of an engage button being selected (e.g., depressed) on the indirect interaction device.","Moreover, it will be appreciated that while the basic model  referenced merely two contacts, more than two contacts may be applied relative to the sensory surface of the indirect interaction device. Similarly, more than one contact may be applied relative to the sensory surface of the indirect interaction device when the advanced model  is specified (e.g., by a computer manufacturer, user, etc.). As an example, in an advanced model, respective contacts can independently transitioned from hover mode to engage mode (e.g., and therefore can independently engage the same and\/or different objects within the presented user interface). For example, when an increased pressure is applied by a first contact, a first target corresponding to the first contact may be transitioned from hover mode to engage mode while a second target corresponding to a second contact that does not increase the pressure applied relative to the sensory surface may remain in a hover mode (e.g., and may continue to hover over objects while the first target engages an object). In another example, the multiple targets are concurrently transitioned between modes. For example, when an engage button is selected, a plurality of targets (e.g., corresponding to a respective contact that is applied relative to the sensory surface) may transition from hover mode to engage mode (e.g., causing objects proximate respective targets to be engaged). For example, multiple applications can be launched, multiple images may \u201cgrabbed\u201d, etc.",{"@attributes":{"id":"p-0070","num":"0069"},"figref":["FIG. 5","FIG. 5"],"b":["500","504","500","504","514","502"]},"The example system  comprises an input receiver component  that may be configured to receive input from the indirect interaction device . As an example, the indirect interaction device  (e.g., which may comprise a touch sensitive surface, such as a touchpad, a heat sensitive surface, etc.) may be configured to detect particular types of events and\/or translate those events into input data. It may be appreciated that event and\/or the like is used herein in a broad sense to describe, among other things, the detection of one or more contacts, movement of contacts, changes in the contacts (e.g., such as pressure changes of respective contacts), etc. For example, the indirect interaction device  may be configured to detect one or more contacts that are applied relative to a sensory surface and to generate input data based upon the detection of the one or more contacts. Such input data may provide information regarding (e.g., indicative of), among other things, a location of the contact(s) applied relative to the sensory surface, an amount of pressure applied by respective contacts, etc. Where the contacts are moved relative to the sensory surface, the input data may also indicate such movement, for example.","The example system  also comprises a mapping component  that is configured to map events that are provided for in the input data to the display  using one or more mapping techniques (e.g., absolute mapping techniques, relative mapping techniques, and\/or heterogeneous mapping techniques (e.g., as described above)). It will be appreciated that the particular mapping technique that is utilized may be provided for in user specified, manufacture specified, etc. interaction models. For example, in one embodiment, an operating system manufacture may specify that the system  is to map events that occur on the indirect interaction device  to the display  absolutely (e.g., the manufacture selects to user a basic, absolute interaction model and\/or an advanced, absolute interaction model). In another embodiment, a user of the system  may specify an interaction model (e.g., thus specifying whether events are to be mapped relatively, absolutely, and\/or heterogeneously).","The example system  also comprises a presentation component  configured to provide one or more targets for presentation on the display  based at least upon the input data received by the input receiver component  and the selected interaction model. More particularly, the presentation component is configured to insert one or more targets into a user interface that is presented on the display . The position of the one or more targets and\/or movement of the one or more targets relative to the boundaries of the display , for example, may be determined by the mapping component  based upon the received input data.","The presentation component  may also be configured to transition the computer system and\/or a portion thereof (e.g., respective targets) between an idle mode (e.g.,  in ), hover mode (e.g.,  in ), and\/or engage mode (e.g.,  in ), for example, based at least in part upon the received input data and\/or the specific interaction model.","By way of example, a basic interaction model (e.g.,  in ), specifying particular events that may cause the computer system and\/or portion thereof to transition between modes, may be utilized by the presentation component  to determine when to transition between modes. As an example, the basic interaction model may provide that when the input data received by the input receiver component  is indicative of at least two contacts being applied relative to a sensory surface of the indirect interaction device , the presentation component  is to transition the target(s) from a hover visualization (e.g., where the targets are configured to hover over objects of the user interface) to an engage visualization (e.g., where the targets are configured to engage objects of the user interface). As another example, the basic interaction model may provide that when the input data received by the input receiver component  is indicative of at least one contact being applied relative to the sensory surface of the indirect interaction device  and indicative of an intention to engage (e.g., depression of an engage button), the presentation component  is to transition the target(s) from a hover visualization to an engage visualization. Thus, when the input data is indicative of two contacts being applied and\/or indicative of one contact being applied and indicative of an intention to engage, the presentation component  may be configured to transition the target from a hover visualization to an engage visualization (e.g., or from an idle mode to an engage mode if the target was not previously in a hover mode). Similarly, when the input data is indicative of merely one contact and\/or is not indicative of an intention to engage, the presentation component  may be configured, for example, to transition the target from engage visualization to a hover visualization (e.g., or from an idle mode to a hover mode if the target was previously in an idle mode).","As another example, an advanced interaction model (e.g.,  in ), specifying particular events that may cause the computer system and\/or portion thereof to transition between modes, may be utilized by the presentation component  to determine when to transition between modes. As an example, the advanced interaction model may provide that when the input data received by the input receiver component  is indicative an increase in the pressure applied by a contact, the presentation component  is to transition the target associated with the contact from a hover visualization to an engage visualization. As another example, the advanced interaction model may provide that when the input data received by the input receiver component  is indicative of at least one contact being applied relative to the sensory surface of the indirect interaction device  and indicative of an intention to engage (e.g., depression of an engage button), the presentation component  is to transition the target(s) from a hover visualization to an engage visualization. Thus, when the input data is indicative of a change in pressure applied to the sensory surface by one or more contacts and\/or indicative of one contact being applied and indicative of an intention to engage, the presentation component  may be configured to transition the target from a hover visualization to an engage visualization (e.g., or from an idle mode to an engage mode if the target was not previously in a hover mode). Similarly, when the input data is indicative of a contact that is applying less than a specified amount of pressure and\/or is not indicative of an intention to engage, the presentation component  may be configured, for example, to transition the target from engage visualization to a hover visualization (e.g., or from an idle mode to a hover mode if the target was previously in an idle mode).","It will be appreciated that in one embodiment, the presentation component  may also be configured to provide for the presentation of at least two targets on the display  when the input data received by the input receiver component  is indicative of two or more contacts being applied relative to a sensory surface of the indirect interaction device  (e.g., indicative of two or more contacts being detected by the sensory surface). As an example, if the input data is indicative of two contacts being applied, the presentation component  may be configured to provide for the presentation of two targets on the display ; if the input data is indicative of three contacts being applied, the presentation component  may be configured to provide for the presentation of three targets on the display; etc. It will be appreciated that in another embodiment, the presentation component  may be configured to provide for some different ratio of targets to contacts detected other than the one-to-one ratio described above. For example, in another embodiment, the presentation component  may be configured to provide for the presentation of one target per two contacts detected by the sensory surface of the indirect interaction device , etc.","Where the presentation component  provides for the presentation of a plurality of targets, it will be appreciated that the targets may function independently of one another. For example, where the targets correspond to a respective contact, movement of a first contact on the sensory surface may cause movement of a first, corresponding target on the display while targets associated with other, non-moving contacts remain stationary. Moreover, in one embodiment, the presentation component  may be configured to transition respective targets between hover and engage modes independently. For example, in one embodiment (e.g., such as where the presentation component  utilizes an advanced interaction model), respective targets can transition between hover and engage modes based upon the pressure applied to the sensory surface by respective contacts. For example, a first contact, corresponding to a first target, may apply a pressure that exceeds a specified threshold, causing the first target to enter an engage mode. Concurrently, a second contact, corresponding to a second target, may apply a pressure that is less than a specified threshold, causing the second target to enter\/remain in a hover mode.","The example system  also comprises an engagement component  configured to provide for the engagement of an object in a user interface presented on the display  based upon the target entering engage mode (e.g., based upon an event(s) specified in the interaction model that is being used). As an example, the engagement component may be configured to manipulate that size of an object the target is engaging (e.g., by at least partially overlapping the object), pan the object, drag the object, rotate the object, and\/or otherwise manipulate the object.","Still another embodiment involves a computer-readable medium comprising processor-executable instructions configured to implement one or more of the techniques presented herein. An exemplary computer-readable medium that may be devised in these ways is illustrated in , wherein the implementation  comprises a computer-readable medium  (e.g., a CD-R, DVD-R, or a platter of a hard disk drive), on which is encoded computer-readable data . This computer-readable data  in turn comprises a set of computer instructions  configured to operate according to one or more of the principles set forth herein. In one such embodiment , the processor-executable computer instructions  may be configured to perform a method , such as at least some of the exemplary method  of  and\/or the exemplary method  of , for example. In another such embodiment, the processor-executable instructions  may be configured to implement a system, such as at least some of the exemplary system  of , for example. Many such computer-readable media  may be devised by those of ordinary skill in the art that are configured to operate in accordance with the techniques presented herein.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims.","As used in this application, the terms \u201ccomponent,\u201d \u201cmodule,\u201d \u201csystem\u201d, \u201cinterface\u201d, and the like are generally intended to refer to a computer-related entity, either hardware, a combination of hardware and software, software, or software in execution. For example, a component may be, but is not limited to being, a process running on a processor, a processor, an object, an executable, a thread of execution, a program, and\/or a computer. By way of illustration, both an application running on a controller and the controller can be a component. One or more components may reside within a process and\/or thread of execution and a component may be localized on one computer and\/or distributed between two or more computers.","Furthermore, the claimed subject matter may be implemented as a method, apparatus, or article of manufacture using standard programming and\/or engineering techniques to produce software, firmware, hardware, or any combination thereof to control a computer to implement the disclosed subject matter. The term \u201carticle of manufacture\u201d as used herein is intended to encompass a computer program accessible from any computer-readable device, carrier, or media. Of course, those skilled in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.",{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIG. 7","FIG. 7"]},"Although not required, embodiments are described in the general context of \u201ccomputer readable instructions\u201d being executed by one or more computing devices. Computer readable instructions may be distributed via computer readable media (discussed below). Computer readable instructions may be implemented as program modules, such as functions, objects, Application Programming Interfaces (APIs), data structures, and the like, that perform particular tasks or implement particular abstract data types. Typically, the functionality of the computer readable instructions may be combined or distributed as desired in various environments.",{"@attributes":{"id":"p-0086","num":"0085"},"figref":["FIG. 7","FIG. 7"],"b":["710","712","712","716","718","718","714"]},"In other embodiments, device  may include additional features and\/or functionality. For example, device  may also include additional storage (e.g., removable and\/or non-removable) including, but not limited to, magnetic storage, optical storage, and the like. Such additional storage is illustrated in  by storage . In one embodiment, computer readable instructions to implement one or more embodiments provided herein may be in storage . Storage  may also store other computer readable instructions to implement an operating system, an application program, and the like. Computer readable instructions may be loaded in memory  for execution by processing unit , for example.","The term \u201ccomputer readable media\u201d as used herein includes computer storage media. Computer storage media includes volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions or other data. Memory  and storage  are examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, Digital Versatile Disks (DVDs) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by device . Any such computer storage media may be part of device .","Device  may also include communication connection(s)  that allows device  to communicate with other devices. Communication connection(s)  may include, but is not limited to, a modem, a Network Interface Card (NIC), an integrated network interface, a radio frequency transmitter\/receiver, an infrared port, a USB connection, or other interfaces for connecting computing device  to other computing devices. Communication connection(s)  may include a wired connection or a wireless connection. Communication connection(s)  may transmit and\/or receive communication media.","The term \u201ccomputer readable media\u201d may include communication media. Communication media typically embodies computer readable instructions or other data in a \u201cmodulated data signal\u201d such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d may include a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.","Device  may include input device(s)  such as keyboard, mouse, pen, voice input device, touch input device, infrared cameras, video input devices, and\/or any other input device. Output device(s)  such as one or more displays, speakers, printers, and\/or any other output device may also be included in device . Input device(s)  and output device(s)  may be connected to device  via a wired connection, wireless connection, or any combination thereof. In one embodiment, an input device or an output device from another computing device may be used as input device(s)  or output device(s)  for computing device .","Components of computing device  may be connected by various interconnects, such as a bus. Such interconnects may include a Peripheral Component Interconnect (PCI), such as PCI Express, a Universal Serial Bus (USB), firewire (IEEE 1394), an optical bus structure, and the like. In another embodiment, components of computing device  may be interconnected by a network. For example, memory  may be comprised of multiple physical memory units located in different physical locations interconnected by a network.","Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network. For example, a computing device  accessible via a network  may store computer readable instructions to implement one or more embodiments provided herein. Computing device  may access computing device  and download a part or all of the computer readable instructions for execution. Alternatively, computing device  may download pieces of the computer readable instructions, as needed, or some instructions may be executed at computing device  and some at computing device .","Various operations of embodiments are provided herein. In one embodiment, one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media, which if executed by a computing device, will cause the computing device to perform the operations described. The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. Alternative ordering will be appreciated by one skilled in the art having the benefit of this description. Further, it will be understood that not all operations are necessarily present in each embodiment provided herein.","Moreover, the word \u201cexemplary\u201d is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as \u201cexemplary\u201d is not necessarily to be construed as advantageous over other aspects or designs. Rather, use of the word exemplary is intended to present concepts in a concrete fashion. As used in this application, the term \u201cor\u201d is intended to mean an inclusive \u201cor\u201d rather than an exclusive \u201cor\u201d. That is, unless specified otherwise, or clear from context, \u201cX employs A or B\u201d is intended to mean any of the natural inclusive permutations. That is, if X employs A; X employs B; or X employs both A and B, then \u201cX employs A or B\u201d is satisfied under any of the foregoing instances. In addition, the articles \u201ca\u201d and \u201can\u201d as used in this application and the appended claims may generally be construed to mean \u201cone or more\u201d unless specified otherwise or clear from context to be directed to a singular form. Also, at least one of A and B or the like generally means A or B or both A and B.","Although the disclosure has been shown and described with respect to one or more implementations, equivalent alterations and modifications will occur to others skilled in the art based at least in part upon a reading and understanding of this specification and the annexed drawings. The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims. In particular regard to the various functions performed by the above described components (e.g., elements, resources, etc.), the terms used to describe such components are intended to correspond, unless otherwise indicated, to any component which performs the specified function of the described component (e.g., that is functionally equivalent), even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations of the disclosure. In addition, while a particular feature of the disclosure may have been disclosed with respect to only one of several implementations, such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Furthermore, to the extent that the terms \u201cincludes\u201d, \u201chaving\u201d, \u201chas\u201d, \u201cwith\u201d, or variants thereof are used in either the detailed description or the claims, such terms are intended to be inclusive in a manner similar to the term \u201ccomprising.\u201d"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
