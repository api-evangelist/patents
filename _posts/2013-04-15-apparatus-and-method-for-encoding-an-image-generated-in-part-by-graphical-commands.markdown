---
title: Apparatus and method for encoding an image generated in part by graphical commands
abstract: A method and apparatus for encoding an image is disclosed. In one embodiment, the method comprises identifying initial pixels within a spatially defined sub-section, the initial pixels at least a defined number of pixels each comprising a first color; identifying background pixels, the background pixels comprising the first color and in a first defined spatial proximity to the initial pixels; identifying text pixels, the text pixels contrasting the first color and in a second defined spatial proximity to the background pixels; identifying picture pixels as all pixels other than the background pixels and the text pixels; generating a background encoding comprising (i) spatial locations of the background pixels and (ii) a lossless encoding of the first color; generating a text encoding identifying a spatial location and a lossless color encoding of each of the text pixels; and generating a picture encoding comprising a lossy encoding of the picture pixels.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08855414&OS=08855414&RS=08855414
owner: Teradici Corporation
number: 08855414
owner_city: Burnaby, B.C.
owner_country: CA
publication_date: 20130415
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"p":["This application is a continuation of co-pending U.S. patent application Ser. No. 12\/825,092, filed Jun. 28, 2010, which is a continuation-in-part of (i) U.S. patent application Ser. No. 11\/173,303, filed Jun. 30, 2005, which claims benefit of U.S. provisional patent application Ser. No. 60\/584,869 filed Jun. 30, 2004, and is also a continuation-in-part of (ii) U.S. patent application Ser. No. 11\/333,955, filed Jan. 17, 2006, which claims benefit of U.S. provisional patent application Ser. No. 60\/703,767, filed Jul. 28, 2005. Each of the aforementioned related patent applications is herein incorporated by reference in its entirety.","1. Field of the Invention","The present invention relates broadly to encoding computer display images for communications across a network. Specifically, the present invention relates to applying decomposition methods, compressing and transmitting images rendered by a processing system. More specifically, the present invention relates to display images in a frame buffer that are accessed, compressed and transmitted in priority sequence with the aid of multi-layer image decomposition performed by the processing system in conjunction with drawing command hints issued by a processor.","2. Description of the Related Art","Masked wavelets have been used to improve the compression of natural images with superimposed text or lines images, as might be characteristic of a computer display image that requires compression in order for it to be transmitted to a remote system. Generally, some form of pixel-level image filter is applied to the image in order for select areas of the image which are better suited to alternative encoding methods are designated as \u201cdon't care\u201d regions, and these areas are excluded from the wavelet compression process.","However, pixel level pre-processing of an image to determine its characteristics prior to encoding is a processing intensive task, especially when performed by computing resources also tasked with maintaining a high quality user experience by servicing latency-sensitive functions such as image rendering and application software processing.","Therefore, there is a need in the art for a system and method for improving the performance of image decomposition in such a manner as to overcome degradation in the user experience.","Embodiments of the present invention generally relate to a method and apparatus for encoding images. In one embodiment, the method comprises identifying initial pixels within a spatially defined sub-section, the initial pixels at least a defined number of pixels each comprising a first color; identifying background pixels, the background pixels comprising the first color and in a first defined spatial proximity to the initial pixels; identifying text pixels, the text pixels contrasting the first color and in a second defined spatial proximity to the background pixels; identifying picture pixels as all pixels other than the background pixels and the text pixels; generating a background encoding comprising (i) spatial locations of the background pixels and (ii) a lossless encoding of the first color; generating a text encoding identifying a spatial location and a lossless color encoding of each of the text pixels; and generating a picture encoding comprising a lossy encoding of the picture pixels.","The present invention discloses a system and method for preparation of a computer display image for efficient encoding so that the encoded computer display may be transmitted across the network and accurately reproduced at the remote computer. Embodiments of the present invention decompose a computer display image into different layer types and associated masks based on the unique nature of the image. These include text, object, background and picture layer types. A set of image masks is used to uniquely identify different layer types within an image, where each layer type includes none, some or all of the pixels of the original image. Each layer of the image is processed prior to transmission (i.e., compressed) using a lossy or lossless encoding method appropriate for the characteristics of that layer.","In order to determine if a pixel from the original image is represented on a layer, each layer is assigned a single-bit pixel mask of the same dimensions of the original image. If a pixel from the original image is represented on a layer, the corresponding bit in the pixel mask for that layer is set. Once the image is decomposed, the original image and the mask is forwarded to the processing method defined for that layer and the mask is used by the processing method to identify which pixels of the image should be processed.",{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 1","b":["100","106","106","106","105","104","104","103","101","102","107"]},"Embodiments of the present invention decompose image  into layers of different image types and corresponding masks as in preparation for image compression. Each mask is generally implemented as an array, i.e., each mask is a map of one-bit pixels of image  where a bit value of 1 positively identifies a pixel as an element of that mask. In one case, image  is decomposed into four mutually exclusive layers, so therefore a mask set that defines image  comprises a two-dimensional array (of the same dimension as image ) with each array element defined as a two bit value. In such a case, each two-bit value describes four different states and each state identifying the presence of a pixel on one of the four layers of the image. In alternative embodiments, for example in cases where the masks are not mutually exclusive or cases where fewer or more than four masks are defined, other structures including single-bit or three bit elements are used.","In some embodiments, one or more masks comprise information related to drawing commands associated with the rendering of image  in a frame buffer. In one such embodiment, additional mask layers are defined for lossy and lossless image types as classified through the interpretation of the drawing commands. Lossy image types include rendered Joint Photographic Experts Group (JPEG) files, computer wallpapers and the like while lossless image types include rendered text and icons. Such classified image areas are thereby enabled to bypass pixel level filtering processes described herein and be forwarded directly to downstream lossless or lossy encoding methods. In another such embodiment, a mask layer is defined for video image type in which case the classified area of image  is processed using a video encoding method which may perform lossy encoding of rendered pixels, transcoding of the source video stream or forwarding of the source video stream to the remote computer in different embodiments.",{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIGS. 2A through 2E"},"Text image types are generally encoded using lossless or high quality lossless methods to ensure accurate reproduction. Referring to , text image type  is identified by text mask . Text image type  is defined by any small high-contrast area that is surrounded by a background image with characteristics that enable the background image to be described using one or more graphical primitives. Background image type  in , as identified by background mask  represents such a background image type. In an embodiment, the basic graphical primitive is a line. Multiple lines of the same color represent solid color areas . Multiple lines of different colors represent gradient areas  of the background. When image  is regenerated, regions of text image type  overwrite the regions of background image type , thus enabling the background image to be defined as continuous graphical objects through the text regions.","To maximize the area of regions comprising background image type  without constraint by regions of text, the decomposition process first identifies regions of text image (identified by text mask ) which are then marked as \u201cdon't-care\u201d regions for the subsequent background decomposition analysis. Using this approach, areas of background image may be specified as simple graphics descriptors that define long lines of the same length and the same color. Such descriptors are efficiently compressed and enable lossless background image reproduction.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":["FIG. 2C","FIG. 2D","FIG. 2C","FIG. 2D"],"b":["118","107","119","116","105","100","116","117"]},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 2E","b":["114","100","115"]},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 3","b":["300","300","5","8","100","300"]},"Process  proceeds to step  (\u201cBackground Identification and Mask Generation\u201d). Step  identifies and marks background image areas suitable for identification before other image types are identified. Process  proceeds to step  (\u201cText Identification and Mask Generation\u201d) in which high-contrast filters, including saturated pixel filters and other pixel pattern filters, are used to identify and mark high-contrast areas including text, graphics or icons. Following step , the text mask contains both text images and type 2 object types. Process  proceeds to step  (\u201cBackground Expansion and Mask Update\u201d) in which the background mask is updated to include areas that have been marked in the text mask as additional background areas in the background mask. Process  proceeds to step  (\u201cText Expansion and Mask Update\u201d) in which the updated background mask is used as a reference to clear the text mask of pixels that are assigned as both text and background pixels. In some embodiments, optional step  attempts to expand the text mask through iterations of steps  and  until a desired level of quality is achieved for the text mask and the background mask. Process  proceeds to step  (\u201cEnclosed Object Additions\u201d) in which small areas that are not identified in the text or background masks are reviewed based on the image type of neighboring pixels. Small areas adjacent to text, background, or type 1 objects are generally reclassified as text image type. Process  proceeds to step  (Separate Object Layer from Text Layer\u201d) in which the text mask is divided into two layers i.e. an object layer associated with type 2 object image  and text image layer associated with text image type . The object layer consists of areas on the original text mask that are not fully surrounded by background. Pixels in the object layer are removed from the text mask and placed in the object mask. The text layer consists of areas on the original text mask that are fully surrounded by background. Pixels in the text layer remain on the text mask. Process  proceeds to step  (\u201cGenerate Picture Mask\u201d) in which pixels that are not already identified as text, objects or background are identified as picture pixels in the picture mask. Process  proceeds to step  (\u201cOptimize Filter Mask\u201d) in which the mask set is filtered to reassign small, isolated image regions that may hinder optimum compression and can be reclassified without degrading the image quality. Process  ends at step .",{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 4","b":["120","121"]},"For series i={1, 2, 3, . . . , m} and j={1, 2, 3, . . . n}\n\n|()\u2212()|<=\u2003\u2003(1)\n","Such a filter seeks a line of adjacent pixels that is 16 pixels in length with all pixels matching in color. A variation of this filter allows small variations in color. In cases where these variations are not factored into the graphics primitive for the background, the compression process reduces the image quality. Other variations include rectangular area filters, diagonal lines, dotted or dashed lines, or color lines of even gradient to identify background pixels, or a graphic descriptor that determine a default background color for an area or an entire display.",{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 5","b":["100","300","125","126","129","128","127","101","102","107"]},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 6","b":["600","11","300","600","112"]},"To meet an underlying need for accurate text reproduction, a conservative analysis for text identification is generally prudent. While accidental classification of non-text areas as text areas does not impact image quality, text areas should always be correctly identified to ensure lossless compression. Graphical images that happen to incorporate lines of a constant color (e.g., line of 16 pixels) are generally decomposed onto the background layer rather than the text layer if they are identified by the background filter. This may decrease the overall compression ratio but both the background and high-contrast features are reproduced accurately.","Based on the variety of shapes and forms expected for text, embodiments of the present invention use a series of contrast filters in conjunction with an accumulated pixel density integration filter to positively identify text pixels. Each contrast filter is applied to the image and marks are assigned to individual pixels identified as text prospects. Once the image has been processed by the series of contrast filters, the marks for each pixel are accumulated and the image is filtered by the pixel density integration filter to select only areas that have a high density of text markings.","Process  starts at step  (\u201cIdentification and Marking of Saturated Text Pixels\u201d) in which a first filter method identifies and marks saturated text pixels. Due to their vivid nature, saturated pixels in computer display images have a high probability of being text. In a 24-bit color space embodiment, a saturated color in Red Green Blue (RGB) space is defined as any color where R, G and B are each 0 or 255, where each RGB color is represented by an 8-bit value. For a grayscale embodiment, these values correspond to the values used for the colors black and white. The mere presence of saturated color pixels does not guarantee that the pixels are text so the saturated color pixel needs to be adjacent to a pixel of contrasting color. The filter seeks saturated color pixels with the additional constraint that each be adjacent to a pixel of reasonably high-contrast. Background pixels are usually saturated, so an additional constraint is that the saturated pixel should not be a background pixel as determined by previous filters.","Process  proceeds to step  (\u201cApplication of 3, 4, and 5 Element Pixel Patterns\u201d) in which pixel regions of various sizes that match, either exactly or within some predefined difference, pre-determined pixel patterns. These pixel patterns are based on the expected color gradient and contour of text. In addition these pixel patterns may include the expected location of background pixels (where a background pixel is a pixel that has been detected by the aforementioned background filter). In an embodiment multiple pixel pattern filters that compare groups of 1\u00d73, 1\u00d74 or 1\u00d75 regions of pixels are applied to the image to determine which pixels are assigned text pixel markings.","Process  proceeds to step  (\u201cIntegration and Filtering of Marked Text\u201d) in which prospective text pixels receive multiple markings from the multiple pixel pattern filters. Once all of the text filters have been applied, the marks are accumulated and integrated over a small area. The output of the integration filter is a value that is used to measure if the area has a sufficient density of text marks. If the area passes the threshold, then all text marks in that area of the text mask identify text pixels. If the area does not pass the threshold, then all text markings are considered to be noise and the text marks in that area are removed.","Process  proceeds to step  (\u201cText Mask Generation\u201d) in which the remaining text pixel markings are converted into a text mask after the text pixel markings determined to indicate noise have been removed. Indicia for pixels that are identified as both text and background are also removed from a text mask as step  (\u201cRemove Background Pixels from Text Mask\u201d).","Following step , the text mask contains both text and high-contrast objects. These high-contrast objects are removed from the text mask by a later filter. Text indication is not a perfect process and not every text pixel is positively identified by the aforementioned pixel patterns. As a next step  (Text Surround Mask Generation\u201d), a blocking operation is performed to mark the pixels surrounding text pixels to ensure the mask is expanded to include all text pixels. The expanded area is also useful for background identification.",{"@attributes":{"id":"p-0065","num":"0064"},"figref":["FIG. 7","FIG. 7"],"br":[{},{},{}],"in-line-formulae":[{},{},{},{},{},{}],"i":["A\u2212B|>=d","A=","FF "]},"and\n\n","Pixel B may be to the right , left , above  or below  the saturated color pixel A. The saturated pixel filter may be applied in multiple directions, for example in some embodiments, diagonal filter  is also used.",{"@attributes":{"id":"p-0068","num":"0068"},"figref":"FIG. 7","b":["135","136","137"]},"Anti-aliased text does not comprise boundaries as sharp as standard, aliased text. As such, saturated pixels in anti-aliased text are normally adjacent to gray pixels and the color difference between them may not meet the minimum difference requirement. A variation better suited to anti-aliased text, measures the contrast between the saturated pixel \u2018A\u2019 and the pixel \u2018B\u2019 where \u2018B\u2019 is two pixels away from A rather than directly adjacent as shown for pixel pair . In such an embodiment, the middle pixel (between pixel A and pixel B) is either not considered in the filter equation or the filter coefficient for that pixel has a reduced weighting. For example, a weighted average value may be calculated across the two non-saturated pixels where the weighting for the center pixel is lower than the weighting for the outer pixel. This averaged contrast level is then used to determine if a contrast threshold is exceeded.","In another embodiment, color pixels that are saturated in one or two of the R, G, or B levels are also considered for text identification. However, the probability of false detection increases as the number of saturated colors is reduced from three to two or one. The probability of errors further increases as the filter width increases. In these cases, additional filtering is required to remove the unwanted detections. For example, one approach decreases the contrast threshold between the saturated color pixel and the adjacent pixel that positively identifies the color pixel as text.",{"@attributes":{"id":"p-0071","num":"0071"},"figref":["FIG. 8","FIG. 8"],"b":["144","145","146","140","141","142","143"]},"An embodiment of the 3-pixel filter comprises two control values for determining if pixel or group of pixels matches this pattern and should be marked as text. The first control value is the minimum difference between the center pixel and the nearest outside pixel. The second control value is the maximum difference between the two outer pixels. While the minimum difference of the center pixel need not be large if the end pixels are identical, in cases where the maximum allowable difference between the end pixels is increased, the center pixel minimum difference should also be increased to prevent excessive false text markings. An optional parameter for the filter is to use the background information to determine if a pixel is text. Pixels A, B and C are marked as text according to the criteria in expressions (4) and (5) below:\n\n||<=maximum difference between the two outside pixels\u2003\u2003(4)\n\nand\n\n||>=minimum difference between center pixel and nearest outside pixel and optionally A and\/or B are background pixels\u2003\u2003(5)\n","If there are background pixels at both ends of the filter, and the center pixel is not a background pixel, then there is a high probability that the center pixel is a text pixel. If only one end of the filter is an identified background pixel but there is minimal difference between the two ends, then there is a reasonable probability that the text is on a gradient background. In cases where a pixel identified as a background pixel is under filter examination, the other two parameters may be reduced without increased false text detection.",{"@attributes":{"id":"p-0074","num":"0074"},"figref":["FIG. 9","FIG. 9"],"b":["155","154","155","150","151","152","155"],"br":[{},{},{}],"in-line-formulae":[{},{},{},{},{},{}],"i":["A\u2212D","A\u2212B","C\u2212D"]},"4-Pixel filter  also depends on the background in a digital image being precisely constant without any noise i.e. pixels |A\u2212D|<=maximum difference as the filter covers adjacent text pixels B and C on background pixels A and D . Filter  also utilizes the characteristics that text pixels are general surrounded by pixels of high-contrast e.g. |A\u2212B|>=minimum difference or |C\u2212D|>=minimum difference for readability purposes.","An application of a 1\u00d74 pixel pattern that accounts for pixels in the same text character being exactly equal comprises pixels A, B, C and D marked as text using the middle pixels according to the expression:\n\n||<=maximum difference\u2003\u2003(8)\n\nand\n\n||>=minimum difference\u2003\u2003(9)\n\nand\n\n||>=minimum difference\u2003\u2003(10)\n\nThe 1\u00d74 pixel pattern filter may be applied to detect large font over a wide area of flat text. In addition, some pixel patterns associated with small fonts can only be properly expressed by a 1\u00d74 pixel pattern. A variation on the 4-pixel filter uses background pixel information to improve the search in a similar mode to the 1\u00d73 pattern filter. Pixel pattern filters of 1\u00d75 format are also useful for detecting wider text. While the simple n\u00d7m pixel pattern recognition works well for small values of n and m, as the pixel pattern increases in size, it loses its suitability to capturing generic text characteristics and becomes better suited to character recognition applications.\n",{"@attributes":{"id":"p-0077","num":"0077"},"figref":"FIG. 10","b":["168","100","165","169","160","165"]},"Next, accumulated text markings provided by the text filters are filtered to evaluate the text mark density and remove erroneous text detections. If the number of text marks over a small area exceeds a defined threshold, the text pixels in that area remain marked as text pixels. In different embodiments, the weighting of text marks and the text density threshold may be varied in different areas of the image. Nevertheless, depending on how the text markings are accumulated and the defined threshold value, some false text indications may result, especially in areas where text is drawn over textured image .",{"@attributes":{"id":"p-0079","num":"0079"},"figref":["FIG. 11","FIG. 10","FIG. 11"],"b":["25","600","168","170","168","23","24","25","172","100","171","168"]},{"@attributes":{"id":"p-0080","num":"0080"},"figref":["FIG. 12","FIG. 12","FIG. 12"],"b":["12","300","181","180","1","180","2","182","186","182","183","184","186","187"]},{"@attributes":{"id":"p-0081","num":"0081"},"figref":["FIG. 13","FIG. 14"],"b":["190","191","192","111","111","172","197","13","300","197"]},{"@attributes":{"id":"p-0082","num":"0082"},"figref":["FIG. 15","FIG. 15"],"b":["14","300","201","200","201","202","203","204","12","12"]},{"@attributes":{"id":"p-0083","num":"0083"},"figref":["FIG. 16","FIG. 17"],"b":["15","300","210","211","210","213","119","16","300","115","111","197"]},{"@attributes":{"id":"p-0084","num":"0084"},"figref":["FIG. 18","FIG. 19"],"b":["197","117","113","197","119","221"]},"Small areas of image types may be filtered at step  of process  once the masks are created. This filter reclassifies small areas of one image type based on the type of adjacent pixels in order to improve the compression ratio of the image. One filter method changes small areas of background pixels that are surrounded by text pixels to text pixels. The reason this is more efficient is that background image types compress well if they define a large area, but the text compression algorithms may be better at handling small groups of pixels. Another filter method changes small groups of background pixels that are surrounded by picture pixels to picture pixels because these areas are likely a flat area of the picture. Yet another filter method converts small groups of picture pixels surrounded by background or text pixels to text pixels using methods similar to the detection method of process  stet .","The described decomposition method primarily discusses a grayscale image for simplicity purposes. However, various embodiments apply decomposition methods to an RGB computer display image by individually testing each color component using the steps of process  described. The text filters used in such color applications may select the number of colors required in order to satisfy the positive text identification criteria. In other embodiments, color space translation is used to improve or simplify the decomposition method. In such case, the image compression process that follows decomposition should generally use either an RGB format or an alternative lossless translation to ensure accurate reproduction of the image.",{"@attributes":{"id":"p-0087","num":"0087"},"figref":["FIG. 20","FIG. 20"],"b":"2001"},"Referring to , host system  is connected to remote system  by network . Host system  is comprised of CPU  connected to system memory  and drawing processor  by chipset . While a single CPU  is illustrated, it is to be understood that alternative embodiments where multiple CPUs are utilized in a cooperative arrangement can also be realized. In an embodiment, drawing processor  is a discrete silicon component such as a GPU connected to CPU  by a data bus via chipset . In other embodiments, drawing processor  comprises a silicon function integrated into chipset  or CPU . In yet other embodiments such as a virtualized architecture uses in Virtualized Desktop Infrastructure (VDI), system  comprises multiple drawing processors , each implemented at least in part as machine readable instructions that are executed by CPU  (i.e., drawing processor  may be a virtualized GPU). Drawing processor  is coupled to drawing memory  which incorporates one or more frame buffers. In discrete embodiments of drawing processor , drawing memory  is generally high speed memory double data rate (e.g., DDR3) or extreme data rate (XDR) memory dedicated for access by drawing processor  and encoding system . In a virtualized embodiment, drawing memory  comprises one or more regions of system memory . Drawing memory  generally stores information associated with an image representation including image vector or pixel data, attributes, drawing commands, file information or other details pertinent to an image.","In some embodiments, host system  also includes other peripherals, such as host USB controller  and\/or host audio controller  connected to CPU  by chipset . In an embodiment, host USB controller  is bridged at the buffer management layer with remote USB system  to provide a synchronized data path that enables the communications of different traffic types including control and status packets in addition to packet transport of different USB data types such as isochronous and bulk data types. Host audio controller  is bridged at the buffer management layer with remote audio system  to provide synchronized communications of packetized audio data and audio control information between host and remote systems. In some embodiments, host USB controller  and host audio controller  are implemented, at least in part as software functions executed by CPU  and\/or embedded in other host subsystems, including chipset  or encoding system .","In an embodiment, encoding system  is connected to drawing memory  so that it can read and encode sections of the display image in drawing memory . In such an embodiment, encoding system  may have directly addressable access to a drawing memory that is used by drawing processor . In an alternative embodiment, drawing memory  may be part of system memory  connected to CPU  or chipset , and in which case, encoding system  also has access to the drawing memory. In some embodiments, at least part of encoding system  is implemented as machine readable instructions suitable for execution by CPU  or a second processor in communication with drawing memory .","In the embodiment of , encoding system  is connected to traffic manager  for transfer of encoded display data from the encoding system to the traffic manager. Traffic manager  aggregates display data with other CPU or peripheral traffic and forwards it to network controller , which manages the transport of network packets from host system  to remote system . Network controller  also receives media streams such as audio, USB and control messages from remote system  which are forwarded to traffic manager , which in turn passes them to destination host USB controller  or audio controller .","In some embodiments, network controller  and encoding system  are connected to chipset  by a system bus such that encoded display data  and network management data may be communicated between network controller  and encoding system  over the system bus. In such implementations, traffic manager  may not be necessary to the encoding and transmission system.","Drawing operations may be performed using published methods such as existing industry compatible application programming interfaces (APIs) available to existing application software. CPU  issues drawing commands to drawing processor , which renders display images in drawing memory . Encoding system  then accesses image sections from drawing memory  and compresses them using encoding methods described below.","In an embodiment, encoded image sections are forwarded from encoding system  to traffic manager  where they are prioritized and multiplexed with audio, USB and other control signals from CPU  or peripherals that are also destined for the remote system. Traffic manager  prioritizes the outgoing traffic based on the real-time demands of the image, audio and USB media streams and the attributes of the present image to ensure perceptually insignificant delays at remote system . As one example, display update information receives higher priority than bulk USB transfers. As a second example, outbound display updates are multiplexed with outbound audio data updates in situations where a portion of the display has been identified as a video sequence. This ensures that a video sequence remains synchronized with its audio channels. As a third example, each traffic type is allocated a fixed maximum bandwidth. For example, image data may be granted 80% of the network bandwidth while audio and USB data may each be allocated 10% of the available bandwidth. In the case where audio data meets its allocated bandwidth, a higher compression ratio may be activated. In the case of bulk USB data meeting its threshold, the USB data may be delayed until competing higher priority transfers have completed. In the case where image data exceeds its bandwidth, a different image encoding method that requires less bandwidth may be selected and used. Other methods of traffic management such as the real-time allocation to different traffic types according to traffic type and priority may also be used.","Traffic manager  may also feed network availability information back to encoding system  so that suitable encoding methods may be selected based on network conditions. Such network availability information may be determined by monitoring the bandwidth requirements of inbound and outbound USB and audio streams, monitoring error rates and receiving performance information provided by remote system  and optionally real-time network management equipment. In an exemplary embodiment, multiplexed media and control streams are encapsulated using an appropriate network protocol, for example UDP\/IP are then forwarded to network controller  for transmission over an Ethernet network . Network controller  then manages the physical and link-layer communication of the data streams to remote network controller  in the remote system .","Remote network controller  manages the physical and link-layer communication of the data streams to and from host network controller . Remote network controller  forwards inbound traffic to remote traffic manager , which reconverts the aggregated streams from host system  into separate audio, USB and image streams. USB data and audio streams are directed to remote USB  and remote audio  systems respectively while display image data is directed to remote display decoder . Remote traffic manager  also directs host-bound traffic from the remote USB and audio systems to remote network controller  for encapsulation and transfer to host system .","The display data received from host system  is decoded by remote display decoder  and stored in remote frame buffer . Alternatively, the image may be stored directly in frame buffer  in compressed form and decoded by remote display decoder  in real-time as controlled by display controller . Display controller  accesses the image from frame buffer  and generates a timed display video signal, e.g., Digital Visual Interface (DVI) signal, which is used to drive remote display .","Network errors and bandwidth availability are managed at various protocol levels by different modules. At the physical and network protocol layers, the transport is managed between network controller  and remote network controller . Remote traffic manager  monitors network congestion and availability based on the timing of received packets, sequence numbers and lost packets and periodically signals traffic manager  regarding network and data transfer status. Traffic manager  forwards this status information to encoding system , which adapts the encoding scheme in real-time based in part on bandwidth availability. Encoding system  may also predict future bandwidth requirements based on interpreted drawing commands as described.","At a higher protocol layer, remote display decoder  detects if image sections are corrupt, late or dropped. In these cases, remote display decoder  signals encoding system  that the section should be retransmitted. Encoding system  either retransmits the requested section or an updated version, depending on the availability of refreshed information in the drawing memory .",{"@attributes":{"id":"p-0100","num":"0100"},"figref":"FIG. 21","b":["2006","2010","2016","2006","2010","2012","2016","2012","2012","2010","2006"]},"Drawing memory  incorporates one or more designated areas that are used by drawing processor  to render and store display image frames (ref. frame buffers ). The presence of a bus arbiter between the drawing memory and drawing processor \/encoding system  enables processor  to draw to drawing memory  in a transparent manner (i.e., as if an encoding system were not also connected to drawing memory ). Such an arbitrated coupling enables the rendering performance of the drawing system to not be impacted by the presence of the encoding system .","In an embodiment, encoding system  comprises three modules. First, encoding sequencer  has read access to drawing memory  and responds to requests for updated display sections by reading the requested sections from the drawing memory . Second, display encoder  is connected to the output of encoding sequencer  and compresses sections of the display image using any of several means described below. Third, command monitor  has access to the drawing commands issued by CPU . The command monitor may either be a software function executing on the CPU, and\/or a dedicated function or functions embedded within encoding sequencer  and\/or display encoder . In select embodiments, the display encoder is a dedicated hardware module but the functionality of encoder  may be implemented either as hardware or software (or a combination) within drawing processor  or CPU  in other embodiments. Encoding sequencer  uses synchronized timing means to access pixels, blocks, lines, frames or other sections of image from a frame buffer  in the drawing memory . This access is initiated by any of several mechanisms, including an incoming request from remote display decoder  or locally generated timing. In select embodiments, regions of a frame buffer  are read on request by remote display decoder  only after drawing processor  has signaled that the rendering of a current frame is complete, for example using frame buffer timing signal . To prevent the tearing of a display image during encoding, it is generally recommended to delay the encoding of a frame until the completion of some raster operations such as \u201cmove\u201d operations.","In some embodiments, the drawing command stream rate at which a software application executed by CPU  calls drawing processor  is controlled (e.g., using CPU blocking commands ) so that drawing memory  is updated at a rate that matches the image throughput rate. The optimum frame update rate is determined by identifying image throughput bottlenecks. In one embodiment, the bottleneck is identified by comparing the throughput of the drawing, encoding, transmitting and decoding functions and the rate at which drawing command are issued is controlled to match the slowest throughput. In another embodiment, the encoding method is selected so that the transmission rate matches the slowest of the drawing command throughput rate, the encoding rate and the decoding rate. In an embodiment, frame buffer timing signal  is used to establish the frame update rate used by the encoder. In embodiments where network bandwidth is unconstrained, a frame buffer  is read by encoding system  prior to the drawing processor flagging the completion of the rendering operation. In such cases, encoding system  may encode and transmit the image prior to drawing completion. In this embodiment, encoding system  keeps track of sections updated by drawing changes that occur after the selection of a particular frame buffer  and transmits these changed sections after the drawing processor signals the availability of the rendered image. The advantage of this method in systems with a high availability of network bandwidth is that even though some data may be transmitted twice, pre-encoding and pre-transmission of image sections reduces the overall latency between the rendering operations and remote display operations.","Encoding sequencer  reads the requested image segment and forwards it to display encoder  for compression. Encoding sequencer  may also emulate a local display controller  by providing timing signals (e.g. VSYNC signal ) for drawing processor . Command monitor  filters drawing commands  issued by CPU  to drawing processor  for useful information that may facilitate or optimize image decomposition and\/or display encoding functions. Useful information includes an understanding of image type, co-ordinates, image quality, display priority (i.e., latency) and other attributes of the display. Display encoder  uses knowledge gained from the drawing commands that have been forwarded by command monitor  and additional knowledge of which areas of the frame buffer have been updated to compresses image sections or changed areas of the image sections.","Command monitor  may also monitor source commands executed by CPU  for display setup parameters, configuration instructions and timing requirements including display refresh rates issued to display controller and forwards configuration information  to remote display controller . Timing requirements are forwarded to encoding sequencer  which uses the information to provide emulated timing for the drawing processor (e.g., VSYNC signal ). In select cases where a software application is blocked waiting for the completion of drawing operations (e.g., a waitforvsync( ) function call), CPU  is abstracted from the fact that the VSYNC signal is generated by the encoding system rather than the drawing processor. Encoding system  determines the timing of drawing processor  but in the case of a blocking command, the token is returned by the drawing system to CPU  (ref. signal ) on command completion. In an embodiment, command monitor  initiates a low power state based on the absence of drawing commands. In an exemplary power saving application, the access circuitry of drawing memory  associated with a particular frame buffer is temporarily disabled if the frame buffer is not updated over a determined period.",{"@attributes":{"id":"p-0106","num":"0106"},"figref":["FIG. 22","FIG. 22"],"b":["2201","2000","2200","2006","2202","2204","2206","2010"]},"The image drawn to a frame buffer in the same way as a system without the presence of an encoding system. When a drawing API function is called, a graphic instruction is issued to graphics device driver  that interprets the instruction for the particular hardware implementation of the drawing processor. Some embodiments comprise an additional command monitoring software processing layer  between drawing command API  and graphics driver . The drawing command monitor issues the command to the drawing processor (via the graphics driver) and forwards selective duplicate commands to encoding sequencer  and display encoder .","Command monitor  extracts and forwards essential elements of the drawing commands including sequencer-related commands  which comprise useful hints based on what part of the image is being drawn and encoder-related commands  which describe properties of the image used to influence the selection of encoding method. Command monitor  may also monitor operating system  for system commands and display setup and configuration instructions  destined for the display controller. Configuration instructions are forwarded to the remote display controller  while synchronization instructions that synchronize image updates with the display refresh rate are sent to the encoding sequencer  to enable the appropriate frame buffer to be encoded, transmitted, decoded and displayed at the remote display .",{"@attributes":{"id":"p-0109","num":"0109"},"figref":"FIG. 23","b":["2301","2102","2102","2102","2300","2302","2304","2300","2010","2320","2302","2302","2322","2040","2304","2300","2302","2324","2326","2106","2102","2010","2302","2302","2322","2324","2326","2016","2304","2302","2300","2304"]},"Frame buffer read and sequence module  may also generate synchronization signals  for drawing processor  such as the vertical retrace and blanking signals by using the ability of read timing control module  to synchronize with the timing of the remote display.",{"@attributes":{"id":"p-0111","num":"0111"},"figref":"FIG. 24","b":["2450","2102","2400","2409","2410","2010","2036","2411","2412","2413"]},"In some embodiments, process  proceeds to step  in which encoding sequencer access to the frame buffer is delayed until a \u201cframe buffer ready\u201d signal is received. In such embodiments, the frame buffer is made available for reading only following its released by drawing processor . Alternatively, in the case of a host system with a single frame buffer, step  may be bypassed and encoding sequencer  may access the frame buffer asynchronously to rendering functions.","As a next step  (\u201cCopy frame buffer change map\u201d), the frame buffer change map is copied. As a next step  (\u201cReset frame buffer change map\u201d), the frame buffer change map is reset. As a next step  (\u201cRead display sections\u201d), the sections, pixels, lines, blocks or frames identified in the buffer change map copy are then accessed and assembled with the other information described. As a next step  (\u201cWrite to encoder\u201d), the display sections and other information is forwarded to the display encoder.",{"@attributes":{"id":"p-0114","num":"0114"},"figref":"FIG. 25","b":["2501","2624","2500","2531","2532","2533","2534","2535","2536","2502","2501","2504","2002","2040","2016","2012","2002","2002","2012","2000","2002","2002"]},"System power management module  is enabled to reduce the power consumed by elements of encoding system , for example by shutting down elements of the multi-method encoder based on frame buffer change activity and the selected encoding method. In one embodiment, motion estimation circuit  is disabled when there is no motion. Examples of useful drawing commands associated with the reduction of power consumption are shown in TABLE 9.","Image decomposition module  is enabled to classify the image type as a precursor to the encoding operation. In an embodiment, the image is classified into different image types such as background, text, picture or object layers based on a combination of spatial features detected using multilayer image decomposition method , temporal features (such as periodic image change rate useful in detection of video image type) and drawing commands interpreted by command interpreter . A selective list of drawing commands that identify image types are listed in Table 2. The various masked layers as classified by module  are subjected to different encoding methods that may include application of lossless encoders for text, background and object image types, application of lossy encoders to picture and video image types and context selection and application of different entropy encoders.","In an embodiment, various multilayer masks generated by the image decomposition process (i.e., process ) are encoded using a lossless encoding technique supported by encoder  and multiplexed with encoded image payloads prior to transmission to remote system . In an embodiment, four masks M1, M2, M3 and M4 associated with text, background, picture and object image types are multiplexed with data comprising encoded 16\u00d716 pixel blocks in a packet stream with a frame header as shown.","|etc.| - - - 16\u00d716 data - - - |M4|M3|M2|M1| - - - 16\u00d716 data - - -|M4|M3|M2|M1| - - - 16\u00d716 data - - - |M4|M3|M2|M1|FRAME HEADER","Each sequence of mask M1-M4 information fields describes a compressed block area of 16\u00d716 pixels. The compressed image and masks are transmitted to the remote system  as a data stream. In alternative embodiments, the blocks may comprise other dimensions, including larger blocks, lines or entire frames.","Generally, remote display decoder  is enabled to interpret the received data stream and extract the mask information from the mask information fields and decodes the image based on this information to reconstruct the original image frame. Remote display decoder  maintains decoder algorithms complementary to the various elements of encoder  such as is necessary to decompress the image data using the methods identified by the mask information. Depending on the compression method used, the compressed display stream may be decompressed on a per block basis, across multiple blocks (e.g., LZW, JPEG), or across frame updates (e.g., MPEG). In some embodiments, background and picture layers are decompressed and reconstructed before the text and object layers. In the case of the background mask, the mask provides the co-ordinates for the start and end co-ordinates of graphic descriptors or the predictive background decoder. Alternatively, the descriptors themselves may define the background co-ordinates. In some embodiments, the remote display decoder  uses the received picture mask to identify the co-ordinates and boundaries of the picture areas once they have been decompressed. The object mask identifies the exact location of object pixels in the original image although the mask does not specify the object texture. Objects are decompressed and the pixels are populated over the background of the reconstructed image using the co-ordinate positions provided by the mask.","In the case of anti-aliased text, the text mask defines the boundaries of the text. Texture detail is derived through a lossless decoding method used for the text layer text. In the case of simple, fine text, the text mask provides an accurate specification of the form and texture of the text. For example, in the case of simple single color text, accurate text reconstruction is accomplished by populating the locations of the image specified by the text mask with the pixels matching the color specified by the text layer.","Drawing command interpreter  interprets drawing commands identified to enhance the image decomposition process. In one embodiment, a drawing command identifies a section of the display as a video sequence which allows the decomposition function to classify the defined region as a picture or natural image region, independent of the contrast features of the region. If the video sequence displays text, it may be desirable to classify the text overlay as either picture or text dependent on other attributes of the video sequence. This enhanced classification is used to optimize the trade-off between image quality and network bandwidth limitations.","In another embodiment, a video sequence is identified by drawing commands. Drawing command information relating to the video such as blocking information, motion vectors and quantization levels are captured and used to select the blocking information, motion vectors and quantization levels of the encoding method. If the parameters are well matched, the image may be encoded at a quality level and bandwidth comparable to the original video sequence.","In another embodiment, drawing commands enhance the decomposition process by identifying font copy commands that indicate the presence of text, fill commands are identified to indicate the presence of background and texture-related commands are identified to indicate textured regions.","In another embodiment, drawing command hints identify the status of changes to image areas so that an encoding method may be selected based at least in part on change status information. In such an embodiment, information extracted from a drawing command is passed to section change detection module  regarding areas of the inbound image sections from encoding sequencer  that have changed and therefore require encoding and transmission. Block change, pixel change and motion vector commands all provide status information used to identify status changes.","In another embodiment, drawing command hints improve the efficiency of encoding by providing target quality predictions If incorrect predictions are made based on the hints, the image is encoded and transmitted using a higher bandwidth than predicted, but without sacrificing quality.","In another embodiment, the encoding sequence is prioritized to improve the encoding quality based on drawing command hints. As listed in Tables 3 and 8 below, OpenGL drawing commands provide quality and performance hints which provides insight into the quality and performance intended by the application and the encoding method may be set accordingly.","In an embodiment, encoder method selector  selects an appropriate encoding method based on various established criteria. Compression is based on the type of image. Drawing commands may be interpreted to understand attributes of the different sections of the display (based on interpreted drawing commands), where sections may have regular or arbitrary pixel boundary shapes. The commands may be used to identify areas as background, text, photographs, video etc. Each region may then be encoded using an optimum encoding method.","Compression is also based on network availability as indicated by traffic manager . Traffic manager  determines network bandwidth based on availability information from remote traffic manager  and feeds this back to encoding system . Drawing command interpreter  then determines the most effective encoding process based on the combination of the current encoding process, quality requirements, how much of the image is changing as indicated by drawing commands and the available network bandwidth as indicated by traffic manager information. For example, in an embodiment in which a set portion of available bandwidth is allocated to peripheral data traffic and the remaining available bandwidth is granted to image data traffic, the image encoding method is changed when the image data is predicted or measured to exceed its allocated bandwidth.","Based on the desired quality level and the network availability, for example as indicated by traffic manager , suitable encoding methods are selected. For each image type (e.g., picture, video, text, etc.), a lookup table may be used either to determine the bandwidth required (in bits\/sec) to achieve a given quality or the quality (in bits\/pixel) achievable for a unit of image area using a given bandwidth. In cases where bandwidth is limited due to low network availability or frequent screen changes over a large area, a higher compression mode may be selected or progressive build sequence may be used. In the case of progressive encoding, a relatively low network bandwidth is used to transfer a baseline image or image section of perceptually acceptable quality over a short period of time. Assuming the image or section does not change, more detail is added to the original baseline over time using small amounts of network bandwidth until the image reaches a perceptually lossless quality level. Progressive encoding methods are typically applied at different times and different rates to different sections of an image dependent on quality requirements and the nature of section changes. As a result, at any given time the different sections of an image will be at different progressive encoding states.","In the case of an actively changing image, knowledge of the area of the image that must be updated and an indication of the type of image provides significant information on how much data will be generated when the changing image is encoded. This information may be used in context with information from the traffic manager to modify the encoder method selection. As one example, a low bandwidth encoding method such as lossy encoding may be applied to the changing image in the case of low network availability. As a second example, a higher bandwidth encoding method may be applied to the changing image in the case of high network availability.","In an architecture that shares processing resources between drawing and compression functions (for example a CPU architecture with a single graphic processing unit or drawing processor used for both compression and drawing functions), the processing resource is actively balanced between updating the image (e.g., rendering activities) and updating the remote display (e.g., compression activities). The processing load is balanced in such a way as to equalize all processing-based and transmission-based bottlenecks at a minimum level across the data path.","One example is the case where the frame buffer update rate is higher than the frame transfer rate. In this case, the frame buffer update rate may be decreased to balance the compression transfer rate. If the same resources are used, lowering the frame buffer update rate may have the desirable effect of increasing the frame transfer rate. A second example is the case where the frame buffer update rate is lower than the frame transfer rate. In this case the transfer rate may be lowered to balance the frame buffer update rate. Similarly, if the same resources are used, lowering the transfer rate may increase the frame update rate with an overall effect of improving the new frame rate.",{"@attributes":{"id":"p-0134","num":"0134"},"figref":"FIG. 26","b":["2016","2010","2012","2006","2011","2016","2620","2102","2622","2106","2624","2104"]},"In such an embodiment, drawing processor  is connected to chipset  by a high capacity bus  such as a PCI-Express bus, an AGP bus or alternative interconnect suited to graphic data transfer. In alternative embodiments, drawing processor  may be integrated with chipset  or CPU . Drawing processor  uses image bus  to write rendered images into drawing memory . As encoding sequencer  also accesses drawing memory , access between the competing resources is arbitrated by drawing memory arbiter .","The arbitration sub-system generally grants encoding system  memory access according to strict encoding timing requirements while simultaneously accommodating the variable requirements of drawing processor . In an embodiment, arbitration between the two resources is achieved by granting drawing processor  a fixed priority and granting encoding system  a low priority. Encoding system  monitors the actual encoding rate in comparison with the desired encoding rate, as determined by the frame update rate. If the encoding system exceeds a time lag threshold, it signals drawing memory arbiter  to change its priority. In another embodiment, drawing memory arbiter  increases memory burst sizes when encoding system  is granted higher priority. Once encoding system  exceeds a lead time threshold, it is once again granted a low priority and burst size is reduced. As a result, encoding system  maintains a desirable memory access priority without impeding drawing processor .","Drawing processor  comprises control bus , with timing signals such as synchronization and control signal  and frame buffer ready signal  previously described connected to encoding sequencer . It also carries drawing commands  and display controller instructions captured by command monitoring method  destined for command monitor . As previously described, these commands typically originate from CPU . Drawing processor  receives the commands across data bus  and forwards them to command monitor . In an alternative embodiment, drawing commands are stored in drawing memory  and are directly accessible by command monitor .","Any of several methods may be deployed to lower the memory bandwidth requirements between encoding system  and drawing memory . One method deploys frame buffer change map  to ensure fewer memory read operations. Frame buffer change map  indicates which memory areas have been updated so that memory areas that have not changed need not be re-read. Another method deploys command monitor  to interpret drawing commands which provides an indication of the type of image in a given area and how it is changing. Frame buffer read and sequence module  may then limit memory access based on status information. As one example, a rapid changing video sequence may be read at a reduced frame rate. Another method for reducing memory bandwidth takes advantage of drawing processor cache memory . While the embodiment of  generally reads image sections from drawing memory  this may not be ideal once image sections have been updated. For example, in applications such as video sequences that occupy a large display area, the rendering function demands a high proportion of the available bandwidth of image bus . In such applications, it may be desirable to reduce the competing bandwidth requirements of encoding system . One method achieves bandwidth reduction by providing encoding system  with access to drawing processor cache memory . In such an embodiment, image sections are encoded directly from drawing processor cache memory  rather than external drawing memory which reduces maximum bandwidth requirements of memory interface .",{"@attributes":{"id":"p-0139","num":"0139"},"figref":["FIG. 27","FIG. 26"],"b":["2620","2700","2304","2702","2300","2704","2302","2010","2702","2706","2016"]},"Command monitor  uses control bus  to write the description of identified image regions (previously described ) to the register file of frame buffer read sequencer . On read request command  from read timing control , frame buffer read sequencer  accesses frame buffer change table  from bus  to determine which sections of the image have changed. Frame buffer read sequencer  reads the relevant sections of drawing memory  (on ) using image bus  and resets frame buffer change map using reset signal . In an embodiment where multiple displays are supported, only the bitmap relating to the current display need be reset. Image data is read directly into display encoder across image bus .","Read timing control  implements a state sequencer to generate timing control signal  for drawing processor  and read timing signal . Timing requirements are derived from remote decoder timing requests written across control bus  to the register file of read timing control  (ref.  in ) as well as frame buffer ready signal  in the case of an embodiment with multiple frame buffers.",{"@attributes":{"id":"p-0142","num":"0142"},"figref":["FIG. 28","FIG. 25","FIG. 25"],"b":["2624","2800","2512","2802","2508","2804","2500","2808","2504","2810","2510","2622","2810","2632","2812","2800","2802","2804","2808","2810","2520","2018","2632","2800","2634","2620","2820","2800","2802","2822","2824","2826","2804","2018","2614"]},{"@attributes":{"id":"p-0143","num":"0143"},"figref":"FIG. 29","b":["2901","2802","2804","2802","2820","2900","2900","300","2990","2990","2904"]},"In various embodiments supported by process , the image is processed by fill detection filter  which identifies regions of identical color as background regions (i.e., \u2018fill\u2019 regions). In one such embodiment, fill detection filter  identifies contiguous pixels of constant color exceeding a threshold (e.g., a threshold number of pixels) which are designated as background pixels  in a binary mask associated with the image. Background pixels  are suited to lossless encoding by constant color lossless encoder . Fill detection filter  also detects additional text pixels  which include additional pixels within a threshold distance of the identified contiguous pixels of constant color separated by a selection of text candidate pixels. Additional text pixels  also include text candidate pixels surrounded by background pixels and non-background pixels. Select pixels not identified as background but within a threshold distance of one of these additional text pixels are also added to the additional text pixels . In an embodiment, pixels identified as neither text nor background are designated as picture type pixels  in a binary mask. Picture type pixels  are suited to lossy encoding by lossy discrete transform encoder  (also referred to as lossy encoder ).","Discrete color lossless encoder  comprises an encoder circuit such as a masked color cache encoder or a masked dictionary encoder enabled to encode text pixel areas of the image designated by the text mask. Constant color lossless encoder  comprises an encoder circuit such as a lossless run length encoder, or a predictive encoder enabled to encode background or fill pixel areas of the image designated by the background mask. In some embodiments, constant color lossless encoder  identifies select pixels (previously identified as text pixels ) as \u201cdon't care\u201d pixels which are encoded as background pixels for encoding efficiency purposes. Lossy encoder  comprises an encoder such as a DCT or wavelet encoder enabled to encode picture type pixel areas of the image designated by the picture mask (i.e., lossy encoder  may comprise a masked discrete wavelet transform encoder or a masked discrete cosine transform encoder in different embodiments). Mask encoder  is enabled to encode positional information of the identified text pixels , background pixels  and picture type pixels . In an embodiment, mask encoder  is an entropy encoder such as a predictive encoder or a context adaptive binary arithmetic encoder (CABAC) enabled to encode each of the binary mask layers.","The encoded lossy and lossless data sets, in addition to encoded masks from mask encoder  are multiplexed on encoded image bus  and forwarded to traffic manager .","While the set of coupled elements  illustrates the logical data path for text, background and picture pixel types, a physical embodiment of image decomposition circuit  coupled to multi-method encoder  generally comprises a shared memory region (not shown in ) that stores a set of masks (or a multi-layer mask) and associated image pixels. The various processing modules or circuits (i.e., \u201cmasked encoders\u201d) evaluate relevant mask layers to determine which sections of the image should be accessed and processed. For example, discrete color lossless encoder  evaluates (i.e., scans) the text mask to identify regions of the image suitable for text encoding prior to retrieving text image sections for lossless encoding, constant color lossless encoder  evaluates the background mask prior to retrieving background image sections for lossless encoding, and lossy encoder  evaluates a picture mask prior to retrieving candidate picture image sections for lossy encoding.","In some embodiments, multi-method encoder  also uses drawing command hints and\/or decomposition hints generated from copy( ) fill( ) and BitBlt commands received from CPU  by command monitor  to improve encoding. As one example, select pixels identified by the fill( ) command are designated as background pixels. As another example, select pixels defined by a BitBlt command define the boundaries for a consistent pixel type (e.g., the boundary of a background pixel type ending part way into a 16\u00d716 pixel block. By concatenating adjacent pixel boundaries as defined by contiguous BitBlt operations, larger areas of an identified pixel type may be established. As one example, a larger section of picture type may be identified by contiguous BitBlt operations. As another example, a section of background pixels combined with a section of text pixels may be identified by a series of contiguous BitBlt operations. A periodically timed BitBlt operation over a region may be an indication of video image type. In some embodiments, pixel type is determined by weighting decomposition hints for a region against the results of the pixel filters for the region (e.g., weighted against the number of positive contrast filter hits for a region of candidate text pixels to determine if the region should be designated as text or picture type). The boundary specified for commands such as copy( ) commands can be used to prevent small areas of low gradient images (e.g., pale blue sky) from being detected as background which is best encoded using a lossy transform encoder rather than a constant color lossless encoder to maximize compression efficiency and prevent the generation of image artifacts.","In some embodiments, other information related to a source image including alpha blending information, mouse movement, screensaver activity or image composition information obtained from a desktop manager (e.g., Microsoft Windows Desktop Window Manager (WDM)) provides decomposition or encoding hints. As one example, a pattern of alpha blending for a group of pixels is generally indicative that an image region is of an object type and should be encoded at text pixels. As another example, a moving mouse or copy( ) command provides hints for motion estimation circuitry. As another example, an active screen saver generally indicates that an image may be encoded at reduced quality to preserve network bandwidth. A desktop manager provides application level information to the encoder. For example a Computer Aided Design (CAD) application window generally demands high quality encoding whereas a video window generally tolerates reduced quality encoding without reduced user experience.","In an embodiment, encoding method selector  sets encoding parameters for the filters and encoders shown by writing to control registers of the circuits across control bus .",{"@attributes":{"id":"p-0151","num":"0151"},"figref":"FIG. 30","b":["3000","2016","2006","3000","3002","3010","2006","2106"]},"Method  proceeds to step  (\u201cAnalyze image\u201d) in which image decomposition circuit  analyzes the image based on the decomposition hints to identify pixel types, such as picture pixels, background pixels and text pixels, suited for processing by different masked encoders. In various embodiments, step  is conducted in conjunction with process  such that pixels not identified by drawing commands or decomposition hints are identified as picture pixels, background pixels or text pixels by application of a set of pixel filters in decomposition circuit  (e.g., text detection filter  and fill detection filter ). In an embodiment, the decomposition hints are weighted with results of the set of pixel filters to generate a final determination of whether pixels should be assigned to picture pixel, background pixel or text pixel layers.","Method  proceeds to step  (\u201cEncode image\u201d), in which the image is encoded based on identified pixel type. In an embodiment, multi-method encoder circuit  comprises encoders ,  and  for encoding of text pixels, background pixels and picture pixels, respectively. The image mask which identifies positional information of the text pixels, background pixels and picture pixels is encoded by mask encoder . Method  ends at step  (\u201cEnd\u201d).","The tables below illustrate examples of drawing commands from various APIs that may be used by the display encoder to optimize image compression and transfer.",{"@attributes":{"id":"p-0155","num":"0155"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Drawing Command Structures for Change Detect Optimization"},{"entry":"CHANGE DETECT CIRCUIT"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"126pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Command",{},"API"]},{"entry":["Example","Application of Command in the Circuit","Example"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Draw . . . ","Track absence of writing commands","GDI"]},{"entry":["Fill . . .\/Floodfill","in area",{}]},{"entry":["Rect","Track absence of writing commands ","OpenGL"]},{"entry":["Viewport","in area",{}]},{"entry":["Raster",{},{}]},{"entry":"Commands"},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0156","num":"0156"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 2"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Drawing Command Structures for Decomposition Optimization"},{"entry":"DECOMPOSITION CIRCUIT"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Command","Application of","API"]},{"entry":["Example","Command in the Circuit","Example"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["FillRectangle","Identifies area for possible ","GDI"]},{"entry":["FillEllipse","background layer decomposition",{}]},{"entry":["Line Draw",{},"OpenGL"]},{"entry":["Commands",{},{}]},{"entry":["DrawString","Indicates area for text layer ","OpenGL"]},{"entry":[{},"decomposition",{}]},{"entry":["TextRenderingHint","Indicates desired quality of text ","GDI+"]},{"entry":[{},"display",{}]},{"entry":["BitBlt\/Bitmap","Indicates area for picture or object ","GDI\/OpenGL"]},{"entry":[{},"layer decomposition",{}]},{"entry":["IVideoWindow","Indicates area for picture layer","DirectShow"]},{"entry":[{},"decomposition"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0157","num":"0157"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 3"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Drawing Command Structures for Encoder Selector Optimization"},{"entry":"ENCODER SELECTOR CIRCUIT"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Command","Application of","API"]},{"entry":["Example","Command in the Circuit","Example"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Quality and","Influences selection of encoder ","OpenGL"]},{"entry":["performance Hints","method and parameters",{}]},{"entry":["IDMOQualityControl","Influences selection of encoder ","DirectX"]},{"entry":["IDMOVideoOutput-","method and parameters","DirectX"]},{"entry":["Optimizations",{},{}]},{"entry":["MPEG1VIDEOINFO","This structure describes an MPEG-1 ","DirectShow"]},{"entry":[{},"video stream",{}]},{"entry":["MPEG2VIDEOINFO","This structure describes an MPEG-2 ",{}]},{"entry":[{},"video stream",{}]},{"entry":["VIDEOINFO","This structure describes the bitmap ",{}]},{"entry":[{},"and color information for a video ",{}]},{"entry":[{},"image"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0158","num":"0158"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 4"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Drawing Command Structures for Compression Method Selection"},{"entry":"ENCODER SELECTOR CIRCUIT"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"105pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Command","Application of ","API"]},{"entry":["Example","Command in the Circuit","Example"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Quality and","Influences selection of encoder ","OpenGL"]},{"entry":["performance Hints","method and parameters",{}]},{"entry":["IDMOQualityControl","Influences selection of encoder ","DirectX"]},{"entry":["IDMOVideoOutput-","method and parameters","DirectX"]},{"entry":["Optimizations",{},{}]},{"entry":["MPEG1VIDEOINFO","This structure describes an MPEG-1 ","DirectShow"]},{"entry":[{},"video stream",{}]},{"entry":["MPEG2VIDEOINFO","This structure describes an MPEG-2 ",{}]},{"entry":[{},"video stream",{}]},{"entry":["VIDEOINFO","This structure describes the bitmap ",{}]},{"entry":[{},"and color information for a video ",{}]},{"entry":[{},"image"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0159","num":"0159"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 5"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Drawing Command Structures for Predictive Encoding"},{"entry":"PREDICTIVE ENCODER CIRCUIT"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"42pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Application of ","API"]},{"entry":["Command Example ","Command in the Circuit","Example"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Viewport","Motion Search","OpenGL"]},{"entry":["Rotate; Translate; Scale","Motion Search","OpenGL"]},{"entry":["CopyPixel","Motion Search; Display Update ","OpenGL"]},{"entry":["Quality and performance ","Compression Parameters","OpenGL"]},{"entry":["Hints",{},{}]},{"entry":["IDMOVideoOutput-","Compression Parameters","DirectX"]},{"entry":["Optimizations",{},{}]},{"entry":["IAMVideoCompression","Sets and retrieves video","DirectShow"]},{"entry":[{},"compression properties",{}]},{"entry":["IAMVideoAccelerator","Enables a video decoder filter ",{}]},{"entry":[{},"to access video accelerator",{}]},{"entry":[{},"functionality",{}]},{"entry":["IAMVideoDecimation-","Enables an application to control",{}]},{"entry":["Properties","where video decimation occurs",{}]},{"entry":["IDecimateVideoImage","Interface specifies decimation on",{}]},{"entry":[{},"a decoder filter. The term",{}]},{"entry":[{},"decimation refers to scaling the",{}]},{"entry":[{},"video output down to a size",{}]},{"entry":[{},"smaller than the native size of",{}]},{"entry":[{},"the video"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}]}}},{"@attributes":{"id":"p-0160","num":"0160"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 6"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Drawing Command Structures for Progressive Encoding"},{"entry":"PROGRESSIVE ENCODER CIRCUIT"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Command","Application of ","API"]},{"entry":[{},"Example","Command in the Circuit","Example"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"Lighting Enable","Build lighting features","OpenGL"]},{"entry":[{},"Quality and","Minimize progressive build in areas ","OpenGL"]},{"entry":[{},"performance","flagged as high quality",{}]},{"entry":[{},"Hints"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0161","num":"0161"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 7"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Drawing Command Structures for Lossless Encoding"},{"entry":"LOSSLESS ENCODER CIRCUIT"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"49pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Command","Application of ","API"]},{"entry":[{},"Example","Command in the Circuit","Example"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"Performance","Influence compression ratio","OpenGL"]},{"entry":[{},"Hints"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0162","num":"0162"},"tables":{"@attributes":{"id":"TABLE-US-00008","num":"00008"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 8"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Drawing Command Structures for Traffic Shaping"},{"entry":"TRAFFIC SHAPER"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"70pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Command","Application of ","API"]},{"entry":[{},"Example","Command in the Circuit","Example"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"Performance Hints","Influence traffic priority for ","OpenGL"]},{"entry":[{},{},"encoded stream",{}]},{"entry":[{},"IDMOVideoOutput-","Influence traffic priority for ","DirectX"]},{"entry":[{},"Optimizations","encoded stream ",{}]},{"entry":[{},"IDMOQualityControl",{},{}]},{"entry":[{},"IVideoWindow"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0163","num":"0163"},"tables":{"@attributes":{"id":"TABLE-US-00009","num":"00009"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 9"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Drawing Command Structures for Power Management"},{"entry":"POWER MANAGEMENT"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"56pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"112pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Command","Application of ","API "]},{"entry":[{},"Example","Command in the Circuit","Example"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"Draw . . .","Track absence of writing commands","GDI"]},{"entry":[{},"Fill . . .\/Floodfill","in area",{}]},{"entry":[{},"Rect","Track absence of writing commands ","OpenGL"]},{"entry":[{},"Viewport","in area",{}]},{"entry":[{},"Raster",{},{}]},{"entry":[{},"Commands"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}}},"While the foregoing is directed to embodiments of the present invention, other and further embodiments of the invention may be devised without departing from the basic scope thereof, and the scope thereof is determined by the claims that follow."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["So that the manner in which the above recited features of the present invention can be understood in detail, a more particular description of the invention, briefly summarized above, may be had by reference to embodiments, some of which are illustrated in the appended drawings. It is to be noted, however, that the appended drawings illustrate only typical embodiments of this invention and are therefore not to be considered limiting of its scope, for the invention may admit to other equally effective embodiments.",{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIGS. 2A-2E"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 25"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 29"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 30"}]},"DETDESC":[{},{}]}
