---
title: Controlling power consumption in object tracking pipeline
abstract: Embodiments related to detecting object information from image data collected by an image sensor are disclosed. In one example embodiment, the object information is detected by receiving a frame of image data from the image sensor and detecting a change in a threshold condition related to an object within the frame. The embodiment further comprises adjusting a setting that changes a power consumption of the image sensor in response to detecting the threshold condition.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09171380&OS=09171380&RS=09171380
owner: MICROSOFT TECHNOLOGY LICENSING, LLC
number: 09171380
owner_city: Redmond
owner_country: US
publication_date: 20111206
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Manage battery life in a mobile device equipped with a visual input sensor may pose challenges. Continuous image capture may require that the device keep track of lighting, focus, and resolution variables while writing the image data to memory. As a result, the mobile device may experience a large current draw during sensor operation. Further, the large current draw may cause the battery and\/or microprocessors inside of the mobile device to become hot, potentially reducing the lifetime of the battery and\/or the microprocessor.","Embodiments related to detecting object information from image data collected by an image sensor are disclosed. For example, one disclosed embodiment comprises receiving a frame of image data from the image sensor and detecting a change in a threshold condition related to an object within the frame. The embodiment further comprises adjusting a setting that changes a power consumption of the image sensor in response to detecting the threshold condition.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter. Furthermore, the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.","Tracking objects often involves capturing a large amount of image data over a short period of time. As such, it may be computationally intensive to track discrete objects from frame-to-frame. These challenges may be magnified for mobile devices, where power supply and form factor considerations may complicate some approaches to addressing the computational demands.","Thus, the disclosed embodiments relate to the detection of object information from image data collected by an image sensor and to adjusting one or more settings that change an image collection, and\/or an image processing setting in response to detecting a change in a threshold condition related to the object. For example, in some embodiments, a hardware setting may be adjusted that changes power consumption for the image sensor. As another example, in some embodiments, a software (or firmware) setting may be adjusted that changes an amount power expended during image data processing. Such adjustments may allow dynamic management of power consumption rates and\/or image data collection rates based how an object moves in a series of frames, where an object is positioned within an individual frame, a comparison between a target detail resolution and\/or motion resolution for the object and respective observed resolutions for the object, and so on.","As the adjustments to such settings may reduce power consumption for an image acquisition device, such as a mobile computing device, the disclosed embodiments have the potential to extend power supply duration. Further, real-time or near real-time adjustment of image data acquisition and processing may be realized. In some examples, a mobile device may conserve power when an object is not being tracked, rapidly scale image data acquisition and\/or processing resources according to motion characteristics and\/or processed image data targets for the object, and\/or throttle back down when the object leaves a field of view for the device. This also may reduce heat generation within the device, which may extend the overall lifetime of the power supply (for example, in the case of a battery pack), and\/or of other components included in the device that may be affected by heat, such as solid state devices and\/or display devices.",{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIGS. 1A","FIG. 1A"],"b":["100","100","100"]},"In the embodiment shown in , computing device  may include a light source  that provides illumination light to a scene  and an image sensor  that captures, for conversion into image data, a portion of the illumination light reflected from scene . The light source  may emit any suitable wavelength of light, including but not limited to visible and\/or infrared wavelengths, and may emit coherent and\/or incoherent light. The image data may include color information, such as RGB, CMYK, and\/or IR information for scene , and\/or gray scale information. For example, each frame of image data may be divided into a plurality of pixels that locate, within the frame, color information for scene .","In some settings, the image data may include depth information for scene  in addition to or in place of the color information described above. Depth information assigned to each pixel may provide spatial location information for objects imaged within the frame. In some embodiments, depth information may be collected to track an object therein. Any suitable object may be tracked. In some embodiments, one or more human users may be tracked while the players interact with an application running on computing device . Tracking articulated objects, like the arms or legs of human users, may allow the physical movements of the articulated objects to act as user inputs to the application. Accordingly, in some embodiments, depth information may be collected to track an articulated object in a frame.","Any suitable light source  and image sensor  may be used to collect image data from scene . For example, in some embodiments, pulsed infrared light may be emitted from light source  and a corresponding reflected light pulse may be captured at image sensor , providing depth information for the scene via time-of-flight and\/or phase difference analysis of the light pulses. Depth information may also be generated by analyzing structured light emitted by light source  and collected by image sensor . As yet another example, depth information may be collected by stereo vision approaches, where image sensor  includes suitable stereo cameras. In some embodiments, image sensor  may include one or more video cameras, (e.g., one or more RGB, CMYK, gray scale, or IR cameras), for collecting color information from scene .","The embodiment of the pipeline  shown in  includes six stages. For illustrative purposes, these stages are grouped into two phases. It will be appreciated that the stages and phases illustrated and described herein are non-limiting conceptual aids. In some embodiments, the processes described herein may occur concurrently and\/or in alternative arrangements. Further, in some embodiments, some of the processes described herein may be omitted. Thus, while the embodiment of pipeline  may accurately track one or more objects within the field of view of image sensor , potentially providing an engaging user experience, the embodiment depicted in  and described herein is provided for illustrative purposes only and is not meant to be limiting in any sense.","As shown in , pipeline  includes an image acquisition phase . In the example shown in , image acquisition phase  includes the stages of pipeline  that manage image acquisition hardware, such light source  and image sensor . As shown, image acquisition phase  includes an illumination control stage , an active sensor area control stage , a frame rate control stage , and a frame resolution control stage .","Illumination control stage  manages the emission of illumination light and, in some embodiments, may have an effect on the collection of reflected illumination light. In some embodiments, illumination control stage  may operatively control light source  via suitable light source control settings. For example, in some embodiments, adjustments to one or more of a light source intensity setting (in specific regions of the image frame or globally across the image frame) and a pulse frequency setting may be made at illumination control stage . Additionally or alternatively, in some embodiments, illumination control stage  may control a direction at which light may be emitted from light source  and\/or an area of image sensor  or a portion of image sensor  that is exposed to reflected illumination light. For example, in some embodiments, a plurality of light sources  may be provided, each light source  being directed, actively and\/or passively, so that a portion of image sensor  is illuminated by reflected illumination light. In one non-limiting example, four light sources  may be arranged in-line to illuminate four respective portions (e.g., a left portion, a left-middle portion, a right-middle portion, and a right portion) of an image sensor .","Active sensor area control stage  manages operation of active areas of image sensor . For example, in some embodiments, active sensor area control stage  may selectively activate portions of image sensor , while other portions of image sensor  are inactive, potentially saving power. Additionally or alternatively, in some embodiments, active sensor area control stage  may operate a plurality of pixels within image sensor  to act as one virtual pixel, potentially increasing the sensitivity of image sensor .","Frame rate control stage  manages the rate at which frames of image data are collected per unit time, referred to herein as a frame capture rate. In some embodiments, frame rate control stage  may operatively control image sensor , and in some embodiments, image sensor  and light source , via suitable adjustments thereto that result in changes to the frame capture rate. In one non-limiting example, thirty frames per second of image data may be captured by image sensor . In some other examples, image data may be captured at less than thirty frames per second, while in still other examples, image data may be captured at or more than thirty frames per second.","Frame resolution control stage  manages the resolution at which frames of image data are collected. In some embodiments, collection resolution may be expressed as a pixel density (e.g., 130 pixels per inch), as a frame dimension (e.g., 1920 pixels\u00d71080 pixels), as a number of lines of video (e.g., 1080p or 1080i). In some embodiments, frame resolution control stage  may suitably adjust image sensor  to vary the collection resolution of image data.","Turning to , frames of image data collected from the image sensor, such as an example frame  shown in , are received by an object tracking phase  of pipeline . Object tracking phase  includes the stages of pipeline  that manage identifying and extracting information about objects in frame . As shown, object tracking phase  may include segmentation stage , classification stage , and model fitting stage .","Segmentation stage  distinguishes objects within the frame from other objects that occupy background regions of the frame. As used herein, the term background refers to anything in the frame that is not a part of the object being tracked, even if the background object is positioned in front of the object being tracked. Distinguishing objects being tracked from background objects may increase tracking efficiency or simplify downstream processing. In the example shown in , background objects have been removed from frame , leaving image data for an articulated object  to be passed to classification stage . Thus, segmentation stage  may extract object information from a frame of image data. In one example, a pixel associated with an upper right arm of the articulated object  shown in  may be stored with image data including one or more of a pixel address, color information, and depth information.","Classification stage  analyzes an object to determine what kind of object the image data associated with it represents. For example, each pixel of the image data associated with articulated object  may be assessed to determine a body part with which that pixel belongs. In the embodiment shown in , the image data associated with articulated object  is depicted as a classified structure . Thus, classification stage  may extract object information from a frame of image data. In some embodiments, classification stage  may supplement object information extracted in segmentation stage . Continuing with the example upper right arm pixel information described above, the pixel address, color information, and depth information may be supplemented with a body part index (labeled bp9 in the example shown in ) and a confidence value associated with the classification.","Model fitting stage  finds one or more possible models that act as machine representations of articulated object . Suitable models may include volumetric representations of objects (e.g., ellipsoidal, cuboidal, and\/or polygonal representations of objects), skeletal representations of objects (e.g., ball and stick representations of objects), and so on.","In the example shown in , a skeleton  is used as a machine representation of articulated object . Skeleton  may have a lower dimensionality than articulated object , even if scaled to approximately the same spatial representation, and may increase downstream processing as a result. In some embodiments, each joint of skeleton  may have a three-dimensional coordinate locating that joint within the frame of image data. Thus, model fitting stage  may extract object information from a frame of image data, and may supplement object information extracted in segmentation stage  and\/or classification stage  in some embodiments. Continuing the example upper right arm, a shoulder joint (labeled  in ) may be fit by a suitable model fitting routine and stored in a data structure along with color information and depth information for that location.","It will be appreciated that suitable object motion information for the object may also be extracted at one or more of the stages within object tracking phase  in some embodiments. Extracting motion information may permit kinetic information for the object to be extracted and\/or modeled. In turn, the kinetic information may provide user input to computing device  and\/or information about projected movement of the object. For example, extracted kinetic information may be used to generate a motion vector for the object. In some settings, motion vector information may be provided via APIs and the like to consumers. In one scenario, a motion vector for a user's arm may be provided to a boxing game application that uses the information to determine whether the user landed a knockout blow to an opponent. In some other settings, motion vector information may be consumed by pipeline  and\/or computing device . For example, a motion vector for the user's arm may be used to determine potential trajectories for the user's arm.","Once fit at model fitting stage , the model (for example, skeleton  in the example shown in ) may be provided to other applications as output  from pipeline . Output  may be consumed in any suitable manner by a receiving application. For example, in some embodiments, an application programming interface may receive output  and communicate the model and\/or other image data information related to the model included in output  to a receiving application, such as gaming application. The receiving application may then use communicated output  in any suitable way. It will be understood that, in some embodiments, only a portion of a model may be provided as output. Further, it will be understood that the pipeline described with reference to  is presented for the purpose of example, and is not intended to be limiting in any manner.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":["FIG. 2","FIG. 2"],"b":["200","200","200","200","200","200","200"]},"At , method  includes receiving a frame of image data from the image sensor and, at , detecting an object within the frame. As used herein, detecting an object may refer to detecting a threshold condition related to an object in an image frame. Any suitable threshold condition may be detected. For example, the threshold condition may correspond to a change in presence or absence of an object in the image frame. As another example, the threshold condition may comprise a change in image data characteristics that may increase or decrease a difficulty of recognizing the object at the current settings. Such characteristics may relate to size, speed of motion (e.g. blur), orientation, and\/or any other suitable characteristics of the object in the image frame. Such detection of the threshold condition may occur at any suitable stage within a pipeline for tracking an object. For example, in some embodiments, the object may be detected at a pipeline stage related to image data acquisition. In some other embodiments, the object may be detected at a pipeline stage related to extracting and\/or processing acquired image data.","In some embodiments, detecting the threshold condition may include, at , extracting object information for the object from the first frame of image data. It will be appreciated that the object information may be extracted from the image data in any suitable manner. For example, in some embodiments, the object information may be extracted at a suitable stage in a pipeline for tracking one or more objects. Example stages include, but are not limited to, segmentation stages, classification stages, and model fitting stages such as those described herein.","In some embodiments, object motion information may be extracted from a plurality of frames. For example, a series of frames including a particular object may provide image data from which kinetic and\/or other motion information for that object may be extracted.","In some embodiments, extracting object information at  may include, at , recognizing the object. For example, the object information extracted may include information that allows the image data associated with the object to be recognized, identified, classified, and\/or fit to one or more model candidates. In such embodiments, recognition may occur at any suitable stage within a pipeline for tracking an object, including segmentation, classification, and\/or model fitting stages like those described herein.","In some embodiments, extracting articulating information at  may include, at , identifying a background region of the frame that excludes the object. In some of such embodiments, identification of the background region may be performed in response to recognition of the object. For example, identification of an object at a segmentation stage may occur concurrently with identification of a background region. Identifying such background regions may permit those regions to be ignored during downstream processing, potentially simplifying downstream processing and\/or improving tracking efficiency. In turn, improved processing may conserve power and extend power supply duration.","At , method  includes adjusting a setting in response to detecting the threshold condition in one or more frames of image data. It will be appreciated that any suitable setting related to a pipeline for tracking an object may be adjusted without departing from the scope of the present disclosure. As used herein, a setting may refer to a hardware setting and\/or a software setting. A hardware setting may refer to one or more settings that control operational aspects of an image data acquisition device. Any suitable device setting may be adjusted without departing from the scope of the present disclosure. Non-limiting examples of hardware settings include device illumination settings, device frame rate settings, and device frame resolution settings.","A software setting may refer to one or more settings that control operational aspects of an image processing pipeline. Any suitable software setting may be adjusted without departing from the scope of the present disclosure. Non-limiting examples of image processing pipeline settings include segmentation resolution settings, classification resolution settings, and model fitting resolution settings.","It will be appreciated that hardware and software settings may be related in some embodiments. For example, an image resolution may be adjusted in hardware by sampling a lesser number of pixels and\/or logically tying groups of pixels together, and in software by downsampling an image after acquisition.","In some embodiments, adjusting a setting at  may include, at , adjusting a setting that changes a power consumption of an image acquisition device, in response to detecting the object. Any suitable setting may be adjusted to change the power consumption of the image acquisition device, including but not limited to hardware settings changes that affect the power consumption of a light source, an image sensor, a camera, and\/or other hardware components. For example, an adjustment may be made that increases power consumption of a camera in response to detecting a change in a threshold condition that indicates that more image data would be helpful for object tracking. By maintaining power settings at a lower consumption setting unless and until more information is desired, power may be conserved both by reducing hardware usage, and reducing an amount of data provided for downstream processing of image frames.","As a more specific example, in some embodiments, settings for collecting image data for selected portions of an image frame data may be configured to operate at reduced power levels (e.g. reduced illuminating settings and\/or resolution settings) as a default state until an initial presence of the object is detected and\/or recognized within a field of view of the image acquisition device. Once the initial presence of the object is detected, the settings may be adjusted so that power levels are increased to thereby obtain better data for object tracking.",{"@attributes":{"id":"p-0043","num":"0042"},"figref":["FIG. 3","FIG. 3","FIG. 3"],"b":["300","302","304","304","302","306","304","302","302","304","306","302","304"]},"It will be appreciated that other settings may be adjusted that change the power consumption of the image acquisition device in response to detecting the object. For example, in some embodiments, a frame collection rate setting may be adjusted to vary power consumption for an image acquisition device. Increasing a rate at which frames of image data are collected may increase the power consumption for the image acquisition device. Thus, a setting for a frame collection rate may initially be set to a first, lower value until a change in a suitable threshold condition (e.g. initial presence of an object or a change in rate of motion of the object between image frames) is detected, in response to which the setting for the frame collection rate may be set to a second, higher value.","Additionally or alternatively, in some embodiments, adjustments made to a frame resolution setting may be used to vary power consumption for an image acquisition device. For example, in some embodiments, a frame resolution setting may be adjusted to vary power consumption for an image acquisition device in response to initial detection of an object. Increasing the number of pixels captured in each frame of image data may increase the amount of power consumed by the image acquisition device. In some embodiments, a setting for a frame resolution may initially be set to a first, lower value until an initial presence of the object is detected, after which the setting for the frame resolution may be set to a second, higher value. As another example, a frame resolution setting may be adjusted in response to detected changes in a threshold tracking condition related to the object. In this example, a setting for a frame resolution setting may be set to a first, lower value until it is determined that more image detail is desired for object tracking. In response to the detected change in the threshold tracking condition, the setting for the frame resolution setting may be adjusted to a second, higher value. For example, the frame resolution setting may be adjusted from 80\u00d760 pixels to 320\u00d7240 pixels, or to any suitable frame resolution. In some embodiments, such adjustments may be made continuously at a predetermined rate (e.g., according to a predetermined change in resolution per time), though it will be appreciated that any suitable manner of adjusting the setting may be employed without departing from the scope of the present disclosure. Such adjustments may conserve power during conditions when less detail may be desired, permitting lower frame resolution settings that may extend power supply duration.","The adjustments described above each may affect the amount of image data collected per frame or per unit time, as shown in  at . For example, the amount of image data per unit time collected by an image sensor may be increased in situations where more image data is acquired (e.g. upon initial detection of an object, and\/or while trying to image fine details or track fast motion). Likewise, an amount of image data per unit time collected by the image sensor may be decreased, for example, when no object is detected, and\/or when less detailed image data may be used for tracking (e.g. when motion is slow, when tracking large body parts, etc.).","In some embodiments where a background region is identified, a predicted candidate exclusion region that is projected to exclude the object in a later image data frame may be identified within the background region. Identification of the candidate exclusion region may be based on object information extracted from a previously-received frame or series of frames based upon applied constraints. For example, in some embodiments, motion vector information and\/or other kinetic information for a particular object extracted from previously-received frames may be extracted from a plurality of frames. The extracted motion vector information may provide a potential trajectory for that object based upon constraints related to the particular body part being tracked. In turn, adjustments may be made to a setting that decreases an image data per unit time collected by the image sensor within the candidate exclusion region in a later frame of image data.","It will be appreciated that adjustments may also be made to settings that affect an amount of object information extracted from image data and\/or otherwise processed. By potentially simplifying downstream processing of the image data and\/or increasing tracking efficiency, such adjustments may extend power source duration for a computing device. Thus, in some embodiments, adjusting a setting at  may include, at , adjusting an object information extraction parameter that changes the object information extraction process.","For example, in some embodiments, adjustments may be made to a segmentation resolution parameter that decreases an amount of object information extracted from image data. In some embodiments, such settings may be dynamically adjusted at a segmentation stage of an object tracking pipeline. In some settings, such an adjustment may result in downsampling the extracted image data to a lower processing resolution. Downsampling may allow the observed image data to be more readily utilized and\/or processed with less computing overhead. Additionally or alternatively, in some settings, such an adjustment may include removing and\/or smoothing noise variation included in the image data, which may enhance data integrity and\/or downstream processing.","As another example, in some embodiments, adjustments may be made to a classification resolution parameter that decreases an amount of object information extracted from image data. In some embodiments, such settings may be dynamically adjusted at a classification stage of an object tracking pipeline. In some examples, reducing a number of pixels that are classified may reduce power consumption, potentially extending power supply lifetime.  schematically shows an embodiment of decreasing pixel classification resolution for an object from a first, higher classification resolution (shown at ) to a second, lower classification resolution (shown at ).","As yet another example, in some embodiments, adjustments may be made to a model fitting resolution parameter that decreases an amount of object information extracted from image data. In some embodiments, such settings may be dynamically adjusted at a model fitting stage of an object tracking pipeline. For example, in some embodiments, a resolution at which rasterized volumetric models may be compared with classified objects may be decreased in response to a setting adjustment. As another example, in some embodiments, a different type of representation may be selected at the model fitting stage in response to a setting adjustment.  schematically shows an embodiment of decreasing classification resolution for an object from a first, higher classification resolution using a volumetric representation (shown at ) to a second, lower classification using a skeletal resolution (shown at ). In some examples, reducing a number of pixels compared during model fitting, such as by downsampling the resolution or by selecting a different representation type, may reduce power consumption and\/or speed processing at an application receiving the output from the pipeline.","While the examples described above relate to decreasing an amount of object information extracted from image data, it will be appreciated that in some other embodiments, adjustments may be made that increase an amount of object information extracted from image data. In some settings, additional information may be extracted in situations where higher levels of detail may assist in image processing. For example, higher frame collection rates may assist in tracking faster motion of the object. As another example, higher frame collection, segmentation, classification, and\/or model fitting resolutions may assist when trying acquire and\/or process the image data.","Further, in some embodiments, an adjustment in one setting that increases power consumption for one portion of a device may be accompanied by an adjustment to another setting that decreases power consumption in another portion of the device. In some examples, these competing adjustments may result in offsetting and\/or net negative power consumption changes, potentially extending power supply lifetime in the device.","At , method  includes extracting object information from a second frame of image data using the adjusted setting. It will be appreciated that the second frame of image data may be a previously-received frame of image data or a subsequently-received frame of image data. In some embodiments, the adjusted setting may be applied by feeding the setting forward or backward in a pipeline.",{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIG. 6","FIG. 6","FIGS. 1A-1B","FIG. 6","FIG. 6","FIG. 6"],"b":["600","600","108","602","602","600","604","604","606","608","600","604","600","604","600","2","600","600"]},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 7","b":["700","700","700"]},"Computing system  includes a logic subsystem  and a data-holding subsystem . Computing system  may optionally include a display subsystem , a communication subsystem , and\/or other components not shown in . Computing system  may also optionally include user input devices such as keyboards, mice, game controllers, cameras, microphones, and\/or touch screens, for example.","Logic subsystem  may include one or more physical devices configured to execute one or more instructions. For example, logic subsystem  may be configured to execute one or more instructions that are part of one or more applications, services, programs, routines, libraries, objects, components, data structures, or other logical constructs. Such instructions may be implemented to perform a task, implement a data type, transform the state of one or more devices, or otherwise arrive at a desired result.","Logic subsystem  may include one or more processors that are configured to execute software instructions. Additionally or alternatively, logic subsystem  may include one or more hardware or firmware logic machines configured to execute hardware or firmware instructions. Processors of logic subsystem  may be single core or multicore, and the programs executed thereon may be configured for parallel or distributed processing. Logic subsystem  may optionally include individual components that are distributed throughout two or more devices, which may be remotely located and\/or configured for coordinated processing. One or more aspects of logic subsystem  may be virtualized and executed by remotely accessible networked computing devices configured in a cloud computing configuration.","Data-holding subsystem  may include one or more physical, non-transitory, devices configured to hold data and\/or instructions executable by logic subsystem  to implement the herein described methods and processes. When such methods and processes are implemented, the state of data-holding subsystem  may be transformed (e.g., to hold different data).","Data-holding subsystem  may include removable media and\/or built-in devices. Data-holding subsystem  may include optical memory devices (e.g., CD, DVD, HD-DVD, Blu-Ray Disc, etc.), semiconductor memory devices (e.g., RAM, EPROM, EEPROM, etc.) and\/or magnetic memory devices (e.g., hard disk drive, floppy disk drive, tape drive, MRAM, etc.), among others. Data-holding subsystem  may include devices with one or more of the following characteristics: volatile, nonvolatile, dynamic, static, read\/write, read-only, random access, sequential access, location addressable, file addressable, and content addressable. In some embodiments, logic subsystem  and data-holding subsystem  may be integrated into one or more common devices, such as an application specific integrated circuit or a system on a chip.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 7","b":["710","710"]},"It is to be appreciated that data-holding subsystem  includes one or more physical, non-transitory devices. In contrast, in some embodiments aspects of the instructions described herein may be propagated in a transitory fashion by a pure signal (e.g., an electromagnetic signal, an optical signal, etc.) that is not held by a physical device for at least a finite duration. Furthermore, data and\/or other forms of information pertaining to the present disclosure may be propagated by a pure signal.","The terms \u201cmodule,\u201d \u201cprogram,\u201d and \u201cengine\u201d may be used to describe an aspect of computing system  that is implemented to perform one or more particular functions. In some cases, such a module, program, or engine may be instantiated via logic subsystem  executing instructions held by data-holding subsystem . It is to be understood that different modules, programs, and\/or engines may be instantiated from the same application, service, code block, object, library, routine, API, function, etc. Likewise, the same module, program, and\/or engine may be instantiated by different applications, services, code blocks, objects, routines, APIs, functions, etc. The terms \u201cmodule,\u201d \u201cprogram,\u201d and \u201cengine\u201d are meant to encompass individual or groups of executable files, data files, libraries, drivers, scripts, database records, etc.","It is to be appreciated that a \u201cservice\u201d, as used herein, may be an application program executable across multiple user sessions and available to one or more system components, programs, and\/or other services. In some implementations, a service may run on a server responsive to a request from a client.","When included, display subsystem  may be used to present a visual representation of data held by data-holding subsystem . As the herein described methods and processes change the data held by the data-holding subsystem, and thus transform the state of the data-holding subsystem, the state of display subsystem  may likewise be transformed to visually represent changes in the underlying data. Display subsystem  may include one or more display devices utilizing any suitable type of technology. Such display devices may be combined with logic subsystem  and\/or data-holding subsystem  in a shared enclosure, or such display devices may be peripheral display devices.","When included, a communication subsystem  may be configured to communicatively couple computing system  with one or more other computing devices. Communication subsystem  may include wired and\/or wireless communication devices compatible with one or more different communication protocols. As non-limiting examples, the communication subsystem may be configured for communication via a wireless telephone network, a wireless local area network, a wired local area network, a wireless wide area network, a wired wide area network, etc. In some embodiments, the communication subsystem may allow computing system  to send and\/or receive messages to and\/or from other devices via a network such as the Internet.","It is to be understood that the configurations and\/or approaches described herein are exemplary in nature, and that these specific embodiments or examples are not to be considered in a limiting sense, because numerous variations are possible. The specific routines or methods described herein may represent one or more of any number of processing strategies. As such, various acts illustrated may be performed in the sequence illustrated, in other sequences, in parallel, or in some cases omitted. Likewise, the order of the above-described processes may be changed.","The subject matter of the present disclosure includes all novel and nonobvious combinations and subcombinations of the various processes, systems and configurations, and other features, functions, acts, and\/or properties disclosed herein, as well as any and all equivalents thereof."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0005","num":"0004"},"figref":"FIGS. 1A and 1B"},{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
