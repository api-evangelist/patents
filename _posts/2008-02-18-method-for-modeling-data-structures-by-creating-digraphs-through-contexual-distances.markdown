---
title: Method for modeling data structures by creating digraphs through contexual distances
abstract: A method for modeling data affinities and data structures. In one implementation, a contextual distance may be calculated between a selected data point in a data sample and a data point in a contextual set of the selected data point. The contextual set may include the selected data point and one or more data points in the neighborhood of the selected data point. The contextual distance may be the difference between the selected data point's contribution to the integrity of the geometric structure of the contextual set and the data point's contribution to the integrity of the geometric structure of the contextual set. The process may be repeated for each data point in the contextual set of the selected data point. The process may be repeated for each selected data point in the data sample. A digraph may be created using a plurality of contextual distances generated by the process.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07970727&OS=07970727&RS=07970727
owner: Microsoft Corporation
number: 07970727
owner_city: Redmond
owner_country: US
publication_date: 20080218
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application claims benefit of U.S. provisional patent application Ser. No. 60\/998,683, filed Oct. 12, 2007, which is incorporated herein by reference.","In general, the field of machine learning describes the design and development of algorithms and techniques that allow computers to \u201clearn\u201d. A major focus of machine learning research may be to extract information from data automatically. One particular area of machine learning may be concerned with detecting structures in data, which may also be known as structural perception.","Structural perception of data plays a fundamental role in pattern analysis and machine learning. Classical methods to perform structural analysis of data include principal component analysis (PCA) and multidimensional scaling (MDS) which perform dimensionality reduction by preserving global structures of data. Another current method to perform structural analysis of data may be non-negative matrix factorization (NMF) which learns local representations of data. K-means may also be frequently employed to identify underlying clusters in data. The underlying assumption behind the above methods may be that spaces in which data points, or data samples, lie are Euclidean. In other current methods, a non-Euclidean perception of data may be used. Nonlinear structures of data may be modeled by preserving global (geodesic distances for Isomap) or local (locally linear fittings for LLE) geometry of data manifolds. These two methods directed the structural perception of data in manifold ways.","In recent years, spectral graph partitioning has become a powerful tool for structural perception of data. The representative methods may be the normalized cuts for image segmentation and the Ng, Jordan and Weiss (NJW) algorithm for data clustering. For traditional spectral clustering, the structure of data may be modeled by undirected weighted graphs, and underlying clusters are found by graph embeddings. For example, the method may be used to find clusters from spectral properties of normalized weighted adjacency matrices. For semi-supervised structural perception, it may be necessary to detect partial manifold structures of data, given one or more labeled points on data manifolds. Transductive inference (or ranking) may be performed on data manifolds or graph data.","However, existing spectral methods for the structural perception of data may not be robust enough to achieve good results when the structures of data are contaminated by noise points. In addition, structural perception may not be described well in traditional Euclidean based distances. A method for structural analysis of data that may correctly perceive data structures from noisy data may be needed.","Described herein are implementations of various techniques for modeling data structures. In one implementation, a contextual distance may be calculated between a selected data point in a data sample and a data point in a contextual set of the selected data point. The contextual set may include the selected data point and one or more data points in the neighborhood of the selected data point. The contextual distance may be the difference between the selected data point's contribution to the integrity of the geometric structure of the contextual set and the data point's contribution to the integrity of the geometric structure of the contextual set. A contextual distance may be calculated between the selected data point and each data point in the contextual set of the selected data point. The process may be repeated until each data point in the data sample has been the selected data point. A digraph may then be created using a plurality of contextual distances generated by the above process.","The digraph may be used to calculate local digraph Laplacians. The local digraph Laplacians may be aligned to derive a global digraph Laplacian. A digraph embedding of the global digraph Laplacian may be created by computing one or more eigenvectors of the global digraph Laplacian. Noise data points from the digraph embedding may be removed to isolate the data structures. In addition, the global digraph Laplacian may be used to rank the data points in the data sample and clustering may be performed on feature points in the digraph embedding.","The above referenced summary section is provided to introduce a selection of concepts in a simplified form that are further described below in the detailed description section. The summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter. Furthermore, the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.","The discussion below is directed to certain specific implementations. It is to be understood that the discussion below is only for the purpose of enabling a person with ordinary skill in the art to make and use any subject matter defined now or later by the patent \u201cclaims\u201d found in any issued patent herein.","Conventional methods for modeling data structures in data may not accurately separate data structures from noise; however, a method similar to human perception may be more accurate.  illustrate that human perception may rely on context.  illustrates a perceptual problem for the central character in the figure. In , a human may perceive the central character to be a number \u201c13\u201d. In , a human may perceive the central character to be a letter \u201cB\u201d. This implies that the same physical stimulus may be perceived differently in different contexts. This demonstrates that the perceptual relationship between two sample data points heavily relies on the context to which they belong.","The following paragraphs generally describe one or more implementations of various techniques directed to a method for modeling data affinities and data structures in data. First, given a data sample, the contextual set of each data point may be determined. The contextual set of a selected data point may be defined as the selected data point and the data points in the neighborhood of the selected data point. Next, a contextual set descriptor may be selected. The contextual set descriptor may be defined as the structural descriptor of a geometric structure to depict some global structural characteristics. The contextual distance between each data point and every data point in its contextual set may be calculated using the contextual set descriptor. The contextual distance between two data points may be defined by the difference of their contribution to the integrity of the geometric structure of the contextual set. A directed graph (digraph) may be created using the contextual distance to model asymmetry of perception, which is induced by the asymmetry of contextual distances. The edges of the digraph may be weighted based on the contextual distances. The digraph may then be used to calculate local digraph Laplacians. The local digraph Laplacians may be aligned to calculate a global digraph Laplacian. An embedding, which are representations of data in low dimensional Euclidean space, of the global digraph Laplacian may be created by computing the eigenvectors of the global digraph Laplacian. The noise may be removed from the embedding to separate the data structures. The separated data structures may be used in various applications such as recognition and searching. With contextual distances and digraph embeddings, structures of data may be robustly retrieved even when there is heavy noise.","One or more techniques for modeling data affinities and data structures using local contexts in accordance with various implementations are described in more detail with reference to  in the following paragraphs.","Implementations of various techniques described herein may be operational with numerous general purpose or special purpose computing system environments or configurations. Examples of well known computing systems, environments, and\/or configurations that may be suitable for use with the various techniques described herein include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and the like.","The various techniques described herein may be implemented in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The various techniques described herein may also be implemented in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network, e.g., by hardwired links, wireless links, or combinations thereof. In a distributed computing environment, program modules may be located in both local and remote computer storage media including memory storage devices.",{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 2","b":["200","200"]},"The computing system  may include a central processing unit (CPU) , a system memory  and a system bus  that couples various system components including the system memory  to the CPU . Although only one CPU is illustrated in , it should be understood that in some implementations the computing system  may include more than one CPU. The system bus  may be any of several types of bus structures, including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus. The system memory  may include a read only memory (ROM)  and a random access memory (RAM) . A basic input\/output system (BIOS) , containing the basic routines that help transfer information between elements within the computing system , such as during start-up, may be stored in the ROM .","The computing system  may further include a hard disk drive  for reading from and writing to a hard disk, a magnetic disk drive  for reading from and writing to a removable magnetic disk , and an optical disk drive  for reading from and writing to a removable optical disk , such as a CD ROM or other optical media. The hard disk drive , the magnetic disk drive , and the optical disk drive  may be connected to the system bus  by a hard disk drive interface , a magnetic disk drive interface , and an optical drive interface , respectively. The drives and their associated computer-readable media may provide nonvolatile storage of computer-readable instructions, data structures, program modules and other data for the computing system .","Although the computing system  is described herein as having a hard disk, a removable magnetic disk  and a removable optical disk , it should be appreciated by those skilled in the art that the computing system  may also include other types of computer-readable media that may be accessed by a computer. For example, such computer-readable media may include computer storage media and communication media. Computer storage media may include volatile and non-volatile, and removable and non-removable media implemented in any method or technology for storage of information, such as computer-readable instructions, data structures, program modules or other data. Computer storage media may further include RAM, ROM, erasable programmable read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM), flash memory or other solid state memory technology, CD-ROM, digital versatile disks (DVD), or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by the computing system . Communication media may embody computer readable instructions, data structures, program modules or other data in a modulated data signal, such as a carrier wave or other transport mechanism and may include any information delivery media. The term \u201cmodulated data signal\u201d may mean a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media may include wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media. Combinations of any of the above may also be included within the scope of computer readable media.","A number of program modules may be stored on the hard disk, magnetic disk , optical disk , ROM  or RAM , including an operating system , one or more application programs , a data structure modeling module , program data  and a database system . The operating system  may be any suitable operating system that may control the operation of a networked personal or server computer, such as Windows\u00ae XP, Mac OS\u00ae X, Unix-variants (e.g., Linux\u00ae and BSD\u00ae), and the like. The data structure modeling module  will be described in more detail with reference to  in the paragraphs below.","A user may enter commands and information into the computing system  through input devices such as a keyboard  and pointing device . Other input devices may include a microphone, joystick, game pad, satellite dish, scanner, or the like. These and other input devices may be connected to the CPU  through a serial port interface  coupled to system bus , but may be connected by other interfaces, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device may also be connected to system bus  via an interface, such as a video adapter . In addition to the monitor , the computing system  may further include other peripheral output devices, such as speakers and printers.","Further, the computing system  may operate in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be another personal computer, a server, a router, a network PC, a peer device or other common network node. Although the remote computer  is illustrated as having only a memory storage device , the remote computer  may include many or all of the elements described above relative to the computing system . The logical connections may be any connection that is commonplace in offices, enterprise-wide computer networks, intranets, and the Internet, such as local area network (LAN)  and a wide area network (WAN) .","When using a LAN networking environment, the computing system  may be connected to the local network  through a network interface or adapter . When used in a WAN networking environment, the computing system  may include a modem , wireless router or other means for establishing communication over a wide area network , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the serial port interface . In a networked environment, program modules depicted relative to the computing system , or portions thereof, may be stored in a remote memory storage device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.","It should be understood that the various techniques described herein may be implemented in connection with hardware, software or a combination of both. Thus, various techniques, or certain aspects or portions thereof, may take the form of program code (i.e., instructions) embodied in tangible media, such as floppy diskettes, CD-ROMs, hard drives, or any other machine-readable storage medium wherein, when the program code is loaded into and executed by a machine, such as a computer, the machine becomes an apparatus for practicing the various techniques. In the case of program code execution on programmable computers, the computing device may include a processor, a storage medium readable by the processor (including volatile and non-volatile memory and\/or storage elements), at least one input device, and at least one output device. One or more programs that may implement or utilize the various techniques described herein may use an application programming interface (API), reusable controls, and the like. Such programs may be implemented in a high level procedural or object oriented programming language to communicate with a computer system. However, the program(s) may be implemented in assembly or machine language, if desired. In any case, the language may be a compiled or interpreted language, and combined with hardware implementations.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 3","b":"300"},"At step , the contextual set for each data point may be determined. As described above, the contextual set of a selected data point may be defined as the selected data point and the data points in the neighborhood of the selected data point. Let S={x, . . . , x} be the set of m sample data points in R. The contextual set Sof the point xmay consist of xand its nearest neighbors in some distance, e.g. Euclidean distance. The contextual set may be written as S={x, x, . . . , x}, where xis the j-th nearest neighbor of x, K is the number of nearest neighbors and iis set to i=i.","At step , a contextual set descriptor may be selected. The contextual set descriptor may be user selected or designed into the algorithm. As described above, a contextual set descriptor may be defined as the structural descriptor of a geometric structure to depict some global structural characteristics. The geometric structure of Smay be of interest. A structural descriptor f(S) of Smay depict some global structural characteristics of S. If a point xcomplies with the structure of S, then removing xfrom Smay not affect the structure substantially. In contrast, if the point xis noise or a data point in a different cluster, then removing xfrom Smay change the structure significantly. Equation 1 may be defined as the contribution of xto the integrity of the structure of S, i.e., the variation of the descriptor with and without x:\n\n\u03b4()\u2212(})|, 0, 1, . . . , \u2003\u2003Equation 1\n\nwhere | | denotes the absolute value for a scalar and a kind of norm for a vector. The descriptor f(S) may not be unique. However, f(S) may need to satisfy the structural consistency among the data points in S, in the sense that \u03b4fmay be relatively small if xis compatible with the global structure formed by sample data points in Sand relatively large if not.\n","The contextual distance from xto xmay be defined as\n\n()=|\u03b40, 1, . . . , \u2003\u2003Equation 2\n\nwhere the notation \u2192 emphasizes that the distance is in the direction from xto x. As such, p(x\u2192x)\u22670 and the equality holds if j=0. The contextual distance p(x\u2192x) defined above may be consistent with the concept that context influences structural perception. The set S, consisting of the point xand its nearest neighbors {x, x, . . . , x}, may be taken as the context for computing the distances from xto its neighbors. The relative perception may be modeled by investigating how much the structure of Schanges by removing a point from S. It should be noted that the contextual distance defined in Equation 2 may be asymmetric because p(x\u2192x) may not necessarily be equal to p(x\u2192x), and in the extreme case xmay even not be in the contextual set of x. The contextual distance may heavily rely on the structural characteristic of the contextual set.\n","The following may be some examples of contextual set descriptors which may be applied for computing the contextual distances. First, the contextual set descriptor may be a trivial descriptor where the Euclidean distance may be a special case of contextual distance. In this case, K=1 and f(S)=\u03b3x+(1\u2212\u03b3)x, where \u03bb<0 or \u03b3>1. The norm in Equation 1 may be the Euclidean norm \u2225 \u2225. It may be verified that p(x\u2192x)=\u2225x\u2212x\u2225. Therefore, the contextual distance may coincide with the Euclidean distance in this special case.","Next, the contextual set descriptor may be a geometric descriptor, such as a centroid. Let K>1 and x(S) denote the centroid of S, i.e.,",{"@attributes":{"id":"p-0045","num":"0044"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"msub":{"mi":["x","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["S","i","x"]}}},{"mfrac":{"mn":"1","mrow":{"mi":"K","mo":"+","mn":"1"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"0"},"mi":"K"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":{"mi":"x","msub":{"mi":["i","j"]}},"mo":"\u00b7","mrow":{"msub":{"mi":["x","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["S","i","x"]}}}}}}],"mo":"="}}},"br":{},"sub":["i","i",{"sub2":"j "},"i","i","i","i","i"],"sup":["x","x ","x","x","x"]},"Another example of a contextual set descriptor may be an informative descriptor, such as coding length. The coding length L(S) of a vector-valued set Smay be the intrinsic structural characterization of the set. L(S) may be exploited as a kind of scalar-valued descriptor of S, i.e. f(S)=L(S). L(S) may be defined as follows. Let X=\u2514x, x, . . . , x\u2518 and",{"@attributes":{"id":"p-0047","num":"0046"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mover":{"mi":["x","_"]},"mi":"i"},"mo":"=","mrow":{"mfrac":{"mn":"1","mrow":{"mi":"K","mo":"+","mn":"1"}},"mo":["\u2062","\u2062"],"msub":{"mi":["X","i"]},"mi":"e"}},"mo":","}}},"br":{},"o":["X","x"],"sub":["i","i","i","i "],"sup":"T"},{"@attributes":{"id":"p-0048","num":"0047"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["S","i"]}}},{"mrow":[{"mfrac":{"mrow":{"mi":["K","n"],"mo":["+","+"],"mn":"1"},"mn":"2"},"mo":["\u2062","\u2062","\u2062"],"mi":"log","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"det","mo":["(",")"],"mrow":{"mi":"I","mo":"+","mrow":{"mfrac":{"mi":"n","mrow":{"msup":{"mi":"\u025b","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"K","mo":"+","mn":"1"}}}},"mo":["\u2062","\u2062"],"msub":{"mover":{"mi":["X","_"]},"mi":"i"},"msubsup":{"mover":{"mi":["X","_"]},"mi":["i","T"]}}}}},{"mfrac":{"mi":"n","mn":"2"},"mo":"\u2062","mrow":{"mi":"log","mo":["(",")"],"mrow":{"mn":"1","mo":"+","mfrac":{"mrow":{"msubsup":{"mover":{"mi":["x","_"]},"mi":["i","T"]},"mo":"\u2062","msub":{"mover":{"mi":["x","_"]},"mi":"i"}},"msup":{"mi":"\u025b","mn":"2"}}}}}],"mo":"+"}],"mo":"="}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mn":"3"}}]}}}},"br":{}},{"@attributes":{"id":"p-0049","num":"0048"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"det","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"I","mo":"+","mrow":{"mfrac":{"mi":"n","mrow":{"msup":{"mi":"\u025b","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"K","mo":"+","mn":"1"}}}},"mo":["\u2062","\u2062"],"msub":{"mover":{"mi":["X","_"]},"mi":"i"},"msubsup":{"mover":{"mi":["X","_"]},"mi":["i","T"]}}}}},{"mi":"det","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"I","mo":"+","mrow":{"mfrac":{"mi":"n","mrow":{"msup":{"mi":"\u025b","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"K","mo":"+","mn":"1"}}}},"mo":["\u2062","\u2062"],"msubsup":{"mover":{"mi":["X","_"]},"mi":["i","T"]},"msub":{"mover":{"mi":["X","_"]},"mi":"i"}}}}}],"mo":"="}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"4"}}]}}}},"br":{},"sub":["i","i"],"sup":["x","x"]},{"@attributes":{"id":"p-0050","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"\u025b","mo":"=","mrow":{"msqrt":{"mfrac":{"mrow":{"mn":"10","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"n"},"mi":"K"}},"mo":"."}}}}},"At step , the contextual distance between each data point and every data point in its contextual set may be calculated. As mentioned above, the contextual distance between two data points may be defined by the difference of their contribution to the integrity of the geometric structure of the contextual set. Contextual distance may only be defined within contextual sets of data points. Typical Euclidean distance measures the distance between two data points and is symmetric in that the distance from point A to point B is the same as the distance from point B to point A. Contextual distance may be asymmetric in that the distance from point A to point B may not equal the distance from point B to point A. This occurs because different contextual sets of different points may be considered in the contextual distance computation. In this manner, the contextual distance defined here may be a kind of dissimilarity instead of a formal distance in the mathematical sense.",{"@attributes":{"id":"p-0052","num":"0051"},"figref":["FIGS. 4A-F","FIG. 4A","FIG. 4B","FIGS. 4D and 4E"],"b":["440","440","470","480","490","440","410","420"]},"However,  illustrates the contextual distances from data point  to the other data points. The contextual distances from data point  to data points  and  in Cluster II may be much larger than the contextual distances to the data points in Cluster I including data points ,  and .  illustrates the 2-D representation of the data using contextual distances. Cluster I and Cluster II are well separated now and clearly distinguishable. In essence, contextual distances model human perception. In , consider the perceptual relationship between data point  and the \u2018o\u2019 markers making up Cluster I. Using human perception, Cluster I may be a set of data points with a consistent global structure and data point  may be noise, or an outlier, with respect to Cluster I when compared to the underlying structure of data point  and Cluster I. Human perception retrieves structural information of data point  by taking Cluster I as a reference. Therefore, structural perception may be relative and context-based. A data point itself may not be an outlier, but it may be an outlier when its neighboring data points are taken as reference. Thus, the set of contextual data points may be taken into account in order to compute distances compatible with the mechanism of human perception.","At step , a digraph may be created using the calculated contextual distances. The digraph may be made up of many data points that may be directionally linked and weighted. Any given pair of data points may be linked in both directions; however, the weights may be different for each direction such that the digraph may be asymmetric. The edges of the digraph may be weighted based on the contextual distances. A digraph for S may be built. Each point in S may be a vertex of the digraph. A directed edge may be positioned from xto xif xis one of the K nearest neighbors of x. The weight wof the directed edge may be defined as",{"@attributes":{"id":"p-0055","num":"0054"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"w","mrow":{"mi":["i","j"],"mo":"\u2192"}},"mo":"=","mrow":{"mo":"{","mtable":{"mtr":[{"mtd":{"msup":{"mi":"\u2147","mrow":{"mo":"-","mfrac":{"msup":[{"mrow":{"mo":["[","]"],"mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["x","i"]},{"mi":["x","j"]}],"mo":"-"}}}},"mn":"2"},{"mi":"\u03c3","mn":"2"}]}}}}},{"mtd":{"mn":"0"}}]}}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"5"}}]}}}},"br":{},"sub":["j ","i ","1","s"]},{"@attributes":{"id":"p-0056","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"p","mo":"=","mrow":{"mrow":[{"mfrac":{"mn":"1","mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":["p","i"]},{"mi":["\u03c3","p"]}],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":"and"}}},{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mfrac":{"mn":"1","mi":"s"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"s"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["p","i"]},"mo":"-","mover":{"mi":["p","_"]}}},"mn":"2"}}}},"mfrac":{"mn":["1","2"]}},"mo":"."}],"mo":"="}}}},"br":{},"o":"p","sub":"p"},"The direction of the edge from xto xmay arise because the distance between xto xmay be asymmetric. Locally, the point xmay be connected to its nearest neighbors by a K-edge directed star (-distar). Hence, the induced digraph on the data may be composed of m K-distars. Let W \u03b5 Rdenote the weighted adjacency matrix of the weighted digraph, i.e., W(i,j)=w. W may be asymmetric. The structural information of the data may be embodied by the weighted digraph, and data mining reduces to mining the properties of the digraph.  illustrate a digraph and associated weighted adjacency matrix in accordance with implementations of various techniques described herein.  illustrates a simple induced digraph on the data from .  illustrates the asymmetry of the associated weighted adjacency matrix, W. The darker areas correspond to higher weights and the lighter areas correspond to lower weights.","At step , the local digraph Laplacians may be calculated using the digraph. When the data are modeled by a digraph, data processing may be reduced to mining the properties of the digraph. In general, the properties of a digraph may be contained in the structure of the global digraph Laplacian. Therefore, the global digraph Laplacian may be derived by the alignment of local digraph Laplacians defined on local data patches. In one implementation, the local digraph Laplacians may be calculated using the functionals defined on the digraph.","The local digraph Laplacians may be calculated as follows. Let {x, x, . . . , x} be the neighborhood of x, the i-th data patch, and the index set be I={i, i, . . . , i}, where i=i. Suppose that {tilde over (Y)}=[{tilde over (y)}, . . . , {tilde over (y)}] may be a kind of representation yielded by the digraph embedding. The local weighted adjacency matrix Wmay be a sub-matrix of W: W=W(I,I). The local transition probability matrix Pof the random walk on the local digraph may be given by P=DW, where D(u,u)=\u03a3W(u,v) and zeros elsewhere. The corresponding stationary distribution vector \u03c0may be the left eigenvector of Pcorresponding to 1, i.e. \u03c0P=\u03c0and \u2225\u03c0\u2225=1. Also, let",{"@attributes":{"id":"p-0060","num":"0059"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["\u03a6","\u03b9"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","u"],"mo":","}}},"mo":"=","mfrac":{"mrow":[{"msub":{"mi":["\u03c0","\u03b9"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"u"}},{"mo":["(",")"],"mrow":{"msubsup":{"mi":["\u03c0","\u03b9","T"]},"mo":"\u2062","mi":"e"}}]}}}},"br":{}},{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mrow":{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"Y","mo":"~"}}},"mo":"=","mfrac":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03b1","i"]}},{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03b2","i"]}}]}},"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}},"mi":"where"}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"6"}}]},{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["\u03b1","i"]},"mo":"=","mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"u","mo":",","mrow":{"mi":"v","mo":"=","mn":"0"}},"mi":"K"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mover":{"mi":"y","mo":"~"},"msub":{"mi":["i","u"]}},{"mover":{"mi":"y","mo":"~"},"msub":{"mi":["i","v"]}}],"mo":"-"}},"mn":"2"},"mo":["\u2062","\u2062"],"mrow":[{"msub":{"mi":["\u03c0","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"u"}},{"msub":{"mi":["P","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","v"],"mo":","}}}]}}}},"mo":",","mi":"and"}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"7"}}]},{"mtd":[{"mrow":{"msub":{"mi":["\u03b2","i"]},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"v","mo":"=","mn":"0"},"mi":"K"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msup":{"mrow":{"mo":["\uf605","\uf606"],"msub":{"mover":{"mi":"y","mo":"~"},"msub":{"mi":["i","v"]}}},"mn":"2"},"mo":"\u2062","mrow":{"msub":{"mi":["\u03c0","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"u"}}}}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"8"}}]}]}}},"br":{},"sub":["i","i","i","i","i","i","i","i"],"sup":["T","T"]},{"@attributes":{"id":"p-0062","num":"0061"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["L","i"]},"mo":"=","mrow":{"msub":{"mi":["\u03a6","i"]},"mo":"-","mfrac":{"mrow":{"mrow":[{"msub":[{"mi":["\u03a6","i"]},{"mi":["P","i"]}],"mo":"\u2062"},{"msubsup":{"mi":["P","i","T"]},"mo":"\u2062","msub":{"mi":["\u03a6","i"]}}],"mo":"+"},"mn":"2"}}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"9"}}]}}}},"br":{},"sub":["i","i"]},"At step , a global digraph Laplacian may be derived by aligning the local digraph Laplacians. To do so, let {tilde over (Y)}=[{tilde over (y)}, . . . , {tilde over (y)}]. Thus, {tilde over (Y)}={tilde over (Y)}S, where Sis a binary selection matrix. Thus,",{"@attributes":{"id":"p-0064","num":"0063"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mi":"\u03b1","mo":"=","mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03b1","i"]}},{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"tr","mo":["(",")"],"mrow":{"mover":{"mi":"Y","mo":"~"},"mo":["\u2062","\u2062","\u2062","\u2062"],"msub":[{"mi":["S","i"]},{"mi":["L","i"]}],"msubsup":{"mi":["S","i","T"]},"msup":{"mover":{"mi":"Y","mo":"~"},"mi":"T"}}}},{"mi":"tr","mo":["(",")"],"mrow":{"mover":{"mi":"Y","mo":"~"},"mo":["\u2062","\u2062"],"mi":"L","msup":{"mover":{"mi":"Y","mo":"~"},"mi":"T"}}}],"mo":"="}],"mo":"="}},"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}},"mi":"where"}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"10"}}]},{"mtd":[{"mrow":{"mover":{"mi":"L","mo":"~"},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":["S","i"]},{"mi":["L","i"]}],"mo":["\u2062","\u2062"],"msubsup":{"mi":["S","i","T"]}}}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"11"}}]}]}}},"br":{}},{"@attributes":{"id":"p-0065","num":"0064"},"maths":[{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"\u03b2","mo":"=","mrow":{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msub":{"mi":["\u03b2","i"]}},{"mi":"tr","mo":["(",")"],"mrow":{"mover":[{"mi":"Y","mo":"~"},{"mrow":{"mi":["\u00a0","\u03a6"],"mo":"\u2062"},"mo":"~"}],"mo":["\u2062","\u2062"],"msup":{"mover":{"mi":"Y","mo":"~"},"mi":"T"}}}],"mo":"="}},"mo":[",","\u2062"],"mstyle":{"mtext":{}},"mi":"where"}}},{"@attributes":{"id":"MATH-US-00012-2","num":"00012.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mover":{"mi":"\u03a6","mo":"~"},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":["S","i"]},{"mi":["\u03a6","i"]}],"mo":["\u2062","\u2062"],"mrow":{"msubsup":{"mi":["S","i","T"]},"mo":"."}}}}}}],"br":{}},{"@attributes":{"id":"p-0066","num":"0065"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"Y","mo":"~"}}},{"mfrac":[{"mrow":[{"mi":"tr","mo":["(",")"],"mrow":{"mover":[{"mi":"Y","mo":"~"},{"mi":"L","mo":"~"}],"mo":["\u2062","\u2062"],"msup":{"mover":{"mi":"Y","mo":"~"},"mi":"T"}}},{"mi":"tr","mo":["(",")"],"mrow":{"mover":[{"mi":"Y","mo":"~"},{"mi":"\u03a6","mo":"~"}],"mo":["\u2062","\u2062"],"msup":{"mover":{"mi":"Y","mo":"~"},"mi":"T"}}}]},{"mrow":[{"mi":"tr","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["YLY","T"]}}},{"mi":"tr","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["YY","T"]}}}]}],"mo":"="}],"mo":"="}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"12"}}]}}}},"br":{}},{"@attributes":{"id":"p-0067","num":"0066"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"Y","mo":"=","mrow":{"mover":{"mi":"Y","mo":"~"},"mo":"\u2062","msup":{"mi":"\u03a6","mfrac":{"mn":["1","2"]}}}}}},"br":{}},{"@attributes":{"id":"p-0068","num":"0067"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"L","mo":"=","mrow":{"msup":[{"mi":"\u03a6","mrow":{"mo":"-","mfrac":{"mn":["1","2"]}}},{"mi":"\u03a6","mrow":{"mo":"-","mfrac":{"mn":["1","2"]}}}],"mo":["\u2062","\u2062"],"mover":{"mi":"L","mo":"~"}}}}},"br":{}},"The global Laplacian may be defined in a different, perhaps simpler manner. The global transition probability matrix P may be defined as P=DW, where D(u,u)=\u03a3W(u,v) and zeros elsewhere. The stationary distribution of the random walk on the global digraph be \u03c0 may be obtained by \u03c0P=\u03c0. Let",{"@attributes":{"id":"p-0070","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["\u03a6","\u03b9"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","u"],"mo":","}}},"mo":"=","mfrac":{"mrow":[{"msub":{"mi":["\u03c0","\u03b9"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"u"}},{"mo":["(",")"],"mrow":{"msubsup":{"mi":["\u03c0","\u03b9","T"]},"mo":"\u2062","mi":"e"}}]}}}},"br":{}},{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mover":{"mi":"Y","mo":"~"}}},{"mfrac":{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"u","mo":",","mrow":{"mi":"v","mo":"=","mn":"0"}},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"msub":[{"mover":{"mi":"y","mo":"~"},"mi":"u"},{"mover":{"mi":"y","mo":"~"},"mi":"v"}],"mo":"-"}},"mn":"2"},"mo":["\u2062","\u2062"],"mrow":[{"mi":"\u03c0","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"u"}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["u","v"],"mo":","}}}]}}},{"munderover":{"mo":"\u2211","mrow":{"mi":"v","mo":"=","mn":"0"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msup":{"mrow":{"mo":["\uf605","\uf606"],"msub":{"mover":{"mi":"y","mo":"~"},"mi":"v"}},"mn":"2"},"mo":"\u2062","mrow":{"mi":"\u03c0","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"u"}}}}]},"mo":"."}],"mo":"="}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"13"}}]}}}},"br":{}},{"@attributes":{"id":"p-0072","num":"0071"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mrow":{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"Y"}},"mo":"=","mfrac":{"mrow":[{"mi":"tr","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["YLY","T"]}}},{"mi":"tr","mo":"\u2061","mrow":{"mo":["(",")"],"msup":{"mi":["YY","T"]}}}]}},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0073","num":"0072"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"\u0398","mo":"=","mrow":{"mfrac":{"mrow":{"mrow":[{"msup":[{"mi":"\u03a6","mfrac":{"mn":["1","2"]}},{"mi":"\u03a6","mrow":{"mo":"-","mfrac":{"mn":["1","2"]}}}],"mo":["\u2062","\u2062","\u2062"],"mi":"P","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"msup":[{"mi":"\u03a6","mrow":{"mo":"-","mfrac":{"mn":["1","2"]}}},{"mi":["P","T"]},{"mi":"\u03a6","mfrac":{"mn":["1","2"]}}],"mo":["\u2062","\u2062"]}],"mo":"+"},"mn":"2"},"mo":"."}}},{"mrow":{"mi":"Equation","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"mn":"14"}}]}}}}},"At step , an embedding of the global digraph Laplacian may be created by computing the eigenvectors of the global digraph Laplacian. An embedding of the global digraph Laplacian may project the data structures from the data space into the feature space. Ymay correspond to the c eigenvectors of the global Laplacian L associated with the c smallest nonzero eigen-values. Alternatively, the columns of Ymay also be the c non-constant eigenvectors of \u0398 associated with the c largest eigen-values. Note that for digraphs modeled by this method, there may exist nodes that have no in-links. In-links may be depicted by arrows pointing toward the node. For example, the bottom node  of the digraph in  has no in-links, as depicted by the fact that no arrows are pointing to the node. Thus the elements in the corresponding column of the weighted adjacency matrix in  may be all zeros. Such dangling nodes may not be visited by random walkers. This issue may be circumvented by adding a perturbation matrix to the transition probability matrix,",{"@attributes":{"id":"p-0075","num":"0074"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"P","mo":"\u2190","mrow":{"mrow":[{"mi":["\u03b2","P"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","mi":"\u03b2"}},"mo":["\u2062","\u2062"],"mfrac":{"mn":"1","mi":"m"},"msup":{"mi":["ee","T"]}}],"mo":"+"}},"mo":","}}},"br":{}},"At step , the noise may be removed to isolate the data structures. Noise or outlying data points may be naturally mapped near the origin in the perceptual feature space of the embedding such that the noise points may be identified as the data points near the origin. Thus the data may be separated by data structure, or data class, and noise, or outliers.  illustrate sample data consisting of data points forming three data classes, two half-cylinders and noise in accordance with implementations of various techniques described herein.  illustrates sample data.  illustrates that the sample data may be correctly perceived as three classes of data, two separate surfaces forming two half-cylinders depicted with \u201c+\u201d symbols (top half) and \u201c\u2022\u201d symbols (bottom half) and a set of noise points depicted with grey triangles.  illustrate the embeddings of the two half-cylinders and noise data of  in the perceptual feature space in accordance with implementations of various techniques described herein.  illustrates the 3-D representation of the data. The data points corresponding to the two half-cylinders are separated and depicted with \u201c+\u201d symbols (lower left) and \u201c\u2022\u201d symbols (upper right). The data points corresponding to the noise are at the origin  and depicted with grey triangles.  illustrates a zoom-in view of the origin. The noise data points at the origin  may be removed to isolate the data structures, the data points corresponding to the two half-cylinders.","At step , the isolated data structures may be used in various applications such as clustering and ranking. In one implementation, a perceptual clustering algorithm may consist of modeling the digraph of the data and forming \u0398 in Equation 14. Then computing the c eigenvectors {y, . . . , y} of \u0398 corresponding to the first c largest eigen-values except the largest one. These eigenvectors form a matrix Y=[y, . . . , y]. The row vectors Y may be the mapped feature points of the data. The clustering may now be performed on the feature points. In one implementation, the clustering may be performed on feature points in the embedding spaces.  illustrate the results of perceptual clustering on the two half-cylinders data in accordance with implementations of various techniques described herein.  illustrate the results of perceptual clustering using the coding length contextual set descriptor on the two half-cylinders data with increasing noise data points, , ,  and  noise data points, respectively.  illustrates the results of perceptual clustering using the centroid contextual set descriptor on the two half-cylinders data. The perceptual clustering accurately detects the data structures regardless of the noise. The results of  may be compared to the results using conventional methods depicted in  described below.",{"@attributes":{"id":"p-0078","num":"0077"},"figref":["FIGS. 9A-C","FIG. 9A","FIG. 9B","FIG. 9C","FIGS. 9A-C","FIGS. 10A-B","FIG. 6A","FIG. 10A","FIG. 10B","FIGS. 10A-B"]},"In another perceptual clustering example, samples of handwritten digits 1, 2, and 3 in the test set of the MNIST handwritten digit database may be used. There are 1135, 1032, and 1010 samples, respectively. The representations of samples may be directly visualized in the associated feature spaces instead of a quantified comparison as different clustering methods should be chosen for different distributions of mapped points. It may be more intuitive to compare the distinctive characteristics of the involved algorithms by visual perception.  illustrate the embeddings of the handwritten digits clustering. Grey hearts represent the digit \u201c1\u201d, white squares \u201c2\u201d and black diamonds \u201c3\u201d.  illustrates clustering using the NJW clustering technique. The three data classes (digits 1, 2 and 3) are not clearly separated.  illustrates perceptual clustering using a coding length contextual set descriptor in accordance with implementations of various techniques described herein.  illustrates perceptual clustering using a centroid contextual set descriptor in accordance with implementations of various techniques described herein. As shown in , the perceptual clustering algorithms yield more compact and clearer representations of clusters than conventional methods.","In perceptual clustering, different clusters may be mapped approximately into different linear subspaces. The clusters may be easily identified by various methods. For each identified cluster in the perceptual feature space, the portion of the data structure mapped farthest from the origin may contain the least noise and the portion of the data structure mapped nearest from the origin contain more noise.  illustrate the data spaces corresponding to the feature spaces in the embeddings in , respectively. The first ten columns of digits correspond to the data points farthest from the origin in . The last ten columns of digits correspond to the data points closest to the origin in . As expected, the digits containing more noise are in the last ten columns of digits. However, the perceptual clustering algorithm was able to correctly cluster the digits regardless of the noise.","In another implementation, a perceptual ranking algorithm may consist of modeling the digraph of the data and forming \u0398 in Equation 14. Then, given a vector \u03bd whose i-th elements is 1 if it corresponds to a labeled point and zeros elsewhere, the score vectors s=(I\u2212\u03b1\u0398)\u03bd, where \u03b1 is a free parameter in [0,1], may be computed. The scores, s, may be sorted in descending order. The sample data points with large scores may be considered to be in the same class as the labeled point.  illustrate the results of perceptual ranking on the two half-cylinders data in accordance with implementations of various techniques described herein. One data point may be randomly selected and labeled on one of the half-cylinders for each trial. The selected data point in each  may be depicted by the larger circular dot . Once the data point  is selected, the remaining data points may be ranked according to similarity to the selected data point . Then the largest ranking scores may be identified as a single data class. In , the data class may be depicted with white squares and be the upper half-cylinder, the half-cylinder containing the selected data point. In this example, K=10 and \u03b1=0.999.  illustrate the results of perceptual ranking using the coding length contextual set descriptor on the two half-cylinders data with increasing noise data points, , ,  and  noise data points respectively.  illustrates the results of perceptual ranking using the centroid contextual set descriptor on the two half-cylinders data. The perceptual ranking algorithm accurately labels the data points on the labeled surface. The results may be robust against noise.","In another perceptual ranking example, a database of real photos of a family and its friends may be used. The faces in photos may be automatically detected, cropped and aligned according to the positions of eyes. For example, 980 faces of 26 persons may be used. The algorithm of local binary pattern may be applied to extract the expressive features and then exploit dual-space LDA to extract the discriminating features from the LBP features. Then conventional ranking and perceptual ranking, in accordance with implementations of various techniques described herein, may be performed. The ratio of the number of correctly ranked faces to the total number of faces in the first 50 ranked faces may be considered as the accuracy measure. Specifically, let Z denote the ranked faces and z the correctly ranked ones. Then, the accuracy may be defined as z\/Z. For each face, the ranking experiment may be performed for two hundred trials.  illustrate the accuracy and stability of perceptual ranking in accordance with implementations of various techniques described herein.  illustrates the mean accuracy results for five perceptually ranked faces. Each graph depicts the results for a single face where the line with circles represents the results of perceptual ranking using the coding length contextual set descriptor, the line with squares represents the results of perceptual ranking using the centroid contextual set descriptor and the line with x's represents conventional ranking. Perceptual ranking may consistently have higher accuracy.  illustrate that perceptual ranking may be stable with the variations of \u03b1 and K, respectively.","Although the above examples describe the analyzed data as being image data, it should be understood that any type of data maybe analyzed using the method  for modeling data affinities and data structures using local contexts in accordance with implementations of various techniques described herein.","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts described above are disclosed as example forms of implementing the claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIGS. 1A-C"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIGS. 4A-F"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIGS. 5A-B"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIGS. 6A-B"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIGS. 7A-B","FIG. 6A"]},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIGS. 8A-E"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIGS. 9A-C"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIGS. 10A-B","FIG. 6A"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIGS. 11A-C"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":["FIGS. 12A and 121B","FIGS. 11B and 11C"]},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIGS. 13A-E"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIGS. 14A-C"}]},"DETDESC":[{},{}]}
