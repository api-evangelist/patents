---
title: Apparatus and method for implementing a multi-level memory hierarchy
abstract: A system and method are described for integrating a memory and storage hierarchy including a non-volatile memory tier within a computer system. In one embodiment, PCMS memory devices are used as one tier in the hierarchy, sometimes referred to as “far memory.” Higher performance memory devices such as DRAM placed in front of the far memory and are used to mask some of the performance limitations of the far memory. These higher performance memory devices are referred to as “near memory.”
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09600416&OS=09600416&RS=09600416
owner: Intel Corporation
number: 09600416
owner_city: Santa Clara
owner_country: US
publication_date: 20110930
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATION","BACKGROUND","DETAILED DESCRIPTION","INTRODUCTION","An Exemplary System Memory Allocation Scheme","Exemplary PCM Bus and Packaging Configurations","Embodiments of a MSC Architecture","Embodiments Having Different Near Memory Modes of Operation"],"p":["This patent application is a U.S. National Phase Application under 35 U.S.C. \u00a7371 of International Application No. PCT\/US2011\/054430, filed Sep. 30, 2011, entitled APPARATUS AND METHOD FOR IMPLEMENTING A MULTI-LEVEL MEMORY HIERARCHY.","Field of the Invention","This invention relates generally to the field of computer systems. More particularly, the invention relates to an apparatus and method for implementing a multi-level memory hierarchy.","Description of the Related Art","A. Current Memory and Storage Configurations","One of the limiting factors for computer innovation today is memory and storage technology. In conventional computer systems, system memory (also known as main memory, primary memory, executable memory) is typically implemented by dynamic random access memory (DRAM). DRAM-based memory consumes power even when no memory reads or writes occur because it must constantly recharge internal capacitors. DRAM-based memory is volatile, which means data stored in DRAM memory is lost once the power is removed. Conventional computer systems also rely on multiple levels of caching to improve performance. A cache is a high speed memory positioned between the processor and system memory to service memory access requests faster than they could be serviced from system memory. Such caches are typically implemented with static random access memory (SRAM). Cache management protocols may be used to ensure that the most frequently accessed data and instructions are stored within one of the levels of cache, thereby reducing the number of memory access transactions and improving performance.","With respect to mass storage (also known as secondary storage or disk storage), conventional mass storage devices typically include magnetic media (e.g., hard disk drives), optical media (e.g., compact disc (CD) drive, digital versatile disc (DVD), etc.), holographic media, and\/or mass-storage flash memory (e.g., solid state drives (SSDs), removable flash drives, etc.). Generally, these storage devices are considered Input\/Output (I\/O) devices because they are accessed by the processor through various I\/O adapters that implement various I\/O protocols. These I\/O adapters and I\/O protocols consume a significant amount of power and can have a significant impact on the die area and the form factor of the platform. Portable or mobile devices (e.g., laptops, netbooks, tablet computers, personal digital assistant (PDAs), portable media players, portable gaming devices, digital cameras, mobile phones, smartphones, feature phones, etc.) that have limited battery life when not connected to a permanent power supply may include removable mass storage devices (e.g., Embedded Multimedia Card (eMMC), Secure Digital (SD) card) that are typically coupled to the processor via low-power interconnects and I\/O controllers in order to meet active and idle power budgets.","With respect to firmware memory (such as boot memory (also known as BIOS flash)), a conventional computer system typically uses flash memory devices to store persistent system information that is read often but seldom (or never) written to. For example, the initial instructions executed by a processor to initialize key system components during a boot process (Basic Input and Output System (BIOS) images) are typically stored in a flash memory device. Flash memory devices that are currently available in the market generally have limited speed (e.g., 50 MHz). This speed is further reduced by the overhead for read protocols (e.g., 2.5 MHz). In order to speed up the BIOS execution speed, conventional processors generally cache a portion of BIOS code during the Pre-Extensible Firmware Interface (PEI) phase of the boot process. The size of the processor cache places a restriction on the size of the BIOS code used in the PEI phase (also known as the \u201cPEI BIOS code\u201d).","B. Phase-Change Memory (PCM) and Related Technologies","Phase-change memory (PCM), also sometimes referred to as phase change random access memory (PRAM or PCRAM), PCME, Ovonic Unified Memory, or Chalcogenide RAM (C-RAM), is a type of non-volatile computer memory which exploits the unique behavior of chalcogenide glass. As a result of heat produced by the passage of an electric current, chalcogenide glass can be switched between two states: crystalline and amorphous. Recent versions of PCM can achieve two additional distinct states.","PCM provides higher performance than flash because the memory element of PCM can be switched more quickly, writing (changing individual bits to either 1 or 0) can be done without the need to first erase an entire block of cells, and degradation from writes is slower (a PCM device may survive approximately 100 million write cycles; PCM degradation is due to thermal expansion during programming, metal (and other material) migration, and other mechanisms).","In the following description, numerous specific details such as logic implementations, opcodes, means to specify operands, resource partitioning\/sharing\/duplication implementations, types and interrelationships of system components, and logic partitioning\/integration choices are set forth in order to provide a more thorough understanding of the present invention. It will be appreciated, however, by one skilled in the art that the invention may be practiced without such specific details. In other instances, control structures, gate level circuits and full software instruction sequences have not been shown in detail in order not to obscure the invention. Those of ordinary skill in the art, with the included descriptions, will be able to implement appropriate functionality without undue experimentation.","References in the specification to \u201cone embodiment,\u201d \u201can embodiment,\u201d \u201can example embodiment,\u201d etc., indicate that the embodiment described may include a particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it is submitted that it is within the knowledge of one skilled in the art to effect such feature, structure, or characteristic in connection with other embodiments whether or not explicitly described.","In the following description and claims, the terms \u201ccoupled\u201d and \u201cconnected,\u201d along with their derivatives, may be used. It should be understood that these terms are not intended as synonyms for each other. \u201cCoupled\u201d is used to indicate that two or more elements, which may or may not be in direct physical or electrical contact with each other, co-operate or interact with each other. \u201cConnected\u201d is used to indicate the establishment of communication between two or more elements that are coupled with each other.","Bracketed text and blocks with dashed borders (e.g., large dashes, small dashes, dot-dash, dots) are sometimes used herein to illustrate optional operations\/components that add additional features to embodiments of the invention. However, such notation should not be taken to mean that these are the only options or optional operations\/components, and\/or that blocks with solid borders are not optional in certain embodiments of the invention.","Memory capacity and performance requirements continue to increase with an increasing number of processor cores and new usage models such as virtualization. In addition, memory power and cost have become a significant component of the overall power and cost, respectively, of electronic systems.","Some embodiments of the invention solve the above challenges by intelligently subdividing the performance requirement and the capacity requirement between memory technologies. The focus of this approach is on providing performance with a relatively small amount of a relatively higher-speed memory such as DRAM while implementing the bulk of the system memory using significantly cheaper and denser non-volatile random access memory (NVRAM). Embodiments of the invention described below define platform configurations that enable hierarchical memory subsystem organizations for the use of NVRAM. The use of NVRAM in the memory hierarchy also enables new usages such as expanded boot space and mass storage implementations, as described in detail below.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 1","FIG. 1"],"b":["120","121","106","107","109","122"]},"A. Non-Volatile Random Access Memory (\u201cNVRAM\u201d)","There are many possible technology choices for NVRAM, including PCM, Phase Change Memory and Switch (PCMS) (the latter being a more specific implementation of the former), byte-addressable persistent memory (BPRAM), storage class memory (SCM), universal memory, Ge2Sb2Te5, programmable metallization cell (PMC), resistive memory (RRAM), RESET (amorphous) cell, SET (crystalline) cell, PCME, Ovshinsky memory, ferroelectric memory (also known as polymer memory and poly(N-vinylcarbazole)), ferromagnetic memory (also known as Spintronics, SPRAM (spin-transfer torque RAM), STRAM (spin tunneling RAM), magnetoresistive memory, magnetic memory, magnetic random access memory (MRAM)), and Semiconductor-oxide-nitride-oxide-semiconductor (SONOS, also known as dielectric memory).","NVRAM has the following characteristics:","(1) It maintains its content even if power is removed, similar to FLASH memory used in solid state disks (SSD), and different from SRAM and DRAM which are volatile;","(2) lower power consumption than volatile memories such as SRAM and DRAM;","(3) random access similar to SRAM and DRAM (also known as randomly addressable);","(4) rewritable and erasable at a lower level of granularity (e.g., byte level) than FLASH found in SSDs (which can only be rewritten and erased a \u201cblock\u201d at a time\u2014minimally 64 Kbyte in size for NOR FLASH and 16 Kbyte for NAND FLASH);","(5) used as a system memory and allocated all or a portion of the system memory address space;","(6) capable of being coupled to the processor over a bus using a transactional protocol (a protocol that supports transaction identifiers (IDs) to distinguish different transactions so that those transactions can complete out-of-order) and allowing access at a level of granularity small enough to support operation of the NVRAM as system memory (e.g., cache line size such as 64 or 128 byte). For example, the bus may be a memory bus (e.g., a DDR bus such as DDR3, DDR4, etc.) over which is run a transactional protocol as opposed to the non-transactional protocol that is normally used. As another example, the bus may one over which is normally run a transactional protocol (a native transactional protocol), such as a PCI express (PCIE) bus, desktop management interface (DMI) bus, or any other type of bus utilizing a transactional protocol and a small enough transaction payload size (e.g., cache line size such as 64 or 128 byte); and","(6) one or more of the following:","(a) faster write speed than non-volatile memory\/storage technologies such as FLASH;","b) very high read speed (faster than FLASH and near or equivalent to DRAM read speeds);","c) directly writable (rather than requiring erasing (overwriting with 1 s) before writing data like FLASH memory used in SSDs);","d) a greater number of writes before failure (more than boot ROM and FLASH used in SSDs); and\/or","As mentioned above, in contrast to FLASH memory, which must be rewritten and erased a complete \u201cblock\u201d at a time, the level of granularity at which NVRAM is accessed in any given implementation may depend on the particular memory controller and the particular memory bus or other type of bus to which the NVRAM is coupled. For example, in some implementations where NVRAM is used as system memory, the NVRAM may be accessed at the granularity of a cache line (e.g., a 64-byte or 128-Byte cache line), notwithstanding an inherent ability to be accessed at the granularity of a byte, because cache line is the level at which the memory subsystem accesses memory. Thus, when NVRAM is deployed within a memory subsystem, it may be accessed at the same level of granularity as the DRAM (e.g., the \u201cnear memory\u201d) used in the same memory subsystem. Even so, the level of granularity of access to the NVRAM by the memory controller and memory bus or other type of bus is smaller than that of the block size used by Flash and the access size of the I\/O subsystem's controller and bus.","NVRAM may also incorporate wear leveling algorithms to account for the fact that the storage cells at the far memory level begin to wear out after a number of write accesses, especially where a significant number of writes may occur such as in a system memory implementation. Since high cycle count blocks are most likely to wear out in this manner, wear leveling spreads writes across the far memory cells by swapping addresses of high cycle count blocks with low cycle count blocks. Note that most address swapping is typically transparent to application programs_because it is handled by hardware, lower-level software (e.g., a low level driver or operating system), or a combination of the two.","B. Far Memory","The far memory  of some embodiments of the invention is implemented with NVRAM, but is not necessarily limited to any particular memory technology. Far memory  is distinguishable from other instruction and data memory\/storage technologies in terms of its characteristics and\/or its application in the memory\/storage hierarchy. For example, far memory  is different from:","(2) static random access memory (SRAM) which may be used for level 0 and level 1 internal processor caches -, -, -, -, and -dedicated to each of the processor cores -, respectively, and lower level cache (LLC)  shared by the processor cores;","(3) dynamic random access memory (DRAM) configured as a cache  internal to the processor  (e.g., on the same die as the processor ) and\/or configured as one or more caches - external to the processor (e.g., in the same or a different package from the processor ); and","(4) FLASH memory\/magnetic disk\/optical disc applied as mass storage (not shown); and","(5) memory such as FLASH memory or other read only memory (ROM) applied as firmware memory (which can refer to boot ROM, BIOS Flash, and\/or TPM Flash).(not shown).","Far memory  may be used as instruction and data storage that is directly addressable by a processor  and is able to sufficiently keep pace with the processor  in contrast to FLASH\/magnetic disk\/optical disc applied as mass storage. Moreover, as discussed above and described in detail below, far memory  may be placed on a memory bus and may communicate directly with a memory controller that, in turn, communicates directly with the processor .","Far memory  may be combined with other instruction and data storage technologies (e.g., DRAM) to form hybrid memories (also known as Co-locating PCM and DRAM; first level memory and second level memory; FLAM (FLASH and DRAM)). Note that at least some of the above technologies, including PCM\/PCMS may be used for mass storage instead of, or in addition to, system memory, and need not be random accessible, byte addressable or directly addressable by the processor when applied in this manner.","For convenience of explanation, most of the remainder of the application will refer to \u201cNVRAM\u201d or, more specifically, \u201cPCM,\u201d or \u201cPCMS\u201d as the technology selection for the far memory . As such, the terms NVRAM, PCM, PCMS, and far memory may be used interchangeably in the following discussion. However it should be realized, as discussed above, that different technologies may also be utilized for far memory. Also, that NVRAM is not limited for use as far memory.","C. Near Memory","\u201cNear memory\u201d  is an intermediate level of memory configured in front of a far memory  that has lower read\/write access latency relative to far memory and\/or more symmetric read\/write access latency (i.e., having read times which are roughly equivalent to write times). In some embodiments, the near memory  has significantly lower write latency than the far memory  but similar (e.g., slightly lower or equal) read latency; for instance the near memory  may be a volatile memory such as volatile random access memory (VRAM) and may comprise a DRAM or other high speed capacitor-based memory. Note, however, that the underlying principles of the invention are not limited to these specific memory types. Additionally, the near memory  may have a relatively lower density and\/or may be more expensive to manufacture than the far memory .","In one embodiment, near memory  is configured between the far memory  and the internal processor caches . In some of the embodiments described below, near memory  is configured as one or more memory-side caches (MSCs) - to mask the performance and\/or usage limitations of the far memory including, for example, read\/write latency limitations and memory degradation limitations. In these implementations, the combination of the MSC - and far memory  operates at a performance level which approximates, is equivalent or exceeds a system which uses only DRAM as system memory. As discussed in detail below, although shown as a \u201ccache\u201d in , the near memory  may include modes in which it performs other roles, either in addition to, or in lieu of, performing the role of a cache.","Near memory  can be located on the processor die (as cache(s) ) and\/or located external to the processor die (as caches -) (e.g., on a separate die located on the CPU package, located outside the CPU package with a high bandwidth link to the CPU package, for example, on a memory dual in-line memory module (DIMM), a riser\/mezzanine, or a computer motherboard). The near memory  may be coupled in communicate with the processor  using a single or multiple high bandwidth links, such as DDR or other transactional high bandwidth links (as described in detail below).",{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 1","b":["101","109","116","119","100","101","104","101","104","101","104","100","105"],"i":["a","a ","b","b"]},"The caches - illustrated in  may be dedicated to a particular system memory address range or a set of non-contiguous address ranges. For example, cache  is dedicated to acting as an MSC for system memory address range #1  and caches  and  are dedicated to acting as MSCs for non-overlapping portions of system memory address ranges #2  and #3 . The latter implementation may be used for systems in which the SPA space used by the processor  is interleaved into an address space used by the caches - (e.g., when configured as MSCs). In some embodiments, this latter address space is referred to as a memory channel address (MCA) space. In one embodiment, the internal caches - perform caching operations for the entire SPA space.","System memory as used herein is memory which is visible to and\/or directly addressable by software executed on the processor ; while the cache memories - may operate transparently to the software in the sense that they do not form a directly-addressable portion of the system address space, but the cores may also support execution of instructions to allow software to provide some control (configuration, policies, hints, etc.) to some or all of the cache(s). The subdivision of system memory into regions - may be performed manually as part of a system configuration process (e.g., by a system designer) and\/or may be performed automatically by software.","In one embodiment, the system memory regions - are implemented using far memory (e.g., PCM) and, in some embodiments, near memory configured as system memory. System memory address range #4 represents an address range which is implemented using a higher speed memory such as DRAM which may be a near memory configured in a system memory mode (as opposed to a caching mode).",{"@attributes":{"id":"p-0079","num":"0078"},"figref":["FIG. 2","FIG. 1"],"b":["140","144","140","150","150","101","105","150","151","151","174","151","152","152","152","142","153","170","172","173"]},"As indicated, near memory  may be implemented to operate in a variety of different modes including: a first mode in which it operates as a cache for far memory (near memory as cache for FM B); a second mode in which it operates as system memory A and occupies a portion of the SPA space (sometimes referred to as near memory \u201cdirect access\u201d mode); and one or more additional modes of operation such as a scratchpad memory  or as a write buffer . In some embodiments of the invention, the near memory is partitionable, where each partition may concurrently operate in a different one of the supported modes; and different embodiments may support configuration of the partitions (e.g., sizes, modes) by hardware (e.g., fuses, pins), firmware, and\/or software (e.g., through a set of programmable range registers within the MSC controller  within which, for example, may be stored different binary codes to identify each mode and partition).","System address space A  in  is used to illustrate operation when near memory is configured as a MSC for far memory B. In this configuration, system address space A  represents the entire system address space (and system address space B  does not exist). Alternatively, system address space B  is used to show an implementation when all or a portion of near memory is assigned a portion of the system address space. In this embodiment, system address space B  represents the range of the system address space assigned to the near memory A and system address space A  represents the range of the system address space assigned to NVRAM .","In addition, when acting as a cache for far memory B, the near memory  may operate in various sub-modes under the control of the MSC controller . In each of these modes, the near memory address space (NMA) is transparent to software in the sense that the near memory does not form a directly-addressable portion of the system address space. These modes include but are not limited to the following:","(1) Write-Back Caching Mode:","In this mode, all or portions of the near memory acting as a FM cache B is used as a cache for the NVRAM far memory (FM) B. While in write-back mode, every write operation is directed initially to the near memory as cache for FM B (assuming that the cache line to which the write is directed is present in the cache). A corresponding write operation is performed to update the NVRAM FM B only when the cache line within the near memory as cache for FM B is to be replaced by another cache line (in contrast to write-through mode described below in which each write operation is immediately propagated to the NVRAM FM B).","(2) Near Memory Bypass Mode:","In this mode all reads and writes bypass the NM acting as a FM cache B and go directly to the NVRAM FM B. Such a mode may be used, for example, when an application is not cache friendly or requires data to be committed to persistence at the granularity of a cache line. In one embodiment, the caching performed by the processor caches A and the NM acting as a FM cache B operate independently of one another. Consequently, data may be cached in the NM acting as a FM cache B which is not cached in the processor caches A (and which, in some cases, may not be permitted to be cached in the processor caches A) and vice versa. Thus, certain data which may be designated as \u201cuncacheable\u201d in the processor caches may be cached within the NM acting as a FM cache B.","(3) Near Memory Read-Cache Write Bypass Mode:","This is a variation of the above mode where read caching of the persistent data from NVRAM FM B is allowed (i.e., the persistent data is cached in the near memory as cache for far memory B for read-only operations). This is useful when most of the persistent data is \u201cRead-Only\u201d and the application usage is cache-friendly.","(4) Near Memory Read-Cache Write-Through Mode:","This is a variation of the near memory read-cache write bypass mode, where in addition to read caching, write-hits are also cached. Every write to the near memory as cache for FM B causes a write to the FM B. Thus, due to the write-through nature of the cache, cache-line persistence is still guaranteed.","When acting in near memory direct access mode, all or portions of the near memory as system memory A are directly visible to software and form part of the SPA space. Such memory may be completely under software control. Such a scheme may create a non-uniform memory address (NUMA) memory domain for software where it gets higher performance from near memory  relative to NVRAM system memory . By way of example, and not limitation, such a usage may be employed for certain high performance computing (HPC) and graphics applications which require very fast access to certain data structures.","In an alternate embodiment, the near memory direct access mode is implemented by \u201cpinning\u201d certain cache lines in near memory (i.e., cache lines which have data that is also concurrently stored in NVRAM ). Such pinning may be done effectively in larger, multi-way, set-associative caches.",{"@attributes":{"id":"p-0093","num":"0092"},"figref":"FIG. 2","b":["142","172","170","172","101","104","170","115","173"]},"Thus, as indicated, the NVRAM  may be implemented to operate in a variety of different modes, including as far memory B (e.g., when near memory  is present\/operating, whether the near memory is acting as a cache for the FM via a MSC control  or not (accessed directly after cache(s) A- and without MSC control )); just NVRAM system memory  (not as far memory because there is no near memory present\/operating; and accessed without MSC control ); NVRAM mass storage A; BIOS NVRAM ; and TPM NVRAM . While different embodiments may specify the NVRAM modes in different ways,  describes the use of a decode table .",{"@attributes":{"id":"p-0095","num":"0094"},"figref":["FIG. 3","FIG. 3"],"b":["300","300","310","380","142","142","300","142","142","151","152","173","173","142"]},"By way of example, operation while the near memory as cache for FM B is in the write-back caching is described. In one embodiment, while the near memory as cache for FM B is in the write-back caching mode mentioned above, a read operation will first arrive at the MSC controller  which will perform a look-up to determine if the requested data is present in the near memory acting as a cache for FM B (e.g., utilizing a tag cache ). If present, it will return the data to the requesting CPU, core - or I\/O device through I\/O subsystem . If the data is not present, the MSC controller  will send the request along with the system memory address to an NVRAM controller . The NVRAM controller  will use the decode table  to translate the system memory address to an NVRAM physical device address (PDA) and direct the read operation to this region of the far memory B. In one embodiment, the decode table  includes an address indirection table (AIT) component which the NVRAM controller  uses to translate between system memory addresses and NVRAM PDAs. In one embodiment, the AIT is updated as part of the wear leveling algorithm implemented to distribute memory access operations and thereby reduce wear on the NVRAM FM B. Alternatively, the AIT may be a separate table stored within the NVRAM controller .","Upon receiving the requested data from the NVRAM FM B, the NVRAM controller  will return the requested data to the MSC controller  which will store the data in the MSC near memory acting as an FM cache B and also send the data to the requesting processor core -, or I\/O Device through I\/O subsystem . Subsequent requests for this data may be serviced directly from the near memory acting as a FM cache B until it is replaced by some other NVRAM FM data.","As mentioned, in one embodiment, a memory write operation also first goes to the MSC controller  which writes it into the MSC near memory acting as a FM cache B. In write-back caching mode, the data may not be sent directly to the NVRAM FM B when a write operation is received. For example, the data may be sent to the NVRAM FM B only when the location in the MSC near memory acting as a FM cache B in which the data is stored must be re-used for storing data for a different system memory address. When this happens, the MSC controller  notices that the data is not current in NVRAM FM B and will thus retrieve it from near memory acting as a FM cache B and send it to the NVRAM controller . The NVRAM controller  looks up the PDA for the system memory address and then writes the data to the NVRAM FM B.","In , the NVRAM controller  is shown connected to the FM B, NVRAM mass storage A, and BIOS NVRAM  using three separate lines. This does not necessarily mean, however, that there are three separate physical buses or communication channels connecting the NVRAM controller  to these portions of the NVRAM . Rather, in some embodiments, a common memory bus or other type of bus (such as those described below with respect to ) is used to communicatively couple the NVRAM controller  to the FM B, NVRAM mass storage A, and BIOS NVRAM . For example, in one embodiment, the three lines in  represent a bus, such as a memory bus (e.g., a DDR3, DDR4, etc, bus), over which the NVRAM controller  implements a transactional protocol to communicate with the NVRAM . The NVRAM controller  may also communicate with the NVRAM  over a bus supporting a native transactional protocol such as a PCI express bus, desktop management interface (DMI) bus, or any other type of bus utilizing a transactional protocol and a small enough transaction payload size (e.g., cache line size such as 64 or 128 byte).","In one embodiment, computer system  includes integrated memory controller (IMC)  which performs the central memory access control for processor , which is coupled to: 1) a memory-side cache (MSC) controller  to control access to near memory (NM) acting as a far memory cache B; and 2) a NVRAM controller  to control access to NVRAM . Although illustrated as separate units in , the MSC controller  and NVRAM controller  may logically form part of the IMC .","In the illustrated embodiment, the MSC controller  includes a set of range registers  which specify the mode of operation in use for the NM acting as a far memory cache B (e.g., write-back caching mode, near memory bypass mode, etc, described above). In the illustrated embodiment, DRAM  is used as the memory technology for the NM acting as cache for far memory B. In response to a memory access request, the MSC controller  may determine (depending on the mode of operation specified in the range registers ) whether the request can be serviced from the NM acting as cache for FM B or whether the request must be sent to the NVRAM controller , which may then service the request from the far memory (FM) portion B of the NVRAM .","In an embodiment where NVRAM  is implemented with PCMS, NVRAM controller  is a PCMS controller that performs access with protocols consistent with the PCMS technology. As previously discussed, the PCMS memory is inherently capable of being accessed at the granularity of a byte. Nonetheless, the NVRAM controller  may access a PCMS-based far memory B at a lower level of granularity such as a cache line (e.g., a 64-bit or 128-bit cache line) or any other level of granularity consistent with the memory subsystem. The underlying principles of the invention are not limited to any particular level of granularity for accessing a PCMS-based far memory B. In general, however, when PCMS-based far memory B is used to form part of the system address space, the level of granularity will be higher than that traditionally used for other non-volatile storage technologies such as FLASH, which can only perform rewrite and erase operations at the level of a \u201cblock\u201d (minimally 64 Kbyte in size for NOR FLASH and 16 Kbyte for NAND FLASH).","In the illustrated embodiment, NVRAM controller  can read configuration data to establish the previously described modes, sizes, etc. for the NVRAM  from decode table , or alternatively, can rely on the decoding results passed from IMC  and I\/O subsystem . For example, at either manufacturing time or in the field, computer system  can program decode table  to mark different regions of NVRAM  as system memory, mass storage exposed via SATA interfaces, mass storage exposed via USB Bulk Only Transport (BOT) interfaces, encrypted storage that supports TPM storage, among others. The means by which access is steered to different partitions of NVRAM device  is via a decode logic. For example, in one embodiment, the address range of each partition is defined in the decode table . In one embodiment, when IMC  receives an access request, the target address of the request is decoded to reveal whether the request is directed toward memory, NVRAM mass storage, or I\/O. If it is a memory request, IMC  and\/or the MSC controller  further determines from the target address whether the request is directed to NM as cache for FM B or to FM B. For FM B access, the request is forwarded to NVRAM controller . IMC  passes the request to the I\/O subsystem  if this request is directed to I\/O (e.g., non-storage and storage I\/O devices). I\/O subsystem  further decodes the address to determine whether the address points to NVRAM mass storage A, BIOS NVRAM , or other non-storage or storage I\/O devices. If this address points to NVRAM mass storage A or BIOS NVRAM , I\/O subsystem  forwards the request to NVRAM controller . If this address points to TMP NVRAM , I\/O subsystem  passes the request to TPM  to perform secured access.","In one embodiment, each request forwarded to NVRAM controller  is accompanied with an attribute (also known as a \u201ctransaction type\u201d) to indicate the type of access. In one embodiment, NVRAM controller  may emulate the access protocol for the requested access type, such that the rest of the platform remains unaware of the multiple roles performed by NVRAM  in the memory and storage hierarchy. In alternative embodiments, NVRAM controller  may perform memory access to NVRAM  regardless of which transaction type it is. It is understood that the decode path can be different from what is described above. For example, IMC  may decode the target address of an access request and determine whether it is directed to NVRAM . If it is directed to NVRAM , IMC  generates an attribute according to decode table . Based on the attribute, IMC  then forwards the request to appropriate downstream logic (e.g., NVRAM controller  and I\/O subsystem ) to perform the requested data access. In yet another embodiment, NVRAM controller  may decode the target address if the corresponding attribute is not passed on from the upstream logic (e.g., IMC  and I\/O subsystem ). Other decode paths may also be implemented.","The presence of a new memory architecture such as described herein provides for a wealth of new possibilities. Although discussed at much greater length further below, some of these possibilities are quickly highlighted immediately below.","According to one possible implementation, NVRAM  acts as a total replacement or supplement for traditional DRAM technology in system memory. In one embodiment, NVRAM  represents the introduction of a second-level system memory (e.g., the system memory may be viewed as having a first level system memory comprising near memory as cache B (part of the DRAM device ) and a second level system memory comprising far memory (FM) B (part of the NVRAM ).","According to some embodiments, NVRAM  acts as a total replacement or supplement for the flash\/magnetic\/optical mass storage B. As previously described, in some embodiments, even though the NVRAM A is capable of byte-level addressability, NVRAM controller  may still access NVRAM mass storage A in blocks of multiple bytes, depending on the implementation (e.g., 64 Kbytes, 128 Kbytes, etc.). The specific manner in which data is accessed from NVRAM mass storage A by NVRAM controller  may be transparent to software executed by the processor . For example, even through NVRAM mass storage A may be accessed differently from Flash\/magnetic\/optical mass storage A, the operating system may still view NVRAM mass storage A as a standard mass storage device (e.g., a serial ATA hard drive or other standard form of mass storage device).","In an embodiment where NVRAM mass storage A acts as a total replacement for the flash\/magnetic\/optical mass storage B, it is not necessary to use storage drivers for block-addressable storage access. The removal of storage driver overhead from storage access can increase access speed and save power. In alternative embodiments where it is desired that NVRAM mass storage A appears to the OS and\/or applications as block-accessible and indistinguishable from flash\/magnetic\/optical mass storage B, emulated storage drivers can be used to expose block-accessible interfaces (e.g., Universal Serial Bus (USB) Bulk-Only Transfer (BOT), 1.0; Serial Advanced Technology Attachment (SATA), 3.0; and the like) to the software for accessing NVRAM mass storage A.","In one embodiment, NVRAM  acts as a total replacement or supplement for firmware memory such as BIOS flash  and TPM flash  (illustrated with dotted lines in  to indicate that they are optional). For example, the NVRAM  may include a BIOS NVRAM  portion to supplement or replace the BIOS flash  and may include a TPM NVRAM  portion to supplement or replace the TPM flash . Firmware memory can also store system persistent states used by a TPM  to protect sensitive system information (e.g., encryption keys). In one embodiment, the use of NVRAM  for firmware memory removes the need for third party flash parts to store code and data that are critical to the system operations.","Continuing then with a discussion of the system of , in some embodiments, the architecture of computer system  may include multiple processors, although a single processor  is illustrated in FIG.  for simplicity. Processor  may be any type of data processor including a general purpose or special purpose central processing unit (CPU), an application-specific integrated circuit (ASIC) or a digital signal processor (DSP). For example, processor  may be a general-purpose processor, such as a Core\u2122 i3, i5, i7, 2 Duo and Quad, Xeon\u2122, or Itanium\u2122 processor, all of which are available from Intel Corporation, of Santa Clara, Calif. Alternatively, processor  may be from another company, such as ARM Holdings, Ltd, of Sunnyvale, Calif., MIPS Technologies of Sunnyvale, Calif., etc. Processor  may be a special-purpose processor, such as, for example, a network or communication processor, compression engine, graphics processor, co-processor, embedded processor, or the like. Processor  may be implemented on one or more chips included within one or more packages. Processor  may be a part of and\/or may be implemented on one or more substrates using any of a number of process technologies, such as, for example, BiCMOS, CMOS, or NMOS. In the embodiment shown in , processor  has a system-on-a-chip (SOC) configuration.","In one embodiment, the processor  includes an integrated graphics unit  which includes logic for executing graphics commands such as 3D or 2D graphics commands. While the embodiments of the invention are not limited to any particular integrated graphics unit , in one embodiment, the graphics unit  is capable of executing industry standard graphics commands such as those specified by the Open GL and\/or Direct X application programming interfaces (APIs) (e.g., OpenGL 4.1 and Direct X 11).","The processor  may also include one or more cores -, although a single core is illustrated in , again, for the sake of clarity. In many embodiments, the core(s) - includes internal functional blocks such as one or more execution units, retirement units, a set of general purpose and specific registers, etc. If the core(s) are multi-threaded or hyper-threaded, then each hardware thread may be considered as a \u201clogical\u201d core as well. The cores - may be homogenous or heterogeneous in terms of architecture and\/or instruction set. For example, some of the cores may be in order while others are out-of-order. As another example, two or more of the cores may be capable of executing the same instruction set, while others may be capable of executing only a subset of that instruction set or a different instruction set.","The processor  may also include one or more caches, such as cache  which may be implemented as a SRAM and\/or a DRAM. In many embodiments that are not shown, additional caches other than cache  are implemented so that multiple levels of cache exist between the execution units in the core(s) - and memory devices B and B. For example, the set of shared cache units may include an upper-level cache, such as a level 1 (L1) cache, mid-level caches, such as level 2 (L2), level 3 (L3), level 4 (L4), or other levels of cache, an (LLC), and\/or different combinations thereof. In different embodiments, cache  may be apportioned in different ways and may be one of many different sizes in different embodiments. For example, cache  may be an 8 megabyte (MB) cache, a 16 MB cache, etc. Additionally, in different embodiments the cache may be a direct mapped cache, a fully associative cache, a multi-way set-associative cache, or a cache with another type of mapping. In other embodiments that include multiple cores, cache  may include one large portion shared among all cores or may be divided into several separately functional slices (e.g., one slice for each core). Cache  may also include one portion shared among all cores and several other portions that are separate functional slices per core.","The processor  may also include a home agent  which includes those components coordinating and operating core(s) -. The home agent unit  may include, for example, a power control unit (PCU) and a display unit. The PCU may be or include logic and components needed for regulating the power state of the core(s) - and the integrated graphics unit . The display unit is for driving one or more externally connected displays.","As mentioned, in some embodiments, processor  includes an integrated memory controller (IMC) , near memory cache (MSC) controller, and NVRAM controller  all of which can be on the same chip as processor , or on a separate chip and\/or package connected to processor . DRAM device  may be on the same chip or a different chip as the IMC  and MSC controller ; thus, one chip may have processor  and DRAM device ; one chip may have the processor  and another the DRAM device  and (these chips may be in the same or different packages); one chip may have the core(s) - and another the IMC , MSC controller  and DRAM  (these chips may be in the same or different packages); one chip may have the core(s) -, another the IMC  and MSC controller , and another the DRAM  (these chips may be in the same or different packages); etc.","In some embodiments, processor  includes an I\/O subsystem  coupled to IMC . I\/O subsystem  enables communication between processor  and the following serial or parallel I\/O devices: one or more networks  (such as a Local Area Network, Wide Area Network or the Internet), storage I\/O device (such as flash\/magnetic\/optical mass storage B, BIOS flash , TPM flash ) and one or more non-storage I\/O devices  (such as display, keyboard, speaker, and the like). I\/O subsystem  may include a platform controller hub (PCH) (not shown) that further includes several I\/O adapters  and other I\/O circuitry to provide access to the storage and non-storage I\/O devices and networks. To accomplish this, I\/O subsystem  may have at least one integrated I\/O adapter  for each I\/O protocol utilized. I\/O subsystem  can be on the same chip as processor , or on a separate chip and\/or package connected to processor .","I\/O adapters  translate a host communication protocol utilized within the processor  to a protocol compatible with particular I\/O devices. For flash\/magnetic\/optical mass storage B, some of the protocols that I\/O adapters  may translate include Peripheral Component Interconnect (PCI)-Express (PCI-E), 3.0; USB, 3.0; SATA, 3.0; Small Computer System Interface (SCSI), Ultra-640; and Institute of Electrical and Electronics Engineers (IEEE) 1394 \u201cFirewire;\u201d among others. For BIOS flash , some of the protocols that I\/O adapters  may translate include Serial Peripheral Interface (SPI), Microwire, among others. Additionally, there may be one or more wireless protocol I\/O adapters. Examples of wireless protocols, among others, are used in personal area networks, such as IEEE 802.15 and Bluetooth, 4.0; wireless local area networks, such as IEEE 802.11-based wireless protocols; and cellular protocols.","In some embodiments, the I\/O subsystem  is coupled to a TPM control  to control access to system persistent states, such as secure data, encryption keys, platform configuration information and the like. In one embodiment, these system persistent states are stored in a TMP NVRAM  and accessed via NVRAM controller ","In one embodiment, TPM  is a secure micro-controller with cryptographic functionalities. TPM  has a number of trust-related capabilities; e.g., a SEAL capability for ensuring that data protected by a TPM is only available for the same TPM. TPM  can protect data and keys (e.g., secrets) using its encryption capabilities. In one embodiment, TPM  has a unique and secret RSA key, which allows it to authenticate hardware devices and platforms. For example, TPM  can verify that a system seeking access to data stored in computer system  is the expected system. TPM  is also capable of reporting the integrity of the platform (e.g., computer system ). This allows an external resource (e.g., a server on a network) to determine the trustworthiness of the platform but does not prevent access to the platform by the user.","In some embodiments, I\/O subsystem  also includes a Management Engine (ME) , which is a microprocessor that allows a system administrator to monitor, maintain, update, upgrade, and repair computer system . In one embodiment, a system administrator can remotely configure computer system  by editing the contents of the decode table  through ME  via networks .","For convenience of explanation, the remainder of the application sometimes refers to NVRAM  as a PCMS device. A PCMS device includes multi-layered (vertically stacked) PCM cell arrays that are non-volatile, have low power consumption, and are modifiable at the bit level. As such, the terms NVRAM device and PCMS device may be used interchangeably in the following discussion. However it should be realized, as discussed above, that different technologies besides PCMS may also be utilized for NVRAM .","It should be understood that a computer system can utilize NVRAM  for system memory, mass storage, firmware memory and\/or other memory and storage purposes even if the processor of that computer system does not have all of the above-described components of processor , or has more components than processor .","In the particular embodiment shown in , the MSC controller  and NVRAM controller  are located on the same die or package (referred to as the CPU package) as the processor . In other embodiments, the MSC controller  and\/or NVRAM controller  may be located off-die or off-CPU package, coupled to the processor  or CPU package over a bus such as a memory bus (like a DDR bus (e.g., a DDR3, DDR4, etc)), a PCI express bus, a desktop management interface (DMI) bus, or any other type of bus.",{"@attributes":{"id":"p-0124","num":"0123"},"figref":["FIGS. 4A-M","FIGS. 4A-M"]},"While some of the same numerical designations are used across multiple figures in , this does not necessarily mean that that the structures identified by those numerical designations are always identical. For example, while the same numbers are used to identify an integrated memory controller (IMC)  and CPU  in several figures, these components may be implemented differently in different figures. Some of these differences are not highlighted because they are not pertinent to understanding the underlying principles of the invention.","While several different system platform configuration approaches are described below, these approaches fall into two broad categories: split architecture, and unified architecture. Briefly, in the split architecture scheme, a memory side cache (MSC) controller (e.g., located in the processor die or on a separate die in the CPU package) intercepts all system memory requests. There are two separate interfaces that \u201cflow downstream\u201d from that controller that exit the CPU package to couple to the Near Memory and Far Memory. Each interface is tailored for the specific type of memory and each memory can be scaled independently in terms of performance and capacity.","In the unified architecture scheme a single memory interface exits the processor die or CPU package and all memory requests are sent to this interface. The MSC controller along with the Near and Far Memory subsystems are consolidated on this single interface. This memory interface must be tailored to meet the memory performance requirements of the processor and must support a transactional, out-of-order protocol at least because PCMS devices may not process read requests in order. In accordance with the above general categories, the following specific platform configurations may be employed.","The embodiments described below include various types of buses\/channels. The terms \u201cbus\u201d and \u201cchannel\u201d are used synonymously herein. The number of memory channels per DIMM socket will depend on the particular CPU package used in the computer system (with some CPU packages supporting, for example, three memory channels per socket).","Additionally, in the embodiments described below which use DRAM, virtually any type of DRAM memory channels may be used including, by way of example and not limitation, DDR channels (e.g., DDR3, DDR4, DDR5, etc). Thus, while DDR is advantageous because of its wide acceptance in the industry, resulting price point, etc., the underlying principles of the invention are not limited to any particular type of DRAM or volatile memory.",{"@attributes":{"id":"p-0130","num":"0129"},"figref":["FIG. 4A","FIG. 4A"],"b":["403","406","401","450","451","407","401","403","406","310","331","124","124","331"]},"The DIMMs - use DDR slots and electrical connections defining a DDR channels  with DDR address, data and control lines and voltages (e.g., the DDR3 or DDR4 standard as defined by the Joint Electron Devices Engineering Council (JEDEC)). The PCM devices on the DIMMs - provide the far memory capacity of this split architecture, with the DDR channels  to the CPU package  able to carry both DDR and transactional protocols. In contrast to DDR protocols in which the processor  or other logic within the CPU package (e.g., the IMC  or MSC controller ) transmits a command and receives an immediate response, the transactional protocol used to communicate with PCM devices allows the CPU  to issue a series of transactions, each identified by a unique transaction ID. The commands are serviced by a PCM controller on the recipient one of the PCM DIMMs, which sends responses back to the CPU package , potentially out of order. The processor  or other logic within the CPU package  identifies each transaction response by its transaction ID, which is sent with the response. The above configuration allows the system to support both standard DDR DRAM-based DIMMs (using DDR protocols over DDR electrical connections) and PCM-based DIMMs configurations (using transactional protocols over the same DDR electrical connections).",{"@attributes":{"id":"p-0132","num":"0131"},"figref":"FIG. 4B","b":["452","440","310","331","124","453","442","401","453","442","401","452","453"]},{"@attributes":{"id":"p-0133","num":"0132"},"figref":"FIG. 4C","b":["403","406","401","407","403","406","310","331","124","455","454","455"]},{"@attributes":{"id":"p-0134","num":"0133"},"figref":"FIG. 4D","b":["452","440","310","331","124","455","454","455","401","440","452"]},{"@attributes":{"id":"p-0135","num":"0134"},"figref":"FIG. 4E","b":["456","454","310","331","124","456"]},{"@attributes":{"id":"p-0136","num":"0135"},"figref":"FIG. 4F","b":["458","457","458","458","124","458","457","457"]},{"@attributes":{"id":"p-0137","num":"0136"},"figref":"FIG. 4G","b":["124","310","410","411","411","410","411","411"],"i":["a ","b ","a ","b"]},{"@attributes":{"id":"p-0138","num":"0137"},"figref":"FIG. 4H","b":["461","461","124","460","470","461","462","461","461","470"],"i":["a ","b "]},{"@attributes":{"id":"p-0139","num":"0138"},"figref":"FIG. 4I","b":["464","401","471","463","463","124","460","470","462","463","463","463","470"],"i":["a","b ","b "]},{"@attributes":{"id":"p-0140","num":"0139"},"figref":"FIG. 4J","b":["465","465","462","401","124","465"]},{"@attributes":{"id":"p-0141","num":"0140"},"figref":"FIG. 4K","b":["466","470","401","124","466"]},{"@attributes":{"id":"p-0142","num":"0141"},"figref":"FIG. 4L","b":["464","471","310","331","124","467","468","401","467"]},{"@attributes":{"id":"p-0143","num":"0142"},"figref":"FIG. 4M","b":["469","469","468","401","469","469","124"]},"In some of the embodiments described above, such as that illustrated in , the DRAM DIMMS and PCM-based DIMMS reside on the same memory channel. Consequently the same set of address\/control and data lines are used to connect the CPU to both the DRAM and PCM memories. In order to reduce the amount of data traffic through the CPU mesh interconnect, in one embodiment, a DDR DIMM on a common memory channel with a PCM-based DIMM is configured to act as the sole MSC for data stored in the PCM-based DIMM. In such a configuration, the far memory data stored in the PCM-based DIMM is only cached in the DDR DIMM near memory within the same memory channel, thereby localizing memory transactions to that particular memory channel.","Additionally, to implement the above embodiment, the system address space may be logically subdivided between the different memory channels. For example, if there are four memory channels, then \u00bc of the system address space may be allocated to each memory channel. If each memory channel is provided with one PCMS-based DIMM and one DDR DIMM, the DDR DIMM may be configured to act as the MSC for that \u00bc portion of the system address space.","The choice of system memory and mass storage devices may depend on the type of electronic platforms on which embodiments of the invention are employed. For example, in a personal computer, tablet computer, notebook computer, smartphone, mobile phone, feature phone, personal digital assistant (PDA), portable media player, portable gaming device, gaming console, digital camera, switch, hub, router, set-top box, digital video recorder, or other devices that have relatively small mass storage requirements, the mass storage may be implemented using NVRAM mass storage A alone, or using NVRAM mass storage A in combination with a flash\/magnetic\/optical mass storage B. In other electronic platforms that have relatively large mass storage requirements (e.g., large-scale servers), the mass storage may be implemented using magnetic storage (e.g., hard drives) or any combination of magnetic storage, optical storage, holographic storage, mass-storage flash memory, and NVRAM mass storage A. In such a case, system hardware and\/or software responsible for storage may implement various intelligent persistent storage allocation techniques to allocate blocks of persistent program code and data between the FM B\/NVRAM storage A and a flash\/magnetic\/optical mass storage B in an efficient or otherwise useful manner.","For example, in one embodiment a high powered server is configured with a near memory (e.g., DRAM), a PCMS device, and a magnetic mass storage device for large amounts of persistent storage. In one embodiment, a notebook computer is configured with a near memory and a PCMS device which performs the role of both a far memory and a mass storage device (i.e., which is logically partitioned to perform these roles as shown in ). One embodiment of a home or office desktop computer is configured similarly to a notebook computer, but may also include one or more magnetic storage devices to provide large amounts of persistent storage capabilities.","One embodiment of a tablet computer or cellular telephony device is configured with PCMS memory but potentially no near memory and no additional mass storage (for cost\/power savings). However, the tablet\/telephone may be configured with a removable mass storage device such as a flash or PCMS memory stick.","Various other types of devices may be configured as described above. For example, portable media players and\/or personal digital assistants (PDAs) may be configured in a manner similar to tablets\/telephones described above, gaming consoles may be configured in a similar manner to desktops or laptops. Other devices which may be similarly configured include digital cameras, routers, set-top boxes, digital video recorders, televisions, and automobiles.","In one embodiment of the invention, the bulk of DRAM in system memory is replaced with PCM. As previously discussed, PCM provides significant improvements in memory capacity at a significantly lower cost relative to DRAM and is non-volatile. However, certain PCM characteristics such as asymmetrical Read-vs-Write performance, write cycling endurance limits, as well as its non-volatile nature makes it challenging to directly replace DRAM without incurring major software changes. The embodiments of the invention described below provide a software-transparent way to integrate PCM while also enabling newer usages through software enhancements. These embodiments promote a successful transition in memory subsystem architecture and provide a way to consolidate both memory and storage using a single PCM pool, thus mitigating the need for a separate non-volatile storage tier in the platform.","The particular embodiment illustrated in  includes one or more processor cores  each with an internal memory management unit (MMU)  for generating memory requests and one or more internal CPU caches  for storing lines of program code and data according to a specified cache management policy. As previously mentioned, the cache management policy may comprise an exclusive cache management policy (in which any line present in one particular cache level in the hierarchy is not present in any other cache level) or an inclusive cache management policy (in which duplicate cache lines are stored at different levels of the cache hierarchy). The specific cache management policies which may be employed for managing the internal caches  are well understood by those of skill in the art and, as such, will not be described here in detail. The underlying principles of the invention are not limited to any particular cache management policy.","Also illustrated in  is a home agent  which provides access to the MSC  by generating memory channel addresses (MCAs) for memory requests. The home agent  is responsible for managing a specified memory address space and resolves memory access conflicts directed to that memory space. Thus, if any core needs to access a given address space, it will send requests to that home agent , which will then send the request to that particular MMU . In one embodiment, one home agent  is allocated per MMU ; however, in some embodiments, a single home agent  may service more than one memory management unit .","As illustrated in , a MSC  is configured in front of the PCM-based far memory . The MSC  manages access to a near memory  and forwards memory access requests (e.g., reads and writes) to the far memory controller  when appropriate (e.g., when the requests cannot be serviced from the near memory ). The MSC  includes a cache control unit  which operates responsive to a tag cache  which stores tags which identify the cache lines contained within the near memory . In operation, when the cache control unit  determines that the memory access request can be serviced from the near memory  (e.g., in response to a cache hit), it generates a near memory address (NMA) to identify data stored within the near memory . A near memory control unit  interprets the NMA and responsively generates electrical signals to access the near memory . As previously mentioned, in one embodiment, the near memory is a dynamic random access memory (DRAM). In such a case, the electrical signals may include row address strobe (RAS) and column address strobe (CAS) signals. It should be noted, however, that the underlying principles of the invention are not limited to the use of DRAM for near memory.","Another component that ensures software-transparent memory application is an optimized PCM far memory controller  that manages the PCM far memory  characteristics while still providing the performance required. In one embodiment, the PCM controller  includes an Address Indirection Table  that translates the MCA generated by the cache control unit  to a PDA which is used to directly address the PCM far memory . These translations may occur at the granularity of a \u201cblock\u201d which is typically 5 KB. The translation is required as, in one embodiment, the far memory controller  continuously moves the PCM blocks throughout the PCM device address space to ensure no wear-out hot spots due to a high frequency of writes to any specific block. As previously described, such a technique is sometimes referred to herein as \u201cwear-leveling\u201d.","Thus, the MSC  is managed by the cache control unit  which allows the MSC  to absorb, coalesce and filter transactions (e.g., reads and writes) to the PCM far memory . The cache control unit  manages all data movement and consistency requirements between the near memory  and the PCM far memory . Additionally, in one embodiment, the MSC cache controller  interfaces to the CPU(s) and provides the standard synchronous load\/store interface used in traditional DRAM based memory subsystems.","Exemplary read and write operations will now be described within the context of the architecture shown in . In one embodiment, a read operation will first arrive at the MSC controller  which will perform a look-up to determine if the requested data is present (e.g., utilizing the tag cache ). If present, it will return the data to the requesting CPU, core  or I\/O device (not shown). If the data is not present, the MSC controller  will send the request along with the system memory address (also referred to herein as the memory channel address or MCA) to the PCM far memory controller . The PCM controller  will use the Address Indirection Table  to translate the address to a PDA and direct the read operation to this region of the PCM. Upon receiving the requested data from the PCM far memory , the PCM controller  will return the requested data to the MSC controller  which will store the data in the MSC near memory  and also send the data to the requesting CPU core , or I\/O Device. Subsequent requests for this data may be serviced directly from the MSC near memory  until it is replaced by some other PCM data.","In one embodiment, a memory write operation also first goes to the MSC controller  which writes it into the MSC near memory . In this embodiment, the data may not be sent directly to the PCM far memory  when a write operation is received. For example, the data may be sent to the PCM far memory  only when the location in the MSC near memory  in which the data is stored must be re-used for storing data for a different system memory address. When this happens, the MSC controller  notices that the data is not current in PCM far memory  and will thus retrieve it from near memory  and send it to the PCM controller . The PCM controller  looks up the PDA for the system memory address and then writes the data to the PCM far memory .","In one embodiment, the size of the MSC near memory  will be dictated by the workload memory requirements as well as the near and far memory performance. For a DRAM-based MSC, the size may be set to a tenth the size of the workload memory footprint or the PCM far memory  size. Such an MSC is very large compared to conventional caches found in current processor\/system architectures. By way of example, and not limitation, for a PCM far memory size of 128 GB, the size of the MSC near memory can be as large as 16 GB.",{"@attributes":{"id":"p-0159","num":"0158"},"figref":"FIG. 5B","b":["510","542","544","545","518","550","511","518","515"]},"This embodiment also includes a set of logical units responsible for data routing and processing including a set of data buffers  for storing data fetched from near memory or stored to near memory. In one embodiment, a prefetch data cache  is also included for storing data prefetched from near memory and\/or far memory. However, the prefetch data cache  is optional and is not necessary for complying with the underlying principles of the invention.","An error correction code (ECC) generator\/checker unit  generates and checks ECCs to ensure that data written to or read from near memory is free from errors. As discussed below, in one embodiment of the invention, the ECC generator\/checker unit  is modified to store cache tags. Specific ECCs are well understood by those of ordinary skill in the art and will therefore not be described here in detail. The channel controllers  couple the data bus of the near memory  to the MSC  and generate the necessary electrical signaling for accessing the near memory  (e.g., RAS and CAS signaling for a DRAM near memory).","Also illustrated in  is a far memory control interface  for coupling the MSC  to far memory. In particular, the far memory control interface  generates the MCAs required to address the far memory and communicates data between the data buffers  and far memory.","As mentioned, the near memory  employed in one embodiment is very large compared to conventional caches found in current processor\/system architectures. Consequently, the tag cache  that maintains the system memory address translation to near memory addresses may also be very large. The cost of storing and looking up the MSC tags can be a significant impediment to building large caches. As such, in one embodiment of the invention, this issue is resolved using an innovative scheme that stores the cache tags within the storage allocated in the MSC for ECC protection, thereby essentially removing the cost of storage for the tags.","This embodiment is illustrated generally in  which shows an integrated tag cache and ECC unit  for storing\/managing cache tags, storing ECC data, and performing ECC operations. As illustrated, the stored tags are provided to the tag check\/command scheduler  upon request when performing tag check operations (e.g., to determine if a particular block of data is stored within the near memory cache ).",{"@attributes":{"id":"p-0165","num":"0164"},"figref":["FIG. 5D","FIG. 5D"],"b":["524","523","522","522","523","554","554","554","525","523","525","525"],"i":"a "},"In one embodiment, a 3-Byte (24-bit) tag  is used with the bit assignments illustrated in . Specifically, bits  to  are address bits which provide the upper address bits of the cache line. For a system address having 56 bits (e.g., SPA [55:00]), bits  to  map to bits - of the system address, allowing for the smallest cache size of 512 MB. Returning to the 3-Byte tag, bits - are reserved; bits - are directory bits which provide information on remote CPU caching of the cache line (e.g., providing an indication as to the other CPUs on which the line is cached); bits - indicate the current state of the cache line (e.g., 00=clean; 01=dirty; 10 and 11=unused); and bit  indicates whether the cache line is valid (e.g., 1=valid; 0=invalid).","Utilizing a direct-mapped cache architecture as described above, which allows the near memory address to be directly extracted from the system memory address reduces or eliminates the latency cost of looking up the tag store before the MSC  can be read, thereby significantly improving performance. Moreover, the time to check the cache tags to decide if the MSC  has the required data is also eliminated as it is done in parallel with the ECC check of the data read form the MSC.","Under certain conditions, storing tags with the data may create an issue for writes. A write first reads the data in order to ensure that it does not over-write data for some other address. Such a read before every write could become costly. One embodiment of the invention employs a dirty line tag cache that maintains the tags of recently-accessed near memory addresses (NMAs). Since many writes target recently accessed addresses, a reasonably small tag cache can get an effective hit rate to filter most of the reads prior to a write.","Additional details associated with one embodiment of a PCM DIMM  including a PCM far memory controller  and a set of PCM far memory modules -is illustrated in . In one embodiment, a single pool of PCM far memory -is dynamically shared between system memory and storage usages. In this embodiment, the entire PCM pool -may be subdivided into \u201cblocks\u201d of 4 KB size. A PCM Descriptor Table (PDT)  identifies the use of each PCM block as either memory or storage. For example, each row of the PDT may represent a particular block with a particular column identifying the use of each block (e.g., 1=memory; 0=storage). In this embodiment, an initial system configuration can partition the PCM blocks within the PCM -between storage and memory use (i.e., by programming the PDT ). In one embodiment, the same table is used to exclude bad blocks and provide spare blocks for wearing-leveling operations. In addition, the PDT  may also include the mapping of each PCMS block to a \u201clogical\u201d block address used by software. In the case of System Memory, the logical block address is the same as the MCA or SPA. This association is needed to update the Address Indirection Table (AIT)  whenever the PCMS block is moved due to wear leveling. When this happens the logical block address used by software has to be mapped to a different PCMS Device Address (PDA). In one embodiment, this mapping is stored in the AIT and is updated on every wear-level move.","As illustrated, the PCM controller  includes a system physical address (SPA)-to-PCM mapper  which operates in response to a wear management unit  and an address indirection unit  to map SPAs to PCM blocks. In one embodiment, the wear management logic  implements a wear leveling algorithm to account for the fact that the storage cells of the PCM -begin to wear out after too many write and\/or erase accesses. Wear leveling spreads writes and erases across the PCM device's memory cells by, for example, forcing data blocks with low cycle counts to occasionally move, and thereby allowing high cycled data blocks to be placed in memory cells that stored the low cycled data blocks. Typically, the majority of blocks do not cycle, but high cycle count blocks are most likely to fail and wear leveling swaps addresses of high cycle count blocks with low cycle count blocks. The wear management logic  may track the cycle counts using one or more counters and registers (e.g., the counters may increment by one each time a cycle is detected and the result may be stored in the set of registers).","In one embodiment, the address indirection logic  includes an address indirection table (AIT) containing an indication of the PCM blocks to which write operations should be directed. The AIT may be used to automatically move blocks between memory and storage usages. From the software perspective, the accesses to all the blocks uses traditional memory load\/store semantics (i.e., wear leveling and address indirection operations occur transparently to software). In one embodiment, AIT is used to translate the SPA that is generated by software to a PDA. This translation is required as the need to uniformly wear the PCMS devices, the data will need to be moved around in PDA space to avoid any hotspots. When such a move occurs, the relationship between SPA and PDA will change and the AIT will be updated to reflect this new translation.","Following the SPA to PCM mapping, a scheduler unit  schedules the underlying PCM operations (e.g., reads and\/or writes) to the PCM devices -and a PCM protocol engine  generates the electrical signaling required for performing the read\/write operations. An ECC unit  performs error detection and correction operations and data buffers  temporarily buffer data being read from or written to the PCM devices -. A persistent write buffer  is used to hold data that is guaranteed to be written back to PCMS even in the event of an unexpected power failure (e.g., it is implemented using non-volatile storage). Flush support logic  is included to flush the persistent write buffers to PCMS, either periodically and\/or according to a specified data flushing algorithm (e.g., after the persistent write buffers reach a specified threshold).","In one embodiment, the MSC  automatically routes storage accesses directly to the PCM far memory controller  and memory accesses to the MSC cache control unit . Storage accesses coming to the PCM far memory controller  are treated as regular reads and writes and the address indirection and wear leveling mechanisms described herein are applied as usual. An additional optimization is employed in one embodiment of the invention which can be implemented when data needs to move between storage and memory. Since a common PCM pool -is used, data movement can be eliminated or deferred by simply changing the pointers in the translation tables (e.g., the AIT). For example, when data is transferred from storage to memory, a pointer identifying the data in a particular physical PCM storage location may be updated to indicate that the same physical PCM storage location is now a memory location in system memory. In one embodiment, this is done by hardware in a software-transparent manner to provide both performance and power benefits.","In addition to the software-transparent mode of operation, one embodiment of the MSC controller  provides alternate modes of operations as indicated by the MSC range registers (RRs) . These modes of operation may include, but are not limited to the following:","(1) Direct access of PCM memory for storage class applications. Such usage will also require the MSC controller  to ensure that writes submitted to PCM  are actually committed to a persistent state.","2) Hybrid use of the near memory , exposing portions of it to software for direct use while maintaining the remaining as an MSC. When a portion of near memory  is exposed to software for direct use, that portion is directly addressable within the system address space. This allows certain applications to explicitly split their memory allocation between a high-performance small region (the near memory ) and a relatively lower performance bulk region (the far memory ). By contrast, the portion allocated as a cache within the MSC does not form part of the system address space (but instead acts as a cache for far memory  as described herein).","As previously discussed, the MSC architecture is defined such that several different system partitioning approaches are possible. These approaches fall into two broad buckets:","(1) Split Architecture: In this scheme the MSC controller  is located in the CPU and intercepts al system memory requests. There are two separate interfaces from the MSC that exit the CPU to connect to the Near Memory (e.g., DRAM) and Far memory (e.g., PCM). Each interface is tailored for the specific type of memory and each memory can be scaled independently in terms of performance and capacity.","(2) Unified Architecture: In this scheme a single memory interface exits the CPU and all memory requests are sent to this interface. The MSC controller  along with the Near Memory (e.g., DRAM) and Far Memory (e.g., PCM) subsystem are consolidated external to the CPU on this single interface. In one embodiment, this memory interface is tailored to meet the memory performance requirements of the CPU and supports a transactional, out-of-order protocol. The Near and Far memory requirements are met in a \u201cunified\u201d manner on each of these interfaces.","Within the scope of the above buckets several different portioning options are feasible some of which are described below.","1) Split Example:","Near Memory: DDR5 DIMM's","Near Memory Interface: One or more DDR5 channels","Far Memory: PCM controller\/device on a PCI express (PCIe) card","Far Memory Interface: x16 PCIe, Gen ","2) Unified Example:","CPU Memory Interface: one or more KTMI (or QPMI) channels","Near\/Far Memory with MSC\/PCM Controller on a Riser Card","Near Memory Interface off MSC\/PCM Controller: DDR5 Interface","Far Memory Interface off MSC\/PCM Controller: PCM Device Interface","As discussed above, a two-level memory hierarchy may be used for introducing fast non-volatile memory such as PCM as system memory while using a very large DRAM-based near memory. The near memory may be used as a hardware-managed cache. However, some applications are not hardware cache-friendly and, as such, would benefit from alternate ways to use such memory. Because there may be several different applications running on a server at any given time, one embodiment of the invention allows multiple usage modes to be enabled concurrently. Additionally, one embodiment provides the ability to control the allocation of near memory for each of these usage modes.","In one embodiment, the MSC controller  provides the following modes for using near memory. As previously mentioned, in one embodiment, the current mode of operation may be specified by operation codes stored in the MSC range registers (RRs) .","(1) Write-Back Caching Mode: In this mode, all or portions of the near memory  is used as a cache for the PCM memory . While in write-back mode, every write operation is directed initially to the near memory  (assuming that the cache line to which the write is directed is present in the cache). A corresponding write operation is performed to update the PCM far memory  only when the cache line within the near memory  is to be replaced by another cache line (in contrast to write-through mode described below in which each write operation is immediately propagated to the far memory ).","In one embodiment, a read operation will first arrive at the MSC cache controller  which will perform a look-up to determine if the requested data is present in the PCM far memory  (e.g., utilizing a tag cache ). If present, it will return the data to the requesting CPU, core  or I\/O device (not shown in ). If the data is not present, the MSC cache controller  will send the request along with the system memory address to the PCM far memory controller . The PCM far memory controller  will translate the system memory address to a PCM physical device address (PDA) and direct the read operation to this region of the far memory . As previously mentioned this translation may utilize an address indirection table (AIT)  which the PCM controller  uses to translate between system memory addresses and PCM PDAs. In one embodiment, the AIT is updated as part of the wear leveling algorithm implemented to distribute memory access operations and thereby reduce wear on the PCM FM .","Upon receiving the requested data from the PCM FM , the PCM FM controller  returns the requested data to the MSC controller  which stores the data in the MSC near memory  and also sends the data to the requesting processor core , or I\/O Device (not shown in ). Subsequent requests for this data may be serviced directly from the near memory  until it is replaced by some other PCM FM data.","In one embodiment, a memory write operation also first goes to the MSC controller  which writes it into the MSC near memory acting as a FM cache . In this embodiment, the data may not be sent directly to the PCM FM  when a write operation is received. For example, the data may be sent to the PCM FM  only when the location in the MSC near memory acting as a FM cache  in which the data is stored must be re-used for storing data for a different system memory address. When this happens, the MSC controller  notices that the data is not current in the PCM FM  and will thus retrieve it from near memory acting as a FM cache  and send it to the PCM FM controller . The PCM controller  looks up the PDA for the system memory address and then writes the data to the PCM FM .","(2) Near Memory Bypass Mode: In this mode all reads and writes bypass the NM acting as a FM cache  and go directly to the PCM far memory . Such a mode may be used, for example, when an application is not cache friendly or requires data to be committed to persistence at the granularity of a cache line. In one embodiment, the caching performed by the processor caches  and the NM acting as a FM cache  operate independently of one another. Consequently, data may be cached in the NM acting as a FM cache  which is not cached in the processor caches  (and which, in some cases, may not be permitted to be cached in the processor caches ) and vice versa. Thus, certain data which may be designated as \u201cuncacheable\u201d in the processor caches  may be cached within the NM acting as a FM cache .","(3) Near Memory Read-Cache Write Bypass Mode: This is a variation of the above mode where read caching of the persistent data from PCM  is allowed (i.e., the persistent data is cached in the MSC  for read-only operations). This is useful when most of the persistent data is \u201cRead-Only\u201d and the application usage is cache-friendly.","(5) Near Memory Read-Cache Write-Through Mode: This is a variation of the previous mode, where in addition to read caching, write-hits are also cached. Every write to the MSC near memory  causes a write to the PCM far memory . Thus, due to the write-through nature of the cache, cache-line persistence is still guaranteed.","(5) Near Memory Direct Access Mode: In this mode, all or portions of the near memory are directly visible to software and form part of the system memory address space. Such memory may be completely under software control. Any data movement from the PCM memory  to this region of near memory requires explicit software copies. Such a scheme may create a non-uniform memory address (NUMA) memory domain for software where it gets much higher performance from near memory  relative to PCM far memory . Such a usage may be employed for certain high performance computing (HPC) and graphics applications which require very fast access to certain data structures. This near memory direct access mode is equivalent to \u201cpinning\u201d certain cache lines in near memory. Such pinning may be done effectively in larger, multi-way, set-associative caches.","Table A below summarizes each of the above-described modes of operation.",{"@attributes":{"id":"p-0194","num":"0193"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"77pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"77pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE A"},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Mode","Reads","Writes"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Write-Back Cache","Allocate on Miss","Allocate on Miss"]},{"entry":[{},"Writeback on Dirty Evict","Writeback on Dirty Evict"]},{"entry":["Cache Bypass","Bypass to Far Memory","Bypass to Far Memory"]},{"entry":["Read Cache\/Write","Allocate on Miss","Bypass to Far Memory"]},{"entry":["Bypass",{},"Invalidate Cached Line"]},{"entry":["Read Cache\/Write","Allocate on Miss","Update only on Hit"]},{"entry":["Through",{},"Write through to Far"]},{"entry":[{},{},"Memory"]},{"entry":["Direct Access","Read Direct from Near","Write Direct to Near"]},{"entry":[{},"Memory","Memory"]},{"entry":[{},"No Far Memory Access","No Far Memory Access"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"The processor and chipset components used to implement the above modes of operation include the following:","(1) A Memory-Side-Cache Controller  that manages the near memory in a two layer memory (2LM) hierarchy.","(2) A set of Range Registers  (see ) in the memory-side-cache  that determines the system address ranges for each of the above-described operating modes.","(3) A mechanism to acknowledge write completions from the PCM memory subsystem  to the MSC controller .","(5) A mechanism to invalidate lines in the near memory .","(5) A flush engine to evict dirty lines to PCM and invalidate in specified regions of the near memory address space.","In one embodiment, the memory ranges for each of the usage modes are contiguous in system address space. However multiple, disjoint, regions may use the same mode. In one embodiment, each mode range register within the set of MSC RRs  provides the following information:","(1) the mode of operation (e.g., write-back, near memory bypass mode, etc);","(2) the range base in the system address space (e.g., at 2 MB granularity or greater); and","(3) a range mask field which identifies the size of the region.","In one embodiment, the number of modes supported is implementation-specific but it is assumed that only one contiguous system address range is available for each mode of operation. If a near memory direct access range register is specified, then it is assumed that this is will be mapped to a contiguous region starting at the bottom of the near memory address space. Such a contiguous region must be smaller than the size of near memory. Additionally, if any of the caching modes are being used, the direct access region size must be smaller than the near memory size to allow for adequate cache size for the required performance. Such allocation of near memory for various modes may be configurable by the user.","In summary, one embodiment of the invention is implemented in accordance with the following set of operations:","(1) When any Read or Write Access reaches the Memory-Side-Cache controller , it checks the Range Registers  () to determine the current mode of operation.","(2) For any read cache\/write bypass access, the MSC controller  checks to see if the address is currently cached. If it is, it must invalidate the line before sending the write completion back to the source.","(3) For any Write Bypass direct PCM operation, the MSC Controller  awaits a completion back from the PCM controller  to ensure that the write is committed to a globally visible buffer.","(4) Any Read or Write to the Direct Access mode space in Near Memory, is directed to the appropriate region of Near Memory. No transactions are sent to the PCM memory.","(5) Any change in the Range Register configuration to increase or decrease any existing region or add a new region, will require flushing of appropriate cached regions to PCM. For example, if software wishes to increase the size of the Direct Access mode region by reducing the Write-Back Cache region, it may do so by first evicting and invalidating the appropriate portion of the Near Memory Region and then changing the Near Memory Direct Access Mode Range Register. The MSC Controller  will then know that future caching is done to a smaller Near Memory Address Space.","One particular embodiment of the invention in which the system physical address (SPA) space is divided up among multiple MSCs is illustrated in . In the illustrated embodiment, MSC cache  and controller  are associated with SPA region ; MSC cache  and controller  are associated with SPA region ; MSC cache  and controller  are associated with SPA region ; and MSC cache  and controller  are associated with SPA region . Two CPUs,  and , are illustrated, each with four cores,  and , respectively, and a home agent,  and , respectively. The two CPUs,  and , are coupled to a common far memory controller  via far memory interfaces,  and , respectively.","Thus, in , the entire SPA memory space is subdivided into regions, with each region being associated with a particular MSC and controller. In this embodiment, a given MSC may have a non-contiguous SPA space allocation but no two MSCs have will have overlapping SPA space. Moreover, the MSCs are associated with non-overlapping SPA space and no inter-MSC coherency techniques are required.","Any of the near memory modes described above may be employed on the architecture shown in . For example, each MSC controller -, - may be configured to operate in Write-Back Caching Mode, Near Memory Bypass Mode, Near Memory Read-Cache Write Bypass Mode, Near Memory Read-Cache Write-Through Mode, or Near Memory Direct Access Mode. As previously discussed, the particular mode is specified within the range register (RR)  for each MSC .","In one embodiment, different MSCs may concurrently implement different modes of operation. For example, the range registers of MSC controller  may specify the Near Memory Direct Access mode, the range registers of MSC controller  may specify the Write Back Cache mode, the range registers of MSC controller  may specify the Read Cache\/Write Bypass mode, and MSC controller  may specify the Read Cache\/Write Through mode. In addition, in some embodiments, individual MSCs may concurrently implement different modes of operation. For example, MSC controller  may be configured to implement near memory direct access mode for certain system address ranges and a near memory bypass mode for other system address ranges.","The foregoing combinations are, of course, merely illustrative of the manner in which MSC controllers ma be independently programmed. The underlying principles of the invention are not limited to these or any other combinations.","As described with respect to some of embodiments described above (e.g., such as that described with respect to ), an MSC and its MSC controller are configured to operate on the same memory channel (e.g., the same physical DDR bus) as the PCM DIMM responsible for that particular SPA range. Consequently, in this embodiment, memory transactions which occur within the designated SPA range are localized within the same memory channel, thereby reducing data traffic through the CPU mesh interconnect.",{"@attributes":{"id":"p-0218","num":"0217"},"figref":"FIG. 6B","b":["620","621","622","606","605","620","602","603","605","606","621","608","609","610","622","611","612","620","621","622"]},{"@attributes":{"id":"p-0219","num":"0218"},"figref":["FIGS. 6C and 6D","FIG. 6C"],"b":["675","676","605","677","605","675","612","612","678","680","679","681"]},"One embodiment of an optional interleaving process is illustrated in  which shows how software pages can be broken up across multiple MSCs and PCM address spaces using interleaving. In the example shown in , two pages - within the SPA space are interleaved by cache-line interleave logic  to generate two sets of interleaved lines - within the MCA space. For example, all of the odd lines from the memory pages - (e.g., lines 1, 3, 5, etc.) may be sent to a first MCA space , and all of the even lines from the memory pages - (e.g., lines 2, 5, 6, etc.) may be sent to a second MCA space . In one embodiment, the pages are 5 KByte pages, although the underlying principles of the invention are not limited to any page size. PCM controllers - operating in accordance with Address Indirection Tables (AITs) and wear management logic then rearrange the cache lines within the PCM device address (PDA) memory space (as described above). Interleaving of this nature may be used to distribute the workload across MSCs  and\/or PCM devices  (e.g., as an alternative to non-uniform memory address (NUMA)).","Embodiments of the invention may include various steps, which have been described above. The steps may be embodied in machine-executable instructions which may be used to cause a general-purpose or special-purpose processor to perform the steps. Alternatively, these steps may be performed by specific hardware components that contain hardwired logic for performing the steps, or by any combination of programmed computer components and custom hardware components.","As described herein, instructions may refer to specific configurations of hardware such as application specific integrated circuits (ASICs) configured to perform certain operations or having a predetermined functionality or software instructions stored in memory embodied in a non-transitory computer readable medium. Thus, the techniques shown in the figures can be implemented using code and data stored and executed on one or more electronic devices (e.g., an end station, a network element, etc.). Such electronic devices store and communicate (internally and\/or with other electronic devices over a network) code and data using computer machine-readable media, such as non-transitory computer machine-readable storage media (e.g., magnetic disks; optical disks; random access memory; read only memory; flash memory devices; phase-change memory) and transitory computer machine-readable communication media (e.g., electrical, optical, acoustical or other form of propagated signals\u2014such as carrier waves, infrared signals, digital signals, etc.). In addition, such electronic devices typically include a set of one or more processors coupled to one or more other components, such as one or more storage devices (non-transitory machine-readable storage media), user input\/output devices (e.g., a keyboard, a touchscreen, and\/or a display), and network connections. The coupling of the set of processors and other components is typically through one or more busses and bridges (also termed as bus controllers). The storage device and signals carrying the network traffic respectively represent one or more machine-readable storage media and machine-readable communication media. Thus, the storage device of a given electronic device typically stores code and\/or data for execution on the set of one or more processors of that electronic device. Of course, one or more parts of an embodiment of the invention may be implemented using different combinations of software, firmware, and\/or hardware. Throughout this detailed description, for the purposes of explanation, numerous specific details were set forth in order to provide a thorough understanding of the present invention. It will be apparent, however, to one skilled in the art that the invention may be practiced without some of these specific details. In certain instances, well known structures and functions were not described in elaborate detail in order to avoid obscuring the subject matter of the present invention. Accordingly, the scope and spirit of the invention should be judged in terms of the claims which follow."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The following description and accompanying drawings are used to illustrate embodiments of the invention. In the drawings:",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 4A"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 4B"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 4C"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 4D"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 4E"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 4F"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 4G"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 4H"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 4I"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 4J"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 4K"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 4L"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 4M"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 5C"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 5D"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 5E"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 6B"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 6C"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 6D"}]},"DETDESC":[{},{}]}
