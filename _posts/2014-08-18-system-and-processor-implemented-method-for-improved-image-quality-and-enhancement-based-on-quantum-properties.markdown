---
title: System and processor implemented method for improved image quality and enhancement based on quantum properties
abstract: A method and system for generating an image utilizing entangled quantum particle pairs comprising at least one processor; at least one source of entangled quantum particles having first and second channels, the first and second channel s outputting first and second pairs of entangled quantum particles, respectively, a first beam splitter operatively connected to the first channel; the first beam splitter configured to split the first pairs of entangled particles for entry into first and second spatial detectors; at least one focusing device operatively connected to the second channel configured to direct the second pairs of entangled quantum particles towards a distant target; each of the first and second spatial detectors detecting one particle of the first pairs of entangled quantum particles; the at least one processor operating to record the detection of entangled quantum particles by the first and second spatial detectors and create image data for display.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09123114&OS=09123114&RS=09123114
owner: The United States of America as represented by the Secretary of the Army
number: 09123114
owner_city: Washington
owner_country: US
publication_date: 20140818
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","STATEMENT OF GOVERNMENT INTEREST","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS","Entanglement Swapping","Second Order Imaging"],"p":["This application is a Continuation-In-Part application of application Ser. No. 13\/477,890 (U.S. Pat. No. 8,811,768) entitled \u201cImage Enhancement System and Method,\u201d by R. Meyers & K. Deacon, filed May 22, 2012, which is a continuation-in-part of application Ser. No. 13\/247,470 (now U.S. Pat. No. 8,532,427) entitled \u201cSystem and Method for Image Enhancement\u201d by R. Meyers & K. Deacon, filed Sep. 28, 2011, and application Ser. No. 13\/198,133 (now U.S. Pat. No. 8,373,107) entitled \u201cMethod and System for Non-line-of-sight Imaging\u201d filed Aug. 4, 2011, which in turn claims priority of U.S. patent application Ser. No. 12\/819,602 (now U.S. Pat. No. 8,242,428) entitled \u201cMethod and System for LIDAR Utilizing Quantum Properties,\u201d filed Jun. 21, 2010, which in turn claims priority of U.S. application Ser. No. 12\/330,401 (now U.S. Pat. No. 7,812,303) entitled \u201cMethod and System for Creating an Image Using Quantum Properties of Light Based Upon Spatial Information From a Second Light Beam Which Does not Illuminate the Subject,\u201d filed Dec. 8, 2008, which claims priority to U.S. Provisional Patent Application Ser. No. 60\/992,792 filed Dec. 6, 2007. The present application, through application Ser. No. 13\/477,890 (U.S. Pat. No. 8,811,768), U.S. patent application Ser. No. 13\/198,133 entitled \u201cMethod and System for Non-line-of-sight Imaging\u201d and U.S. application Ser. No. 12\/819,602, entitled \u201cMethod and System for LIDAR Utilizing Quantum Properties,\u201d filed Jun. 21, 2010, also claims priority of U.S. patent application Ser. No. 12\/343,384 filed Dec. 23, 2008, entitled \u201cMethod and System for Quantum Imaging Using Entangled Photons Pairs,\u201d now U.S. Pat. No. 7,847,234, issued Dec. 7, 2010. The present application through application Ser. No. 13\/477,890 (U.S. Pat. No. 8,811,768), and application Ser. No. 13\/198,133 (now U.S. Pat. No. 8,373,101) entitled \u201cMethod and System for Non-line-of-sight Imaging\u201d claims the benefit of U.S. application Ser. No. 12\/837,668 entitled \u201cMethod and System for Creating an Image Using the Quantum Properties of Sound or Quantum Particles, filed Jul. 16, 2010, now U.S. Pat. No. 8,053,715, which is a divisional application of U.S. Pat. No. 7,812,303. The present application also claims priority of U.S. patent application Ser. No. 14\/303,078 titled \u201cSystem and processor implemented method for improved image quality and generating an image of a target illuminated by quantum particles,\u201d by R. Meyers & K. Deacon, filed 12 Jun. 2014, U.S. patent application Ser. No. 14\/086,463 titled \u201cSystem and Processor Implemented Method for Improved Image Quality and Enhancement,\u201d by R. Meyers & K. Deacon, filed Nov. 21, 2013, U.S. patent application Ser. No. 14\/022,148 titled \u201cSystem and Method for Image Improved Image Enhancement,\u201d by R. Meyers & K. Deacon, filed Sep. 9, 2013, and U.S. patent application Ser. No. 13\/838,249 (now U.S. Pat. No. 8,594,455 titled \u201cSystem and Method for Image Enhancement and Improvement,\u201d by R. Meyers & K. Deacon, filed Mar. 15, 2013. All of the patent applications and patents mentioned in this paragraph are hereby incorporated by reference.","The embodiments herein may be manufactured, used, and\/or licensed by or for the United States Government without the payment of royalties thereon.","Image processing is a form of signal processing for which the input is an image, such as, for example, a photograph or video frame, and the output is either an image (or series of images) or a set of characteristics or parameters related to the image (or series of images). Forms of image processing include, for example, face detection, feature detection, medical image processing, computer vision (extraction of information from an image by a computer), microscope image processing, etc.","Image resolution relates to the detail that an image possesses. For satellite images, the resolution generally correlates to the area represented by each pixel. Generally speaking, an image is considered to be more accurate and detailed as the area represented by each pixel is decreased. As used herein, the term images include digital or analog images, film images, and\/or other types of images. When an image is captured by a monochrome camera, a single charge-coupled device (CCD) or complementary metal-oxide semiconductor (CMOS) sensor is used to form an image via the light intensity projected onto the sensor. Cameras taking pictures from great distances, such as aerial photos, may not obtain detailed information about the subject matter. Also, the taking of photographs may be subject to motion of the camera and\/or jitter. Consequently, subtle or detail information are not present in the images.","Quantum imaging is a relatively new science that is developing new technology such as Quantum Ghost Imaging (QGI) to exploit quantum optical information. The exploitation of quantum optical information leads to increased resolution over conventional classical optical imaging. Furthermore, quantum imaging is adaptable to adverse imaging situations and there is a benefit to exploiting quantum optical information to image objects through partially obscuring media, i.e., optical turbulence, obstructions, smoke, and fog. Imaging through obscuring media is difficult; such as the difficulty of driving in foggy weather.","Quantum entanglement is a quantum mechanical phenomenon in which the quantum states of two or more quantum particles are linked together such that the quantum state of one quantum particle appears to interact with its counterpart; even though the individual quantum particles may be spatially separated. This apparent interconnection leads to correlations between observable physical properties of remote systems, since the interaction of the remote system with quantum state of one of a pair can be observed though observation of the counterpart. For example, according to quantum mechanics, the spin of a quantum particle is indeterminate until such time as some physical intervention is made to measure the spin; which, in general, could equally be spin-up or spin-down. However, when two members of a spin entangled pair are measured, they will either be correlated or anti-correlated using spin measurements, regardless of the distance between the two particles. It is normally taught in quantum theory that no hidden variable theory can account for these results of quantum mechanics. The statistics of multiple measurements must generally relate to an inequality (called Bell's inequality), which is violated both by quantum mechanical theory and experimental results.","The non-classical two-photon interaction or quantum entanglement was described by Albert Einstein et al. (Einstein, Podolsky, Rosen (hereinafter Einstein, et al.) paradox), \u201cCan Quantum-Mechanical Description of Physical Reality Be Considered Complete?\u201d Physical Review, Volume 47, May 15, 1935, pgs. 777-800. The paradox of quantum entanglement, as described therein, relates to the concept that as a result of the process of measurement of a first system, using quantum mechanics, two different physical quantities are obtainable in the second system, despite the fact that at the time of the measurements, the two systems no longer interact and the second system is not disturbed in any way by the first. Einstein, et al, were unable to reconcile this quantum mechanical description of reality with the so-called classical physics determination that no \u201creal\u201d change can take place in the second system as a consequence of anything that may be done to the first system after the two systems no longer interact.","The theoretical work reported by Klyshko in \u201cCombined EPR and Two-Slit Experiments: Interference of Advanced Waves\u201d, Physics Letters A, Volume 132, number 6, 7, pp. 299-304 (1988) (see also, Soy. Phys. Usp. 31, 74) suggested a non-classical two-photon interaction could exist.","The first two-photon imaging experiment was reported by Pittman et al., in \u201cOptical Imaging by Means of Two-photon Quantum Entanglement,\u201d Physical Review, A, Vol. 52, No. 5, November 1995. According to the Pittman article, a two-photon optical imaging experiment was performed to test the two-particle entanglement as described by Albert Einstein et al., referenced above, to determine if there was a correlation in position and in momentum for an entangled two-photon system; using \u201ctest beam or path\u201d and \u201creference beam or path\u201d photon pairs. Specifically, an aperture placed in front of a fixed detector was illuminated by a signal beam through a convex lens. A sharp magnified image of the aperture was found in the coincidence counting rate when a mobile detector was scanned in the transverse plane of the reference beam at a specific distance in relation to the lens. The experiment was named \u201cghost imaging\u201d due to its surprising nonlocal feature.","Additional experiments are reported in Pittman, et al. \u201cOptical Imaging by Means of Two-Photon Entanglement,\u201d Phys. Rev. A, Rapid Comm., Vol. 52, R3429 (1995) and ghost interference by Strekalov, et al, \u201cObservation of Two-Photon \u2018Ghost\u2019 Interference and Diffraction,\u201d Phys. Rev. Lett., Vol. 74, 3600 (1995), which together stimulated the foundation of quantum imaging in terms of multi-photon geometrical and physical optics.","The above publications are merely examples of the development and attempt to understand the science of quantum mechanics as it relates to photons. The present invention in effect uses similar principles and extensions thereof relating to quantum interactions.","The present invention is directed to a system for generating an image of a target utilizing entangled quantum particle pairs comprising:","at least one processor;","at least one source of entangled quantum particles configured to sequentially output pairs of entangled particles through first and second channels, the first channel being configured to output first pairs of entangled quantum particles, the second channel being configured to output second pairs of entangled quantum particles, the first and second pairs of entangled quantum particles being entangled;","a first beam splitter operatively connected to the first channel of the at least one source of entangled quantum particles; the first beam splitter configured to split the first pairs of entangled particles for entry into first and second spatial detectors;","at least one focusing device operatively connected to the second channel configured to direct the second pairs of entangled quantum particles towards a distant target;","the first and second spatial detectors being operatively connected to the at least one processor; each of the first and second spatial detectors detecting one particle of the first pairs of entangled quantum particles; the at least one processor operating to record the detection of entangled quantum particles by the first and second spatial detectors and create image data representing the target;","at least one display operatively connected to the at least one processor for displaying an image of the target.","Optionally, the system may further comprise:","a receiver operatively connected to the at least one processor configured to receive second entangled quantum pairs reflected by the target;","a second beam splitter operatively connected to the receiver, the second beam splitter having third and fourth outputs configured such that one particle from the second pairs of entangled particles are outputted from the third and fourth outputs;","third and fourth spatial detectors operatively connected to the third and fourth outputs and the at least one processor; the third and fourth spatial detectors operating to detect entangled quantum particles reflected by the target, the at least one processor operating to record the detection of entangled quantum particles by the third and fourth detectors, perform timing measurements and create image data of the target for display on the at least one display.","Optionally, the second pairs are either reflected or absorbed by the target and the effect of the reflection or absorption is transferred to the corresponding first pairs of entangled quantum particles through the properties of quantum entanglement. Optionally, the focusing device and the receiver are telescopes, and the at least one processor is operatively connected to the at least one source of entangled quantum particles.","The present invention is also directed to a method for generating an image of a target utilizing entangled quantum particle pairs comprising:","providing at least one source of entangled quantum particle pairs having first and channels; the at least one source of entangled quantum particle pairs outputting first and second pairs of entangled quantum particles, the first pairs of entangled quantum particles being entangled with the second pairs of entangled quantum particles;","outputting first pairs of entangled quantum particles through the first channel and into a first beam splitter;","outputting second pairs of entangled quantum particles through a second channel towards a target;","utilizing the first beam splitter, splitting the first pairs of entangled quantum particle pairs such that the first particle of each pair enters into a first spatial detector and the second particle of each pair enters into a second spatial detector;","detecting coincidences utilizing the first and second spatial detectors and at least one processor to create image data representing the target;","displaying an image of the target utilizing the image data.","Optionally, the method further comprises the steps of:","providing a receiver configured to receive second entangled quantum pairs reflected by the target operatively connected to the at least one processor;","providing a second beam splitter operatively connected to the receiver, the second beam splitter having third and fourth outputs configured such that one particle from the second pairs of entangled particles are outputted from the third and fourth outputs;","providing third and fourth spatial detectors operatively connected to the third and fourth outputs and the at least one processor;","detecting entangled quantum particles reflected by the target using the third and fourth spatial detectors,","using the at least one processor, recording the detection of entangled quantum particles by the third and fourth detectors, performing coincidence measurements and creating image data of the target for display based upon the coincidence measurements.","Optionally, the second pairs of entangled quantum particles are either reflected or absorbed by the target and the effect of the reflection or absorption is transferred to the corresponding first pairs of entangled quantum particles through the properties of quantum entanglement. Optionally, the second pairs of entangled quantum particles are directed towards a target using a telescope, the receiver is a telescope, and the at least one processor is operatively connected to the at least one source of entangled quantum particles.","These and other aspects of the embodiments herein will be better appreciated and understood when considered in conjunction with the following description and the accompanying drawings. It should be understood, however, that the following descriptions, while indicating preferred embodiments and numerous specific details thereof, are given by way of illustration and not of limitation. Many changes and modifications may be made within the scope of the embodiments herein without departing from the spirit thereof, and the embodiments herein include all such modifications.","A more complete appreciation of the invention will be readily obtained by reference to the following Description of the Preferred Embodiments and the accompanying drawings in which like numerals in different figures represent the same structures or elements. The representations in each of the figures are diagrammatic and no attempt is made to indicate actual scales or precise ratios. Proportional relationships are shown as approximates.","The embodiments herein and the various features and advantageous details thereof are explained more fully with reference to the non-limiting embodiments that are illustrated in the accompanying drawings and detailed in the following description. Descriptions of well-known components and processing techniques are omitted so as to not unnecessarily obscure the embodiments herein. The examples used herein are intended merely to facilitate an understanding of ways in which the embodiments herein may be practiced and to further enable those of skill in the art to practice the embodiments herein. Accordingly, the examples should not be construed as limiting the scope of the embodiments herein.","The terminology used herein is for the purpose of describing particular embodiments only and is not intended to limit the full scope of the invention. As used herein, the singular forms \u201ca\u201d, \u201can\u201d and \u201cthe\u201d are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms \u201ccomprises\u201d and\/or \u201ccomprising,\u201d when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and\/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and\/or groups thereof.","It will be understood that when an element is referred to as being \u201cconnected\u201d or \u201ccoupled\u201d to another element, it can be directly connected or coupled to the other element or intervening elements may be present. In contrast, when an element is referred to as being \u201cdirectly connected\u201d or \u201cdirectly coupled\u201d to another element, there are no intervening elements present.","It will be understood that, although the terms first, second, etc. may be used herein to describe various elements, components, regions, layers and\/or sections, these elements, components, regions, layers and\/or sections should not be limited by these terms. For example, when referring first and second photons in a photon pair, these terms are only used to distinguish one element, component, region, layer or section from another region, layer or section. Thus, a first element, component, region, layer or section discussed below could be termed a second element, component, region, layer or section without departing from the teachings of the present invention.","Furthermore, relative terms, such as \u201clower\u201d or \u201cbottom\u201d and \u201cupper\u201d or \u201ctop,\u201d may be used herein to describe one element's relationship to other elements as illustrated in the Figures. It will be understood that relative terms are intended to encompass different orientations of the device in addition to the orientation depicted in the Figures. For example, if the device in the Figures is turned over, elements described as being on the \u201clower\u201d side of other elements would then be oriented on \u201cupper\u201d sides of the other elements. The exemplary term \u201clower\u201d, can therefore, encompass both an orientation of \u201clower\u201d and \u201cupper,\u201d depending of the particular orientation of the figure. Similarly, if the device in one of the figures is turned over, elements described as \u201cbelow\u201d or \u201cbeneath\u201d other elements would then be oriented \u201cabove\u201d the other elements. The exemplary terms \u201cbelow\u201d or \u201cbeneath\u201d can, therefore, encompass both an orientation of above and below. Furthermore, the term \u201couter\u201d may be used to refer to a surface and\/or layer that is farthest away from a substrate.","Unless otherwise defined, all terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this invention belongs. It will be further understood that terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein.","It will also be appreciated by those of skill in the art that references to a structure or feature that is disposed \u201cadjacent\u201d another feature may have portions that overlap or underlie the adjacent feature.","Illustrated in  is a methodology for resolution improvement performed by the at least one processor comprises the following steps not necessarily in the order recited:\n\n","Note that the steps in  are comparable to the numbered boxes in , as denoted by the reference to \u201cBox_\u201d correlating to the Box number of .","Referring now to  in Box  a series or collection of high resolution measurements (or frames) of the illuminating light source (which may be, for example, the sun) are inputted into the memory or input of a processor or image processor. As used herein the terminology \u201cprocessor\u201d or \u201cimage processor\u201d as used in the following claims includes a computer, multiprocessor, CPU, minicomputer, microprocessor or any machine similar to a computer or processor which is capable of processing algorithms.","In Box , using the input from Box , the frame data or value of each pixel at each pixel location is determined for each frame. In Box , the pixel values in the low resolution set of measurements Pis determined. The low resolution frames may comprise photographs of the same region of interest. The region of interest may be a scene, landscape, an object, a subject, person, or thing. Where the low resolution source is a low resolution camera, the value of a pixel correlates to a \u201cbucket value\u201d determination and correlates to the light intensity measured by the detector. In the case of an electronic display formed by pixels, the intensity of each pixel value at each pixel location Pis determined. At Box , the values in Box  are multiplied by the values determined in Box . Box  represents the Frame Data\u00d7PProduct. Inasmuch as the Boxes  and  are arrays of pixel values, the Box  Product is also an array of values. At Box , the products of Box  are repeatedly calculated for each frame in a selected plurality of frames and summed together. As an example, one hundred frames may be selected. At Box , the summation Box  (Products for the determined in Box ) is divided by the number of frames (such as for example one hundred) to determine the Frame Data\u00d7PProducts Average for the plurality of frames. The Product Average in Box  is an array containing pixel values at each pixel location within the frame.",{"@attributes":{"id":"p-0101","num":"0108"},"figref":["FIG. 3","FIG. 2","FIG. 2","FIG. 3"],"b":["7","8","9","9","9","9"],"sub":["ij ","ij ","ij ","ij ","ij"]},"Box  represents the multiplication of Boxes  and A to form the Average Frame Data\u00d7Average PProduct (Box ), which is an array. As shown in the bottom portion of , the Average Frame Data\u00d7Average PProduct is subtracted from the Frame Data\u00d7PProducts Average to form the intermediate Gij Image of Box . In Box  the sum of the intermediate Gij Images for the frames 1\u2212N is calculated to produce the final composite image.","In accordance with the method described, multiple photo-sensor buckets scanned or in an array and high resolution images of the illuminating light source. Depending on the object and the light source that it is scattering and reflecting it is possible that light is scattering or reflecting from any location on the subject such that any or all of the photo-sensors in the array has a probability of measuring one or more photons of light. A low resolution camera can be used as the multiple photo-sensor bucket array. A higher resolution Gimage of the target can be produced using high-resolution images of the illumination source coupled with information from the multiple photo-sensor bucket array. Use of the multiple photo-sensor bucket array can improve the convergence rate of the high resolution Gimage. Each photo-sensor in the multiple photo-sensor bucket array may measure light scattered and reflected from distinct portions of the target object with appropriate optics that images the subject onto the multiple photo-sensor bucket array.","A concept is that if the nonspatial information resolving single-pixel \u201cbucket\u201d detector that measures light from the target object that is typically used for Gimaging was replaced with a low resolution spatial information resolving device such as a Charge Coupled Device (CCD) camera and the detector that measures the light from the source of illumination is a high resolution spatial information resolving device, i.e., a high resolution CCD, then one could use the techniques of Gimaging to generate an image that would be at the resolution and quality of the high-resolution device using the extra information measured by the low-resolution target object detector. This may be accomplished by treating each nonspatial information resolving pixel of the low-resolution detector as a separate \u201cbucket\u201d measurement to create a Gimage. The generation of Gimages is performed over the entire set of pixels of the low resolution camera and each low-resolution Gimage is accumulated into a composite Gimage that provides the final result. It should be noted that prior to generating a low-resolution pixel Gimage, the low-resolution pixel value can be tested to determine by some metric if a Gimage should be computed using that low-resolution pixel, i.e., an option includes not computing a Gimage if all the values at that low resolution pixel are 0 or below some threshold value.","A single bucket detector may be used to scan over different areas of a target. At each step of the scan a Gimage would be computed and accumulated into a composite Gimage for all positions that the detector scanned.","A third embodiment utilizes the concept occurring when a set of random or pseudo-random pixels of the multiple photo-sensor bucket array measure light from the target subject. This random set of pixels may be different from one measurement time to another measurement time and these sets of random bucket pixels are used to compute sets of Gimages that are accumulated into a higher resolution and higher quality composite Gimage.","It should be noted that the calculation of the Gimage may be accomplished by using optimization methods such as Compressive Sensing techniques.",{"@attributes":{"id":"p-0108","num":"0115"},"figref":["FIG. 4","FIG. 4"],"b":["23","22","24","29"]},"Shown in  is a laser  that sends light through a thermalizing element  which creates a light pattern. A beam splitter  is used to split the beam from the laser  into a target path A and a reference path B. The pattern of the beam is recorded by a charge coupled device (CCD)  or the like which records spatial information concerning the light pattern as discussed more fully in U.S. application Ser. No. 12\/330,401, hereby incorporated by reference. In its simplest terms, a charge coupled device (CCD) is a device for the movement of electrical charge from the detector area () to an area where the charge can be manipulated, for example conversion into a digital value. CCDs may be implemented as shift registers that move charge between capacitive bins in the device. The CCD device may be made up of semiconductors arranged in such a way that the electric charge output of one semiconductor charges an adjacent one. The CCD device may be integrated with an image sensor, such as a photoelectric device to produce the charge that is being read for digital imaging. The CCD device  may optionally be a camera, photodetector array or a photographic device capable of imaging the beam pattern B. The beam pattern comprising the spatial information concerning the light beam B is sent to computer . Light Beam A is directed to the target  and the returned and scattered light is collected by a first detector or sensor . Detector  may be a plurality of bucket detectors, or any kind of detectors which have the capability of detecting a photon strike. Detectors  may be of a large variety of photo detectors well known to those of ordinary skill in the art. A feature of the embodiments of  is that the detectors , individually, need not record spatial information regarding the target . However, cumulatively, spatial information is derived; although at low resolution. The spatial information derived by spatial detector  is transmitted to the computer  which combines and correlates this spatial information with the coincidence data received from detectors . For example, the data recorded by detectors  may be transmitted to computer  in a form resembling that depicted in , for example, where roughly 16 \u201cbucket\u201d measurements are represented.","The spatial information from detector  is combined with the coincidence information from the detectors  in computer . Computer  may be a microprocessor, processor, multiprocessor, CPU, mainframe, or any computing element capable of combining the spatial information from the detector  with the coincidence information from detectors . Further description of the coincidence detection feature is found in U.S. Pat. Nos. 7,536,012 and 7,812,303, both of which are hereby incorporated by reference. Since the photonic information detected by each particular detector  need not encompass spatial information, but simply indicate the occurrence of a photon returned from the target , this capability facilitates the use of the embodiment systems in environments in which the returned photons may be impacted by environmental conditions such as fog, smoke, atmospheric particles and the like.","A quantum photon mathematical equation will project the reference light intensity from the CCD . This will be combined with \u201cbucket\u201d photon information (such as that exemplified in ) for an ensemble of pulses to produce coincidence measurements needed for \u201cghost\u201d imaging. The terminology \u201cghost\u201d relates to the feature that the spatial information is not reflected from the target but is either derived from the modulation of the laser beam (not shown) or the spatial information obtained through the use of beam splitter  and detector  which records spatial information from a beam which has not \u201cseen\u201d or illuminated the target.",{"@attributes":{"id":"p-0112","num":"0119"},"figref":["FIG. 5","FIG. 4","FIG. 5","FIG. 4"],"b":["21","21","23","24","21","29","23","29","29","21","23"]},{"@attributes":{"id":"p-0113","num":"0120"},"figref":["FIG. 6","FIG. 6","FIG. 6","FIG. 3"],"b":["29","24","12","13"],"sup":["(2) ","(2) ","(2) "]},"By way of example,  correlates to the image as produced using the methodology described in FIG. 5A of U.S. Pat. No. 7,812,303 and  correlates to an image produced through to the processing stage as depicted at Box  of .","As further examples of image processing,  is an image produced as represented at Box  and  correlates to the image produced when the process reaches Box .","As further examples of image processing,  is a low resolution image correlating to Box  of  where blocks of 2\u00d72 pixels are processed together.  is 1 by 1 a spatially averaged set of pixels of the target subject which correlates to an equivalent high resolution image.",{"@attributes":{"id":"p-0117","num":"0124"},"figref":["FIGS. 10A and 10B","FIG. 3","FIG. 10A","FIG. 10B"],"b":"12"},{"@attributes":{"id":"p-0118","num":"0125"},"figref":["FIGS. 11A and 11B","FIG. 3","FIG. 11A","FIG. 11B"],"b":["9","13"]},{"@attributes":{"id":"p-0119","num":"0126"},"figref":["FIGS. 12A and 12B","FIG. 3","FIG. 12A","FIG. 12B"],"b":"12"},{"@attributes":{"id":"p-0120","num":"0127"},"figref":["FIGS. 13A and 13B","FIG. 3","FIG. 13A","FIG. 13B"],"b":["9","13"]},{"@attributes":{"id":"p-0121","num":"0128"},"figref":["FIGS. 14A and 14B","FIG. 3","FIG. 14A","FIG. 14B"],"b":"12"},{"@attributes":{"id":"p-0122","num":"0129"},"figref":["FIGS. 15A and 15B","FIG. 3","FIG. 15A","FIG. 15B"],"b":["9","13"]},{"@attributes":{"id":"p-0123","num":"0130"},"figref":["FIGS. 16A and 16B","FIG. 3","FIG. 16A","FIG. 16B"],"b":"13"},{"@attributes":{"id":"p-0124","num":"0131"},"figref":["FIGS. 17 through 19","FIG. 3"],"b":"13"},{"@attributes":{"id":"p-0125","num":"0132"},"figref":"FIG. 17A"},{"@attributes":{"id":"p-0126","num":"0133"},"figref":"FIG. 17B"},{"@attributes":{"id":"p-0127","num":"0134"},"figref":"FIG. 18A"},{"@attributes":{"id":"p-0128","num":"0135"},"figref":"FIG. 18B"},{"@attributes":{"id":"p-0129","num":"0136"},"figref":"FIG. 19A"},{"@attributes":{"id":"p-0130","num":"0137"},"figref":["FIG. 19B","FIG. 20","FIGS. 20-31"],"b":"301"},"The region of interest may be a scene, landscape, an object, a subject, person, or thing. In Box , the low resolution Average Frame Data is determined by computing the average value of each pixel at each pixel location for the plurality of frames. In Box  the low resolution Deviation Frame Data is determined by subtracting the low resolution Average Frame Data from the low resolution Frame Data for all pixels in each frame for the plurality of frames.","The method proceeds to . On a quantum level, to get the improved image, the issue is what high resolution light measured from the illuminating source contributed to the low resolution light measured from the target in a quantative manner. In Box , a series of high resolution frames are entered into the memory or input of a processor or image processor. The frames may comprise photographs of the illuminating source of the region of interest. For example, in a surveillance application, the central station may measure high resolution measurements of the sun, while many low resolution images are taken by distributed cameras in distant areas. Combining the low resolution image data in accordance with the procedures detailed herein results in a higher resolution image from each low resolution camera. The region of interest may be a scene, landscape, an object, a subject, person, or thing. In Box  the high resolution Average Frame Data is determined by computing the average value of each pixel at each pixel location for the plurality of frames of the source of illumination. In Box  the high resolution Deviation Frame Data is determined by subtracting the high resolution Average Frame Data from the high resolution frame data for all pixels (i.e., value of each pixel) in each frame for the plurality of frames.","The method proceeds to  which shows how to generate a third set of data which is referred to here at SET 3. SET 3 data includes conditional product terms using low resolution pixels having a positive intensity deviation and high resolution positive deviation frame data pixels. SET 3 may be determined as follows: In Box  low resolution pixels with a Positive Intensity Deviation multiply the high resolution Positive Frame Data Deviation pixels within that set of frames. Positive intensity deviation is defined occurring when the deviation is greater than zero at the pixel location. In Box , for each pixel location, the processor calculates (a) the square of the low resolution Positive Intensity Deviation, (b) the product of the low resolution Positive Intensity Deviation\u00d7the high resolution Positive Deviation Frame Data pixels, (c) the square of the low resolution Positive Deviation Frame Data pixels are recorded and accumulated and (d) the square of the high resolution Positive Deviation Frame Data pixels are recorded and accumulated. In part (a) each time a pixel position is used, it is recorded and accumulated. The number of times the pixel is used is recorded, which will be used to compute the averages. As an example of the square of the low resolution Positive Intensity Deviation, if the low resolution pixel value is 8, the average pixel value is 6, the square of the low resolution pixel positive intensity deviation is 2\u00d72 or 4. If the high resolution pixel value is 20, the average pixel value is 16, the square of the high resolution pixel positive intensity deviation is 4\u00d74 or 16. In this example of the product of the low resolution Positive Intensity Deviation\u00d7the high resolution Positive Deviation Frame Data pixels would 8. These products are computed for each pixel in each frame for the low resolution and high resolution images, respectively. Thus, there is an array for each product computed in Box . In Boxes A-C, the pre-normalized Positive-Positive Product pixel values (Box A), are determined by dividing the product of the low resolution Positive Intensity Deviation pixels\u00d7the high resolution Positive Deviation Frame Data pixels by the number of times the pixel is used for the calculation of the positive low resolution deviation times the positive high resolution deviation values (the product). The average of the squares of the low resolution Positive Intensity is determined by dividing by the number of times the pixel was used for the calculation of the product (Box B). The average of the squares of the high resultion Positive Deviation Frame Data pixels is determined by dividing by the number of times the pixel was used for the calculation of the product (Box C).","Continuing to , the method proceeds to Box . In Box  the standard deviation of the low resolution Positive Intensity Deviation is determined by taking the square root of the average of the squares of the low resolution Positive Intensity Deviations (determined in Box B). In Box  the standard deviations of the high resolution Positive Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the high resolution Positive Deviation Frame Data pixels (determined in Box C). Box  determines the Normalized Positive-Positive Product pixel values by dividing the pre-Normalized Positive-Positive Product (Box A), by the product of the standard deviation of the low resolution Positive Intensity Deviations (Box ), and the standard deviation of the high resolution Positive Deviation Frame Data pixels (Box ).","The method proceeds via path  to  which shows how to generate a fourth set of data which is referred to here at SET 4. SET 4 frame data includes conditional product terms using frames having a low resolution negative intensity deviations and high resolution negative deviation frame data pixels. SET 4 may be determined as follows: In Box  pixels with a low resolution Negative Intensity Deviation multiply the value of the low resolution Intensity Deviation by the high resolution Negative Frame Data Deviation pixels within that set of frames. In Box , for each pixel location, the processor calculates (a) the square of the low resolution Negative Intensity Deviation, (b) the product of the low resolution Negative Intensity Deviation\u00d7the high resolution Negative Deviation Frame Data pixels, (c) the square of the low resolution Negative Deviation Frame Data pixels are recorded and accumulated and (d) the square of the high resolution Negative Deviation Frame Data pixels are recorded and accumulated. In Boxes  A-C the pre-normalized Negative-Negative Product pixel values (Box A) are determined by dividing the product of the low resolution Negative Intensity Deviations\u00d7the high resolution Negative Deviation Frame Data pixels by the result of (). In Box B, the average of the squares of the low resolution Negative Intensity is determined by dividing () by (). In Box C, the average of the squares of the high resolution Negative Deviation Frame Data pixels is determined by dividing () by ().","From Box  in , the method can proceed via path  to . In Box  the standard deviation of the low resolution Negative Intensity Deviation is determined by taking the square root of the average of the squares of the low resolution Negative Intensity Deviation (determined in Box A). In Box  the standard deviations of the high resolution Negative Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the high resolution Negative Deviation Frame Data pixels (determined in Box B. Box  determines the Normalized Negative-Negative Product pixel values by dividing the pre-Normalized Negative-Negative Product, (determined in Box A), by the product of the standard deviation of the low resolution Negative Intensity Deviation (determined by Box ), and the standard deviation of the high resolution Negative Deviation Frame Data pixels (determined by Box ). At the conclusion of Box , the method proceeds via path  to  for determining an improved image data , or to  for determining alternative improved image data  by an alternative embodiment.","Returning to , the method also can concurrently proceeds via path  to  which shows how to generate a fifth set of data which is referred to here at SET 5. SET 5 frame data includes conditional product terms using pixels having a low resolution negative intensity deviation and high resolution positive deviation frame data pixels. SET 5 may be determined as follows: In Box  pixels with a low resolution Negative Intensity Deviation multiply the values of the low resolution Intensity Deviation by the high resolution Positive Frame Data Deviation pixels within that set of frames. In Box , the processor records and accumulates (a) pixel locations, (b) the square of the low resolution Negative Intensity Deviations, (c) the product of the low resolution Negative Intensity Deviations\u00d7the high resolution Positive Deviation Frame Data pixels and (d) the square of the high resolution Positive Deviation Frame Data pixels are recorded and accumulated. In Box  A-C, the pre-normalized Positive-Negative Product pixel values (Box A), are determined by dividing the product of the low resolution Negative Intensity Deviations\u00d7the high resolution Positive Deviation Frame Data pixels by the result of Box A. The average of the squares of the low resolution Negative Intensity (Box B) is determined by dividing the result of Box B by the result of Box A. The average of the squares of the high resolution Positive Deviation Frame Data pixels (Box C) is determined by dividing () by ().","From Box  in , the method can proceed via path  to . In Box  the standard deviation of the low resolution Negative Intensity Deviations is determined by taking the square root of the average of the squares of the low resolution Negative Intensity Deviations determined in Box B. In Box  the standard deviations of the high resolution Positive Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the high resolution Positive Deviation Frame Data pixels, determined in Box C. Box  determines the Normalized Positive-Negative Product pixel values by dividing the pre-Normalized Positive-Negative Product determined in Box A, by the product of the standard deviation of the low resolution Negative Intensity Deviations (determined Box ), and the standard deviation of the high resolution Positive Deviation Frame Data pixels (determined in Box ).","At the conclusion of Box  in , the method proceeds via path  to  for determining an improved image data , or to  for determining alternative improved image data  by an alternative embodiment.","Similar as discussed above with respect to the fifth set of data (SET 5), returning to , the method also can concurrently proceed via path  to  which shows how to generate a sixth set of data which is referred to here at SET 6. SET 6 frame data includes conditional product terms using frames having a low resolution positive intensity deviation and a high resolution negative deviation frame data pixels. SET 6 may be determined as follows: In Box  pixels with a low resolution Positive Intensity Deviation multiply the value of the low resolution Intensity Deviation by the high resolution Negative Frame Data Deviation pixels within that set of frames. In Box , (a) the pixel locations are recorded and accumulated, (b) the square of the low resolution Positive Intensity Deviation is determined, recorded and accumulated, (c) the product of the low resolution Positive Intensity Deviations\u00d7the high resolution Negative Deviation Frame Data pixels is determined, recorded and accumulated, and (d) the square of the high resolution Negative Deviation Frame Data pixels are determined, recorded and accumulated. In Box  A, the pre-normalized Negative-Positive Product pixel values are determined by dividing the product of the low resolution Positive Intensity Deviations\u00d7the high resolution Negative Deviation Frame Data pixels by determined in Box (). In Box B, the average of the squares of the low resolution Positive Intensity is determined by dividing the result of Box () by the result of Box (). In Box C, the average of the squares of the high resolution Negative Deviation Frame Data pixels is determined by dividing the result of Box () by the result of Box ().","From Box  in , the method can proceed via path  to . In Box  the standard deviation of the low resolution Positive Intensity Deviation is determined by taking the square root of the average of the squares of the low resolution Positive Intensity Deviations (determined in Box A). In Box  the standard deviations of the high resolution Negative Deviation Frame Data pixels is computed by taking the square root of the average of the squares of the high resolution Negative Deviation Frame Data pixels (determined in Box B). Box  determines the Normalized Negative-Positive Product pixel values by dividing the pre-Normalized Negative-Positive Product, (determined in Box A), by the product of the standard deviation of the low resolution Positive Intensity Deviations (Box ), and the standard deviation of the high resolution Negative Deviation Frame Data pixels (Box ).","At the conclusion of Box  in , the method proceeds via path  to  for determining an improved image data , or to  for determining alternative improved image data  by an alternative embodiment.",{"@attributes":{"id":"p-0143","num":"0150"},"figref":["FIG. 30","FIG. 30"],"b":["701","323","405","505","605"]},{"@attributes":{"id":"p-0144","num":"0151"},"figref":["FIG. 31","FIG. 31"],"b":["702","323","405","805","605"]},"With respect to , SETS 3-6 are arbitrarily named, In fact, SETS 3-6 could have renumbered SETS 1-4 corresponding to first, second, third and fourth data sets, but the invention could be accomplished by using a lesser number of sets or different combinations of an equal or lesser number of sets.",{"@attributes":{"id":"p-0146","num":"0153"},"figref":["FIG. 32","FIG. 34","FIG. 32","FIG. 34"],"b":["7906","8001","8002","8003","8004","8003","8005","8003","4904","02","8006","4907","4909","4910"]},{"@attributes":{"id":"p-0147","num":"0154"},"figref":["FIG. 33","FIG. 34","FIG. 34"],"b":["7906","8001","8002","8001","8100","8100","8003","8003","8003","8003","8003","8003","8001","8004","8003","8005","8003","4904","02","8006","4907","4909","4910","8100","8003","8001"],"sub":["1 ","2","2","1","1 ","2","1 ","2 ","1 ","2","1 ","2","1","2","p ","p","m"]},{"@attributes":{"id":"p-0148","num":"0155"},"figref":"FIG. 34","b":["4901","4902","4903","4904","4904","4904","4904","4904"]},"Entangled photon source  is an entangled photon source such as of the types illustrated by . The entangled photon source generates entangled photon pairs that are entangled in time-energy, H-V polarization or between other conjugate pair properties of the photons. Element , may be for example, a polarizing beam splitter, dichroic-mirror or other optical element that operates to direct one portion of an entangled photon pair towards spatially resolving detector  and directs the remaining portion of an entangled photon pair toward spatially resolving detector . Focusing elements  may be lenses used to focus the photons onto detector  and detector . Spatially resolving detector  measures the time and spatial (x, y) location of one part of an entangled pair that has interacted with the remote scene, target or subject. Spatially resolving detector  measures time and spatial (x, y) location of the second part of an entangled pair that has interacted with the remote scene, target or subject. Box B represents coincidence and timing electronics that operates to register when a pixel on detector  and a pixel on detector  occur inside within a user defined coincidence window \u0394T. A coincidence window is a time difference within which two photon measurements are defined to be co-incident. The timing electronics further operate to record the time that has elapsed since a user chosen laser pulse and the first coincidence pair detection for ranging calculations. Box  indicates a processor, memory, and algorithms to generate enhanced average second-order images of the region of interest. Memory A, associated with processor , stores input images, algorithms, intermediate computations, and enhanced second order images of the region of interest. Box  represents software operationally configured to perform the image improvement and enhancement processes. Display A is operatively connected to processor  for displaying the generated enhanced second-order or higher order image of the region of interest. Box  represents optionally-present pointing and aiming (e.g., beam steering) components that may be used to direct the entangled photon pairs to a specific point within the region of interest.","A second set of entangled photon pairs generated by swapped entangled photon source  are directed towards element A, shown in the bottom left corner of . The entangled photons from element  of  are directed towards element A via delay line element  and the corresponding set of entangled photons from element  of  are directed towards element A through beam steering or pointing element . Element A is similar to  and directs portions of the entering entangled photon pairs towards measurement devices  and . Coincidence measurements of the entangled photons directed to element A are used to generate a reflection image of the target  with information that is provided by the shared entanglement properties of the entangled photons that were directed from swapped entangled photon source  to focusing device or telescope A. It is to be appreciated that this invention will operate to generate an improved image of the target where the target may be partially absorbing and\/or partially reflecting. As state above, the entangled photons from element  of  are directed towards element A via delay line element  and the corresponding set of entangled photons from element  of  are directed towards element A through beam steering or pointing element . Displays  will display an image created by entangled photons reflected by the target  and display A will display an image created by entangled photons that never directly interacted with the target , but due to the properties of entanglement display an image of created by corresponding entangled photons (a) absorbed by target , (b) reflected by the target  due to the properties of entangled photon pairs.","As described above, generally speaking, the progression of image stages in  are represented in the presentation and comparison of images in  in order to demonstrate that high resolution images are producible from low resolution images using the above principles.","At least some of the embodiments described above may be based upon the calculation of a G\u201cghost\u201d image for each bucket detector over the set of all bucket detectors that comprise the low resolution target object detector. Each of the intermediate Gimages are summed into a final composite image of the target object","A \u201cghost\u201d or Gimage as used herein may be mathematically expressed as a result of a convolution between the aperture function (amplitude distribution function) of the object A({right arrow over (\u03c1)}) and a \u03b4-function like second-order correlation function G({right arrow over (\u03c1)}, {right arrow over (\u03c1)})\n\n({right arrow over (\u03c1)})=\u222b({right arrow over (\u03c1)})({right arrow over (\u03c1)},{right arrow over (\u03c1)}),\u2003\u2003(1)\n\nwhere G({right arrow over (\u03c1)}, {right arrow over (\u03c1)})\u2245\u03b4({right arrow over (\u03c1)}\u2212{right arrow over (\u03c1)}\/m), {right arrow over (\u03c1)}and {right arrow over (\u03c1)}are 2D vectors of the transverse coordinate in the object plane and the image plane, respectively, and m is the magnification factor. The term \u03b4 function as used herein relates to the Dirac delta function which is a mathematical construct representing an infinitely sharp peak bounding unit area expressed as \u03b4(x), that has the value zero everywhere except at x=0 where its value is infinitely large in such a way that its total integral is 1. The \u03b4 function characterizes the perfect point-to-point relationship between the object plane and the image plane. If the image comes with a constant background, as in this experiment, the second-order correlation function G({right arrow over (\u03c1)}, {right arrow over (\u03c1)}) in Eq. (1) must be composed of two parts\n\n({right arrow over (\u03c1)},{right arrow over (\u03c1)})=+\u03b4({right arrow over (\u03c1)}\u2212{right arrow over (\u03c1)}),\u2003\u2003(2)\n\nwhere Gis a constant. The value of Gdetermines the visibility of the image. One may immediately connect Eq. (2) with the Gfunction of thermal radiation)\n\n|,\u2003\u2003(3)\n\nwhere GG\u02dcGis a constant, and |G|\u02dc\u03b4({right arrow over (\u03c1)}\u2212{right arrow over (\u03c1)}) represents a nonlocal position-to-position correlation. Although the second-order correlation function Gis formally written in terms of Gs as shown in equation (3), the physics are completely different. As we know, Gis usually measured by one photodetector representing the first-order coherence of the field, i.e., the ability of observing first-order interference. Here, in equation (3), Gmay be measured by independent photodetectors at distant space-time points and represents a nonlocal EPR correlation.\n","When using a bucket detector, the calculation of a Gghost image for each bucket detector over the set of all bucket detectors that comprise the low resolution target object detector is used as a basis. Each of the intermediate Gimages are summed into a final composite image of the target object\n\nImage\u2003\u2003(4)\n\nwhere \u03a3 indicates a summation operation. Similarly, when using Compressive Sensing (CS) techniques the R term of Eq. (9) is computed for each bucket for an intermediate image and these intermediate images are then summed as show in Eq. (4) to produce a final image of the target object.\n","Typically ghost imaging uses two detectors, one to observe the light source and the other, single pixel or bucket detector, to observe the light scattering and reflecting from the target object.\n\n()()()()\u2003\u2003(5)\n\nwhere   denotes an ensemble average. As used herein, and terminology \u201cbucket\u201d in general means a single pixel detector, a detector comprising a plurality or grouping of pixels, a low resolution imaging device, a low resolution scanning detector (or device) or the like. The terminology I(t)means the measurements taken from a single pixel detector, a detector comprising a plurality or grouping of pixels, a low resolution imaging device, a low resolution scanning detector (or device) or the like.\n","A relatively new mathematical field named Compressive Sensing (CS) or Compressive Imaging (CI) can be used to good effect within the context of Gimaging. The use of compressive techniques in the context of Ghost Imaging was performed by the Katz group (see O. Katz, et al., \u201cCompressive Ghost Imaging,\u201d Appl. Phys. Lett., 95, 131110 (2009))(hereby incorporated by reference) who demonstrated a ghost like imaging proposal of Shapiro (see J. Shapiro, \u201cComputational Ghost Imaging,\u201d Phys. Rev. A 78 061802 (R) (2008)) (hereby incorporated by reference).","The use of CS and Compressive Imaging (CI) herein is based on finding approximate solutions to the integral equations using the Gradient Projection for Sparse Reconstruction (GPSR) mathematical methodology where\n\n\u2003\u2003(6)\n\nand\n\n()\u2003\u2003(7)\n\nis the object reflectance. The term J is a matrix, where the rows are the illumination patterns at time k and the B vector:\n\n]\u2003\u2003(8)\n\nrepresents the bucket values. In cases where the system is underdetermined (too few [B]), then Lconstraints are applied to complete the system and sparseness is used:\n",{"@attributes":{"id":"p-0158","num":"0165"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"arg","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"munder":{"mi":["min","R"]}},{"mrow":[{"mfrac":{"mn":["1","2"]},"mo":"\u2062","msubsup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["B","JR"],"mo":"-"}},"mn":["2","2"]}},{"mi":"\u03c4","mo":"\u2062","msub":{"mrow":{"mo":["\uf605","\uf606"],"mi":"R"},"mn":"1"}}],"mo":"+"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}}},"The CS computational strategy takes advantage of the fact that it is normally true in images that not all pixels in an image contain new information and the system is said to be sparse on some basis since fewer degrees of freedom are needed to describe the system than the total number of pixels in the image. The parameter \u03c4 in Eq. (9) is often a constant.","The problem is then solvable using, for example, an L1 minimization as described further in \u201cCompressed Sensing, IEEE Transactions on Information Theory, Vol. 52, NO. 4, APRIL 2006\u201d and \u201cGradient Projection for Sparse Reconstruction: Application to Compressed Sensing and Other Inverse Problems, IEEE J. Sel. Top. in Sig., Proc. 1, 586 2007.\u201d (both of which are hereby incorporated by reference).","In another embodiment, when illuminating light source comprises entangled photons, in order to resolve the issue of multiple counts (i.e., counts originating from background photons), the following sequence is preformed:","providing a high speed time stamped series of low resolution frames of a given region of interest from the array of pixel locations;","providing a high speed time stamped series of high resolution images of a light source from the spatial detector;","the high speed time stamped frames being such that there is only one photon counted (or measured) on a pixel in both the low resolution frame and the high resolution frame per unit time (if more than one pixel is counted, the frame of pixels is discarded\u2014This is to ensure proper discrimination of entangled photons from background light.)","determining the value of each pixel at each location per unit time within each high resolution frame to form first arrays of pixel values;","determining the value of each pixel at each location per unit time within each low resolution frame to form a second array of pixel values;","for each low resolution pixel location in the second array:\n\n","Applications of the embodiments described herein improve the ability to image through obscuring media (e.g., smoke or clouds), which remains a problem in a variety of fields, such as satellite imaging analysts, firefighters, drivers, oceanographers, astronomers, military personnel, and medical personnel. The embodiments described improve the ability to improve resolution in each of these exemplary instances and represents an opportunity to derive more information from images and presumably the decisions made from such images. By way of example, improved resolution in x-ray or endoscopy medical imagery facilitates lower radiation dosing and diagnosis of abnormal morphologies earlier than currently possible with conventional imaging methodologies. Conventional imaging techniques have, to a large extent, arrived at the theoretical limits of image resolution owing to wavelength-limited resolution, optical element distortions, and the reflective interaction between photons and an object to be imaged.","As used herein the terminology Gtechnique means where you have two measurements where the actual image is the convolution of the object function\n\n({right arrow over (\u03c1)})=\u222b({right arrow over (\u03c1)})({right arrow over (\u03c1)},{right arrow over (\u03c1)}),\n\nwhere the Object function A is convolved with the correlations between two spatially distinct detections.\n","This application provides further improvements over the embodiments discussed in application Ser. No. 13\/477,890. One key difference is that the improved embodiments provide a method to partition the values in the measured data sets, i.e. frames, into two or more groups for the high resolution frame data (reference fields) of the illuminating source and overall frame intensities (bucket values) or low resolution frame data of the region of interest. These groups are then used to compute products, or cross-correlations, between the different groupings. These individual product terms can be mathematically combined, via addition and\/or subtraction processes, to generate improve images of the target or scene. This method further adapts the techniques presented in application Ser. No. 13\/477,890. For example, in application Ser. No. 13\/477,890, each pixel of the low resolution image operated as a \u201cbucket\u201d measurement and intermediate improved images were computed for each low resolution pixel before being added together to provide a final improved image. Deviations of the high resolution reference images and the deviations of the low resolution pixels are operated with to compute intermediate improved images that are then summed to provide a final improved image of the subject. One key advantage to this method is that it is possible with the following embodiments to generate all positive valued images and largely eliminate background and noise effects. Other advantages include the ability to operate on a computed partitioned image using functions such as logarithms and exponentials to further increase contrast or better identify objects and information on their properties.","The following embodiments are predicated on the appreciation that other operations involving the partitioned sets of above average and below average measurements are beneficial to improve image quality in adverse conditions such as turbulence. These operations would include but are not limited to cross-correlations between above average bucket (overall frame intensities) or above average low resolution frame data and below average reference fields. Typically four correlation types are available when data is partitioned into two distinct sets such as above the average and below the average values. In a non-normalized form this can be written as",{"@attributes":{"id":"p-0172","num":"0180"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["R","m"]},"mo":"=","mrow":{"mfrac":{"mn":"1","msub":{"mi":["N","M"]}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mn":"1","msub":{"mi":["N","M"]}},"mo":"\u2062","mrow":{"msub":[{"mi":["I","a"]},{"mi":["I","b"]}],"mo":"\u2062"}}}}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":{},"sub":["m ","a ","b","M ","m ","a ","b"]},{"@attributes":{"id":"p-0173","num":"0181"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["RN","m"]},"mo":"=","mfrac":{"msub":{"mi":["R","m"]},"mrow":{"msub":[{"mi":"\u03c3","msub":{"mi":["I","a"]}},{"mi":"\u03c3","msub":{"mi":["I","b"]}}],"mo":"\u2062"}}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}}}}},"The data sets Iand Iare used in the preferred embodiments of the current invention as the deviations from the average or mean. I=(M\u2212<M>) where the M's indicate the measurement, either an individual pixel value, a low resolution pixel value, or a bucket value and the < > indicates an average over the ensemble of the measurements.","The product terms that comprise a particular Rare computed conditionally. The Rcan be called conditional product terms. For instance, Rmay be computed for the set of pixel values Ithat are above the mean for those pixels of data with low resolution pixel values Ithat are above the mean. For example:",{"@attributes":{"id":"p-0176","num":"0184"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":["R","m"],"mo":"++"},"mo":"=","mrow":{"mfrac":{"mn":"1","msubsup":{"mi":["N","m"],"mo":"+"}},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mn":"1","msubsup":{"mi":["N","m"],"mo":"+"}},"mo":"\u2062","mrow":{"msubsup":[{"mi":["I","a"],"mo":"+"},{"mi":["I","b"],"mo":"+"}],"mo":"\u2062"}}}}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}}},"The other combinations of above\/below mean high resolution pixel values and above\/below mean low resolution pixel values are computed similarly. These conditional Rmay then be added or subtracted from each other to yield improved images of a scene or target. Furthermore, it should be noted that the particular combination of the Rbelow\n\nR+R\u2212R\u2212R\u2003\u2003(11)\n","A second alterative embodiment may include computing a Gimproved image. This improved image is then partitioned into pixels that are, for example, above the spatial mean Gand pixels that are below the spatial mean G. These alternative positive\/negative Gpartitioned improved images can display higher contrast and can be further operated upon by using mathematical operations such as logarithms to increase the dynamic range. It is to be recognized that other partitions are possible to tailor results needed for specific applications.","A third embodiment may include computing a Rcorrelation image by calculating the correlation coefficient between the Iand Ipartitions where the Ia and Ib are not aligned in time or frame. For instance, at a particular pixel i,j there may be 10 frames in which that pixel is above the mean value of that pixel for all frames, and there may only be 5 frames for which the Ivalues is above the mean of I. A correlation coefficient may be computed between these two sets of data using:",{"@attributes":{"id":"p-0180","num":"0188"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"R","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}}},"mo":"=","mfrac":{"mrow":{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["i","j"],"mo":","}}},"msqrt":{"mrow":{"mrow":[{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["i","i"],"mo":","}}},{"mi":"C","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["j","j"],"mo":","}}}],"mo":"\u2062"}}}}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}},"br":{}},"As an example, for imaging measurements of pixel values and bucket values that have been grouped into two sets each of above and below their respective averages, there is a total of 80 possible ways to present for output the results of the computed conditional product terms. For instance, each conditional product term may be presented for display individually with either a positive or negative sign. Thus, individually for the four conditional product terms there is a total of 8 ways to present them for display. Combinations of two conditional product terms with signs allows for 24 options to present for display, combinations of three conditional product terms allows for 32, and combinations of all four conditional product terms allows for 16 ways to present for output and display.","Entanglement swapping is a quantum process by which particles that are not entangled become entangled with each other. For example, consider that particles 1 (P1) and 2 (P2) are entangled with each other and that particles 3 (P3) and 4 (P4) are entangled with each other. To entangle P1 and P4, particles P2 and P3 are interfered on, for example, a beam splitter and then are measured. The interference and measurement swaps the entanglements P1-P2 and P3-P4 to P1-P4. Particles P2 and P3 are also affected by the measurement device and may be absorbed. The process of entanglement swapping has previously been verified. See, e.g., Jian-Wei Pan, Dik Bouwmeester, et al. \u201cExperimental Entanglement Swapping: Entangling Photons That Never Interacted\u201d Physical Review Letters 80, 3891-3894 May 1998, which described a process of entanglement swapping with experimental verification using entangled photons. Swapping may be considered as the teleportation of an unknown photon\/particle state onto another photon\/particle.","The process of entanglement swapping has many potential applications in the development of quantum technology. Thus far, relatively few applications have found uses for entanglement swapping. Potential applications for entanglement swapping in quantum technology include quantum computing, quantum communications and, in the current invention, quantum imaging. There are potentially many benefits to using entanglement swapping for quantum imaging that have not yet been described or exploited. The reason for this is that entanglement swapping has required high precision in its implementation and great expense for equipment that achieves the high precision. The lack of robust applications for entanglement swapping has been another drawback to its implementation in technology. This technology is being miniaturized in solid state devices and some components are being tested on chips. These quantum chips, can generated entangled particles and perform interference operations and measurements of quantum states.","It would be beneficial to have an entanglement swapping application that is robust and can be implemented with both available and evolving technologies. One way to make entanglement swapping useful would be to apply it information transfer, sharing or communication without the need for a classical communications channel. For example, the current Internet, radio, and telephone are generally considered to be classical communications channels. Another way to make entanglement swapping useful would be to be able to transfer, share or communicate by quantum means without the sender or receiver needing access to information or resources held by the other. For example, the sender having access to photons P2, P3 and the receiver having access to photons P1, P4 is sufficient to transfer information from sender to receiver. Repetition of this process allows the transfer of images without sending classical information and by only sharing entanglement. This type of communication of information, such as data and\/or images, would be difficult to detect by an external observer since there would be no particle or radiation going between the sender and the receiver which an observer would be able to sense and follow. Military and domestic applications requiring stealth and\/or security would benefit from this capability. Experiments by the inventors have verified the working principles. The experiments started with entangled pairs of photons configured as described in . Photon pairs equivalent to the first set of entangled photon pairs (transmitted photons) of  were manipulated to enable and disable a swapping of entanglement to the second set of entangled photons pairs and the effects of that manipulation were measured with the coincidence detection of the second set of entangled photon pairs. The second set of entangled photon pair coincidence measurements were reduced when the entanglement swapping was disabled with the first set of entangled photon pairs and the second set of entangled photon pair coincidence measurements increased when entanglement swapping was enabled with the first set of entangled photon pairs.","Benefits of entanglement swapping for quantum imaging may include performing an entanglement swap to optimize photon detection efficiency while simultaneously optimizing transmission properties from an illumination source to a target. Another benefit is that an entanglement swap may be used to measure absorption maps of a target without the need to measure reflected photons. Furthermore, entanglement swapping may be used to help compute the product of the absorption values at two locations on a target.","Using the environment to enable entanglement swapping provides a direct and remote measurement on the environment. For example, absorption of photons by a remote target can be sensed by the enabling of quantum swapping of entangled particles which can be measured remotely without need for the return of photons from the target. It should be noted that besides images of absorption fields of targets any property can be imaged by enabling quantum swapping when the quantum particle is sensitive to the effects of object. Furthermore, with time sequencing this provides range information from, for example, the source of entangled quantum particles to target features. It should be further realized that the source or sources of the entangled quantum particles need not be located with the equipment used to direct particles towards a target (sender) or located with the equipment that measured those entangled particles that never directly interacted with the target (receiver). For example, the source or sources of the entangled particles may be on a satellite that would send the entangled particle pairs to the \u201csender\u201d equipment and \u201creceiver\u201d equipment. Alternately, both the sender and receiver may have a single entangled quantum particle source and each shares one particle of their entangled particle pairs with the other. The identification of which particles are entangled with each other relative to initial entangled pair creation times may be achieved using an auxiliary time stamp, e.g. a laser pulse encoded with time information for each entangled photon pair created, that propagates with each particle of each entangled particle pair. Also, the use of an entanglement source such as the one described in  does not have an issue (or question) as to the identification of which particles are entangled as there is only a single source that sequentially generates entangled particles. Although not obvious, we consider it possible to use thermal light photon number fluctuations and their correlations and quantum illumination for variations of teleportation and swapping in our current inventions with swapping.","Further benefits of entanglement swapping applied to quantum imaging using measurements of reflected photons may include application to quantum imaging of remote targets and microscopy with the images being generated for the user at a distant location with entangled photons that did not interact directly with the target. The reflected photons may be further used to compute the product of reflectance or the product of reflected intensities of at least two locations on the target. Current imaging systems, such as cameras, are dependent on producing imaging using photons that have directly interacted with the target. The sharing of images taken by a camera normally requires communication by electromagnetic radiation that takes specific paths to communicate a facsimile of the image between sender and receiver. Even quantum teleportation requires a classical communication channel using electromagnetic radiation that takes specific paths to communicate. It would be beneficial to use entanglement swapping to communicate images or quantum images that does not require a classical communications channel to complete the transfer of images between a sender and a distant user at the receiver in order to avoid having the classical communications channel blocked which would also block image communication. Communication information transfer using entanglement swapping would be an entirely quantum process. The speed of quantum information has been recently been reported as being greater than or equal to 1.37*10times the speed of light See, el Yin et al. \u201cLower Bound on the Speed of Nonlocal Correlations without Locality and Measurement Choice Loopholes,\u201d Physical Review Letters 110, 260407 June 2013. The benefits of utilizing swapping in the process of quantum communications is that communications would be at the speed of the quantum information even if it is faster than the speed of light which can be beneficial for many applications.","Imaging of a scene or subject is typically accomplished by mapping an illuminated scene or subject onto an image sensor where there is a light measurement component such as film, CCD, or other sensing device. Light consists of a plurality of photons that may be measured. The illuminating light may be from one or more light sources either natural or artificial, or both. Common sources of light include for example the sun, coherent, incoherent, or partially coherent light, entangled photons, infrared radiation emitted by atoms and molecules, accelerating charges, lasers, light bulbs, light emitting diodes (LEDs), chaotic laser light, pseudo-thermal light generated by passing laser light through a rotating ground glass or other scattering material, stars, moons, clouds, planets, space objects, fluorescent lamps, electrical discharges, plasmas, bio-luminescence, and stimulated emission. Although it is not absolutely necessary a lens is often used to perform this mapping. Imaging is often susceptible to adverse affects such as obscuration, turbulence, low signal to noise ratio such as when operating in low-light conditions, jitter, and noise. Often, this type of imaging is referred to as \u201cFirst Order\u201d imaging due to the time, ensemble, or mixed time-ensemble averaging of the sensors involved. For instance, a first-order light intensity image I(x, y, t) can be produced by light interacting with a sensor for some time \u0394t, i.e. shutter or integration time. A single instance of this may be referred to as a \u201cframe\u201d. Multiple frames of images, I(x, y, t), may be averaged over some or all of the frames in a sequence of frames to generate an averaged first-order image of the subject <I(x, y, t)> where < > indicates an ensemble average. A second order image involves averages of products of two first-order intensity or normalized intensity measurements. An enhanced image results from the subtraction of products of averages of first order intensities from the average of the product of the intensities. An intensity or normalized intensity can be decomposed into a mean (<I>) plus a deviation from the mean or average (\u0394I).","The terms Iand Iare intensities or normalized intensities measured by sensors 1 and 2, I=<I>+\u0394Iand I=<I>+\u0394Iwith Iand Ibeing functions of space and time, i.e., x, y, t, <I> is the ensemble average of intensity or normalized measurements of sensor 1 and \u0394Iis the deviation from the mean for the intensity or normalized intensity measurements of sensor 1. <I> is the ensemble average of intensity or normalized measurements of sensor 2 and \u0394Iis the deviation from the mean for the intensity or normalized intensity measurements of sensor 2. The deviation is often called a fluctuation.","There is a trend in modern imaging devices, i.e. cameras, to provide more measured quantum properties at each pixel. Thus, the measurements may include measurements such as wavelength (color or derived color mappings such as RGB, CMY, CMYK, etc.), polarization, Stokes parameters, spatial modes, orbital angular momentum (OAM), spin, etc. For example, a color camera may provide separate wavelength measurements, red (R), green (G), and blue (B) measurement values at each pixel, the polarization Stokes parameters at each pixel, and modern infrared (IR) cameras can provide measurements of long-wave infrared (LWIR) and mid-wave infrared (MWIR) measurement values at each pixel of the imaging device, or combinations of these measurements. In the current invention at least one of the available measured quantities is selected to provide the frame data for the generation of the improved image of the region of interest.","It is to be appreciated that measurements of quantities such as wavelength and polarization are typically dependent on the responsiveness of the measurement device to the quantity being measured. As an example, color cameras typically use band-pass filters arranged in a pattern over the pixels of the measurement device. These filters are usually labeled Red, Green, and Blue (R, G, and B). The wavelengths that each of the R, G, B filters pass is centered at a particular wavelength and also passes nearby wavelengths with wavelengths being more distant from the center wavelength being more highly attenuated. This effect is referred to as the bandwidth of the filter. Polarization filters have similar bandwidths with respect to the orientation of the filter. The responsiveness to wavelength, polarization, etc., of an element on a measurement may also be adjusted by applying, for example, larger or smaller voltages to increase or decrease the degree to which each element (pixel) reacts to the wavelength or polarization of light that interacts with that pixel.","The invention can be used with measurements of quantum particles. There are many quantum particles including but not limited to photons, electrons, neutrons, protons, atoms, ions, mesons, and positrons. Photons, mesons, neutral atoms or molecules are bosons. Fermions include quantum particles such as electrons, ionized atoms and ionized molecules sometimes referred to as ions.","The invention can be used to generate improved images through a variety of gaseous, solid, or liquid media or mixtures of these that are at least partially transparent to quantum particles. These media may include but are not limited to, for instance, glasses, diamond, silicon, water and air. As an example, images captured with an underwater camera can be used as input for the inventive process for enhancement as well as images taken through say an air-water interface such as an imaging system on a boat looking down into the water or a submerged imaging looking into the air above the water.","As used in the following, the terminology photon light source means or is defined to include, thermal photon light, partially coherent, and entangled photon light. As used in the following, the terminology media means or is defined to include vacuum, air, water, turbid fluids, turbulence fluids, soft tissues and partially transmissive solids.","As used herein, the terminology \u201csubject\u201d or \u201cobject\u201d or \u201ctarget\u201d may include a photograph, a thing, a person, animate or inanimate subject, a plurality of objects, the ground, ground covering (e.g., grass), a localized piece or pieces of the environment, surface, physical entity (or entities) or anything that can be observed.","As used herein, the terminology \u201cbucket\u201d refers to a single-pixel (bucket) detector.","As used herein, the terminology \u201csecond order\u201d is defined as follows. A second order image involves averages of products of two first-order intensity or normalized intensity measurements. Imaging of a scene or subject is typically accomplished by mapping an illuminated scene or subject onto an image sensor where there is a light measurement component such as film, CCD, or other sensing device. Light consists of a plurality of photons that may be measured. The illuminating light may be from one or more light sources either natural or artificial, or both. Common sources of light include for example the Sun, coherent, incoherent, or partially coherent light, infrared radiation emitted by atoms and molecules, accelerating charges, lasers, light bulbs, light emitting diodes (LEDs), chaotic laser light, pseudo-thermal light generated by passing laser light through a rotating ground glass or other scattering material, stars, moons, clouds, planets, space objects, fluorescent lamps, electrical discharges, plasmas, bio-luminescence, and stimulated emission. Although it is not absolutely necessary a lens is often used to perform this mapping. Imaging is often susceptible to adverse affects such as obscuration, turbulence, low signal to noise ratio such as when operating in low-light conditions, jitter, and noise. Often, this type of imaging is referred to as \u201cFirst Order\u201d imaging due to the time, ensemble, or mixed time-ensemble averaging of the sensors involved. For instance, a first-order light intensity image I(x, y, t) can be produced by light interacting with a sensor for some time \u0394t, i.e. shutter or integration time. A single instance of this may be referred to as a \u201cframe\u201d. Multiple frames of images, I(x, y, t), may be averaged over some or all of the frames in a sequence of frames to generate an averaged first-order image of the subject <I(x, y, t)> where < > indicates an ensemble average. A second order image involves averages of products of two first-order intensity or normalized intensity measurements. An enhanced image results from the subtraction of products of averages of first order intensities from the average of the product of the intensities. An intensity or normalized intensity can be decomposed into a mean plus a deviation from the mean or average.","I=<I>+\u0394Iand I=<I>+\u0394Iwith Iand Ibeing functions of space and time, i.e. x, y, t. Where Iand Iare intensities or normalized intensities measured by sensors 1 and 2. <I> is the ensemble average of intensity or normalized measurements of sensor 1 and \u0394Iis the deviation from the mean for the intensity or normalized intensity measurements of sensor 1. <I> is the ensemble average of intensity or normalized measurements of sensor 2 and \u0394Iis the deviation from the mean for the intensity or normalized intensity measurements of sensor 2. The deviation is often called a fluctuation. Mathematically the second-order enhanced image can be represented by\n\n<II>=<<I><I>+\u0394II>+\u0394I<I>+\u0394I\u0394I>. Simplifying this expression yields <II>=<I><I>+<\u0394I\u0394I>.\n\nSimilarly\n",{"@attributes":{"id":"p-0199","num":"0207"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mtable":{"mtr":[{"mtd":{"mrow":{"mo":["<",">=","<"],"mrow":[{"mi":["\u0394","\u0394"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":[{"mi":"I","mn":"1"},{"mi":"I","mn":"2"}]},{"mrow":[{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":"I","mn":"1"},"mo":"-"},"mo":["<",">"],"msub":{"mi":"I","mn":"1"}}},{"mo":"(","mrow":{"mrow":{"mrow":{"msub":{"mi":"I","mn":"2"},"mo":"-"},"mo":"<","msub":{"mi":"I","mn":"2"}},"mo":">>"}}],"mo":"\u2062"}],"mi":{}}}},{"mtd":{"mrow":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mo":["<","<",">","<",">"],"mrow":[{"mrow":{"msub":[{"mi":"I","mn":"1"},{"mi":"I","mn":"2"}],"mo":"\u2062"},"mo":"-","msub":{"mi":"I","mn":"1"}},{"mo":"-","msub":{"mi":"I","mn":"2"}},{"mo":"+","mrow":{"mo":["<",">","<"],"msub":[{"mi":"I","mn":"1"},{"mi":"I","mn":"2"}]}}],"msub":[{"mi":"I","mn":"2"},{"mi":"I","mn":"1"}]}},"mo":">>"}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mo":["<",">","<",">","<",">"],"mrow":[{"msub":[{"mi":"I","mn":"1"},{"mi":"I","mn":"2"}],"mo":"\u2062"},{"mo":"-","mn":"2"},{"mo":"+","mrow":{"mo":["<",">","<",">","."],"msub":[{"mi":"I","mn":"1"},{"mi":"I","mn":"2"}]}}],"msub":[{"mi":"I","mn":"1"},{"mi":"I","mn":"2"}]}}}}]},"mo":["\u2062","<",">=","<",">"],"mstyle":{"mtext":{}},"mrow":[{"mi":["\u0394","\u0394"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msub":[{"mi":"I","mn":"1"},{"mi":"I","mn":"2"}]},{"msub":[{"mi":"I","mn":"1"},{"mi":"I","mn":"2"}],"mo":"\u2062"},{"mo":"-","mrow":{"mo":["<",">","<",">"],"msub":[{"mi":"I","mn":"1"},{"mi":"I","mn":"2"}]}}]}}},"br":[{},{}],"sub":["1 ","2 ","1","1","2","1","1","1 ","2","2","2 ","1 ","2 ","1 ","2 ","1","1 ","2","2 ","1","2","1","2","1","2","2","1","1","2","1","2","1","2","1 ","2","1","2","1","2","1","2","1","2","1 ","2 ","1 ","2","1","2","1","2","1","2","1","2","1","2"]},"The terminology \u201cat least second order\u201d means an enhanced image contained in <\u0394I\u0394I> or <\u0394I\u0394I\u0394I> or <\u0394I\u0394I. . . \u0394I> where N is an integer, and where Irepresents an image utilizing a third sensor and Irepresents an image utilizing the nth sensor.","The invention is not restricted to the illustrative examples described above. Examples are not intended as limitations on the scope of the invention. Methods, apparatus, compositions, and the like described herein are exemplary and not intended as limitations on the scope of the invention. Changes therein and other uses will occur to those skilled in the art. The scope of the invention is defined by the scope of the claims.","Patents, patent applications, or publications mentioned in this specification are incorporated herein by object to the same extent as if each individual document was specifically and individually indicated to be incorporated by object.","The foregoing description of the specific embodiments are intended to reveal the general nature of the embodiments herein that others can, by applying current knowledge, readily modify and\/or adapt for various applications such specific embodiments without departing from the generic concept, and, therefore, such adaptations and modifications should and are intended to be comprehended within the meaning and range of equivalents of the disclosed embodiments. It is to be understood that the phraseology or terminology employed herein is for the purpose of description and not of limitation. Therefore, while the embodiments herein have been described in terms of preferred embodiments, those skilled in the art will recognize that the embodiments herein can be practiced with modification within the spirit and scope of the appended claims."],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The present invention can be better understood when reading the following detailed description with reference to the accompanying drawings, which are incorporated in and form a part of the specification, illustrate alternate embodiments of the present invention, and together with the description, serve to explain the principles of the invention. In the drawings:",{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":["FIG. 3","FIGS. 1"],"b":["2","3"]},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 4","b":"17"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 5","FIG. 4"]},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 6","sup":"(2) "},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 7A"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 7B"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 8A"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 8B"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 9A"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 9B"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 10A"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 10B"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 11A"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 11B"},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 12A"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 12B"},{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 13A"},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 13B"},{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 14A"},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 14B"},{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 15A"},{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 15B"},{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 16A"},{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 16B"},{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 17A"},{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 17B"},{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 18A"},{"@attributes":{"id":"p-0070","num":"0069"},"figref":"FIG. 18B"},{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 19A"},{"@attributes":{"id":"p-0072","num":"0071"},"figref":"FIG. 19B"},{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIGS. 20-31"},{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0077","num":"0076"},"figref":["FIG. 23","FIG. 22"]},{"@attributes":{"id":"p-0078","num":"0077"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0079","num":"0078"},"figref":["FIG. 25","FIG. 24"]},{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0081","num":"0080"},"figref":["FIG. 27","FIG. 26"]},{"@attributes":{"id":"p-0082","num":"0081"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0083","num":"0082"},"figref":["FIG. 29","FIG. 28"]},{"@attributes":{"id":"p-0084","num":"0083"},"figref":"FIG. 30"},{"@attributes":{"id":"p-0085","num":"0084"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0086","num":"0085"},"figref":"FIG. 32"},{"@attributes":{"id":"p-0087","num":"0086"},"figref":"FIG. 33"},{"@attributes":{"id":"p-0088","num":"0087"},"figref":"FIG. 34"}]},"DETDESC":[{},{}]}
