---
title: Middleware layer between speech related applications and engines
abstract: The present invention provides an application-independent and engine-independent middleware layer between applications and engines. The middleware provides speech-related services to both applications and engines, thereby making it far easier for application vendors and engine vendors to bring their technology to consumers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07139709&OS=07139709&RS=07139709
owner: Microsoft Corporation
number: 07139709
owner_city: Redmond
owner_country: US
publication_date: 20001229
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["The present application is based on and claims the benefit of U.S. provisional patent application Ser. No. 60\/219,861, filed Jul. 20, 2000, the content of which is hereby incorporated by reference in its entirety.","The present invention deals with services for enabling speech recognition and speech synthesis technology. In particular, the present invention relates to a middleware layer which resides between applications and engines (i.e., speech recognizers and speech synthesizers) and provides services, on an application-independent and engine-independent basis, for both applications and engines.","Speech synthesis engines typically include a decoder which receives textual information and converts it to audio information which can be synthesized into speech on an audio device. Speech recognition engines typically include a decoder which receives audio information in the form of a speech signal and identifies a sequence of words from the speech signal.","In the past, applications which invoked these engines communicated directly with the engines. Because the engines from each vendor interacted with applications directly, the behavior of that interaction was unpredictable and inconsistent. This made it virtually impossible to change synthesis or recognition engines without inducing errors in the application. It is believed that, because of these difficulties, speech recognition technology and speech synthesis technology have not quickly gained wide acceptance.","In an effort to make such technology more readily available, an interface between engines and applications was specified by a set of application programming interfaces (API's) referred to as the Microsoft Speech API version 4.0 (SAPI4). Though the set of API's in SAPI4 specified direct interaction between applications and engines, and although this was a significant step forward in making speech recognition and speech synthesis technology more widely available, some of these API's were cumbersome to use, required the application to be apartment threaded, and did not support all languages.","The process of making speech recognition and speech synthesis more widely available has encountered other obstacles as well. For example, many of the interactions between the application programs and the engines can be complex. Such complexities include cross-process data marshalling, event notification, parameter validation, default configuration, and many others. Conventional operating systems provide essentially no assistance to either application vendors, or speech engine vendors, beyond basic access to audio devices. Therefore, application vendors and engine vendors have been required to write a great deal of code to interface with one another.","The present invention provides an application-independent and engine-independent middleware layer between applications and engines. The middleware provides speech-related services to both applications and engines, thereby making it far easier for application vendors and engine vendors to bring their technology to consumers.","In one embodiment, the middleware layer provides a rich set of services between speech synthesis applications and synthesis engines. Such services include parsing of input data into text fragments, format negotiation and conversion to obtain optimized data formats, selecting default values and managing data output to an audio device.","In another embodiment, the middleware layer manages single-application, multivoice processes. The middleware layer, in another embodiment, also manages multi-application, multivoice mixing processes.","In yet another embodiment, the invention includes a middleware component between speech recognition applications and speech recognition engines. In such an embodiment, the middleware layer illustratively generates a set of COM objects which configures the speech recognition engine, handles event notification and enables grammar manipulation.","In yet another embodiment, the middleware layer between the speech recognition application and speech recognition engine marshals calls from multiple application process to the speech recognition engine, and directs recognition results to the appropriate application process.","Appendix A illustrates an exemplary set of APIs.","Appendix B illustrates an exemplary set of DDIs.",{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 1","b":["100","100","100","100"]},"The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems, environments, and\/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and the like.","The invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit . The system bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier WAV or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, FR, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way o example, and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs  other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard , a microphone , and a pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  may operate in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.",{"@attributes":{"id":"p-0042","num":"0041"},"figref":["FIG. 2","FIG. 1"],"b":["200","200","200","202","204","206","208"]},"In one illustrative embodiment, speech middleware component  is implemented in the operating system  illustrated in . Speech middleware component , as shown in , includes speech recognition middleware component , context free grammar (CFG) engine  and text-to-speech middleware component .","Briefly, in operation, speech middleware component  resides between applications  and engines  and . Applications  can be speech recognition and speech synthesis applications which desire to invoke engines  and . In doing so, applications  make calls to speech middleware component  which, in turn, makes calls to the appropriate engines  and  in order to have speech recognized or synthesized. For example, applications  may provide the source of audio data for speech recognition. Speech middleware component  passes that information to speech recognition engine  which simply recognizes the speech and returns a recognition result to speech recognition middleware component . Speech recognition middleware component  places the result in a desired format and returns it to the application  which requested it. Similarly, an application  can provide a source of textual data to be synthesized. TTS middleware component  assembles that data, and provides it to TTS engine , for synthesis. TTS engine  simply synthesizes the data and returns audio information to TTS middleware component , which handles spooling of that information to an audio device, writing that information to memory, or placing that information in any other desired location, as specified by the application  which requested it.","CFG engine , briefly, assembles and maintains grammars which are to be used by speech recognition engine . This allows multiple applications and multiple grammars to be used with a single speech recognition engine . This is discussed in greater detail later in the specification.",{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 3","FIG. 2","FIG. 3"],"b":["200","214","214","216","218","220","214","222","224","214","202","214","208"]},"A general discussion of the operation of TTS middleware component , with applications  and engine , is illustrated by the flow diagram in . Initially, application  opens an instance of the SpVoice object . In one illustrative embodiment, the application calls the COM CoCreateInstance for the component CLSID_SpVoice to get a pointer to the interface ISpVoice of the SpVoice object. SpVoice object  then creates lexicon container object  and an XML parser object . This is indicated by blocks ,  and  in .","Next, application  can either specify the attributes of TTS engine , such as whether the engine which is the synthesizer exhibits male or female voice qualities, the language of the synthesis, etc. This is done, for example, by calling the SetVoice method on the SpVoice object . This is indicated by optional block  in . In addition, the application can optionally specify the particular audio output object  which is desired. This is indicated by optional block  in .","The application  can set other attributes associated with the voice speaking, such as the rate and volume of speech, using for example, the SetRate and the SetVolume methods exposed by the SpVoice object . These are optional as well.","It should be noted that specifying the attributes of the engine  and audio output object  are optional. If the application does not specify these items, the first call to the SpVoice object  requiring synthesis results in the SpVoice object  choosing and initializing the default voice (i.e., the default TTS engine ) and the default audio output object .","Once these items are configured properly, application  can call the SpVoice object  and request that textual information by synthesized. This can be done, for example, by calling the Speak or the SpeakStream methods on the SpVoice object . This is indicated by block .","The SpVoice object  then performs format negotiation. This is discussed in greater detail later in the specification with respect to . Briefly, however, the Spvoice object  attempts to optimize the format of data created by TTS engine  and that accepted by audio output object  for optimal synthesis. Format negotiation is indicated by block  in .","The SpVoice object  then breaks the textual information provided by application  into text fragments. For example, if the textual information is in XML, the SpVoice object  invokes the XML parser  to parse the XML input into text fragments. While the textual information can come from a variety of sources (such as a text buffer, straight textual information, XML, etc.) that information is broken into text fragments by SpVoice object , as indicated by block  in .","The SpVoice object  then calls a speak method on TTS engine , passing in the information to be synthesized. This is indicated by block . In doing this, the SpVoice object  also specifies a Site object  to be used by the TTS engine for returning the synthesized information.","TTS engine  receives the text fragments, synthesizes the text into WAV data (or other suitable audio data) and provides an indication of where events occur in the WAV data. For example, TTS engine  can illustratively provide an indication where word and phoneme boundaries occur in the WAV data. This information is all provided from TTS engine  to SpVoice object  through the Site object .","It should be noted that, in performing the synthesis, TTS engine  can access the lexicon object  contained in TTS middleware component . This is discussed in greater detail later in the specification with respect to . Briefly, the lexicon container object  contains all lexicons of interest and the TTS engine  simply needs to access object  as if it were a single lexicon.","Synthesizing the actual fragments and writing them to the Site object are indicated by blocks  and  in .","During the format negotiation step , the SpVoice object  determines whether the format of the audio output object  or the format of the information provided by TTS engine  need to be converted. If conversion is required, information is provided to format converter object , such as through the ISpAudio or ISpStream interfaces, where the information is converted into a desired format for the audio output object . Format converter object  then manages the process of spooling out the audio information to audio output object  and also manages returning events noticed by the audio output object  to the Site object  and the SpVoice object  for transmission back to the application . This is indicated by blocks  and  in  Where no format conversion is desired, the information from the Site object  is spooled out to the audio output object  by the SpVoice object , through a suitable interface such as the IspStream interface. This is indicated by block .","Of course, it should also be noted that rather than providing the information directly to an audio output object , the information can be written to memory, as indicated by block , or provided at some other specified output or location as indicated by block  in .",{"@attributes":{"id":"p-0060","num":"0059"},"figref":["FIG. 5","FIG. 4","FIG. 5"],"b":["242","254","208","224","216","224","262","264","202","224"]},"In any case, once the appropriate audio output object  is initiated, SpVoice object  queries the audio output object  to obtain the default format from the audio output object . Obtaining the default format from the audio device object  is indicated by block  in .","Once the default format of information expected by the audio output object is obtained, the Spvoice object  queries TTS engine  to see what format it will provide based on the format that is input to it. This is indicated by block . It is next determined whether the output from TTS engine  is in the proper format to be received by the input to the audio output object . This is indicated by block . If the output format from TTS engine  matches the desired input format at audio output object , the information can be output in that format, to audio output object . This is indicated by block .","However, if, at block , it is determined that the output format from TTS engine  is not the same as the desired input format at audio output object , then the SpVoice object  determines whether it can reconfigure the audio output object  to accept the format output by TTS engine . This is indicated by block . Recall that, if the application specifies an audio output object  it can also specify that the input format not be changed.","If, at block , it is admissible to change the input format expected by the audio output object , then the audio output object  is simply reconfigured to accept the format output by TTS engine . This is indicated by block . The information can then be provided to the audio output object  as indicated by block .","However, if it is determined at block  that the expected input format of the audio output object  cannot be changed, the SpVoice object  determines whether a format converter  is available for converting the output format from the TTS engine  to the desired input format of audio output object . This is indicated by block . If no such converter is available, SpVoice object  simply provides an error message to application  indicating that the format conversion cannot be made. However, if a format converter is available to make the desired format conversion, the format converter is invoked so that the audio information from TTS engine  can be converted to the appropriate format. This is indicated by block . In that case, the converted audio information is provided from format converter object  to the audio output object , as indicated by block .",{"@attributes":{"id":"p-0066","num":"0065"},"figref":["FIG. 6","FIG. 6","FIG. 3","FIG. 6"],"b":"214"},"In order accomplish this, the application first performs the same first several steps illustrated in the flow diagram of . For example, the application first opens SpVoice object , which in turn creates the lexicon and XML parsers. These steps are not shown in  for the sake of clarity. The application  then specifies the engines, or the attributes of the voices which the application desires. This is indicated by block .","Setting the attributes of the engine (or the voice) can be done, for instance, by calling the method SetVoice on SpVoice object . In response to these specified voices, SpVoice object  instantiates two different TTS engine objects A and B, which contain the desired attributes specified by the application. Therefore, for example, if the application  specifies one male voice and one female voice, SpVoice object  instantiates a TTS engine A which has attributes of a female voice and TTS engine B which has attributes of a male voice. This is indicated by block . Application  also sets the priority for those specified voices (or engines). This is indicated by block . The priority basically indicates which TTS engine takes precedence in speaking, and is described in greater detail below. Setting the priority can be called, for instance, by invoking the method SetPriority on the SpVoice object .","Once the engines have been instantiated and the priorities set, the application indicates to the SpVoice object  that is wishes some textual information to be spoken. This is indicated by block  and can be done, for example, by calling Speak or SpeakStream on the SpVoice object . The information provided will also identify the particular engine A or B which application  wishes to have speak the information.","The textual input information is then parsed into text fragments as indicated by block . For example, if the input is an XML input, the XML is parsed into text fragments.","Based on the indication from application  (such as an XML tag on the input information) SpVoice object  calls the appropriate TTS engine A or B requesting synthesis and passing in the information to be synthesized. This is indicated by block . The TTS engine A or B which has been called, synthesizes the text fragments and writes the audio information to its corresponding Site object A or B. This is indicated by block . The synthesized information is then provided from Site A or B to audio output object  which provides it in turn to an audio device, such as speaker  or to another set of API's or objects, as desired.","It should be noted that, in setting priority as shown in block , a number of different things can be accomplished. If the priorities are set to normal, then the requests by application  to speak text are simply queued and are spoken in the order received. However, other priorities can be set as well. If a priority is set to alert, an audio message can be injected, in the middle of another audio message which is playing. Similarly, if the priority is set to speakover, then that audio text will simply speak at the same time as the audio which is currently being spoken.","The priorities are better illustrated with respect to .  shows a multiprocess, multivoice implementation of the present invention. In , two applications A and B have created two separate instances of the SpVoice Object A and B. Those objects have created separate Site objects A and B, TTS engine objects A and B and audio objects A and B. It should be noted that separate (i.e., A and B) lexicon container objects can be created as well. In , however, a unified lexicon container object  is illustrated. Both alternatives are within the scope of the present invention. The outputs from the audio output objects A and B are provided to a multimedia application programming interface (API) , such as that supported by the WINDOWS98 operating system, Second Edition or by the WINDOWS2000 operating system. The output of the multimedia API  is provided to an audio device, such as speaker .","The operation of processes A and B shown in  is similar to that illustrated by  discussed above. It should also be mentioned, however, that in addition to setting the priority for a given voice, or TTS engine, the application can also specify the insertion points in a synthesized stream for alerts. Therefore, in one example, assume that application A has specified its request to speak as having a normal priority, and application B has specified its request to speak as having an alert priority, and further assume that audio output object A is speaking data which is being spooled out by either SpVoice object A or Site object A. Now assume that TTS engine B returns synthesis information which has been prioritized with an alert priority. Audio output object A will be allowed to speak to the alert boundary set by application A (such as the end of the current word) at which point the audio output object A will be closed and control will be assumed by SpVoice object B and audio output object B such that only its information can be output to multimedia API  and subsequently to speaker . This can be accomplished using a shared mutex scheme such as that provided through WinAP services. When audio output object A is closed, the SpVoice object A simply does not return on the call which TTS engine A has made to Site A. Therefore, TTS engine A simply pauses. After the alert message has been spoken, SpVoice object B and audio output object B release the mutex such that SpVoice object A and audio output object A can continue speaking. At that point, Spvoice object A returns on the TTS engine call such that TTS engine A can continue its processing.","If the two speak commands by applications A and B are indicated as speakover priority, then assuming that the multimedia API layer  supports mixing, the audio information from both audio output object A and audio object B will be spoken by speaker , at the same time. If the speak requests are indicated as normal, then the speak requests are queued and are spoken, in turn.","It should also be noted that if, within either process A or process B multiple speak requests are received, then processing is handled in a similar fashion. If a normal speak request is followed immediately by an alert request, than the normal speak request is halted at an alert boundary and the alert message is spoken, after which the normal speak request is again resumed. If more then one alert message is received within a single process, the alert messages are themselves queued, and spoken in turn.","It should also be noted that the configuration illustrated in  can be implemented by one application , rather than two applications. In that case, a single application  simply co-creates two instances of the SpVoice object . Those instances create the remaining objects, as illustrated in .",{"@attributes":{"id":"p-0078","num":"0077"},"figref":"FIG. 9","b":["220","220","400","402","404","208"]},"In any case, once the lexicon container object  is created, it examines the registry for user and application lexicons. Lexicon container object  can also expose an interface  accessible by TTS engine . This allows the TTS engine  to not only access various lexicons ,  and  stored in lexicon container object , but also allows TTS engine  to add a lexicon to lexicon container object  as well. Lexicon container object  represents all of the lexicons contained therein, as one large lexicon to TTS engine . Therefore, TTS engine  or application  need not handle providing access to multiple lexicons, as that is all handled by lexicon container object  through its exposed interface.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIG. 10","FIG. 10"],"b":["220","208","208","408","406","410","220","218","412"]},"This provides significant advantages. For example, in the past, TTS engines  contained the lexicon. If a user had terms with user-specified pronunciations, every time an application opened up a separate TTS engine that engine would speak the user's pronunciations improperly, until the TTS engine lexicon was modified. In contrast, using lexicon container object , each time a different TTS engine  is opened, it will automatically be directed to the user lexicon  such that the user's preferred pronunciations will always be used, regardless of the TTS engine  which is opened. This engine-independent lexicon thus greatly improves the process.",{"@attributes":{"id":"p-0082","num":"0081"},"figref":["FIG. 11","FIG. 2","FIG. 11","FIG. 11"],"b":["200","210","210","420","422","206","210","424","426","428","430","432","424","216","214","210","424","202","424","206"]},"The SpRecoGrammar object  represents the grammar which the SR engine  associated with the SpRecoGrammar object  will be listening to. The SpRecoGrammar object  can contain a number of different items, such as a dictation topic grammar, a context free grammar (CFG), a proprietary grammar loaded either by SR engine  or application  and a word sequence data buffer which is explained in greater detail later in the specification.",{"@attributes":{"id":"p-0084","num":"0083"},"figref":["FIG. 12","FIG. 11","FIG. 12"],"b":["210","202","420","206","422","206","422","440"]},"The SpRecoContext object  is then created as illustrated by block  in . The application can then call exposed interfaces on SpRecoContext object  to create the SpRecoGrammar object . Such an interface can include, for instance, the CreateGrammar method. Creation of the SpRecoGrammar object is illustrated by block  in .","The application then calls the SpRecoContext object  to set desired attributes of recognition, as indicated by block . For example, the application can determine whether it would like alternatives generated by SR engine  by calling the SetMaxAlternative method and can also enable or disable the retention of the audio information along with the results. In other words, SR middleware component  will retain the audio information which is provided by audio object  upon which SR engine  performs recognition. That way, the audio information can be reviewed later by the user, if desired. The application can also call interfaces exposed by the SpRecoContext object  in order to change the format of the retained audio. Otherwise, the default format which was used by the recognition engine  in performing recognition is used.","The application then illustratively configures the SpRecoGrammar object  as desired. For example, the application  can load a grammar into the SpRecoGrammar object by calling the LoadDictation method. The application can also set a word sequence data buffer in engine  by calling the SetWordSequenceData method. Further, the application can activate or deactivate grammar rules by either rule ID or by rule name, by calling the SetRuleIDState method or the SetRuleState method, respectively. The application can also enable or disable grammars within the SpRecoGrammar object  by calling the SetGrammarState method. It should be noted that, when a grammar is disabled, the SpRecoGrammar object  stores the state of the grammar prior to it being disabled. Therefore, when it is again enabled, the SpRecoGrammar object can automatically activate and deactivate rules in that grammar to obtain its previous activation state. Further, the application can load command and control grammars by calling the LoadCmdFromXXXX where \u201cXXXX\u201d can be a file, object, resource or memory. Configuring the SpRecoGrammar object is indicated by block  in .","The SpRecoContext object  then performs a format negotiation as indicated with the speech synthesis embodiment. In other words, the SpRecoContext object  queries the audio input object  to determine the format of the audio input. The SpRecoContext object  also quires SR engine  to determine what format it desires, and will reconfigure the audio object  or the SR engine  as desired, if possible. The format negotiation is indicated by block  in .","SpRecoContext object  then calls device driver interfaces exposed by SR Engine  to configure the engine and to set SrEngineSite , as indicated by block . The Site for the engine to use is set by calling the SetSite method on SR engine . This provides the handle to Site object  which is the object that SR engine  calls to communicate events and recognitions as well as to synchronize with and make other communications with, SR middleware component .","Acoustic recognition information is also set in engine  by, for instance, calling the SetRecoProfile method exposed by engine . The acoustic profile information may vary, for example, with user, or with application. Therefore, the appropriate acoustic profile information is obtained from the registry and loaded into SR engine .","The engine can also be loaded with specific or proprietary grammars or language models by calling the LoadProprietaryGrammar method or the LoadSLM method, respectively. The SpRecoContext object  can also set up a text buffer structure and hand SR engine  a pointer to it by calling the OnCreateGrammar method and can also set a word sequence data buffer in engine  by calling the SetWordSequenceData method.","The word sequence data buffer is a buffer which can be populated, on-the-fly, by the application. In one illustrative embodiment the word sequence data buffer contains double null terminated entries which can be used by SR engine  in making a recognition. For example, a CFG rule, which spawns a recognition by SR engine , can point SR engine  into the word sequence data buffer to look for matches of subsequent word sequences. In one illustrative embodiment, such a rule may spawn a recognition of the words \u201cSend e-mail to\u201d. In that case, the application can populate the word sequence data buffer with electronic mail aliases. SR engine  then searches the word sequence data buffer to better refine the recognition process in making a recognition of the following speech.","Once SR engine  is configured, the SpRecoContext object  can call SR engine  to begin recognition. Such a call can be made on, for example, the RecognizeStream method. When such a method is called, SR engine  begins recognition on an input data stream and the process continues until a buffer containing the data to be recognized is empty, or until the process is affirmatively stopped. Beginning recognition is illustrated by block  in .","During recognition, SR engine  illustratively calls Site object  with intermittent updates. This is indicated by block . The Site object  exposes interfaces which are called by SR engine  to return these intermittent updates, to get audio data for recognition, and to return sound events and recognition information. For example, SR engine  calls the Site object to indicate when a sound has begun, and when it has ended. The SR engine  also calls Site to provide the current position of recognition in the input stream, such as by calling the UpdateRecoPos method. SR engine  can also call the Synchronize method to process changes in the state of its active grammar. In other words, the application may have changed the state of the active grammar in the SpRecoGrammar object being used by SR engine  during recognition. Therefore, SR engine  periodically calls Synchronize to stop processing and update the state of its active grammar. This can be done by obtaining word, rule, and state transition information for CFG rules, words and transitions in the SpRecoGrammar object . It does this, for example, by calling the GetRuleInfo, GetWordInfo, and GetStateInfo methods on the Site object.","SR engine  also illustratively calls Site  when either a recognition hypothesis or an actual final recognition has been obtained, by calling the Recognition method and either setting or resetting a hypothesis flag contained in the input parameters for the method. Once the final result is obtained, it is returned to Site  by calling the Recognition method and indicating that data is available, and by having the hypothesis flag reset. This is indicated by block  in . Of course, it should also be noted that where alternatives are requested, SR engine  passes those alternatives back to Site  along with the result.","Once Site contains the information indicating a final recognition, CFG engine  creates a complex result from the recognition information. The application  can then obtain the recognition result by calling the SpRecoResult object  or an associated SpPhrase object (not shown). For example, on the SpPhrase object, the application can call the GetPhrase or GetText methods which retrieve data elements associated with the phrase. The application can also obtain elements associated with alternatives and replace the original phrase with the alternatives by calling the GetAltInfo method and the Commit method, respectively","One illustrative data structure which identifies a recognized result is as follows:",{"@attributes":{"id":"p-0098","num":"0097"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"SPPHRASE"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"Typedef [restricted] struct SPPHRASE"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"119pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"98pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["ULONG","cbSize;"]},{"entry":["LANGID","LangID;"]},{"entry":["WORD","wReserved;"]},{"entry":["ULONGLONG","ftStartTime;"]},{"entry":["ULONGLONG","ullAudioStreamPosition;"]},{"entry":["ULONG","ulAudioSizeBytes;"]},{"entry":["ULONG","ulAudioSizeTime;"]},{"entry":["SPPHRASERULE","Rule;"]},{"entry":["const SPPHRASEPROPERTY","*pProperties;"]},{"entry":["const SPHRASEELMENT","*pElements;"]},{"entry":["ULONG","cReplacements;"]},{"entry":["const SPPHRASEREPLACEMENT","pReplacements;"]},{"entry":["GUID","SREngineID;"]},{"entry":["ULONG","ulSREnginePrivateDataSize;"]},{"entry":["const BYE","*pSREnginePrivateData;"]},{"entry":"SPPHRASE"},{"entry":{"@attributes":{"namest":"1","nameend":"2","align":"center","rowsep":"1"}}}]}}]}},"br":{},"ul":{"@attributes":{"id":"ul0001","list-style":"none"},"li":["CbSize\u2014The size of this structure in bytes.","LangID\u2014The language ID of the current language.","WReserved\u2014Reserved for future use.","FtStart Time\u2014The start time of the recognition in the input stream.","UllAudioStreamPosition\u2014The start position of the recognition in the input stream.","UlAudioSizeBytes\u2014The size of audio information.","UlAudioSizeTime\u2014The time of audio information.","Rule\u2014The rule that spawned this result.","pProperties\u2014The pointer to the semantic properties for the rule that spawned this result.","PElements\u2014The pointer to the elements of the result.","pReplacements\u2014The pointer to the replacement elements.","SREngineID\u2014The ID of the SR engine which produced the results.","UlSREnginePrivateDataSize\u2014The size of any proprietary data sent by the SR engine.","PSREnginePrivateData\u2014The pointer to the proprietary data."]}},"Application  can also set book marks in the audio stream to be recognized. For example, the application  may desire a bookmark so that it can note cursor position when the user clicks the mouse, as this event is temporally related to the audio stream. Therefore, the application calls the Bookmark method exposed by the SpRecoContext object to set a bookmark within the current recognition stream. Because SR engine  in intermittently calling Site  with updates as to its position within the recognition steam, the SpRecoContext object  can determine when the SR engine  has reached the bookmark. When this happens, an event  is added to the event queue which is communicated back to application . This allows application  to coordinate its state with events coming back from SR engine .","This can be quite useful in speech recognition applications. For example, a user manipulation of the mouse can change the state of the application. However, prior to actually changing the state of the application, the application may wish to wait until SR engine  has reached the same temporal point in the recognition stream. This allows the application to synchronize with SR engine  exactly where the application desires to take action.",{"@attributes":{"id":"p-0101","num":"0114"},"figref":"FIG. 13","b":["202","424","502","424","504","206","506","424","202","508"]},"Application  can also cause SR engine  to pause and synchronize with it in another way.  is a flow diagram which better illustrates this. Application program  calls a method (such as the Pause method) exposed by the SpRecoContext object  to stop SR engine  for synchronization. This is indicated by block . On the next call from the SR engine  to Site, the SpRecoContext object  does not return on that call to the SR engine  until the SR application  has said to resume recognition. This is indicated by block . At that time, the application can do necessary work in updating the state of the active grammar or loading another grammar to be used by SR engine  as indicated by block . During the pause mode, the SR engine  still calls the sync method exposed by Site , and asks it for updates to its active grammar as discussed above. This is indicated by block . After the synchronization has been completed, the SpRecoContext object  returns to the application  and the application calls Resume on SpRecoContext object . This is indicated by block . In response, SpRecoContext object  returns on the SR engine call so that the SR engine can continue processing.",{"@attributes":{"id":"p-0103","num":"0116"},"figref":"FIG. 15","b":["206","202","426","206","206","202"]},"Therefore, SR engine  first returns a result to Site . This is indicated by block . The SpRecoContext object  calls Site  to find that the rule which fired to spawn the recognition is an autopause rule. This is indicated by block . The SpRecoContext object  then notifies application  and does not return on the SR engine  at that time. This effectively pauses SR engine , and audio input is buffered in the meantime. This is indicated by block .","During this pause state, application  updates the grammar rules, words, transitions, etc., as desired. This is indicated by block . Because a recognition event is also a synchronize event, SR engine  still calls Site  while in the pause mode. This is indicated by block . Thus, the SR engine obtains the updated state of its active grammar.","The application  then calls Resume on SpRecoContext object , as indicated by block . The SpRecoContext object then returns on the recognition call from SR engine , allowing SR engine  to continue recognition. This is indicated by block .",{"@attributes":{"id":"p-0107","num":"0120"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0108","num":"0121"},"figref":["FIG. 16","FIG. 11","FIG. 16","FIG. 16","FIG. 16"],"b":["202","202","422","206","600"]},{"@attributes":{"id":"p-0109","num":"0122"},"figref":["FIG. 17","FIG. 11","FIG. 17"],"b":["422","206","206","602","428","604","606"]},"SR engine  then returns preliminary information (such as its position within the recognition stream, when sound has been heard and has ended, and hypotheses). This is indicated by block . The SAPI server  notifies all applications A and B, which are currently operating, of the events returned by SR engine . SAPI server  illustratively does this through the RecoContext objects associated with the applications. This is indicated by block .","SR engine  then returns a result by calling the Recognition method exposed by Site . This is indicated by block . SAPI server  then determines whether it is a hypothesis (e.g., a preliminary result) by examining the hypothesis bit in the result returned by SR engine . This is indicated by block . If it is a hypothesis, then SAPI server  sends a global notification to all SpRecoContext objects that a hypothesis result has been received, and waits for a finally recognized result. This is indicated by block  and .","If, at block , it is determined that the result is final, then SAPI server  sends a global notification to all SpRecoContext objects indicating that a final result has been received. This is indicated by .","To better understand the remaining process, a brief discussion of CFG engine  may be helpful. The operation of CFG engine  is described in greater detail in U.S. Pat. No. 6,957,184 referred to above. Briefly, for the sake of completeness, CFG engine  combines all grammars from all applications and RecoContext objects and combines them into a single set of grammars which is communicated to SR engine . Therefore, the single SR engine  only sees a large collection of words, rules, and transitions which it is to recognize. In maintaining the collection of grammars, CFG engine  maintains an indication as to where the grammars came from (i.e., which process they came from).","Recall that when SR engine  returns its results, it indicates the rule which fired to spawn the result. Therefore, by examining the rule identifier (or rule name) that fired to spawn the result, CFG engine  can identify the particular SpRecoGrammar object which the rule came from. The CFG engine  can then call methods exposed by that SpRecoGrammar object to obtain the SpRecoContext object associated with that grammar (such as by calling the GetRecoContext method). Identifying the grammar which the rule came from, and identifying the SpRecoContext object associated with that grammar is indicated by blocks  and , respectively.","This information is passed to SAPI server , which in turn notifies the SpRecoContext object associated with that grammar. The notification indicates that its result has been recognized. That SpRecoContext object can then notify its application and pass the recognition event on to the application, as indicated by block .","In conclusion, it can be seen that the middleware layer between the applications and engines provides many services for both the applications and engines, which had previously been performed by either the application or the engine. The present middleware layer does this in an application-independent and engine-independent manner.","Although the present invention has been described with reference to preferred embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"],"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":["FIG. 3","FIG. 2"]},{"@attributes":{"id":"p-0016","num":"0015"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 7","FIG. 6"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 10","FIG. 9"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 11","FIG. 2"],"b":"210"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 12","FIG. 11"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIGS. 14 and 15"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 17"}]},"DETDESC":[{},{}]}
