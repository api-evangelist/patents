---
title: Cartoon personalization
abstract: Embodiments that provide cartoon personalization are disclosed. In accordance with one embodiment, cartoon personalization includes selecting a face image having a pose orientation that substantially matches an original pose orientation of a character in a cartoon image. The method also includes replacing a face of the character in the cartoon image with the face image. The method further includes blending the face image with a remainder of the character in the cartoon image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08831379&OS=08831379&RS=08831379
owner: Microsoft Corporation
number: 08831379
owner_city: Redmond
owner_country: US
publication_date: 20080828
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["PRIORITY CLAIM","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["This application claims priority to U.S. Provisional Patent Application No. 61\/042,705 to Fang et al, entitled \u201cCartoon Personalization System\u201d, filed Apr. 4, 2008 and incorporated herein by reference.","Computer users are generally attracted to the idea of personal identities in the digital world. For example, computer users may send digital greeting cards with personalized texts and messages. Computer users may also interact with other users online via personalized avatars, which are graphical representations of the computer users. Thus, there may be growing interest on the part of computer users in other ways of personalizing their digital identities.","This Summary is provided to introduce a selection of concepts in a simplified form that is further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","Described herein are embodiments of various technologies for integrating facial features from a photographic image into a cartoon image. This integration process is also referred to as cartoon personalization. Cartoon personalization may enable the creation of cartoon caricatures of real individuals. Generally speaking, the process of cartoon personalization involves the selection of a photographic image and replacing facial features of a cartoon character present in the cartoon image with the facial features from the photographic image.","In one embodiment, cartoon personalization includes selecting a face image having a pose orientation that substantially matches an original pose orientation of a character in a cartoon image. The method also includes replacing a portion of the character in the cartoon image with the face image. The method further includes blending the face image with a remainder of the character in the cartoon image. Other embodiments will become more apparent from the following detailed description when taken in conjunction with the accompanying drawings.","This disclosure is directed to embodiments that enable personalization of a cartoon image with features from real individuals. Generally speaking, cartoon personalization is the process in which facial features of a cartoon character in a cartoon image are replaced with the facial features of a person, as captured in a photographic portrait. Nevertheless, the process may also involve the replacement of other parts of the cartoon image with portions of photographs. For example, additional content of a cartoon image, such as, but not limited to, bodily features, clothes, cars, or even family pets, may be replaced with similar content captured in actual photographs. The integration of a cartoon image and a photograph may enable the generation of custom greeting material and other novelty items.","The embodiments described herein are directed to technologies for achieving cartoon personalization As described, the cartoon personalization mechanisms may enable the automatic or semi-automatic selection of one or more candidate photographs that contains the desired content from a plurality of photographs, dynamic processing of the selected candidate photograph for the seamless composition of the desired content into a cartoon image, and integration of the desired content with the cartoon image. In this way, the embodiments described herein may reduce or eliminate the need to create cartoon personalization by using manual techniques such as, but not limited to, cropping, adjusting, rotating, and coloring various images using graphics editing software. Thus, the embodiments described herein enable the creation of cartoon personalization by consumers with little or no graphical or artistic experience and ability. Various examples of cartoon personalization in accordance with the embodiments are described below with reference to .","Exemplary Scheme",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1","b":["100","100","104","102","102","102","104","104","104"]},"The cartoon personalization system  may receive one or more source cartoon images  from cartoon providers . Cartoon providers  may include professional and amateur graphic artists and animators. The one or more source cartoon images  are generally artistically created pictures that depict cartoon characters or subjects in various artificial sceneries and backgrounds. In various embodiments, the cartoon image  may be a single image, or a still image from a video clip.","In the exemplary system , the source images  and the source cartoon images  may be transferred to a cartoon personalization engine  via one or more networks . The one or more networks  may include wide-area networks (WANs), local area networks (LANs), or other network architectures. However, in other embodiments, at least one of the source images  or the source cartoon images  may also reside within a memory of the cartoon personalization engine . Accordingly, in these embodiments, the cartoon personalization engine  may access at least one of the source images  or the source cartoon images  without using the one or more networks  .","The cartoon personalization engine  is generally configured to integrate the source images  with the cartoon images . According to various embodiments, the cartoon personalization engine  may replace the facial features of a cartoon character from a cartoon image  with a face image from a source image . The integration of content from the source image  and the cartoon image  may produce a personalized cartoon .","Exemplary Components",{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 2","b":["110","110","202","204","204"]},"The memory  may store program instructions. The program instructions, or modules, may include routines, programs, objects, components, and data structures that perform particular tasks or implement particular abstract data types. The selected program instructions may include a pre-processor module , a selection module , a fitter module , a user interface module , and a data storage module .","In turn, the pre-processor module  may include a face detection engine , a face alignment engine , and face tagging engine . The selection module  may include a face pose engine  and a face filter engine . The fitter module  may include a geometry engine , and a blending engine . In some embodiments, the exemplary cartoon personalization engine  may include other components associated with the proper operation of the engine.","The face detection engine  may implement various techniques to detect images of faces in photographs. In various embodiments, the face detection engine  may implement an \u201cintegral image\u201d technique for the detection of faces in photographs.","Integral Image technique","The aforementioned \u201cintegral image\u201d technique implemented by face detection engine  includes the use of \u201cintegral images\u201d that enables the computation of features for face detection. The \u201cintegral image\u201d technique also involves the use of a met-algorithm known as the AdaBoost learning algorithm to select a small number of critical visual features from a very large set of potential features for the detection of faces. The \u201cintegral image\u201d technique may be further implemented by combining classifiers in a \u201ccascade\u201d to allow background regions of the photographic images that contain the faces to be quickly discarded so that the face detection engine  may spend more computation on promising face-like regions.","In some embodiments, the \u201cintegral image\u201d technique may include the computation of rectangular features. For example, the integral image at location x, y contains the sum of the pixels above and to the left of x, y, inclusive:\n\n()=\u03a3() \u2003\u2003(1)\n\nwhere ii(x, y) is an integral image and i(x, y) is an original image. Using the following pair of recurrences:\n\n()=(1)+() \u2003\u2003(2)\n\n()=(1, )+() \u2003\u2003(3)\n\nwhere s(x, y) is the cumulative row sum, s(x, \u22121)=0, and ii (\u22121, y)=0, the integral image may be computed in one pass over the original image. Using the integral image any rectangular sum may be computed in four array references. Moreover, the difference between two rectangular sums may be computed in eight references. Since the two-rectangle features, as defined above, involve adjacent rectangular sums they can be computed in six array references, eight in the case of the three-rectangle features, and nine for four-rectangle features.\n","The implementation of the AdaBoost algorithm may involve the restriction of a weak learner, that is, a simple learning algorithm, to a set of classification functions where each function depends on a single feature. Accordingly, the weak learner may be designed to select the single rectangle feature which best separates the positive and negative examples. For each feature, the weak learner may determine the optimal threshold classification function, such that the minimum number of examples that are misclassified. Thus, a weak classifier (h(x, f, p, \u03b8)) may consist of a feature (f), a threshold (\u03b8), and a polarity (p) indicating the direction of the inequality:",{"@attributes":{"id":"p-0029","num":"0028"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","f","p","\u03b8"],"mo":[",",",",","]}}},{"mo":"{","mtable":{"mtr":[{"mtd":{"mrow":{"mn":"1","mo":",","mrow":{"mrow":[{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"mrow":{"mi":"pf","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}}},{"mi":["p","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"<"}}}},{"mtd":{"mrow":{"mn":"0","mo":",","mrow":{"mi":"otherwise","mo":"."}}}}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{}},"Furthermore, the weak classifiers may be implemented in a cascade in an algorithm to achieve increased detection performance while reducing computation time. In this algorithm, simpler classifiers may be used to reject the majority of sub-windows before more complex classifiers are called upon to achieve low false positive rates. Stages in the cascade may be constructed by training classifiers using the AdaBoost learning algorithm. For example, starting with a two-feature strong classifier, an effective face filter can be obtained by adjusting the strong classifier threshold to minimize false negatives. The initial AdaBoost threshold,",{"@attributes":{"id":"p-0031","num":"0030"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"T"},"mo":"\u2062","msub":{"mi":["\u03b1","t"]}}},"mo":","}}},"br":{}},"Accordingly, the overall form of the face detection process is that of a degenerate decision tree, also referred to as a \u201ccascade.\u201d A positive result from the first classifier triggers the evaluation of a second classifier which has also been adjusted to achieve very high detection rates. A positive result from the second classifier triggers a third classifier, and so on. A negative outcome at any point leads to the immediate rejection of the sub-window.","Multiview Technique","In other embodiments, the face detection engine  may employ a \u201cmultiview\u201d technique for the detection of faces in photographs. The \u201cmultiview\u201d technique includes the use of a three-step face detection approach, in combination with a two-level hierarchy in-plane pose estimator, to detect photographs containing facial features.","The three-step detection approach may include a first step of linear filtering. Linear filtering, as implemented in the multiview technique, may include the use of the AdaBoost algorithm previous described. For example, given (x, y) . . . , (x, yas the training set, where y\u03b5 {\u22121, +1} is the class label associated with example x, the decision function may be the following:\n\n()=(()>)\u039b((()+())>) \u2003\u2003(5)\n\nwhere \u03b1, b, and r \u03b5 {\u22121, 1} are the coefficients which could be determined during the learning procedure. The first term in decision function (5) is a simple decision stump function, which may be learned by adjusting threshold according to the face\/non-face histograms of this feature. The parameters in the second term could be acquired by a linear support vector machine (SVM). The target recall could be achieved by adjusting bias terms bin both terms.\n","The three-step detection approach may also include a second step of implementing a boosting cascade. During the face detection training procedure, windows which are falsely detected as faces by the initial classifier are processed by successive classifiers. This structure may dramatically increase the speed of the detector by focusing attention on promising regions of the image. In some embodiments, efficiency of the boosting cascade may be increased using a boosting chain. In each layer of the boosting cascade, a classifier is adjusted to a very high recall ratio to preserve the overall recall ratio. For example, for a 20-layer cascade, to anticipate overall detection rates at 96% in the training set, the recall rate in each single layer may be 99.8% (\u221a{square root over (0.96=0.998)}) on average. However, such a high recall rate at each layer may result in decreasing sharp precision. Accordingly, a chain structure of boosting cascades may be implemented to remedy the decreasing sharp precision.","Further, during each step of the boosting chain, false rates may be reduced by optimization via the use of a linear SVM algorithm. The optimization is obtained by the linear SVM algorithm that resolves the following quadratic programming problem:",{"@attributes":{"id":"p-0037","num":"0036"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"Maximize","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}}],"mrow":{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"\u03b2"}}},{"mrow":[{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":"\u2062","msub":{"mi":["\u03b2","i"]}},{"mfrac":{"mn":["1","2"]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":",","mrow":{"mi":"j","mo":"=","mn":"1"}},"mi":"n"},"mo":"\u2062","mrow":{"msub":[{"mi":["\u03b2","i"]},{"mi":["\u03b2","j"]},{"mi":["y","i"]}],"mo":["\u2062","\u2062","\u2062"],"mrow":{"msub":{"mi":["y","j"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","j"]}}}],"mo":"\u00b7"}}}}}}],"mo":"-"}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}},"br":{}},{"@attributes":{"id":"p-0038","num":"0037"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mrow":[{"munderover":{"mo":"\u2211","mi":["i","n"]},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":["\u03b2","i"]},{"mi":["y","i"]}],"mo":"\u2062"}},{"mrow":{"mn":"0","mo":["\u2062","\u2062","\u2062","\u2062"],"mrow":{"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mo":"\u2062"},"mi":"and","mstyle":{"mspace":{"@attributes":{"width":"1.1em","height":"1.1ex"}}},"msub":{"mi":["C","i"]}},"mo":["\u2265","\u2265"],"msub":{"mi":["\u03b2","i"]},"mn":"0"}],"mo":"="},{"mi":"i","mo":"=","mn":"1"},{"mi":"n","mo":"."}],"mo":[",",",","\u2062",","],"mi":"\u2026","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}}},"br":{},"sub":"i "},{"@attributes":{"id":"p-0039","num":"0038"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["C","i"]},"mo":"=","mrow":{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mi":["\u03c9","C"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":","}},{"mrow":{"mi":["if","is","a","face","pattern"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"msub":{"mi":["x","i"]}}}]},{"mtd":[{"mrow":{"mi":"C","mo":","}},{"mrow":{"mi":"otherwise","mo":"."}}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"7"}}]}}}},"br":{},"sup":["0","0","0 ","0"],"sub":["1","2","n","t "]},{"@attributes":{"id":"p-0040","num":"0039"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["\u03b1","t"]},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"n"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"msub":[{"mi":["\u03b2","i"]},{"mi":["y","i"]}],"mo":["\u2062","\u2062"],"mrow":{"mrow":{"msub":{"mi":["h","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","i"]}}},"mo":"."}}}}}},"br":{}},"The three-step detection system may also include a third step of post filtering. The posting filtering may include image processing and color filtering. Image processing may alleviate background, lighting, and contrast variations. It consists of three steps. First, a mask, which is generated by cropping out the four edge corner from a window, is applied to the candidate region. Second, a linear function may be selected to estimate the intensity distribution on the current window. By subtracting the plane generated by this linear function, the lighting variations could be significantly reduced. Finally, histogram equalization is performed. With this nonlinearly mapping, the range of pixel intensities is enlarged and thus improves the contrast variance which caused by camera input difference.","Color filtering may be achieved by adopting a YCCspace, where the Y mainly represents image grayscale information that is relevant to skintone color, and the CCcomponents are used for false alarm removal. Since the color of face and non-face images is distributed as nearly Gaussian in the CCspace, a two-degree polynominal function may be used as an effective function. For any point(c, c) in the space(C, C), the decision function can be written as:\n\n()=sign(\u2003\u2003(8)\n\nwhich is a linear function in the feature space with dimension (c, cc, ccc). Consequently, a linear SVM classifier is constructed in this five-dimensional space to separate skin tone color from the non-skin tone color.\n","For each face training sample, the classifier F(c, c) may be applied to each pixel of face image. Statistics results can therefore be collected, where the grayscale value of each pixels corresponding to its ratio to be skin tone color in the training set. Thus, the darker the pixel, the less possible it may be a skin tone color.","In some embodiments, the three-step face detection may be further combined with a two-level hierarchy in-plane pose estimator to detect photographs containing facial features. The two-level hierarchy in-plane pose estimator may include an in-plane orientation detector to determine the in-plane orientation of a face in an image with respect to the upright position. The pose estimator may further include an upright face detector this is capable of handling out-plane rotation variations in the range of \u0398=[\u221245\u00b0, 45\u00b0].","The in-plane pose estimator includes the division of \u03a6 into three sub ranges, \u03a6=[\u221245\u00b0, \u221215\u00b0], \u03a6=[\u221215\u00b0, \u221215\u00b0], \u03a6=[15\u00b0, 45\u00b0]. Further, the input image is in-plane rotated by \u00b130\u00b0. In this way, there are totally three images including the original image, and each corresponds to one of the three sub ranges, respectively. Third, in-plane orientation of each window on the original image is estimated. Finally, based on the in-plane orientation estimation, the upright multiview detector is applied to the estimated sub range at the corresponding location. In various embodiments, the pose estimator may adopt the coarse-to-fine strategy. The full range of in-plane rotation is first divided into two channels, and each one covers the range of [\u221245\u00b0, 0\u00b0] and [0\u00b0, 45\u00b0]. In this step, only one Haar-like feature is used and results in the prediction accuracy of 99.1%. Subsequently, a finer prediction based on AdaBoost classifier with six Haar-like features may be performed in each channel to obtain the final prediction of the sub range.","It will be appreciated that while some of the face detection techniques implemented by the face detection engine  have been discussed, the face detection engine  may employ other techniques to detect face portions of photographic images. Accordingly, the above discussed face detection techniques are examples rather than limitations.","The face alignment engine  may be configured to obtain landmark points for each detected face from a photographic image. For example, in one embodiment, the face alignment engine  may obtain up to 87 landmarks for each detected face. These landmarks may include 19 landmark points for a profile, 10 landmark points for each brow, eight landmarks for each eye, 12 landmarks for a nose, 12 landmark points for outer lips, and 8 for inner lips.","In various embodiments, the face alignment engine  may make use of the tangent shape approximated algorithm referred to as the Bayesian Tangent Shape Model (BTSM). BTSM makes use of a likelihood, P(y|x, \u03b8), that is a probability distribution of the grey levels conditional on the underlying shape, where \u03b8 represents the pose parameters, x represents the tangent shape vector, and y represents the observed shape vector. Assume yis the shape estimated in the last iteration, by updating each landmarks of ywith its local texture, y, the observed shape vector may be obtained. The distance between observed shape y and the rue shape may be modeled as an adaptive Gaussian as:\n\n\u2003\u2003(9)\n\nwhere y is the observed shape vector, s is the scale parameter. Moreover,\n",{"@attributes":{"id":"p-0049","num":"0048"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msub":{"mi":["U","\u03b8"]},"mo":"=","mrow":{"msub":{"mi":["I","N"]},"mo":"\u2297","mrow":{"mo":["(",")"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mrow":{"mrow":{"mo":"-","mi":"sin"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"\u03b8"}}]},{"mtd":[{"mrow":{"mi":["sin","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mrow":{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}]}]}}}},"mo":","}}},"br":{}},{"@attributes":{"id":"p-0050","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"c","mo":"=","mrow":{"msub":{"mi":["I","N"]},"mo":"\u2297","mrow":{"mo":["(",")"],"mfrac":{"msub":[{"mi":"c","mn":"1"},{"mi":"c","mn":"2"}]}}}},"mo":","}}},"br":{},"sup":["2","2","old","2","old "],"sub":"2n"},"Subsequently, the posterior of model parameters (b,s,c,\u03b8) may be computed given the observed shape of y, where b represents the shape parameters. If the tangent shape x is known, the face alignment engine  may implement an expectation-maximization (EM) based parameters estimation algorithm.","For example, given a set of complete data {x,y}, the complete posterior of the model parameters is a product of the following two distributions:\n\n()\u221d exp{\u22121\/2\u039b\u2003\u2003(10)\n\n(\u03b3|)\u221d exp{\u22121\/2[\u03c1\u2003\u2003(11)\n\nwhere X=(x,x*,e,e*) and \u03b3=(s\u00b7sin \u03b8, s\u00b7sin \u03b8, c, c). Accordingly, by taking the logarithm and the conditional expectation, the following equation may be obtained:\n",{"@attributes":{"id":"p-0053","num":"0052"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"Q","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u03b3","mo":"\u2758","msub":{"mi":["\u03b3","old"]}}}},{"mo":"-","mrow":{"mfrac":{"mn":["1","2"]},"mo":"[","mrow":{"mrow":{"mo":["[","]"],"mrow":{"mrow":[{"msup":[{"mi":["b","T"]},{"mi":"\u039b","mrow":{"mo":"-","mn":"1"}}],"mo":["\u2062","\u2062"],"mi":"b"},{"msup":{"mi":"\u03c3","mrow":{"mo":"-","mn":"2"}},"mo":"\u2062","mrow":{"mo":["\u2329","\u232a"],"msup":{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":["x","\u03bc"],"mo":["-","-"],"mrow":{"msub":{"mi":["\u03a6","r"]},"mo":"\u2062","mi":"b"}}},"mn":"2"}}},{"mo":["\u2329","\u232a"],"mrow":{"msup":[{"mi":"\u03c1","mrow":{"mo":"-","mn":"2"}},{"mrow":{"mo":["\uf605","\uf606"],"mrow":{"mi":"y","mo":"-","mrow":{"mi":["X","\u03b3"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},"mn":"2"}],"mo":"\u2062"}}],"mo":["+","+"]}},"mo":"+","mi":"const"}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}},"br":[{},{},{},{}],"sup":["2 ","T","\u22121","2","2","2 ","2","2","\u22122","2","2","\u22122","2","\u22122","\u22121"],"in-line-formulae":[{},{},{},{}],"img":[{"@attributes":{"id":"CUSTOM-CHARACTER-00001","he":"3.13mm","wi":"0.68mm","file":"US08831379-20140909-P00001.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00002","he":"3.13mm","wi":"0.68mm","file":"US08831379-20140909-P00002.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00003","he":"3.13mm","wi":"0.68mm","file":"US08831379-20140909-P00003.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00004","he":"3.13mm","wi":"0.68mm","file":"US08831379-20140909-P00004.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00005","he":"3.13mm","wi":"0.68mm","file":"US08831379-20140909-P00005.TIF","alt":"custom character","img-content":"character","img-format":"tif"}},{"@attributes":{"id":"CUSTOM-CHARACTER-00006","he":"3.13mm","wi":"0.68mm","file":"US08831379-20140909-P00006.TIF","alt":"custom character","img-content":"character","img-format":"tif"}}],"i":["x","\u2212p","b+p\u03a6\u03a6","T","y","x\u2225","=\u2225","x","\u2225","N\u2212"],"sub":"\u03b8"},"Subsequently, the face alignment engine  may maximize the Q-function over model parameters. The computation of the derivations of the Q-function may provide:",{"@attributes":{"id":"p-0055","num":"0054"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mover":{"mi":"b","mo":"~"},"mo":"=","mrow":{"mrow":[{"msup":{"mrow":[{"mi":"\u039b","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u039b","mo":"+","msup":{"mi":"\u03c3","mn":"2"}}}},{"mo":"-","mn":"1"}]},"mo":"\u2062","mrow":{"msubsup":{"mi":["\u03a6","r","T"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mo":["\u2329","\u232a"],"mi":"x"},"mo":"-","mi":"\u03bc"}}}},{"msup":{"mrow":[{"mi":"\u039b","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"\u039b","mo":"+","msup":{"mi":"\u03c3","mn":"2"}}}},{"mo":"-","mn":"1"}]},"mo":["\u2062","\u2062"],"msubsup":{"mi":["\u03a6","r","T"]},"mrow":{"mo":["\u2329","\u232a"],"mi":"x"}}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"15"}}]},{"mtd":[{"mrow":{"mover":{"mi":"\u03b3","mo":"~"},"mo":"=","mrow":{"mo":["(",")"],"mrow":{"mfrac":[{"mrow":{"msup":{"mi":["y","T"]},"mo":"\u2062","mrow":{"mo":["\u2329","\u232a"],"mi":"x"}},"msup":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"mo":["\uf605","\uf606"],"mi":"x"}},"mn":"2"}},{"mrow":{"msup":[{"mi":["y","T"]},{"mrow":{"mo":["\u2329","\u232a"],"mi":"x"},"mo":"*"}],"mo":"\u2062"},"msup":{"mrow":{"mo":["\u2329","\u232a"],"mrow":{"mo":["\uf605","\uf606"],"mi":"x"}},"mn":"2"}}],"mo":[",",",",","],"mrow":[{"mfrac":{"mn":"1","mi":"N"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","msub":{"mi":"y","mrow":{"mn":"1","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"i"}}}},{"mfrac":{"mn":"1","mi":"N"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"i","mo":"=","mn":"1"},"mi":"N"},"mo":"\u2062","msub":{"mi":"y","mrow":{"mn":"2","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"i"}}}}]}}}},{"mrow":{"mo":["(",")"],"mn":"16"}}]}]}}},"br":[{},{}],"in-line-formulae":[{},{}],"i":["s","a ","{tilde over (c)}"],"sub":["1","1","1","2","3","4"],"sup":["2","2","T "]},"It will be appreciated that while some of the face alignment techniques implemented by the face alignment engine  has been discussed, the face alignment engine  may employ other techniques to obtain landmark points for each detected face. Accordingly, the above discussed face alignment techniques are examples rather than limitations.","The face tagging engine  may be configured facilitate the selection of detected and aligned faces from photographic images by grouping similar face images together. For example, the face tagging engine  may group face images of the same individual, as present in the different photographic images, into one group for identification and labeling.","In various embodiments, the face tagging engine  may use a three-stage technique to group similar face images. The stages may include offline pre-processing, online clustering, and online interactive labeling.","In the pre-processing stages, facial features from detected and aligned face images may be extracted. The extraction may be based on two techniques. For example, the facial features may be extracted from various face images using the local binary pattern (LBP) feature, a widely used feature for face recognition. In another example, facial feature extraction may be based on the recognition of scene features, as well as cloth contexture features and Color Correlogram features extracted from the human body areas present in the photographic images.","In the online clustering stage, the face tagging engine  may apply a spectral clustering algorithm that handles noise data to the LBP features, scene features, and cloth features to group similar face images into groups. In some embodiments, the face tagging engine may create a plurality of groups for the same person to account for face diversity.","In the online interactive stage, the face tagging engine  may ranking the face images in each group according to a confidence that each face image belongs in the group. For example, the face tagging engine  may use a first clustering algorithm that reorders the faces so that the faces are ordered from the most confident to the least confidence in a group. In another example, the face tagging engine  may use a second clustering algorithm that orders different groups of clustered face images according to confidence level.","The face tagging engine  may interact with the user interface module . In some embodiments, the face tagging engine  may have the ability to dynamically rearrange the order of face images and\/or the groups of face images according to user input provided via the user interface module . The face tagging engine  may implement the rearrangement using methods such as linear discriminate analysis, support vector machine (SVM), or simple nearest-neighbor. Moreover, the face tagging engine  may be further configured to enable a user to annotate each face image and\/or each group of face images with labels to facilitate subsequent retrieval or interaction.","It will be appreciated that while some of the face grouping and ranking techniques implemented by the face tagging engine  has been discussed, the face tagging engine  may employ other techniques to create and rank groups of similar face images. Accordingly, the above discussed face tagging techniques are examples rather than limitations.","The face pose engine  may be configured to determine the pose orientation, also commonly referred to as face orientation, of the face images. Pose orientation is an important feature in face selection. For instance, if the pose of the selected face is distinct from that of the cartoon image, the synthesized result may look unnatural.","In order to calculate the pose orientation, the face pose engine  may first estimate the symmetric axis in a 2D face plane of the face image. Suppose \u03b8 is the in-plane rotation of a face and C is the shift parameter, then \u03b8 and C may be estimated by optimizing the following symmetric criteria:\n\n({circumflex over (\u03b8)}, {circumflex over ()},=argmin\u03a3|=(cos \u03b8, sin \u03b8, )\u2003\u2003(18)\n\nwhere L is the set of landmarks in the left nose and mouth, p=(x, y, 1)is a point in L, and p\u2032is the corresponding point of pin the right part of the face. The points in the nose and mouth to estimate the symmetric axis may be especially ideal for the calculation of pose orientation as points far away from the symmetric axis tend increase the amount of effect caused by 3D rotation.\n","Following the calculation of ({circumflex over (\u03b8)}, \u0108) from criteria (18), the distance between the symmetric axis and points in the left and right face are calculated as:\n\n=\u03a3=\u03a3, \u2003\u2003(19)\n\nwhere \u00fb=(cos {circumflex over (\u03b8)},sin {circumflex over (\u03b8)}, \u0108), and L\u2032 and R\u2032 are the set of points excluding nose and mouth in left and right respectively. The final pose orientation may be calculated as:\n",{"@attributes":{"id":"p-0067","num":"0066"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mi":"o","mo":"=","mfrac":{"msub":[{"mo":"\u2146","mi":"r"},{"mo":"\u2146","mi":"l"}]}}},{"mrow":{"mo":["(",")"],"mn":"20"}}]}}}},"br":{},"b":["222","222","214","222","212"]},"The face filter engine  may be configured to filter out face images that are not suitable for transference to a cartoon image. In various embodiments, the face filter engine  may filter out face images that exceed predetermined brightness and\/or darkness thresholds. Faces that exceed the predetermined brightness and\/or darkness thresholds may not be suitable for integration into a cartoon image. For example, when a face from a photographic image that is too dark is inserted into the head of a character in a cartoon image, the face may cause artifacts. Moreover, since faces with size much smaller than that of the cartoon template may cause blurring in the synthesis result, the face filter engine  may also filter out face images that are below a predetermined resolution.","In some embodiments, given that ldenotes an illumination of point i on a face image, and the mean value of illumination of all points in the face may be denoted as l, the brightness of the face may be expressed as:",{"@attributes":{"id":"p-0070","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["l","m"]},"mo":"=","mrow":{"mfrac":{"mn":"1","mi":"N"},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mi":"i","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},"mo":"\u2062","mi":"li"}}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"21"}}]}}}},"br":{},"b":"224","sub":["min ","max","min","m","max"]},"In other embodiments, given that the width and height of a cartoon image are wand h, and the width and height of the face are wand h. For a given ratio s, face filtering based on resolution may be implemented as follows: if w\/w\u2267s & h\/h\u2267s, the face filter engine  may designate the face as having an acceptable resolution. Otherwise, the face filter engine  may be filter out due to the lack of proper resolution.","The geometry engine  may be configured to geometrically fit a selected face image to a cartoon image. In other words, the geometry engine  may integrate a face image into a cartoon image by changing the appearance of the face image.","The geometry engine  may implement a two part algorithm to blend a selected face into a cartoon image. The first part may include determine the place in the cartoon image that the face should be positioned. The second part may include estimating the transformation of the face, that is, warp the face image to matched the cartoon image.","In various embodiments, the geometry engine  may estimate the affine transform matrix \u00c2 via the following equation:\n\n=argmin\u03a3|\u2003\u2003(22)\n\nwhere A is a 3\u00d73 affine matrix, M is the set of landmarks in the face image, pis a landmark in the face, and p\u2032is the corresponding point in the cartoon image, where pand p\u2032are represented in a homogenous coordinates system. After estimating the affine transformation, for a point p in the selected face, the corresponding coordinate in the cartoon image p\u2032=\u00c2p.\n","It will be appreciated that while some of the geometric fitting techniques implemented by the geometry engine  have been discussed, the geometry engine  may employ other techniques to geometrically fit a face image into a cartoon image. Accordingly, the above discussed face alignment techniques are examples rather than limitations.","The blending engine  may be configured to blend a face image and a host cartoon image following the insertion of face image into the cartoon image. To conduct appearance blending, the blending engine  may calculate a face mask indicating the region \u03a9 where the face is inserted. The face mask may be obtained by calculating a convex hull of face image landmarks. In order to accomplish appearance blending, the blending engine  may perform three operations. The first operation includes the application of shading to the integrated image. The second operation includes illumination blending, which may make the illumination appear natural in the integrated image. The third operation includes color adjustment, which adjusts the real face color to match the face color in the original cartoon image.","In various embodiments, the blending engine  may convert a color RGB image into a L*a*b* color space. The \u201cL\u201d channel is the illumination channel, and \u201ca*b*\u201d are the color channels. In these embodiments, shading application and illumination blending may be performed on the \u201cL\u201d channel, while color adjustment is performed on the \u201ca*b*\u201d channels.","Since photographs may be taken in different environments, the face images in those photographs may have different shading than the faces in cartoon images. In order to match the shading of a face image from a photograph with the shading of a face in a cartoon image, as well as make the overall integrated image look natural, the blending engine  may implement cartoon shading on the face image. In one embodiment, the blending engine  may sample the cartoon image to obtain skin color information. For example, \u201cillumination\u201d of the cartoon image, as depicted by the cartoon image creator, implies shading information, as regions with shading are darker than others. Accordingly, the blending engine may use this illumination information to provide shading to the face image.","In one embodiment, suppose that the shading of a cartoon image in the illumination channel is u. The shading includes K uniform color regions and the illumination value for each region k is u(k). Further, the illuminated image of the warped face may be denoted as u, and urepresents the illumination of point p. The blending engine  may first find all points in the face whose corresponding points belong to the same region in the cartoon skin image. Suppose that the corresponding point of p belongs to region k. Then the new illumination value of p may be set by the blending engine  as:",{"@attributes":{"id":"p-0080","num":"0079"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msubsup":{"mi":["u","p"],"mo":";"},"mo":"=","mfrac":{"mrow":[{"msub":{"mi":["u","p"]},"mo":"\u00b7","mrow":{"msup":{"mi":["u","c"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}},{"mover":{"mi":["u","_"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"k"}}]}}},{"mrow":{"mo":["(",")"],"mn":"23"}}]}}}},"br":{},"b":"228"},"The blending engine  may further provide illumination blending to an integrated image. The illumination blending may occur after shade matching as described above. Illumination blending naturally blends the illumination image u into the region \u03a9 (face mask) by keeping illumination smooth across \u03b4\u03a9, the boundary of \u03a9. The blending engine  may perform illumination blending via Poisson Blending. In one embodiment, the Poisson approach may formulate the blending as the following variational energy minimization problem:\n\n(\u2032)=min \u222b\u2003\u2003(24)\n\nwhere u is the unknown function in \u03a9, v is the vector field in \u03a9, and f is the cartoon image on \u03b4\u03a9. Generally, v is the gradient field of the image u, i.e. v=\u2207u. Further, the minimization problem may be discretized to be a sparse linear system, and the blending engine  may solve the system using the Gauss-Seidel method. In some embodiments, the blending engine  may contract the face mask so that its boundary does not coincide with the boundary of the face image.\n","The blending engine  may be further configured to perform color adjustment on the integrated image. In various embodiments, the blending engine  may determine the skin color of a face image and transform the real face colors into skin color present the cartoon image. As described above, this process is performed in \u201ca*b*\u201d color channels. To estimate the skin color of the face image, the face is clustered into K (where K=6) components via the Gaussian Mixture Model (GMM). Each component is represented as (m, v, w), where m, v, and ware the mean, variance, and weight of the kth component. Intuitively, the largest component may be the skin color component, denoted as (m, v, w), and the others are the non-skin components.","After skin component is obtained, the blending engine  may label each point in the face image via a simple Bayesian inference. The posterior of point p belonging to the skin point is:",{"@attributes":{"id":"p-0084","num":"0083"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"p","mo":"\u2208","mrow":{"mo":["{","}"],"mi":"skin"}}}},"mo":"=","mfrac":{"mrow":[{"mrow":{"mi":"G","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["m","s"]},{"mi":["v","s"]}],"mo":[",",","],"mi":"p"}}},"mo":"\u2062","msub":{"mi":["w","s"]}},{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"1"},"mi":"K"},"mo":"\u2062","mrow":{"mrow":{"mi":"G","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["m","k"]},{"mi":["v","k"]}],"mo":[",",","],"mi":"p"}}},"mo":"\u2062","msub":{"mi":["w","k"]}}}]}}},{"mrow":{"mo":["(",")"],"mn":"25"}}]}}}},"br":{},"sub":["k","k"],"b":"228"},"In one embodiment, suppose that p is a skin point, its original color is c(values in \u201ca*b*\u201d channels), and the modified color is C\u2032, then the blending engine  may implement the following simple linear transform to map the center of the skin component to the center of the skin color of the cartoon image:\n\n\u2003\u2003(26)\n\nwhere mis the mean of skin component and cis the center of the skin color of the cartoon image. The colors of non-skin points are kept unchanged.\n","It will be appreciated that while some of the image shading and blending techniques implemented by the blending engine  have been discussed, the blending engine  may employ other techniques to shade and blend the face image with the cartoon image. Accordingly, the above discussed shading and blending techniques are examples rather than limitations.","The user interface module  may be configured to enable a user to provide input to the various engines in the pre-preprocessor module , the selection module , and the fitter module . The user interface module  may interact with a user via a user interface. The user interface may include a data output device such as a display, and one or more data input devices. The data input devices may include, but are not limited to, combinations of one or more of keypads, keyboards, mouse devices, touch screens, microphones, speech recognition packages, and any other suitable devices or other electronic\/software selection methods.","In various embodiments, the user interface module  may enable a user to supply input to the face tagging engine  to group similar images together, as described above, as well other engines. The user interface module  may also enable the user to supply the minimum and maximum brightness threshold values to the face filter engine . In other embodiments, the user interface module  may enable the user to review, then accept or reject the processed results produced by the various engines at each stage of the cartoon personalization, as well as to add, select, label and\/or delete face images and groups of face images, as stored in the cartoon personalization engine . Additional details regarding the operations of the user interface module  are further described with respect to .","The data storage module  may be configured to store data in a portion of memory  (e.g., a database). In various embodiments, the data storage module  may be configured to store the source photos  and the source cartoon images . The data storage module  may also be configured to store the images derived from the source photos  and\/or source cartoon , such as any intermediary products produced by the various modules and engines of the cartoon personalization engine .",{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 3","b":["300","212","300","302","304"]},"The photo group selection interface , which may include scroll control , may be configured to enable a user to select a group of face images from a plurality of groups for possible synthesis with a cartoon image. The plurality of groups may have been organized by the face tagging engine , with input from the user. Further, the user may have provided each group with a label. For example, a first group of face images are images of a particular individual, and may be grouped under a label \u201cPerson One.\u201d Likewise, a second group of face images belong to a second individual, and may be grouped under a label \u201cPerson Two.\u201d Accordingly, the user may use the scroll control  to select the desired group of images for potential synthesis with the cartoon image, if there are more groups that are capable of being simultaneously displayed by the photo group selection interface . In turn, the display area  may be configured to display the selected face images. It will be appreciated that the scroll control  may be substituted with a suitable control that performs the same function.","The modulator interface  may include a \u201cbrightness\u201d control , a \u201cscale\u201d control , and a \u201cpose\u201d control . Each of the controls - may be the form of a slider bar, wherein different positions on the bar correspond to gradually incremented differences in settings. Nevertheless, it will be appreciated that the controls - may also be implemented in other forms, such as, but not limited to, a rotary-style control, arrow keys that manipulate numerical values, etc., so long as the control provides gradually incremented settings.","The \u201cbrightness\u201d control  may be configured to enable a user to eliminate face images that that are too dark or too bright. For example, depending on the bright and dark thresholds set using the positions on the slider bar of the control , face images falling outside the range of brightness may be excluded as candidate images for potential integration with a cartoon image. In turn, the display area  may display face images that have been selected based on the bright and dark thresholds.","The \u201cscale\u201d control  may be configured to enable a user to eliminate face images that are that are too small. Face images that are too small may need scaling during synthesis with a cartoon image, which may result in the blurring of the image due to inadequate resolution. Accordingly, depending on the particular minimum size specified using the \u201cscale\u201d control , one or more face images may be excluded. In turn, the display area  may display face images that have been selected based on the minimum size threshold.","The pose\u201d control  may be in the form of a slider bar that represents the pose orientation. Different positions of the bar correspond to different pose orientations. The operation of the \u201cpose\u201d control may be further illustrated with respect to .",{"@attributes":{"id":"p-0096","num":"0095"},"figref":"FIG. 4","b":["312","314","402","312","222","314","404","312","222","314"]},"Returning to , the user interface  may also display the cartoon image  that has been selected by a user for integration with a face image. In one embodiment, the user may select the cartoon image  via an affirmative action (e.g., clicking, dragging, etc.). Moreover, the user interface  may also display a final integrated image  that is produced from the cartoon image  and one of the face images the user selected from the display area . As shown in , the face of a character in the cartoon image  may be replaced to generate the integrated image .","Exemplary Processes",{"@attributes":{"id":"p-0098","num":"0097"},"figref":["FIGS. 5-7","FIGS. 5-7","FIG. 1"],"b":"110"},{"@attributes":{"id":"p-0099","num":"0098"},"figref":"FIG. 5","b":"500"},"At block , the cartoon personalization engine  may detect one or more face images from at least one photographic image, such as source images . In some embodiments, a face image may be detected from photographic images via an integral image technique. In other embodiments, the face images may be detected from the photographic images via a multiview technique.","At block , the cartoon personalization engine  may determine the suitability of the face images for integration with a cartoon image. In various embodiments, the cartoon personalization engine  may determine the suitability based on a determination of whether each of the face images is within acceptable illumination range. In other embodiments, suitability may be determined by the cartoon personalization engine  based on whether the image exceeds a minimum resolution threshold. In some embodiments, the upper and lower bounds of the illumination range, and\/or the minimum resolution threshold may be supplied to the cartoon personalization engine  by a user via the user interface.","At block , the cartoon personalization engine  may determine the pose orientation of each of the face images. In various embodiments, the pose orientation of a face image, also referred to as face orientation, may be determined based on landmarks extracted from the face image. In turn, the landmarks may be extracted from the face image by using a Bayesian Tangent Shape Model (BTSM).","At block , the cartoon personalization engine  may enable a user to select at least one of the detected face images as a candidate face image for integration with a cartoon image. In some embodiment, the cartoon personalization engine  may provide a user interface, such as user interface , which enables the user to select the candidate face images based on criteria such as the desired pose orientation, the desired size of the image, and the desired brightness for the candidate face images. In other embodiments, the cartoon personalization engine  may enable user to browser different groups of faces images for the selection of candidate face images. In these embodiments, the cartoon personalization engine  may have organized the groups based on the similarity between the face images. For example, face images may be organized into a group in the descending order of similarity according to user input.","At block , the cartoon personalization engine  may enable a user to choose a particular candidate face image for integration with the cartoon image. In various embodiments, the user may select the particular candidate face image via a user interface, such as the user interface .","At block , the cartoon personalization engine  may replace a face of a cartoon character in the cartoon image with the chosen face image using a transformation technique.","At block , the cartoon personalization engine  may complete the integration of face image into the cartoon image to synthesize a transformed image, such as the personalized cartoons  (), using various blending techniques.",{"@attributes":{"id":"p-0107","num":"0106"},"figref":["FIG. 6","FIG. 6"],"b":["600","512","500"]},"At block , the cartoon personalization engine  may determine a portion of the cartoon character in a cartoon image to be replaced via an affine transform. The portion of the cartoon character to be replaced may include the face of the cartoon character.","At block , the face image chosen to replace the portion of the cartoon character may be transformed via the affine transform. In various embodiments, the cartoon personalization engine  may use the affine transform to warp the face image for integration with the cartoon character.","At block , the cartoon personalization engine  may substitute the portion of the cartoon character with the transformed face image.",{"@attributes":{"id":"p-0111","num":"0110"},"figref":["FIG. 7","FIG. 7"],"b":["700","514","500"]},"At block , the cartoon personalization engine  may perform blending using illumination information of the cartoon character that is being transformed. The illumination information may include shading information, as originally implied by a graphical creator of the cartoon character through shading.","At block , the cartoon personalization engine  may perform further blending by using a Poisson blending technique.","At block , the cartoon personalization engine may adjust the color of the face image that is inserted into the cartoon character based on the original coloring of the cartoon character face.","Exemplary Computing Environment",{"@attributes":{"id":"p-0115","num":"0114"},"figref":["FIG. 8","FIG. 1","FIG. 8"],"b":["800","110","800","800","800"]},"In a very basic configuration, computing device  typically includes at least one processing unit  and system memory . Depending on the exact configuration and type of computing device, system memory  may be volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.) or some combination of the two. System memory  typically includes an operating system , one or more program modules , and may include program data . The operating system  includes a component-based framework  that supports components (including properties and events), objects, inheritance, polymorphism, reflection, and provides an object-oriented component-based application programming interface (API), such as, but by no means limited to, that of the .NET\u2122 Framework manufactured by the Microsoft Corporation, Redmond, Wash. The device  is of a very basic configuration demarcated by a dashed line . Again, a terminal may have fewer components but will interact with a computing device that may have such a basic configuration.","Computing device  may have additional features or functionality. For example, computing device  may also include additional data storage devices (removable and\/or non-removable) such as, for example, magnetic disks, optical disks, or tape. Such additional storage is illustrated in  by removable storage  and non-removable storage . Computer storage media may include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data. System memory , removable storage  and non-removable storage  are all examples of computer storage media. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computing device . Any such computer storage media may be part of device . Computing device  may also have input device(s)  such as keyboard, mouse, pen, voice input device, touch input device, etc. Output device(s)  such as a display, speakers, printer, etc. may also be included. These devices are well known in the art and are not discussed at length here.","Computing device  may also contain communication connections  that allow the device to communicate with other computing devices , such as over a network. These networks may include wired networks as well as wireless networks. Communication connections  are some examples of communication media. Communication media may typically be embodied by computer readable instructions, data structures, program modules, etc.","It is appreciated that the illustrated computing device  is only one example of a suitable device and is not intended to suggest any limitation as to the scope of use or functionality of the various embodiments described. Other well-known computing devices, systems, environments and\/or configurations that may be suitable for use with the embodiments include, but are not limited to personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-base systems, set top boxes, game consoles, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and\/or the like.","The ability to replace the facial features of a cartoon character with a face image from a photographic image may provide personalized cartoon images that are customized to suit the unique taste and personality of a computer user. Thus, embodiments in accordance with this disclosure may provide cartoon images suitable for personalized greeting and communication needs.","Conclusion","In closing, although the various embodiments have been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended representations is not necessarily limited to the specific features or acts described. Rather, the specific features and acts are disclosed as exemplary forms of implementing the claimed subject matter."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The use of the same reference number in different figures indicates similar or identical items.",{"@attributes":{"id":"p-0008","num":"0007"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 8"}]},"DETDESC":[{},{}]}
