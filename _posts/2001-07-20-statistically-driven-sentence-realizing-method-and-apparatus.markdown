---
title: Statistically driven sentence realizing method and apparatus
abstract: A method of, and system for, generating a sentence from a semantic representation maps the semantic representation to an unordered set of syntactic nodes. Simplified generation grammar rules and statistical goodness measure values from a corresponding analysis grammar are then used to create a tree structure to order the syntactic nodes. The sentence is then generated from the tree structure. The generation grammar is a simplified (context free) version of a corresponding full (context sensitive) analysis grammar. In the generation grammar, conditions on each rule are ignored except those directly related to the semantic representation. The statistical goodness measure values, which are calculated through an analysis training phase in which a corpus of example sentences is processed using the full analysis grammar, are used to guide the generation choice to prefer substructures most commonly found in a particular syntactic/semantic context during analysis.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07003445&OS=07003445&RS=07003445
owner: Microsoft Corporation
number: 07003445
owner_city: Redmond
owner_country: US
publication_date: 20010720
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS","Generation Stages","Generation Algorithm","Example Generation","Alternative Generation Algorithm: Generate All Possible Trees","Statistical Goodness Measure"],"p":["The present invention relates to natural language processing. More particularly, the present invention relates to the field of sentence realization in natural language generation.","Natural language processing can involve various different aspects, such as natural language processing and natural language generation. In processing natural languages, such as English, Hebrew and Japanese, a parser is typically used to analyze sentences. To conduct the analysis, parsers utilize extensive analysis grammars developed for the task. Analysis grammars are sets of grammar rules which attempt to codify and interpret the actual grammar rules of a particular natural language, such as English.","A subtask of natural language generation is sentence realization: the process of generating a grammatically correct sentence from an abstract semantic\/logical representation. Where an extensive grammar has been constructed for automatic natural language analysis, specifying the legal syntactic constructions of a language, it is desirable to use the same grammar specification when automatically producing sentences. However, wide coverage analysis grammars allow many syntactic variations of the same semantic representation, for example the alternative sentences \u201cJohn ran quickly\u201d, \u201cJohn quickly ran\u201d and \u201cQuickly, John ran\u201d may all be assigned the same semantic representation, of the form:\n\n","When generating sentences from such a representation using the same grammar, a single preferred form must be chosen, and in cases where the analysis grammar allows ungrammatical sentences to be processed (intentionally or not) these ungrammatical forms will be additional options in the grammar for generation and must be excluded. Also, the formalism used to represent an analysis grammar is typically chosen without considering generation, and converting an existing grammar to a form suitable for generation is often more difficult than writing a new generation-specific grammar. Where it is possible to automatically simplify the grammar to aid the conversion process, this will typically lead to an increase in the range of ungrammatical sentences allowed by the grammar (termed over-generation), which must again be excluded during generation.","The present invention includes a method of, and a system for, generating a sentence from a semantic representation. The semantic representation is mapped to an unordered set of syntactic nodes. Simplified generation grammar rules and statistical goodness measure values from a corresponding analysis grammar are then used to create a tree structure to order the syntactic nodes. The sentence is then generated from the tree structure. The generation grammar is a simplified (context free) version of a corresponding full (context sensitive) analysis grammar. In the generation grammar, conditions on each rule are ignored except those directly related to the semantic representation. The statistical goodness measure values, which are calculated through an analysis training phase in which a corpus of example sentences is processed using the full analysis grammar, guide the generation choice to prefer substructures most commonly found in a particular syntactic\/semantic context during analysis.","Computing Environment",{"@attributes":{"id":"p-0023","num":"0025"},"figref":"FIG. 1","b":["100","100","100","100"]},"The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems, environments, and\/or configurations that may be suitable for use with the invention include, but are not limited to, personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-based systems, set top boxes, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and the like.","The invention may be described in the general context of computer-executable instructions, such as program modules, being executed by a computer. Generally, program modules include routines, programs, objects, components, data structures, etc. that perform particular tasks or implement particular abstract data types. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment, program modules may be located in both local and remote computer storage media including memory storage devices.","With reference to , an exemplary system for implementing the invention includes a general-purpose computing device in the form of a computer . Components of computer  may include, but are not limited to, a processing unit , a system memory , and a system bus  that couples various system components including the system memory to the processing unit . The system bus  may be any of several types of bus structures including a memory bus or memory controller, a peripheral bus, and a local bus using any of a variety of bus architectures. By way of example, and not limitation, such architectures include Industry Standard Architecture (ISA) bus, Micro Channel Architecture (MCA) bus, Enhanced ISA (EISA) bus, Video Electronics Standards Association (VESA) local bus, and Peripheral Component Interconnect (PCI) bus also known as Mezzanine bus.","Computer  typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer  and includes both volatile and nonvolatile media, removable and non-removable media. By way of example, and not limitation, computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable instructions, data structures, program modules or other data. Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical disk storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the desired information and which can be accessed by computer .","Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term \u201cmodulated data signal\u201d means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, FR, infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.","The system memory  includes computer storage media in the form of volatile and\/or nonvolatile memory such as read only memory (ROM)  and random access memory (RAM) . A basic input\/output system  (BIOS), containing the basic routines that help to transfer information between elements within computer , such as during start-up, is typically stored in ROM . RAM  typically contains data and\/or program modules that are immediately accessible to and\/or presently being operated on by processing unit . By way of example, and not limitation,  illustrates operating system , application programs , other program modules , and program data .","The computer  may also include other removable\/non-removable volatile\/nonvolatile computer storage media. By way of example only,  illustrates a hard disk drive  that reads from or writes to non-removable, nonvolatile magnetic media, a magnetic disk drive  that reads from or writes to a removable, nonvolatile magnetic disk , and an optical disk drive  that reads from or writes to a removable, nonvolatile optical disk  such as a CD ROM or other optical media. Other removable\/non-removable, volatile\/nonvolatile computer storage media that can be used in the exemplary operating environment include, but are not limited to, magnetic tape cassettes, flash memory cards, digital versatile disks, digital video tape, solid state RAM, solid state ROM, and the like. The hard disk drive  is typically connected to the system bus  through a non-removable memory interface such as interface , and magnetic disk drive  and optical disk drive  are typically connected to the system bus  by a removable memory interface, such as interface .","The drives and their associated computer storage media discussed above and illustrated in , provide storage of computer readable instructions, data structures, program modules and other data for the computer . In , for example, hard disk drive  is illustrated as storing operating system , application programs , other program modules , and program data . Note that these components can either be the same as or different from operating system , application programs , other program modules , and program data . Operating system , application programs , other program modules , and program data  are given different numbers here to illustrate that, at a minimum, they are different copies.","A user may enter commands and information into the computer  through input devices such as a keyboard , a microphone , and a pointing device , such as a mouse, trackball or touch pad. Other input devices (not shown) may include a joystick, game pad, satellite dish, scanner, or the like. These and other input devices are often connected to the processing unit  through a user input interface  that is coupled to the system bus, but may be connected by other interface and bus structures, such as a parallel port, game port or a universal serial bus (USB). A monitor  or other type of display device is also connected to the system bus  via an interface, such as a video interface . In addition to the monitor, computers may also include other peripheral output devices such as speakers  and printer , which may be connected through an output peripheral interface .","The computer  may operate in a networked environment using logical connections to one or more remote computers, such as a remote computer . The remote computer  may be a personal computer, a hand-held device, a server, a router, a network PC, a peer device or other common network node, and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in  include a local area network (LAN)  and a wide area network (WAN) , but may also include other networks. Such networking environments are commonplace in offices, enterprise-wide computer networks, intranets and the Internet.","When used in a LAN networking environment, the computer  is connected to the LAN  through a network interface or adapter . When used in a WAN networking environment, the computer  typically includes a modem  or other means for establishing communications over the WAN , such as the Internet. The modem , which may be internal or external, may be connected to the system bus  via the user-input interface , or other appropriate mechanism. In a networked environment, program modules depicted relative to the computer , or portions thereof, may be stored in the remote memory storage device. By way of example, and not limitation,  illustrates remote application programs  as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.",{"@attributes":{"id":"p-0035","num":"0037"},"figref":"FIG. 2","b":["200","200","202","204","206","208","210"]},"Memory  is implemented as non-volatile electronic memory such as random access memory (RAM) with a battery back-up module (not shown) such that information stored in memory  is not lost when the general power to mobile device  is shut down. A portion of memory  is preferably allocated as addressable memory for program execution, while another portion of memory  is preferably used for storage, such as to simulate storage on a disk drive.","Memory  includes an operating system , application programs  as well as an object store . During operation, processor  from memory  preferably executes operating system . Operating system , in one preferred embodiment, is a WINDOWS\u00ae CE brand operating system commercially available from Microsoft Corporation. Operating system  is preferably designed for mobile devices, and implements database features that can be utilized by applications  through a set of exposed application programming interfaces and methods. The objects in object store  are maintained by applications  and operating system , at least partially in response to calls to the exposed application programming interfaces and methods.","Communication interface  represents numerous devices and technologies that allow mobile device  to send and receive information. The devices include wired and wireless modems, satellite receivers and broadcast tuners to name a few. Mobile device  can also be directly connected to a computer to exchange data therewith. In such cases, communication interface  can be an infrared transceiver or a serial or parallel communication connection, all of which are capable of transmitting streaming information.","Input\/output components  include a variety of input devices such as a touch-sensitive screen, buttons, rollers, and a microphone as well as a variety of output devices including an audio generator, a vibrating device, and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition, other input\/output devices may be attached to or found with mobile device  within the scope of the present invention.","System and Method",{"@attributes":{"id":"p-0040","num":"0042"},"figref":["FIG. 3","FIG. 4","FIG. 5"],"b":["110","200"]},"Shown in  is a sentence generator or realizer . Sentence generator  receives as an input a semantic representation. Using a simplified generation grammar  and a statistical goodness measure (SGM) , generator  produces an output sentence which has a high probability of being grammatically correct. The simplified generation grammar  is a simplified version of the full analysis grammar  used by a parser  when analyzing natural language. The SGM  is used by both the generator  and the parser . Parser  of a natural language analyzing system is shown in  to illustrate the relationships between generation grammar  and analysis grammar , and to illustrate the shared SGM (or SGM database) . Inclusion of parser  in  is not intended to imply that the parser is part of the natural language generating system which includes sentence generator .","In the methods and systems disclosed herein, sentence generation is carried out using a simplified (context free) version  of a grammar, combined with an SGM  from the full (context sensitive) version  of the same grammar. The SGM is a measure of the probability of syntactic substructures derived from the syntactic\/semantic analysis of a corpus with the full grammar , and used to guide subsequent analysis. When generating a syntactic structure for a sentence, at each choice point in the simplified generation grammar, the SGM is used to guide the generation choice to prefer the substructure most commonly found in a particular syntactic\/semantic context during analysis. A detailed discussion of one SGM technique is described later with reference to .","As noted previously, producing a generation grammar is a difficult task, and conversion of analysis grammars into a generation grammar is a complex task due to the large number of conditions which govern the application of specific rules. The solution proposed here is to convert an analysis grammar for generation, and then use an independently motivated SGM, computed automatically during the syntactic analysis of a corpus and used to improve subsequent analyses, to guide the choice of syntactic construction type during sentence generation. Reference to the SGM values will indicate a preferred construction in every case and so avoid the generation of ungrammatical forms which may be allowed by the grammar. This therefore allows the use of a simplified form of the analysis grammar for generation, because the increased over-generation can be constrained by the SGM.","A key feature of the design is the sharing of a single SGM (or SGM database)  for both analysis and generation. The SGM  is calculated through an analysis training phase, processing a corpus of example sentences using the full analysis grammar, and recording frequency counts of syntactic substructures in particular contexts, including word collocations and dependencies in syntactic and\/or semantic relationships. After training, the SGM is used in normal analysis to direct the parser to prefer more probable structures, and in generation to guide rule selection.","Grammar Conversion","Techniques for using a single grammar for both analysis and generation exist in research literature (\u201creversible\u201d or \u201cbidirectional\u201d grammars), but place strong restrictions on the formalism in which the grammar is represented. In the present invention the conversion is of a large well-established analysis-specific grammar , developed independently of generation requirements. The full analysis grammar  includes extensive lists of conditions on each grammar rule, many of which are either irrelevant for generation or cannot be automatically converted. The approach here is to produce only a simple context free grammar for generation, ignoring all conditions from the analysis grammar except those directly related to the semantic representation.","For example, the rule VPwNPl (verb phrase with noun phrase on the left) describes the syntactic tree structure shown in . PhraseLevel is an attribute used to partition the grammar into sets of rules that apply to nodes at different levels of a syntactic tree structure, primarily to constrain the search space through the grammar. The rule is of the form VP()\u2192NP() VP(), which, in analysis, is read as \u201ca noun phrase (level ) followed by a verb phrase (level ) can be combined to produce a new verb phrase (level )\u201d. However, besides the PhraseLevel tests in this grammar rule, there are nearly 3000 lines of code expressing further conditions on when the rule can apply, testing syntactic features of the nodes and their neighbors (i.e. the rule is context sensitive). These conditions are always tested during analysis, and therefore during training for the SGM , so that the SGM values effectively capture the effects of the conditions (a \u201ccondition memory\u201d).","From the generation perspective, the VPwNPl rule is read as \u201ca verb phrase (level ) can be expanded into a noun phrase (level ) followed by a verb phrase (level )\u201d. All the detailed conditions of the analysis rule can be ignored, apart from the PhraseLevel and any syntactic role of the nodes that is directly related to the semantic representation. For this rule, the noun phrase is assigned the role of Subject in analysis, so that in generation the verb phrase at level  must have a Subject attribute (the rule's condition for use in generation) and the verb phrase at level , after expansion, does not.","Simplifying the grammar for generation, by ignoring almost all of the analysis conditions, will mean that several rules may now apply to a particular node, where only one would apply if the conditions were tested. The grammar without conditions will therefore greatly over-generate\u2014a term meaning that an analysis grammar will accept, or a generation grammar will produce, many ungrammatical sentences that should be excluded. The simplified grammar  alone is therefore inadequate for generation, but by adding reference to the SGM , the effects of the analysis grammar's detailed conditions are implicitly retained (the \u201ccondition memory\u201d). The SGM indicates preferred grammar rules to apply to a particular node, given the features and context of the node, so that a selection can be made in generation without needing to convert and test all the conditions from the analysis grammar.","Referring back to , shown is a diagram illustrating the stages of the generation process implemented by generator  () in the context of a system.  is a flow diagram illustrating the general method steps. The overall generation process operates in three distinct stages:\n\n","The second stage makes use of the technique proposed here to integrate the SGM  from the analysis grammar  with the use of the generation grammar . The following describes the algorithm for this stage and then steps through a worked example.","Generating a Single Preferred Tree",{"@attributes":{"id":"p-0051","num":"0056"},"figref":["FIG. 7","FIG. 4"],"b":"355","ul":{"@attributes":{"id":"ul0003","list-style":"none"},"li":["1. Make the syntactic node of the top unit the root node of the new syntactic tree.","2. For each non-terminal leaf node (phrase level>0) in the tree as determined at step :\n    \n    "]}},"In step  (step 2. described above), finding the next non-terminal node to expand, can be done using several search strategies. One implementation effectively uses a \u201chead-driven\u201d strategy, by first selecting the most recently generated node of the same type as its parent, before selecting other node types. However, since all attributes of all nodes are available in the input, and no additional nodes with attributes are introduced within the grammar, the order of expansion chosen by a particular search strategy will not exclude any possible paths and so will give the same result.","Steps  and  (step 2.a.iii. described above) can be viewed as calculating a \u201csyntactic future\u201d for the expression of a particular semantic attribute, to decide whether the attribute should be expressed now, at the current phrase level, or later, at a lower phrase level. The term reflects the \u201csyntactic history\u201d used as a measure of context in the SGM.","Given an input semantic representation of the form:\n\n","This unordered set of three nodes is then passed to the next generation stage to be expanded into a full syntactic tree. The VP node is made the root node of the new tree, because it corresponds to the top level unit of the semantic representation.","Pass . The first step is to search the tree for a leaf node not at phrase level , which will return the VP node, since that is the only node present. The set of applicable grammar rules for the VP node at the maximum phrase level contains the Sentence rule only. Generating the substructure described by the Sentence rule acts to add a sentence final period and move the VP node down one phrase level, resulting in the syntactic substructure shown in .","The SGM value (0.009) for this substructure is checked from the SGM database . A copy of the original VP node is then taken and all applicable rules, expressing the same semantic content (in this case none), at each lower phrase level are searched for (the \u201csyntactic future\u201d). None are found and the substructure shown in  is added to the tree.","Pass . Again, search the tree for a leaf node above phrase level , returning the newly created VP node at level . Check for applicable grammar rules for the VP node, returning only SwAVPl (sentence with adverbial phrase on the left) where the semantic condition of this rule requires a Modifier of type AVP. Generate the substructure from this rule (AVP(quickly)+VP(run)) and find its SGM value (0.073) Step down each phrase level from the level  VP node and check for other applicable grammar rules expressing the same Modifier:\n\n","The final generation stage (step  shown in ) is to produce inflected forms of each leaf node, using features such as tense and number on the nodes, in this example only modifying \u201crun\u201d to \u201cran\u201d. Then, the tokens are read off from left to right to give the sentence: \u201cJohn ran quickly.\u201d","One variation on the above algorithm is a mechanism to produce a ranked list of possible output sentences, not just a single preferred choice, by \u201cbacktracking\u201d through the tree construction process to produce multiple trees from a single input, each with separate overall SGM values. This method is illustrated in the flow diagram of .","Referring to , after each unit in the input semantic representation is mapped into a corresponding syntactic node:\n\n","As discussed above, the SGM used with the simplified generation grammar is the same SGM used in the corresponding full analysis system. Therefore, the following discussion of the SGM is in the context of the parser  () which uses the SGM and the full analysis grammar. The exemplary SGM of the parser  uses a generative grammar approach. In a generative grammar approach, each sentence has a top-down derivation consisting of a sequence of rule applications (i.e., transitions). The probability of the parse tree is the product of the probabilities of all the nodes. The probability for a given node is the probability that from the node one would take a specific transition, given the syntactive features. The exemplary SGM of the parser may be calculated using either of the following equivalent formulas: \n\n\nwhere\n\n","The parser defines phrase levels and labels them. Previous conventional approaches clustered transitions by segtype. For example, transitions focused on noun phrases, transitions focused verb phrases, etc. However, within each such grouping, the rules can be further subdivided into multiple levels. These levels are called \u201cphrase levels\u201d herein. These phrase levels are highly predictive of whether a transition will occur.","A null transition is utilized for each phrase level to account for no change from one level to the next. The null transition enables a node to move to the next level without being altered. The null transition is assigned probabilities just like other transitions.","The parser  defines each node's syntactic history. Using the parser, phenomena that are predicative but appear elsewhere in the tree (other than simply a node's immediate decedents or ancestors) are included in the probability calculation.","The probabilities of the parser are conditioned on transition name, headword, phrase level, and syntactic history. Since the probabilities are conditioned on the transition name in the parser instead of just the structure of the rule (e.g. VP\u2192NP VP), the parser may give the same structure different probabilities. In other words, there may be two transitions with the same structure that have different probabilities because their transition names are different. The probabilities of the exemplary SGM of the parser are computed top down. This allows for an efficient and elegant method for computing the goodness function.","A training corpus of approximately 30,000 sentences can be used to initially calculate the conditioned probabilities of factors such as transition name, headword, syntactic bigrams, phrase level, and syntactic history. The sentences in this training corpus have been annotated with ideal parse trees and the annotations contain all the linguistic phenomena on which the parser conditions.","The probabilities computation method has two phases: training and run-time. During the training phase, the system examines the training corpus, and pre-computes the probabilities (which may be represented as a \u201ccount\u201d) required at run-time. At run-time, the goodness function is quickly computed using these pre-computed probabilities (which may be \u201ccounts\u201d).","Conditioning on Headwords","Consider parse trees  and  shown in . Assume the two parse trees are identical except for the transition that created the top-most VP (verb phrase). In Tree  of , the verb phrase was created using the rule:\n\nVPwNPr: VP\u2192VP NP\n\nVPwNPr is used to add an object to a verb. For example, \u201cJohn hit the ball\u201d or \u201cThey elected the pope.\u201d In Tree  of , the verb phrase was created using the rule:\n\nVPwAVPr: VP\u2192VP AVP\n\nVPwAVPr is used when an adverbial phrase modifies a verb. For example, \u201cHe jumped high\u201d or \u201cI ran slowly.\u201d\n","To determine which tree was most probable using the conventional Transition Probability Approach (TPA), the number of occurrences of VPwNPr and VPwAVPr in the corpus is counted. If VPwNPr occurred most often, the conventional TPA's goodness function would rank Tree  of  highest.","This may be correct, but often it will be wrong since it will choose Tree  of  regardless of the linguistic context in which the rules appear. For example, assume that the headword was \u201csmiled\u201d. Parse trees  and  shown in  illustrate the same parses shown in trees  and  in , but the headword \u201csmiled\u201d is noted. English-speaking humans know that Tree  of  is highly unlikely. \u201cSmiled\u201d is intransitive and cannot take a direct object. In other words, \u201cShe smiled the ball\u201d is incorrect because someone cannot \u201csmile\u201d a \u201cball.\u201d Although, it is correct to say, \u201cShe smiled the most\u201d because the \u201cmost\u201d is not an object of \u201csmiled.\u201d Although \u201cthe most\u201d can act as a noun phrase in other contexts, it is an adverb in this case.","If the headword is included into the probability calculations, the goodness function is more likely to pick the correct parse. In particular, instead of just counting up all occurrences of VPwNPr and VPwAVPr in the corpus, a count is made of how often these rules appear with the headword \u201csmiled.\u201d In doing so, it likely to be discovered that there are no instances of VPwNPr occurring with the headword \u201csmiled.\u201d Thus, the goodness function would calculate the probability of Tree  to be zero.","Phrase Level","Phrases (e.g., noun phrases or verb phrases) have a natural structure. The job of the grammar (i.e., grammar rules) is to build this structure. Because of the rules of the language and because of conventions used by the grammarian, there are constraints on how the phrasal structure can be built. This translates into constraints on the order in which the rules can be applied. In other words, some rules must run before other rules. The exemplary SGM of parser  implements phrase levels to make this set of constraints explicit. Since phrase levels are predicative of what transition can occur at each node in a parse tree, incorporating them into the goodness function makes the goodness function more accurate.","To define the phrase levels for a given segtype, rules that create the given segtype are grouped into levels. All the rules at a given level modify the segtype in the same way (e.g., add modifiers to the left). The levels are numbered from one to N. Each level contains a null transition that allows a node to move to the next level without having an effect on the phrase being built.","The analysis grammar  build a phrase up by first producing an HW\u03a6 from a word. This is the head word of the phrase. It then enforces an order of levels by attaching modifiers of the headword in increasing phrase level order. For example, consider simple noun phrases in English. When building the parse tree for a noun phrase, the determiner (e.g., \u201cthe\u201d) is attached after the adjectives describing the noun. For example, \u201cthe red book\u201d is correct, but \u201cred the book\u201d is not correct. Therefore, a rule that adds a determiner to a noun phrase must come after the rule(s) that add adjectives. Again, \u201cafter\u201d is relevant to creation of a parse tree and the ordering of the application of the grammar rules. The term does not relate to the order of standard writing or reading.","For more complex noun phrases, the grammarian building a set of rules has some options. For example, consider the phrase: \u201cThe red toy with the loud siren.\u201d In one set of grammar rules, the structure may be like this:\n\n","All prepositional phrases (e.g. \u201cwith the loud siren\u201d) are attached to noun first; adjectives are attached next, and finally the determiner (\u201cthe\u201d) is added last. Once a determiner is attached to a noun phrase, it is not possible to add additional adjectives or prepositional phrases. Another set of grammar rules might structure it this way:\n\n","However, as long as a grammar  clearly defines the structure of noun phrases, there exist constraints on the order of the rules. In the parser's exemplary SGM, this ordering is made explicit by adding phrase level information to the rules and conditioning our probabilities on these phrase levels.","As another example, consider the grammar shown in  that builds verb phrases. This grammar supports verbs, noun phrases, and adjective phrases, but it has been simplified and does not support a range of other valid linguistic phenomena like adverbs, infinitive clauses, prepositional phrases, and conjunctions. This grammar can parse simple verb phrases like those shown in the description column of  and complex phrases like:","\u201cMore surprising, we have all found useful the guidelines which were published last year\u201d",{"@attributes":{"id":"p-0080","num":"0125"},"figref":["FIG. 17A","FIG. 18"],"b":["600","1"]},"As shown in the table of , on the right-hand side of each rule, each constituent is associated with a particular phrase level that is required for that constituent. Specifically, the number in parenthesis indicates the phrase level of the constituent (e.g., \u201cVP()\u201d).","On the left-hand side of the rule, the phrase level of the resulting node is specified. For example, consider the null transition:\n\n","This null transition can be applied to a VP at phrase level three and create a VP at phrase level four.","\u201cPL_Max\u201d in a phrase level indicator means the highest phrase level that occurs for a given segtype. For example, for the grammar above VP(PL_Max) would be the same as VP(). As another example:\n\n","Sometimes the phrase level of a constituent of the same segtype is the resulting node and may be either at the phrase level of the resulting node of less than then phrase level of the resulting node. For example:\n\n","To see an example of null transitions, consider the phrase:\n\n","Notice that this phrase differs from the similar phrase used above in that \u201c. . . we have all found useful . . . \u201d has been simplified to be \u201c. . . we found useful . . . \u201d","The rule VpwNul at transition null requires the seond constituent to have PL. Because the constituent has PL we construct a null transition first.",{"@attributes":{"id":"p-0089","num":"0141"},"figref":["FIG. 17B","FIG. 17B"],"b":["610","612","2","3"]},"The phrase levels and null transitions of the exemplary parser models the grammar of the English natural language. For example, consider the noun \u201cnut.\u201d You would never see a sentence such as \u2018I want nut.\u2019 or \u2018Nut is on the table.\u2019 The word \u201cnut\u201d wants a determiner such as \u201ca\u201d or \u201cthe\u201d. The phrase levels and null transitions force the exemplary parser to explicitly consider the absence of modifiers, as well as their presence.","Syntactic History","A node's syntactic history is the relevant grammatical environment that a node finds itself in. It may include the history of transitions that occur above the node. For example, is the node below a NREL, PRPRT, PTPRT, RELCL, or AVPVP? It may include whether the node is in a passive or an active construction. It may include information that appears elsewhere in the tree. For example, whether the headword of a sibling node is singular or plural. The specifics of what it relevant is dependent upon the specifics of the grammar (i.e., rewrite rules or transitions) being used.","For example,  shows two parse trees,  and , for the same verb phrase. Both trees are parsing a verb phrase having the mono-transitive headword (hw=\u201chit\u201d) and the verb phrase is known to be passive (sh=passive). In tree , the verb has a direct object as represented by NP at . In tree , the verb does not take a direct object.","In English, a mono-transitive verb inside a passive construction does not take a direct object. In contrast, when in the active form, the mono-transitive verb \u201chit\u201d takes a direct object. For example, \u201cI hit the ball\u201d in the active form has a direct object \u201cball\u201d to the verb \u201chit\u201d, but \u201cthe ball was hit\u201d in the passive form has no direct object to \u201chit.\u201d English-speaking humans know that tree  will never occur. In other words, there is a zero probability of a mono-transitive verb (like \u201chit\u201d) taking a direct object when the sentence is passive.","In some embodiments of parser , the transition probabilities are conditioned on syntactic history as well as headwords. Using a training corpus, the exemplary parser counts up how often VPwNPr occurs in a passive construction with a mono-transitive verb and finds that it never occurs. Thus, the probability of tree  would be calculated to be zero.","Syntactic history can be propagated down many levels of the tree. Take, for example, the sample sentence, \u201cGraceland, I love to visit.\u201d The thing (\u201cGraceland\u201d) that \u201cI\u201d love to visit is stated before it is revealed the \u201cI\u201d loves to visit anything.  shows an annotated parse tree  of a parse of this sample sentence. As can be seen in , the \u201ctopicalization\u201d feature is propagated past the verb \u201clike\u201d to the verb \u201cvisit.\u201d A complete discussion of syntactic phenomena which can be incorporated into syntactic history is not provided here, but the concepts of syntactic phenomena are well known by linguists.","SGM Probabilities","As noted previously, an exemplary SGM uses a generative grammar approach-each sentence has a top-down derivation consisting of a sequence of rule applications (transitions). For analysis, the probability of a parse tree is the product of the probabilities of all the nodes within that tree.","Generally, the probability of a node is defined as a conditional probability:\n\nProb(node)=Prob(what_is_unknown|what_is_known)\u2003\u2003Formula 1\n\nAssume that each node is visited in a depth-first tree walk. What is known is the information associated with the node and\/or with any node previously encountered in the tree walk. For example, the properties of the node, it is headword, phrase level, syntactic history, and segtype. What is unknown is what occurs below the node (i.e., the transition taken and the properties of its children).  shows a portion of a parse tree  and visually illustrates what is known and unknown at a node . What is known is above line  because it has already been processed. Below line  is what is unknown because it has not been processed.\n","With reference to the parse tree  of , during analysis the conditional probability is: \n\n\nwhere nranges over all nodes in the tree and the transition named by trn(n) is of the form X\u2192Y Z or of the form X\u2192Y.\n","To simplify Formula 2, it is noted that not all the parameters are independent. In particular, trn(n) and pl(n) imply pl(n) and pl(n). In other words, the name of the transition and the phrase level at node X implies the phrase levels of nodes Y and Z. Therefore, pl(n) and pl(n) may be removed from the left-hand side of the formula:\n\n=\u03a0Prob((), (), (), (), )|(), (), (), segtype())\u2003\u2003Formula 3\n","Similarly, Formula 3 may be simplified because trn(n), hw(n), and sh(n) imply sh(n) and sh(n). In other words, the name of the transition, the headword, and the syntactic history at node X implies the syntactic history of nodes Y and Z. Therefore, sh(n) and sh(n) may be removed from the left-hand side of the formula:\n\n=\u03a0Prob((), (), ()|() (), (), segtype())\u2003\u2003Formula 4\n","Formula 4 may be further simplified. Tracking both hw(n) and hw(n) is not particularly valuable because one of them is the same as hw(n). The one that is not the same is the modifying headword. The notation modhw(n) to refer to this modifying headword. This yields:\n\n=\u03a0Prob((), mod()|(), (), (), segtype())\u2003\u2003Formula 5\n","Formula 5 may be simplified still further by applying the chain rule (as understood by those skilled in the art of statistics), yields this:\n\n=\u03a0Prob(()|(),(),(),segtype())*Prob(mod()|()(),(),(),segtype())\u2003\u2003Formula 6\n","Since trn(n) implies pl(n) and segtype(n), the Formula 6 can further simplify this to be: \n\n","Finally, since it has been found that sh(n) is not very predicative of what the modifying headword will be, Formula 7 can be approximated by removing sh(n) from that part of Formula 7:\n\n\u2245\u03a0Prob(()|(), (), (), segtype()) Prob(mod()|(), ())\u2003\u2003Formula 8\n\n(SGM for a parse)\n","Notice that Formula 8 above is Formula B recited near the beginning of this detailed description.","PredParamRule Probability and SynBigram Probability","As described above, the probability of a parse tree is the products of the probabilities of each node. The probability of each node is the product of two probabilities. Thus, the SGM probability formula for a single node in a tree may be rewritten like this:\n\nProb(trn(n)|hw(n), pl(n), sh(n), segtype(n)) Prob(modhw(n)|trn(n) hw(n))\u2003\u2003Formula 9\n\n(SGM probability at a given node X)\n\nwhere X ranges over all the nodes in the parse tree.\n\nThis represents the statistical goodness measure (SGM) of the exemplary parser. This may be divided into to two parts. For convenience, the first probability will be called the predictive-parameter-and-rule probability or simply \u201cPredParamRule Probability\u201d and the second probability will be called the \u201cSynBigram Probability\u201d.\n\nThe PredParamRule Probability is:\n\nProb(trn(n)|hw(n), pl(n), sh(n), segtype(n))\u2003\u2003Formula 10\n\n(PredParamRule Probability)\n","Unlike the Simple Content Dependent Approach (described above in the background section), the PredParamRule Probability conditions upon headword, segtype, phrase level, and syntactic history. Since these are highly predicative of the contextually correct parse, this PredParamRule Probability is a significantly more accurate goodness function than conventional techniques.","The SynBigram Probability is:\n\nProb(modhw(n)|trn(n), hw(n))\u2003\u2003Formula 11\n\n(SynBigram Probability)\n\nThe SynBigram Probability computes the probability of a syntactic bigram. Syntactic bigrams are two-word collocation. The probability a measure of the \u201cstrength\u201d of the likelihood of a pair of words appearing together in a syntactic relationship. For example, the object of the verb \u201cdrink\u201d is more likely to be \u201ccoffee\u201d or \u201cwater\u201d than \u201chouse\u201d.\n","As described above in the background section, this is a conventional technique to calculate a goodness measure. However, with existing conventional syntactic bigram approaches, it is used alone to calculate the goodness function and it requires a huge training corpus. The parser overcomes the limitations of conventional syntactic bigram approaches by further conditioning the goodness measure on independent probability characteristics. In particular, those characteristics are represented by the PredParamRule Probability formula (Formula 10).","As a review, the following is a known about calculating conditional probabilities by counting appearances in a training corpus: \n\n","Therefore, the PredParamRule Probability and the SynBigram Probability can be calculated by counting the appearances of relevant events in the training corpus. The probabilities of a given training corpus that are determined by the PredParamRule Probability and the SynBigram Probability may be generally called \u201clanguage-usage probabilities\u201d for that given training corpus.","Thus, the PredParamRule Probability formula (Formula 10) may be calculated as follows:","PredParamRule Probability \n\n\nMoreover, the SynBigram Probability formula (Formula 11) may be calculated as follows: \n\n\nTwo Phases of SGM Calculation\n","Typically, a parser  () of an NLP system is designed to quickly calculate the goodness measure for many parse trees of parses of a phrase. To accomplish this, the exemplary parser is implemented in two phases: \u201ctraining\u201d and \u201crun-time.\u201d","During the training phase, the exemplary parser pre-calculates the counts that are needed to compute the PredParamRule Probability and the SynBigram Probability at run-time. Although this process tends to be time-consuming, processor-intensive, and resource-intensive, it only need be once for a given training corpus.","The result of the training phase is a set of counts for headword, phrase level, syntactic history, and segtype. If the training corpus approximates the natural language usage of a given purpose (general, specific, or customized), then the counts also approximate the natural language usage for the same purpose.","At run-time, these pre-calculated counts are used to quickly determine the probability of the parse tree. Each phrase is parsed into multiple parse trees. Each parse tree is given a SGM based upon the pre-calculated counts.","Alternatively, the training and run-time phase may be performed nearly concurrently. The training phase may be performed on a training corpus (or some subset of such corpus) just before the run-time phase is performed. Those who are skilled in the art will understand that time and space trade-offs may be made to accommodate the given situation. Regardless, the training phase (or some portion thereof) is performed, at least momentarily, before the run-time phase. This is because the training phase provides the foundation for the run-time phase to base its SGM calculations.","Although the present invention has been described with reference to particular embodiments, workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention. For example, references to a string of text being stored or acted upon should be understood to include various representations, such as parse trees, of the string of text."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0007","num":"0009"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0008","num":"0010"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0009","num":"0011"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0010","num":"0012"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0011","num":"0013"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0012","num":"0014"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0013","num":"0015"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0014","num":"0016"},"figref":["FIGS. 8\u201313","FIG. 7"]},{"@attributes":{"id":"p-0015","num":"0017"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0016","num":"0018"},"figref":"FIGS. 15A and 15B"},{"@attributes":{"id":"p-0017","num":"0019"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0018","num":"0020"},"figref":"FIGS. 17A and 17B"},{"@attributes":{"id":"p-0019","num":"0021"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0020","num":"0022"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0021","num":"0023"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0022","num":"0024"},"figref":"FIG. 21"}]},"DETDESC":[{},{}]}
