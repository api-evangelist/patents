---
title: Techniques for generating a static representation for time-based media information
abstract: Techniques for generating a static representation of time-based media information. A static representation is generated that comprises a timeline representing the duration of the time-based media information. Occurrences of one or more events that occur in the time-based representation are indicated along the timeline in the static representation. The static representation may be printed on a paper medium.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07703044&OS=07703044&RS=07703044
owner: Ricoh Company, Ltd.
number: 07703044
owner_city: Tokyo
owner_country: JP
publication_date: 20041223
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCES TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","BRIEF SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["The present application claims priority from and is a continuation-in-part (CIP) application of the following applications, the entire contents of which are incorporated herein by reference for all purposes:","(1) U.S. application Ser. No. 10\/001,895, \u201cPAPER-BASED INTERFACE FOR MULTIMEDIA INFORMATION\u201d, filed Nov. 19, 2001;","(2) U.S. application Ser. No. 10\/001,894, \u201cTECHNIQUES FOR RETRIEVING MULTIMEDIA INFORMATION USING A PAPER-BASED INTERFACE\u201d, filed Nov. 19, 2001;","(3) U.S. application Ser. No. 10\/001,849, \u201cTECHNIQUES FOR ANNOTATING MULTIMEDIA INFORMATION\u201d, filed Nov. 19, 2001;","(4) U.S. application Ser. No. 10\/001,891, \u201cPAPER-BASED INTERFACE FOR MULTIMEDIA INFORMATION STORED BY MULTIPLE MULTIMEDIA DOCUMENTS\u201d, filed Nov. 19, 2001;","(5) U.S. application Ser. No. 10\/001,893, \u201cTECHNIQUES FOR GENERATING A COVERSHEET FOR A PAPER-BASED INTERFACE FOR MULTIMEDIA INFORMATION\u201d, filed Nov. 19, 2001;","(6) U.S. application Ser. No. 10\/175,540, \u201cDEVICE FOR GENERATING A MULTIMEDIA PAPER DOCUMENT\u201d, filed Jun. 18, 2002;","(7) U.S. application Ser. No. 10\/645,821, \u201cPAPER-BASED INTERFACE FOR SPECIFYING RANGES\u201d, filed Aug. 20, 2003;","The present application is also related to the following co-pending patent applications, each of which is hereby incorporated by reference in its entirety for all purposes:\n\n","The present application relates to the field of time-based media information, and more particularly to techniques for generating a representation of time-based media streams that may be printed on a paper medium.","An increasing amount of information is now available in electronic form. The information may be stored as electronic (or digital) documents. The electronic information may include time-based media information such as audio streams, video streams, animation information, etc. that is captured over a time period. Example of time-based media information includes multimedia information that may include information of one or more different types such as audio information, video information, animation information, etc.","Several different applications and tools are available today for outputting time-based media information to a user. For example, several proprietary and\/or customized multimedia players, video players, televisions, personal digital assistants (PDAs), and the like are available for outputting time-based media information. Examples of media players include RealPlayer\u2122 provided by RealNetworks\u2122, Microsoft Windows Media Player\u2122 provided by Microsoft Corporation\u2122, QuickTime\u2122 Player provided by Apple Corporation, Shockwave\u2122 multimedia player, and others. These players generally run on a computer system and output the time-based media information via output devices coupled to the computer such as a monitor, a speaker, and the like.","While retrieving time-based media information in digital form is adequate for many applications, the dynamic nature of the output makes it unfeasible for other applications. For example, the capability to easily review time-based media information is commonly needed today. Conventionally, to search for a particular content within time-based media information, one has to actually playback the time-based media information to find the desired content. For example, a user has to playback audio recording of a radio talk show to determine when a particular guest speaker spoke during the show. There is presently no easy way for a user to search through a time-based media information segment to easily identify features of interest from the time-based media information.","Further, it is a well-known fact that many users find it easier to comprehend and review information when the information is printed on a paper medium. However, techniques for generating static representations of time-based media information that can be printed on a paper medium. Techniques are thus desired that are capable of providing a static representation of time-based media information suitable for printing on a paper medium.","Embodiments of the present invention provide techniques for generating a static representation of time-based media information. According to an embodiment of the present invention, a static representation is generated that comprises a timeline representing the duration of the time-based media information. Occurrences of one or more events that occur in the time-based representation are indicated along the timeline in the static representation. The static representation may be printed on a paper medium.","According to an embodiment of the present invention, techniques are provided for generating a representation of time-based media information. In this embodiment, time-based media information is received. A set of events occurring in the time-based media information between a first time and a second time is determined. For each event in the set of events, a time is determined when the event occurs in the time-based media information. A static representation is generated for the time-based media information. The static representation comprises a timeline having a start point corresponding to the first time and an end point corresponding to the second time, an indication, for each event in the set of events, of when the event occurred along the timeline, and a tag for each event in the set of events, each tag for an event corresponding to a time determined for the event. Each tag enables access to the time-based media information from a time corresponding to the tag. The static representation may be printed on a paper medium.","According to another embodiment of the present invention, techniques are provided for generating a representation of time-based media information. In this embodiment, a set of events to be included in a static representation of the time-based media information is determined, the set of events comprising one or more events occurring in the time-based media information between a first time and a second time. For each event in the set of events, a time when the event occurs in the time-based media information is determined. A static representation is generated for the time-based media information. The static representation comprises (1) a timeline having a start point corresponding to the first time and an end point corresponding to the second time, the timeline comprising a plurality of sections, each timeline section corresponding to a segment of time between the first time and the second time, each timeline section having an associated start time and an end time; (2) an indication, for each event in the set of events, of when the event occurs along the timeline; and (3) a tag for each event in the set of events, each tag for an event corresponding to a time determined for the event, wherein each tag enables access to the time-based media information from a time corresponding to the tag. The static representation may be on a paper medium.","The foregoing, together with other features, embodiments, and advantages of the present invention, will become more apparent when referring to the following specification, claims, and accompanying drawings.","In the following description, for the purposes of explanation, specific details are set forth in order to provide a thorough understanding of the invention. However, it will be apparent that the invention may be practiced without these specific details.","Embodiments of the present invention provide techniques for generating a static representation of time-based media information. According to an embodiment of the present invention, a static representation is generated that comprises a timeline representing the duration of the time-based media information. Occurrences of one or more events that occur in the time-based representation are indicated along the timeline in the static representation. The static representation may be printed on a paper medium. The paper medium may be any physical medium on which information can be printed, written, drawn, imprinted, embossed, etc. Printing on a paper medium may thus include printing, writing, drawing, imprinting, embossing, etc. the information on a paper medium.","Embodiments of the present invention thus take advantage of the high resolution and portability of paper and provide a readable representation of the time-based media information and an indication of events occurring within the time-based media information. The paper medium may be used to select, retrieve, and access time-based media information. One may use the paper medium on which the static representation is printed to review contents of the time-based media information","Time-based media information refers to information that has a temporal component associated with it. Time-based media information generally includes information that is captured or recorded over a period of time. Time-based media information is generally characterized by a start time and an end time. The time period between the start time and end time represents the time duration of the time-based media information. Time-based media information may comprise information of one or more types (or streams) such as audio stream information, video stream information, animation information, etc. or combinations thereof. Time-based media information may include one or more channels, where each channel represents information of a particular type. Multimedia information is an example of time-based media information. Multimedia information may refer to any one of or a combination of text information, graphics information, animation information, audio information, video information, images (e.g., slides, whiteboard images,), etc. Various formats may be used to store the multimedia information such as various MPEG formats (e.g., MPEG 1, MPEG 2, MPEG 3, MPEG 4, MPEG 7, etc.), MP3 format, SMIL format, HTML+TIME format, WMF (Windows Media Format), RM (Real Media) format, Quicktime format, Shockwave format, various image formats (e.g., JPEG, etc.), various streaming media formats, formats being developed by the engineering community, proprietary and customized formats, and others. Time-based media information may comprise media objects of different types such as an audio media object, video media object, etc. The media objects may comprise information one or more types.",{"@attributes":{"id":"p-0033","num":"0042"},"figref":["FIG. 1","FIG. 1"]},"As shown in , a data processing system  may be configured to generate a static representation  for time-based media information . Time-based media information  may be provided by various sources. For example, the time-based media information may be captured by one or more capture devices . Examples of capture devices  include video capture devices (e.g., video cameras), audio capture devices (e.g., microphones, voice recorders), presentation recorders, screen capture devices (e.g., a whiteboard information capture device), symbolic information capture devices, etc. In addition to capturing the information, capture devices  are able to capture temporal information associated with the captured information that may be used to synchronize the information with other captured information.","A presentation recorder is a device that is able to capture information presented during a presentation, for example, by tapping into and capturing streams of information from an information source. For example, if a computer executing a PowerPoint application is used to display slides from a *.ppt file, a presentation recorder may be configured to tap into the video output of the computer and capture keyframes every time a significant difference is detected between displayed video keyframes of the slides. The presentation recorder is also able to capture other types of information such as audio information, video information, slides information stream, whiteboard information, etc. The temporal information associated with the captured information indicating when the information was output or captured is then used to synchronize the different types of captured information. Examples of presentation recorders include a screen capture software application, a PowerPoint application that allows recording of slides and time elapsed for each slide during a presentation, and various others.","A symbolic information capture device is able to capture information stored in symbolic presentation documents that may be output during a presentation. For example, a symbolic information capture device is able to record slides presented at a presentation as a sequence of images (e.g., as JPEGs, BMPs, etc.). A symbolic information capture device may also be configured to extract the text content of the slides. For example, during a PowerPoint slide presentation, a symbolic information capture device may record the slides by capturing slide transitions (e.g., by capturing keyboard commands) and then extracting the presentation images based on these transitions. Whiteboard capture devices may include devices such as a camera appropriately positioned to capture contents of the whiteboard, a screen, a chart, etc.","The information captured by the various capture devices may be temporally synchronized. Time-based media information  may also be provided by other sources  such as radios, televisions, media players, CD or DVD players, video cassette recorders (VCRs), streaming information from a satellite signal, and the like.","Previously recorded time-based media information may be augmented by adding new information  to the recorded time-based media information. For example, a user may add annotations to previously recorded time-based media information. The annotations may be added at various points of time during the duration of the time-based media information. For example, the previously recorded time-based media information may comprise information recorded during a presentation using one or more capture devices . The recorded information may include audio information recorded during the presentation, video information recorded during the presentation, and slides information corresponding to slides presented during the presentation. The audio, video, and slides information may be stored as different channels of the time-based media information that are synchronized with each other. A user may at some later point in time review the recorded presentation information and add notes commenting on various portions of the presentation recording. The notes information added by the user may be stored as a separate channel of the time-based media information and temporally synchronized with the other channels of the time-based media information.","Data processing system  receives events criteria  representing information identifying one or more types of events that are to be identified from the time-based media information and included in static representation  generated for the time-based media information. An event may be related to some aspect of the contents of the time-based media information or related to some feature of the time-based media information. For example, for a recording of a presentation, events criteria  may specify a slides event implying that all instances of when a slide was first displayed during the presentation are to be shown in the static representation. As another example, for audio time-based media information, an event may be when a particular speaker speaks. Multiple events may be identified by events criteria . For example, in addition to slides, the event type identified by events criteria  may specify a notes event implying that all instances of notes added by a user to the time-based media information are to be identified and included in the static representation. Events criteria information  is user configurable.","Provision of event criteria  is not required by the present invention. Embodiments of the present inventions may also be configured to analyze the time-based media information to determine occurrences of certain events in the time-based media information. As described below in further detail, this processing may be automated using time-based media information processing programs or may even be performed manually.","Data processing system  may also receive information  that influences the manner in which a static representation is generated for time-based media information  and also influences the appearance of the static representation. Information  may include layout and style information that specifies the layout of the static representation and the styles to be used. The styles may be related to the format for depicting the timeline in the static representation, the fonts to be used, a maximum threshold number of events to be shown along the timeline, the color scheme to be used, and the like. A timeline may be made up of multiple sections (referred to as timeline sections), and the layout and style information may specify the number of timeline sections to be displayed on each page of the static representation, the maximum threshold number of events to be depicted for each timeline section, the time allocated to each timeline section, and the like.","According to the teachings of the present invention, static representation  may be printed on a paper medium. Accordingly, information  may also comprise print parameters identifying the manner in which the static representation is to be printed. The print parameters may identify the paper medium on which the static representation is to be printed, the size of the paper medium (e.g., A4 size paper, 8\u00bd by 11 size paper), whether the static representation is to be printed in landscape or portrait format, the properties of a printer that is to be used for the printing, etc. The print parameters may affect the manner in which static representation  is generated. For example, the size of the paper may influence the number of timeline sections to be displayed on each page of the search engine and also influence the pagination of the static representation. The properties of the printer to be used for the printing may determine the appropriate level of resolution to be used for generating the static representation so that the generated static representation can be printed by the printer. Various other print parameters may also be provided.","Based upon time-based media information , event criteria , and information  (if specified), data processing system  is configured to generate a static representation  for time-based media information . As part of generating the static representation, data processing system is configured to process the time-based media information to determine one or more events occurring in the time-based media information that satisfy events criteria . The determination of events in time-based media information  may be automated using time-based media information processing programs or may be performed manually. For example, various content recognition techniques may be applied to the time-based media information to recognize occurrences of events in the time-based media information. The content recognition programs may be configured to receive events criteria  and determine events in the time-based media information that satisfy the events criteria. In absence of (or in addition to) events criteria , the content recognition programs may be configured to automatically detect events in the time-based media information. For example, events may be automatically determined upon detecting a particular face in the video information, detecting a particular speaker in the audio information, occurrences of certain words (word spotting) in the audio or video information, detecting loud noises in the audio information, detecting sudden movement in the video information, and the like.","Events in the time-based media information may also be manually detected. A person may review the time-based media information and identify occurrences of events in the time-based media information. For example, a person may hear audio information and identify events (e.g., topics of discussion, audio spoken by a particular speaker, etc.) in the audio information.","Information identifying the detected events (detected either automatically or manually) may be annotated to the time-based media information. For example, the events information may be stored as a separate channel of the time-based media information and temporally synchronized with the time-based media information. The events information may also be stored in other ways such that it is accessible during generation of the static representation.","For each determined event, data processing system  is configured to determine the time when the event occurs in the time-based media information. Data processing system then generates static representation  for the time-based media information. The appearance and layout of static representation  may be influenced by information .","According to an embodiment of the present invention, static representation  generated by data processing system  includes a timeline representing the duration of the time-based media information that is analyzed. In one embodiment, the entire time-based media information may be analyzed from the start time to the end time of the time-based media information and the timeline may represent the time duration from the start time to the end time. In another embodiment, a portion of the time-based media information may be analyzed from a first time to a second time, and the timeline in the static representation may represent the time duration from the first time to the second time.","For the events determined in the time-based media information, the static representation comprises indications provided along the timeline indicating when the determined events occur in the time-based media information. In one embodiment, for each event, the time that the event occurs in the time-based media information is translated to a location on the timeline approximately corresponding to that time. A mark is made on the timeline at that location for the event denoting occurrence of the event. Visual representations may also be generated for the events and included in the static representation. For example, for events representing showing of slides, a small image of the slide may be generated and included in the visual representation. Correlation information may be provided correlating the visual representation of an event to a mark on the timeline corresponding to occurrence of the event.","Static representation  may also include a tag for each event included in the static representation. In one embodiment, a tag for an event represents or encodes the time when the event occurs in the time-based media information. Tags associated with events enable access to the time-based media information.","Once static representation  has been generated, multiple actions may be performed on the static representation. The static representation may be output using an output device  such a monitor. Static representation  may also be sent to a printer  for printing. Printer  may print the static representation on a paper medium to generate a paper document comprising the static representation.","As previously indicated, tags may be included in the static representation. One or more tags printed on the paper document may be read by a reader device  to access the time-based media information or perform some action. In one embodiment, reading a tag using a reader device  invokes a media player  that starts playback of a portion of the time-based media information from a time corresponding to the read tag. In this manner, tags enable access to the time-based media information. In one embodiment, barcodes are used as tags and may be read by barcode readers. In alternative embodiment, tags may be in the form of numerical identifiers, and the user may type the numerals in a keypad or computer to enable access to the time-based media information. Tags may also be used to perform actions, as described below in further detail. Further details related to static representation  are provided below.",{"@attributes":{"id":"p-0052","num":"0061"},"figref":["FIG. 2","FIG. 2","FIG. 2","FIG. 2"],"b":["200","202","200","202","204","206","202","208"]},"A timeline  is displayed in static representation . In , timeline  is laid out from start to finish in a left to right and top to bottom format. The start of timeline  is the upper left corner and end of the timeline is at the lower right corner. In , timeline  represents the entire duration of the time-based media information (presentation recording), i.e., the start of the timeline  represents the start of the time-based media information and the end of timeline  represents the end of the time-based media information. In , the entire timeline is displayed on one page of the static representation. However, a timeline may extend across multiple pages of a static representation depending on the duration of the time-based media information represented by the timeline.","It should be apparent that a static representation need not be generated for the entire time-based media information. In alternative embodiments, a user may specify a first time marking the starting time for generation of the static representation and a second time marking the end time for the static representation. The first time may be different from the starting time of the time-based media information and the second time may be different from the end time of the time-based media information. Accordingly, a static representation may be generated for any segment of time of the time-based media information. In embodiments where a first time and a second time have been specified, the start of the timeline corresponds to the first time and the end of the timeline corresponds to the second time and the timeline represents a segment of time of the time-based media information between the second time and the first time.","In the example depicted in , timeline  comprises six timeline sections -, -, -, -, -, and -. Each timeline section (or sub-timeline) corresponds to a portion of time of the total time represented by the timeline. In , the duration of time represented by each timeline section is the same, namely, 04:13 minutes. Accordingly, timeline section - represents the first 04:13 minutes of the time-based media information, timeline section - represents the second 04:13 minutes of the time-based media information, timeline section - represents the next 04:13 minutes of the time-based media information, and so on, and timeline section - represents the last 04:13 minutes of the time-based media information. The duration of each timeline section may be user configurable or may be set automatically by the application responsible for generating the static representation.","Various different styles may be used for depicting a timeline. The styles may be user configurable. In the example depicted in , timeline  comprises keyframes extracted from the video stream portion of the time-based media information. The keyframes provide a visual context for the displayed information. In alternative embodiments, if the keyframes cannot be extracted from the time-based media information, then the timeline may be shown as a line or box without any images, as shown in .","Events determined from the time-based media information are mapped to timeline  in . In the example in , the events that are depicted comprise slides events (i.e., showing of slides during the presentation) and notes events (notes taken during or after the presentation). The notes may have been recorded during the presentation or may have been added by a user after the presentation and synchronized with the other channels of information of the time-based media information.","The events may be represented using different ways. In one embodiment, a visual representation is used to depict each event. For example, in , visual representations are generated for slide events. The visual representation  for a slides event comprises a small image of the slide that was displayed. The visual representation  for a note comprises a text image of the note. Various different visual representations may be used for different events. Different techniques may be used to differentiate between the different types of events. According to one technique, different colors may be used for the visual representations of the events to differentiate between the events. For example, in , a first color may be used for the visual representations of slide events and a second color that is different from the first color may be used for the visual representations of the notes.","For each event, information is displayed along timeline  indicating a point along the timeline corresponding approximately to the time when the event occurs in the time-based media information. In , the point on timeline  for each event is indicated by a mark . Accordingly, for each event, the time that the event occurs in the time-based media information is translated to a mark or location on the timeline approximately corresponding to that time. For example, in , each mark  on timeline  for a slide event denotes the approximate time in the time-based media information when the slide was displayed during the presentation. For each event, information is also printed correlating the mark on timeline corresponding to the occurrence of the event in the time-based media information. In , the correlation information is a line or arrow  from the mark along the timeline towards the visual representation of the event.","For example, in , three events (2 slides, 1 note) are plotted along timeline section -, ten events (8 slides, 2 notes) are plotted along timeline section -, four events (3 slides, 1 note) are plotted along timeline section -, ten events (8 slides, 2 notes) are plotted along timeline section -, ten events (10 slides) are plotted along timeline section -, and eight events (8 slides) are plotted along timeline section -. For each event, a mark  is indicated on timeline  representing the time when the event occurred in the time-based media information.","The static representation also includes a tag  associated with each event. In , each tag  is a barcode. In one embodiment, each tag  for an event represents or encodes the time when the event occurs in the time-based media information. In , the time corresponding to a tag for an event (i.e., the approximate time when the event occurs in the time-based media information) is also printed (reference ) on top of the visual representation of the event.","Tags  associated with events enable access to the time-based media information. In one embodiment, a user may select a tag associated with an event and access time-based media information related to that event from the time represented by the selected tag. For example, in , selecting a tag associated with a slides event invokes an application (e.g., a media player) that outputs the slides information for the selected slides event and also starts playback of the video (and audio or other information if available) information from the time corresponding to the selected tag (i.e., from a time when that event occurs in the time-based media information).","The static representation depicted in  also comprises tags  associated with each timeline section. As shown, three tags are printed along the bottom of each timeline section corresponding to the start time of the time segment represented by the timeline section, a middle time point of the time segment represented by the timeline section, and an end time of the time segment represent by the timeline section. A user may select any of these tags to initiate playback of information from the time corresponding to the selected tag. In , the time represented by each tag associated with a timeline section is also printed below the tag.","A set of tags  is printed for controlling playback of the output information. For example, tags  may be selected to play\/pause the playback, fast forward the playback, perform rewind operations, or perform other user configurable operations on the time-based media information that is output. Tags for performing other types of actions may also be included in the static representation in alternative embodiments. For example, tags may be included for performing actions such as sending an email, sending a fax, saving a document (e.g., the time-based media information or potion thereof), printing a document, etc.","Tags , , and  displayed in  are barcodes. However, in alternative embodiments, other kinds of tags and mechanisms that enable facilitate access to the time-based media information may also be used.","The static representation depicted in  also includes a global timeline . Global timeline  provides a summary of the entire timeline  and its timeline sections. Global timeline  is especially useful when timeline  comprises several timeline sections and may be spread across multiple pages. Global timeline  provides an overall context for the static representation by showing all the timeline sections in one place. For example, in , the static representation for the time-based media information is printed on a single page and the timeline comprises of six timeline sections. Accordingly, global timeline  comprises six parts corresponding to the six timeline sections, each part representing an individual timeline section. Each part of global timeline  is identified by a numbers printed below the part. The number is also printed next to the timeline section corresponding to the part. If timeline  were printed across several pages (i.e., comprises several timeline sections printed across multiple pages), then global timeline  would show the entire timeline and contain a part for each individual timeline section. In this manner, global timeline  provides a context for the entire static representation. Global timeline  may be printed on each page of the static representation when printed on paper.","Global timeline  also indicates the occurrences of events. Events that are mapped to the individual timeline sections are shown in global timeline  in the part of the global timeline corresponding to the individual timeline sections. For example, in , the occurrences of events that are mapped to timeline  are also shown in global timeline  using small boxes . The boxes may be of different colors to denote the different types of events. The boxes are printed along global timeline  at the approximate location each event occurs in the time-based media information and has been mapped to timeline . Global timeline  thus provides an overall context for the static representation document.","The static representation depicted in  and printed on a paper medium provides a time-oriented representation of time-based media information and provides an overview of events that occur in the time-based media information. For each event, the static representation displays information indicating the type of the event. The timeline and marks along the timeline provide an approximate temporal location in the time-based media information when the events occur (or alternatively does not occur). Further, the static representation enables access to the time-based media information. One or more tags printed on the paper medium may be selected to access time-based media information corresponding to the selected tags. In this manner, the paper document on which the static representation is printed provides a paper-based interface for accessing time-based media information. Tags may also be selected for performing other actions.","As depicted in , the events are horizontally plotted to each timeline section. The visual representations of the events are also vertically staggered to prevent the barcodes (tags) associated with the events from aligning or overlapping with each other which may cause errors when scanning using a reader device such as a barcode scanner. Since the static representation is generated such that it can be printed on a paper sheet, the horizontal width of the static representation is limited to that of the paper. As a result, the space available for plotting the events along a timeline section is limited. There are only a limited number of positions along the timeline to place the visual representation of the events. For example, when using 8\u2033\u00d711\u2033 paper in portrait mode such as depicted in , there is space only for depicting up to ten visual representations of events for a timeline section given the current size of the event visual representation and the barcode (tag) associated with each event.","In some cases the tight clustering of multiple events along a timeline section can be reduced by decreasing the time allotted to each timeline section, thereby increasing the spacing between the events. The time allocated for each timeline section may be automatically determined by the static representation generation application or may be specified by a user.","In many instances however, it is not always possible to decrease the time allocated to each timeline section to reduce the tight clustering of events. Accordingly, techniques (referred to as \u201cplacement techniques\u201d) are provided for determining which events to show, how many events to show per timeline section, and where to position them to optimize the layout. The maximum threshold number of events (or visual representations of the events) to be shown for each timeline section may be automatically determined by the static representation generation application based upon the size of the visual representations of the events, size of tags associated with the events, size of the timeline sections, and other like factors. Alternatively, the maximum threshold number of events (or visual representations of the events) to be shown for each timeline section may be set by a user.","For example, in ,  events were determined corresponding to the time (events between the times 00:12:41-00:16:54) depicted by timeline section -, but only 10 events are shown in the static representation. For each timeline section, information identifying the total number of events determined for the timeline section and the number of events selected for inclusion in the static representation is shown at the right hand side of each timeline section (e.g., 10 of 17 for timeline section -). Given a maximum threshold of 10 events per timeline section, the placement technique selected only 10 events to be included in the static representation fro timeline section -.","The placement technique also handles placement of visual representations of the events when the events occur along a timeline section are closely bunched together. For example, for timeline section - depicted in , seven slides events all co-occur in close proximity to each other with respect to time and are closely marked together on timeline section -. This may be due to several reasons such as a presenter either quickly flipping through a slide collection or using animation. In this example, eight different slides are presented in a very short time. The placement technique figures out the optimal way to organize the events such that the resultant static representation is easily readable. In one embodiment, this is done by centering the visual representation corresponding to the \u201cmiddle\u201d event (e.g., event 00:22:59) over the general timeline location. The remaining events (or visual representations of the events) are then added, by time, from left to right.","Lines or arrows (or some other information correlating the visual representations of the events to the marks along the timeline) are then drawn from the visual representations of the events to the approximate location on the timeline corresponding to the times when the events occurred in the time-based media information. The lines show where an event has occurred along the timeline. The static representation also shows what other events have occurred in close proximity to this event and provides a context for the event. Because the points on the timeline are tightly clustered, it may be assumed that the presenter was flipping through this collection of slides until the eighth slide was reached which may have been accompanied with an explanation. In a similar manner, for timeline section - depicted in , a reader of the static representation might assume several things about the slide shown on the left side of this timeline section. The first event shown on timeline section - is somewhat isolated with respect to all other events on the page. This may indicate that the presenter spent a lot of time explaining this slide or that the presentation of the slide caused some discussion which interrupted the presentation for a short period of time. Accordingly, a user may derive a lot of contextual information from the static representation and the placement of events along the timeline in the static representation.","The static representation depicted in  is generated for time-based media information that comprises at least video information, slides information, and notes information, in addition to potentially other types of information. Static representations may be generated for other types of time-based media information. For example, static representations may be generated for media streams that have no video images or text. For example, a static representation may be generated for time-based media information that comprises only audio content such as a radio broadcast recording, audio recorded by an audio recorder or dictation device, etc. The timeline representation accommodates such audio content.","There are various ways in which events may be determined from or annotated to audio content. The audio content may be processed to identify categories of events by speaker, by topic, etc. The events may be related to audio spoken by a particular speaker, discussions related to a particular topic, specific portions of audio of interest to a user, etc. The processing may be performed manually or using automated programs (e.g., content recognition programs that are able to recognize the context and content of the audio information). The events information may then be annotated to the audio. For example, a separate channel with event annotations may be added to the time-based media audio information and temporally synchronized with the audio information. In this manner events may be determined for audio information.","In other embodiments, a transcription of the audio content may be obtained to facilitate determination of events in the audio information. Based upon the transcription, events may be annotated to the audio information. For example, occurrences of certain words in the audio transcription may signal the occurrence of an event. Since the audio-to-text transcription techniques may not always be accurate, a threshold may be configured identifying that a certain threshold number have to be present before an event based upon the words is identified. Alternatively, events may be identified based upon confidence scores associated with the transcribed text. The events information may be stored as a separate channel along with the audio content and synchronized with the audio content. The events information associated with the audio information may then be used to identify events to be included in the static representation for the audio information.","A static representation comprising a timeline and events plotted along the timeline may be generated for audio information. Examples of events that may be determined for audio information include speaker detection and recognition, laughing or yelling etc., when a particular topic is discussed (content recognition), periods of silence, etc.  depicts a static representation generated for a radio broadcast (audio information) according to an embodiment of the present invention. Two types of events are displayed in the static representation, namely, events representing topics of discussion (indicated by visual representation with a dark background) and events representing sound bites (indicated by visual representation with a light background). Different colors may be used to differentiate between the two types of events. The static representation depicted in  may be printed on a paper medium. The generated paper document may then be used to access portions of the radio broadcast related to one or more events. The paper document thus provides a structure that allows a user to navigate and randomly access portions of the audio content of the radio broadcast. The paper document comprising the static representation thus provides a visual tool for navigating the underlying audio media content.",{"@attributes":{"id":"p-0079","num":"0088"},"figref":["FIG. 4","FIG. 4","FIG. 4","FIG. 4"],"b":["400","400"]},"As depicted in , processing is initiated upon receiving a signal to generate a static representation for time-based media information (step ). The signal may indicate that the static representation is to be generated for the entire time-based media information (i.e., from the start time of the time-based media information to the end time of the time-based media information). Alternatively, information may be received specifying that the static representation is to be generated for a portion of the time-based media information between a first time and a second time. Information specifying the location of the time-based media information may also be received in .","Events criteria information may be received (step ). The events criteria information specifies the one or more types of events that are to be identified in the static representation. Events criteria information may be provided by a user. As previously described, event criteria information is not required for the present invention. Events may also be automatically determined from the time-based media information for which a static representation is to be generated.","Information that influences the manner in which a static representation is generated and\/or influences the appearance of the static representation may be received (step ). The information received in  may include layout and style information that specifies the layout of the static representation and the styles to be used. The layout and styles may include information related to the format for depicting the timeline in the static representation, the fonts to be used, a maximum threshold number of events to be shown along the timeline, the color scheme to be used, the number of timeline sections to be displayed on each page of the static representation, the time duration represented by each timeline section, the maximum threshold number of events to be depicted for each timeline section, and the like. The information received in  may comprise print parameters identifying the manner in which the static representation is to be printed. The print parameters may identify the paper medium on which the static representation is to be printed, the size of the paper medium (e.g., A4 size paper, 8\u00bd by 11 size paper), whether the static representation is to be printed in landscape or portrait format, the properties of the printer that is to be used for the printing, etc.","Although steps , , and  are shown as being performed sequentially in , the steps may be performed in any order or even in a single step.","The time-based media information is then processed to identify events that occur in the time-based media information and are of the type specified by the events criteria information (step ). The processing may be performed for the entire time-based media information or for a portion of the time-based media information as indicated by the first and second times received in . As previously indicated, the time-based media information may comprise information of different types. The processing may be performed on one or more type of information included in the time-based media information. Different techniques may be used to process the different types of information to determine events occurring in that type of information.","In one embodiment, the occurrences of events may be indicated in the time-based media information itself. For example, the occurrence of a slide (or note) in the time-based media information indicates occurrence of a slide (or notes) event. The channels of the time-based media information may thus indicate when an event occurs. Other types of techniques may be used for processing other types of information.","The time-based media information may also be processed to determine occurrences of one or more events in the time-based media information that satisfy the events criteria if provided. Various time-based media information processing programs may be used for the detection of events. The content recognition programs may be configured to receive the events criteria and determine events in the time-based media information that satisfy the events criteria. In absence of (or in addition to) events criteria, the content recognition programs may be configured to automatically detect events in the time-based media information. For example, events may be automatically determined upon detecting a particular face in the video information, detecting a particular speaker in the audio information, occurrences of certain words (word spotting) in the audio or video information, detecting loud noises in the audio information, detecting sudden movement in the video information, and the like.","Events in the time-based media information may also be manually detected. A person may review the time-based media information and identify occurrences of events in the time-based media information. For example, a person may hear audio information and identify events (e.g., topics of discussion, audio spoken by a particular speaker, etc.) in the audio information.","Information identifying the detected events (detected either automatically or manually) may be annotated to the time-based media information. For example, the events information may be stored as a separate channel of the time-based media information and temporally synchronized with the time-based media information. The events information may also be stored in other ways such that it is accessible during generation of the static representation.","For each event determined in , the time that the event occurs in the time-based media information is also determined (step ). The events determined in  may then be ordered based upon the times determined for the events in  (step ).","A static representation is then generated for the time-based media information (step ). The static representation generated in  may have some or all the features of a static representation described above. For example, the static representation may comprise a timeline with events determined in  mapped to various locations of the timeline depending upon the times determined for the events in . The static representation may also comprise tags for the events that enable access to the time-based media information.","Various actions may then be performed on the static representation generated in . As shown in , the static representation may be output to a user via a display device (step ) or may be printed on a paper medium (step ) to generate a paper document on which the static representation is printed.",{"@attributes":{"id":"p-0092","num":"0101"},"figref":["FIG. 5","FIG. 5","FIG. 5","FIG. 5","FIG. 5","FIG. 4"],"b":["500","500","414"]},"The duration of the time-based media information for which the static representation is to be generated is determined (step ). As previously indicated, the duration may be the entire duration of the time-based media information from the start to the end or may be between a first time and a second time specified by a user.","Based upon the duration of the time-based media information, a number of timeline sections needed for representing the duration are then determined (step ). The number of timeline sections depends on the duration or segment of time allocated to each timeline section. As previously indicated, the amount of time for each section may be user-configurable. For example, the duration of time represented by each timeline section may be specified by the layout and style information. In other embodiments, the time for each timeline section may be automatically determined. Factors such as the size of the paper on which the static representation is to be printed, the amount of clustering of events, etc. may influence the time allocated to each timeline sections.","The number of pages to be generated for the static representation is then determined (step ). The number of pages depends on the number of timeline sections determined in  and the number of timeline sections to be included on a page. Information specifying the number of timeline sections to be included on each page may be user-configurable and may be included in the layout and styles information. In other embodiments, the number of timeline sections to be printed on a page may be automatically determined.","The processing depicted in step  need not be performed in certain embodiments of the present invention. For example, step  may not be performed in embodiments where the printing application (or some other application) is capable of performing the pagination function. In these embodiments, the pagination related functions may be performed by the printing application itself.","Events (from the events identified in  in ) corresponding to each timeline section determined in  are then determined (step ). An event is selected for a timeline section if the event occurs during the time segment corresponding to the timeline section. Zero or more timeline events may be determined for each timeline section.","A static representation is then generated such that for each timeline section, an indication is provided of when events determined for that timeline section occur along the timeline section (step ). As part of , for each event determined for a timeline section, a location is determined on the timeline section when that event occurs in the time-based media information. A mark (reference  as depicted in ) may be made on the timeline section for each event to denote the occurrence of the event.","There are different ways in which events may be represented in the static representation. In one embodiment, as part of , a visual representation is generated for each event. Different types of visual representations may be generated for different types of events. For example, as shown in , an image of a slide may be generated for each slides event, text may be used to represent notes, etc. The visual representations generated for events determined for a timeline section are positioned along the timeline section in the static representation. Correlation information is provided for each event correlating the visual representation generated for the event to the mark on the timeline section identifying when the event occurs along the timeline section. The correlation information may be a line or arrow correlating the visual representation to the mark along the timeline section. Other types of correlation information may also be provided.","Tags may also be generated for each event corresponding to the time when the event occurs in the time-based media information and included in the static representation. For each event the tag generated for the event may be placed in a location proximal to the visual representation for the image. As depicted in , tags are placed below the visual representations of the events. Tags may also be generated for each timeline section. For example, in one embodiment (as depicted in ), three tags  are generated for each timeline section corresponding to a start time for the timeline section, a middle time, and an end time of the time segment represented by the timeline section.","The contents of the static representation are arranged to enhance readability of the static representation. For example, as depicted in , events are horizontally plotted to each timeline section and the visual representations of the events are also vertically staggered to prevent the barcodes (tags) associated with the events from aligning or overlapping with each other which may cause errors when scanning using a reader device such as a barcode scanner. Since the static representation is generated such that it can be printed on a paper sheet, the horizontal width of the static representation is limited. As a result, there are only a limited number of positions along the timeline to place the visual representation of the events. Accordingly, as previously described, placement techniques are provided for determining which events to show, how many events to show per timeline section, and where to position them along the timeline section to optimize the layout and readability of the static representation.",{"@attributes":{"id":"p-0102","num":"0111"},"figref":["FIG. 6","FIG. 6","FIG. 6","FIG. 6","FIG. 6","FIG. 4"],"b":["600","600","414"]},"Steps , , and  are similar to steps , , and  depicted in  and described above. A timeline section from the timeline sections determined in  is then selected for processing (step ). Events (from the events determined in  in ) corresponding to the selected timeline section are then determined (step ). An event is determined for the selected timeline section if the event occurs during a time segment represented by the selected timeline section.","A check is then made to see if the number of events selected for the timeline section exceeds a pre-configured threshold value (step ). The pre-configured threshold value specifies the maximum number of events (or visual representations of the events) to be shown for each timeline section. The threshold may also be automatically determined by the static representation generation application based upon the size of the visual representations of the events and the tags associated with the events. Alternatively, the threshold value may be user configurable.","If it is determined in , that that threshold value is not exceeded, then all the events determined for the timeline section are selected to be included in the static representation (step ).","If it is determined in , that the threshold value is exceeded, then a subset of the events determined for the timeline section are selected to be included in the static representation such that the number of events in the subset does not exceed the threshold value (step ). Various different techniques may be used to determine which events to select. In one embodiment, the events determined for a timeline section may be prioritized and events with higher priorities may be selected. According to another technique, the events may be compared to each other to determine which ones to select for inclusion in the static representation.","According to one events selection technique, the determined events are first ranked based on their type and a score is generated for each event. For example, a notes event relating to a note taken during a meeting may rank higher than a slides event corresponding to a slide shown during the meeting. For example, a notes event may receive a score of 0.3 while a slides event may receive a score of 0.1. This may be done because notes are typically more relevant than slides because they are human generated. Also, events such as whiteboard or photos taken during a meeting may rank higher than a slide (e.g., be assigned a score of 0.2). Events which co-occur in close proximity to each other may be \u201cpenalized\u201d based on the logic that one event from a particular time period is better than multiple events which could potentially show the same content. So, for instance, if the event being evaluated co-occurs within, say 3 seconds, of another event, that event may be penalized by reducing its score by 0.1. Events may also be \u201crewarded\u201d by increasing their scores. For instance, an event with a long duration (e.g., more than 10 seconds may be rewarded), an event with lots of text (notes) may be rewarded, and the like. The events are then sorted based upon their scores. From the sorted list of events, the top \u201cX\u201d number of events is selected, where \u201cX\u201d is the maximum threshold number of events that can be displayed along a timeline section. The selected events are then resorted based upon times associated with the events. The events are then depicted along the timeline section based on the resorted event order. Locations along the timeline section corresponding to the times when the events selected occur are marked. Correlation information (e.g., a line, arrow, etc.) is then provided for each selected event correlating the event and a mark along the timeline section corresponding to the event.","A check is then made to see if there are more unprocessed timeline sections (step ). If more unprocessed timeline sections exist, then an unprocessed timeline section is selected in step  and processing continues with step . If all timeline sections have been processed, then processing continues with step .","A static representation is then generated such that for each timeline section an indication is provided when events selected in  or  for that timeline section occur along the timeline section (step ). For each event selected for a timeline section, a location is determined on the timeline section when that event occurs in the time-based media information. A mark (reference  as depicted in ) may be made on the timeline section for each event to denote the occurrence of the event. Visual representations may be generated for the events selected in  or  and included in the static representation. Correlation information may be provided for each selected event correlating the visual representation generated for the event to the mark on the timeline section identifying when the event occurs along the timeline section. Tags may also be generated for each selected event corresponding to the time when the selected event occurs in the time-based media information and included in the static representation.","The static representation may also comprise information identifying the number of events that were determined for each timeline sections and the number of events that were eventually selected for inclusion in the static representation. For example, as depicted in , for timeline section - thirteen events were determined for the timeline section but only ten (10 of 13) were selected for inclusion in the static representation. On the other hand, for timeline section - three events were determined for the timeline section and all three (3 of 3) were selected for inclusion in the static representation.","According to an embodiment of the present invention, the placement technique that is used to determine which events to select for inclusion in the static representation is also configured to determine the locations of the visual representations of the events along the timeline sections. This is especially useful when the events occur along a timeline and are closely bunched together. For example, for timeline section - depicted in , seven slides events all co-occur in close proximity to each other with respect to time and are closely marked together on timeline section -. The placement technique figures out the optimal way to organize the events such that the resultant static representation is easily readable. In one embodiment, this is done by centering the visual representation corresponding to the \u201cmiddle\u201d event (e.g., event 00:22:59) over the general timeline location. The remaining events (or visual representations of the events) are then added, by time, from left to right.","As described above, different kinds of tags may be included in the visual representation. For example, tags may be included in the static representation for each event included in the static representation. Each tag for an event may represent or encode the time when the event occurs in the time-based media information. Tags associated with events enable access to the time-based media information. Tags may also be associated with a timeline or sections of a timeline. For example, as depicted in , tags  associated with each timeline section correspond to the start time of the time segment represented by the timeline section, a middle time point of the time segment represented by the timeline section, and an end time of the time segment represent by the timeline section. A user may select any of these tags to initiate playback of information from the time corresponding to the selected tag. Tags may also be printed for (e.g., tags  depicted in ) for controlling playback of the output information. For example, tags  may be selected to play\/pause the playback, fast forward the playback, perform rewind operations, or perform other user configurable operations on the time-based media information that is output. Tags  may also include tags for performing other types of actions. For example, tags may be included for performing actions such as sending an email, sending a fax, saving a document (e.g., the time-based media information or potion thereof), printing a document, etc.","The tags may be read using a reading device from a paper document on which the static representation is printed to initiate playback of the time-based media information or to initiate performance of an action corresponding to the tag. In the embodiments depicted in , the tags are in the form of barcodes that may be read by a barcode reader. Other types of tags may also be used.",{"@attributes":{"id":"p-0114","num":"0123"},"figref":["FIG. 7","FIG. 7","FIG. 7","FIG. 7"],"b":["700","700"]},"Processing is initiated upon receiving information identifying a tag read from a static representation (step ). A check is then made to see if the read tag is a time tag or an action tag (step ). A time tag is a tag encoding a time, for example, tags associated with events and timelines. An action tag is a tag that encodes information identifying an action to be performed, for examples tags .","If it is determined in  that the tag is an action tag, then the action corresponding to the tag is initiated (step ). If it is determined in  that the tag is a time tag, then playback of time-based media information from the time (or around the time, plus or minus some time) corresponding to the read tag is initiated (step ).","For example, in , selecting a tag associated with a slides event invokes an application (e.g., a media player) that outputs the slides information for the selected slides event and also starts playback of the video (and audio or other information if available) information from the time corresponding to the selected tag (i.e., from a time when that event occurs in the time-based media information). The output device to be used for the playback is may be user-configurable. In one embodiment, a signal is sent from the tag reading device to a server that has access to the time-based media information. The server may then invoke an application (e.g., a media player) and start playback of the information on an output device. The server may also be configured to communicate a portion of the time-based media information to another device for output.",{"@attributes":{"id":"p-0118","num":"0127"},"figref":["FIG. 8","FIG. 8","FIG. 8"],"b":["800","800"]},"Page  depicts a static representation generated for a video game \u201cHero Rescue\u201d. As shown in , information  related to the game is printed at the top of page . Information  includes the name of the game, information identifying a player of the game, the level of the game for which the static representation is generated, the duration of the time-based media information for which the static representation is generated, and the date on which the game was played. Information  also indicates the events (occurrences of \u201chealth packs\u201d) that are included in the static representation.","A timeline  (laid vertically) is printed on page . The top of timeline  represents the start time (00:00:00) of the game recording and the bottom of timeline  represents the end (01:23:43) of the time period for which the static representation is generated. Various events  are mapped to timeline . In , the events correspond to occurrences in the time-based media information when the user picked up health packs in the game. A visual representation is provided for each event. In , the visual representation for each event is a keyframe extracted from the recorded video portion of the time-based media information for the game when the event occurs. In alternative embodiments, multiple visual representations may be used to depict each event.","A tag  is printed for each event. Each tag  (\u201ctime tag\u201d) for an event represents or encodes a time during the game simulation when the event approximately occurred. In , barcodes are used to implement the tags. A time  corresponding to each tag associated with an event is also printed. For each event, a line or arrow  (correlation information) is printed showing the approximate location along timeline  when the event occurs in the recorded time-based media information for the game simulation. Time tag  enables access to the recorded time-based media information and may initiate replay of the time-based media information from a time corresponding to the scanned tag. A set of tags  is also printed for performing various actions such as controlling playback of the output information, fast forward the playback, perform rewind operations, etc.","As depicted in , , and , various different formats may be used for representing a timeline in the static representation. Further, the time-based media information for which the static representation is generated may be provided from different sources. Various other styles and formats may also be used.",{"@attributes":{"id":"p-0123","num":"0132"},"figref":["FIG. 9","FIG. 9"],"b":["900","900","902","904","906","908","910","912","914","916"]},"Bus subsystem  provides a mechanism for letting the various components and subsystems of computer system  communicate with each other as intended. Although bus subsystem  is shown schematically as a single bus, alternative embodiments of the bus subsystem may utilize multiple busses.","Network interface subsystem  provides an interface to other computer systems, networks, and devices. Network interface subsystem  serves as an interface for receiving data from and transmitting data to other systems from computer system .","User interface input devices  may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a barcode scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and other types of input devices. In general, use of the term \u201cinput device\u201d is intended to include all possible types of devices and mechanisms for inputting information to computer system .","User interface output devices  may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices, etc. The display subsystem may be a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), or a projection device. In general, use of the term \u201coutput device\u201d is intended to include all possible types of devices and mechanisms for outputting information from computer system . The output devices may be used to playback time-based media information.","Storage subsystem  may be configured to store the basic programming and data constructs that provide the functionality of the present invention. Software (code modules or instructions) that provides the functionality of the present invention may be stored in storage subsystem . These software modules or instructions may be executed by processor(s) . Storage subsystem  may also provide a repository for storing data used in accordance with the present invention. Storage subsystem  may comprise memory subsystem  and file\/disk storage subsystem .","Memory subsystem  may include a number of memories including a main random access memory (RAM)  for storage of instructions and data during program execution and a read only memory (ROM)  in which fixed instructions are stored. File storage subsystem  provides persistent (non-volatile) storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a Compact Disk Read Only Memory (CD-ROM) drive, an optical drive, removable media cartridges, and other like storage media.","Computer system  can be of various types including a personal computer, a portable computer, a workstation, a network computer, a mainframe, a kiosk, or any other data processing system. Due to the ever-changing nature of computers and networks, the description of computer system  depicted in  is intended only as a specific example for purposes of illustrating the preferred embodiment of the computer system. Many other configurations having more or fewer components than the system depicted in  are possible.","Although specific embodiments of the invention have been described, various modifications, alterations, alternative constructions, and equivalents are also encompassed within the scope of the invention. The described invention is not restricted to operation within certain specific data processing environments, but is free to operate within a plurality of data processing environments. Additionally, although the present invention has been described using a particular series of transactions and steps, it should be apparent to those skilled in the art that the scope of the present invention is not limited to the described series of transactions and steps. The various techniques discussed in the applications identified in the \u201cCross-Reference to Related Applications\u201d section may also be used to generate the static representation.","Further, while the present invention has been described using a particular combination of hardware and software, it should be recognized that other combinations of hardware and software are also within the scope of the present invention. The present invention may be implemented only in hardware, or only in software, or using combinations thereof.","The specification and drawings are, accordingly, to be regarded in an illustrative rather than a restrictive sense. It will, however, be evident that additions, subtractions, deletions, and other modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0020","num":"0029"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0021","num":"0030"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0022","num":"0031"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0023","num":"0032"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0024","num":"0033"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0025","num":"0034"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0026","num":"0035"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0027","num":"0036"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0028","num":"0037"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
