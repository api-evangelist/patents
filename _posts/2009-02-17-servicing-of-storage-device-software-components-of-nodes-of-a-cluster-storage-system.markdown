---
title: Servicing of storage device software components of nodes of a cluster storage system
abstract: Described herein are method and apparatus for servicing software components of nodes of a cluster storage system. During data-access sessions with clients, client IDs and file handles for accessing files are produced and stored to clients and stored (as session data) to each node. A serviced node is taken offline, whereby network connections to clients are disconnected. Each disconnected client is configured to retain its client ID and file handles and attempt reconnections. Session data of the serviced node is made available to a partner node (by transferring session data to the partner node). After clients have reconnected to the partner node, the clients may use the retained client IDs and file handles to continue a data-access session with the partner node since the partner node has access to the session data of the serviced node and thus will recognize and accept the retained client ID and file handles.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09215279&OS=09215279&RS=09215279
owner: NetApp, Inc.
number: 09215279
owner_city: Sunnyvale
owner_country: US
publication_date: 20090217
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION","I. CLUSTER ENVIRONMENT","II. STORAGE OPERATING SYSTEM","III. SHARED STORAGE","IV. CLIENT DATA-ACCESS SESSION","V. SERVICING OF SOFTWARE COMPONENTS OF A NODE","VI. SERVICING OF N-BLADE SOFTWARE COMPONENT OF A NODE","VII. SERVICING OF D-BLADE SOFTWARE COMPONENT OF A NODE"],"p":["The present invention relates to storage systems, and particularly, to servicing of storage device software components of nodes of a cluster storage system.","A storage system typically comprises one or more storage devices into which information may be entered, and from which information may be obtained, as desired. The storage system includes a storage operating system that functionally organizes the system by, inter alia, invoking storage operations in support of a storage service implemented by the system. The storage system may be implemented in accordance with a variety of storage architectures including, but not limited to, a network-attached storage environment, a storage area network and a disk assembly directly attached to a client or host computer. The storage devices are typically disk drives organized as a disk array, wherein the term \u201cdisk\u201d commonly describes a self-contained rotating magnetic media storage device. The term disk in this context is synonymous with hard disk drive (HDD) or direct access storage device (DASD).","The storage operating system of the storage system may implement a high-level module, such as a file system, to logically organize the information stored on volumes as a hierarchical structure of data containers, such as files and logical units (LUs). For example, each \u201con-disk\u201d file may be implemented as set of data structures, i.e., disk blocks, configured to store information, such as the actual data for the file. These data blocks are organized within a volume block number (vbn) space that is maintained by the file system. The file system may also assign each data block in the file a corresponding \u201cfile offset\u201d or file block number (fbn). The file system typically assigns sequences of fbns on a per-file basis, whereas vbns are assigned over a larger volume address space. The file system organizes the data blocks within the vbn space as a \u201clogical volume\u201d; each logical volume may be, although is not necessarily, associated with its own file system.","A known type of file system is a write-anywhere file system that does not overwrite data on disks. If a data block is retrieved (read) from disk into a memory of the storage system and \u201cdirtied\u201d (i.e., updated or modified) with new data, the data block is thereafter stored (written) to a new location on disk to optimize write performance. A write-anywhere file system may initially assume an optimal layout such that the data is substantially contiguously arranged on disks. The optimal disk layout results in efficient access operations, particularly for sequential read operations, directed to the disks. An example of a write-anywhere file system that is configured to operate on a storage system is the Write Anywhere File Layout (WAFL\u00ae) file system available from NetApp, Inc. Sunnyvale, Calif.","The storage system may be further configured to operate according to a client\/server model of information delivery to thereby allow many clients to access data containers stored on the system. In this model, the client may comprise an application, such as a database application, executing on a computer that \u201cconnects\u201d to the storage system over a computer network, such as a point-to-point link, shared local area network (LAN), wide area network (WAN), or virtual private network (VPN) implemented over a public network such as the Internet. Each client may request the services of the storage system by issuing access requests (read\/write requests) as file-based and block-based protocol messages (in the form of packets) to the system over the network.","A plurality of storage systems may be interconnected to provide a storage system architecture configured to service many clients. In some embodiments, the storage system architecture provides one or more aggregates and one or more volumes distributed across a plurality of nodes interconnected as a cluster. The aggregates may be configured to contain one or more volumes. The volumes may be configured to store content of data containers, such as files and logical units, served by the cluster in response to multi-protocol data access requests issued by clients. Each node of the cluster includes (i) a storage server (referred to as a \u201cD-blade\u201d) adapted to service a particular aggregate or volume and (ii) a multi-protocol engine (referred to as an \u201cN-blade\u201d) adapted to redirect the data access requests to any storage server of the cluster.","In the illustrative embodiment, the storage server of each node is embodied as a disk element (D-blade) and the multi-protocol engine is embodied as a network element (N-blade). The N-blade receives a multi-protocol data access request from a client, converts that access request into a cluster fabric (CF) message and redirects the message to an appropriate D-blade of the cluster. In some embodiments, the D-blade and N-blade of a node comprise software components that are serviced (e.g., upgraded, re-installed, perform maintenance, repaired, etc.) from time to time.","Typically, clients will connect with a node for data-access sessions with the node. During a data-access session with a node, a client may obtain a client identifier (ID) for connecting with the N-blade and one or more file handles to access files through the D-blade. The client ID needs to be produced through a connection authentication procedure and each file handle needs to be produced through an access request validation procedure. The client then uses the client ID and file handles in subsequent access requests sent to the node. The node also stores session data comprising the client ID and file handles of each connected client, so it may recognize the client IDs and file handles sent in the access requests. If the node does not recognize the client ID and file handle in an access request, the node may deny processing of the access request.","Typically, servicing of the D-blade and N-blade of a node requires the serviced node to be taken offline, thereby disconnecting any client data-access sessions with the serviced node. Conventionally, upon disconnect with the serviced node, the client will drop\/delete the client ID and all file handles and the serviced node will close all open files accessed by the file handles. Upon reconnection with a failover partner node of the serviced node, the client ID needs to be reproduced through the connection authentication procedure and each file handle needs to be reproduced through an access request validation procedure. Thus, servicing of the D-blade and N-blade of each node typically causes substantial disruption to client data-access sessions. As such, there is a need for a less disruptive way of servicing software components of nodes of a cluster.","In some embodiments, a servicing module residing on each node of the cluster provides less disruptive servicing of the software components of the nodes of a cluster. In these embodiments, each client is configured to retain its client ID and any file handles upon disconnection with a serviced node. Each client is also configured to automatically attempt to reconnect with a partner node and attempt to re-establish the data-access session with the partner node using the retained client ID and file handles. While the clients attempt to reconnect with the partner node, session data (comprising the client IDs and file handles) stored in the serviced node made available to the partner node (e.g., is transferred to the partner node). After the clients have reconnected to the partner node, the clients may use the retained client ID and file handles to continue the data-access session with the partner node. This is possible since the partner node now has access to the session data of the serviced node and thus will recognize and accept the retained client ID and file handles. The partner node may \u201crecognize\u201d the received client IDs and file handles by comparing and matching the received client IDs to the stored client IDs and file handles in the session data of the serviced node.","In the embodiments described herein, there is less disruption to clients accessing the cluster for data-access sessions during servicing of software components of the nodes of a cluster. In particular, when a serviced node is taken offline and clients are disconnected from the serviced node, the disconnected clients can continue the data-access sessions with the partner node using the previously obtained client IDs and file handles. As such, the client ID does not need to be reproduced through the connection authentication procedure and each file handle does not need to be reproduced through an access request validation procedure.","In some embodiments, the D-blade and N-blade of a node comprise software components that are serviced (e.g., upgraded, re-installed, perform maintenance, repaired, etc.). In some embodiments, the N-blade of a node comprises a network software component configured for interacting\/interfacing with clients through a network connection (e.g., for receiving access requests from the clients and routing the access requests to the appropriate D-blade). In some embodiments, the D-blade of a node comprises a storage device software component configured for interacting\/interfacing with a set of storage devices comprising a shared storage of the cluster (e.g., for receiving access requests from the N-blade and performing the access requests on the shared storage).","The shared storage of the cluster may be accessible by each node of the cluster. However, particular storage sub-portions of the shared storage may be accessible only to an assigned\/associated node in normal operating conditions. For example, a system aggregate and a data aggregate in the shared storage is typically assigned\/associated for each node for the node to access exclusively. In a failover event (such as a particular node being offline), the data aggregate assigned to the particular node may be made accessible to the partner node as well. The system aggregate, however, is typically accessible only to the particular node and not to the partner mode, even during a failover event.","In some embodiments, only the D-blade software component of one or more nodes of a cluster is serviced. Servicing of the D-blades of the cluster begins with a serviced node A having partner node B, and is repeated for each node of the cluster. Prior to servicing, the serviced D-blade of node A has stored session data A (e.g., comprising \u201coriginal\u201d client IDs file handles and permission and lock state data) to the system aggregate A and the partner D-blade of node B has stored session data B to the system aggregate B. Also, the serviced D-blade services data from data aggregate A and the partner D-blade services data from data aggregate B in the shared storage.","Prior to servicing, session data A may contain one or more file handles produced by the serviced node for one or more files (stored on the shared storage) that were accessed using the serviced D-blade. Each file handle for a file may be produced by the serviced node for a client submitting an initial access request for the file. The file handle may be produced during an access request validation procedure (performed by the serviced D-blade) that validates the initial access request (e.g., by determining permission and lock type associated with the client submitting the initial access request).","After servicing is initiated, the serviced D-blade of node A is taken offline. For each client having an open file handle for a file stored on data aggregate A, the network connection between the client and an N-blade is disconnected by closing the respective network port on the N-blade. As such, each client currently accessing a file using the serviced D-blade is disconnected. Each disconnected client is configured to begin to attempt re-connections with the N-blade while also retaining any original client ID and file handles received prior to the network disconnection. Servicing of the serviced D-blade software component then begins.","Session data A stored in system aggregate A is transferred to system aggregate B for storage. The partner D-Blade will now have access to session data A (stored in system aggregate B) for files stored on data aggregate A. The partner D-Blade will also be configured to access data aggregate A upon failure of the serviced D-blade. The network ports are then re-opened and the disconnected clients will reconnect with their respective N-blades. The partner D-blade begins servicing data on data aggregate A using the session data A by receiving access requests (containing original client ID and original file handles) from the reconnected clients. The partner D-blade may \u201crecognize\u201d the received client IDs and file handles by matching the received client IDs to the stored client IDs and file handles in session data A.","As such, the partner D-blade may use the original client ID and original file handles to validate access requests to previously opened files (by using the permission and lock state data). Thus the partner D-blade accesses the previously opened files using the original client ID and original file handles, without having to perform an access request validation procedure for the previously opened files and without having to produce new file handles for the previously opened files.","This process may be repeated for each node in the cluster, whereby each node in the cluster is serviced one after the other. By performing servicing on one node in the cluster at a time, the entire cluster system does not need to be taken offline, thus reducing disruption to client data-access.","In the following description, numerous details are set forth for purpose of explanation. However, one of ordinary skill in the art will realize that the embodiments described herein may be practiced without the use of these specific details. In other instances, well-known structures and devices are shown in block diagram form in order not to obscure the description with unnecessary detail.","The description that follows is divided into seven sections. Section I describes a cluster environment in which some embodiments operate. Section II describes a storage operating system having a servicing module for servicing software components of nodes of the cluster. Section III describes a shared storage of the cluster. Section IV describes a client data-access session between a client and a node. Section V describes servicing of software components of nodes of the cluster. Section VI describes servicing of network software components (N-blades) of nodes of the cluster. Section VII describes servicing of storage device software components (D-blades) of nodes of the cluster.",{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 1","b":["100","100","200","200","100","200","310","350"]},"The N-blade  includes functionality that enables the node  to connect to clients  over a computer network , while each D-blade  connects to one or more storage devices, such as disks  of a disk array . The nodes  are interconnected by a cluster switching fabric  which, in the illustrative embodiment, may be embodied as a Gigabit Ethernet switch. An exemplary distributed file system architecture is generally described in U.S. Patent Application Publication No. US 2002\/0116593 titled METHOD AND SYSTEM FOR RESPONDING TO FILE SYSTEM REQUESTS, by M. Kazar et al. published Aug. 22, 2002.","It should be noted that while there is shown an equal number of N and D-blades in the illustrative cluster , there may be differing numbers of N and\/or D-blades in accordance with various embodiments. For example, there may be a plurality of N-blades and\/or D-blades interconnected in a cluster configuration  that does not reflect a one-to-one correspondence between the N and D-blades. As such, the description of a node  comprising one N-blade and one D-blade should be taken as illustrative only. For example, a node  may also have one N-blade and a plurality of D-blades, a plurality of N-blades and one D-blade, or a plurality of N-blades and a plurality of D-blades.","The clients  may be general-purpose computers configured to interact with the node  in accordance with a client\/server model of information delivery. That is, each client  may request the services of the node  (e.g., by submitting read\/write requests), and the node  may return the results of the services requested by the client , by exchanging packets over the network . The client  may submit access requests by issuing packets using file-based access protocols, such as the Common Internet File System (CIFS) protocol or Network File System (NFS) protocol, over the Transmission Control Protocol\/Internet Protocol (TCP\/IP) when accessing information in the form of files and directories. Alternatively, the client may submit access requests by issuing packets using block-based access protocols, such as the Small Computer Systems Interface (SCSI) protocol encapsulated over TCP (iSCSI) and SCSI encapsulated over Fibre Channel (FCP), when accessing information in the form of blocks.","In some embodiments, a client  connects to a node  for a data-access session, during which time the client  may receive one or more file handles from the node  for accessing one or more files (as discussed below). Upon a network connection failure with the node , the client  may be configured to retain any received file handles for a predetermined grace time period after the initial connection failure occurs. During this predetermined grace period, the client  may also be configured to automatically perform a predetermined number of reconnection attempts. If a reconnection is not successful after the predetermined number of reconnection attempts, the client  may be configured to drop\/delete the previous file handle(s) obtained by the client . If within the predetermined number of reconnection attempts, a reconnection is successful, the client  may be configured to retain the previous file handles and re-use the file handles to access the same files, without needing to reproduce new file handles. This feature is sometimes referred to as the \u201cdurable file handle\u201d feature. In some embodiments, a client  executes a current Windows\u00ae operating system that implements a current network file protocol, such as Server Message Block (SMB) 2.0 that provides this \u201cdurable file handle\u201d feature.","In some embodiments, the totality of storage space provided by the disks  and disk arrays  of the cluster  comprise a total shared storage space (referred to as \u201cshared storage \u201d) of the cluster . The shared storage  is accessible by each D-blade  of each node  in the cluster . The shared storage  is discussed in detail in Section III. In some embodiments, the cluster  may provide high availability of service to clients  in accessing the shared storage . For example, the nodes  may be configured to communicate with one another (e.g., via cluster switching fabric ) to act collectively to offset any single node  failure within the cluster . In these embodiments, each node  may have a predetermined failover \u201cpartner\u201d node . When a node  failure occurs (where the failed node is no longer capable of processing access requests for clients ), access requests sent to the failed node  may be re-directed to the partner node  for processing. Note that a node  failure may occur unintentionally or intentionally (e.g., where a node is taken offline for servicing).",{"@attributes":{"id":"p-0041","num":"0040"},"figref":["FIG. 2","FIG. 1"],"b":["200","200","222","224","225","226","228","230","223","230"],"i":"a,b"},"The cluster access adapter  comprises a plurality of ports adapted to couple the node  to other nodes of the cluster . In the illustrative embodiment, Ethernet is used as the clustering protocol and interconnect media, although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate embodiments where the N-blades and D-blades are implemented on separate storage systems or computers, the cluster access adapter  is utilized by the N\/D-blade for communicating with other N\/D-blades in the cluster .","Each node  is illustratively embodied as a dual processor storage system executing a storage operating system  that preferably implements a high-level module, such as a file system, to logically organize the information as a hierarchical structure of named data containers, such as directories, files and special types of files called virtual disks (hereinafter generally \u201cblocks\u201d) on the disks. However, it will be apparent to those of ordinary skill in the art that the node  may alternatively comprise a single or more than two processor system. Illustratively, one processor executes the functions of the N-blade  on the node, while the other processor executes the functions of the D-blade .","The memory  illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data used in some embodiments. The processor and adapters may, in turn, comprise processing elements and\/or logic circuitry configured to execute the software code and manipulate the data. The storage operating system , portions of which is typically resident in memory and executed by the processing elements, functionally organizes the node  by, inter alia, invoking storage operations in support of the storage services implemented by the node. It will be apparent to those skilled in the art that other processing and memory means, including various computer readable media, may be used for storing and executing program instructions pertaining to the invention described herein.","The network adapter  comprises a plurality of ports adapted to couple the node  to one or more clients  over point-to-point links, wide area networks, virtual private networks implemented over a public network (Internet) or a shared local area network. The network adapter  thus may comprise the mechanical, electrical and signaling circuitry needed to connect the node to the network. Illustratively, the computer network  may be embodied as an Ethernet network or a Fibre Channel (FC) network. Each client  may communicate with the node  over the network  by exchanging discrete frames or packets of data according to pre-defined protocols, such as TCP\/IP.","The storage adapter  cooperates with the storage operating system  executing on the node  to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape, optical, DVD, magnetic tape, bubble memory, electronic random access memory, micro-electro mechanical and any other similar media adapted to store information, including data and parity information. However, as illustratively described herein, the information is preferably stored on the disks  of array . The storage adapter comprises a plurality of ports having input\/output (I\/O) interface circuitry that couples to the disks over an I\/O interconnect arrangement, such as a conventional high-performance, FC link topology.","Storage of information on each array  is preferably implemented as one or more storage \u201cvolumes\u201d that comprise a collection of physical storage disks  cooperating to define an overall logical arrangement of volume block number (vbn) space on the volume(s). Each logical volume is generally, although not necessarily, associated with its own file system. The disks within a logical volume\/file system are typically organized as one or more groups, wherein each group may be operated as a Redundant Array of Independent (or Inexpensive) Disks (RAID). Most RAID implementations, such as a RAID-4 level implementation, enhance the reliability\/integrity of data storage through the redundant writing of data \u201cstripes\u201d across a given number of physical disks in the RAID group, and the appropriate storing of parity information with respect to the striped data. An illustrative example of a RAID implementation is a RAID-4 level implementation, although it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.","To facilitate access to the disks , the storage operating system  implements a write-anywhere file system that cooperates with one or more virtualization modules to \u201cvirtualize\u201d the storage space provided by disks . The file system logically organizes the information as a hierarchical structure of named directories and files on the disks. Each \u201con-disk\u201d file may be implemented as set of disk blocks configured to store information, such as data, whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module(s) allow the file system to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers (luns).","In the illustrative embodiment, the storage operating system is preferably the Data ONTAP\u00ae software operating system available from NetApp, Inc., Sunnyvale, Calif. that implements a Write Anywhere File Layout (WAFL\u00ae) file system. However, it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such, where the term \u201cWAFL\u201d is employed, it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.",{"@attributes":{"id":"p-0050","num":"0049"},"figref":["FIG. 3","FIG. 2"],"b":["300","200","300","325","310","180","200","325","312","314","316","315"]},"A file system protocol layer provides multi-protocol file access and, to that end, includes support for the Direct Access File System (DAFS) protocol , the NFS protocol , the CIFS protocol  and the Hypertext Transfer Protocol (HTTP) protocol . A VI layer  implements the VI architecture to provide direct access transport (DAT) capabilities, such as RDMA, as required by the DAFS protocol . An iSCSI driver layer  provides block protocol access over the TCP\/IP network protocol layers, while a FC driver layer  receives and transmits block access requests and responses to and from the node. The FC and iSCSI drivers provide FC-specific and iSCSI-specific access control to the blocks and, thus, manage exports of luns to either iSCSI or FCP or, alternatively, to both iSCSI and FCP when accessing the blocks on the node .","In addition, the storage operating system  includes a series of software layers organized to form a storage server  (D-blade ) that provides data paths for accessing information stored on the disks  of the node . To that end, the storage server  includes a file system module , a RAID system module  and a disk driver system module . The RAID system  manages the storage and retrieval of information to and from the volumes\/disks in accordance with I\/O operations, while the disk driver system  implements a disk access protocol such as, e.g., the SCSI protocol.","The file system  implements a virtualization system of the storage operating system  through the interaction with one or more virtualization modules illustratively embodied as, e.g., a virtual disk (vdisk) module (not shown) and a SCSI target module . The SCSI target module  is generally disposed between the FC and iSCSI drivers ,  and the file system  to provide a translation layer of the virtualization system between the block (lun) space and the file system space, where luns are represented as blocks.","The file system  is illustratively a message-based system that allocates storage space for itself in the disk array  and controls the layout of information on the array. The file system further provides logical volume management capabilities for use in access to the information stored on the storage devices, such as disks. That is, in addition to providing file system semantics, the file system  provides functions normally associated with a volume manager. These functions include (i) aggregation of the disks, (ii) aggregation of storage bandwidth of the disks, and (iii) reliability guarantees, such as mirroring and\/or parity (RAID). The file system  illustratively implements the WAFL file system (hereinafter generally the \u201cwrite-anywhere file system\u201d) having an on-disk format representation that is block-based using, e.g., 4 kilobyte (kB) blocks and using index nodes (\u201cinodes\u201d) to identify files and file attributes (such as creation time, access permissions, size and block location). The file system uses files to store metadata describing the layout of its file system; these metadata files include, among others, an inode file. A file (data container) handle, i.e., an identifier that includes an inode number, is used to retrieve an inode from disk.","All inodes of the write-anywhere file system may be organized into the inode file. A file system (fs) info block specifies the layout of information in the file system and includes an inode of a data container, e.g., file, that includes all other inodes of the file system. Each logical volume (file system) has an fsinfo block that may be stored at a fixed or variable location within, e.g., a RAID group. The inode of the inode file may directly reference (point to) data blocks of the inode file or may reference indirect blocks of the inode file that, in turn, reference data blocks of the inode file. Within each data block of the inode file are embedded inodes, each of which may reference indirect blocks that, in turn, reference data blocks of a file.","Operationally, an access request (read\/write request) from the client  is forwarded as a packet over the computer network  and onto the node  where it is received at the network adapter . A network driver (of layer  or layer ) processes the packet and, if appropriate, passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write-anywhere file system . Here, the file system produces operations to load (retrieve) the requested data from disk  if it is not resident \u201cin core\u201d, i.e., in memory . If the information is not in memory, the file system  indexes into the inode file using the inode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system ; the logical vbn is mapped to a disk identifier and disk block number (disk,dbn) and sent to an appropriate driver (e.g., SCSI) of the disk driver system . The disk driver accesses the dbn from the specified disk  and loads the requested data block(s) in memory for processing by the node. Upon completion of the access request, the node  (and storage operating system ) returns a reply to the client  over the network .","It should be noted that the software \u201cpath\u201d through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is, in an alternate embodiment of the invention, a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array (FPGA) or an application specific integrated circuit (ASIC). This type of hardware implementation increases the performance of the storage service provided by node  in response to a request issued by client . Moreover, in another alternate embodiment of the invention, the processing elements of adapters ,  may be configured to offload some or all of the packet processing and storage access operations, respectively, from processor , to thereby increase the performance of the storage service provided by the node. It is expressly contemplated that the various processes, architectures and procedures described herein can be implemented in hardware, firmware or software.","As used herein, the term \u201cstorage operating system\u201d generally refers to the computer-executable code operable on a computer to perform a storage function that manages data access and may, in the case of a node , implement data access semantics of a general purpose operating system. The storage operating system  can also be implemented as a microkernel, an application program operating over a general-purpose operating system, such as UNIX\u00ae or Windows NT\u00ae, or as a general-purpose operating system with configurable functionality, which is configured for storage applications as described herein.","In addition, it will be understood to those skilled in the art that the invention described herein may apply to any type of special-purpose (e.g., file server, filer or storage serving appliance) or general-purpose computer, including a standalone computer or portion thereof, embodied as or including a storage system. Moreover, the teachings of this invention can be adapted to a variety of storage system architectures including, but not limited to, a network-attached storage environment, a storage area network and disk assembly directly-attached to a client or host computer. The term \u201cstorage system\u201d should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system, the teachings of the present invention may be utilized with any suitable file system, including a write in place file system.","In some embodiments, the storage server  is embodied as D-blade  of the storage operating system  to service one or more volumes of array . In addition, the multi-protocol engine  is embodied as N-blade  to (i) perform protocol termination with respect to a client issuing incoming data access request packets over the network , as well as (ii) redirect those data access requests to any storage server  of the cluster . Moreover, the N-blade  and D-blade  cooperate to provide a highly-scalable, distributed storage system architecture of the cluster . To that end, each blade includes a cluster fabric (CF) interface module adapted to implement intra-cluster communication among the blades (e.g., communication between blades of the same node or communication between blades of different nodes) using CF protocol messages.","For example, the protocol layers (e.g., the NFS\/CIFS layers and the iSCSI\/FC layers) of the N-blade  may function as protocol servers that translate file-based and block-based access requests from clients  into CF protocol messages used for communication with the D-blade . In some embodiments, the N-blade servers convert the incoming client access requests into file system primitive operations (commands) that are embedded within CF protocol messages by the CF interface module  for transmission to the D-blades  of the cluster . Notably, the CF interface modules  cooperate to provide a single file system image across all D-blades  in the cluster . Thus, any network port of an N-blade that receives a client request can access any data container within the single file system image located on any D-blade  of the cluster.","In some embodiments, the N-blade  and D-blade  are implemented as separately-scheduled processes of storage operating system . In other embodiments, the N-blade  and D-blade  may be implemented as separate software components\/code within a single operating system process. Communication between an N-blade and D-blade in the same node  is thus illustratively effected through the use of CF messages passing between the blades. In the case of remote communication between an N-blade and D-blade of different nodes, such CF message passing occurs over the cluster switching fabric .","A known message-passing mechanism provided by the storage operating system to transfer information between blades (processes) is the Inter Process Communication (IPC) mechanism. The protocol used with the IPC mechanism is illustratively a generic file and\/or block-based \u201cagnostic\u201d CF protocol that comprises a collection of methods\/functions constituting a CF application programming interface (API). Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from NetApp, Inc. The SpinFS protocol is described in the above-referenced U.S. Patent Application Publication No. US 2002\/0116593.","The CF interface module  implements the CF protocol for communicating file system commands\/messages among the blades of cluster . Communication is illustratively effected by the D-blade exposing the CF API to which an N-blade (or another D-blade) issues calls. To that end, the CF interface module  is organized as a CF encoder and CF decoder. The CF encoder of, e.g., CF interface on N-blade  encapsulates a CF message as (i) a local procedure call (LPC) when communicating a file system command to a D-blade  residing on the same node  or (ii) a remote procedure call (RPC) when communicating the command to a D-blade residing on a remote node of the cluster . In either case, the CF decoder of CF interface on D-blade  de-encapsulates the CF message and processes the file system command. As used herein, the term \u201cCF message\u201d may be used generally to refer to LPC and RPC communication between blades of the cluster.","In some embodiments, the storage operating system  also comprises a servicing module for performing servicing on the N-blade  and\/or D-blade  software components of one or more nodes  of a cluster. For example, the servicing module may perform upgrading (replacing software with newer versions), re-installing, maintenance, repairing, etc. of the N-blade  and\/or D-blade  software components.  shows a conceptual diagram of a node  comprising N-blade  and D-blade , each blade comprising a servicing module . The servicing modules  in each blade may operate in conjunction to perform servicing of the N-blade  and\/or D-blade  software components. For example, the servicing modules  may issue CF messages or other commands to the N-blade  and D-blade  to perform the methods described herein. After initiation, the servicing modules  may do so automatically (without human initiation or intervention) for one or more nodes  of a cluster  with reduced disruption to clients  connected to the nodes .","The servicing module  may be initiated to perform the servicing of software components by administrative command. The administrative command may be received by a message interface module  comprising, for example, a user interface or command interface. The message interface  may be used to receive administrative commands (e.g., in the form of CF messages) for managing and issuing commands to the node . The message interface  then routes the received command to the proper software module(s).","As discussed above, a node  may have a network adapter  comprising a plurality of network ports  (data-access ports). Each network port  may provide a network connection between the node  and a client  to provide data-access service to the client  over a network . In some embodiments, each network port  is implemented as a virtual component comprising a virtual interface (VIF). Each network port \/VIF may have an associated unique identifier (e.g., an Internet Protocol (IP) address endpoint) within the cluster  that is received and used by the client  to establish the network connection. A VIF may transparently change association from one network port  to another network port  (within the same node  or across different nodes ) while data-access service to a client  continues uninterrupted through the change. The VIF may retain the same identifier (e.g., an IP address endpoint) with the client  through the change of association so that changes of the underlying network ports  occur transparently to a client  connected with a VIF. As such, the network connection  with the client  is maintained transparently to the client  as changes of association of the underlying network ports  occur.","A VIF may provide a client interface to the N-blade  of a node. As such, in the following description and figures, a VIF is sometimes conceptually described and shown as part of the N-blade . In some embodiments, when an N-blade  of a node  is being serviced by the servicing module , any VIFs of the serviced N-blade  and serviced node  may change association from network ports  of the serviced N-blade  and serviced node  to network ports  of the partner N-blade  and partner node  during the servicing. In this way, data-access to any clients  formerly connected to the serviced N-blade  and serviced node  may continue through the partner N-blade  and partner node  during the servicing.","As discussed above, in relation to , the totality of storage space provided by the disks  and disk arrays  of the cluster  comprise a total shared storage space (referred to as \u201cshared storage \u201d) of the cluster . The shared storage  is accessible by each D-blade  of each node  in the cluster .  shows a conceptual diagram of an exemplary shared storage  comprising a plurality of aggregates, each aggregate comprising a sub-portion of the total available storage space of the shared storage .","In these embodiments, each node  of the cluster  is assigned\/associated with a system aggregate  and a data aggregate . For example, node A may be assigned\/associated with \u201csystem aggregate A\u201d and \u201cdata aggregate A\u201d and node B may be assigned\/associated with \u201csystem aggregate B\u201d and \u201cdata aggregate B.\u201d Each system aggregate may be used for storing system information for the associated node . Such system information may include session data  comprising data collected from the N-blade  and D-blade  during data-access sessions with clients . Each data aggregate may be used for storing client data for the associated node , whereby clients  may read and write to the data.","Each node  may be configured to access only the system and data aggregates assigned to the respective node . As such, the D-blade  of each node  may be configured to access only the system and data aggregates assigned to the node . In the example of , the D-blade  of node A may be configured to access and serve data from only system aggregate A or data aggregate A and the D-blade  of node B may be configured to access and serve data from only system aggregate B or data aggregate B. Therefore, in normal operation (when node failures have not occurred), all access requests (received at any N-blade  of any node  in the cluster) for data in data aggregate A are routed through the D-blade  of node A (and have physical addresses\/file handles that specify the D-blade  of node A).","In the event of a node failure, the failed node is no longer capable of processing access requests (read\/write requests) from clients  for data in the data aggregate assigned to the failed node. In such an event, the access requests sent to the failed node  may be re-directed to the predetermined partner node  for processing. The predetermined partner node  of the failed node may be configured to replace the failed node by accessing and serving data in the data aggregate assigned to the failed node (as well as the accessing and serving data in its own assigned data aggregate).","As used herein, node A is the \u201cprimary\u201d node and the D-blade  of node A is the \u201cprimary\u201d D-blade  that \u201cown\u201d and \u201cservice\u201d data containers stored in data aggregate A. As used herein, node B is the \u201cprimary\u201d node and the D-blade  of node B is the \u201cprimary\u201d D-blade  that \u201cown\u201d and \u201cservice\u201d data containers stored in data aggregate B. As used herein, node B is the partner node of node A, whereby node B and the D-blade  of node B \u201cown\u201d and \u201cservice\u201d data containers stored in data aggregates A and B upon failure of node A. Under normal operating conditions (where node A has not failed), node B and the D-blade  of node B do not service data containers stored in data aggregate A. Note, however, that the N-blade  of each node can receive access requests for data in any data aggregate  of the shared storage , and will route the access requests to the appropriate D-blade  that services the requested data.",{"@attributes":{"id":"p-0074","num":"0073"},"figref":"FIG. 6","b":["600","600","610","610","600","610","602","604","606","608","600","650","650","660","630"]},"A. Client Data-Access Sessions",{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 7","b":["180","200","135","135","135","410","200","310","200","180"]},"During a client data-access session, the N-blade  and D-blade  of a node  may collect and store session data  to the system aggregate  associated with the node .  shows a conceptual diagram of exemplary session data  that may be collected and stored to a system aggregate . In the example of , the session data  is shown as a table having a plurality of entries, however, in other embodiments, the session data  may also be stored and organized in different forms other than a table, such as a database, file, or any other data structure.","In the example of , the session data  comprises N-Blade session data  that is collected and stored by the N-Blade  and D-Blade session data  that is collected and stored by the D-Blade . Note that the N-Blade  and D-Blade  collecting and storing the session data  may reside on different nodes . As such, the N-Blade session data  and the D-Blade session data  shown in  may be stored on different system aggregates . This may occur, for example, if a client  connects with the N-Blade  of node A (whereby the N-Blade stores its session data to system aggregate A) to access a file serviced by the D-Blade  of node B (whereby the D-Blade stores its session data to system aggregate B). In general, the N-Blade  may collect and store information relating to current connections with clients  and the D-Blade  may collect and store information relating to files accessed by currently connected clients .","To begin a data-access session with a node, a connection authentication procedure may be performed. For example, to initiate the data-access session with a node, the client  may send a connection request to the N-blade . The connection request may contain, for example, a user identification\/identifier (ID) and password. Upon authenticating the received client ID and password (e.g., by verifying that the user ID has permission to connect to the cluster  and the password is correct), the N-blade  may produce a client ID  and send the client ID  to the client  (which stores the received client ID). Each N-blade  of each node  may store information regarding each client  that is connected with the N-blade  to its session data  in the associated system aggregate . As shown in the example of , the N-blade  may store the user ID  and the client ID  to its session data  for each connected client .","In some embodiments, the client ID  may comprise a unique identifier produced by the N-blade  that uniquely identifies the client . A client  submitting a valid client ID, that is recognized by the N-blade , indicates to the N-blade  that the client  has already undergone the connection authentication procedure. In contrast, a client  submitting a user ID or a non-valid client ID, that is not recognized by the N-blade , indicates to the N-blade  that the client  has not already undergone the connection authentication procedure. As such, the client  may subsequently use the received client ID  to connect and interface with the N-blade  (for accessing data stored in the shared storage ), without having to resend the user ID and password and without the N-blade  having to re-authenticate the connection and reproduce the client ID. Thus use of the client ID  by the client  and recognition of the client ID  by the N-blade avoids having to re-perform the connection authentication procedure.","After the connection authentication procedure, the client  may then send an initial access request for a particular file (referred to as \u201crequested file N\u201d) in the shared storage . The initial access request may include the client ID (for interfacing with the N-blade ), a request type (read\/write), data to be written (for write requests), and a virtual address of requested file N. The virtual address of requested file N may comprise a file path specifying, for example, directory, filename, etc. Note that although the client  may interface, for example, with N-blade  of node A, the client  may request any file in the shared storage  (whether the file is stored in data aggregate A or another data aggregate).","The N-blade  receives the initial access request and uses the virtual address of requested file N to determine a D-blade identifier (ID). The D-blade ID identifies which D-blade  in the cluster is servicing the requested file N specified by the virtual address (i.e., the primary D-blade  in the cluster that accesses the data aggregate  in which the requested file N is stored). For example, if request file N is stored in data aggregate A, the D-blade ID would identify the D-blade  of node A. The N-blade  then sends the access request, the user ID, and client ID to the appropriate D-blade  that services the request file N (as specified by the D-blade ID).","The D-blade  receives the initial access request and may perform an access request validation procedure to validate the initial access request (e.g., by determining permission and lock type associated with the client submitting the initial access request). For example, the D-blade  may use the virtual address of requested file N to determine a file inode number for the requested file N. The D-blade  may then retrieve and analyze information from the file inode specified by the file inode number. A file inode typically contains metadata for an associated file, such as permission\/security and lock metadata.","The permission metadata may specify which users or user types (e.g., administrators) have permission to access the file. Permission metadata may also indicate the type of access permission a user or user type has (e.g., read only, write, etc.). The lock metadata may specify what type of lock a user or user type can obtain when accessing the file. Lock types may include, for example, exclusive (E), shared (S), or none (N). A user\/owner having an exclusive lock has exclusive read or write access to the file (whereby all other users\/processes are prevented from having read or write access to the file). Typically, only one user is given an exclusive lock on a file. A user\/owner having a shared lock has write access to the file (whereby other users\/processes are prevented from having write access but are allowed to have read access to the file). Typically, one or more users may be given a shared lock on a file. A user\/owner having a none lock has no read or write access lock on the file (whereby all other users\/processes are allowed read or write access to the file).","Using the received user ID and the metadata retrieved from the file inode, the D-blade  determines whether the received access request is valid (i.e., the user\/client  has permission to perform the specific access request on the requested file N). If so, the D-blade  may then perform the received access request on the requested file N (e.g., read data from or write data to file N) that is stored in its associated data aggregate . The D-blade  may send a message to the N-blade  indicating that the access request has been performed.","The D-blade  may also form a file handle for the requested file N, the file handle comprising the D-blade ID (produced by the N-blade ) and the file inode number (produced by the D-blade ). The file handle of a file may comprise a physical address indicating where the file is stored in the shared storage . As such, the N-blade  and D-blade  together translate\/map the received virtual address of requested file N (e.g., file path) to a physical address that may be used for locating and accessing requested file N in the shared storage . The D-blade  may send the file handle of requested file N to the N-blade .","The N-blade  then sends the file handle of file N to the client  which stores the file handle . Thereafter, the client  then includes, along with the client ID, the file handle  (rather than the file path) in subsequent access requests (after the initial access request) for file N. Submitting the file handle (physical address) of file N in subsequent access requests for file N avoids the user\/client having to re-submit the file path (virtual address) of file N and also allows for more efficient processing of the subsequent access requests by the N-blade  and D-blade . Since the file handle provides a detailed path to where the requested file is physically stored in the shared storage , the requested file may be directly accessed using the file handle, thereby avoiding having to again translate\/map between a file path (virtual address) and the file handle (physical address).","The N-blade  may store the file handle  to its session data  and associate the file handle  with the corresponding user ID  and client ID  (that identify the user\/client that obtained and is using the file handle ). The user ID \/client ID  and associated file handle  may comprise a single entry in the N-blade session data .","The D-blade  may also store information to its session data  in its associated system aggregate . As shown in the example of , the D-blade  may store to its session data  the user ID  and the client ID  (received from the N-blade) and the associated file handle . The D-blade  may also store to its session data , a permission flag  and lock state type  associated with a specific combination of a particular client ID  (or user ID ) and a particular file handle .","Note that the user ID  and the client ID  both uniquely identify a user\/client that is accessing the shared storage  and the file handle  uniquely identifies a file stored in the shared storage . Since each user\/client may be simultaneously storing and using multiple file handles (for accessing multiple files in the shared storage ) and each file handle may be simultaneously stored and used by multiple users\/clients (to simultaneously access the same file), the permission flag  and lock state type  is associated (in the session data ) with a specific combination of a particular client ID  (or user ID ) and a particular file handle . In some embodiments, each combination of a particular client ID  (or user ID ) and a particular file handle  and the associated permission flag  and lock state type  may comprise a single entry in the D-blade session data . In these embodiments, the entries of the D-blade session data  may be indexed by the combination of a client ID  (or user ID ) and a file handle .","As discussed above, after the initial access request for file N (which is used to produce the file handle ), the client  then includes the client ID  and the file handle  in subsequent access requests for file N. The client ID  may be used to interface with the N-blade  (e.g., by using the VIF specified by the client ID ). The N-blade  receives the access request from the client and uses the D-blade ID in the file handle to identify which D-blade  to route the access request. The N-blade  then sends the access request (having the client ID  and the file handle ) to the appropriate D-blade  to validate and perform the access request.","The D-blade  may validate the access request by locating an entry (\u201cmatching entry\u201d) in the D-blade session data  indexed by the received client ID  and file handle  combination. The D-blade  may then analyze the permission flag  and lock state type  (as found in the matching entry) that is associated with the client ID  and file handle  combination. By doing so, the D-blade  may determine whether the received access request is valid (i.e., the user\/client  has permission to perform the specific access request on the requested file N). If so, the D-blade  performs the access request.","As discussed above, the lock metadata in the file inode may specify what type of lock a user or user type can obtain when accessing the file, such as exclusive (E), shared (S), or none (N). Assuming the client  (specified by the user ID ) has permission to access the requested file N (specified by the file handle ), the D-blade  may record the lock state type  given to the client  for requested file N. Lock state data  may be stored to session data  to provide data consistency across multiple data-access sessions with multiple clients . Where two or more clients  may simultaneously attempt to write to the same file, the lock state data  may be used to determine which client (if any) is permitted to write to the file and to prevent two simultaneous write requests being performed on the same file (which would cause data inconsistency).","For example, a first client  may be given an exclusive lock state on file N, which is reflected in the lock state data  for the first client  (as identified by the user ID  or client ID ) in the session data . As such, a subsequent second client  will not be given an exclusive lock state on file N, which is reflected in the lock state data  for the second client  in the session data . If both the first and second clients attempt to perform a write request on file N, the D-blade  will check the lock state data  in the session data  to determine which client (if any) is permitted to write to file N. In this example, the D-blade  will determine that only the first client has the exclusive lock state and is permitted to write to file N, thus preventing two clients from writing to the same file at the same time.","B. Dropped Connections","During a client data-access service, however, the connection between the client  and the N-blade  may be dropped\/disconnected whether intentionally or unintentionally. For example, client connections may be intentionally dropped when performing servicing of the N-Blades or D-blade software components of the nodes of the cluster. Under previous network file protocols (such as SMB 1.0) in previous Windows\u00ae operating systems installed on clients ), upon a network disconnection, the client  was configured to immediately drop\/delete the file handle(s) obtained by the client . Also, when a connection failed, SMB 1.0 nodes were configured to close all open file handles. As such, for each disconnected client, each previous file handle must be reproduced.","In the current network file protocols (such as SMB 2.0 in current Windows\u00ae operating systems installed on clients ), upon a network disconnection, the client  is configured to drop\/delete the file handle(s) obtained by the client  only after a predetermined grace time period after the initial connection failure occurs. During this predetermined grace period, the client  may also be configured to automatically perform a predetermined number of reconnection attempts (e.g.,  attempts under SMB 2.0). If a reconnection is not successful after the predetermined number of reconnection attempts, the client  may be configured to drop\/delete the previous file handle(s) obtained by the client . If within the predetermined number of reconnection attempts, a reconnection is successful, the client  may be configured to retain the previous file handles and re-use the file handles to access the same files, without needing to re-open the files and reproduce new file handles. The file handles under of SMB 2.0 are sometimes referred to as \u201cdurable file handles.\u201d The durable file handles feature was designed to allow clients  to retain file handles for a grace period after a network disconnect to increase the reliability of the protocol when used over networks experiencing frequent network disconnects (such as wireless networks).","When the N-blade and D-blade software components of a node are being serviced (referred to herein as the \u201cserviced\u201d N-blade, D-blade, and node), the serviced node may be considered to have \u201cfailed\u201d since it may be taken offline for servicing. As such, the partner node (and thereby the partner N-blade and partner D-blade) are configured to take over the work load of the serviced node, whereby the partner N-blade begins performing the functions previously performed by the serviced N-blade and the partner D-blade begins performing the functions previously performed by the serviced D-blade. Note that the partner D-blade is allowed to access the data aggregate  of the serviced D-blade in a failover-type event (such as servicing of the D-blade).","Conventionally, the disconnected clients may attempt to reconnect with the partner node to access files serviced by the partner node that were previously requested (and which file handles were obtained). In particular, a disconnected client may attempt to reconnect with the partner N-blade using the previously obtained client ID (referred to as the \u201coriginal\u201d client ID) and attempt to re-access previously requested files using the file handles (referred to as the \u201coriginal\u201d file handles) stored to the client . The system aggregate  associated with the partner node, however, will not have the session data  that was collected and stored by the serviced node (which is stored on the system aggregate  associated with the serviced node), and thus the partner node will not have access to the session data  of the serviced node.","As such, the partner N-blade will not have access to the N-blade session data  collected by the serviced N-blade, which includes user IDs  and \u201coriginal\u201d client IDs  of clients  that were previously connected to the serviced N-blade and are now attempting reconnection to the partner N-blade. Since the partner N-blade does not have access to this N-blade session data  of the serviced N-blade, the partner N-blade will not recognize the original client IDs  sent by the clients  attempting reconnection to the partner N-blade and may refuse the reconnection of the clients . As such, re-authentication of the client connection may need to be performed and a \u201cnew\u201d client ID produced. For example, the client  may submit a user ID and password and the partner N-blade may verify that the user ID has permission to connect to the cluster  and then produce a new client ID.","Similarly, the partner D-blade will not have access to the D-blade session data  collected by the serviced D-blade, which includes original client IDs  and original file handles  obtained by clients  that were previously connected to the serviced D-blade, along with permission data  and lock state data  associated with each client ID  and file handle  combination. The clients  may then attempt to access the previously opened\/accessed files using access requests containing the original client IDs  and original file handles , the access requests being sent to the partner D-blade for processing. However, since the partner D-blade does not have access to this D-blade session data  of the serviced D-blade, the partner D-blade will not recognize the original client IDs  and original file handles  in the access requests and will not be able to validate and perform the access requests. As such, generation of new file handles for the previously opened\/accessed files may need to be performed. For example, the client  may submit a virtual address of the requested file and the N-blade  and D-blade  may translate\/map the virtual address to a file handle (physical address) of the requested file.","As described above, since the partner N-blade will not have access to the N-blade session data  of the serviced N-blade, re-authentication of connections for clients  attempting reconnection to the partner N-blade may need to be performed (whereby a new client ID produced). Also, since the partner D-blade will not have access to the D-blade session data  of the serviced D-blade, re-generation of file handles for the previously opened\/accessed files may need to be performed. So during servicing of the N-blade and D-blade software components of a node, although the original client IDs and file handles may have been retained by the clients  through the network disconnection, they may be useless since the partner N-blade and partner D-blade will not recognize the original client IDs and file handles and new client IDs and file handles may need to be produced regardless. Therefore, conventionally, servicing of the N-blade and D-blade software components of a node may be substantially disruptive to clients  using the node .","In some embodiments, the storage operating system  comprises a servicing module  for performing servicing on the N-blade  and\/or D-blade  software components of one or more nodes  of a cluster. For example, the servicing module  may perform upgrading, re-installing, maintenance, repairing, etc. of the N-blade  and\/or D-blade  software components of a node. The N-blade  and D-blade  may each comprise a servicing module . The servicing modules  in each blade may operate in conjunction to perform servicing of the N-blade  and\/or D-blade . For example, the servicing modules  may issue CF messages or other commands to the N-blade  and D-blade  to perform the methods described herein. After initiation (e.g., by an administrative command received at message interface ), the servicing modules  may do so automatically (without human intervention) for one or more nodes  of a cluster  with reduced disruption to clients  connected to the nodes .","For illustrative purposes, in the description below, the N-blade and\/or D-blade of node A are being serviced and are referred to as the \u201cserviced\u201d N-blade and D-blade. Node A is assigned\/associated with system aggregate A and data aggregate A. The serviced N-Blade stores and uses N-Blade session data A  in system aggregate A and the serviced D-Blade stores and uses D-Blade session data A  in system aggregate A. The serviced D-Blade also services data in data aggregate A in response to received access requests.","For illustrative purposes, in the description below, Node B is the predetermined failover partner of node A. As such, the N-blade and D-blade of node B are referred to as the \u201cpartner\u201d N-blade and D-blade. The partner N-blade is configured to begin performing the functions of the serviced N-blade upon the serviced N-blade being taken offline. Likewise, the partner D-blade is configured to begin performing the functions of the serviced D-blade upon the serviced D-blade being taken offline. Node B is assigned\/associated with system aggregate B and data aggregate B. The serviced N-Blade stores and uses N-Blade session data B  in system aggregate B and the partner D-Blade stores and uses D-Blade session data  in system aggregate B. The partner D-Blade services data in data aggregate B in response to received access requests. The partner D-Blade is also configured to service data in data aggregate A upon the serviced D-blade being taken offline.","For illustrative purposes, in the description below, a client  is configured under current network file protocols (such as SMB 2.0). In these embodiments, upon a connection failure with an N-blade, the client  is configured to retain a client ID and any file handles (for accessing particular files) received prior to the connection failure for a predetermined grace time period. During this predetermined grace period, the client  may also be configured to automatically perform a predetermined number of reconnection attempts (e.g., 5 attempts under SMB 2.0). If a reconnection is successful, the client  may be configured to retain the previous client ID and file handles and re-use the previous client ID and file handles to access the same files.","In some embodiments, only the N-blade  software component of one or more nodes  of a cluster is serviced. In other embodiments, only the D-blade  software component of one or more nodes  of a cluster is serviced. In further embodiments, the N-blade  and D-blade  software components of one or more nodes  of a cluster are serviced. It may be desirable to service only the N-blade  or only the D-blade  in some situations. For example, if there is only an upgrade available for the N-blade , or only the N-blade  requires repair, it is desirable to only service the N-blades  of the cluster  without also taking the D-blades  offline for servicing. This is advantageous since the N-blade  and D-blade  of a node can still operate while the other is being serviced.",{"@attributes":{"id":"p-0106","num":"0105"},"figref":"FIG. 9A-C","b":["310","200","100","310","450","310","400","310","350","200","100","100"]},"As shown in , the servicing module  may take the serviced N-blade  of node A offline (as indicated by the \u201cX\u201d mark through the connections of the serviced N-blade ) so it can no longer perform client data-access sessions (i.e., can no longer receive or send data relating to client data-access sessions). As such, the serviced N-blade  can no longer receive access requests from clients  and route the access requests to the appropriate D-blade .","Taking the serviced N-blade  offline also closes all network ports  of the serviced N-blade . Doing so prevents clients  from making new network connections to the serviced N-blade  and forces network disconnection with all clients  currently connected to the serviced N-blade  for a data-access session. As discussed above, the network ports  may be implemented as VIFs. A VIF may transparently change association from one network port  to another network port  within the same node  or across different nodes . As shown in , any VIFs of the closed network ports  of the serviced N-blade  may migrate\/change association to open network ports  of the partner N-blade . In this way, as shown in , data-access to any clients  formerly connected to the serviced N-blade  may connect to the partner N-blade  for continuing a data-access session. The serviced N-blade  software component then begins to be serviced. For example, the serviced N-blade  may be upgraded (by loading new N-blade  software on the serviced node A), re-installed, maintenanced, or repaired.","The servicing module  may then provide access to session data A to the partner N-blade . For example, as shown in , the servicing modules  may begin transferring N-Blade session data A  stored in system aggregate A to system aggregate B for storage. In some embodiments, the N-Blade session data A  is transferred through a network, such as the cluster switching fabric . For example, the servicing module  on serviced node A may send a CF message to the servicing module  on partner node B to prepare to receive session data. The servicing module  on serviced node A may then begin sending the N-Blade session data A  from system aggregate A to partner node B through the cluster switching fabric . The servicing module  on partner node B then receives and stores the N-Blade session data A  to its system aggregate B. As such, the partner N-Blade  will have access to the N-Blade session data A  stored in system aggregate B. Thus, the partner N-Blade  will have access to the user IDs , client IDs , and the file handles  obtained by clients  previously connected to the serviced N-Blade .","Each client  previously connected to the serviced N-Blade  will experience a network disconnection (upon the forced disconnection) and will begin to attempt reconnections (e.g., 5 attempts under SMB 2.0) with the partner N-blade  (while retaining any client ID  and file handles  received from the serviced N-blade  prior to the network disconnection). Each disconnected client  may attempt reconnections with the partner N-blade  using the client ID  received from the serviced N-blade  prior to the network disconnection.","If the N-Blade session data A  has not been transferred to system aggregate B yet, the partner N-blade will not have access to the N-blade session data A  yet. Thus, the partner N-blade will not recognize the client IDs  sent by the clients  attempting reconnection to the partner N-blade and may refuse the reconnection of the clients . Typically, however, the N-blade session data A  will be transferred to system aggregate B within the predetermined grace time period and the 5 attempted reconnections under SMB 2.0.","Once the N-blade session data A  is transferred to system aggregate B, the partner N-blade will have access to the N-blade session data A  and will then recognize the client IDs  sent by the clients  attempting reconnection. As such, the partner N-blade will begin accepting the connection attempts based on the received client IDs and session data A. For example, the partner N-blade may compare a received client ID with the client IDs in N-blade session data A . If a matching client ID is found in N-blade session data A , the partner N-blade may accept the connection attempt. Thus, the partner N-blade may accept the connection of a client using an original client ID without having to perform the connection authentication procedure with the client (whereby re-authentication of the client connection and generation of a new client ID would need to be performed). However, if a match is not found, the partner N-blade may refuse the connection attempt (based on the client ID) and require that the client re-perform the connection authentication procedure.","After a client  is reconnected with the partner N-blade  (as shown in ), the partner N-blade  begins receiving access requests from the client . The received access requests may contain the original client ID and original file handles (for previously opened files) received from the serviced N-blade  (which the client retained as part of the durable handles feature of SMB 2.0). The partner N-blade then processes the access requests based on the original file handles (e.g., by routing, using the original file handles, the access requests to the appropriate D-blade  for processing).","The receiving D-blade  uses the original client ID and original file handle to validate the access request to a previously opened file (e.g., by analyzing the permission and lock state data associated with the original client ID and original file handle combination). As such, the original file handles may be used to access client's previously opened files, without requiring closing of previously opened files and generation of new file handles for the previously opened files by the partner node. Also, the permission and lock type of the requested file does not need to be re-determined for the client .","While servicing the reconnected clients , the partner N-blade  may collect and store new information to the N-Blade session data A . For example, a reconnected client  may request access to a new file, whereby a new file handle  is produced and stored to the N-Blade session data A . As such, during the servicing of serviced N-blade , the partner N-blade  collects N-Blade session data A  and N-Blade session data B  which are both stored in system aggregate B. In these embodiments, N-Blade session data A  and N-Blade session data B  are kept separate (e.g., in different data structures) in system aggregate B so the data does not intermingle.","Upon the servicing of the serviced N-blade being completed, the servicing modules  then begin to transfer N-Blade session data A  (which may or may not contain new information) stored in system aggregate B to system aggregate A for storage (as shown in ). The servicing modules  may use similar data transfer methods described above. The serviced N-blade  is then brought back online so it can resume client data-access sessions by receiving access requests from clients  and routing the access requests to the appropriate D-blade  (as shown in ). The serviced N-blade  may do so using the N-Blade session data A . As shown in , for the clients  previously connected with the serviced N-blade, the VIFs that currently connect these clients  with the partner N-blade are changed back to associate to a network port  on the serviced N-blade.","The above process may be repeated for each node in the cluster, whereby each N-blade in the cluster is serviced in turn. For example, if the cluster comprises node A and node B, each being the failover partner of the other, the above process would be repeated where node A is replaced by node B and vice versa. By performing servicing on one node in the cluster at a time, the entire cluster system does not need to be taken offline, thus reducing disruption to client data-access.",{"@attributes":{"id":"p-0118","num":"0117"},"figref":"FIGS. 10A-B","b":["1000","310","200","1000","1000","400","310","350","1000"]},"The method  begins when servicing of the N-blade  is initiated (at step ). For example, servicing may be initiated by an administrative command received by a message interface module . In some embodiments, after initiation, some or all of the servicing steps may be performed automatically (without human initiation or intervention) for one or more nodes  of a cluster . Upon initiation, servicing of the N-blades of the cluster  begins with a current node (serviced node A) having a partner node (partner node B), and is repeated for each node of the cluster.","The serviced N-blade  of node A is then taken offline (at step ) so it can no longer perform client data-access sessions (i.e., can no longer receive access requests from clients  and route them to the appropriate D-blade ), whereby all network ports  of the serviced N-blade  are closed. Closing the network ports  forces a network disconnection with all clients  currently connected to the serviced N-blade  for a data-access session. The VIFs of the closed network ports  of the serviced N-blade  then migrate\/change association (at ) to open network ports  of the partner N-blade  of node B. Each disconnected client  will begin to attempt reconnections (e.g., 5 attempts under SMB 2.0) with the network ports  of the partner N-blade  (while also retaining any \u201coriginal\u201d client ID  and file handles  received from the serviced N-blade  prior to the network disconnection).","After the serviced N-blade  is taken offline, the method  begins servicing (at ) the serviced N-blade  software component (e.g., upgrading, etc.). Also, the method  begins transferring (at ) the N-Blade session data A  stored in system aggregate A to system aggregate B for storage (e.g., transferred through the cluster switching fabric ). This is to provide access to N-Blade session data A  to the partner N-blade. Note that the N-Blade session data A  will contain the original client IDs  and file handles  given to the disconnected clients  during data-access sessions with the serviced N-Blade of node A. While the N-Blade session data A  is being transferred, the partner N-blade  will receive and refuse (at step ) connection requests from the disconnected clients  using the original client IDs  and file handles . Since the partner N-blade  does not yet have access to the N-Blade session data A , the partner N-blade  will not recognize the client IDs  and may refuse the connection requests.","At step , the transfer of N-blade session data A  to system aggregate B is completed. As such, the partner N-blade of node B will now have access to the N-blade session data A  and will then recognize the client IDs  sent by the clients  attempting reconnection. As such, the partner N-blade will then begin accepting (at ) the connection attempts of the disconnected clients  based on the received original client IDs and N-Blade session data A . For example, the partner N-blade may compare received client IDs with the client IDs in N-blade session data A . If a matching client ID is found in N-blade session data A , the partner N-blade may accept the connection attempt by the client  sending the matching client ID. Thus, re-performing of the connection authentication procedure may be avoided (whereby re-authentication of the client connection and generation of a new client ID is performed). However, if a match is not found, the partner N-blade may refuse the connection attempt (based on the client ID) and require that the client re-perform the connection authentication procedure.","The partner N-blade  then begins receiving (at ) access requests (containing the original client ID and original file handles) from the reconnected clients  and routing (using the original file handles) the access requests to the appropriate D-blade . The receiving D-blade  uses the original client IDs and original file handles in the access requests to validate and perform (at ) the access requests to previously opened files stored on its data aggregate (e.g., by analyzing the permission and lock state data associated with original client ID and original file handle combinations in its session data ). While servicing the reconnected clients , the partner N-blade  may collect and store (at ) new information to the N-Blade session data A  (e.g., new file handles), whereby the N-Blade session data A  and N-Blade session data B  are kept separate in system aggregate B so the data does not intermingle.","Upon the servicing of the serviced N-blade being completed, the servicing modules  then begin to transfer (at ) N-Blade session data A  (which may or may not contain new information) stored in system aggregate B to system aggregate A for storage. The serviced N-blade  is then brought back online (at ) and, for the clients  previously connected with the serviced N-blade, the VIFs that currently connect these clients  with the partner N-blade are changed back to associate to a network port  on the serviced N-blade. The serviced N-Blade then resumes (at ) client data-access sessions by receiving access requests from clients  and routing the access requests to the appropriate D-blade  using the transferred N-Blade session data A . Then method  then ends. The method  may be repeated for each node in the cluster, whereby each N-blade in the cluster is serviced in turn. By performing servicing on one node in the cluster at a time, the entire cluster system does not need to be taken offline, thus reducing disruption to client data-access.",{"@attributes":{"id":"p-0125","num":"0124"},"figref":"FIG. 11A-C","b":["350","200","100","350","802"]},"Prior to servicing, session data A may contain one or more file handles produced by the serviced node for one or more files (stored on the shared storage) that were accessed using the serviced D-blade. Each file handle for a file may be produced by the serviced node for a client submitting an initial access request for the file. The file handle may be produced during an access request validation procedure (performed by the serviced D-blade) that validates the initial access request (e.g., by determining permission and lock type associated with the client submitting the initial access request).","Servicing of the D-blade  may be initiated, for example, by administrative command received at message interface  specifying servicing of the D-blade . The servicing modules  in the N-blade  and D-blade  may then perform (in conjunction) the servicing automatically (without human intervention) for one or more nodes  of a cluster . Upon initiation, servicing of the D-blades of the cluster  begins with a first node (serviced node A), and is repeated for each node of the cluster.","As shown in , serviced D-blade  of node A is taken offline (as indicated by the \u201cX\u201d mark through the connections of the serviced D-blade ) so it can no longer perform client data-access sessions. As such, the serviced D-blade  can no longer receive or perform access requests on the data aggregate A. Each N-blade  in the cluster is also configured to automatically begin to route their access requests to the partner D-blade  in the event the serviced D-blade  fails or is taken offline.","For every client  having an open file handle (as reflected in the D-Blade session data A ) for a file stored on data aggregate A and is accessing the file using a particular N-blade, the network connection with the client  and the particular N-blade  is forced to disconnect by closing the network port  that the client  is connected with. As such, each client  currently accessing a file using the serviced D-blade is disconnected. This may be performed by collecting all client IDs  from the D-Blade session data A  (stored in system aggregate A) and disconnecting all clients  having the collected client IDs  from all N-blades  of the cluster. This may be performed for each N-blade  by determining client IDs  in its N-Blade session data  (stored in its associated system aggregate) that match the collected client IDs , and disconnecting each of these clients . In the example of , the client  connected with the N-blade of node A has an open file handle for a file stored on data aggregate A (i.e., has a client ID  listed in D-Blade session data A . Thus the network connection between the client  and the N-blade of node A is forcibly disconnected. In some embodiments, the closed network ports  are kept closed until transfer of D-Blade session data A  to system aggregate B (discussed below) is completed.","The serviced D-blade  software component then begins to be serviced. For example, the serviced D-blade  may be upgraded (by loading new D-blade  software on the serviced node A), re-installed, maintenanced, or repaired. As shown in , the servicing modules  then begin transferring D-Blade session data A  stored in system aggregate A to system aggregate B for storage. In some embodiments, the D-Blade session data A  is transferred through a network, such as the cluster switching fabric  (as described above).","Each disconnected client  will begin to attempt reconnections (e.g., 5 attempts under SMB 2.0) with the N-blade  it was previously connected with (while retaining any client ID  and file handles  received prior to the network disconnection). Each disconnected client  may attempt reconnections using the retained client ID . In some embodiments, the closed network ports  (causing the network disconnections) of the N-blades  are kept closed until transfer of D-Blade session data A  to system aggregate B is completed. Until the network ports are re-opened, the attempted reconnections of each client  will fail. Typically, however, the D-blade session data A  will be transferred to system aggregate B within the predetermined grace time period and the 5 attempted reconnections under SMB 2.0.","Once the D-blade session data A  is completely transferred to system aggregate B, the network ports are re-opened and the disconnected clients will reconnect with their respective N-blades  (as shown in ). The partner D-Blade  will now have access to the D-Blade session data A  stored in system aggregate B. Thus, the partner D-Blade  will have access to the client IDs , file handles , permission flag  and lock state type  data collected for clients  for files stored on data aggregate A. The partner D-Blade will also be configured to have access to data aggregate A upon failure of the serviced D-blade (as well as access to data aggregate B per usual).","As shown in , after the D-blade session data A  is completely transferred to system aggregate B, the partner D-blade  may begin servicing data on data aggregate A using the D-blade session data A . In particular, the partner D-blade  may begin receiving access requests from the disconnected clients . The received access requests may contain the original client ID and original file handles for previously opened files on data aggregate A. The partner D-blade  may use the original client ID  and original file handle  to validate an access request to a previously opened file (e.g., by analyzing the permission data  and lock state data  associated with the original client ID and original file handle combination in the D-blade session data A ). Thus the partner D-blade accesses the previously opened files using the original client ID and original file handles, without having to perform an access request validation procedure for the previously opened files and without having to produce new file handles for the previously opened files. Also, the permission and lock type of the requested file does not need to be re-determined for the client .","While servicing data on data aggregate A, the partner D-blade  may collect and store new information to the D-Blade session data A . For example, a reconnected client  may request access to a new file, whereby a new file handle , new permission data , and new lock state data  is produced and stored to the D-Blade session data A . As such, during the servicing of serviced D-blade , the partner D-blade  collects D-Blade session data A  and D-Blade session data B  which are both stored in system aggregate B. In these embodiments, D-Blade session data A  and D-Blade session data B  are kept separate (e.g., in different data structures) in system aggregate B so the data does not intermingle.","Upon the servicing of the serviced D-blade being completed, the servicing modules  then begin to transfer D-Blade session data A  (which may or may not contain new information) stored in system aggregate B to system aggregate A for storage (as shown in ). The servicing modules  may use similar data transfer methods described above. The serviced D-blade  is then brought back online so it can resume servicing data on data aggregate A by receiving access requests (from N-blades ) and performing access requests on files stored in data aggregate A (as shown in ). The serviced D-blade  may do so using the D-Blade session data A . Each N-blade  in the cluster is also configured to automatically begin to route their access requests to the serviced D-blade  once the serviced D-blade  is brought back online.","The above process may be repeated for each node in the cluster, whereby each D-blade in the cluster is serviced in turn. For example, if the cluster comprises node A and node B, each being the failover partner of the other, the above process would be repeated where node A is replaced by node B and vice versa. If both the N-blade  and D-blade  software components of the nodes  of a cluster are to be serviced, the above described method for servicing the N-blade and the method for servicing the D-blade can be performed on each node. This process may be repeated for each node in the cluster, whereby each node in the cluster is serviced one after the other. By performing servicing on one node in the cluster at a time, the entire cluster system does not need to be taken offline, thus reducing disruption to client data-access.",{"@attributes":{"id":"p-0137","num":"0136"},"figref":"FIGS. 12A-B","b":["1200","350","200","1200","1200","400","310","350","1200"]},"The method  begins when servicing of the D-blade  is initiated (at step ). For example, servicing may be initiated by an administrative command received by a message interface module . In some embodiments, after initiation, some or all of the servicing steps may be performed automatically (without human initiation or intervention) for one or more nodes  of a cluster . Upon initiation, servicing of the D-blades of the cluster  begins with a current node (serviced node A) having a partner node (partner node B), and is repeated for each node of the cluster.","The serviced D-blade  of node A is then taken offline (at step ) so it can no longer perform client data-access sessions and can no longer receive or perform access requests on the data aggregate A. Each N-blade  in the cluster is also configured to automatically begin to route their access requests to the partner D-blade  in the event the serviced D-blade  fails or is taken offline. For every client  having an open file handle (as reflected in the D-Blade session data A ) for a file stored on data aggregate A and is accessing the file using a particular N-blade, the network connection with the client  and the particular N-blade  is disconnected (at step ) by closing the respective network ports , whereby the closed network ports  are kept closed until transfer of D-Blade session data A  to system aggregate B is completed. As such, each client currently accessing a file using the serviced D-blade is disconnected (at step ). Each disconnected client  is configured to begin to attempt reconnections with the respective N-blade  (while also retaining any original client ID  and file handles ).","The method  begins servicing (at ) the serviced D-blade  software component (e.g., upgrading, etc.). Also, the method  begins transferring (at ) the D-blade session data A  stored in system aggregate A to system aggregate B for storage. This is to provide access to D-Blade session data A  to the partner D-blade. Upon the D-blade session data A  being completely transferred to system aggregate B, the network ports are re-opened (at step ). The partner D-Blade  will now have access to the D-Blade session data A  (stored in system aggregate B) for files stored on data aggregate A. The partner D-Blade will also be configured to have access to data aggregate A upon failure of the serviced D-blade.","The disconnected clients will reconnect (at step ) with their respective N-blades . The partner D-blade  may begin servicing data (at step ) on data aggregate A using the D-blade session data A  by receiving access requests (containing original client ID and original file handles for previously opened files on data aggregate A) from the reconnected clients . The partner D-blade  may use the original client ID  and original file handle  to validate an access request to a previously opened file (e.g., by analyzing the permission data  and lock state data  associated with the original client ID and original file handle combination in the D-blade session data A ).","While servicing the reconnected clients , the partner D-blade  may collect and store (at ) new information to the D-blade session data A , whereby the D-blade session data A  and D-blade session data B  are kept separate in system aggregate B so the data does not intermingle. Upon the servicing of the serviced D-blade being completed, the servicing modules  then begin to transfer (at ) D-blade session data A  stored in system aggregate B to system aggregate A for storage. The serviced D-blade  is then brought back online (at ) and resumes servicing data on data aggregate A using the D-Blade session data A .","Then method  then ends. The method  may be repeated for each node in the cluster, whereby each D-blade in the cluster is serviced in turn. If both the N-blade  and D-blade  software components of the nodes  of a cluster are to be serviced, the above described method  for servicing the N-blade and the method  for servicing the D-blade can be performed on each node. This process may be repeated for each node in the cluster, whereby each node in the cluster is serviced one after the other. By performing servicing on one node in the cluster at a time, the entire cluster system does not need to be taken offline, thus reducing disruption to client data-access.","Some embodiments may be conveniently implemented using a conventional general purpose or a specialized digital computer or microprocessor programmed according to the teachings of the present disclosure, as will be apparent to those skilled in the computer art. Appropriate software coding may readily be prepared by skilled programmers based on the teachings of the present disclosure, as will be apparent to those skilled in the software art. Some embodiments may also be implemented by the preparation of application-specific integrated circuits or by interconnecting an appropriate network of conventional component circuits, as will be readily apparent to those skilled in the art.","Some embodiments include a computer program product which is a storage medium (media) having instructions stored thereon\/in which may be used to control, or cause, a computer to perform any of the processes of an embodiment. The storage medium may include without limitation any type of disk including floppy disks, mini disks (MD's), optical disks, DVDs, CD-ROMs, micro-drives, and magneto-optical disks, ROMs, RAMs, EPROMs, EEPROMs, DRAMs, VRAMs, flash memory devices (including flash cards), magnetic or optical cards, nanosystems (including molecular memory ICs), RAID devices, remote data storage\/archive\/warehousing, or any type of media or device suitable for storing instructions and\/or data.","Stored on any one of the computer readable medium (media), some embodiments include software for controlling both the hardware of the general purpose\/specialized computer or microprocessor, and for enabling the computer or microprocessor to interact with a human user or other mechanism utilizing the results of an embodiment. Such software may include without limitation device drivers, operating systems, and user applications. Ultimately, such computer readable media further includes software for performing some embodiments, as described above. Included in the programming (software) of the general\/specialized computer or microprocessor are software modules for implementing the teachings of some embodiments.","While the embodiments described herein have been described with reference to numerous specific details, one of ordinary skill in the art will recognize that the embodiments can be embodied in other specific forms without departing from the spirit of the embodiments. Thus, one of ordinary skill in the art would understand that the embodiments described herein are not to be limited by the foregoing illustrative details, but rather are to be defined by the appended claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0023","num":"0022"},"figref":["FIG. 3","FIG. 2"],"b":"300"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 9A-C"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIGS. 10A-B"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 11A-C"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIGS. 12A-B"}]},"DETDESC":[{},{}]}
