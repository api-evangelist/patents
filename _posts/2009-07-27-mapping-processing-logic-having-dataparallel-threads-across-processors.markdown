---
title: Mapping processing logic having data-parallel threads across processors
abstract: A method for executing a plurality of data-parallel threads of a processing logic on a processor core includes grouping the plurality of data-parallel threads into one or more workgroups, associating a first workgroup from the one or more workgroups with an operating system thread on the processor core, and configuring threads from the first workgroup as user-level threads within the operating system thread. In an example, a method enables the execution of GPU-kernels that has been previously configured for a GPU, to execute on a CPU such as a multi-core CPU. The mapping of the numerous data-parallel threads to the CPU is done in such a manner as to reduce the number of costly operating system threads instantiated on the CPU, and to enable efficient debugging.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09354944&OS=09354944&RS=09354944
owner: Advanced Micro Devices, Inc.
number: 09354944
owner_city: Sunnyvale
owner_country: US
publication_date: 20090727
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND OF THE INVENTION","BRIEF SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["1. Field of the Invention","The present invention relates generally to computer processors and data-parallel threads.","2. Background Art","Computers and other such data processing devices have at least one control processor that is generally known as a control processing unit (CPU). Such computers and processing devices can also have other processors such as graphics processing units (GPU) that are used for specialized processing of various types. For example, GPUs are designed to be particularly suited for graphics processing operations. GPUs generally comprise multiple processing elements that are ideally suited for executing the same instruction on parallel data streams, such as in data-parallel processing. In general, a CPU functions as the host or controlling processor and hands-off specialized functions such as graphics processing to other processors such as GPUs.","With the availability of multi-core CPUs where each CPU has multiple processing cores, substantial processing capabilities that can also be used for specialized functions are available in CPUs. One or more of the computation cores of multi-core CPUs or GPUs can be part of the same die (e.g., AMD Fusion\u2122) or in different dies (e.g., Intel Xeon\u2122 with NVIDIA GPU). Recently, hybrid cores having characteristics of both CPU and GPU (e.g., CellSPE\u2122, Intel Larrabee\u2122) have been generally proposed for General Purpose GPU (GPGPU) style computing. The GPGPU style of computing advocates using the CPU to primarily execute control code and to offload performance critical data-parallel code to the GPU. The GPU is primarily used as an accelerator. The combination of multi-core CPUs and GPGPU computing model encompasses both CPU cores and GPU cores as accelerator targets. Many of the multi-core CPU cores have performance that is comparable to GPUs in many areas. For example, the floating point operations per second (FLOPS) of many CPU cores are now comparable to that of some GPU cores.","Several frameworks have been developed for heterogeneous computing platforms that have CPUs and GPUs. These frameworks include BrookGPU by Stanford University, CUDA by NVIDIA, and OpenCL by an industry consortium named Khronos Group. The OpenCL framework offers a C-like development environment using which users can create applications for GPU. OpenCL enables the user, for example, to specify instructions for offloading some computations, such as data-parallel computations, to a GPU. OpenCL also provides a compiler and a runtime environment in which code can be compiled and executed within a heterogeneous computing system.","What are needed, therefore, are methods and systems that enable the efficient use of CPU capabilities for the execution of functions that are generally processed on a GPU.","Systems and methods for executing a plurality of data-parallel threads of a processing logic on a processor core are disclosed. The method for executing a plurality of data-parallel threads of a processing logic on a processor core includes grouping the plurality of data-parallel threads into one or more workgroups, associating a first workgroup from the one or more workgroups with an operating system thread on the processor core, and configuring threads from the first workgroup as user-level threads within the operating system thread. The processing logic can include GPU-kernels that are previously configured for execution on a GPU. The processor cores can include CPU cores, such as, the processing cores of a multi-core CPU.","Another embodiment is a method for executing a GPU-kernel that includes loading the GPU-kernel to a host processor, forming one or more workgroups where each workgroup includes data-parallel threads, and associating the workgroups with operating system threads of one or more processor cores coupled to the host processor. Each workgroup can be associated with one or more operating system threads, and each operating system thread is configured to execute only on one processor core. The method can also include the step of configuring data-parallel threads of each workgroup as user-level threads within each respective associated operating system thread.","Another embodiment is a system for executing a plurality of data-parallel threads of a processing logic on a processor core. The system includes at least one processor including the processor core, at least one memory coupled to the processor core, and a GPU-kernel scheduler. The GPU-kernel scheduler is configured for grouping the plurality of data-parallel threads into one or more workgroups, associating a first workgroup from the one or more workgroups with an operating system thread on the processor core, and executing threads from the first workgroup as user-level threads within the operating system thread.","Yet another embodiment is a computer program product comprising a computer readable medium having computer program logic recorded thereon for causing at least one processor to group a plurality of data-parallel threads into one or more workgroups, associating a first workgroup from the one or more workgroups with an operating system thread on the processor core, and executing threads from the first workgroup as user-level threads within the operating system thread.","Further embodiments, features, and advantages of the present invention, as well as the structure and operation of the various embodiments of the present invention, are described in detail below with reference to the accompanying drawings.","Embodiment of the present invention may yield substantial advantages by enabling the use of the same or similar code base on CPU and GPU processors and also by facilitating the debugging of such code bases. While the present invention is described herein with illustrative embodiments for particular applications, it should be understood that the invention is not limited thereto. Those skilled in the art with access to the teachings provided herein will recognize additional modifications, applications, and embodiments within the scope thereof and additional fields in which the invention would be of significant utility.","Embodiments of the present invention may be used in any computer system, computing device, entertainment system, media system, game systems, communication device, personal digital assistant, or any system using one or more processors. The present invention is particularly useful where the system comprises a heterogeneous computing system. A \u201cheterogeneous computing system,\u201d as the term is used herein, is a computing system in which multiple kinds of processors are available.","Embodiments of the present invention enable the same code base to be executed on different processors, such as GPUs and CPUs. Embodiments of the present invention, for example, can be particularly advantageous in processing systems having multi-core CPUs, and\/or GPUs, because code developed for one type of processor can be deployed on another type of processor with little or no additional effort. For example, code developed for execution on a GPU, also known as GPU-kernels, can be deployed to be executed on a CPU, using embodiments of the present invention.","Also, the present invention enables the deployment of GPU-kernels that typically involve numerous data-parallel threads, on CPUs and other processors in a manner that helps in debugging the threads.","A Heterogeneous Computing System","An example heterogeneous computing system , according to an embodiment of the present invention, is shown in . Heterogeneous computing system  can include one or more CPUs, such as CPU , and one or more GPUs, such as GPU . Heterogeneous computing system  can also include at least one system memory , at least one persistent storage device , at least one system bus , at least one input\/output device , a GPU-kernel scheduler , and debugger .","CPU  can include any commercially available control processor or a custom control processor. CPU , for example, executes the control logic that controls the operation of heterogeneous computing system . CPU  can be a multi-core CPU, such as a multi-core CPU with two CPU cores  and . CPU , in addition to any control circuitry, can include CPU cache memory such as the cache memories  and  of CPU cores  and , respectively. CPU cache memories  and  can be used to temporarily hold instructions and\/or parameter values during the execution of an application on CPU cores  and , respectively. For example, CPU cache memory  can be used to temporarily hold one or more control logic instructions, values of variables, or values of constant parameters, from the system memory  during the execution of control logic instructions on CPU core . In some embodiments, CPU  can also include specialized vector instruction processing units. For example, CPU core  can include a Streaming SIMD Extensions (SSE) unit that can efficiently process vectored instructions. A person skilled in the art will understand that CPU  can include more or less than the CPU cores in the example chosen, and can also have either no cache memories, or more complex cache memory hierarchies.","GPU  can include any commercially available graphics processor or custom designed graphics processor. GPU , for example, can execute specialized code for selected functions. In general, GPU  can be used to execute graphics functions such as graphics pipeline computations such as geometric computations and rendering of image on a display.","GPU  can include a GPU global cache memory  and one or more compute units  and . A graphics memory  can be included in or coupled to GPU . Each compute unit  and  are associated with a GPU local memory  and , respectively. Each compute unit includes one or more GPU processing elements (PE). For example, compute unit  includes GPU processing elements  and , and compute unit  includes GPU processing elements  and . Each GPU processing element , , , and , is associated with at least one private memory (PM) , , , and , respectively. Each GPU processing element can include one or more of a scalar and vector floating-point units. The GPU processing elements can also include special purpose units such as inverse-square root units and sine\/cosine units. GPU global cache memory  can be coupled to a system memory such as system memory , and\/or graphics memory such as graphics memory .","System memory  can include at least one non-persistent memory such as dynamic random access memory (DRAM). System memory  can hold processing logic instructions, constant values and variable values during execution of portions of applications or other processing logic. For example, in one embodiment, the control logic and\/or other processing logic of GPU-kernel scheduler  can reside within system memory  during execution of GPU-kernel scheduler  by CPU . The term \u201cprocessing logic,\u201d as used herein, refers to control flow instructions, instructions for performing computations, and instructions for associated access to resources.","Persistent memory  includes one or more storage devices capable of storing digital data such as magnetic disk, optical disk, or flash memory. Persistent memory  can, for example, store at least parts of instruction logic of GPU-kernel scheduler  and debugger . For example, at the startup of heterogeneous computing system , the operating system and other application software can be loaded in to system memory  from persistent storage .","System bus  can include a Peripheral Component Interconnect (PCI) bus, Advanced Microcontroller Bus Architecture (AMBA) bus, Industry Standard Architecture (ISA) bus, or such a device. System bus  can also include a network such as a local area network (LAN). System bus  includes the functionality to couple components including components of heterogeneous computing system .","Input\/output interface  includes one or more interfaces connecting user input\/output devices such as keyboard, mouse, display and\/or touch screen. For example, user input can be provided through a keyboard and mouse connected user interface  to heterogeneous computing system . The output of heterogeneous computing system , such as the output GPU-kernel scheduler  and debugger , can be output to a display through user interface .","Graphics memory  is coupled to system bus  and to GPU . Graphics memory  is, in general, used to hold data transferred from system memory  for fast access by the GPU. For example, the interface between GPU  and graphics memory  can be several times faster than the system bus interface .","Debugger  includes functionality to debug application code and other processing logic executing on heterogeneous computing system . For example, debugger  can include functionality to monitor one or more variables during the execution of application code and\/or other processing logic. In heterogeneous computing environments, debugger  may require the ability to monitor the execution of application code and other processing logic on the CPUs as well as GPUs.","GPU-kernel scheduler  includes functionality to schedule functions and processing logic written specifically for execution on a GPU, on either a GPU or on a CPU. GPU-kernel scheduler is further described in relation to  below. A person of skill in the art will understand that debugger  and GPU-kernel scheduler  can be implemented using software, firmware, hardware, or any combination thereof. When implemented in software, for example, GPU-kernel scheduler  can be a computer program written in C or OpenCL, that when compiled and executing resides in system memory . In source code form and\/or compiled executable form, GPU-kernel scheduler  can be stored in persistent memory . In one embodiment, some or all of the functionality of GPU-kernel scheduler  and\/or debugger  is specified in a hardware description language such as Verilog, RTL, netlists, to enable ultimately configuring a manufacturing process through the generation of maskworks\/photomasks to generate a hardware device embodying aspects of the invention described herein.","A person of skill in the art will understand that heterogeneous computing system  can include more or less components that shown in . For example, heterogeneous computing system  can include one or more network interfaces, and or software applications such as the OpenCL framework.","GPU-kernels","A \u201cGPU-kernel\u201d, as the term is used herein, comprises code and\/or processing logic that executes one or more functions or operations on a GPU. For example, GPU-kernels are developed for matrix multiplication, matrix transpose, subdivision, equation solving, and numerous other operations. In the GPGPU model, for example, GPU-kernels are code fragments of functions and\/or other operations that can be offloaded by a CPU to be executed on a GPU. In a typical GPGPU application, the same GPU-kernel code will execute simultaneously on multiple parallel data streams. This is known as \u201cdata-parallel\u201d computing. For example, in multiplying two matrices, multiple pairs of corresponding elements from the two source matrices can be simultaneously multiplied using different processing cores. Although a simple example, matrix multiplication can illustrate the data-parallel operations on a GPU.","Following the above matrix multiplication example, the computation for each element of the resulting matrix can be considered as a separate thread of execution (thread). Threads corresponding to each element of the resulting matrix can execute in parallel. In GPU , for example, threads may simultaneously execute on each processing element , , , and .","If multiple pairs of matrixes are to be multiplied, for example, as is common in graphics applications, then each pair of matrices can be assigned to a compute element, such as compute elements  and .","The execution of GPU-kernels can be described with respect to GPU . A GPU-kernel generally executes as multiple threads in parallel. Each of these parallel threads is a \u201cworkitem.\u201d Each workitem can execute on a processing element, such as, processing elements ,,, or . The workitems assigned to a compute unit are known as a \u201cworkgroup.\u201d","In general, synchronization among workitems is limited to those within a single workgroup. This restriction is generally enforced in environments with large numbers of threads, so that the number of threads can be efficiently managed. However, in some embodiments synchronization may not be limited to workitems in a single workgroup.","GPU-kernel Scheduler",{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 2","b":["109","109","201","203","205","207","109","201","207"]},"Code analyzer  includes functionality to analyze code, such as OpenCL code. Code analyzer  can identify code fragments and\/or GPU-kernels in the code being analyzed that can be advantageously scheduled a CPU or alternatively on a GPU. Code analyzer  can also include the functionality to know the particulars of the target platform. For example, code analyzer  can identify the number of CPU cores, and GPU cores available on the target platform. It may also identify the amount of system memory, cache memory, global GPU cache memory, and the like.","CPU memory mapper  includes functionality to determine the memory availability of the CPUs to which code can be dispatched for execution, and to prepare the CPU memory for executing mapped GPU-kernels.","Workgroup scheduler  includes functionality to schedule a workgroup. For example, workgroup scheduler  can determine the appropriate set of compute units of a GPU that can be used for executing each workgroup. In another embodiment, workgroup scheduler can also determine how the workitem threads of a GPU-kernel should be grouped to workgroups so that the workgroups can be assigned to threads on one or more CPU cores. Workgroup scheduler can also include functionality to map a workgroup of GPU-kernel threads to a CPU or CPU core.","Thread scheduler  includes functionality to schedule threads of a GPU-kernel on a GPU or CPU. For example, thread scheduler  can determine the assignment of threads to processing elements , , , or  in a manner that processing requirements of the threads can be accommodated. In one embodiment, thread scheduler  determines at which processing element in GPU  each workitem can be scheduled. In another embodiment, thread scheduler  schedules workitem threads on CPU . For example, thread scheduler  can schedule workitem threads of one workgroup on a single CPU core, such as CPU core . The functionality of modules - are further described in relation to  below, according to an embodiment of the present invention.","Mapping and Executing GPU-kernels on CPU Cores",{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 3","b":["300","300","100","300","109"]},"Without loss of generality, the following description is primarily based on the OpenCL framework. Persons skilled in the art will recognize that embodiments of the present invention can also be implemented using other development and execution frameworks such as, but not limited to, the CUDA framework, BrookGPU framework, and the like. As described above, OpenCL provides a development environment, compiler and a runtime environment in which code can be developed, compiled and executed within a heterogeneous computing system.","OpenCL also provides built-in library functions and application programming interfaces (API) that can be called by the user applications to execute various functions, such as, deploying a GPU-kernel to be executed on a selected processor. In general, OpenCL framework primarily executes on a host system and controls the operation of coupled processors by deploying commands (e.g., executable code including GPU-kernels) to each processor. For example, OpenCL may primarily execute on CPU core , and control the operation of GPU , and CPU cores  and  by deploying commands to be executed on the respective devices.","In step , the code for one or more applications is loaded. The loaded code may be source code or other form of pre-compilation code such as byte-code. In some embodiments, the code can also include embedded compiled functions. For example, some functions can be dynamically linked from a library based on embedded function calls in the application. For purposes of description, it will be assumed that the loaded code is in source code form, in a programming language such as OpenCL. The source code can initially reside, for example, on persistent storage device . In loading the code in step , the source code can be loaded into memory, for example, system memory . It should be noted, that the loaded code can include a combination of source code, pre-compilation code, and\/or compiled binaries. For example, the OpenCL development, compilation, and execution framework, within which process  may be implemented, allows the loading of source code and\/or compiled binaries of GPU-kernels. An OpenCL program, for example, can include GPU-kernels, control flow code, and other functions called by GPU-kernels.","In step , the loaded code is analyzed. Analysis of the code can include parsing the code. In some embodiments, the code analyzed in step  can include valid OpenCL code or code in another programming language or framework such as C. In the analysis step , the required CPU resources can be determined. For example, the analysis and\/or parsing can determine the total number of threads (workitems) that are required to be scheduled on the one or more CPU cores. Also, the analysis step  can be used to determine memory requirements, such as the actual amounts of private memory and local memory used by the GPU-kernel. In one embodiment, steps  and  can be implemented using the processing logic of code analyzer .","In step , the heterogeneous computing system is configured to run the code loaded in step . For example, based on the code analysis performed in step , a number of the CPUs and GPUs can be allocated to run the loaded code. Step  can also include configuring memory resources, such as, memory , to prepare for the execution of the loaded code.","Steps - are repeated for each function and\/or GPU-kernel in the loaded code. For example, the loaded source code can be scanned in a top-to-bottom manner identifying each function and\/or GPU-kernel. In step , a function and\/or GPU-kernel is identified. In one embodiment, for example, GPU-kernels may be identified by a predetermined identifier such as the _kernel identifier in OpenCL.","In step , it is determined whether to execute the code segment identified in step , on a CPU or a GPU. For example, the default behavior may be to execute all functions other than GPU-kernels on a CPU and to execute all GPU-kernels on a GPU. Another consideration can be to determine if a command or function is data-parallel, for example, based on identifiers embedded in the code. Data-parallel commands or functions can be preferably scheduled to a GPU. Other criteria may also be used, such as load balancing requirements and application characteristics.","If it is determined (in step ) that the processing logic and\/or code segment is to be executed on a GPU, it is scheduled to be implemented on a GPU in step . In some embodiments, the processing logic and\/or code segments are scheduled to be implemented on a specific GPU.","In step , the processing logic and\/or code segment is scheduled on a selected GPU. Step  may include determining the structure of the GPU, e.g., the number of compute devices in the GPU, and the number of processing elements within each compute device. Step  can also include creating a context and a command queue for that GPU, creating memory objects associated with the context, compiling and creating GPU-kernel objects, issuing corresponding commands to the command queue to execute the GPU-kernel objects, and performing any synchronization of the issued commands.","Determining the structure of the GPU can involve, for example, using an OpenCL API call to query for compute devices (e.g., CPUs, GPUs) available in the system, and then to query one or more of the identified compute devices for its local information. From a list of compute devices, a GPU device can be queried to determine the number of GPU processing elements, range limits, workgroup size, sizes of different memory spaces, maximum memory object sizes, and the like.","In scheduling a processing logic and\/or code segment to be executed on a selected GPU, a command including an executable code including the code segment (e.g., executable kernel object) can be scheduled to a command queue created in the context of the selected GPU. A \u201ccontext,\u201d as used herein, defines the execution environment for the GPU-kernel. The context can include handles and other information for devices, memory, and other program objects associated with the kernel. For example, the context can provide the executing kernel with the GPU device and memory locations it is permitted to use.","Memory objects created can correspond to source and destination objects. For example, if the processing logic and\/or code segment performs an iterative array addition, then the source objects can be the two source arrays. The destination object can be the array in which the output result is written. Memory objects can be created in the system memory  or in the graphics memory . Memory objects may subsequently be cached in various caches such as caches , , and . Each memory object created is associated with a corresponding context.","Creation of the executable kernel objects is done by compiling the processing logic and\/or code segment, and linking any required device specific modules. For example, the compiled kernel object can include the kernel function code or instructions, any arguments provided to the kernel, the associated context, and library code related to the GPU on which it is to be executed.","After the kernel objects are created, the command to execute the kernel is queued to a command queue. The manner in which the command is enqueued may be specific to whether the command is a data-parallel or task-parallel command. For example, in the iterative array addition example, a data-parallel command can enqueue a single command with instructions to parallelly execute in a specified number of compute devices, whereas a task-parallel model can result in a number of separate commands being enqueued, one command for each array element.","In step , if the determination is made that the processing logic and\/or code segment should be executed on a CPU, process  proceeds to step . In step , one or more CPUs can be selected to execute the code segment. In some embodiments, for example, in multi-core CPU systems, the selection of the CPU can be based on the load or processing capability of the CPU. Other consideration, such as the availability and magnitude of available cache memory, and the like, can also be considered.","For ease of description the description of steps - assumes that the processing logic and\/or code segment to be executed on the CPU is a GPU-kernel. Mapping the GPU-kernel to the selected CPU can include mapping of the fine-grained GPU threads (i.e., workitems) to the CPU core and also mapping of the GPU memory hierarchy to the CPU memory system. In step , the GPU-kernel threads are mapped to the selected CPU. If the GPU-kernel is naively mapped to the CPU, each GPU workitem would map to an operating system (OS) thread. The processing of step  can be part of the functionality provided by workgroup scheduler .","CPUs have a much more restricted number of hardware contexts, and often have only one hardware context. The OS is generally responsible for scheduling and executing OS threads. The naive mapping of workitems to OS threads can result in hundreds if not thousands of threads. Having numerous threads execute in a single context can result in substantial inefficiencies for reasons including numerous context-switches required when each process executes. Also, having the OS manage a large number of threads can be very inefficient. Embodiments of the present invention, accomplish the mapping from GPU to CPU in such a way as to achieve high performance and also to facilitate debugging of applications having numerous threads.","Mapping the workitems of a GPU-kernel to the CPU core, in embodiments of the present invention, is done in such a manner as to take advantage of multi-core CPUs where available and to have a manageable number of OS threads to avoid inefficiencies related to context switching and the like. Therefore, in embodiments of the present invention, instead of workitems, a workgroup is the unit that is mapped and\/or associated with an OS thread. One or more workitems are grouped to a workgroup. The number of workitems in a workgroup can be predetermined or can be determined based on criteria such as the number and capacity of available processing cores. Mapping and\/or associating a workgroup with an OS thread can include configuration for either spawning a new OS thread for the workgroup, or attaching the workgroup to an already existing OS thread. API functions for spawning new OS threads and for attaching to an existing thread are provided by one or more of the OpenCL framework (including the OpenCL runtime system) and the underlying operating system.","Each workgroup of the GPU-kernel can be mapped to a CPU core, for example, by setting a processor affinity for the OS thread associated with the workgroup. Each CPU core can be assigned one or more workgroups. Multiple workgroups per CPU core can be executed in parallel. In an embodiment, each CPU core is configured to execute only a single workgroup at a time. For example, only a single OS thread is created in each CPU core. Each CPU core can execute multiple workgroups by executing one workgroup to a finish, and then executing another.","In step , threads within a workgroup are mapped and\/or configured. The processing of step  can be part of the functionality provided by thread scheduler . Within each OS thread that is mapped to a workgroup, the workitem threads are configured to execute in sequence, one after another. The workitem threads are implemented as user-level threads. Implementing the workitem threads as user-level threads avoids the overhead of having to switch between user space and kernel space, and the associated cost of numerous context switches. Note that in most cases, GPU-kernels apply to user space applications and do not require direct access to system resources requiring kernel space access. As is known by persons skilled in the art, multiple user-level threads can be implemented within a single operating system level thread.","In some embodiments of the present invention, workitem threads can be executed in groups within a workgroup. For example, in environments where a vector unit such as a SSE unit (e.g., SSE unit  in system ) that can simultaneously accommodate more than one floating-point or double computation is available, the compiler can combine a number of workitems to execute in parallel such that the full processing capacity of the SSE unit is utilized.","In step , the memory accessible to the CPU cores is mapped to accommodate the memory hierarchy available to a GPU. The processing of step  can be part of the functionality provided by CPU memory mapper . Mapping of the GPU memory hierarchy to the CPU cores so that the GPU-kernels can execute on the CPU cores can include several aspects. The mapping from the GPU memory hierarchy to the CPU memory can be required because the GPU-kernel code can include references to locations in the GPU memory hierarchy. The CPU cores typically have only a single address space for its memory (e.g., system memory ), whereas GPU cores can have several separately addressable memories. The GPU global memory, compute device local memory, and processing element private memory, all can be mapped to CPU memory.","The GPU global memory, such as, GPU memory , can be mapped directly to system memory, such as, memory . Mapping of the GPU local memory and private memory require a more nuanced approach. Mapping, as used in relation to memory, can involve setting up an automatic address translation mechanism by which a memory address included in a GPU-kernel executing on a CPU core is redirected to system memory  instead of a location in a memory hierarchy associated with a GPU.","GPU local memory, such as, GPU local memory  and , is shared between the processing elements of a compute unit. Therefore, one GPU local memory is shared between all workitems of a workgroup. When the workgroups are mapped to a CPU, because a workgroup is mapped to a single CPU core, the corresponding GPU local memory can be brought into this CPU core. In many cases, the actual usage of local memory in a GPU can be such that it may fit entirely in the cache memory of a CPU core.","GPU local memory, such as GPU local memories  and , are shared by workitems in a workgroup. Embodiments of the present invention may restrict each CPU core to execute only a single workgroup at a time. When a CPU executes only a single workgroup at a time, at any instance, only a single allocation that corresponds to GPU local memory is utilized by a CPU core. Accordingly, a single local memory per CPU core can be allocated, and the allocated local memory can be reused for successive workgroups that execute on that CPU core. A local memory can be allocated for each compute device in a contiguous memory area of a system memory. Preferably, a cache memory in a CPU core can be of sufficient magnitude to accommodate an instance of the allocation of local memory in system memory, corresponding to that CPU core. A person of skill in the art will understand that the local memory area allocated in the memory and\/or cache may require being flushed before being reused for each successive workgroup. In some embodiments, the local memory area allocated in the memory and\/or cache may be reused by overwriting the previous contents.","GPU private memory, such as GPU private memories , , , and , are private to each processing element. Each private memory in the GPU environment is used by a single workitem at a time. Therefore, similarly to local memories, private memories can be allocated in system memory and reused for successive workitems. For example, a single allocation for a private memory in the system memory  can be reused by successively executing workitems of a single workgroup, and within successive workgroups executed by the same CPU core. GPU private memory, and therefore the allocation in system memory  corresponding to the GPU private memory, is typically used for holding stack variables that are internal to the kernel and are local to each workitem.","Constant memory contains data that have values which are constant throughout the execution of the code. For GPU-kernels executing on a GPU, constant memory can be in graphics memory  and\/or GPU global cache memory . Constant memory, in general, can be read only and is not dependent on the workgroup. In embodiments of the present invention, the constant memory is mapped to an area in system memory that can be accessible to all CPU cores, because any workgroup or workitem executing on any CPU core may require access to constant memory. Also, it is preferable that the constant memory allocation from the main memory is replicated in the cache memory of each CPU core. An example layout of mapping a GPU memory hierarchy to CPU is shown in .","In step , the GPU-kernel can be scheduled to execute on one or more selected CPU cores, such as, cores  and\/or . Scheduling the GPU-kernel for execution can include configuring the execution environment, such as the OpenCL runtime system, to implement the mapping of workgroups and workitems on selected CPU cores.","In one embodiment of the present invention, where the OpenCL framework is being used to compile and execute code on heterogeneous computing system , step  can involve the queuing of one or more commands to a command queue. For example, using the OpenCL framework in heterogeneous computing system , a GPU-kernel can be scheduled to be executed on one or more cores  and multi-core CPU . Typically, in running an application on heterogeneous computing system  using the OpenCL framework the control portion runs on the host that initiates the execution of selected code on devices of system  such as GPU  or cores  or . A CPU core, such as one of the cores  or , can function as the host and distribute the GPU-kernels to devices of system  to be executed. For example, commands to map CPU memory, and commands to execute the GPU-kernel can be scheduled in a command queue created in the context created for the multi-core CPU . The OpenCL runtime environment executes each of the commands written to the respective command queue in the specified device in system .","In scheduling the GPU-kernel for execution on multi-core CPU , instructions to enforce the mappings and configurations determined in steps - can be inserted in the code to be executed on the multi-core CPU . For example, the workitems within a workgroup may require a synchronization point to be inserted either automatically or manually by a programmer.","In implementing workitem synchronization within a workgroup on multi-core CPU , embodiments of the present invention can enable workitems to execute up to a synchronization point. For example, one by one each workitem executes up to a synchronization point (e.g., a barrier( ) function) in a serial manner and halts at the synchronization point before the next workitem is executed. When all workitems in the workgroup having the synchronization point have reached there, each workitem serially executes from the synchronization point to the end of the workitem. Functions such as setjmp( ) to save the workitem state, and longjmp( ) to restore the state of that workitem, can be inserted into the code. In embodiments of the present invention, such additional code is not inserted in the source code of the GPU-kernels. Rather, such additional code is inserted in the entry, exit, and\/or barrier routines of the GPU-kernels (and not within the GPU-kernel code). By not altering the source code of the GPU-kernels themselves, embodiments of the present invention facilitates debugging efforts.","In step , it is determined whether there is additional code to be executed, and if so, process  is returned to step . If no more code is to be executed, process  terminates in step .","An Example Memory Mapping to CPU",{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIG. 4","b":["401","103","102","402","109","101"]},"For example, local memory  and  can correspond to local memory  and , respectively, of GPU . Private memories , , , and , can correspond to private memories , , , and , respectively, of GPU . Constant memory can correspond to constant memory  (i.e., memory area where constant parameters are stored). As shown in , the cache of a CPU core can have one local memory block, a series of private memory blocks corresponding to one block for each workitem in the workgroup, and a constant memory area. In some embodiments, it can be particularly advantageous to have the local memory, private memory, and constant memory required for processing by a CPU core to all is available in the cache of the CPU core, such that delays associated with access to system memory  can be reduced.","As shown in , a contiguous region of system memory can be allocated to each local memory  and , such that it emulates local memories  and , respectively of GPU . The local memory addresses accessed by the GPU-kernel code can be translated to offsets within this allocated region. The total size of the local memory allocated in system memory can be M*(local memory size per workgroup), where M is the number of CPU cores. The total size of the private memory can be M*N*(size of private memory per workitem), where M is the number of cores and N is the number of workitems per workgroup.","For added debugging capability, some embodiments of the present invention can include guard pages between separate memory allocations in system memory. For example, guard pages , , , , , and  can be inserted between allocations for different OS threads, as well as between different memory allocations for the same thread. In an example implementation, a register can be set each time an instruction attempts to access a guard page area of the memory. The fast detection of memory access errors can be particularly useful in applications where numerous parallel threads are execute in a substantially parallel manner.","An Example Scheduling of Workitems on CPU",{"@attributes":{"id":"p-0084","num":"0083"},"figref":"FIG. 5","b":["501","523","317","501","141","142","141","142"]},"In step , it is determined whether the GPU-kernel requires synchronization with other parallel executing GPU-kernels. For example, the OpenCL code may include one or calls to a barrier( ) function. If such a call for synchronization exists, then it is determined that the GPU-kernel requires synchronization; otherwise the GPU-kernel may not require synchronization.","If it is determined that the GPU-kernel does not require synchronization, then the GPU-kernel is scheduled to execute from beginning to end according to the number instances required. For example, within each of the OS threads created in step , a predetermined number of GPU-kernels, or workitems, are scheduled to execute as user-level threads. Because, no synchronization is required among the workitems, each workitem is scheduled to execute from beginning to end.","When the execution of a workitem user thread is completed, in step , the OpenCL execution framework can be notified, for example, by the use of a callback function. When it is notified that a workitem has completed execution, in step , the next workitem in the workgroup can be scheduled for execution as a user level thread in step . The OpenCL execution framework can also periodically query one or more status parameters to determine if a workitem has completed execution. When no more workitems remain to be scheduled within the workgroup, the workgroup has completed execution.","It should be noted that, in some embodiments, more than one workgroup can be scheduled on each CPU core, to be executed serially one after another. If in step , it is determined that all workitems of the executing workgroup has completed execution, then, in step  it is determined if there are other workgroups to be executed on the current core. If there are more workgroups to be scheduled on the current CPU core, then the next workgroup is scheduled for execution. Scheduling another workgroup may include re-initializing the memory allocations for the currently executing OS thread on the current CPU core. Note, that in embodiments of the present invention, the memory allocations can be done once when the OS thread is created, and generally does not require reallocation for each workgroup assigned to that OS thread.","If, in step , it is determined that synchronization is required, a GPU-kernel is scheduled to execute from beginning to the next synchronization point (e.g., barrier( ) call), according to the number instances required. For example, in step , the user-level thread of a workitem is scheduled to execute from beginning to the next occurring call to barrier( ). When the call to barrier( ) is encountered, the state of the executing thread can be saved. In one embodiment, a function call, such as setjmp( ), can be used to save the state of the user-level thread. The state of the thread that is saved can include contents of registers, such as, stack pointer, frame pointer, and program counter. The state of each workitem user-level thread can be saved in an area of the system memory such as system memory , or in persistent memory, such as persistent memory .","When, in step , the executing workitem thread reaches the synchronization point, then in step , it is determined if more workitems are to be scheduled within the current workgroup. When the execution of a workitem user-level thread reaches the synchronization point, in step , the OpenCL execution framework can be notified, for example, using a callback function. When it is notified that a workitem has reached the synchronization point, in step , the next workitem in the workgroup can be scheduled for execution as a user-level thread in step . The OpenCL execution framework can also periodically query one or more status parameters to determine if a workitem has reached the synchronization point. When no more workitems remain to be scheduled within the workgroup, the workgroup has completed executing each workitem to the synchronization point.","If, in step , the workitems are determined to have more synchronization points before reaching the end of the thread execution, steps - are repeated for each workitem to execute from the current synchronization point to the next synchronization point.","If, in step , it is determined that the workitems in the workgroup have no more synchronization points, then in step , one or the workitems that has reached the synchronization point is scheduled to execute from that point to the end. Scheduling a workitem thread to execute from the synchronization point to the end can involve restoring the state of the thread. In one embodiment, a function call, such as longjmp( ), can be used to restore the state of the thread that was previously saved using setjmp( ). It is noted that setjmp( ) and longjmp( ) are function names that are conventionally identified with saving and restoring the state, respectively, of a process or thread.","In step , it is determined if the executing workitem thread has completed execution. As described in relation to step , the determination of whether the workitem thread has completed execution can be based on a callback function or periodic query. If it is determined that the workitem has completed execution, then in step , it is determined whether more workitem threads are to be executed from the last encountered synchronization point to the end. If so, the next workitem is scheduled and steps - repeated for each workitem to be executed.","When all of the workitem threads of a workgroup have completed execution, then, in step , it is determined if other workgroups are to be scheduled on the currently selected CPU core. If other workgroups are to be scheduled, then steps - are repeated for each of the other workgroups.",{"@attributes":{"id":"p-0095","num":"0094"},"figref":"FIG. 6","b":["601","601","602"],"sub":["0 ","n-1 ","0 ","n-1 "]},"Conclusion","The Summary and Abstract sections may set forth one or more but not all exemplary embodiments of the present invention as contemplated by the inventor(s), and thus, are not intended to limit the present invention and the appended claims in any way.","The present invention has been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed.","The foregoing description of the specific embodiments will so fully reveal the general nature of the invention that others can, by applying knowledge within the skill of the art, readily modify and\/or adapt for various applications such specific embodiments, without undue experimentation, without departing from the general concept of the present invention. Therefore, such adaptations and modifications are intended to be within the meaning and range of equivalents of the disclosed embodiments, based on the teaching and guidance presented herein. It is to be understood that the phraseology or terminology herein is for the purpose of description and not of limitation, such that the terminology or phraseology of the present specification is to be interpreted by the skilled artisan in light of the teachings and guidance.","The breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments, but should be defined only in accordance with the following claims and their equivalents."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS\/FIGURES","p":["The accompanying drawings, which are incorporated in and constitute part of the specification, illustrate embodiments of the invention and, together with the general description given above and the detailed description of the embodiment given below, serve to explain the principles of the present invention. In the drawings:",{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
