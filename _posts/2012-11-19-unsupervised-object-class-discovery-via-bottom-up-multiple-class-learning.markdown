---
title: Unsupervised object class discovery via bottom up multiple class learning
abstract: Techniques for unsupervised object class discovery via bottom-up multiple class learning are described. These techniques may include receiving multiple images containing one or more object classes. The multiple images may be analyzed to extract top saliency instances and least saliency instances. These saliency instances may be clustered to generate and/or update statistical models. The statistical models may be used to discover the one or more object classes. In some instances, the statistical models may be used to discover object classes of novel images.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09224071&OS=09224071&RS=09224071
owner: Microsoft Technology Licensing, LLC
number: 09224071
owner_city: Redmond
owner_country: US
publication_date: 20121119
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["One application of machine learning uses computer vision techniques to analyze and understand images in order to produce numerical or symbolic information from the images. These types of techniques can be used by a machine to recognize that a picture of a book contains an image of a book. The computer vision techniques achieve great success in fully supervised object recognition in which label images are used to train a recognition system. However, fully supervised object recognition demands a large amount of labeled training data, which is costly to obtain and not always available because most labeled training data is created by manual human labeling of images. To avoid the need for extensive human involvement, many unsupervised approaches have been proposed for training object recognition systems. While important progresses have been made, these unsupervised approaches require certain conditions, e.g., large occupation of foreground objects, exclusion of irrelevant other object types and clean backgrounds. These conditions limit application of unsupervised object recognition.","Described herein are techniques for unsupervised object class discovery. The techniques may retrieve a set of training images for object recognition model learning. The techniques may then automatically and simultaneously localize objects of the training image, discover object classes of the objects and train machine learning models. The trained machine learning models may then be used to discover object classes of novel images.","This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used to limit the scope of the claimed subject matter.","Overview","This disclosure is directed, in part, to unsupervised object class discovery via bottom-up multiple class learning. Embodiments of the present disclosure train statistical models using a bottom-up saliency detection algorithm as well as a maximum margin algorithm, and discover object classes of images using the trained statistical models.","In accordance with the embodiments, a set of images may be received, and saliency instances may be extracted from the set of images. Top saliency instances extracted from individual image may be labeled as a positive bag, while least saliency instances may be labeled as a negative bag. Both positive and negative bags of the set of images may be collected to train the statistical models using a maximum margin learning algorithm. This algorithm may be implemented to discriminate positive bags (e.g., foreground objects) from negative bags (e.g., background objects) and to maximize differences among the positive bags. The trained statistical models may be then used to discover object classes.","Existing techniques for unsupervised object learning includes Multiple Instance Learning (MIL) and Multiple Instance Clustering (MIC). MIL significantly reduces efforts in manual labeling for object detection. However, existing MIL solutions cannot be directly applied in unsupervised object discovery since they assume a single object class among positive bags. While existing MIC solutions are designed to perform localized content-based image clustering with fewer constraints, their performance is poor because they treat all the images as positive bags. But embodiments of the present disclosure automatically and simultaneously localize objects, discover object classes and train statistical models thereby significantly improving object learning.","Illustrative Architecture",{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 1","b":["100","100","102","104","104","102","106","102","108","108","110"]},"In the illustrated embodiment, the training images  are retrieved by the computer vision platform . Individual image of the training images  may include an object that belongs to an object class, which is a class, category, or pattern of the object. In some instances, the object may belong to various object classes. Here, the object may be an object of interest (e.g., cat, dog, house, bike, sky, and the like) in the individual images of the training images , and the individual image may include one or more objects. For instance, a training image  may include an object  (i.e., a book) and an object  (i.e., a chair). In some instances, the object may include the object of interest and attributes of the object of interest, e.g., cloudy sky, yellow cats, and high-speed trains. In some instances, the object may include concepts, e.g., fashion, beauty, and hot\/cool. For some of the concepts, the meaningful content, value, or boundaries of their application may vary considerably based on context or conditions. In other words, these concepts may be hard to describe in terms of quantitative limits or parameters.","In some embodiments, the computer vision platform  may retrieve the training images  using search engines, e.g., Bing\u00ae and Google\u00ae, based on a keyword (e.g., book). In some instances, the computer vision platform  may be provided (e.g., manually by users, programmers, or the like) a predetermined number of object classes that need to be discovered and\/or learned from the training images . In other instances, the number of object classes to be discovered may not be available to the computer vision platform . In such instances the computer vision platform  operates without a predetermined number of object classes.","After retrieving the training images , the computer vision platform  may automatically and simultaneously localize objects, discover object classes, and generate and\/or update the statistical models . Using the statistical models , the computer vision platform  may localize objects and discover object classes of images , which were not previously processed by the computer vision platform . In some embodiments, one individual model of the statistical models  may be trained to discover an object class. Thus, the individual model corresponds to an object class and functions in identifying objects that belong to that object class. In these instances, the learned individual model may be used to discover and localize objects that belong to the object class in the images .","Turning back to the illustrated embodiment, a novel image  may be analyzed using the trained statistical models , and an object  (i.e., a book) may be localized as indicated by a window indicator . In some instances, the computer vision platform  may identify the discovered object  in the novel image  as a book, and\/or indicate that this discovered object  and the object  of the training image  belong to the same object class.","Illustrative Scheme",{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 2","b":["200","200","102","108","108","202","204","206","202","102","204","110","110","206"]},"In some embodiments, the extractor  may determine top saliency instances using a window-based saliency detection algorithm. The extractor  may generate multiple salient windows on the training images  to represent saliency instances. Here, the instances may be raw data of an image region (e.g., image patches). Accordingly, a saliency instance or salient window may include one or more salient objects that are distinguished from backgrounds of images. The extractor  may calculate saliency scores for the multiple salient windows and then extract top salient windows for clustering. In some instances, such as for a training image (i.e., the image ), the extractor  may determine a predetermined number of the salient windows that have saliency scores greater than other windows of the multiple windows extracted from the training image (e.g., 70 salient windows of the image  having top saliency scores).","It has been observed in the Spatially Independent, Variable Area, and Lighting (SIVAL) dataset that 98% of objects are covered in the top 70 salient windows. This property naturally allows defining positive and negative bags for multiple instance learning. In some instances, the positive bag  may include the top salient windows (e.g., the predetermined number of the salient window) of a training image . The negative bag  may include randomly sampled and\/or bottom salient windows of the training image . Here, individual image of the training images  may have a positive bag  and a negative bag . In this way, unsupervised object discovery may be converted into a weakly supervised learning problem.","After determination of positive bags  and negative bags  for the training images , the clustering unit  may collect top salient windows of the training images , and perform an initial clustering using a data mining algorithm (e.g., K-mean algorithm) to obtain initial clusters. Based on the initial clusters, the clustering unit  may perform a refined clustering using a local optimal algorithm to generate and\/or update the statistical models. In some embodiments, the local optimal algorithm may be bottom-up Multiple Class Learning (bMCL) algorithm, which is discussed in a greater detail below. In other embodiments, the local optimal algorithm may be a Convex-Concave Computational Procedure (CCCP) algorithm or a Boost Multiple Instance Learning (MIL) algorithm.","In the illustrated embodiment, for the training image , a set of top saliency windows  and a set of bottom saliency windows  may be generated, as indicated by solid rectangles and dashed rectangles respectively in . For example, the top saliency windows  may include a predetermined number (e.g., 70 windows) of saliency windows having saliency scores greater than other saliency windows of the image . The bottom saliency windows  may include a number of randomly sampled saliency windows and a predetermined number (e.g., 20) of the bottommost saliency windows having saliency scores less than other saliency windows of the image .","The top saliency windows  may be labeled as being grouped in a positive bag , while the bottom saliency windows  may be labeled as being grouped in a negative bag . Accordingly, the positive bag  represents the top saliency windows  having saliency scores greater than other saliency windows of the image . The negative bag  represents the bottom saliency windows  and may also include other randomly sampled saliency windows of the image  that are not included in either the top saliency windows  or the bottom saliency windows .","Similarly, positive bags  and negative bags  may be extracted from other images of the training images . In some embodiments, each image of the training images  may include a positive bag (e.g., the positive bag ) and a negative bag (e.g., the negative bag ). By way of example and not limitation, individual images of the training images  may be assumed to have a foreground object class (i.e., the images show at least one foreground object). For example, it may be assumed that the training image  has an object class (e.g., book or chair).","Based on the positive bags and negative bags of the training images , a clustering may be performed using maximum margin clustering  to maximize margins among the positive bags of different object classes, and between positive bags  and negative bags . In these instances, the clustering may be associated with two-level hidden variables: hidden variables associated with saliency instances and hidden variables associated object classes. These hidden variables may be estimated using Discriminative Expectation\u2014Maximization Algorithm (DiscEM). Accordingly, the clustering may be performed to generate and\/or update the statistical models  under hidden variables. The hidden variable may be optimized to maximize margins among positive bags of different object classes and between positive bags  and negative bags . The learned statistical models  may be used to localize objects and discover object classes of the images .","Illustrative Operations",{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 3","b":["300","300"]},"and other processes described throughout this disclosure including the processes , , and  are illustrated as collections of blocks in logical flow graphs, which represent sequences of operations that can be implemented in hardware, software, or a combination thereof. In the context of software, the blocks represent computer-executable instructions that, when executed by one or more processors, cause the one or more processors to perform the recited operations. Generally, computer-executable instructions include routines, programs, objects, components, data structures, and the like that perform particular functions or implement particular abstract data types. The order in which the operations are described is not intended to be construed as a limitation, and any number of the described blocks can be combined in any order and\/or in parallel to implement the process.","At , the image retrieval module  may retrieve the training images  for the purpose of saliency-guided multiple class learning. For example, the training images may be retrieved using search engines, e.g., Bing\u00ae and Google\u00ae. In some embodiments, the training images  may include an object of interest or at least majority of the training images  may include the object of interest (e.g., the book in the training image ). For example, a keyword (e.g., book) may be searched for images using the search engines, and a set of images may be returned corresponding to the keyword. The set of images may be used as the training images  for saliency-guided multiple class learning.","In some embodiments, the saliency-guided multiple class learning may be performed without information regarding what object classes the computer vision platform  is to discover and\/or learn. In some instances, a number of object classes may be assigned as input to notify the computer vision platform  how many object classes need to be discovered from the training images . In other instances, the computer vision platform  may not know how many object classes of training images  need to be discovered.","At , the object-class discovery module  may localize objects and discover object classes, and train the detector  simultaneously and in an unsupervised way. The operation  may be implemented using bottom-up (i.e., saliency-guided) multiple class learning (bMCL). To formulate the bMCL, a saliency detection algorithm may be adopted to convert unsupervised learning into weakly supervised multiple instance learning, which is discussed in a greater detail in . Further, a DiscEM algorithm may be utilized to solve bMCL problems regarding hidden variables, and therefore to localize objects, discover object classes, and train the detector  simultaneously in an integrated framework. In some embodiments, single object location and class discovery may be performed using the bMCL framework.","Previous unsupervised object discovery methods cannot obtain discriminative object models in an integrated manner. They are either restricted to only categorization (i.e., no object localization) or have to resort to a separate detector training process using their localization results or only obtain specialized detectors, such as chamfer distance based shape templates. By contrast, bMCL integrates the detector training into the framework for generic object classes.","At , the trained detector  may be used to detect an object in a novel image (e.g., the novel image ). The trained detector  may both localize the object and discover the object class in the novel image at the same time as illustrated in . In some embodiments, the computer vision platform  may automatically retrieve a large amount of images using search engines, and train the statistical models  to learn various object classes. Accordingly, the computer vision platform  may enable search engines to filter searched images before returning them to users or to filter raw images before indexing them.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 4","FIG. 2","FIG. 2"],"b":["400","402","202","102"]},"Based on the computed saliency scores of saliency windows, the extractor  may determine top saliency windows as top saliency instances of individual image of the training images  (e.g., the training image ) at . For example, the top saliency windows may include a predetermined number of saliency windows having saliency scores greater than other saliency windows of saliency windows generated from the image . In other instances, the top saliency windows may include saliency windows having saliency scores such as to satisfy a predetermine condition (e.g., greater than a predetermined saliency scores).","At , the extractor  may determine bottom saliency windows as bottom saliency instances. For example, the bottom saliency windows may include a predetermine number of saliency windows having saliency scores less than other saliency windows generated from the image . The bottom saliency windows may also include saliency windows that are randomly sampled in the image . In other examples, the bottom saliency windows may include saliency windows having saliency scores that satisfy a predetermine condition (e.g., less than a predetermined saliency score).","At , positive bags and negative bags may be generated to label the top saliency instances and the bottom saliency instances. In some embodiments, a training image may have a positive bag containing the top saliency instances of the training image, and a negative bag containing the bottom saliency instances of the training image. The positive bags and negative bags may be collected and utilized for maximum margin clustering, which is discussed in a greater detail in .",{"@attributes":{"id":"p-0038","num":"0037"},"figref":["FIG. 5","FIG. 2"],"b":["500","216","502","504"]},"At , another clustering may be performed based on the initial clusters to generate and\/or update the statistical models . The clustering may be implemented using a local optimal algorithm, e.g., bMCL, CCCP, or Boost MIL. The local optimal algorithm may function to maximize 1) margins among the positive bags derived from the training images , and 2) margins between the positive bags and the negative bags derived from the training images .","To optimize the maximum margin clustering , discriminative learning may be conducted under the presence of hidden variables using the DiscEM algorithm. The hidden variables may be divided into two types based on their levels. One type is hidden variables associated with saliency instances, and the other type is hidden variables associated the object classes. The DiscEM algorithm may be implemented via two steps: an E step and an M step. The E step may include sampling and applying a statistical model to the probability estimation of an instance label and a class label corresponding to the statistical model. The M step may include training a new detector based on the sampled data. In some embodiments, EM steps for hidden instance labels may be replaced by a standard Boost MIL, and class labels may be then integrated out. The learned detector may be used to localize objects and discover object classes of novel images.",{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 6","b":["600","602","104","104"]},"In some embodiments, the query may include a keyword associated with an image (e.g., a description of the image) or with a concept. For example, the query may include keywords like \u201cfind  in fashion.\u201d This type of query is difficult for the conventional image search techniques to perform due to lack of low-level descriptions (e.g., textures, colors, and shapes) and\/or high-level descriptions (e.g., shoes, coats, and cars). But embodiments of the present disclosure perform unsupervised object class discovery via bMCL, which is suitable for searches using middle-level features (e.g., fashion, beauty, hot\/cool).","At , the computer vision platform  may determine whether the query is associated with an image. For example, users may submit a sample image, instead of a string of text, that image searches may be performed based upon. If the query includes an image (i.e., the branch of \u201cYes\u201d of ), the computer vision platform  may, at , extract saliency instances from the image using techniques similar to those discussed in the process  as shown in . At , the computer vision platform  may discover one or more objects in the image based on the extracted saliency instances using the trained detector , which is similar to those discussed in the process  as shown in . Based on the discovered objects, candidate images may be identified corresponding to the image at . Similarly, if the query does not include an image (i.e., the branch of \u201cNo\u201d of ), the candidate images may be identified based on matches between the query (e.g., middle-level descriptions) and one or more object classes of the candidate images at .","Illustrative Algorithms",{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 7","b":["700","700","700","700","702","700"]},"The algorithm  may include lines , which may collect positive bags and negative bags of the training images  using a saliency detection algorithm. The positive bags may include top saliency instances of the training images , while the negative bags may include bottom saliency instances and randomly sampled saliency instances of the training images . In some embodiments, an image of the training images  may be analyzed to obtain a positive bag and a negative bag. In some instances, the image may be assumed to have a foreground object class. In lines -, positive and negative bags of the training images  may be analyzed as input of the algorithm , and a number of clusters (i.e., K classifiers) may be determined as learned results of the algorithm .","In lines -, a DiscEM algorithm may be implemented to obtain the solution, K object detectors. Starting from step , lines - illustrate that clustering may be initialized using all the top saliency windows to obtain K initial clusters using a data mining algorithm (e.g., K-means algorithm). Lines - illustrate that individual cluster of the number of clusters may be trained based on weights of samples. The weight of individual sample may then be updated based on the clusters that have been trained as illustrated in lines -. Lines - illustrate that the training may stop when no more improvement can be made.","Illustrative Multiple Instance Learning","In Multiple Instance Learning (MIL), each bag x\u03b5Xmay include a set of instances {x, . . . , x}(x\u03b5X). While each bag xhas a class label y\u03b5y={\u22121,1} as training input, instance labels y\u03b5y may be unknown and treated as hidden variables. In some embodiments, a bag may be labeled as positive if at least one instance is positive, while a bag may be labeled as negative if all instances are negative, y=max(y). In these instances, each bag may be assumed to have the same number of instances, n=m (i=1, . . . , n).","Standard boosting MIL assumes an additive model on instance-level decisions: h=h(x), where h(x)=\u03a3\u03bbh(x) is a weighted vote of weak classifiers h: x\u2192y. It may be assumed that y\u03b5y is the hidden instance label having a probability of being positive, as shown in Equation 1:",{"@attributes":{"id":"p-0049","num":"0048"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["p","ij"]},"mo":"=","mrow":{"mrow":[{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":{"mi":["y","ij"]},"mo":"=","mrow":{"mn":"1","mo":"|","msub":{"mi":["x","ij"]}}},"mo":";","mi":"h"}}},{"mfrac":{"mn":"1","mrow":{"mn":"1","mo":"+","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":"-","msub":{"mi":["h","ij"]}}}}}},"mo":"."}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}}},"The bag-level probability may be computed via a Noisy-OR (NOR) model using Equation 2:\n\n(=1)=1\u2212\u03a0(1).\u2003\u2003(2)\n\nSince the bag label is given in the training set, the negative log-likelihood function may be optimize using L=\u2212\u03a3(1(y=1)log p+1(y=\u22121)log(1\u2212p)), wherein 1(\u00b7) is an indicator function; the algorithm greedily searches for hover a weak classifier candidate pool, followed by a line search for \u03bb. In Boost MIL framework, the weigh won each instance xis updated as Equation 3:\n",{"@attributes":{"id":"p-0051","num":"0050"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":["w","ij"]},"mo":"=","mrow":{"mrow":[{"mo":"-","mfrac":{"mrow":[{"mo":"\u2202","msub":{"mi":["\u2112","MIL"]}},{"mo":"\u2202","msub":{"mi":["h","ij"]}}]}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mo":"-","mfrac":{"mn":"1","mrow":{"mn":"1","mo":"-","msub":{"mi":["p","ij"]}}}},"mo":"\u2062","mfrac":{"mrow":[{"mo":"\u2202","msub":{"mi":["p","ij"]}},{"mo":"\u2202","msub":{"mi":["h","ij"]}}]}}},{"mrow":{"mrow":[{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["y","i"]}},{"mo":"-","mn":"1"}],"mo":"="}}]},{"mtd":[{"mrow":{"mfrac":[{"mrow":[{"mn":"1","mo":"-","msub":{"mi":["p","i"]}},{"msub":{"mi":["p","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":["p","ij"]}}}}]},{"mrow":[{"mo":"\u2202","msub":{"mi":["p","ij"]}},{"mo":"\u2202","msub":{"mi":["h","ij"]}}]}],"mo":"\u2062"}},{"mrow":{"mrow":{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["y","i"]}},"mo":"=","mn":"1"}}]}]}}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}},"br":{}},"Given K object classes and N unlabeled images, n=2N bags (i.e., N positive bags and N negative bags based on bottom-up saliency detection) may be obtained. There are two kinds of hidden variables in the formulation: 1) the instance-level label yfor each instance xin bag xand 2) the class latent label k\u03b5K={0, 1, . . . , K} for the instance xthat belongs to the kclass (k=0 and k=0 for the negative instance and bag respectively). In some embodiments, it is assumed that the existence of only one foreground object class in each positive bag. In other words, only one class of objects is allowed to appear in each image. Thus, the class label kfor each positive bag is defined based on the class labels of instances of the positive bag as shown in Equation 4:",{"@attributes":{"id":"p-0053","num":"0052"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msub":{"mi":["k","i"]},"mo":"=","mrow":{"munder":{"mi":["max","j"]},"mo":"\u2062","msub":{"mi":["k","ij"]}}},{"mo":"\u2200","mi":"j"},{"msub":{"mi":["k","ij"]},"mo":["\u2208","\u2208"],"mrow":[{"mrow":{"mo":["{","}"],"mrow":{"mn":"0","mo":",","mi":"k"}},"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mi":["with","k"]},{"mrow":{"mo":["{","}"],"mrow":{"mn":"1","mo":[",","\u2062",","],"mi":["\u2026","K"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},"mo":"."}]}],"mo":[",",",","\u2062",","],"mi":"and","mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{},"sub":["K","1","K","i","1","ij","ij","ij","i"]},"For bags X={x, . . . , x} with their corresponding labels Y={y, . . . , y}, the overall negative log-likelihood function L(\u03b8; Y, X) may be defined as shown in Equation 5:",{"@attributes":{"id":"p-0055","num":"0054"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mi":"\u2112","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["\u03b8","Y"],"mo":";"},"mo":",","mi":"X"}}},{"mrow":[{"mo":"-","mi":"log"},{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["Y","X"],"mo":"|"},"mo":";","mi":"\u03b8"}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mrow":[{"mo":"-","mi":"log"},{"munder":{"mo":"\u2211","msub":{"mi":["H","K"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"Y","mo":",","mrow":{"mrow":{"msub":{"mi":["H","K"]},"mo":"|","mi":"X"},"mo":";","mi":"\u03b8"}}}}}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}}},{"mtd":{"mrow":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mrow":[{"mo":"-","mi":"log"},{"munder":{"mo":"\u2211","msub":{"mi":["H","K"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"munder":{"mo":"\u2211","msub":{"mi":["H","I"]}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"Y","mo":",","mrow":{"mrow":{"mi":["H","X"],"mo":"|"},"mo":";","mi":"\u03b8"}}}}}}],"mo":"\u2062"}},"mo":","}}}]}},{"mrow":{"mo":["(",")"],"mn":"5"}}]}}}},"br":{},"sup":["1","k","K","k ","th ","th "],"sub":"ij "},{"@attributes":{"id":"p-0056","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msubsup":{"mi":["q","ij","k"]},"mo":"=","mrow":{"mrow":{"msup":{"mi":["q","k"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["x","ij"]}}},"mo":"=","mfrac":{"mn":"1","mrow":{"mn":"1","mo":"+","mrow":{"mi":"exp","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mo":"-","msubsup":{"mi":["h","ij","k"]}}}}}}}}}},"br":[{},{}],"sub":["ij","ij","ij","ij","ij","t=1","ij","ij"],"sup":["k","k","k","k","K","t","1(t=k)","t","1(t\u2260k)."],"in-line-formulae":[{},{}],"i":["p","=Pr","k","=k|x","q","\u2212q"]},"Accordingly, the probability Pr(Y, H|X; \u03b8) may be derived, and all the bags are assumed to be conditionally independent as shown in Equation 7:\n\n()=\u03a0(;\u03b8)=\u03a0(;\u03b8)\u00b7],\u2003\u2003(7)\n\nwhere s=1((y=\u22121k=0)(y=1k\u22600)).\n","The probability for each positive or negative bag, with Pr(k=k|x; \u03b8)\u2248qand k\u03b5{0, 1, . . . , K} as (the full derivation is combinatorial), may be approximated using Equation 8:\n\n=\u03a0{[1\u2212\u03a0(1)]) \u00b7[\u03a0(1)]) },\u2003\u2003(8)\n\nwhere 1=\u03a0(1\u2212p)=Pr(\u2203j, k=t|x; \u03b8) denotes the measure for at least one instance xin bag xbelonging to the tclass. Pr(Y, H|X; \u03b8) may be then denoted in a class-wise manner as shown in Equation 9:\n\n()\u221d\u03a0\u03a0[())(1))].\u2003\u2003(9)\n","The computer vision platform  may further explicitly use the instance-level hidden variables Hand denote Pr(Y, H|X; \u03b8). Similar to the overall loss function L(\u03b8; Y, X), the bag-level loss function L(\u03b8; Y, X, H)=\u2212log Pr(Y, H|X; \u03b8) and instance-level loss function L(\u03b8; Y, X, H)=\u2212log Pr(Y, H|X; \u03b8) may be defined. These functions may use the Discriminative EM (DiscEM) algorithm, which is discussed in greater detail below.","In the DiscEM algorithm, if the expectation of H={H, H} is estimated, the minimization of the overall loss function",{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}},"mo":"\u2062","mrow":{"mi":"\u2112","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["\u03b8","Y"],"mo":";"},"mo":",","mi":"X"}}}}}},"br":{}},{"@attributes":{"id":"p-0062","num":"0061"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mfrac":{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}},"mo":"\u2062","mrow":{"mi":"\u2112","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["\u03b8","Y"],"mo":";"},"mo":[",",","],"mi":["X","H"]}}}},"mo":","}}},"br":[{},{}],"sub":["ij","ij","ij","t","t","t","ij","t"],"sup":["k","k","k","k","k"],"b":"104"},"The optimization of Equation 5 deals with the hidden variable H. A general formulation of Discriminative EM (DiscEM) algorithm may be provided to perform discriminative learning in the presence of hidden variables. The computer vision platform  may directly apply the DiscEM to explore the hidden variable H in bMCL. It is observed that under the MIL assumption, Boost MIL may be equivalent to the formulation of bMCL. Based on this observation, the EM step for the instance-level hidden variables Hmay be dealt with in a standard Boost MIL and only the class labels are tagged Hexplicitly. Since the DiscEM algorithm is a general discriminative learning framework in the presence of hidden variables, the DiscEM algorithm can be applied to other situations with hidden space of explicit forms.","In some embodiments, labels Y={y, . . . , y} may be provided in addition to observations X={x, . . . , x}, and the computer vision platform  may estimate the model \u03b8 that minimizes the negative log-likelihood function L(\u03b8; Y, X). In some instances, H may be integrated out based on two theorems: Theorem 1 and Theorem 2, which are discussed in greater detail below.","Theorem 1 The discriminative expectation maximization (DiscEM) algorithm may be implemented to optimize the training set log likelihood L(\u03b8; Y, X) model parameters \u03b8 in the presence of hidden variable H, via Equation 10:",{"@attributes":{"id":"p-0066","num":"0065"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mfrac":{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}},"mo":"\u2062","mrow":{"mi":"\u2112","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["\u03b8","Y"],"mo":";"},"mo":",","mi":"X"}}}},{"msub":{"mi":"E","mrow":{"mi":"H","mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["H","Y"],"mo":"|"},{"mi":["X","\u03b8"],"mo":";"}],"mo":","}}}}},"mo":["\u2062","\u2062"],"mfrac":{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}},"mrow":{"mi":"\u2112","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["\u03b8","Y"],"mo":";"},"mo":[",",","],"mi":["X","H"]}}}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"10"}}]}}}},"br":{}},{"@attributes":{"id":"p-0067","num":"0066"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["H","Y"],"mo":"|"},{"mi":["X","\u03b8"],"mo":";"}],"mo":","}}},"mo":"=","mfrac":{"mrow":[{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"Y","mo":",","mrow":{"mrow":{"mi":["H","X"],"mo":"|"},"mo":";","mi":"\u03b8"}}}},{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["Y","X"],"mo":"|"},"mo":";","mi":"\u03b8"}}}]}}}},"br":{}},"The general form of the DiscEM algorithm may be similar to the standard EM. It is observed that an initial estimate \u03b8with successively better estimates \u03b8, \u03b8, . . . , may be performed until convergence. Each phase r consists of two steps: an E step which computes Pr(H|Y, X; \u03b8) via previous estimate \u03b8and an M step which updates \u03b8by minimizing L(\u03b8; Y, X). In some instances, parameter \u03b8 is a parameter of a classifer and parameter \u03b8 may be purely discriminative to take advantages of discriminative learning algorithms. This differentiates the DiscEM from other conditional-EM frameworks in which the task is to learn generative parameters through a discriminative objective. Compared with standard supervised algorithms, the DiscEM algorithm can better handle hidden variables and embrace a weakly supervised learning setting.","If all the data are assumed to be conditionally independent Pr(Y|X; \u03b8)=\u03a0Pr(y|x; \u03b8), Theorem 2 can connect Boost MIL and DiscEM.","Theorem 2 When the instance-level model (i.e., Equation 1) and the bag-level model (i.e., Equation 2) are used, Boost MIL's update rule (i.e., Equation 3) may be equivalent to DiscEM, which reads as Equation 11:",{"@attributes":{"id":"p-0071","num":"0070"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mfrac":{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}},"mo":["\u2062","\u2062","\u2062"],"mi":"log","mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msub":[{"mi":["y","i"]},{"mi":["x","i"]}],"mo":"|"},"mo":";","mi":"\u03b8"}}}},{"mo":"{","mtable":{"mtr":[{"mtd":[{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mfrac":[{"mrow":[{"mo":"-","mn":"1"},{"mn":"1","mo":"-","msub":{"mi":["p","ij"]}}]},{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}}],"mo":["\u2062","\u2062"],"msub":{"mi":["p","ij"]}}}},{"mrow":{"mrow":[{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["y","i"]}},{"mo":"-","mn":"1"}],"mo":"="}}]},{"mtd":[{"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"j","mo":"=","mn":"1"},"mi":"m"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mfrac":[{"mrow":[{"mn":"1","mo":"-","msub":{"mi":["p","i"]}},{"msub":{"mi":["p","i"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":["p","ij"]}}}}]},{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}}],"mo":["\u2062","\u2062"],"msub":{"mi":["p","ij"]}}}},{"mrow":{"mrow":{"mi":"if","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},"msub":{"mi":["y","i"]}},"mo":"=","mn":"1"}}]}]}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}}}}},"DiscEM may be directly applied to bMCL since bMCL forms an optimization problem for discriminative cost function L(\u03b8; Y, X) under the complex hidden variables H=(H, H) in Equation 5. Based on Theorem 1, the computer vision platform  may alternate between E step (applying model \u03b8to obtain the probability estimation of instance labels Hand class labels H, and sampling) and M step (train new classifiers based on sampled data). Furthermore, taking advantage of the equivalence between DiscEM and Boost MIL, the integration of instance labels Hmay be replaced by a standard Boost MIL and Hmay be integrated out.","Accordingly, Theorem 1 may be used to rewrite",{"@attributes":{"id":"p-0074","num":"0073"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}},"mo":"\u2062","mrow":{"mi":"\u2112","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["\u03b8","Y"],"mo":";"},"mo":",","mi":"X"}}}}}},"br":{}},{"@attributes":{"id":"p-0075","num":"0074"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mfrac":{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}},"mo":"\u2062","mrow":{"mi":"\u2112","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["\u03b8","Y"],"mo":";"},"mo":",","mi":"X"}}}},{"mrow":{"msub":{"mi":"E","mrow":{"msub":{"mi":["H","K"]},"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["H","K"]},"mo":"|","mi":"Y"},{"mi":["X","\u03b8"],"mo":";"}],"mo":","}}}}},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mfrac":{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}},"mo":"\u2062","mrow":{"mi":"\u2112","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"mi":["\u03b8","Y"],"mo":";"},"mo":[",",","],"mi":"X","msub":{"mi":["H","K"]}}}}}}},"mo":"."}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}}},"The loss function may be decomposed in a class-wise manner as L(\u03b8; Y, X, H)=\u03a3L(h; Y, X, H). Using Equation 9, L(h; Y, X, H) may be computed as Equation 13:\n\n()=\u2212\u03a3[1()log 1()log(1)],\u2003\u2003(13)\n\nwhich is valid when all the (y, k) in (Y, H) satisfy the condition s=(y=\u22121k=0)(y=1k\u22600), as shown in Equation 9. In these instances, there may be a normalization term in Equation 9, which may be ignored without affecting the general formulation of DiscEM in Equation 12.\n","Equation 13 may build K classifiers with each classifier htaking bags labeled class k as positive bags and all the rest as negative bags, and minimizes L(h; Y, X, H) respectively. This formulation may maximize margins among positive bags of different classes and also the negative bags since both support vector machines (SVM) and Boosting maximize the margin explicitly and implicitly respectively.","For each L(h; Y, X, H), hidden instance variables Hmay be further integrated out:",{"@attributes":{"id":"p-0079","num":"0078"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mfrac":{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}},"mo":"\u2062","mrow":{"msup":{"mi":["\u2112","k"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msup":{"mi":["h","k"]},"mo":";","mi":"Y"},"mo":[",",","],"mi":"X","msub":{"mi":["H","K"]}}}}},{"mrow":{"msub":{"mi":"E","mrow":{"msub":{"mi":["H","I"]},"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":":"},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}],"mrow":{"mi":"Pr","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"msub":{"mi":["H","I"]},"mo":"|","mi":"Y"},{"mi":["X","\u03b8"],"mo":";"}],"mo":[",",","],"msub":{"mi":["H","K"]}}}}}},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mfrac":{"mo":"\u2146","mrow":{"mo":"\u2146","mi":"\u03b8"}},"mo":"\u2062","mrow":{"msup":{"mi":["\u2112","k"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msup":{"mi":["h","k"]},"mo":";","mi":"Y"},"mo":[",",","],"mi":["X","H"]}}}}}},"mo":"."}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"14"}}]}}}},"br":[{},{}],"sup":["k ","k"],"sub":["K","1 "]},{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIG. 8","FIG. 8"],"b":["800","800","800"]},"Alternatively, or in addition, the functionally described herein may be performed, at least in part, by one or more hardware logic components. For example, and without limitation, illustrative types of hardware logic components that may be used include Field-programmable Gate Arrays (FPGAs), Program-specific Integrated Circuits (ASICs), Program-specific Standard Products (ASSPs), System-on-a-chip systems (SOCs), Complex Programmable Logic Devices (CPLDs), etc.","In a very basic configuration, the computing device  typically includes at least one processing unit  and system memory . Depending on the exact configuration and type of computing device, the system memory  may be volatile (such as RAM), non-volatile (such as ROM, flash memory, etc.) or some combination of the two. The system memory  typically includes an operating system , one or more program modules , and may include program data . For example, the program modules  may includes the object-class discovery module , the image retrieval module  and the statistical models , as discussed in the architecture , the scheme  and\/or the illustrative processes -.","The operating system  includes a component-based framework  that supports components (including properties and events), objects, inheritance, polymorphism, reflection, and the operating system  may provide an object-oriented component-based application programming interface (API). Again, a terminal may have fewer components but will interact with a computing device that may have such a basic configuration.","The computing device  may have additional features or functionality. For example, the computing device  may also include additional data storage devices (removable and\/or non-removable) such as, for example, magnetic disks, optical disks, or tape. Such additional storage is illustrated in  by removable storage  and non-removable storage . Computer-readable media may include, at least, two types of computer-readable media, namely computer storage media and communication media. Computer storage media may include volatile and non-volatile, removable, and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data. The system memory , the removable storage  and the non-removable storage  are all examples of computer storage media. Computer storage media includes RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD), or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other non-transmission medium that can be used to store the desired information and which can be accessed by the computing device . Any such computer storage media may be part of the computing device . Moreover, the computer-readable media may include computer-executable instructions that, when executed by the processor unit(s) , cause the computing device  to perform various functions and\/or operations described herein.","In contrast, communication media may embody computer-readable instructions, data structures, program modules, or other data in a modulated data signal, such as a carrier wave, or other transmission mechanism. As defined herein, computer storage media does not include communication media.","The computing device  may also have input device(s)  such as keyboard, mouse, pen, voice input device, touch input device, etc. In some embodiments, input methods may be implemented via Natural User Interface (NUI). NUI may include any interface technology that enables a user to interact with a device in a \u201cnatural\u201d manner, free from artificial constraints imposed by input devices such as mice, keyboards, remote controls, and the like. Examples of NUI methods may include those relying on speech recognition, touch and stylus recognition, gesture recognition both on screen and adjacent to the screen, air gestures, head and eye tracking, voice and speech, vision, touch, gestures, and machine intelligence. Categories of NUI technologies may include touch sensitive displays, voice and speech recognition, intention and goal understanding, motion gesture detection using depth cameras (such as stereoscopic camera systems, infrared camera systems, RGB camera systems and combinations of these), motion gesture detection using accelerometers\/gyroscopes, facial recognition, 3D displays, head, eye, and gaze tracking, immersive augmented reality and virtual reality systems, all of which provide a more natural interface, as well as technologies for sensing brain activity using electric field sensing electrodes (EEG and related methods). Output device(s)  such as a display, speakers, printer, etc. may also be included. These devices are well known in the art and are not discussed at length here.","The computing device  may also contain communication connections  that allow the device to communicate with other computing devices , such as over a network. These networks may include wired networks as well as wireless networks. The communication connections  are one example of communication media.","It is appreciated that the illustrated computing device  is only one example of a suitable device and is not intended to suggest any limitation as to the scope of use or functionality of the various embodiments described. Other well-known computing devices, systems, environments and\/or configurations that may be suitable for use with the embodiments include, but are not limited to personal computers, server computers, hand-held or laptop devices, multiprocessor systems, microprocessor-base systems, set top boxes, game consoles, programmable consumer electronics, network PCs, minicomputers, mainframe computers, distributed computing environments that include any of the above systems or devices, and\/or the like. For example, some or all of the components of the computing device  may be implemented in a cloud computing environment, such that resources and\/or services are made available via a computer network for selective use by mobile devices.","Conclusion","Although the subject matter has been described in language specific to structural features and\/or methodological acts, it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather, the specific features and acts are disclosed as example forms of implementing the claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The same reference numbers in different figures indicate similar or identical items.",{"@attributes":{"id":"p-0006","num":"0005"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0007","num":"0006"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0008","num":"0007"},"figref":["FIGS. 3-6","FIGS. 1 and 2"]},{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":["FIG. 8","FIG. 1","FIG. 2"]}]},"DETDESC":[{},{}]}
