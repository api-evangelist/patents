---
title: Apparatus and method for real-time volume processing and universal three-dimensional rendering
abstract: An apparatus and method for real-time volume processing and universal three-dimensional rendering. The apparatus includes a plurality of three-dimensional (3D) memory units; at least one pixel bus for providing global horizontal communication; a plurality of rendering pipelines; at least one geometry bus; and a control unit. The apparatus includes a block processor having a circular ray integration pipeline for processing voxel data and ray data. Rays are generally processed in image order thus permitting great flexibility (e.g., perspective projection, global illumination). The block processor includes a splatting unit and a scattering unit. A method for casting shadows and performing global illumination in relation to light sources includes sweeping a two dimensional array of rays through the volume can also be implemented with the apparatus. A method for approximating a perspective projection includes using parallel projection.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07471291&OS=07471291&RS=07471291
owner: The Research Foundation of State University of New York
number: 07471291
owner_city: Albany
owner_country: US
publication_date: 20061106
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["This application claims priority based on an International Application filed under the Patent Cooperation Treaty, PCT\/US01\/06345, filed on Feb. 26, 2001.","This application is a divisional application of, and claims priority to, co-pending U.S. Ser. No. 10\/204,685, filed on Aug. 23, 2002.","This application claims priority to U.S. Provisional Patent Application Ser. No. 60\/185,076, filed on Feb. 25, 2000, which is incorporated herein by reference.","This invention was made with Government support under grant MP9527694 awarded by the National Science Foundation and under grant N000149710402 awarded by the Office of Naval Research.","1. Field of the Invention","The present invention relates generally to three-dimensional (3D) graphics and volume visualization, and more particularly relates to an apparatus and method for real time volume processing and universal three-dimensional rendering.","2. Description of the Prior Art","Computer rendering is the process of transforming complex information into a format which is comprehensible to human thought, while maintaining the integrity and accuracy of the information. Volumetric data, which consists of information relating to three-dimensional phenomena, is one species of complex information that can benefit from improved image rendering techniques. The process of presenting volumetric data, from a given viewpoint, is commonly referred to as volume rendering.","Volume visualization is a vital technology in the interpretation of the great amounts of volumetric data generated by acquisition devices (e.g., biomedical scanners), by supercomputer simulations, or by synthesizing geometric models using volume graphics techniques. Of particular importance for manipulation and display of volumetric objects are the interactive change of projection and rendering parameters, real-time display rates, and in many cases, the possibility to view changes of a dynamic dataset over time, called four-dimensional (4D) visualization (i.e., spatial-temporal), as in the emerging integrated acquisition visualization systems.","A volumetric dataset is commonly represented as a 3D grid of volume elements (voxels), often stored as a full 3D raster (i.e., volume buffer) of voxels. Volume rendering is one of the most common techniques for visualizing the 3D scalar field of a continuous object or phenomenon represented by voxels at the grid points of the volume dataset, and can be accomplished using two primary methods: object-order methods and image-order methods. Using an object-order approach, the contribution of each voxel to the screen pixels is calculated, and the combined contribution yields the final image. Using an image-order approach, sight rays are cast from screen pixels through the volume dataset, and contributions of voxels along these sight rays are used to evaluate the corresponding pixel values.","Over the past three decades graphics systems have evolved duofold: from primarily two-dimensional (2D) to 3D and 4D (space and time), and from vector graphics to raster graphics, where the vector has been replaced by the polygon as the basic graphics primitive. This has led to the proliferation of polygon-based geometry engines, optimized to display millions of triangles per second. In such systems, however, triangle facets only approximate the shape of objects. Still, the 3D polygon-based graphics market continues to boom, and has become one of the hottest arenas of the personal computer (PC) industry.","In response to emerging demands placed on traditional graphics systems, various techniques have been devised to handle and display discrete imagery in order to enhance visual realism of the geometric model, as well as enhance or replace object shape and structure. Among these techniques include 2D texture and photo mapping, environment mapping, range images for image-based rendering, 2D mip-mapping, video streams, 3D volumes, 3D mip-mapping, 4D light fields and lumigraphs, and five-dimensional (5D) plenoptic functions. All these techniques require some sort of dimensionality-based interpolation (bilinear, trilinear, quadlilinear, etc.) between discrete pixels, texels, voxels, or n-oxels.","Special purpose computer architectures and methods for volume visualization are known in the art. Traditional methods of volume visualization typically operate by scanning through a volume dataset in a sequential manner in order to provide an accurate representation of an object. For example, Cube-4, an architecture developed by Dr. Arie Kaufman, Ingmar Bitter and Dr. Hanspeter Pfister, some of whom are also named inventors in the present application, is a special purpose scalable volume rendering architecture based on slice-parallel ray-casting. Cube-4 is capable of delivering true real-time ray-casting of high resolution datasets (e.g., 102416-bit voxels at 30 Hertz frame rate). However, Cube-4 cannot deliver such real-time performance for perspective projections. Presently, in known prior art rendering systems, the use of perspective projections either increases the rendering time or decreases the projected image quality. Additionally, prior architectures do not provide the ability to combine volumes and geometries into a single image.","Referring now to , a conventional volume visualization system  is shown. As illustrated in , the volume data is stored on a disk  and loaded into memory  before rendering. A Central Processing Unit (CPU)  then computes the volume rendered image from the data residing in memory . The final image is written to a frame buffer , which is typically embedded on a graphics card, for displaying on a monitor  or similar display device.","The present invention, therefore, is intended to provide a method and apparatus which significantly enhances the capabilities of known methods and apparatus to the extent that it can be considered a new generation of imaging data processing.","Other and further objects will be made known to the artisan as a result of the present disclosure, and it is intended to include all such objects which are realized as a result of the disclosed invention.","The present invention is tantamount to a departure from the prior art because of the all-encompassing new characteristics. An apparatus, in accordance with the present invention, for real-time volume processing and universal three-dimensional (3D) rendering includes one or more three-dimensional (3D) memory units; at least a first pixel bus; one or more rendering pipelines; one or more geometry busses; and a control unit. The apparatus is responsive to viewing and processing parameters which define a viewpoint, and the apparatus generates a 3D volume projection image from the viewpoint. The projected image includes a plurality of pixels.","The 3D memory units store a plurality of discrete voxels, each of the voxels having a location and voxel data associated therewith. The voxels together form a volume dataset, and the viewing and processing parameters define at least one face of the volume dataset as the base plane of the volume dataset as well as first and last processing slices of the volume dataset. The control unit initially designates the first processing slice as a current slice of sample points, and controls sweeping through subsequent slices of the volume dataset as current slices until the last processing slice is reached.","Each of the plurality of rendering pipelines is vertically coupled to both a corresponding one of the plurality of 3D memory units and the at least first pixel bus, and each of the rendering pipelines has global horizontal communication preferably with at most its two nearest neighbors. The rendering pipelines receive voxel data from the corresponding 3D memory units and generate a two-dimensional (2D) base plane image aligned with a face of the volume dataset. The geometry I\/O bus provides global horizontal communication between the plurality of rendering pipelines and a geometry engine, and the geometry I\/O bus enables the rendering of geometric and volumetric objects together in a single image.","The apparatus and methods of the present invention surpass existing 3D volume visualization architectures and methods, not only in terms of enhanced performance, image rendering quality, flexibility and simplicity, but in terms of the ability to combine both volumes and surfaces (particularly translucent) in a single image. The present invention provides flexible, high quality, true real-time volume rendering from arbitrary viewing directions, control of rendering and projection parameters, and mechanisms for visualizing internal and surface structures of high-resolution datasets. It further supports a variety of volume rendering enhancements, including accurate perspective projection, multi-resolution volumes, multiple overlapping volumes, clipping, improved gradient calculation, depth cuing, haze, super-sampling, anisotropic datasets and rendering of large volumes.","The present invention is more than a mere volume rendering machine; it is a high-performance interpolation engine, and as such, it provides hardware support for high-resolution volume rendering and acceleration of discrete imagery operations that are highly dependent on interpolation, including 2D and 3D texture mapping (with mip-mapping) and image-based rendering. Furthermore, the apparatus and methods of the present invention, coupled with a geometry engine, combine volumetric and geometric approaches, allowing users to efficiently model and render complex scenes containing traditional geometric primitives (e.g., polygonal facets), images and volumes together in a single image (defined as universal 3D rendering).","The apparatus of the present invention additionally provides enhanced system flexibility by including various global and local feedback connections, which adds the ability to reconfigure the pipeline stages to perform advanced imagery operations, such as imaging warping and multi-resolution volume processing. Furthermore, the present invention accomplishes these objectives in a cost-effective manner.","A preferred embodiment of the present invention is a method and apparatus for performing approximate perspective volumetric ray casting of a three-dimensional (3D) volume dataset. The apparatus of the invention is an approximating unit configured with the teachings of the method of the invention. The invention can also be practiced on a machine readable medium The method includes the steps of initially selecting viewing and processing parameters which define a viewpoint and a view direction. The length of the volume dataset is calculated between the location of the nearest voxel to the viewpoint and the farthest voxel from the viewpoint. The length is measured along either a line parallel to the view direction or an axis of the three-dimensional volume dataset that is most parallel to the view direction. The volume dataset is divided along the measured length into a plurality of slabs. Each of the plurality of slabs has an orientation that is perpendicular to the measured length and defines a plane having a position with respect to the viewpoint in three dimensional space. A perspective projection is generated. Each of the plurality of slabs is rendered by parallel projection onto a plurality of separate baseplane images. Each of the plurality of images is textured through the perspective projection onto their respective plane. The plurality of textured images are then blended together to form the final image.","The processing can be performed sequentially for each of the plurality of slabs. Alternatively, the processing can be performed starting with one of the plurality of slabs having the farthest voxel from the viewpoint and ending with one of the plurality of slabs having the nearest voxel, or vice versa. The can be diviced toslabs have an equal thickness. Preferably the slabs are divide so that the position of each respective plane of each of the plurality of slabs is situated at exponentially increasing distances from the viewpoint along measured length. Preferably each successive slab after the slab having the nearest voxel has a thickness along the measured length that is twice the thickness of a preceding adjacent slab.","In another preferred embodiment of the present invention is a method and apparatus for mapping a three dimensional volume dataset in a linear memory array. The volume dataset including a plurality of discrete voxels associated with a three dimensional grid point position P=(x; y; z) and the linear memory array having a plurality of indices O. The apparatus of the invention is an addressing unit configured with the teachings of the method of the invention. The invention can also be practiced on a machine readable medium. The method includes converting integer coordinates (x; y; z) of the grid point position of one of the plurality of discrete voxels into a first bit pattern ( . . . , x, x, x, x; . . . , y, y, y, y; . . . , z, z, z, z). An integer offset for the discrete voxel is determined from a second bit pattern ( . . . , z, y, x, z, y, x, z, y, x, z, y, x). The discrete voxel is mapped onto the linear array at the integer offset. This process is repeated until each of the plurality of voxels are mapped. Preferably the voxels are processed in linear storage order.","Another preferred embodiment of the invention is a block processor for interfacing a ray bus and a plurality of three-dimensional (3D) memory units in a volume processing unit. The volume processing unit generates a plurality of rays for processing a volume dataset having a plurality of discrete voxels stored in a distributed fashion in the plurality of three-dimensional (3D) memory units. Each of the voxels have a location lying on a gridpoint in the volume dataset and have voxel data associated therewith. Each ray has a path and is a data structure having ray data associated therewith. Each ray also has a sample location in the volume dataset with respect to time associated with one or more voxels. The block processor having a circular ray integration pipeline for processing the voxel data and the ray data to represent an exchange of energy between the volume dataset and the ray data along the path of each ray. The plurality of rays are processed simultaneously in a round-robin fashion.","Preferably the block processor includes a queue sorter, at least one ray queue, a prefetch and memory controller, an interleaved volume cache, a resampling unit, a segmentation unit, a compositing unit, a shading unit, a scattering, and a splatting unit. The queue sorter interfaces the plurality of rays over the ray busand separates rays into a plurality of ray queues. The queue sorter assigns a scalar importance to each of the plurality of rays queues for maintaining a sorted rank of importance for each ray queues. The at least one ray queue is connected to the queue sorter and receives the plurality of sorted rays, and controls the processing of each of the plurality of sorted rays. The prefetch and memory controller is connected to the at least one ray queue that has the highest scalar importance for receiving the plurality of sorted rays and prefetches voxels to hide latency. The prefetch and memory controller interfaces the memory units. The interleaved volume cache is connected to the prefetch and memory controller for receiving the plurality of sorted rays and prefetched voxels. The resampling unit is connected to the interleaved volume cache for receiving the plurality of sorted rays and accepting one sample location along a ray and a plurality of voxels that are nearest the sample location. The resampling unit trilinearly interpolates density, tag, gradient and irradiance for the sample location to generate sample data. The segmentation unit is connected to the resampling unit for receiving the plurality of sorted rays and the sample data and classifying the material type including color, opacity, and shading coefficients for the sample data. The compositing unit is connected to the segmentation unit for receiving the plurality of sorted rays and the sample data and updating the opacity and color of the ray based upon the sample data. The shading unit is connected to the compositing unit for receiving the plurality of sorted rays and the sample data. The scattering unit receives the plurality of sorted rays and the sample data and redirects the ray direction based upon the sample data and the material type of the data. The splatting unit is connected to the scattering unit and receives the plurality of sorted rays and the sample data for updating the voxel data and the ray data associated with the sample location. Either the scattering unit or the splatting unit is directly connected to the shading unit and the other is connected to the queue sorter.","Preferably the splatting unit receives the ray data and the voxel data associated with the sample location of the ray and copies the data. The voxel data is updated based upon the copy of the ray data and the ray data is updated based upon the copy of the voxel data. Preferably the shading unit has a reflectance map for performing shading.","A preferred embodiment of the invention is a method and apparatus for scattering at least one ray passing through a three-dimensional (3D) volume dataset. The apparatus of the invention is a scattering unit configured with the teachings of the method of the invention. The volume dataset having a plurality of discrete voxels and an estimated gradient. Each of the voxels having voxel data associated therewith. The at least one ray having a direction, ray data associated therewith, and a sample location in the volume dataset with respect to time associated with at least one voxel. The voxel data associated with the sample location including a reflectivity in a range between 0 and 1, a refractivity in a range between 0 and 1, a glossiness in a range between 0 and 90. The method including the receiving and copying the ray data and voxel data associated with the sample location. A first random number is selected in a range between 0 and 1. The ray direction is reflected about the estimated gradient in the volume dataset at the sample location when the first random number is less than the reflectivity at the sample location. A second random number is selected in a range between 0 and 1. The ray direction is refracted based upon the refractivity of the voxel data associated with the sample location and the estimated gradient in the volume dataset at the sample location when the second random number is less than the refractivity at the sample location. A random direction and a gaussian distributed random angle are selected with the random angle being defined by the glossiness of the voxel data multiplied by a third range between 0 and 1. The ray direction is rotated in the random direction by the random angle based on the glossiness at the sample location.","Another preferred embodiment of the invention is a queue sorter for determining a processing order of a plurality of ray queues for a volume processing system during processing. Each of the plurality of ray queues being assigned a dataset including a queue number and a scalar importance. The queue sorter including a pipelined insertion sorter having a comparison buffer and a selected buffer. The comparison buffer having a first linear storage arrangement for storing at least one dataset of one of said plurality of ray queues. The selected buffer having a second linear storage arrangement for storing the dataset for each of said plurality of ray queues. The pipelined insertion sorter receives a first dataset of one of said plurality of ray queues at a rank of the first storage arrangement of the comparison buffer. The scalar importance of the first dataset is compared with the scalar importance of a second dataset in the selected buffer having the same rank to determine the dataset having the higher scalar importance and the lower scalar importance. The dataset having the higher scalar importance is assigned to the second dataset. The dataset having the lower scalar importance is moved to the first dataset on the first linear storage arrangement at a position located one below the rank of the second dataset. The process is repeated during processing with the scalar importance of the active queue set higher than the scalar importance of the remaining plurality of ray queues and while removing an old dataset from the selected buffer when the first dataset has the same queue number as the old dataset. Preferably the pipelined insertion sorter sorts a plurality of datasets simultaneously.","Another preferred embodiment of the invention is a method for casting shadows of a volume dataset in relation to point light sources located both inside and outside, distant light sources located outside the volume dataset, and area light sources inside the volume dataset. The volume dataset having a plurality of discrete voxels stored in a distributed fashion in a plurality of three-dimensional (3D) memory units. Each of the voxels having a location lying on a gridpoint in the volume dataset and having voxel data associated therewith. The method includes computing sum of the optical path length to all the point light sources for all of the voxels in the volume data set. The sums of the optical path length values are stored in both a radiosity array and an unshot radiosity array. A projection direction and a face of the volume dataset is selected which is most perpendicular to the projection direction. The volume dataset is divided along the projection direction into a plurality of slices which are parallel to the face. The plurality of slices include a first slice having at least one voxel associated with the face. A two dimensional (2D) array of rays is initialized on the selected face with any distant light source energy. Each of the rays has a path parallel to the projection direction and ray data associated therewith. The first slice is assigned as a current slice. The light energy is intergrated and distributed to voxels along each path of each ray within the current slice. The process is repeated by sequentially sweeping along the projection direction through each subsequent slice until each of the plurality of slices is processed with each subsequent slice in turn becoming the current slice.","Preferably the two dimensional array of rays are initialized by making a copy of all the ray data and all voxel data on the current slice with the voxel data including the reflectivity, opacity, radiosity, and unshot radiosity, the ray data representing light energy. A current voxel is selected. An appropriate neighborhood of rays is determined for the current voxel. The ray data at the current voxel is resampled. The ray data is modulated by a factor zeta to distribute the energy. The current voxel data is updated based on the resampled ray data. The data of the neighboring rays is updated based on the current voxel data. The process is repeated for each voxel in the current slice. The ray data is stored back into the neighboring rays and the voxel data is stored back into the volume dataset.","Another preferred embodiment of the invention is a programmable processing element for controlling the storage location of volume data and polygon data. The data is distributed among blocks of a scheduling grid and is stored in a memory hierarchy having a first tier, a second tier, and a third tier. The scheduling grid has a plurality of rays casted there through and stored in ray queues. The programmable processing element includes a dispatcher, a scheduler, and a buffer. The dispatcher controls the volume data and the polygon data movement through the memory hierarchy. The dispatcher is operatively coupled to the first, second and third tiers. The scheduler determines the block processing order based upon the scheduling grid and the plurality of ray queues. The buffer is connected between the dispatcher and the scheduler for facilitating communication between the dispatcher and the scheduler. Preferably the scheduler has a heuristic metric for determining the block processing order.","These and other features and advantages of the present invention will become apparent from the following detailed description of illustrative embodiments thereof, which is to be read in connection with the accompanying drawings.","The apparatus and methods of the present invention are capable of processing data and supporting real-time visualization of high resolution voxel-based data sets. The present invention is a universal three-dimensional (3D) rendering system delivering enhanced volume rendering in addition to the integration of imagery (e.g., volumes, textures and images) with geometry (e.g., polygons). The apparatus and methods are designed for use as a voxel-based system as described in the issued patents and pending applications of Dr. Arie Kaufman, a named inventor in this application, including \u201cMethod of Converting Continuous Three-Dimensional Geometrical Representations Into Discrete Three-Dimensional Voxel-Based Representations Within a Three-Dimensional Voxel-Based System\u201d, which issued on Aug. 6, 1991, as U.S. Pat. No. 5,038,302; \u201cmethod of Converting Continuous Three-Dimensional Geometrical Representations of Polygonal Objects Into Discrete Three-Dimensional Voxel-Based Representations Thereof Within a Three-Dimensional Voxel-Based System\u201d, which issued on Jan. 22, 1991, as U.S. Pat. No. 4,987,554; \u201cMethod and Apparatus for Storing, Accessing, and Processing Voxel-Based Data\u201d, which issued on Jan. 15, 1991, as U.S. Pat. No. 4,985,856; \u201cMethod of Converting Continuous Three-Dimensional Geometrical Representations of Quadratic Objects Into Discrete Three-Dimensional Voxel-Based Representations Thereof Within a Three-Dimensional Voxel-Based System\u201d, which was filed on May 4, 1989, as U.S. Ser. No. 07\/347,593, which was abandoned in favor of U.S. Ser. No. 08\/031,599, filed on Mar. 15, 1993 as a continuation application of the '593 application; \u201cMethod and Apparatus for Generating Arbitrary Projections of Three-Dimensional Voxel-Based Data\u201d, which issued on Mar. 31, 1992 as U.S. Pat. No. 5,101,475; \u201cMethod and Apparatus for Real-Time Volume Rendering From An Arbitrary Viewing Direction\u201d, which was filed on Jul. 26, 1993 as U.S. Ser. No. 08\/097,637; \u201cMethod and Apparatus For Generating Realistic Images Using a Discrete Representation\u201d, which was filed on Mar. 20, 1992 as U.S. Ser. No. 07\/855,223; \u201cApparatus and Method for Parallel and Perspective Real-Time Volume Visualization\u201d, which issued on Dec. 8, 1998 as U.S. Pat. No. 5,847,711; and Apparatus and Method for Real-Time Volume Processing and Universal 3D Rendering which was filed on Jul. 16, 1999 as U.S. Ser. No. 09\/354,876. The entire disclosure of each of these references is incorporated herein by reference.",{"@attributes":{"id":"p-0125","num":"0124"},"figref":"FIG. 2","b":["10","12","14","16","18","16","18","16","18","20","22"]},{"@attributes":{"id":"p-0126","num":"0125"},"figref":["FIG. 3","FIG. 3"],"b":["24","24","26","28","28","34","10","30","32","26","34"]},"Referring now to , the apparatus of the present invention  includes a plurality of 3D memory units  which are preferably connected to an imagery input bus , providing global horizontal communication between the 3D memory units . The volume dataset is commonly represented as a regular grid of volume elements, or voxels, often stored as a full 3D raster (i.e., volume buffer). This volume dataset is preferably distributed across the 3D memory units . With a skewed distribution, the present invention allows conflict-free access to complete beams (i.e., rows) of voxels parallel to any of the major axes, thereby reducing the memory-processor bandwidth bottleneck. As illustrated in , for streaming video or four-dimensional (4D) volume data through the system , each 3D memory unit  is preferably connected to a dedicated real-time input . By providing a dedicated connection to a real-time input source, the memory-processor bandwidth bottleneck is further reduced.","The universal 3D rendering system  of the present invention further includes a plurality of rendering pipelines, shown as functional blocks of Cube-5 units  in . Each rendering pipeline  is connected to a corresponding 3D memory unit  and preferably has horizontal communication with at least preferably its two nearest neighbors. The Cube-5 units  read from their dedicated 3D memory units  and produce a two-dimensional (2D) baseplane image. This baseplane image, which contains a plurality of composited pixels generated by the Cube-5 units , is preferably distributed across a plurality of two-dimensional (2D) memory units . Each of the plurality of 2D memory units  is preferably connected to both a corresponding Cube-5 pipeline unit  and a baseplane pixel bus  which provides global horizontal communication between 2D memory units .","Preferably, the present invention includes a plurality of warp units  connected to the baseplane pixel bus . The warp units  assemble and transform (i.e., warp) the baseplane image stored in the plurality of 2D memory units  onto a user-defined image plane. Although the present invention contemplates using a single warp unit  (e.g., in order to reduce the costs or overhead of the hardware), the use of a plurality of warp units  is desirable to accelerate image transformations.","The output of each of the warp units  is preferably connected to a frame buffer pixel bus  which provides global horizontal communication between warp units . Reading the source pixels over the baseplane pixel bus  and writing the final image pixels over the frame buffer pixel bus  preferably happens concurrently in order to allow greater system throughput. Although not a preferred architecture, the present invention also contemplates sequential reading and writing by the warp units . In this manner, only one pixel bus may be required, assuming the one pixel bus offers sufficient bandwidth for real-time image transfer for a full screen image.","With continued reference to , the present invention preferably includes a geometry input bus  and a geometry output bus , although it is contemplated to combine the two busses into a single geometry input\/output bus of sufficient bandwidth for real-time imaging. The geometry input and output busses  and  are preferably connected to the inputs and outputs of the Cube-5 units  respectively and provide for the unique coupling of at least one geometry pipeline or engine (not shown) to the present system . The architecture of the present invention, coupled with a geometry engine via the geometry busses  and , supports the integration of imagery, such as volumes and textures, with geometries, such as polygons and surfaces. This mixing of geometric data with volumetric objects is a powerful feature which is unique to the present invention.","Referring now to , there is illustrated a block diagram depicting the functional stages of one of the plurality of Cube-5 rendering pipelines (reference number  in ), formed in accordance with one embodiment of the present invention. As shown in , each rendering pipeline  preferably includes four types of processing units: a trilinear interpolation unit (TriLin) , a gradient estimation unit (Gradient) , a shading unit (Shader)  and a compositing unit (Compos) . Each of these rendering pipeline stages is described in detail in the prior issued patents and pending applications of Arie Kaufman relating to prior Cube volume visualization architectures (listed above) and are therefore only briefly discussed herein below.","As discussed above in reference to the 3D memory units , the volume dataset is stored as a regular grid of voxels distributed across the 3D memory units  in a skewed fashion, with each Cube-5 unit  connected to a corresponding 3D memory unit  (see ). Voxels of the same skewed beam are preferably fetched and processed in parallel, distributed across all Cube-5 units . Consecutive slices of the volume dataset parallel to a predefined baseplane (i.e., parallel to a face of the volume dataset which is most perpendicular to a predefined view direction) are preferably traversed in scanline order. Referring again to , an address generation and control unit  preferably generates the addresses for access into the 3D memory unit . The address generation and control unit  additionally designates a first processing slice as the current processing slice and controls sweeping through subsequent slices of the volume dataset until the final slice has been processed.","The trilinear interpolation unit  computes a new slice of interpolated sample values between two processing slices. It is contemplated by the present invention that the trilinear interpolation function may alternatively be performed as a sequence of linear or bilinear interpolations.","The gradient estimation unit  preferably computes central difference gradients using volume data from multiple slices of the volume dataset. Utilizing the central difference gradients generated by the gradient estimation unit , sample points of the current processing slice are subsequently shaded by the shading unit . The shading unit  preferably uses the samples and gradients as indices into one or more look-up tables (LUTs), preferably residing in each shading unit , which store material color and intensity information. The material color table is dataset-type dependent, while the color intensity table is based on a local illumination model, as known by those skilled in the art. In simple terms, the multiplication of color and intensity yields a pixel color for each sample which is used in the compositing unit  to composite such color with the previously accumulated pixels along each sight ray.","With reference again to , data for computing the next sample along a continuous sight ray may reside on a neighboring Cube-5 unit . In this case, the nearest-neighbor connections between Cube-5 units  are preferably used to transfer the necessary data to the appropriate Cube-5 unit , which will continue to process that particular sight ray. When compositing has been completed, the composited pixels (i.e., baseplane pixels) are preferably stored in the corresponding 2D memory unit  connected to the Cube-5 unit pipeline . The baseplane pixels, which form the baseplane image, are subsequently read from the 2D memory units , via the baseplane pixel bus , and assembled by the warp units . The warp units  additionally transform the baseplane image to the final projection plane image.","Referring to , the delay of data required for the trilinear interpolation unit  and gradient estimation unit  is preferably achieved by inserting one or more first-in-first-out (FIFO) units  into the pipeline data path prior to being processed by the trilinear interpolation  and the gradient estimation  units. The FIFO unit(s)  may be implemented as, for example, random access memory (RAM), preferably embedded on the Cube-5 chip. The introduction of a predetermined delay may be particularly important when simultaneously processing beams of multiple slices of the volume dataset, thereby requiring more computation time between slices.","A compositing buffer (Compos Buffer)  operatively coupled to a bilinear interpolation unit (BiLin)  essentially provides a one slice FIFO. The bilinear interpolation unit  preferably interpolates to obtain values between voxels as needed for texture mapping. For volume rendering, BiLin  preferably uses only weights of 0.0 or 1.0 which selects one of the corner voxels of the volume dataset (determined by Select x and Select y). It just moves the ray data, if the ray crosses pipelines. Just a mux [?] for x and y would be enough for volume rendering, but bilinear interpolation is preferred because of texture mapping.","The Cube-5 architecture preferably supports re-ordering of the pipeline stages and a number of multipass rendering and processing operations, which require feedback connections between various stages of the Cube-5 rendering pipelines  and the 3D memory units . For example, correct rendering of overlapping volumetric objects preferably requires at least two passes through the Cube-5 pipeline , where the first pass re-samples the volumetric objects to align them with each other and the second pass renders the volumetric objects in interleaved order. As shown in , a multiple volumes feedback path  is preferably provided, operatively connecting the output of the compositing unit  to the corresponding 3D memory unit , which allows the re-sampled volumes to be written back into the 3D memory unit  after re-sampling, classification and shading. The final rendering pass works on RGB\u03b1 volumes.","Similarly, each Cube-5 rendering pipeline  preferably includes an image-based rendering feedback path  connected between the warp unit  and the 3D memory unit . The image-based rendering feedback line  preferably provides a feedback path for writing the intermediate warped images to the 3D memory unit . This may be particularly useful for accelerating certain image-based rendering operations requiring multiple warp passes. The architecture of the present invention further contemplates feedback connections between the 3D memory unit  and various other Cube-5 rendering pipeline stages, or between the individual pipeline stages themselves. Image rendering speed may be substantially increased by including feedback paths which provide direct and immediate access to the computational results of individual pipeline stages, without having to wait for the results to traverse through the entire Cube-5 rendering pipeline .","In a preferred embodiment of the present invention, the Cube-5 system includes connections which bypass selective stages of the rendering pipeline, that, for example, may not be required for certain imaging operations. By passing these unused pipeline stages, such imaging operations can be accelerated. As illustrated in , a texture map bypass  is preferably included in each Cube-5 rendering pipeline . This texture map bypass connection  substantially speeds up mip-mapping, for instance, which consists of storing multiple levels-of-detail (LOD) of the image to be processed, by passing the shading unit  and compositing unit  and directly presenting the results from the trilinear interpolation unit  and gradient estimation unit  to the bilinear interpolation unit . In this way, the architecture of the present invention can preferably be considered not only as an array of pipelines for performing volume rendering, but as a collection of hardware resources which can be selectively configured to perform a variety of imaging operations. For example, when the Cube-5 system of the present invention is performing volume rendering, essentially all of the hardware resources are required, while texture mapping generally requires only memory, some buffering and the interpolation units.","Another unique and important aspect of the present invention which will now be discussed is the ability of the Cube-5 architecture to preferably interface with at least one conventional geometry engine  to support mixing of geometric data and volumetric objects in a single image. This is preferably accomplished by providing at least one geometry bus, as discussed above, to interface with the geometry engine .","Preferably, the Cube-5 architecture of the present invention is adapted to re-use pipeline components (e.g., interpolation unit, etc.), wherever possible, to accelerate a variety of rendering algorithms using multiple configurations, in particular, rendering scenes of multiple volumetric and polygonal objects, texture mapping, and image-based rendering. Among other important advantages, reusing pipeline components reduces hardware costs. The Cube-5 architecture also supports various unique methods and algorithms for enhancing volume rendering and acceleration of other imaging operations. Some of these methods and algorithms will be discussed individually in greater detail below.","In a preferred embodiment of the Cube-5 system, formed in accordance with the present invention, volume datasets are stored in blocks, thereby taking advantage of spatial locality. Instead of linear blocking (e.g., Voxelator API), hierarchical blocks are used which are preferably stored in a distributed arrangement, skewed across multiple 3D memory units. For example, using current Mitsubishi Electric 16-bit, 125 megahertz synchronous dynamic random access memory (SDRAM) to implement the 3D memory, each block can contain 816-bit voxels requiring 1024 bytes or two SDRAM pages.","Each block is preferably organized as a collection of 2-voxel miniblocks residing in the same 3D memory unit. The banks inside the SDRAM can preferably be accessed in a pipelined fashion such that the current burst transfer essentially completely hides the setup of the subsequent burst transfer. If the view-dependent processing order of the voxels in a miniblock does not coincide with their storage order, then the eight miniblock voxels are preferably reordered on the Cube-5 chip. Hence, a single copy of the volume dataset on the SDRAM is sufficient. Therefore, hierarchical blocking allows random access to miniblocks at essentially full burst mode speed, essentially full (100%) bandwidth utilization, view-independent data storage and balanced workload.","Blocking not only optimizes the memory interface, but has an additional advantage of reducing the inter-chip communication bandwidth (i.e., between Cube-5 hardware units), since only the voxels on the block perimeters need to be exchanged between neighboring chips processing neighboring blocks. While processing a b-voxel block in O(b) time, only the O(b) voxels on the block boundary need to be communicated between chips processing neighboring blocks, where b is the size of a block edge and each block has b\u00d7b\u00d7b (i.e., b) voxels. Therefore, inter-chip communication needs O(1\/b) less bandwidth than with a non-blocking solution. The size of the block edge b can be in the range of about 4\u2266b\u226664, although a block edge size of eight (8) is preferred.","Block look-up tables (LUT) are preferably utilized to store the pointers to all blocks comprising the current volume. This approach provides an easy method to restrict the active volume while zooming into a selected region of interest of a large volume. It also allows rendering of arbitrarily shaped sub-volumes (at block-sized granularity). Additionally, scenes containing many small volumes can be rendered very efficiently, as all volumes can reside anywhere among the 3D memory units, and only the look-up tables must be reloaded for each volume, rather than the 3D memory units.","One method of performing perspective projection and\/or Level-of-Detail (LOD) relies on two-fold super-sampling in the x and y directions. Accordingly, a four-times (4\u00d7) replication of the interpolation units for trilinear interpolation, as well as the gradient estimation units for gradient computation, is preferably employed. As a result, the datapath between the SDRAM and the Cube-5 pipelines is essentially unchanged. However, the bandwidth between Cube-5 pipelines is quadrupled, as is the on-chip throughput and buffers, primarily because each sample of the normal mode is replaced by up to four samples (i.e., 2\u00d7 in the x direction and 2\u00d7 in the y direction).","Handling anisotropic datasets and super-sampling preferably require a modification of opacity \u03b1. The combined function is \u03b1\u2032=1\u2212(1\u2212\u03b1), with super-sampling factor k representing the number of samples per voxel cell, and d representing the distance which a sight ray travels (i.e., the length of the path of the sight ray) through each voxel cell. Preferably, a look-up table (LUT) is employed, for fast look-up of \u03b1\u2032 during rendering.","With continued reference to , the perspective rendering of volumetric data with close to uniform sampling of the underlying volume dataset requires re-scaling of the compositing buffer  with filtering between levels. Level-of-detail (LOD) perspective rendering requires re-alignment of the compositing buffer  between levels. Both of these processes, which incorporate global communication not available in the pipelines , are preferably performed by the warp unit(s) . Although the compositing buffer  is already accessible to the warps units , it is preferred that a feedback line  be used to write the filtered values back into the compositing buffer .","A hardware warp unit is generally necessary to obtain final fall screen images in real time (i.e., a 30 Hertz frame rate). As shown in , the baseplane image, generated by the compositing units  of the Cube-5 rendering pipelines , is preferably buffered in the 2D memory units . To lower the memory bandwidth from the 2D memory units  to the warp unit , each pixel of the baseplane image is preferably accessed only once. To perform a linear interpolation between samples of the current and the previous scanline, another FIFO unit, sized to hold at least one scanline, is required to store the previous scanline samples. The interpolation weights for each grid pixel are preferably pre-calculated on a host machine.","In order to perform the accurate mixing of volumes and geometry, for opaque geometric objects, the Z-buffer image is preferably written to the compositing buffer . The compositing unit  must perform a z-comparison prior to blending each new sample. Additionally, for translucent geometric slices, the geometry engine  preferably utilizes the geometry input bus (reference number  in ) of the present invention to insert each slab of RGB\u03b1 values into the data stream so that each slab is interleaved with the volumetric data slices.","For texture mapping,  shows, by way of example, how 32 bits of texel data are preferably stored for a 2\u00d72 neighborhood in a miniblock of 16-bit voxels in the 3D memory unit, in accordance with the present invention. Therefore, a four-texel neighborhood of 32-bit texels is preferably read during each memory burst read. Without data duplication, the Cube-5 system preferably performs, on average, 2.25 data burst reads to access the appropriate texel neighborhood, since some texture coordinates may lie between stored miniblocks.","With reference again to , in accordance with one form of the present invention, one way to implement image-based rendering in hardware is to utilize the memory control unit , preferably included in each Cube-5 pipeline , to read the appropriate source pixels based on the contributing region for each pipeline. The interpolation units (e.g.,  and ) in that pipeline  will then preferably perform the four-dimensional (4D) interpolations needed for light field rendering or lumigraph. As an alternative implementation, the warp unit  may be utilized to perform this function. The source pixels contributing to the current view are read and assembled into the 2D memory units , preferably through a connection line , followed by the warp transformation. Preferably, four assembled source images are processed in four consecutive warp passes. The final combination of the four intermediate warped images is performed in the Cube-5 pipeline . As described previously above, the image-based rendering feedback line  provides feedback for writing the intermediate warped images to the 3D memory . For either approach, the 3D memory units  provide local storage for a large database of images.","It is to be appreciated that the apparatus of the present invention described herein above (and referred to as Cube-5) may considerably accelerate conventional volume processing methods, beyond the universal rendering already described. Additionally, the Cube-5 apparatus of the present invention may be used in conjunction with a number of unique algorithms adapted for enhancing the performance of and\/or providing enhanced features for real-time volume processing, therefore making the overall Cube-5 system superior to existing volume rendering architectures, such as Cube-4. Some of these unique algorithms, including those for performing image warping, three-dimensional transformations, perspective projections, handling large volumes, high quality rendering, clipping, depth cueing, super-sampling and anisotropic datasets, are discussed in detail below.","In accordance with one form of the present invention, a method for performing image warping is presented which, among other advantages, speeds perspective warping and provides improved image quality. Image warping is preferably the final stage of the Cube-5 volume rendering pipeline. In simple terms, image warping primarily relates to the geometric transformation between two images, namely, a source image and a target image. The geometric transformation defines the relationship between source pixels and target pixels. Efficiency and high quality are equally critical issues in such applications. In the apparatus of the present invention, the warp unit preferably performs the image transformation function. Consequently, applications employing a warp unit benefit from the image warping method of the present invention.","Distinguished by the data flow of the transformation, image warping methods are generally classified as either forward warping or backward warping. In forward warping, the source pixels are processed in scanline order and the results are projected onto the target image. In backward warping, the target pixels in raster order are inversely mapped to the source image and sampled accordingly. Most known prior art warping algorithms employ backward warping.","Compared with affine transformations (i.e., translation, rotation, scaling, shearing, etc.), a perspective transformation is considered to be more expensive and challenging. For perspective projection, an expensive division is needed when calculating the sample location in the baseplane image for a pixel in the projection plane. Conventional perspective warping is typically at least three-fold slower than parallel warping, when implemented by a CPU. Accordingly, some prior art approaches have decomposed the perspective transformation into several simpler transformations requiring multiple passes. One primary problem inherent in multi-pass transformation algorithms, however, is that the combination of two one-dimensional (1D) filtering operations is not as flexible as true two-dimensional (2D) filtering. Furthermore, conventional multi-pass approaches introduce additional filtering operations which degrade image quality.","The present invention preferably employs a unique single-pass forward warping method which can be implemented with substantially the same efficiency as affine transformations. Costly divisions, which were traditionally performed for every pixel, are reduced to only twice per scanline according to the present invention. Thus, by reducing the number of division operations, the present invention provides an alternative perspective warping method which is superior to known prior art methods, at least, for example, in terms of speed and the efficient hardware implementation. A preferred method for perspective warping, in accordance with the present invention, will now be discussed.","Preferably, the present invention uses a scanline approach to perform perspective warping. Rather than scanning in normal raster scanline order, however, the algorithm of the present invention is processed in a special scanline direction in the source image. As illustrated in , this special scanline direction  () preferably has the property that parallel scanlines  in the source image  appear as parallel scanlines  in the target image , and that equi-distant sample points  along a source scanline  remain as equi-distant sample points  in the target scanline . Some advantages of this unique approach include a reduced complexity of perspective-correct image warping (i.e., by eliminating the division per pixel and replacing it with two divisions per scanline), accurate antialiasing by incorporating anisotropic filtering, correction of flaws in Gouraud shading caused by bilinear interpolation and optimization of the memory bandwidth by reading each source pixel exactly once.","The intuition of the special scanline direction is derived from projection geometry, as shown in . Referring to , the source image  is preferably placed on a three-dimensional (3D) surface and the target image  is placed on a screen. As in typical texture mapping, to obtain the pixel on screen, a sight ray (or rays)  is cast from a viewpoint (or eye point)  to 3D space and intersected with the screen  and 3D surface . The intersection points are the sample points . When the scan direction  in screen space is parallel to the 3D planar surface, the scanlines in both images are parallel to each other, and equi-distant sample points  along the scanline remain equi-distant in the 3D surface plane. This parallel-preserving (PP) scanline direction exists and is unique for a given perspective transformation. It is to be appreciated that for parallel projections, any scan direction preserves this parallelism on both images, and thus a raster scanline direction may be preferably used due to its simplicity.","Referring again to , parallel-preserving (PP) scanlines  and  are shown in both the source  and target  images respectively. Once the parallelism property is achieved, pixel access becomes regular, and spatial coherency can be utilized in both images. Additionally, the PP scanline enables the application of a pure incremental algorithm without division to each scanline for calculating the projection of source samples . One division is still needed, however, for the two endpoints of every scanline due to the non-linear projection.","With continued reference to , as the source image  is scanned in the PP scanline direction rather than the raster direction, sample points  on the target scanline  may not necessarily coincide with the target pixels . However, the sample points  can be aligned on the x grid lines  of the target image , thus the sample points  are only off the y grid lines  (they are equi-distant along the scanline). For a more efficient but lower quality implementation, placing the sample value in the nearest-neighbor target pixel is a reasonable approximation, as a half pixel is the maximum error. However, when higher quality is preferred, the present invention may perform pixel correction and effective antialiasing, to be described herein below.","In general, a reduction in the number of divisions from O(n) to O(n) is obtained by the algorithm of the present invention (where n is the linear resolution). For the present algorithm, preferably only two additions are needed to calculate each sample point, while conventional raster scanline algorithms generally require three additions, one division and two multiplications per pixel. A preferred method for performing forward image warping, in accordance with the present invention, is described in detail herein below.","The forward warping algorithm of the present invention is preferably performed in two stages: (1) calculating the special parallel-preserving (PP) scanline direction, and (2) forward mapping the source image to the target image along the special PP scanlines, incrementally within each scanline.","As discussed briefly above, the parallel-preserving (PP) scanline is the intersection line between the three-dimensional (3D) planar surface and the screen (i.e., target image). However, in a two-dimensional (2D) problem, the PP scanline must be calculated based on a 2D matrix. Generally, a perspective transformation can be presented as",{"@attributes":{"id":"p-0167","num":"0166"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"u"}},{"mtd":{"mi":"v"}},{"mtd":{"mn":"1"}}]}},{"mrow":[{"mi":"M","mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"x"}},{"mtd":{"mi":"y"}},{"mtd":{"mi":"z"}}]}}},{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mi":"a"},{"mi":"d"},{"mi":"g"}]},{"mtd":[{"mi":"b"},{"mi":"e"},{"mi":"h"}]},{"mtd":[{"mi":"c"},{"mi":"f"},{"mn":"1"}]}]}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"x"}},{"mtd":{"mi":"y"}},{"mtd":{"mn":"1"}}]}}],"mo":"\u2061"}],"mo":"="}],"mo":"="}}},"br":{}},{"@attributes":{"id":"p-0168","num":"0167"},"maths":[{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["(",")"],"mrow":{"mi":["u","v"],"mo":","}},{"mrow":[{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},{"mi":"C","mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":["ax","dy","g"],"mo":["+","+"]}}},{"mtd":{"mrow":{"mi":["bx","ey","h"],"mo":["+","+"]}}}]}}}],"mo":"="}],"mo":"="}}},{"@attributes":{"id":"MATH-US-00002-2","num":"00002.2"},"math":{"@attributes":{"overflow":"scroll"},"mi":"where"}},{"@attributes":{"id":"MATH-US-00002-3","num":"00002.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"C","mo":"=","mfrac":{"mn":"1","mrow":{"mo":["(",")"],"mrow":{"mi":["cx","fy"],"mo":["+","+"],"mn":"1"}}}}}}]},"A line in the target image can be expressed as y=kx+B, where slope k denotes a line direction and B denotes a line intercept. To calculate slope k for the PP scanline, two parallel lines are preferably defined having identical slope k and intercepts B of 0 and 1, represented by point pairs of (0,0), (1, k) and (0,1), (1, k+1), respectively. The coordinates of these points in the source image are then calculated. Since perspective transformation preserves straight lines, these two lines will remain as straight lines in the source image and their slopes can be calculated from two point pairs. Assuming that the slopes of the two mapped lines are essentially equal, an equation in k is preferably obtained. Solving this equation for k results in",{"@attributes":{"id":"p-0170","num":"0169"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"k","mo":"=","mrow":{"mo":"-","mfrac":{"mi":["c","f"]}}}}},"br":{}},{"@attributes":{"id":"p-0171","num":"0170"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msup":{"mi":["k","\u2032"]},"mo":"=","mfrac":{"mrow":[{"mi":["bf","ec"],"mo":"-"},{"mi":["af","dc"],"mo":"-"}]}}}},"br":{}},{"@attributes":{"id":"p-0172","num":"0171"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"msup":{"mi":["k","\u2032"]},"mo":"=","mrow":{"mo":"-","mfrac":{"mi":["c","f"]}}},"mo":","}}},"br":{}},"The second stage of the preferred forward warping method of the present invention involves scanline processing and is illustrated in  by way of example. Referring now to , the preferred algorithm sweeps the scanlines  (e.g., scanlines S-S) through the source image . As discussed above, the scanlines  have the slope k\u2032. The samples  along each scanline  are preferably incrementally calculated. First, for each scanline , the projection of the endpoints from the target image onto the source image is calculated. Then, based on the number of sample points on the scanline, increments are calculated in both the x and the y directions.","Considering a traditional bilinear interpolation of samples in the source image, every sample essentially requires the contribution of four surrounding source pixels. If pixels are read every time for every sample, each pixel ought to be read four times. This leads to a memory bandwidth of four times the target image size. However, since all scanlines are in parallel, samples on neighboring scanlines usually share contributing source pixels. Consequently, in accordance with the method of the present invention, pixels that have been previously read are preferably buffered so that common pixels are read from the buffer rather than from the source image itself.","With reference to , pixels are preferably read in a fixed pattern, called the pixel read template , calculated based on the Bresenham line algorithm (as appreciated by those skilled in the art). The binary digits shown at the bottom of  represent one way of encoding the read template . The present invention, however, contemplates other encoding schemes, as appreciated by those skilled in the art. As illustrated in , this code indicates the increase in the positive v direction; a \u201c0\u201d represents no increase and a \u201c1\u201d denotes an increase by one unit, while u is always increased by one unit. For the example of , the u axis may preferably be referred to as the primary processing axis. It is preferred that the template  always start from the left-most pixel and moves in the vertical direction (i.e., increasing v direction) so that all pixels are read and placed into the buffer for subsequent use in the sampling. It can be seen from  that in order to provide pixels for sampling on any scanline between the two dotted lines, four pixel templates are preferably required, even though for a specific scanline, only three pixel templates might seem sufficient (e.g., only templates ,  and  are necessary to process the current scanline ). Therefore, the buffer size is preferably four scanlines.","Referring now to , there is illustrated the addressing of samples in the buffer. Whenever the template code value is 1, the sample decreases by one unit in the v direction. The thick zigzag line  represents the output scanline in the buffer. When the sample falls within the shaded region , in which the pixels in the buffer are sheared, care should be taken to read the correct pixels for sampling.  illustrates a preferred procedure for bilinearly interpolating one of the samples, s, in this region.","The contents of the buffer are preferably updated based on the scanline position. For example, referring to , templates , ,  and  are preferably in the buffer when processing scanline S. For scanline S, the buffer preferably remains the same. For scanline S, template  is preferably read into the buffer and template  is discarded. For scanline S, template  preferably replaces template , and so on.","As mentioned above, one of the features of the unique forward image warping method of the present invention is the correction of flaws in Gouraud shading. Gouraud shading is a popular intensity interpolation algorithm used to shade the surfaces of geometric objects. Given color only at the vertices, Gouraud shading bilinearly interpolates the intensities for the entire rasterization of a geometry in a raster scanline order. The flaws of the Gouraud shading approach are known in the art and have been the subject of such articles as, for example, , by G. Wolberg, 1990.","One of the problems associated with the Gouraud approach is that diagonal lines (as an example) are not linearly mapped for perspective projections. When a diagonal line is perspectively projected onto a screen in 3D screen space, Gouraud shading converts this diagonal line into a curve, which violates the property of preserving lines in perspective transformation.","The image warping method of the present invention corrects the perspective distortion in Gouraud shading. The perspective distortion is present because the linear interpolation along a raster in screen space is generally non-linear when transformed into geometrical coordinates. Using the special scan direction of the present invention, however, linearity is preserved by the mapping. Thus, interpolation is linear in both image and geometrical space, thereby fixing the distortion of Gouraud shading. It is to be appreciated that interpolation along the edges is still non-linear, and therefore the scanline endpoints must be transformed into geometrical space for correct interpolation.","The forward mapping algorithm of the present invention, with nearest-neighbor approximation, preferably generates a target image that is essentially indistinguishable from an image generated using traditional methods. However, when a higher image quality is desired, the method of the present invention can preferably calculate the pixel value at exact grid points. A simple target pixel correction scheme may preferably be introduced to perform this correction.","With reference now to , assuming the sample points  in the target image  are aligned on integer x coordinates, in order to obtain the pixel value at the exact pixel grid locations , a linear interpolation of the two samples immediately above and below each pixel is preferably performed. Performing this linear interpolation simply as a second pass may increase the cost, since the samples must be read over again. Instead, as each sample is generated, a preferred method of the present invention spreads the contribution of each sample to the corresponding upper and lower pixels with no intermediate buffering.","As illustrated by the example of , samples  located on the thicker inclined scanline  contribute to the shaded pixels neighboring them (lighter shading above the scanline, darker shading below the scanline). The arrows indicate that each sample  preferably contributes to two pixels. It is preferred that a pixel not be written out until both contributions are collected. Thus, a one scanline buffer is preferably included for storing the intermediate pixel values.","To write out pixels correctly and efficiently, a pixel write pattern, called a pixel write template , is preferably pre-calculated. Unlike the pixel read template (e.g., reference number  in ), the pixel write template  is preferably calculated by truncating the y coordinate value of samples along a scanline. The template  is preferably encoded as a series of integer y steps and fractional distances dy from the true scanline . The weights used for the final linear interpolation are dy and 1\u2212dy for the upper and lower pixels, respectively. Since all scanlines are preferably one unit apart in the vertical direction (i.e., y direction), the template is calculated only once per projection.","The forward image warping method of the present invention can further improve on image quality by antialiasing. Using the parallel-preserving (PP) scanline, a higher quality, less expensive method of antialiasing may be achieved.","Referring again to , the sample points on the upper scanlines of the source image are sparser than on the lower scanlines, resulting in a transition from under-sampling to normal sampling. Thus, an appropriate resampling filter may preferably be used to avoid aliasing on the upper scanlines. Isotropic filtering results in clearly incorrect and blurry images. The need for anisotropic filters has been addressed in such articles as , by P. S. Heckbert, 6(11):56-67, November 1986, and more recently in , by A. Schilling, et al., 16(3):32-41, May 1996.","It is known by those skilled in the art that each filter is defined by its footprint and profile. Taking a target sample as a circle, its projection in the source image is its footprint. As illustrated in , this footprint  should generally be neither circular (i.e., isotropic) nor square-shaped (i.e., as in mip-mapping), but conic in shape. The profile of the filter decides the weights of the contributing pixels within the footprint. Although a sinc filter is optimal, a gaussian filter is easier to implement and is preferred because of its finite footprint and good low-pass characteristics. The perspective warping algorithm of the present invention offers more accuracy in calculating the anisotropic footprint, producing higher image quality at a lower cost.","Using conventional methods for calculating the anisotropic footprint, the main axes of the ellipse must be calculated for every pixel. Although approximations have been proposed, this remains an expensive computation, and no known incremental method is available. To obtain the major axes of the ellipse using these prior art methods, the Jacobian must be calculated. Using the image warping method of the present invention, however, calculation of the Jacobian may be eliminated.","In order to gain insight into a preferred method for calculating the anisotropic footprint in accordance with the present invention, the properties of the Jacobian will first be analyzed. The generalized backward mapping from an xy target image into a uv source image was previously defined above as",{"@attributes":{"id":"p-0190","num":"0189"},"maths":[{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mi":"u"}},{"mtd":{"mi":"v"}}]}},{"mrow":[{"mi":"F","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","y"],"mo":","}}},{"mi":"C","mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":["ax","dy","g"],"mo":["+","+"]}}},{"mtd":{"mrow":{"mi":["bx","ey","h"],"mo":["+","+"]}}}]}}}],"mo":"="}],"mo":"="}}},{"@attributes":{"id":"MATH-US-00006-2","num":"00006.2"},"math":{"@attributes":{"overflow":"scroll"},"mi":"where"}},{"@attributes":{"id":"MATH-US-00006-3","num":"00006.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"C","mo":"=","mfrac":{"mn":"1","mrow":{"mo":["(",")"],"mrow":{"mi":["cx","fy"],"mo":["+","+"],"mn":"1"}}}}}}],"br":{}},{"@attributes":{"id":"p-0191","num":"0190"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"J","mo":"=","mrow":{"msup":{"mi":"C","mn":"2"},"mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":{"mi":"y","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["af","cd"],"mo":"-"}}},"mo":["+","-"],"mi":["a","gc"]}},{"mrow":{"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["af","cd"],"mo":"-"}}},"mo":["-","+"],"mi":["d","gf"]}}]},{"mtd":[{"mrow":{"mrow":{"mi":"y","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["bf","ce"],"mo":"-"}}},"mo":["+","-"],"mi":["b","hc"]}},{"mrow":{"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["bf","ce"],"mo":"-"}}},"mo":["-","+"],"mi":["e","hf"]}}]}]}}}}}}},"In conventional antialiasing approaches, the Jacobian is used to determine the footprint of each pixel in the source image and is necessary for anisotropic filtering. The differences between screen pixels in xy raster space are projected into the source image by computing the directional derivatives in the [1,0] and [0,1] directions. These derivatives in source image space are called r and r, and are defined as",{"@attributes":{"id":"p-0193","num":"0192"},"maths":[{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"r","mn":"1"},"mo":"=","mrow":{"mrow":[{"mi":"J","mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"1"}},{"mtd":{"mn":"0"}}]}}},{"msup":{"mi":"C","mn":"2"},"mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mi":"y","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["af","cd"],"mo":"-"}}},"mo":["+","-"],"mi":["a","gc"]}}},{"mtd":{"mrow":{"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["bf","ce"],"mo":"-"}}},"mo":["+","-"],"mi":["b","hc"]}}}]}}}],"mo":"="}}}},{"@attributes":{"id":"MATH-US-00008-2","num":"00008.2"},"math":{"@attributes":{"overflow":"scroll"},"mi":"and"}},{"@attributes":{"id":"MATH-US-00008-3","num":"00008.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"r","mn":"2"},"mo":"=","mrow":{"mrow":[{"mi":"J","mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"0"}},{"mtd":{"mn":"1"}}]}}},{"msup":{"mi":"C","mn":"2"},"mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["af","cd"],"mo":"-"}}},"mo":["-","+"],"mi":["d","gf"]}}},{"mtd":{"mrow":{"mrow":{"mi":"x","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["bf","ce"],"mo":"-"}}},"mo":["-","+"],"mi":["e","hf"]}}}]}}}],"mo":"="}}}}]},"These vectors, rand r, define the bounding box of an ellipse that approximates the footprint . Typically, these vectors  and  are calculated for every pixel, when needed, for conventional methods of anisotropic filtering (e.g., elliptical weighted average (EWA), footprint assembly). This requires one more division per pixel for calculating C. In accordance with the present invention, a more accurate method for determining the footprint is presented, as described herein below.","Because the Jacobian is a linear approximation of the non-linear mapping, it is more accurate, and therefore preferable, to compute the footprint by taking the distances to neighboring samples in source image space. Since the projections of neighboring samples are already computed, this method of the present invention requires no additional division.","The parallel-preserving (PP) scan direction provides for greater coherency and no division to compute the Jacobian. For each pixel in the PP scanning order, the footprint is preferably defined by r\u2032 and r\u2032. The directional derivative r\u2032 in direction [1, k] along the PP scanline is",{"@attributes":{"id":"p-0197","num":"0196"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"r","msup":{"mn":"1","mi":"\u2032"}},"mo":"=","mrow":{"mrow":[{"msub":{"mo":"\u2207","mrow":{"mo":["[","]"],"mrow":{"mn":"1","mo":",","mi":"k"}}},"mo":"\u2062","mi":"F"},{"mrow":[{"mi":"J","mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mn":"1"}},{"mtd":{"mi":"k"}}]}}},{"msup":{"mi":"C","mn":"2"},"mo":"\u2061","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":{"mrow":{"mi":["af","cd"],"mo":"-"}}},{"mtd":{"mrow":{"mi":["bf","ce"],"mo":"-"}}}]}}}],"mo":"="}],"mo":"="}}}},"br":{}},{"@attributes":{"id":"p-0198","num":"0197"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"C","mo":"=","mfrac":{"mn":"1","mrow":{"mo":["(",")"],"mrow":{"mi":"Bf","mo":"+","mn":"1"}}}}}},"br":[{},{},{}],"sub":["1","2","2","[0,1]","2","2"],"in-line-formulae":[{},{}]},"After efficiently computing all the footprint and source pixel coordinate information, correct anisotropic filtering can be performed using a standard method known by those skilled in the art, such as, for example, Greene and Heckbert's elliptical weighted average (EWA) or Shilling et al.'s footprint assembly. These conventional algorithms are described, for example, in the text , by N. Greene and P. S. Heckbert, 6(6):21-27, June 1986. However, these conventional filtering approaches are not preferred since, as pointed out previously, even the elliptical footprint approximation is inaccurate. Furthermore, such prior art methods result in redundant sampling (i.e., accessing each source pixel multiple times). For instance, for a circular filter region with a footprint radius of 1.0 source pixel, each source pixel is sampled an average of \u03c0 times. By using the forward mapping technique of the present invention, redundant memory access can be essentially eliminated, thus lowering the memory bandwidth by a factor of \u03c0. Preferably, the present invention provides a forward mapping technique in which all source pixels are read once in pixel read template order and subsequently splatted onto the target image with a filter kernel.","As illustrated in , each source pixel  has a \u0394x  and a \u0394y  relative to each of its nearest-neighbor target samples . The \u0394x can be preferably computed incrementally since all samples along a scanline are equi-distant. The special scan direction essentially guarantees that the \u0394y is constant along each scanline. Although the raster grid locations deviate from the true scanline , the actual distances can be estimated preferably by adding a small correction which may be stored in the pixel read template  and is preferably uniform among scanlines. The filter kernel is preferably pre-computed once and stored in a lookup table (LUT). Subsequently, the contribution of each source pixel  is preferably indexed by its \u0394x and \u0394y into the lookup table (LUT) for the four (or more) nearest-neighbor target samples . The number of target samples  depends upon the footprint of the filter used, and it may preferably vary from four to 16 samples. Using this method, each source pixel  is preferably read exactly once from memory, then four (or more) times modulated by a lookup table entry and accumulated in the target pixel. In this manner, the final pixel value is the weighted average of the nearby source pixels . This weighted average requires a division by the sum of the filter weights to normalize each final pixel intensity.","In addition to image warping, which can be broadly defined as a geometric transformation between two images (e.g., a source image and a target image), three-dimensional (3D) volume transformation plays a key role in volume rendering, volume modeling and registration of multiple volumes. Among all affine transformations, rotation generally consumes the most computation time and is considered the most complicated. Accordingly, in providing a universal 3D rendering architecture in accordance with the present invention, several unique methods for performing arbitrary 3D volume rotation are presented, as described in detail herein below. Although the universal 3D rendering hardware of the present invention may be used without the 3D volume rotation methods described herein, these methods, or algorithms, are preferably implemented in conjunction with the apparatus of the present invention to provide enhanced speed and features and are adapted to most efficiently utilize the apparatus of the present invention.","Prior to describing the unique methods for performing 3D volume rotation, it is important to first provide some basic definitions of the terms used. As appreciated by those skilled in the art, relative to the rows and columns of an image, a beam in a volume may be defined as a row of voxels along one major coordinate axis (e.g., an x-beam is a row of voxels in the x direction). A slice of a volume is a plane of voxels which is perpendicular to a major axis (e.g., an x-slice is defined as a plane perpendicular to the x axis).","Prior art methods for performing volume transformations typically utilize multiple-pass algorithms, which are usually direct extensions of the multiple-pass algorithms used for image transformations. Various methods for performing 3D rotation have been proposed, generally involving a decomposition of the 3D transformation into multiple two-dimensional (2D) or one-dimensional (1D) transformations. These prior art methods have been the subject of articles, including , by R. A. Drebin et al., (88 ), Vol. 22, pp 65-74, August 1988, -, by P. Hanrahan, (), Vol. 24, pp 71-78, November 1990 and , by P. Schroder and J. B. Salem, 91, pp. 50-57, 1991, all of which are incorporated herein by reference. However, these known 3D transformation methods typically result in a lower quality rotation and\/or slower processing speed.","One of the properties which make three-dimensional (3D) rotation so difficult is that 3D rotations inherently require global communication and could cause memory contention while writing data back to the distributed memory modules. However, as shear transformation capitalizes on nearest neighbor connections, it lends itself to an extremely feasible multi-pipelined hardware implementation, as provided by the unique architecture of the present invention. The present invention further provides novel methods for performing arbitrary 3D rotation, essentially by decomposing the 3D rotations into sequences of different types of shear transformations.","Using a conventional decomposition approach, since a 2D rotation can be decomposed into three one-dimensional (1D) shears, a direct extension to 3D rotation would require nine 1D shears. However, in accordance with the present invention, four preferred methods of shear decomposition of an arbitrary 3D volume rotation are presented, as described in detail herein below. These methods include a four-pass 2D slice shear, a four-pass 2D beam shear, a three-pass beam-slice shear and a two-pass 3D beam shear decomposition. By not introducing a scale operation, the algorithms of the present invention avoid complications in sampling, filtering and the associated image degradations.","It is to be appreciated by one skilled in the art that a 3D rotation matrix can be expressed as the concatenation of three major axis rotations, R(\u03c6), R(\u03b8), R(\u03b1), where",{"@attributes":{"id":"p-0207","num":"0206"},"maths":[{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["R","x"]},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mrow":{"mi":["cos","\u03d5"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mrow":{"mi":["sin","\u03d5"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}]},{"mtd":[{"mn":"0"},{"mrow":{"mrow":{"mo":"-","mi":"sin"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"\u03d5"}},{"mrow":{"mi":["cos","\u03d5"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}]}]}}}}},{"@attributes":{"id":"MATH-US-00011-2","num":"00011.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["R","y"]},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mn":"0"},{"mrow":{"mrow":{"mo":"-","mi":"sin"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"\u03b8"}}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mrow":{"mi":["sin","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mn":"0"},{"mrow":{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}]}]}}}}},{"@attributes":{"id":"MATH-US-00011-3","num":"00011.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["R","z"]},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":["cos","\u03b1"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mrow":{"mi":["sin","\u03b1"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mn":"0"}]},{"mtd":[{"mrow":{"mrow":{"mo":"-","mi":"sin"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"\u03b1"}},{"mrow":{"mi":["cos","\u03b1"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"1"}]}]}}}}}],"br":{},"sub":["3D","x","y","z"]},{"@attributes":{"id":"p-0208","num":"0207"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"R","mrow":{"mn":"3","mo":"\u2062","mi":"D"}},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":["cos","\u03b8","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}},{"mrow":{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}},{"mrow":{"mrow":{"mo":"-","mi":"sin"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mi":"\u03b8"}}]},{"mtd":[{"mrow":{"mrow":[{"mi":["sin","\u03d5","sin","\u03b8","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["cos","\u03d5","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}],"mo":"-"}},{"mrow":{"mrow":[{"mi":["sin","\u03d5sin","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["cos","\u03d5","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}],"mo":"+"}},{"mrow":{"mi":["sin","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}]},{"mtd":[{"mrow":{"mrow":[{"mi":["cos","\u03d5","sin","\u03b8","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["sin","\u03d5","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}],"mo":"+"}},{"mrow":{"mrow":[{"mi":["cos","\u03d5","sin","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["sin","\u03d5","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}],"mo":"-"}},{"mrow":{"mi":["cos","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}]}]}}}}}},"The above 3D rotation matrix (R) is used for all the decompositions which follow. One of the primary differences between the unique methods of the present invention and other conventional approaches is that in the present invention, the decomposition is applied directly to a 3D rotation matrix, rather than to multiple 2D rotation sequences, to obtain shear sequences. It is to be appreciated that, for any of the shear operations performed in accordance with the present invention, barrel shifters may be used as a preferred hardware implementation, although other means, such as logarithmic shifters or the like, are similarly contemplated.","As shown in , a method for performing two-dimensional (2D) slice shear rotation, in accordance with one embodiment of the present invention, preferably involves a decomposition of the 3D rotation into a sequence of 2D slice shears. In a 2D slice shear, a volume slice (i.e., a plane of voxels along a major projection axis and parallel to any two axes) is merely shifted within its plane. A slice may be arbitrarily taken along any major projection axis. For example,  illustrates a y-slice shear. A 2D y-slice shear is preferably expressed as:\n\n\n\n\n","A 2D y-slice shear may preferably be written as S(xz, y, (a, b)), interpreted as a shear along the y axis by an amount a  in the x-direction and an amount b  in the z-direction. Although both a and b are preferably constants, it is further contemplated that a and b can represent functions as well. A 2D x-slice shear, S(yz, x, (c, d)), and a 2D z-slice shear, S(xy, z, (e, f)), are similarly defined. With reference to , the volume represented by the solid lines  is the shear result of the volume defined by the dotted lines .","Intuitively, consecutive shears along the same axis produce a conforming shear. For example:",{"@attributes":{"id":"p-0213","num":"0212"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":"\u2003","mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"mrow":[{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["xz","y"],"mo":[",",","],"mrow":{"mo":["(",")"],"mrow":{"mi":["a","b"],"mo":","}}}}},{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["xz","y"],"mo":[",",","],"mrow":{"mo":["(",")"],"mrow":{"msup":[{"mi":["a","\u2032"]},{"mi":["b","\u2032"]}],"mo":","}}}}}],"mo":"\u00b7"},{"mrow":[{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mi":"a"},{"mn":"1"},{"mi":"b"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"1"}]}]}},{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"msup":{"mi":["a","\u2032"]}},{"mn":"1"},{"msup":{"mi":["b","\u2032"]}}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"1"}]}]}}],"mo":"\u00b7"}],"mo":["=","\u2062"],"mi":{}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mrow":{"mi":"a","mo":"+","msup":{"mi":["a","\u2032"]}}},{"mn":"1"},{"mrow":{"mi":"b","mo":"+","msup":{"mi":["b","\u2032"]}}}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"1"}]}]}}}}}]}}}}},"In order to build the general 3D matrix from 2D shear matrices, shear products may be restricted to products of different shears: S(yz, x, (c, d)), S(xz, y, (a, b)) and S(y, z, (e, f)). However, the product matrix of these three shear matrices will still not be in the general form due to a constant 1 in the present matrix. Accordingly, another shear matrix is preferably concatenated, where this final shear is the same slice shear as the first one. This results in the following six permutations of shear sequences:\n\nS(xz, y, (a, b))\u00b7S(xy, z, (e, f))\u00b7S(yz, x, (c, d))\u00b7S(xz, y, (g, h))\n\nS(xz, y, (a, b))\u00b7S(yz, x, (c, d))\u00b7S(xy, z, (e, f))\u00b7S(xz, y, (g, h))\n\nS(xy, z, (e, f))\u00b7S(xz, y, (a, b))\u00b7S(yz, x, (c, d))\u00b7S(xy, z, (i, j))\n\nS(xy, z, (e, f))\u00b7S(yz, x, (c, d))\u00b7S(xz, y, (a, b))\u00b7S(xy, z, (i, j))\n\nS(yz, x, (c, d))\u00b7S(xz, y, (a, b))\u00b7S(xy, z, (e, f))\u00b7S(yz, x, (m, n))\n\nS(yz, x, (c, d))\u00b7S(xy, z, (e f))\u00b7S(xz, y, (a, b))\u00b7S(yz, x,(m, n))\n","For each of the shear sequences, the product matrix of the consecutive shear matrices is preferably computed and set equal to the underlying 3D rotation matrix. For example, for the first shear sequence given above (i.e., S(xz, y, (a, b)) S(xy, z, (e, f)) S(yz, x, (c, d)) S(xz, y, (g, h))):\n\n(\u03c6)(\u03b8)(\u03b1)=(,())\u00b7(,())\u00b7(,())\u00b7(,())\n","The above matrix equation implies nine trigonometric equations with eight variables, namely, a, b, c, d, e, f, g and h. In solving these nine equations for the eight variables, a-h, the following results are obtained:",{"@attributes":{"id":"p-0217","num":"0216"},"maths":[{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"a","mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":["sin","\u03b8","sin"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["cos","\u03d5"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"-"}}}},{"mi":"sin","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"\u03d5","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["cos","\u03b1"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"-"}}}}],"mo":"+"},{"msup":{"mrow":{"mo":["(",")"],"mrow":{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},"mn":"2"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mi":["sin","\u03b1sin","\u03d5"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}},{"@attributes":{"id":"MATH-US-00014-2","num":"00014.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"b","mo":"=","mrow":{"mo":"-","mfrac":{"mrow":[{"mrow":{"mi":["cos","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},"mo":"-","mn":"1"},{"mi":["sin","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}}},{"@attributes":{"id":"MATH-US-00014-3","num":"00014.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"c","mo":"=","mrow":{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}}},{"@attributes":{"id":"MATH-US-00014-4","num":"00014.4"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"d","mo":"=","mfrac":{"mrow":[{"mrow":[{"mrow":{"mo":"-","mi":"sin"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["\u03d5","sin","\u03b8","cos","\u03b1"]},{"mi":["cos","\u03d5","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}],"mo":["+","-"]},{"mi":["sin","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}},{"@attributes":{"id":"MATH-US-00014-5","num":"00014.5"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"e","mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":["sin","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["cos","\u03d5","sin","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["sin","\u03d5","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}],"mo":["+","-"]},{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}},{"@attributes":{"id":"MATH-US-00014-6","num":"00014.6"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"f","mo":"=","mrow":{"mrow":{"mo":"-","mi":"sin"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["\u03d5","cos","\u03b8"]}}}},{"@attributes":{"id":"MATH-US-00014-7","num":"00014.7"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"g","mo":"=","mfrac":{"mrow":[{"mrow":{"mi":["cos","\u03b8","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},"mo":"-","mn":"1"},{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}},{"@attributes":{"id":"MATH-US-00014-8","num":"00014.8"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"h","mo":"=","mfrac":{"mrow":[{"mrow":{"mi":["cos","\u03b8","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},"mo":"-","mn":"1"},{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}}]},"In a similar manner, the shear matrices for the remaining five slice shear sequences given above may be obtained. In fact, the slice shear sequence with the solution given above has the simplest expression and is preferably termed the dominant sequence.","Referring now to , a method for performing three-dimensional (3D) rotation by a two-dimensional (2D) beam shear decomposition will now be described. First, a beam shear may be defined as a beam that is merely shifted in its major direction without any change of the other two coordinates. For example, a 2D x-beam shear is preferably expressed as:\n\n\n","A 2D x-beam shear may preferably be written as S(x, yz, (c, d)), interpreted as a shear along the x axis by an amount a in the x-direction and an amount b in the z-direction. A 2D y-beam shear, S(y, xz, (a, b)), and a 2D z-beam shear, S(z, xy, (e, f)), are similarly defined.  illustrates an x-beam shear, wherein the volume represented by the dotted lines  is sheared to the volume position represented by the solid lines .","A two-dimensional (2D) beam shear is advantageous over a 2D slice shear, and is therefore preferred, since a beam is shifted without changing the other two coordinates. Thus, the resampling for each pass of the 2D beam shear approach is simpler, as only a linear interpolation is required. In contrast, a 2D slice shear approach requires a bilinear interpolation which is more complex.","Similar to the 2D slice shear decomposition, in order to build the general 3D matrix from 2D beam shear matrix decompositions, shear products may be restricted to products of different shears: S(x, yz, (c, d)), S(y, xz, (a, b)), S(z, xy, (e,f)). However, the product matrix of these three shear matrices will still not be in the general form due to a constant  in the matrix. Accordingly, as in the slice shear method, another shear matrix is preferably concatenated, where this final shear is the same beam shear as the first one. This results in the following six permutations of shear sequences:\n\nS(y, xz, (a, b))S(z, xy, (e, f))S(x, yz, (c, d))S(y, xz, (g, h))\n\nS(y, xz, (a, b))S(x, yz, (c, d))S(z, xy, (e, f))S(y, xz, (g, h))\n\nS(z, xy, (e, f))S(y, xz, (a, b))S(x, yz, (c, d))S(z, xy, (i, j))\n\nS(z, xy, (e, f))S(x, yz, (c, d))S(y, xz, (a, b))S(z, xy, (i, j))\n\nS(x, yz, (c, d))S(y, xz, (a, b))S(z, xy, (e, f))S(x, yz, (m, n))\n\nS(x, yz, (c, d))S(z, xy, (e, f))S(y, xz, (a, b))S(x, yz, (m, n))\n","For each of the above shear sequences, the product matrix of the consecutive shear matrices is preferably computed and set equal to the underlying 3D rotation matrix. For example, for the first shear sequence given above (i.e., S(y, xz, (a, b)) S(z, xy, (e, f)) S(x, yz, (c, d)) S(y, xz, (g, h))):\n\n(\u03c6)(\u03b8)(\u03b1)=(,())\u00b7(,())\u00b7(())\u00b7(,())\n","The above matrix equation implies nine trigonometric equations with eight variables, namely, a, b, c, d, e, f, g and h. In solving these nine equations for the eight variables, a-h, the following results are obtained:",{"@attributes":{"id":"p-0225","num":"0224"},"maths":[{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"a","mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":["sin","\u03b8","sin"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["cos","\u03d5"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"-"}}}},{"mi":"sin","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"\u03d5","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["cos","\u03b1"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"-"}}}}],"mo":"+"},{"mi":["sin","\u03d5","sin"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msup":{"mrow":{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},"mn":"2"}}]}}}},{"@attributes":{"id":"MATH-US-00015-2","num":"00015.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"b","mo":"=","mrow":{"mo":"-","mfrac":{"mrow":[{"mrow":{"mi":["cos","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},"mo":"-","mn":"1"},{"mi":["sin","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}}},{"@attributes":{"id":"MATH-US-00015-3","num":"00015.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"c","mo":"=","mrow":{"mrow":{"mo":"-","mi":"cos"},"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mi":["\u03b8","sin","\u03b1"]}}}},{"@attributes":{"id":"MATH-US-00015-4","num":"00015.4"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"d","mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["sin","\u03d5","sin","\u03b8cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["cos","\u03d5","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}],"mo":["+","-"]},{"mi":["sin","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}},{"@attributes":{"id":"MATH-US-00015-5","num":"00015.5"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"e","mo":"=","mrow":{"mo":"-","mfrac":{"mrow":[{"mrow":[{"mi":["cos","\u03d5","sin","\u03b8sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["sin","\u03d5cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["sin","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}],"mo":["-","+"]},{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}}},{"@attributes":{"id":"MATH-US-00015-6","num":"00015.6"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"f","mo":"=","mrow":{"mi":["sin","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}}}},{"@attributes":{"id":"MATH-US-00015-7","num":"00015.7"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"g","mo":"=","mrow":{"mo":"-","mfrac":{"mrow":[{"mrow":{"mi":["cos","\u03b8","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},"mo":"-","mn":"1"},{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}}},{"@attributes":{"id":"MATH-US-00015-8","num":"00015.8"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"h","mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":["sin","\u03d5","sin"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"\u03b8","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["cos","\u03b1"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"-"}}}},{"mi":"sin","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["cos","\u03d5"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"-"}}}}],"mo":"+"},{"mi":["sin","\u03d5","sin"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msup":{"mrow":{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},"mn":"2"}}]}}}}]},"In a similar manner, the shear matrices for the remaining five beam shear sequences given above may be obtained. The beam shear sequence with the solution given above is preferably termed the dominant sequence.","With reference now to , a method for performing three-dimensional (3D) rotation by two-dimensional (2D) beam-slice shear decomposition in accordance with the present invention will be described. A 2D beam-slice shear may preferably be defined as a beam that is shifted within a plane. For example, a 2D x-beam-y-slice shear is preferably expressed as:\n\n\n\n\n","A 2D x-beam-y-slice shear may preferably be written as S((x, yz, (a, g)), (z, y, b)), interpreted as a shear along the x axis by an amount a in the y-direction and an amount g in the z-direction, combined with a shear along the z axis by an amount b in the y-direction, where a, g and b are preferably constants. In essence, a beam-slice shear is a combination of a beam shear and a slice shear.  illustrates an x-beam-y-slice shear, S((x, yz, (a, g)), (z, y, b)), wherein the volume represented by the dotted lines  is sheared to the volume position represented by the solid lines .","To build the general 3D matrix from a 2D shear matrix decomposition, shear products may be restricted to products of different shears: y-beam-x-slice shear S((y, xz, (c, h)), (z, x, d)), x-beam-y-slice shear S((x, yz, (a, g)), (z, y, b)), and y-beam shear S(y, xz, (I, f)). As in the case of the slice shear and beam shear approaches, it is to be appreciated that there are also six permutations of beam-slice shear sequences.","For each of the shear sequences, the product matrix of the consecutive shear matrices is preferably computed and set equal to the underlying 3D rotation matrix. For example, for the first beam-slice shear sequence given above (i.e., S((y, xz, (c, h)), (z, x, d)) S((x, (a, g)), (z, y, b)) S(y, sz, (I, f))):\n\n(\u03c6)(\u03b8)(\u03b1)=((,()),())\u00b7(,()),())\u00b7(,())\n","The above matrix equation implies nine trigonometric equations with eight variables, namely, a, b, c, d, f, g, h and I. In solving these nine equations for the eight variables, the following results are obtained:\n\n=sin \u03c6 sin \u03b8 cos \u03b1\u2212cos \u03c6 sin \u03b1\n\n=sin \u03c6 cos \u03b8\n",{"@attributes":{"id":"p-0232","num":"0231"},"maths":[{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"c","mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":"sin","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"\u03d5","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["cos","\u03b1"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"-"}}}},{"mi":["sin","\u03b8","sin"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["cos","\u03d5"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"-"}}}}],"mo":"+"},{"mi":["sin","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msup":{"mrow":{"mi":"\u03d5","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},"mn":"2"}}]}}}},{"@attributes":{"id":"MATH-US-00016-2","num":"00016.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"d","mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":["sin","\u03d5","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["cos","\u03d5","sin","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["sin","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}],"mo":["-","-"]},{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}},{"@attributes":{"id":"MATH-US-00016-3","num":"00016.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"f","mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":["sin","\u03d5","sin"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"mrow":{"mi":"\u03b8","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["cos","\u03b1"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"-"}}}},{"mi":"sin","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"\u03b1","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":[{"mi":["cos","\u03d5"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}],"mo":"-"}}}}],"mo":"+"},{"mi":["sin","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msup":{"mrow":{"mi":"\u03d5","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["cos","\u03b8"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}}},"mn":"2"}}]}}}},{"@attributes":{"id":"MATH-US-00016-4","num":"00016.4"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"g","mo":"=","mfrac":{"mrow":[{"mrow":[{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["sin","\u03d5","sin","\u03b8","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},{"mi":["cos","\u03d5","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}],"mo":["+","-"]},{"mi":["sin","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}},{"@attributes":{"id":"MATH-US-00016-5","num":"00016.5"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"h","mo":"=","mfrac":{"mrow":[{"mrow":{"mi":["cos","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},"mo":["\u2062","-"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"},{"mi":["sin","\u03d5","cos","\u03b8"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}},{"@attributes":{"id":"MATH-US-00016-6","num":"00016.6"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"i","mo":"=","mrow":{"mo":"-","mfrac":{"mrow":[{"mrow":{"mi":["cos","\u03b8","cos","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]},"mo":"-","mn":"1"},{"mi":["cos","\u03b8","sin","\u03b1"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}]}]}}}}}]},"It is to be appreciated that the shear matrices for the remaining five shear sequences may be obtained in a similar manner.",{"@attributes":{"id":"p-0234","num":"0233"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0235","num":"0234"},"figref":["FIG. 17","FIG. 17"],"b":["158","158"]},"The three 3D beam shears may preferably be denoted as SH, SHand SH. Now, using the method of the present invention described herein, an arbitrary 3D rotation can be decomposed into only two consecutive 3D beam shears. The dominant decomposition sequence may be obtained directly from the 2D slice shear sequence as:\n\n\n\nwhere\n",{"@attributes":{"id":"p-0237","num":"0236"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":"\u2003","mrow":{"mtable":[{"mtr":[{"mtd":{"mrow":{"msub":{"mi":"SH","mrow":{"mn":"3","mo":"\u2062","msub":{"mi":["D","x"]}}},"mo":["=","\u2062"],"mi":{},"mrow":{"mrow":[{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["xz","y"],"mo":[",",","],"mrow":{"mo":["(",")"],"mrow":{"mi":["a","b"],"mo":","}}}}},{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["xy","z"],"mo":[",",","],"mrow":{"mo":["(",")"],"mrow":{"mi":["e","f"],"mo":","}}}}}],"mo":"\u00b7"}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mrow":{"mi":["a","be"],"mo":"+"}},{"mrow":{"mn":"1","mo":"+","mi":"bf"}},{"mi":"b"}]},{"mtd":[{"mi":"e"},{"mi":"f"},{"mn":"1"}]}]}}}}}]},{"mtr":[{"mtd":{"mrow":{"msub":{"mi":"SH","mrow":{"mn":"3","mo":"\u2062","msub":{"mi":["D","x"]}}},"mo":["=","\u2062"],"mi":{},"mrow":{"mrow":[{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["yz","x"],"mo":[",",","],"mrow":{"mo":["(",")"],"mrow":{"mi":["c","d"],"mo":","}}}}},{"mi":"S","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["xz","y"],"mo":[",",","],"mrow":{"mo":["(",")"],"mrow":{"mi":["g","h"],"mo":","}}}}}],"mo":"\u00b7"}}}},{"mtd":{"mrow":{"mo":["=","\u2062"],"mi":{},"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mrow":{"mn":"1","mo":"+","mi":"cg"}},{"mi":"c"},{"mrow":{"mi":["d","ch"],"mo":"+"}}]},{"mtd":[{"mi":"g"},{"mn":"1"},{"mi":"h"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"1"}]}]}}}}}]}],"mo":["\u2062","\u2062"],"mstyle":{"mtext":{}}}}}}},"Using the 3D beam shear decomposition approach of the present invention described herein, an arbitrary 3D rotation preferably involves only two major beam transformations, whereas conventional decomposition approaches require three (e.g., Hanrahan's decomposition). In accordance with the 3D beam shear method of the present invention, the first pass involves only x beams and the second pass involves only z beams. By the end of the first shear pass, all voxels of a beam preferably have the same offsets. As there are Nbeams for an Nvolume, there are only Ndifferent offset values. Accordingly, the offset values for Nbeams can be stored at the end of the first pass, while storing the voxels to their nearest neighbor integral positions.","When multiple pass algorithms are used, the resampling techniques chosen are key to achieving high quality. Intuitively, resampling is necessary for each pass because a continuous shear transformation may move voxels off the grid points. One problem inherent with multiple resampling, however, is the quick degradation of the volume quality if consecutive rotations are applied to a volume. It is therefore desirable to sample the volume only once.","Accordingly, a preferred method of the present invention achieves one pass resampling of a volume. In essence, the method of the present invention involves precomputing a sampled volume and then using only zero-order (i.e., nearest neighbor) interpolation in each shear pass, thereby distinguishing from known prior art methods which require global communication (e.g., Wittenbrink and Somani's permutation warping).","Given an original volume (source volume) and the desired rotated volume (target volume), the method of the present invention preferably first builds up a one-to-one correspondence between a source voxel and a target voxel. This one-to-one mapping is guaranteed by the multi-pass shear decomposition of the present invention because each shear is a one-to-one transformation using zero-order interpolation. The concatenation of a sequence of one-to-one mapping remains one-to-one. Once this one-to-one correspondence is built up, the method of the present invention preferably calculates for each source voxel its corresponding target voxel and stores it in the source voxel position. During this procedure, no global communication is required; the resampling is performed by interpolation on the local voxels. The sampling position of each target voxel is preferably computed using a backward transformation of rotation.","After obtaining the values for all target voxels, the method of the present invention preferably shuffles them to their destinations in target volume. Intuitively, this would involve global communication. However, global communication is expensive to perform for parallel implementation. Therefore, the method according to present invention preferably uses multiple shears with a nearest neighbor placement scheme to achieve this voxel shuffling. Since shear is a regular, non-conflict transformation, each pass can be performed more efficiently than if global communication was utilized. Using the 3D beam shear decomposition method of the present invention described herein, only a minimum of two passes of regular local communication are necessary to achieve virtually the same effect as global communication.","It is to be appreciated that care should be taken to avoid the overlapping of beams in 3D beam shear. Consider, for example, the 3D x beam shear equation given above. While each x beam is preserved (i.e., an x beam remains rigid after a 3D x beam shear), several x beams may overlap with each other. To maintain the required one-to-one mapping, recall that a 3D beam shear is the concatenation of two 2D slice shears, as discussed above. A 2D slice shear maintains one-to-one mapping when using zero-order interpolation. Therefore, as a solution, the method of the present invention preferably calculates the destination coordinates using the same order as that of two consecutive 2D slice shears, but communication is preferably only performed once. For a 3D x beam shear, while the x coordinate is calculated directly using the 3D shear matrix (described above), the y and z coordinates of each beam are preferably calculated as\n\n\u2032=round()\n\n\u2032=round(\u2032)\n\nwhere round(w) is a function of rounding w to the nearest integer. Coordinates (y\u2032, z\u2032) determine the integral coordinates of the whole beam for the nearest neighbor storage. In this manner, no overlap occurs.\n","In accordance with another form of the present invention, several unique methods for performing enhanced volume processing will be discussed in detail herein below.","Perspective projections present inherent challenges, particularly when performing ray casting. For parallel projections, sight rays that are cast through a volume dataset maintain a constant sampling rate on the underlying volume data. It is straightforward to set this sampling rate to create an output image of the required quality. For perspective projections, however, the rays do not maintain such a continuous and uniform sampling rate. Instead, the rays diverge as they traverse the volume from front to back. This creates an uneven sampling of the underlying volume, as shown in .","Referring now to , conventional ray casting algorithms generally handle ray divergence from perspective projections by one of two methods. The first method is undersampling (), in which rays  are cast, from a predefined viewpoint , so that the sampling rate at the front of the volume  is appropriate for the desired image quality. However, because of the perspective ray divergence, the underlying volume dataset is undersampled. This may result in severe aliasing by creating \u201choles\u201d in the rear of the volume  where regions of voxels remain unsampled. The second method is oversampling (), in which rays  are cast from a predefined viewpoint  so that the sampling rate at the rear of the volume dataset  is appropriate for the desired image quality. This approach avoids the aliasing of the first method; however, the volume may be radically oversampled in the front . The inefficient oversampling in the front of the volume  dramatically increases the runtime of this method. The rays  can be cast with a sampling rate between undersampling and oversampling. This results in a tradeoff between the image quality of oversampling and the rendering speed of undersampling.","Many prior imaging architectures do not even attempt to perform perspective projections. Other architectures have dealt with perspective projections by casting diverging sight rays from a predefined viewpoint, which produce images with temporal aliasing and either do not achieve true real-time frame rates (i.e., 30 Hertz) or are much more complex than the slice-order method of the present invention.","A ray-splitting method applies the concept of adaptive super-sampling in order to maintain a uniform ray density. In this approach, a ray is split into two child rays when neighboring rays diverge beyond some predetermined threshold. Recently, a method was proposed which divides the viewing frustum into regions based on distance from the viewpoint, such that the ray density in each region is near the underlying volume resolution. Afterwards, such method projects each region onto sub-images and composites them into the frame buffer using texture mapping hardware. In effect, the technique casts continuous rays through a region, then at specified boundaries, splits them into a new set of continuous rays. This, however, creates a potential undesired discontinuity between regions.","A method for performing perspective projections of uniform regular datasets, termed ER-Perspective (exponential regions perspective), in accordance with one form of the present invention, preferably adaptively samples the underlying volume, whereby the above-described problems, inherent in conventional volume rendering systems and methods, are essentially eliminated. The ER-Perspective algorithm combines the desirable properties of both undersampling and oversampling, providing extremely good anti-aliasing properties associated with oversampling methods, while providing runtimes on the order of undersampling methods. Furthermore, this algorithm preferably creates at least one sample for every visible voxel in the volume dataset. ER-Perspective gains a runtime advantage over previous work by utilizing slice-order voxel access, while maintaining equal or better image quality in comparison to known perspective projection methods.",{"@attributes":{"id":"p-0250","num":"0249"},"figref":["FIG. 19","FIG. 19"],"b":["168","172","174","172","174"]},"The ER-Perspective algorithm preferably uses region boundaries , which define the exponential regions, to mark the locations where the sight rays  are preferably merged. By defining the regions and merging all rays  at the boundaries , the algorithm provides a regular pattern of ray merging that is dependent on the global geometry rather than local neighborhood conditions.  more clearly illustrates the merging of sight rays at region boundaries  for contribution to baseplane pixel B, in particular. With reference to , an odd number of rays  are preferably merged such that the resulting ray \u2032 is essentially an exact continuation of the previous center ray, thus eliminating potential discontinuities present at the region boundaries . This is one important advantage of the method of the present invention over known prior approaches. Furthermore, this algorithm can be qualified by characterizing the filtering achieved when adaptively sampling the volume.","An example of a preferred filtering scheme is shown in . Referring to , a Bartlett window (i.e., linear interpolation, triangle filter) is preferably employed. Cascading efficient local Bartlett windows at each region boundary  is essentially the equivalent of resampling the rays  with a single large Bartlett filter for each baseplane pixel (see ). A graphical representation of the preferred filter weights  is shown for contribution to the baseplane pixels (e.g., pixels A, B, C).","The base sampling rate of the algorithm can be set to a predefined value according to a desired image quality. The base sampling rate is the minimum ray density compared to the underlying volume resolution. Although the ER-Perspective method of the present invention supports virtually any sampling rate, a sampling rate of at least one ray per voxel is preferred. The algorithm has the advantage of keeping the ray density between one to two times the base sampling rate. This guarantees that no voxels are missed in the rear of the volume dataset and places an upper bound on the total amount of work performed at two times (2\u00d7) supersampling.","Since the present invention utilizes slice-order processing, the volume dataset is projected onto a baseplane of the volume which is most perpendicular to the view direction. The baseplane image is then warped onto the final image plane in a conventional manner (e.g., in the same manner as in shear-warp or the prior Cube-4 architecture).","The ER-Perspective method of the present invention is ideally suited for implementation on the Cube-5 architecture described above. Specifically, this algorithm preferably only requires nearest neighbor communication between processing elements. While processing a row of voxels on a one-dimensional array of processing elements, the algorithm only requires processing elements to communicate with their immediate left and right neighbors. The Cube-5 rendering pipelines similarly support nearest neighbor communication.","The ER-Perspective algorithm of the present invention preferably employs slice-order processing along one of the three major axes. Consequently, the regions in the ER-perspective algorithm are defined as slabs of slices along a major projection axis. In a preferred embodiment of the ER-perspective method according to the present invention, the volume dataset is projected along slices perpendicular to the z-axis. So as not to limit the methods of the present invention to projections along the z-axis only, it is to be appreciated that the coordinate system may be flipped and the geometry rotated. The algorithm proceeds, as illustrated in , by measuring the distance along the z-axis, from the viewpoint  to the front of the volume dataset , is determined (e). Subsequently, a first region  is created to consist of as many z-slices as this distance. Each successive region after the first region  is preferably twice as deep as the one before it.","When combined with high quality supersampling, the first region is exactly as large as needed to have one ray per voxel at the end of the region when shooting one ray per pixel of the final image. Thus, supersampling higher than 2\u00d7 might be needed in the front of the volume to render high quality close up views.","As illustrated in the example of , if the viewpoint  is three voxel units from the front of the volume (i.e., the z=3 region boundary), for example, then the first region  is preferably three voxel units thick, the next region is six voxel units thick, and so on. In general, the I-th region is preferably e\u00b72slices thick, where eis the distance from the viewpoint  to the front of the volume (see ). Forcing the regions to be thus defined produces the desired effect that any two perspective rays  cast through any of the regions are twice as far apart at the rear boundary (i.e., the z=24 boundary) as they are at the front boundary (i.e., the z=3 boundary). This is shown in  as the distance between the two rays  grows from one unit to two units across the first region , then to four units, and finally to eight units at the rear of the last region. Additionally, since the region boundaries  are dependent on the global geometry, the efficiency of the ray casting algorithm is maximized by providing a mechanism for keeping the ray density between one and two times the underlying volume resolution in each dimension. It also creates a regular topology so that the filtering of the data can be controlled as perspective rays are cast.","Having regions with boundaries at exponential distances produces a ray density twice as high at the front as at the back of the region. Therefore, a mechanism must preferably be provided to adjust the ray density when crossing a region boundary. Since each ray preferably starts on a voxel coordinate at the rear of a region, at the front of the region every second ray in each dimension will preferably coincide directly with a voxel coordinate. The remaining rays preferably intersect the region boundary halfway between two voxel positions. To down-sample the ray density with this deterministic ray pattern, a two-dimensional (2D) Bartlett filter (also known as tent or triangle filter) is preferably employed, with an extent of \u00b11 voxel unit in each dimension. Because the ray density at the front of each region is twice the voxel density, this 3\u00d73 voxel neighborhood is intersected by 5\u00d75 rays. Referring now to , since the edges  each have a weight of zero, only the 3\u00d73 neighboring rays  are used for applying the filter to down-sample the ray density. This effectively merges neighboring rays. A Bartlett filter is preferred over a simple box filter for the added quality it produces in the final image. For the case of front-to-back processing, rays are split instead of merged. Here a bilinear interpolation of the rays is performed to generate the new rays which begin between the other rays. It should be mentioned that a Bartlett filter of size \u00b11 is the inverse of a bilinear interpolation operation.",{"@attributes":{"id":"p-0260","num":"0259"},"figref":"FIG. 22","b":["186","192","196","198","186","198"],"sup":["3 ","3 ","3 "],"sub":"2"},"As illustrated in , the rear of the volume dataset  does not necessarily always coincide with a region boundary . However, since it is preferred that the rays  be on exact voxel coordinates  at all of the region boundaries , the rays  preferably originate on the grid coordinates  at the rear of the last region enclosing the volume dataset  (shaded area). Therefore, the voxel coordinates and the ray sample locations  may not be congruent at the rear of the volume . This not only provides the mentioned boundary conditions, but aids with temporal anti-aliasing when the viewpoint  is moved in smaller than voxel unit distances, because the rays  will continue to originate from the same positions relative to the voxels.",{"@attributes":{"id":"p-0262","num":"0261"},"figref":"FIG. 23"},"With adaptive ray density perspective methods known in the prior art, it is generally difficult to determine the filtering function achieved when rays are merged using irregular patterns. However, since the ER-Perspective method of the present invention preferably uses regular boundaries for the filtering operations and exact ray placement within the boundaries, it is easier to compute the effective filter achieved by the cascading of local Bartlett filters. This is an important advantage of the ER-Perspective algorithm of the present invention. Additionally, the boundaries and filter of the present invention have preferably been chosen to overcome the poor image quality usually associated with conventional successive filtering of discrete data.","Consider, for example, the case of a perspective projection of a volume seven slices deep with the viewpoint two voxel units in front of the volume, as depicted in . Using the ER-Perspective method of the present invention, the rays  that are cast through a region are one voxel unit apart at the rear of the region. However, when the rays reach a region boundary  they are preferably filtered using local Bartlett filters. The Bartlett filters (simplified to 1-dimension) contain the following weights for a kernel of size 2n+1, normalized so that the output has the same scalar range as the input:",{"@attributes":{"id":"p-0265","num":"0264"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mn":["0","0"],"mo":[",",",",",","\u2062",",",",",",",",","\u2062",",",",",","],"mfrac":[{"mn":"1","msup":{"mi":"n","mn":"2"}},{"mn":"2","msup":{"mi":"n","mn":"2"}},{"mrow":{"mi":"n","mo":"-","mn":"1"},"msup":{"mi":"n","mn":"2"}},{"mi":"n","msup":{"mi":"n","mn":"2"}},{"mrow":{"mi":"n","mo":"-","mn":"1"},"msup":{"mi":"n","mn":"2"}},{"mn":"2","msup":{"mi":"n","mn":"2"}},{"mn":"1","msup":{"mi":"n","mn":"2"}}],"mi":["\u2026","\u2026"],"mstyle":[{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}},{"mspace":{"@attributes":{"width":"0.6em","height":"0.6ex"}}}]}}},"br":{}},{"@attributes":{"id":"p-0266","num":"0265"},"maths":{"@attributes":{"id":"MATH-US-00019","num":"00019"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mn":["0","0"],"mo":[",",",",",",","],"mfrac":[{"mn":["1","4"]},{"mn":["2","4"]},{"mn":["1","4"]}]}}}},"By way of example, as illustrated in , consider the contribution of samples a, b, c, d and e to the partially composited ray which changes from region  to region  at location o,",{"@attributes":{"id":"p-0268","num":"0267"},"maths":{"@attributes":{"id":"MATH-US-00020","num":"00020"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"o","mo":"=","mrow":{"mrow":[{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mi":"b"},{"mfrac":{"mn":["2","4"]},"mo":"\u2062","mi":"c"},{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mi":"d"}],"mo":["+","+"]}}}},"br":{}},{"@attributes":{"id":"p-0269","num":"0268"},"maths":[{"@attributes":{"id":"MATH-US-00021","num":"00021"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"p","mo":"=","mrow":{"mrow":[{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mi":"d"},{"mfrac":{"mn":["2","4"]},"mo":"\u2062","mi":"e"},{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mi":"f"}],"mo":["+","+"]}}}},{"@attributes":{"id":"MATH-US-00021-2","num":"00021.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"q","mo":"=","mrow":{"mrow":[{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mi":"f"},{"mfrac":{"mn":["2","4"]},"mo":"\u2062","mi":"g"},{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mi":"h"}],"mo":["+","+"]}}}}],"br":{},"b":"1"},{"@attributes":{"id":"p-0270","num":"0269"},"maths":{"@attributes":{"id":"MATH-US-00022","num":"00022"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"A","mo":"=","mrow":{"mrow":[{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mi":"o"},{"mfrac":{"mn":["2","4"]},"mo":"\u2062","mi":"p"},{"mfrac":{"mn":["1","4"]},"mo":"\u2062","mi":"q"}],"mo":["+","+"]}}}},"br":{}},{"@attributes":{"id":"p-0271","num":"0270"},"maths":{"@attributes":{"id":"MATH-US-00023","num":"00023"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"A","mo":"=","mrow":{"mrow":[{"mfrac":{"mn":["1","16"]},"mo":"\u2062","mi":"b"},{"mfrac":{"mn":["2","16"]},"mo":"\u2062","mi":"c"},{"mfrac":{"mn":["3","16"]},"mo":"\u2062","mi":"d"},{"mfrac":{"mn":["4","16"]},"mo":"\u2062","mi":"e"},{"mfrac":{"mn":["3","16"]},"mo":"\u2062","mi":"f"},{"mfrac":{"mn":["2","16"]},"mo":"\u2062","mi":"g"},{"mfrac":{"mn":["1","16"]},"mo":"\u2062","mi":"h"}],"mo":["+","+","+","+","+","+"]}}}},"br":{}},"Each sample of a slice preferably contributes the same amount to the final image as any other sample in the same region (assuming all other operations on samples, such as color mapping and compositing, are equal). For example, the value that sample e contributes to pixel A with an effective weight of \u00bc after the cascading of the local Bartlett filters. Likewise, sample I contributes to pixel B with an effective weight of \u00bc. Sample f contributes to pixel A with a weight of 3\/16 and to pixel B with a weight of 1\/16 for a total of \u00bc. This can be repeated for samples g and h. The samples to the left of sample e and to the right of sample I partially contribute to pixels left of pixel A and right of pixel B, respectively, such that the sum of their contributions to the final image is also \u00bc. In fact, every sample that is in this region has the same weight. The weight is \u00bc because this region is the second region in the volume. For the first region in the volume, every sample preferably has a weight of \u00bd. This is qualifiable by realizing that there are two rays per final image pixel in this region. There are four rays per final image pixel in the second region, etc. Consequently, the weight which determines the contribution of each sample towards the final image is the ratio",{"@attributes":{"id":"p-0273","num":"0272"},"maths":{"@attributes":{"id":"MATH-US-00024","num":"00024"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mrow":[{"mi":["image","pixels"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}},{"mi":["samples","in","this","slice"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}]},"mo":"."}}}},"Since the ER-Perspective method of the present invention performs a slice-order processing, the total amount of computation may be analyzed by calculating the amount of work performed on each slice. Assuming that the work done on each sample is the same, the count of the number of samples processed can be used as a comparison of the workloads. For example, in the oversampling method (see ), the number of samples on the rear slice of a volume which ends exactly on a region boundary is N. On the front slice, the sample count depends on the geometry of the viewpoint. In particular, using similar triangles and defining eas the distance from the viewpoint to the front of the volume, the number of samples taken is",{"@attributes":{"id":"p-0275","num":"0274"},"maths":{"@attributes":{"id":"MATH-US-00025","num":"00025"},"math":{"@attributes":{"overflow":"scroll"},"msup":{"mrow":{"mo":["(",")"],"mfrac":{"mrow":{"msup":{"mi":"N","mn":"2"},"mo":"+","mrow":{"mi":"N","mo":"\u00b7","msub":{"mi":["e","z"]}}},"msub":{"mi":["e","z"]}}},"mn":"2"}}},"br":{}},{"@attributes":{"id":"p-0276","num":"0275"},"maths":{"@attributes":{"id":"MATH-US-00026","num":"00026"},"math":{"@attributes":{"overflow":"scroll"},"msup":{"mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"msup":{"mi":"N","mn":"2"},"mo":"+","mrow":{"mi":"N","mo":"\u00b7","msub":{"mi":["e","z"]}}},{"msub":{"mi":["e","z"]},"mo":"+","mi":"s"}]}},"mn":"2"}}},"br":{}},{"@attributes":{"id":"p-0277","num":"0276"},"maths":{"@attributes":{"id":"MATH-US-00027","num":"00027"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"s","mo":"=","mn":"0"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"msup":{"mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"msup":{"mi":"N","mn":"2"},"mo":"+","mrow":{"mi":"N","mo":"\u00b7","msub":{"mi":["e","z"]}}},{"msub":{"mi":["e","z"]},"mo":"+","mi":"s"}]}},"mn":"2"}}}},"br":{},"figref":"FIG. 18A"},{"@attributes":{"id":"p-0278","num":"0277"},"maths":{"@attributes":{"id":"MATH-US-00028","num":"00028"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"s","mo":"=","mn":"0"},"mi":"N"},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mfrac":{"msub":{"mi":["N_e","z"]},"mrow":{"msub":{"mi":["e","z"]},"mo":"+","mi":"s"}}},"mn":"2"}}}}},"For the ER-Perspective algorithm of the present invention, the analysis is more complicated. Depending on the viewing geometry,",{"@attributes":{"id":"p-0280","num":"0279"},"maths":{"@attributes":{"id":"MATH-US-00029","num":"00029"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":{"mi":"N","mo":"+","msub":{"mi":["e","z"]}},"msub":{"mi":["e","z"]}}}},"mo":"-","mn":"1"}}},"br":{},"sub":"z","sup":"i "},{"@attributes":{"id":"p-0281","num":"0280"},"maths":{"@attributes":{"id":"MATH-US-00030","num":"00030"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"reg","mo":"=","mn":"0"},{"mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mfrac":{"mrow":{"mi":"N","mo":"+","msub":{"mi":["e","z"]}},"msub":{"mi":["e","z"]}}}},"mo":"-","mi":"l"}]},"mo":"\u2062","mrow":{"munderover":{"mo":"\u2211","mrow":[{"mi":"s","mo":"=","mn":"0"},{"msup":[{"msub":{"mi":["e","z"]},"mo":"*"},{"mn":"2","mi":"reg"}],"mo":"\u2062"}]},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mfrac":{"mrow":[{"msup":{"mi":"N","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mrow":{"msup":[{"msub":{"mi":["e","z"]},"mo":"*"},{"mn":"2","mi":"reg"}],"mo":"\u2062"},"mo":["-","+"],"msub":{"mi":["e","z"]},"mi":"s"}}},{"mrow":{"msup":[{"msub":{"mi":["e","z"]},"mo":"*"},{"mn":"2","mi":"reg"}],"mo":"\u2062"},"mo":"-","msub":{"mi":["e","z"]}}]}},"mn":"2"}}}}},"br":{}},{"@attributes":{"id":"p-0282","num":"0281"},"maths":{"@attributes":{"id":"MATH-US-00031","num":"00031"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"s","mo":"=","mn":"0"},"mi":"N"},"mo":"\u2062","msup":{"mrow":{"mo":["(",")"],"mrow":{"mn":"2","mo":"\u2062","mi":"N"}},"mn":"2"}}}},"br":{}},{"@attributes":{"id":"p-0283","num":"0282"},"maths":{"@attributes":{"id":"MATH-US-00032","num":"00032"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"s","mo":"=","mn":"0"},"mi":"N"},"mo":"\u2062","msup":{"mi":"N","mn":"2"}}}}},"Examining the equation for the total count of samples processed by the oversampling method (given herein above), it can be seen that the oversampling approach could perform O(N) work on the front slice when the viewpoint is very close to the volume. The oversampling run times grow rapidly as the viewpoint is moved closer to the front of the volume. Examining the undersampling equation above, it can be seen that as the viewpoint approaches the front of the volume, the numerator approaches zero. The amount of work performed on the rear slice also approaches zero. The run times of the undersampling method decrease as the viewpoint becomes closer to the volume.","Regardless of the viewpoint geometry, the amount of work performed by the ER-Perspective algorithm of the present invention is bounded by o(N) and \u03c9(4N) per slice. Some advantages of this approach are that the upper bound on the run time of the algorithm is linear with the number of voxels and is independent of the view position, and a lower bound on the image quality achieved is also independent of the view position. Thus, a user can set the base sampling rate for the desired image quality and be sure that the sampling rate is sufficient throughout the volume for that desired image quality.","In contrast, a conventional oversampling approach provides a lower bound on the image quality yet the runtime of the algorithm may become much greater than that of the ER-Perspective method of the present invention. A conventional undersampling method provides an upper bound on the runtime for rendering, but the image quality may become much worse than the ER-Perspective approach.","Referring again to , a preferred back-to-front ER-Perspective ray-casting algorithm, in accordance with the present invention, is illustrated. The algorithm of  is shown as a pseudo-code representation and assumes a Z-major axis projection. The ER-Perspective algorithm of the present invention does not suffer from the traditional pitfalls when performing perspective projections on uniform regular grids. This unique approach runs faster than oversampling methods and produces better quality images than undersampling methods. Employing a Bartlett filter for ray merging provides an image quality improvement over a conventional box filter. The ER-Perspective algorithm is qualified by characterizing the effective filtering on the input data.","In accordance with another form of the present invention, a method is presented for rendering a large volume, wherein the volume dataset exceeds the physical single-pass capacity of the Cube-5 apparatus of the present invention. The preferred method subdivides the volume dataset into a plurality of cuboid bricks. Traversing the bricks in a predefined order preferably enables initialization of the compositing buffer of the Cube-5 apparatus with a baseplane image of a previous brick before rendering it, whereby ray path and compositing are logically extended throughout the entire volume. Information regarding the boundary between bricks is preferably re-read to insure correct sampling. Using this approach, the maximum volume size is limited only by the available intermediate baseplane storage.","In areas of the dataset where, during perspective projection, multiple voxels contribute to the same image pixel, images of equivalent quality may preferably be rendered using a level-of-detail (LOD) tree, which may be generated, for example, by combining voxels of increasing neighborhood size in a preprocessing step. While perspectively rendering a single large volume utilizing LOD, preferably only a small portion of the volume, substantially close to the viewpoint, must be read in its highest detail. The more distant portions of the volume, with respect to the viewpoint, may then be rendered from lower resolution versions of the data. Thus the frame rate and\/or dataset size is preferably increased. Since each region in the perspective algorithm of the present invention (previously described) will now be at a different LOD, there is no longer need to filter the rays between regions, but merely to redistribute them. Preferably, only one region of each LOD tree level is processed; thus, only those regions must be paged into memory.","The level-of-detail (LOD) method of the present invention may also be used for rendering scenes comprised of multiple objects at differing distances from the viewpoint. For such cases, a starting LOD is preferably selected that delivers a baseplane image of about the same size as the screen space image, thereby relating rendering time to image resolution and not to object size (i.e., scale independence).","Although back-to-front rendering is similarly contemplated by and within the scope of the present invention, the unique LOD method will be described herein in a front-to-back rendering context. Rendering front-to-back, it is preferable to start with a slab of the most detailed representation of the volume to be rendered. In a preferred method of the present invention, the thickness of the volume slab is chosen so that projected voxel distances in front and back of the slab differ by a factor of two, similar to perspective projections according to the present invention, as previously described herein. After rendering a slab, the current compositing buffer image is preferably scaled by a factor of 0.5 in the warp unit. This initializes the compositing buffer for the rendering of the next slab of half the resolution. Preferably, only one slab of each LOD actually flows through the rendering pipelines; thus, for large volumes, only those slabs must be paged into the on-board 3D memory.","It is to be appreciated that the apparatus of the present invention can also be employed to speed up off-line computations, such as generation of level-of-detail (LOD) and filtering of datasets. To generate LODs, the trilinear interpolation unit (TriLin) of the present invention preferably sets all its weights to 0.5. Once new samples become available, they are preferably subsampled and compacted into a new volume, which is the next coarser LOD. To filter a dataset, the trilinear interpolation unit again uses only 0.5 weights; this time, however, data is fed back to the beginning of the rendering pipeline without compaction. Each additional pass creates a new filtered volume with a filter kernel having one more voxel extent in every major axis direction.","For higher quality image rendering, the apparatus and methods of the present invention preferably provide the flexibility to utilize a full hardware implementation, multi-pass algorithms, and\/or a combination of the two, depending on the desired tradeoffs. The full hardware implementations and multi-pass methods preferably provide more accurate computations in two primary functional areas: filtering and interpolation.","The Cube-4 architecture, a predecessor of the present invention (Cube-5), utilizes a central difference gradient filter with only two sample points to estimate each of the x, y and z gradients at a particular location. A larger 3D filter can deliver a more accurate gradient estimate, such as a Sobel filter (which is a 3filter with weights derived from the inverse of the Manhattan distance from the center point). A straightforward hardware implementation of a 3filter, however, requires 27 multipliers and 26 adders.","The apparatus of the present invention presents an alternative to this expensive prior art approach by using symmetric convolution filters. The convolution filters can be efficiently implemented with only three multipliers and six adders, at a significant cost savings. Replication of hardware per gradient component can preferably be avoided by applying a three-pass algorithm instead. As an example,  illustrates a symmetric approximation of the x-component of the Sobel gradient filter. Within each stage, the weights are preferably applied to the nearest neighbors before summation. With reference to , if each stage operates on the output of a previous stage instead of on the raw data, the weights presented in  will effectively produce the 3symmetric approximation of the Sobel gradient filter (right side of ). Changing the x-weights to {1 w 1} will produce an approximation of a Gaussian filter instead.","The present invention contemplates higher quality rendering modes in which no additional hardware is needed, but in which the frame rate is lowered. One such example is to achieve larger neighborhood contributions to the gradient estimation by utilizing level-of-detail (LOD) information. If the central difference gradient is computed on data of the next coarser LOD, it is effectively the equivalent of employing a 6\u00d74\u00d72 filter, with 6 being the extent in the direction of the current gradient component. Since the apparatus of the present invention (i.e., Cube-5 architecture) is able to hold mip-mapped LOD representations of the data, this filter is preferably achieved with essentially no increase in hardware, beyond the simple central difference solution.","Another higher quality multi-pass rendering mode provided by the present invention, for which no additional hardware is required, is an approximation of tri-cubic interpolation, which has beneficial applications in the medical field as well as other fields. This mode enables more accurate resampling and iso-position calculation. For this, the present invention preferably decomposes a piecewise 4-voxel filter into a series of linear interpolations and extrapolations which is symmetric in every dimension, thereby allowing efficient reuse of intermediate results.","In performing higher quality rendering, it is to be appreciated that there are certain tradeoffs between using additional hardware for providing more accurate and flexible gradient estimation within the Cube-5 pipeline, as opposed to employing multiple pass algorithms. Generally, using a multiple pass algorithm requires changes in the Address Generation and Control unit (see ) of the present invention to momentarily stall the pipeline for computational purposes, while the hardware approach requires additional application specific integrated circuit (ASIC) logic and additional connections to support larger neighborhoods.","With respect to enhanced volume rendering capabilities, a preferred embodiment of the present invention supports clipping by arbitrary planes. The distance from each plane may preferably be incrementally computed using only registers and one adder per plane. In addition to conventional clipping planes which define only the positive direction as visible, the apparatus of the present invention preferably supports extracting an arbitrarily thick slice from the dataset for oblique multi-planar reformatting (MPR) by invalidating all samples lying outside a predetermined offset.","Axis-aligned cutting planes are preferably implemented by restricting the volume traversal to the cuboid of interest. Alternatively, the present invention contemplates restricting this traversal to exclude a simple cuboid from the volume (e.g., visualizing all but one octant of a volume).","In addition to clipping, the present invention further contemplates depth cueing, which modulates the color of objects to simulate, for example, atmospheric attenuation of light through a translucent medium. This phenomenon, as appreciated by those skilled in the art, is termed fog or haze when the medium also contributes some color (e.g., white or gray). To implement this feature in accordance with the present invention, normally clear regions are preferably replaced with a semi-transparent color (e.g., black for depth cueing, white for fog) by modifying the transfer function. Each final pixel is preferably further attenuated to account for the distance from the viewpoint to the surface of the volume, preferably implemented as a part of warping.","The apparatus of the present invention additionally supports rendering of super-sampled images with a preferred default super-sampling rate of two in the x and y directions, although other sampling rates are contemplated. To improve image quality further, the sampling rate along each ray can also be increased. Neither approach requires re-reading voxels from the 3D memory. The apparatus of the present invention preferably changes the volume traversal order so that voxels already residing in the buffers will be read out repeatedly. Each time they are reused, new weights are preferably utilized in the trilinear interpolation units (TriLin) of the present invention to reflect the new resampling position.","In a preferred embodiment for supersampling in the present invention, central difference gradients are computed between neighbors one distance unit apart to ensure sufficient precision. These gradients are preferably computed by taking the difference first and interpolating afterwards or, alternatively, by interpolating first and then taking the difference between neighbors k positions apart (assuming k times oversampling), and preferably not immediate neighbors. A classification stage must consider the new inters ample distances when computing a new \u03b1\u2032 value. Therefore, during super-sampling, the volume will preferably be traversed in an interleaved pattern within each slice. This essentially ensures that a translucent material (gel) keeps its accumulated opacity (RGB\u03b1 value) independent of the sampling rate. Thus, for example, for an oversampling factor of k in the z-direction, modified \u03b1\u2032 values are preferably used, where: \u03b1\u2032=1\u2212(1\u2212\u03b1).","Anisotropic datasets have different distances between samples along different axes. Thus, the gradient computation and the final two-dimensional (2D) image warp preferably require axis-dependent scaling factors. In addition, the direction in which the sight rays are being cast through the volume dataset preferably require adjustment to account for the implicit volume scaling, which occurs when storing anisotropic data in an isotropic grid. The \u03b1\u2032 value is preferably adjusted according to the direction-dependent distance d which a sight ray travels through a voxel cell. The corrected \u03b1\u2032 is \u03b1\u2032=1\u2212(1\u2212\u03b1), with the direction-dependent distance d preferably being in the range [1, \u221a{square root over (3)}].","In addition to the methods for enhancing volume rendering capabilities described herein above, the present invention further provides several unique methods for universal three-dimensional (3D) rendering, including mixing polygons and volumes, voxelization of polygons, rendering multiple overlapping volumes, performing texture mapping and accelerating image-based rendering. These methods are described in greater detail herein below.","An important aspect of the present invention is its unique ability to correctly mix geometric objects (i.e., polygons) and volumes in a single image. The apparatus of the present invention (i.e., Cube-5) preferably leverages conventional geometry hardware to render opaque and translucent polygons together with the Cube-5 volume rendering pipeline.","In a preferred method according to the present invention, to render a scene containing volumes and opaque polygons, all opaque polygons are first projected onto a Z-buffer coincident with a predefined baseplane and having sufficient resolution to match the volume sample distance. Using the Z-buffer, a determination is preferably made as to which slices of the volume are in front of the polygons for each pixel of the baseplane image. The compositing buffer is then preferably pre-loaded (i.e., initialized) with this projected RGB\u03b1Z (i.e., Z-buffer) image, representing the color and depth image of the polygons. Subsequently, the volume is rendered with z-comparison enabled in the compositing buffer. The depth values of the opaque polygons are checked to keep volume samples which are hidden by opaque polygons from contributing to the final image. Ultimately, the opaque polygons occlude the volume behind, and the volume in front correctly composites over the polygons.","In other words, the compositing buffer is pre-loaded with the z-buffer image {C, Z}, in accordance with the preferred method of the present invention, where Crepresents the value of the geometry sample and Zrepresents the depth of the geometry sample from a predetermined viewpoint. During back-to-front compositing, the resulting output pixel in the compositing buffer, C, will preferably be equal to the geometry sample value, C, when the volume sample is behind the geometry (i.e., when the depth of the sample, Z, is greater than the geometry depth, Z). Similarly, during front-to-back compositing, the samples are preferably composited using the Porter-Duff over operator, as appreciated by those skilled in the art. A more detailed discussion of the Porter-Duff a compositing rules are described, for example, in the text , by T. Porter and T. Duff, (84), vol. 18, no. 3, pp. 253-259, July 1984, which is incorporated herein by reference. Therefore, the resulting output pixel in the compositing buffer, C, will preferably be equal to the volume sample value, C, over the geometry sample value, C, when the volume sample is in front of the geometry (i.e., when the depth of the volume sample, Z, is less than the geometry depth, Z).","Translucent polygons pose a more complicated situation, since all fragments (both translucent polygon pixels and volume samples) must be drawn in topologically depth-sorted order. This is required because compositing translucent fragments with the over operator is not commutative. Therefore, polygons must be re-depth-sorted whenever the scene or viewing geometry changes. Additionally, the sorting must be topologically correct, including the handling of depth cycles.","Although there are proposed architectures which use an A-buffer to provide some hardware sorting support, implementing an A-buffer in hardware allows only limited depth complexity (i.e., number of overlapping polygons per pixel) in a single pass and is costly. A discussion of a conventional A-buffer algorithm may be found, for example, in the text -, by L. Carpenter, (84), vol. 18, no. 3, pages 103-108, July 1984.","In a preferred method, the present invention adapts polygon rendering to slice order ray casting, and synchronizes the overall rendering process on a volume slice-by-slice basis, rather than a polygon-by-polygon or pixel-by-pixel basis. The Cube-5 apparatus preferably utilizes the geometry pipeline and conventional graphics hardware to render geometric objects in thin slabs that are interleaved or dove-tailed between slices of volume samples , as illustrated in .","With reference now to , each slice of the volume is preferably sampled in planes perpendicular to the volume storage axes. The planes are drawn in depth order (e.g., using near and far clipping planes) from farthest from the eye or viewpoint  to nearest to the eye. Therefore, to mix translucent polygons with volumetric data, thin slabs of the polygons  are preferably rendered and composited in between the slices of volume samples . It is to be appreciated that the slabs  represent all of the translucent objects which lay between two consecutive slices of the volume sample planes. The boundaries of the slabs are preferably defined such that the union of all rendered slabs  neither misses nor duplicates any region (e.g., (], (], . . . , (], as shown in ). The data from the volume slices and the translucent polygonal slabs  are dove-tailed together in an alternating fashion. In this manner, the correct depth ordering of all contributing entities is preserved and use of the over operator to composite them creates correct colors in the final image pixels.","In accordance with a preferred method of the present invention, the opaque polygons are drawn first with Z-buffering. Before drawing any volume slices, the translucent polygons which lie behind the volume extent are preferably drawn over the opaque polygons using any conventional translucent polygon rendering algorithm (e.g., painters). Likewise, translucent polygons which lie in front of the volume are preferably drawn after the mixing portion of the algorithm. Polygons which lie depth-wise within the volume boundary, but to the top\/bottom\/side of the volume, are preferably drawn in slice order as if the volume slices were planes that extend to infinity cutting the translucent polygons.","OpenGL may be used to directly render the thin slabs of translucent polygonal objects. The polygons are preferably shaded using the Gouraud shading model included in OpenGL. A naive approach would be to render the complete set of translucent polygons for every slab and set the hither and yon clipping planes to cut the current thin slab of data. However, for an nvolume, there could be up to n thin slabs that must be rendered. Since a typical scene contains very few polygons which span all of the thin slabs, the present invention contemplates an alternative approach which would involve clipping the polygons to the slab boundaries and only rendering the portions of the polygons within each slab. This would substantially reduce the processing load on the polygon pipeline. However, it would require the application to clip every polygon against the two planes of each thin slab which contains that polygon.","As illustrated in , it is contemplated that the present invention may take advantage of the fact that the two clipping planes ,  are parallel to keep only the portions of the polygons which lie between the planes. While this creates fewer polygons than clipping against each plane separately, it still can increase the triangle count dramatically. The first case occurs when a triangle  intersects the thin slab, but no vertices are within the slab boundaries , . When this occurs, one vertex must be on one side of the slab and the other two vertices on the other side of the slab, thus creating a trapezoid which is decomposed into two triangles. Next, consider when one vertex of a triangle is within the slab. In one situation, a triangle  intersects the slab such that the remaining two vertices lay on the same side of the current slab, creating only one triangle. In a second situation, a triangle  intersects the slab such that the remaining two vertices lay on opposite sides of the current slab. This is a worst case situation, since it produces a pentagon, or three triangles. The final case occurs when a triangle  intersects the slab such that two vertices lie within the same slab and once again, a trapezoid is created resulting in two triangles.","In a preferred embodiment of the present invention, a bucket sorting method is applied to the translucent polygons. Whenever the viewing geometry changes, the placement of volume sample planes change their relative positions to the geometry. Therefore, the present invention preferably creates a bucket for each thin slab between two volume sample planes. All of the translucent polygons in a scene are preferably traversed and each of the polygons is placed in a bucket for each of the slabs it intersects. For example, as shown in , triangle T is placed in all six buckets since it spans all six slabs S-S. Triangle T is placed in buckets corresponding to slabs S and S, and likewise for the remaining triangles. For the example shown in , bucketing the four triangles T-T would result in twelve triangles being sent to the graphics pipeline. As a comparison, if the triangles were being clipped to the slab boundaries, twenty triangles would be sent to the graphics pipeline.","An alternative to bucketing is to create an active triangle list similar to the active edge list utilized in scan converting polygons. The triangles may be placed in the active list at the first slice they intersect and removed from the list when they no longer intersect any slices. A data structure is preferably pre-computed which indicates which slice each triangle first encountered. This preprocessing is essentially the same as for bucketing, with the exception that bucketing does not have to check for triangle removal for each slice.","One advantage of the method of the present invention is that for applications which choose to trade off image quality in order to maintain a predetermined frame rate, the number of polygons drawn decreases as the number of slices drawn for the volume decreases. This occurs because the interslice size increases as the number of volume slices decreases. The rendering rate achieved is substantially proportional to the number of polygons drawn and the number of volume samples drawn (which is proportional to the number of volume slices drawn). The image quality degradation resulting from this tradeoff affects only the volume data, similar to taking fewer samples in any volume rendering algorithm.","When mixing translucent geometries and volumes, there exist at least three options for handling two or more translucent polygons being drawn to the same pixel within one thin slab. In the first option, the polygons could be drawn in regular processing order with the over operator. While this method may produce the incorrect color, the amount of color error is limited because the polygons are still sorted by bucketing them into thin slabs.","Another method for handling two or more translucent polygons is to draw thin slabs of translucent polygons between two volume sample slices as on-the-fly voxelization. In conventional voxelization methods, when a surface is 3D scan converted into a 3D volume grid, the resolution of the grid is commonly chosen such that the size of a single voxel represents the smallest area that can be discerned by the human eye when it is rendered. In the X and Y dimensions, the polygons are drawn to screen resolution. In the Z dimension, it is assumed that the volume is being rendered with enough slices such that each volume sample also represents the smallest area that can be discerned by the human eye. Therefore, each pixel bounded by two volume slices in the Z dimension also represents this small area.","In view of the foregoing, a method, performed in accordance with one embodiment of the present invention, may be viewed as computing on-the-fly voxelization by utilizing 3D graphics hardware. Voxelization methods combine polygons into a single voxel by using one of two preferred methods. The first method is to take the max of each color channel. The second method is to take the weighted-max as",{"@attributes":{"id":"p-0322","num":"0321"},"maths":{"@attributes":{"id":"MATH-US-00033","num":"00033"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["C","v"]},"mo":"=","mfrac":{"mrow":[{"mo":["(",")"],"mrow":{"mrow":[{"msub":[{"mi":"C","mrow":{"mi":"p","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"}},{"mi":"D","mrow":{"mi":"p","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"}}],"mo":"\u2062"},{"msub":[{"mi":"C","mrow":{"mi":"p","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2"}},{"mi":"D","mrow":{"mi":"p","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2"}}],"mo":"\u2062"}],"mo":"+"}},{"mo":["(",")"],"mrow":{"msub":[{"mi":"D","mrow":{"mi":"p","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"1"}},{"mi":"D","mrow":{"mi":"p","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mn":"2"}}],"mo":"+"}}]}}}},"br":{},"sub":["p1 ","p1 ","v "],"b":["1","1"]},"The third method of drawing two or more translucent polygons to the same pixel within one thin slab may also be considered the most accurate approach. By utilizing one of the previously described methods of the present invention to perform depth sorting, such as BSP tree, proper ordering of all translucent polygons within each slab is maintained. Depth cycles are preferably handled by the BSP algorithm by splitting polygons which span a plane used in the partitioning, and eventually one of the polygons in the cycle is used as the partitioning plane.","As previously discussed, an important feature of the present Cube-5 invention is the unique ability to couple at least one geometry pipeline or engine to the Cube-5 system. In accordance with the present invention, two preferred methods of connecting one or more geometry pipelines to the claimed Cube-5 system on PC-class machines is provided, as described herein below. Both methods allow the unique mixing of opaque and\/or translucent polygons with volumetric data.","It is to be appreciated that the opaque polygons are preferably rendered such that, after projection through the volume dataset, warping creates the correct footprint on the final image. Furthermore, the Z-depth values are preferably aligned along the processing axis, so that a volume slice index may be used for the Z-depth check.","In accordance with one embodiment of the present invention, a preferred method begins by determining a major viewing axis for the current viewing direction. As illustrated in , a transformation is preferably applied to the geometry  so that the major viewing axis  is along, for example, the Z-axis. Next, the view or eye point  is moved to be along this direction, preferably by rotating the vector between the look-at point  and the eye point  by a predefined angle \u03b1 around the X-axis and an angle \u03b2 around the Y-axis. Preferably, \u03b1 and \u03b2 are always in a range between \u221245 and +45 degrees, otherwise a different baseplane would be chosen. A Z-slice shear transformation along X and Y (also known as a \u201cX and Y according to Z\u201d shear) is preferably subsequently applied to the viewing matrix as follows:",{"@attributes":{"id":"p-0327","num":"0326"},"maths":{"@attributes":{"id":"MATH-US-00034","num":"00034"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mn":"1"},{"mn":"0"},{"mrow":{"mi":["tan","\u03b1"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"1"},{"mrow":{"mi":["tan","\u03b2"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}}},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"}]}]}},"mo":"\u2003"}}}},"With this geometry, when the opaque polygons are drawn, the polygon footprints are \u201cprewarped\u201d so that the warping operation at the end of Cube-5 rendering creates correct polygons in the final image. Additionally, the Z-depths computed are preferably proportional to the distances along the processing axis. It is possible (e.g., if all opaque geometry fits within the volume extents) to set the hither and yon clipping planes to the edges of the volume and, if the precision of the depth buffer is the same, the depths computed are exactly the volume slice indexes for depth checking. Otherwise, a simple scaling must be applied when the computed depths are utilized by the volume rendering system. Light positions should be considered when using this method, however, as the shearing may not move the lights to the correct location.","The thin slices of translucent polygons preferably align geometrically with their 3D positions in space. Preferably, the eye point is first aligned as previously described. Next, in order to keep the objects from projecting all the way to the final image plane, the geometry is preferably translated such that the center of the current thin slab is at the Z=0 plane prior to shearing. Clipping planes allow only the current thin slab to be rendered and the projection plane is set to be within the two volume slices which border that region with, for example, glOrtho (glFrustum for Perspective).","Important to comprehending the present invention is to understand the organization of frame buffer design and composting buffer design. As illustrated in , the Cube-5 volume rendering pipeline  of the present invention preferably utilizes a tightly coupled on-chip SRAM buffer , termed a composting buffer, to hold the partially composited rays as a volume is processed in slice order. This architecture exploits the regular processing sequence inherent in slice order rendering. Specifically, each slice of the volume  is preferably processed in the same order as the previous, left-most voxel to right-most voxel of each row, and bottom-most row to top-most row of each slice (possibly with some skewing). In this way, the SRAM composting buffer  becomes a simple FIFO queue having a length equal to the size of a slice. The SRAM queue is preferably 32 bits wide to hold 8-bit fixed point RGB\u03b1 values (called coxels). Each pipeline  preferably reads a coxel from the front of the queue and writes a coxel to the rear of the queue for each clock cycle.","In contrast, with reference now to , conventional PC-class geometry pipelines  utilize an external DRAM frame buffer , which stores the RGB\u03b1 color values and Z-depth values for each pixel. This frame buffer  must support random access, since polygon rendering does not enjoy the regular access ordering inherent in slice-order volume rendering. Normal polygon rendering produces triangles on a screen of average between 10 and 50 pixels. Therefore, the DRAM memory is organized to maximize access to areas of the screen of this size.","As shown in , when the 3D texture mapping method of the present invention is implemented on geometry pipelines , volume slices  perpendicular to the screen are texture mapped through the volume. The per-vertex geometry calculations for the volume slices  are easily achievable with any level graphics hardware. However, the requirement to support random access to both the texture memory  and frame buffer  limits the performance of this approach to the fill rate achievable with a DRAM frame buffer.","Very high end surface graphics systems typically utilize massive parallelism in the fragment processing section  of the polygon pipeline. This, coupled with a highly distributed frame buffer, allow increased fill rate performance.","In  there is shown one embodiment for connecting a geometry pipeline  to the Cube-5 volume rendering system , according to the present invention. As illustrated in , the SRAM composting buffer is preferably removed from inside the Cube-5 pipeline  and replaced with an external DRAM frame buffer . Rather than organizing the DRAM frame buffer  as in conventional polygon engines, the memory in the frame buffer of the present invention is preferably organized so that it is specifically optimized for volume rendering. The frame buffer  is also preferably accessible from a 3D graphics pipeline  to allow mixing of polygonal data  with volumes.","With continued reference to , the dual use frame buffer  preferably connects the two pipelines , . In a preferred method, to render a scene with both opaque and translucent polygons and also volume data, the geometry pipeline  first renders all opaque polygons with Z-depth. The volume slices, stored in volume memory , and thin slabs of translucent polygons are then rendered in an alternating (e.g., dovetailing) fashion-volume slices by the Cube-5 pipeline  and translucent polygons by the graphics pipeline  (opaque polygons may also be rendered with the same dovetailing algorithm, but with increased demand on the graphics pipeline).","Z-depth checking is preferably utilized to insure correct hidden object removal and blending is set in both pipelines to correctly composite the samples and fragments. The geometry engine  preferably performs the final baseplane warp required by the Cube-5 system of the present invention.","The design of the DRAM buffer  is critical to achieve, for example, the 503 Million samples per second required for 30 Hz rendering of 256volume datasets. Therefore, it is helpful to first create a DRAM buffer for the Cube-5 rendering pipeline itself, before discussing connecting the rendering pipeline to a graphics pipeline. The volume rendering system of the present invention is preferably comprised of multiple Cube-5 pipelines. In each rendering pipeline, at every clock cycle, a coxel (composting buffer element consisting of RGB\u03b1) is read from the SRAM composite buffer FIFO and blended with an appropriate composting equation. The new coxel is then placed at the rear of the FIFO. In a preferred embodiment, the structure of a coxel is changed to contain 32 bits of color, 8 for each RGB\u03b1 and 32 bits of Z-depth information, 24+8-bit stencil. This configuration is required to handle Z-depth checking in the composting stage. Assuming that opaque polygon rendering is completed before any volume rendering begins, the 32 bits of Z-depth\/stencil information is read, but not re-written. Therefore, for every clock cycle, each Cube-5 pipeline needs to read 8 bytes of coxel data and write back 4 bytes.","Preferably, the rendering pipeline of the present invention utilizes memory chips with a word size of 16 bits. Using this configuration, four words must be read by each pipeline every cycle and two words must be written. To do this would require six 16-bit memory interfaces per pipeline. An emerging technology in synchronous DRAM (SDRAM) chips, which the present invention may avail itself, is known as double data rate (DDR), which reads\/writes data at both the rising and falling edges of the clock. Using DDR SDRAMs, the present invention can utilize two 16-bit memory interfaces for reading 64 bits of data per clock and one 16-bit memory interface for writing 32 bits per clock, for a total of three 16-bit memory interfaces per pipeline.","With reference now to , since a read and write must be performed every clock cycle in order to keep the pipeline full, the present invention preferably reads from one set of fine buffer chips (e.g., set A)  and writes to another set (e.g., set B) . The Cube-5 system contemplates reading from set A  and writing to set B  for a complete slice of the volume, and then swapping for the next slice. With this approach, however, each frame buffer chip set would have to be large enough to hold the complete frame buffer. Furthermore, the polygon engine would have to be instructed as to which set is the current set. Therefore, in a preferred embodiment, the present invention alternates reading and writing between set A  and set B  within a frame and buffers the processed coxels from the read set until it becomes the write set. Since every memory access must be a burst, each burst actually lasts four clock cycles and reads\/writes four coxels (i.e., eight words) with 16-bit DDR DRAM chips. The Cube-5 system preferably cycles through all 4 banks to keep the memory bandwidth saturated before writing the new RBG\u03b1 values back. For this reason, there is preferably a 16-coxel FIFO queue  (four coxels for each of four banks) that the newly composited RBG\u03b1 portions of the coxels are stored in.","There are many different possible configurations for the number of pipelines etc. in the Cube-5 volume rendering system of the present invention. An example for a case of four parallel pipelines creating 12 total memory interfaces will now be discussed with reference to . As shown in , each pipeline contains one read interface  to the Z-depth\/stencil portion  of the frame buffer and two read\/write interfaces  and  to set A  and set B , respectively, of the RGB\u03b1 portion of the frame buffer. To render a 256volume at 30 Hz, each of the four pipelines process 125 million voxels per second. Therefore, a 133 MHZ clock is utilized for the chip and the SDRAM. The mapping of the frame buffer pixels onto the memory chips is critical to the performance. It must match exactly the processing order of the Cube-5 pipelines and the parallel access by four pipelines substantially simultaneously. It is assumed that the skewed memory access of the Cube-5 architecture is \u201cun-skewed\u201d so that the volume samples are in order from left to right across each scanline in groups of four, since it is easier to follow in the explanations. The design can be extended to skewed memory, although the geometry pipeline and screen refresh system must be aware of the additional skewing.",{"@attributes":{"id":"p-0341","num":"0340"},"figref":"FIG. 34","b":["274","276","278","274"],"br":[{},{},{},{},{},{},{},{}],"in-line-formulae":[{},{},{},{},{},{},{},{}],"i":["RGB","RGB"],"sup":["3","3","3 "]},"This extra bandwidth is not siting idle. The screen must be refreshed from the data in the frame buffer. If we assume a 1280\u00d71024 screen resolution with 60 Hz refresh rate and that all four RGB\u03b1 bytes are read from the frame buffer (since our burst mode access retrieves them anyway), then\n\n1280\u00d71024\u00d760 Hz\u00d74 bytes=300 Mbytes\n\nare read from the frame buffer per second. Only the RGB\u03b1 portion of the frame buffer is required for refresh. Therefore, the refresh data is read from eight chips. It is sufficient to perform ten data burst reads\/writes (depending on set A or set B) to each chip followed by one read of data for refresh. This distribution of memory accesses provides the refresh hardware with a consistent (although bursty) stream of data. The Cube-5 pipelines also contain a small percentage of excess cycles, and thus will not lose the ability to achieve 30 Hz 256rendering when the memory sub-system is temporarily stalled for refresh.\n","An alternative approach to connecting a graphics pipeline to the Cube-5 volume rendering pipeline, in accordance with a preferred embodiment of the present invention, will now be described. This preferred connection approach keeps both the graphics pipeline and the volume rendering pipeline working at all times and merges the data in the SRAM compositing buffer inside the Cube-5 chip. At any given time, the volume rendering pipeline is composting the current volume slice with the previous thin slab of polygon data over the compositing buffer and the graphics pipeline is rendering the next thin slab of translucent polygons.","The method described herein still utilizes the unique approach of dovetailing volume slices and thin slabs of translucent polygonal data, as previously described herein above. In a first step, all opaque polygons are projected onto a Z-buffer coincident with the baseplane (e.g., the volume face most parallel to the screen). Next, the projected RGB\u03b1Z image is loaded into the composting buffer of the volume rendering pipeline. Subsequently, the volume is rendered with a Z-comparison enabled in the composting stage. The thin slabs of translucent polygons are preferably rendered by the geometry pipeline, and their corresponding RGB\u03b1 data is sent to the volume pipeline of the present invention to be blended into the SRAM composting buffer within the volume pipeline.","Preferably, the composting stage of the volume rendering accelerator is modified to composite two layers (one volume and one translucent polygon) per step, thus not delaying the volume rendering process. This requires the addition of some extra logic. The straightforward formula for performing a double composition of a volume sample v over a translucent pixel fragment p over the old coxel c would require four additions and four multiplies in five stages:\n\n\u03b1\u03b1(1\u2212\u03b1)](1\u2212\u03b1)\n\nHowever, employing simple math allows the double composition to be calculated with four additions and two multiples in six stages with the following formula (some of the calculations are re-used):\n\n=(+()\u03b1)+[\u2212()\u03b1)]\u03b1\n\nAs appreciated by one skilled in the art, the hardware designer would choose the option more desirable for a given implementation (i.e., less logic and more stages, or fewer stages and more logic).\n","Consider the amount of data transferred for a 256volume. There are preferably 255 slabs plus one buffer in front of the volume and one buffer behind the volume. Each of these 257 slabs contains 256 KB (256pixels of RGB\u03b1) of data. This equates to 64 MB being read from the Same buffer and transferred between the two sub-systems each frame. To achieve a 30 Hz frame rate would require a bandwidth of 1.9 GB per second. While this much data could be transferred with sufficiently wide channels, it must also be read from the frame buffer. It would be virtually impossible to read this much data without changing the organization of the current DRAM frame buffers. Additionally, the frame buffer must be cleared 257 times per frame.","To solve this bandwidth challenge, the present invention preferably uses run-length encoding (RLE) of the blank pixels. With this method, each scanline is encoded separately and a \u201crun-of-zeros\u201d is encoded as four zeros (RGB\u03b1) followed by the length of the run. Since typically only a small percentage of the polygons in a scene are translucent, the translucent polygon slabs will be relatively sparse. Run-length-encoding just the blank pixels in these thin slabs results in over 99% reduction in the required bandwidth. Preferably, the method of the present invention utilizes RLE on 2D images of sparse translucent polygons to save on bandwidth.","Using this preferred method requires adding hardware to the Cube-5 system of the present invention. Specifically, additional hardware may be included in the volume rendering pipeline that can decode the RLE input stream and create RGB\u03b1 fragments. However, since these fragments are utilized by the volume pipeline in a regular order, it is preferable to decode the input stream using a double buffer to synchronize the two pipelines. Every clock cycle, a value is output from the decoding hardware. If the volume rendering machine has multiple pipelines (as most current designs do) the decoding hardware is preferably replicated for each pipeline so that they can keep up with pixel demand.","Likewise, RLE hardware at the originating end connected to the geometry pipeline may encode the data in real-time before sending it to the volume pipeline. However, 1.9 GB per second access to the frame buffer would still be required to read all the thin slabs of translucent polygons and clear the frame buffer 257 times per frame. Therefore, a separate frame buffer is preferably employed which stores the data directly in RLE format. Since the thin slabs of translucent data are very sparse, more time is spent clearing and reading than is spent rasterizing. An RLE buffer, while generally not optimal for rasterization, is well suited for both clearing and reading the data. For example, to clear an RLE frame buffer requires merely storing a single run of zeros (in five bytes) for each scanline, instead of writing an entire 256frame buffer.","To minimize the impact on the current geometry pipelines, the RLE frame buffer is preferably implemented using the emerging technology of embedded DRAM and connecting it in parallel to the normal frame buffer. This differs from conventional encoding algorithms which typically assume that the data was given in physical order. Triangle rasterization, however, does not guarantee any ordering of the fragments. Therefore, the apparatus of the present invention must be able to randomly insert an RGB\u03b1 value into an RLE scanline of data.",{"@attributes":{"id":"p-0351","num":"0350"},"figref":["FIG. 35","FIG. 35","FIG. 35","FIG. 36","FIG. 36","FIG. 35"],"b":["280","282","284","286","288","280","282","290","1","3","2"]},"By utilizing an embedded DRAM the present invention takes advantage of the extremely high bandwidth available when processing occurs on the memory chip. The processing is simple enough to be implemented in the DRAM manufacturing process. For example, for a 1280\u00d71024 frame buffer, the maximum amount of memory required is 50 Mbits. This fits onto eDRAM dies with room for over 3 million gates for the encoding hardware.",{"@attributes":{"id":"p-0353","num":"0352"},"figref":"FIG. 37","b":["242","252","292","242","252","252","242","258"]},"In accordance with a preferred embodiment of the present invention, a method is provided to incrementally voxelize triangles into a volumetric dataset with pre-filtering, thereby generating an accurate multivalued voxelization. Multivalued voxelization allows direct volume rendering with intermixed geometry, accurate multiresolution representations, and efficient antialiasing. Prior voxelization methods either computed only a binary voxelization or inefficiently computed a multivalued voxelization. The method, in accordance with the present invention, preferably develops incremental equations to quickly determine which filter function to compute for each voxel value. This preferred method, which is described in greater detail herein below, requires eight additions per voxel of the triangle bounding box.","To avoid image aliasing the present invention preferably employs pre-filtering, in which scalar-valued voxels are used to represent the percentage of spatial occupancy of a voxel, an extension of the two-dimensional line anti-aliasing method conventionally known (, by S. Gupta and R F. Sproull, (81), vol. 15, no. 3, pp. 1-5, August 1981). It has also been shown that the optimal volume sampling filter for central difference gradient estimation is a one-dimensional oriented box filter perpendicular to the surface. The method of the present invention preferably utilizes this filter which is a simple linear function of the distance from the triangle.","Conventional graphics hardware only rasterizes points, lines, and triangles, with higher order primitives expressed as combinations of these basic primitives. Therefore, it is preferable to voxelize only triangles because all other primitives can be expressed in terms of triangles. Polygon meshes, spline surfaces, spheres, cylinders, and others can be subdivided into triangles for voxelization. Points and lines are special cases of triangles and can similarly be voxelized by the present algorithm. To voxelize solid objects, the boundary of the object is preferably voxelized as a set of triangles. The interior of the object is then filled using a volumetric filing procedure.","As appreciated by those skilled in the art, edge functions are linear expressions that maintain a distance from an edge by efficient incremental arithmetic. The methods of the present invention extend this concept into three dimensions and apply antialiasing during the scan conversion of volumetric triangles.","In essence, the general idea of the triangle voxelization method of the present invention is to voxelize a triangle by scanning a bounding box of the triangle in raster order. For each voxel in the bounding box, a filter equation is preferably evaluated and the result is stored in memory. The value of the equation is a linear function of the distance from the triangle. The result is preferably stored using a fuzzy algebraic union operator, namely, the max operator.","With reference now to , there is shown a density profile of an oriented box filter along a line  from the center of a solid primitive  outward, perpendicular to the surface . The width of the filter is defined as W. The inclusion of a voxel in the fuzzy set varies between zero and one, inclusive, determined by the value of the oriented box filter. The surface  of the primitive  is assumed to lie on the 0.5 density isosurface. Therefore, when voxelizing a solid primitive , as in , the density profile varies from one inside the primitive to zero outside the primitive, and varies smoothly at the edge. For a surface primitive, such as the triangle  shown in , the density is preferably one on the surface and drops off linearly to zero at distance W from the surface. Although the present invention similarly contemplates the voxelization of solids, the voxelization of surfaces will be described herein.","With continued reference to , it has been determined that the optimum value for filter width W is 2\u221a{square root over (3)} voxel units (see e.g., , by M. {hacek over (S)}r\u00e1mek and A. Kaufman, 1998 , pp. 111-118, IEEE, October 1998). For shading, the normal is preferably estimated by computing the central difference gradient at the 0.5 isosurface. Because the overall width of the central difference filter is at most 2\u221a{square root over (3)} units, a correct gradient is found on the 0.5 density isosurface. The thickness of the triangle  may be defined as T. Normally, T can be zero, unless thick surfaces are desired. By thresholding at 0.5 density, a 6-tunnel-free set of voxels is generated when W\u22671. This property is useful for volumetric filling (e.g., in order to generate solid objects).","All voxels with non-zero values for a triangle are preferably within a bounding box which is S=W+T\/2 voxel units larger in all directions than a tight bounding box. Therefore, the first step of the present method preferably determines a tight bound for the triangle , then inflates it in all directions by S voxel units and rounds outward to the nearest voxels.","As illustrated in , the area surrounding a triangle defined by vertices C, Cand Cmay be divided into seven regions (e.g., R through R) which must be treated separately. In a preferred method of the present invention, each candidate voxel is tested for inclusion within the seven regions, then filtered with a different equation for each region. In the interior region R of the triangle, the value of the oriented box filter is simply proportional to the distance from the plane of the triangle. In regions along the edges of the triangle, R, R, R, the value of the filter is preferably proportional to the distance from the edge of the triangle. In regions at the corners of the triangle, R, R, R, the value of the filter is preferably proportional to the distance from the corner of the triangle.","With continued reference to , the regions R-R are preferably distinguished by their distance from seven planes. The first plane a is preferably coplanar with the triangle and its normal vector a points outward from the page. The next three planes b, c, and d preferably have normal vectors b, c, and d respectively and pass through the corner vertices C, C, and Cof the triangle, respectively. The final three planes e, f, and g are preferably perpendicular to the triangle and parallel to the edges; their respective normal vectors, e, f, and g, lie in the plane of the triangle and point inward so that a positive distance from all three planes defines region R. All of the plane coefficients are normalized so that the length of the normal is one, except for normal vectors b, c, and d which are normalized so that their length is equal to the inverse of their respective edge lengths. In that manner, the computed distance from the plane varies from zero to one along the valid length of the edge.","For any planar surface, the distance of any point from the surface can be computed using the plane equation coefficients:",{"@attributes":{"id":"p-0365","num":"0364"},"maths":{"@attributes":{"id":"MATH-US-00035","num":"00035"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"Dist","mo":"=","mfrac":{"mrow":{"mi":["Ax","By","Cz","D"],"mo":["+","+","+"]},"msqrt":{"mrow":{"msup":[{"mi":"A","mn":"2"},{"mi":"B","mn":"2"},{"mi":"C","mn":"2"}],"mo":["+","+"]}}}}}},"br":[{},{},{}],"in-line-formulae":[{},{}],"i":"Dist=Ax+By+Cz+D"},{"@attributes":{"id":"p-0366","num":"0365"},"maths":{"@attributes":{"id":"MATH-US-00036","num":"00036"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":{"mrow":{"mrow":[{"msup":{"mi":["D","\u2032"]},"mo":"\u2062","mi":"ist"},{"mrow":{"mi":"A","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":"x","mo":"-","mn":"1"}}},"mo":["+","+","+"],"mi":["By","Cz","D"]}],"mo":"="}}},{"mtd":{"mrow":{"mo":"=","mrow":{"mi":["Ax","By","Cz","D","A"],"mo":["+","+","+","+"]}}}},{"mtd":{"mrow":{"mo":"=","mrow":{"mi":["Dist","A"],"mo":"+"}}}}]}}},"br":[{},{},{}],"sub":["x","y","z"],"in-line-formulae":[{},{}],"i":"Dist\u2032=Dist +r\u2299[A,B,C]","figref":"FIG. 41"},"The Y-step is more complicated than the X-step because it not only steps one unit in the Y direction, but it also steps back multiple units in the X direction. Consider, as an analogy, the operation of a typewriter which glides back to the left margin of the paper and advances the line with one push of the return key. Similarly, the Z-step combines stepping back in both the X and Y directions and stepping forward one unit in the Z direction. This simple pre-processing step ensures efficient stepping throughout the entire volume. If numerical approximation issues arise, then it is possible to store the distance value at the start of each inner loop and restore it at the end, thereby minimizing numerical creep due to roundoff error in the inner loops.","For multivalued voxelization, seven plane distances are required. Therefore, seven additions are required per voxel to compute the plane distances. Other computations per voxel may include incrementing the loop index, comparisons to determine the appropriate region and, if necessary, computations to determine the density.","Referring again to , in region R the density value of a voxel is preferably computed with the box filter oriented perpendicular to plane a. Given a distance DistA from plane a, the density value V is computed using:",{"@attributes":{"id":"p-0370","num":"0369"},"maths":{"@attributes":{"id":"MATH-US-00037","num":"00037"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"V","mo":"=","mrow":{"mn":"1","mo":"-","mfrac":{"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mi":"DistA"},{"mi":"T","mo":"\/","mn":"2"}],"mo":"-"},"mi":"W"}}}}},"br":{},"b":"2"},{"@attributes":{"id":"p-0371","num":"0370"},"maths":{"@attributes":{"id":"MATH-US-00038","num":"00038"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"V","mo":"=","mrow":{"mn":"1","mo":"-","mfrac":{"mrow":{"msqrt":{"mrow":{"msup":[{"mi":"DistA","mn":"2"},{"mi":"DistB","mn":"2"}],"mo":"+"}},"mo":"-","mrow":{"mi":"T","mo":"\/","mn":"2"}},"mi":"W"}}}}},"br":{},"b":["3","4","5"],"sub":"1"},{"@attributes":{"id":"p-0372","num":"0371"},"maths":{"@attributes":{"id":"MATH-US-00039","num":"00039"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mi":"V","mo":"=","mrow":{"mn":"1","mo":"-","mfrac":{"mrow":{"msqrt":{"mrow":{"msup":[{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["C","x"],"mn":"1"},"mo":"-","mi":"x"}},"mn":"2"},{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["C","y"],"mn":"1"},"mo":"-","mi":"y"}},"mn":"2"},{"mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["C","z"],"mn":"1"},"mo":"-","mi":"z"}},"mn":"2"}],"mo":["+","+"]}},"mo":"-","mrow":{"mi":"T","mo":"\/","mn":"2"}},"mi":"W"}}}}},"br":{},"b":["6","7"],"sub":["2 ","3 "]},"At the shared edge of adjacent triangles, it is preferable to avoid discontinuities or cracks. Fortunately, the oriented box filter guarantees accurate filtering of the edges for any polyhedra, provided the union of the voxelized surfaces is correctly computed. The union operator can be defined over multivalued density values V(x) with V\u2261max(V(x), V(x)). Other Boolean operators are available. However, the max operator preserves the correct oriented box filter value at shared edges, and is therefore preferred.","The implication of using max in the method of the present invention is that the current voxel value must be read from memory, then possibly modified and written back into memory. Therefore, a maximum of two memory cycles are required per voxel.","The efficiency of the algorithm of the present invention may be further increased by limiting the amount of unnecessary computation because the bounding box often contains a higher percentage of voxels unaffected by the triangle than affected by it. The bounding box can be made tighter by recursively subdividing the triangle when edge lengths exceed a predetermined constant.","To visualize intermixed polygons and volumes, the polygons are preferably voxelized into the target volume and rendered in a single pass. If the polygons move with respect to the volume, then voxelization should occur into a copy of the original volume so as not to corrupt the data. The multivalued voxelized polygon voxels may be tagged to distinguish them from volume data. In this manner, polygons can be colored and shaded separately from other data.","The preferred triangle voxelization algorithm described above is efficiently implemented in the distributed pipelines of the Cube-5 volume rendering system of the present invention. This algorithm adds just a small amount of hardware to the existing pipelines and performs accurate multivalued voxelization at interactive rates. One important advantage of the claimed Cube-5 volume rendering algorithm is that the volume data is accessed coherently in a deterministic order. This feature allows orderly scanning of a bounding box for this algorithm.","In , a preferred embodiment of the overall voxelization pipeline is shown, in accordance with the present invention. If on-the-fly voxelization is important, the system of the present invention may preferably include separate pipelines for volume rendering and voxelization. If voxelization can occur in a separate pass, then these volume rendering and voxelization pipelines may be combined, with the voxelization pipeline re-using most of the hardware from the system, in a similar manner as setup is performed on the host system for 2D rasterization.","With reference to , in the first hardware stage  of the pipeline, the distances from the seven planes are preferably computed. Seven simple distance units are allocated with four registers for each of the seven planes. Preferably, one register holds the current distance from the plane and the other three registers hold the increments for the X-, Y-, and Z-steps.  shows the distance computation nit  for one of the seven planes, formed in accordance with a preferred embodiment of the present invention. This distance computation unit  may be included as part of the distance calculation stage  of the pipeline (see ). The other six units can be essentially identical in design, but hold different values. During each clock cycle of voxelization, the pipeline preferably steps in either teh X, Y, or Z direction (i.e., preforms an X-Step , Y-Step , or Z-Step ), thereby updating the current distance according to the direction of movement. The hardware of looping through the volume is already present in the volume rendering pipeline and is therefore re-used here to scan the bounding box of the triangle.","After the seven plane distances are calculated, the resulting values preferably flow down the pipeline. As shown in , the next pipeline stage  then preferably determines in which region the current voxel resides. In a preferred embodiment of the region selection stage , only seven comparators are needed to determine the outcome of the truth table, due to the mutual exclusion of some cases. For instance, in , from the negative (lower) side of plane b, it is not necessary to test the distances from plane f or g, depending on the value of the distance from plane e.","With continued reference to , after the region has been determined, the next pipeline stage  computes the filter function. The filter calculation stage  of the pipeline is preferably only activated if the current voxel is within S voxel units of the triangle. Otherwise, the current voxel is essentially unaffected by the triangle and different regions require different calculations, ranging from a simple linear expression to a complex Pythagorean distance evaluation. Since hardware ideally must handle all cases equally well, it is preferred that such hardware be able to perform a square root approximation by means of a limited resolution look up table (LUT). However, the range of inputs and outputs is small, and therefore the size of the required LUT will be small. Furthermore, the Cube-5 hardware of the present invention has several LUTs available for volume rendering which can be re-used for voxelization. Instead of providing three separate units to compute the expression V=1\u2212(\u221a{square root over (Dist)}\u2212T\/2)\/W, it is more efficient to roll all the calculations into one LUT. In this case, the input is Dist, defined over [0,12], and the output is the density value V in the range [0,1].","Due to the mutual exclusion of the seven regions, it is sufficient to provide hardware for only the most complex filter calculation. With reference to , the most complex calculation is the corner distance computation of regions R, R, and R which, in a preferred embodiment, requires five adders and three multipliers, in addition to the square root LUT previously mentioned. The line distance computations in regions R, R, and R are simpler, requiring only one adder, two multipliers and the square root LUT. Region R requires a single multiply to obtain the distance squared, which is the required input to the LUT.","Referring again to , the final stage  of the pipeline preferably computes the max operation using the current voxel value and the computed density estimate. In a preferred embodiment of the present invention, the max operator is simply a comparator attached to a multiplexor such that the greater of the two values is written back to memory. Since most voxels in the bounding box are not close enough to the triangle to be affected by it, memory bandwidth will be saved by only reading the necessary voxels. Further bandwidth savings may be achieved by only writing back to memory those voxels that change the current voxel value. Since there is some latency between requesting and receiving word from memory, the voxel is preferably fetched as soon as possible in the pipeline and the results queued until the memory is received. The final stage  is write-back to memory, which can be buffered without worry of dependencies.","The present invention thus far has been described outside the context of skewing, which complicates the traversal. However, the present invention contemplates building skewing into the Y- and Z-step distance update values-Skewing also adds more complexities to the Cube-5 hardware of the present invention. Specifically, when a left-most voxel moves one unit in the Y-direction, placing it outside of the bounding box, the pipeline actually takes p\u22121 steps in the X direction to keep the voxel within the bounding box. Similarly, when the left-most voxel moves one step in the Z direction, it also moves one step in the negative X direction, which is handled in the same way as before. Therefore, the apparatus of the present invention is preferably adapted to perform skewing by adding fourteen (14) more registers and corresponding logic to determine when the pipeline is currently processing the left-most voxel.","Pre-filtering, which may be performed in combination with the voxelization methods of the present invention, can be used to optimally generate a series of volumes of different resolutions. This technique is useful for rendering images of different sizes; the size of the volume is preferably chosen to correspond to the size of the final image. In this manner, aliasing is avoided at all image resolutions and no unnecessary work is performed rendering parts of a scene not visible at the image scale.","Pre-filtering can additionally be used to model motion blur. For example, as an object sweeps past a camera, it sweeps out a complex volume during the time the shutter is open, causing motion blur. To accurately render motion blur, conventional rendering techniques render multiple images and blend them into a single image. However, this approach is very slow. With pre-filtering, the present invention performs the sweeping operation once, during voxelization, so that motion blur can be rendered in the same time as regular volume rendering. This method works well, particularly for certain cases where the motion is constant (e.g., the same direction and\/or rotation). For example, consider a helicopter blade which spins at a constant speed during flight. For example, to voxelize the blade spinning at the rate of 5 Hz for an animation frame rate of 30 Hz, the blade sweeps out an arc of",{"@attributes":{"id":"p-0387","num":"0386"},"maths":{"@attributes":{"id":"MATH-US-00040","num":"00040"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mn":["5","30"]},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"mn":"2","mo":"\u2062","mi":"\u03c0"}}}}},"br":{}},"When multiple volumetric objects overlap, the projected image of the volumes becomes quite complex. Consider, for example, a scene where smoke rises up through a cloud. Clearly, the two volumetric objects cannot be rendered separately with the images combined in the final frame. Therefore, in a preferred method, performed in accordance with one form of the present invention, multiple objects are combined into one object for a final rendering pass to create the resulting image.","When two or more objects occupy the same space, the colors from each object are preferably modulated together at each sample location along a projected sight ray. Therefore, it is preferred that each object be classified and shaded prior to being combined, followed by color modulation. If, alternatively, voxel data were combined first, a new transfer function would be required for each possible combination. This latter approach is therefore not preferred.","In accordance with one form of the present invention, a preferred method for mixing multiple overlapping volumes resamples all but the first object in the z-dimension of the first object so that slices of each object become interlaced. This includes a classification, a shading and a transformation which aligns all objects. Object transformations include translation and scaling, preferably performed by the apparatus of the present invention using nearest neighbor connections, and rotation, which is preferably performed using the rotation methods of the present invention previously described herein above.","For scenes containing objects which will not change position or orientation with respect to each other, the present invention contemplates optimizations such as high-level scene graph compilation that can preferably be employed. For instance, static objects are preferably combined once and stored for subsequent rendering, while non-static objects are re-combined each time they are moved with respect to the other objects.","Texture mapping is a widely used technique to simulate high-quality image effects, such as surface details, and even lighting and shadows. In general terms, texture mapping involves mapping a two-dimensional (2D) image onto a three-dimensional (3D) surface. Texture mapping occurs while geometric objects are rasterized onto the screen. The (x, y) pixel coordinates are preferably mapped into (u, v) texture coordinates and an RGB\u03b1 value is returned as the color value to use for that pixel on the screen.","There are basically two processes involved in texture mapping: a mapping from (x, y) coordinates to (u, v) coordinates, and a look-up into the image of what RGB\u03b1 value corresponds to a given (u, v) coordinate. The mapping from (x, y) to (u, v) coordinates preferably involves simple matrix multiplication, as appreciated by those skilled in the art. However, the look-up into the image of the (u, v) coordinate to return an RGB\u03b1 value is complex. The very large scale integration (VLSI) hardware requirements for the texture lookup commonly consume large portions of today's graphics boards, at a significant cost. This is primarily due to the fact that (u, v) coordinates rarely map directly to a discrete image coordinate, called a texel. Therefore, the neighboring RGB\u03b1 values are preferably linearly interpolated to produce the RGB\u03b1 value at the exact (u, v) coordinate.","Two-dimensional (2D) interpolations are generally sufficient if the pixel does not cover more than one texel. However, if the mapping produces pixel coverages greater than one texel, artifacts are introduced into the image using the 2D interpolation method. To avoid costly texel combining operations, a technique termed Mip-Mapping may be utilized by conventional graphics pipelines. Mip-Mapping basically consists of storing multiple levels-of-detail (LOD) of an image. Then, when an (x, y) pixel is mapped to a (u, v) texel, the appropriate Mip-Map level texels are chosen so that the pixel is smaller than the texels. A more accurate method is to look-up the four neighborhood texels from both the higher level and lower level of detail texel images and then perform a trilinear interpolation on all eight texels to compute the appropriate RGB\u03b1 value for the pixel.","Texture mapping hardware from conventional graphics pipelines has been used to accelerate volume rendering and has been the subject of such texts as , by K Akeley, (93), 27:109-116, August 1993, and , by B. Cabral, N. Cam and J. Foran, , pp. 91-98, October 1994. This conventional approach, however, neither achieves the cost-performance nor supports the various functionalities (e.g., shading) of the present invention. Furthermore, using known prior art methods, texture mapping is unscalable without data replication, often employs two-dimensional (2D) rather than three-dimensional (3D) interpolation, downloads datasets slowly, and\/or does not support real-time four-dimensional (4D) input.","In accordance with a preferred form of the present invention, described previously herein above, the Cube-5 apparatus is combined with a conventional geometry engine via the geometry input\/output bus ,  (see ). Preferably, the rendering pipeline(s) of the present invention are utilized to perform the texture look-up function, while the geometry engine is used for mapping (x, y) pixel coordinates to (u, v) texture coordinates. In simple terms, once combined with the Cube-5 apparatus, the responsibility of the geometry engine is essentially to rasterize triangles, while the apparatus of the present invention preferably provides the high performance interpolation engine for texture mapping. To perform texture look-ups on the apparatus of the present invention, texel data is preferably loaded into 3D memory included within the Cube-5 unit(s).  illustrate an example of how 32 bits of texel data for a 2\u00d72 neighborhood are preferably arranged in a 2subcube of 16-bit voxels.","Another important advantage of the present invention is the ability to enhance image-based rendering. In general, image-based rendering methods render complex scenes from arbitrary viewpoints based on a finite set of images of that scene. Two similar image-based rendering methods, known by those skilled in the art, which use four-dimensional (4D) interpolation without requiring the depth information of the source images are light field rendering and Lumigraph. The high-performance interpolation engine of the present invention may be used to accelerate these two techniques.",{"@attributes":{"id":"p-0398","num":"0397"},"figref":"FIG. 44","b":["322","320","324","326","322","324","320","322","15"]},"Performing lookups for each projection plane ray usually causes random access into the st images. Therefore, in accordance with a preferred method of the present invention, st images are accessed in object order, which is more appropriately adapted for use with the apparatus of the present invention since the Cube-5 apparatus allows reading of each st image pixel only once. With continued reference to , for each quadrilateral  in the uv plane (e.g., abcd), its projections on the four st planes (e.g., corresponding to abcd) preferably determine which four tile regions  contribute to the final image. All st tile regions  are then preferably assembled into four images and are perspectively projected onto the projection plane . The final image is subsequently formed by bilinear interpolation among the four projected images. Interpolation weights are preferably determined by the intersection between the original ray and the uv plane .","A method and apparatus for generating an approximate perspective projection using a parallel projection will now be described. Many applications for volume rendering require perspective projections to generate usable images. Generally a perspective projection is required when the viewpoint is located close to or inside the object being rendered. For example, in the colonscopy application, the user navigates through a virtual colon. The rendering of an interior view of a pipe-like structure requires perspective projection.","Currently the Cube 5 and Cube-4 architectures discussed above can generate true perspective projections. However, Cube-4 cannot deliver the perspective projections in real-time. The \u201cCube-4\u201d architecture is described in U.S. Pat. No. 5,847,711 to Kaufman et al. and has already been incorporated herein by reference. Some of the teachings of the Cube-4 architecture have been incorporated into a volume rendering PC board developed by Mitsubishi Electric known as VolumePro. A complete description of the VolumePro board is found in -, by H. Pfister et al., (99), pages 251-260, August 1999, which is incorporated herein by reference. A limitation of the VolumePro board is that perspective projections can not be generated. Since the method and apparatus for generating an approximate perspective projection using a parallel projection can be supported by VolumePro a brief description of VolumePro is provided below.","VolumePro is the first real-time volume rendering accelerator for consumer PCs. The ray casting algorithm is implemented in a slice-order method in accordance with the teachings of the Cube-4 technology. Since, trilinear interpolation, gradient estimation, classification, and per-sample Phong illumination are computed in hardware, high quality images are guaranteed. The VolumePro ray casting algorithm provides predictable memory access patterns to maximize memory bandwidth efficiency to standard SDRAM devices. In this design, the parallel rays  are cast through the volume  along the view direction  onto a baseplane  which is subsequently warped onto the final image. Referring now to , the process takes advantage of the texture mapping on conventional 3D graphics cards to perform the final warp into the image .","Performing volume rendering on a PC with the VolumePro hardware has advantages over existing methods even beyond the cost of a PC versus a high end workstation. First, since hardware acceleration is used, it is much faster than software methods. Secondly, because the board computes per sample illumination, the images are higher quality than 3D texture map based solutions. Unfortunately as noted above, the VolumePro board does not quite meet the stringent requirements for some applications. Since the board is only capable of parallel projections and not perspective projections, projections of \u201ctubes\u201d result in \u201cdonuts\u201d being displayed in the final image. In addition, medical datasets\u2014typically 512\u00d7512\u00d7(100-400)\u2014are too large for the board to render in real time. Fortunately, in endoscopic views of tubular structures large portions of the dataset are obscured. This feature can be used to cull the dataset down to a size which the VolumePro board can handle.","Referring now to , the method of generating an approximate perspective projection using a parallel projection generally includes dividing the volume  into a plurality of slabs . Each slab  is a connected set of volume slices along the major viewing direction and is ray cast separately using a parallel projection. The slab images are then texture mapped onto the screen with perspective projection thus providing a perspective impression to the viewer. The rendering and texturing of each slab can be done either sequentially or nonsequentially. In sequential processing, an initial slab is rendered and then textured onto the screen. The remaining slabs are then also rendered and textured onto the screen. In nonsequential processing, all of the slabs are initially rendered prior to the texturing. After all of the slabs are rendered, texturing is performed for each slab. In both sequentially or nonsequentially processing, the order of the processing is not critical. However, preferably the slabs are processed either back to front or front to back to maintain a logical order. The preferred embodiments of the invention will now be described in connection with the commercially available VolumePro board.","In using VolumePro to implement the method of the invention, as noted above initially multiple \u201cthin slabs\u201d  of the dataset are rendered into separate baseplane images. Each slab  represents a portion of the volume data and is created with parallel projections along the view direction. These images are then composited together by texturing them onto planes  placed in 3D space at their respective slab positions and utilize a perspective projection  on the 3D graphics card. The slabs  can be aligned either parallel to the final image plane as shown in , or orthogonal to one of the three volume axes as shown in .","The portion of the subvolume which contributes to the current baseplane image is preferably limited by one of two methods with the VolumePro board. The first method involves utilizing the thick cut plane feature and the second utilizes active subvolumes. The thick cut planes can be positioned and oriented at arbitrary places, while active subvolumes are generally required to be axis aligned and aligned at multiple-32 boundaries (integer positions which are perfectly divisible by 32). Cutplane rendering is implemented on the VolumePro board by simply adjusting the \u201calpha\u201d value of a sample just before the compositing stage in the pipeline. Referring now to , all the voxels  in the entire volume are processed including a portion located outside the thick cut plane. Cutplane rendering is preferred for image-aligned slabs  as shown in . Utilizing the thick cut plane feature of VolumePro is preferred because aliasing can be reduced by setting a falloff on the edges of the cutplane. This feature specifies a width to the transition between full and zero opacity creating a fuzzy cutplane instead of binary as shown in . When the slabs  are aligned orthogonal to one of the three volume axes, the active subvolume method can provide faster rendering due to the fact that it actually limits the voxels that are processed by the board. Since the slabs  must also be a multiple of 32 voxels thick to use the active subvolume method, thick cutplanes are also normally used to further limit the contributing portion of the dataset to the desired thickness.","Referring now to , an algorithm for the image aligned method is shown for use with the VolumePro board. Steps  through  initialize the cutplane parameters. Steps  through  start the VolumePro card for the current slab. Step  waits for the first render call to finish and step  transfers the baseplane image to the 3D graphics card. Steps  through  place the baseplane image at its position in 3D space and projects\/texture-warps it to the screen (blending occurs on the hardware). Step  adjusts the cutplane position. The loop in steps  through  repeats through the entire view space which is the volume dataset intersected with the view frustum. If axis aligned cutting planes are utilized, the computation of the cutplane equation in step  changes to determining the baseplane most parallel to the image plane and the transformation of the baseplane in 3D space in step  changes to transform by both the cutplane position and orientation instead of just depth.","The individual slabs can be defined by simply spacing them out evenly over the volume so that each slab has substantially the same slab thickness as illustrated in Step  of . As discussed above with respect to the ER-Perspective method, it was shown that rays diverge as an exponential function of the distance from the camera. This finding can be used to adapt the number of rays to remain close to the underlying voxel density, defining regions that were twice as thick as the previous region towards the camera. Preferably the slab thicknesses are similarly defined to minimize the artifacts in the image, without creating too many passes for the algorithm.","Since the method of generating an approximate perspective projection is a multi-pass algorithm the rendering is slowed down. Therefore, it is often worth the extra effort to perform tight bounding box culling on the portion of the dataset which contributes to the image. In all culling algorithms (including polygon and other volume methods) there is a computation price paid to calculate the portion of the data which must be processed. The tradeoff between the amount of time spent each frame performing culling operations and the rendering speed increase that the culling produces is used to determine if culling is worthwhile. Since the multipass perspective method incurs a high rendering cost per volume sample, culling algorithms generally payoff to a higher degree. Culling algorithms are mostly application specific, and therefore should be analyzed in a per solution basis. A representative example of a culling algorithm for endoscopic views of medical CT scans is presented below.","Virtual colonoscopy is a non-invasive procedure for mass-screening of patients for small polyps which are the precursors of colo-rectal cancer. In general, this procedure consists of three steps. First, the patient's colon is cleansed (either physically, or electronically) and inflated with air in a way similar to that of optical colonoscopy. Second, while the patient is holding his or her breath, a helical CT scan of the patient's abdomen is taken, capturing a sequence of 2D slices which covers the entire range of the colon. The colon is then viewed either by automatic planned navigation or interactive navigation. While the discussion below focusses on virtual colonoscopy, various other endoscopic procedures are possible using the method to view tubular organs in the human body. For example, Bronchial tubes, the esophagus, arteries, stomach, and the like can all be examined endoscopically on a low cost PC.","Typical virtual colonoscopy datasets are 512\u00d7512\u00d7400. A normal VolumePro parallel projection rendering of this dataset would result in only 5 frames per second. While this is close to interactive frame rates, the multipass perspective algorithm lowers this below 1 frame per second and would result in an unusable system. As noted above, culling portions of the dataset which do not contribute to the final image should be considered for each application. Referring now to , since virtual colonoscopy creates images of twisted tubular structures, only a small portion of the dataset is required for the given camera position because the colon quickly turns outside the view frustum. The portions of the colon that are deeper within the view frustum (higher in the picture) are invisible since the colon wall is rendered as opaque. This feature is used to cull off these portions and render only a small subset of the volume to contain all the visible data from the current viewpoint.","Since the caching and queuing on the VolumePro board requires cuboid shaped volumes of multiple-32 boundaries, a cuboid like this is created of all visible voxels for each viewpoint. An apparent method for this would be to prepocess the volume and store, for each possible camera position the visible subvolume. Then when rendering, just lookup and utilize the correct subvolume. For colonoscopy, the camera can move anywhere within the colon dataset. If subvolumes for all voxels within the colon were stored, the subvolume could be used by looking up the \u201cnearest\u201d voxel determined from the current camera location. Unfortunately, there are 2-3 million voxels within a typical colon. This would create a large table to lookup the correct subvolume. Additionally, due to the topology of the colon, many voxels (especially those close to each other) utilize the exact same subvolume. Consider the case shown in . Due to the twisted nature of the colon , only the shaded portion  is visible from point X. However, when the multiple-32 bounding box  is placed around this region, it extends past the boundaries of the shaded region . For this reason, other portions of the colon  would utilize the same multiple- bounding box . For example, both points B and C have visible regions completely enclosed by the same bounding box . The lines shown in , alpha and beta, are the lines where the visibility changes. This occurs because, for points past these lines (e.g., A or D), the visible region extends past the current bounding box. All viewpoints within the large area between the two dashed lines utilize the same multiple-32 subvolume.","Referring now to , the subvolumes and regions are created in a pre-processing step by walking through the colon  from one end to the other and using a greedy algorithm. The first subvolume which includes all visible voxels from the end of the colon  is initially created. The colon  is then walked through searching for the first point where we can \u201csee\u201d outside the current multiple-32 subvolume. At this point, a new one is created which encompasses all voxels visible from the new viewpoint. The colon is then walked again. This is repeated the entire way through the colon. When switching from one subvolume to the next, this algorithm always results in overlap as shown in the example bounding boxes in . While this does not create the minimum number of subvolumes, the number created is easily manageable. For example, for the colon in ,  subvolumes were created with this method. The size of the subvolume rendered is drastically reduced, with the average being two mega-voxels, or the same order of magnitude as 128. Of course the subvolumes are not all exactly cubic (example subvolumes include 160\u00d7192\u00d796 or 160\u00d7128\u00d7128 or 128\u00d796\u00d796). The rendering speed depends on the total count of voxels.","This algorithm provides a good tradeoff between computational effort per frame and resulting rendering speedup. To compute which subvolume to use, a search of an array with 49 distances is performed. A binary search on this array completes essentially instantly. The resulting cull operation reduces the number of voxels to consider from 94 Meg to approx 2 Meg. The pre-processing step using the greedy algorithm to create the subvolumes does not effect the rendering rates. Therefore, a 97.8% reduction in the amount of data needed to be accessed is achieved during rendering with a preprocessing step combined with an essentially unmeasurable computation step during rendering. Since the multipass algorithm accesses each voxel multiple times, the rendering performance increase produced by this culling operation is even more magnified.","The apparatus of the invention is an approximating unit configured with the teachings of the method of the invention. The addressing unit can be, for example, any suitable computer, processor (e.g., digital signal processor, microprocessor, etc.), microcontroller, or circuit designed to be incorporated into a volume rendering system.","Referring now to , a ray tracing architecture in accordance with the present invention is now described. This is a modified embodiment of the Cube 5 architecture designed to work within the structure of a hierarchical memory with programmable processing units  attached to the lowest level of the memory. The processing paradigm has been used before and is described in -, by M. Pharr et al., 97, pages 101-108, August 1997. In the ray tracing architecture in accordance with the present invention, the scheduling of the processing is refined and applied in a different manner to graphics problems.","A memory hierarchy is used to support flexible volume rendering and large volumes. The memory hierarchy is a cascade of memories with increasing bandwidth and decreasing size toward the Cube 5 units. The data is organized in hierarchical blocks (cuboid subvolumes) as described in ---, by R. Oborne et al., , pages 131-138, August 1997. The hierarchical blocks allow transfers between levels appropriate for the bandwidth and latency for those levels. Normal volume data is read-only and thus can be discarded upon replacement in the lower levels of the memory. However, some blocks including: ray data, reconstruction volume data, and global illumination voxel data are read-write and thus need to be written back up the hierarchy.","Referring again to , a block diagram of a preferred embodiment of the implementation of the memory hierarchy with programmable processing element is shown interfacing the system. A single board  is connected to the system bus . The board  preferably contains one or more chips having at least one processing unit  and DRAM memory , at least one programmable processing element , and a set of standard memory devices . The chips are preferably embedded-DRAM (eDRAM) chips and the standard memory devices  are preferably SDRAM. The eDRAM chips contain one or more processing units  as well as DRAM  memory organized in one or more banks. The programmable processing element  can also be an eDRAM chip. Preferably a memory controller  is provided to interface the level of memory which are external to the DRAM memory . Preferably the processing units  have a very high bandwidth to the local memory inside the chip.","The ray tracing architecture takes advantage of the high bandwidth to quickly process the data currently loaded in the eDRAM device. Normal datasets will not fit within the memory  within the eDRAM chips. The SDRAM memories  are utilized to store the data until the processing units  within the eDRAM are ready to utilize it. Large datasets will not fit within a plurality of SDRAM devices that can fit onto one card. The main memory  of the system is utilized to store the data until the board is ready to work on it. This is a three tiered memory hierarchy that optimizes the locality of reference.","The memory hierarchy enables sufficient rendering of large datasets which are much larger then Level 1 of the Cube 5 memory hierarchy\u2014the double slice voxel FIFO  shown in . Space leaping and early ray termination can be employed to take advantage of the condition that the entire dataset generally does not contribute to the final image due to empty or opaque data regions These and interframe coherence allow the working set of blocks to remain resident in the lower hierarchy levels, providing higher bandwidth while accessing the data. Local connections between spatially coherent cells allow high-bandwidth communication of ray information instead of the usual up-over-down communication that occurs within the memory hierarchy.","The processing in the ray tracing architecture does not follow a strict sequence as found in prior art volume rendering accelerators. Instead, the work to be performed is grouped into 3D blocks. The 3D blocks are then processed (e.g. rays cast or traced, or segmentation performed) according to an order where the block which will provide the most results while being the closest to the eDRAM memory  is processed next.","Referring now to , a 2D top-down view of a typical 3D scene to be rendered is shown divided into cells  by a scheduling grid . All volume  and polygonal  data are distributed among the scheduling grid cells  which they intersect. Rays  are cast into the scheduling grid  and stored in queues .","Referring now to , the processing element  of the ray tracing architecture includes a scheduler  and dispatcher  that communicates between a buffer (FIFO) . The scheduler  and dispatcher  have knowledge of the scheduling grid  and the associated list of ray queues . The scheduler  determines which blocks to process and in which order. The scheduler  selects the best block based on the current state of ray queues  and the current state of the memories according to a heuristic metric. The scheduler  stores a list of blocks to process in the FIFO buffer . The dispatcher  transfers blocks to individual processing units  and controls the movement of sampled and geometry data among the three levels of the memory hierarchy\u2014Level 1 (eDRAM) , Level 2 (SDRAM) , and Level 3 (main) . In this way, it is assured that the correct data are available when a processing unit  begins to process a block.","The cells  are scheduled for processing in an order the minimize the total processing timebased on the status of the rays  and ray queues . Further, the scheduler  considers that data distribution throughout the memory hierarchy as well as the contribution of each cell  toward completion of the processing.","Traditionally, scheduling can be performed in one of two ways, either geometrically, for deterministic processing such as raycasting, or heuristically for non-deterministic algorithms such as ray tracing. Examples, of heuristic methods are greedy algorithms such as scheduling the next cell with the most work, or statistical probability methods such as ratios of ray-queue length to count of non-transparent objects as described in -, by M. Pharr et al., 97, pages 101-108, August 1997.","Prior attempts have been made to design an algorithm which utilizes inter-frame coherence to schedule more efficiently. A dependent graph for each frame showing which cells send rays to which other cells was built. For the next frame, the graph built from the previous frame to guide the new cell processing order was utilized. Unfortunately, for ray tracing the dependency graph becomes \u201clocally complete\u201d. This means that any cell depends on every single one of its neighbors, and is thus connected to every neighbor. Since rays cannot jump over cells, each cell is connected only to its neighbors, but is connected to all of its neighbors. This is what \u201clocally complete\u201d means. The graph is not \u201ccomplete\u201d because that means every node is connected to every other node. In addition, the \u201clocally complete\u201d graph is not much use for aiding in the scheduling of cells since a selection cannot be made as to which cell should be scheduled before it's neighbors. The dependency graph does not contain the information needed to schedule cells because there is no order to the dependency relations.","Referring now to , a method known as ray forest for scheduling cells based on inter-frame ordered dependency relations is now described. A ray forest  is a group of trees, one tree  for each ray , which represents ordered information about how rays propagate through the scheduling grid . Since rays  in a ray tracing application span multiple child rays, the data structure associated with each ray  is a tree  where each node  represents a ray  traversing from one cell  to another, and leaves represent cells where rays terminate and place their contribution into the resulting image. There are three different actions that could occur when a ray  is processed in a cell :\n\n","In the ray forest method, each scheduling grid cell appears in multiple nodes, and a single cell can occur in the same tree multiple times. For example, consider the case where a ray bounces back and forth between two reflective objects in alternating cells. Referring now to , RAY \u201cA\u201d enters cell number  and traverses through cell number  to interact with an object . A first reflected ray  bounces back through cell number  into cell number  and out through cell number . A second reflected ray  bounces back through cell number  and out through cell number  towards a light  illuminating the scene. The tree  labeled RAY \u201cA\u201d in FIG. C has a data structure representing the path of RAY \u201cA.\u201d RAY \u201cB\u201d also illustrates these concepts in .","Referring now to , to utilize the ray forest method to schedule calls, the top k nodes of all ray trees are considered. Heuristic algorithms such as the greedy or statistical methods mentioned above determine the next cell to process. When a cell is processed, all the rays waiting to enter that cell are processed. The trees which represent these rays are \u201cpopped\u201d. This means that the top node is taken off these trees. If the node is a \u201ctwig\u201d, the tree remains the same, if the node has multiple children, each child \u201cbranch\u201d becomes a new tree in the forest representing all the new rays which were spawned.","Since k nodes at the top of each tree are looked at, \u201cfuture\u201d information is acquired about what cells will soon be processed and an attempt to not remove them from the lower levels of the memory hierarchy right before they are about to be processed is made. The number of levels k that are considered, is a tradeoff on accuracy of the scheduling and runtime of the scheduling portion of the algorithm.","The forest of trees will only represent what actually happens to rays in the new frame if no rendering parameters (viewpoint, transfer function, lights, etc,) change since ray forest information from the previous frame is used. Although this is not the normal case, the change from one frame to the next is generally considered to be minimal. Inter frame coherence takes advantage of these minimal changes to estimate the best scheduling for the next frame. However, since there is some change, the ray forest must be able to deal with the rays not acting exactly like they did the previous frame.","Texture mapping is widely used technique to stimulate high-quality image effects, such as surface details, lighting and shadows. The memory requirements for texturing consume large portions of today's graphics boards and their cost.","The cost-performance of polygon engines is improved by supplying the texture mapping functionality within the hierarchical memory architecture. In this combined architecture, the polygon engine's only responsibility is to rasterize triangles, while Cube 5 will perform the texture lookups. Rays are initialized with the data from the rasterized polygonal image. This would include the (u,v) texture coordinates and a texture index. In this way deferred texturing can be accomplished so that only visible pixels are texture mapped reducing the texture image accesses.","Another advantage of the programmable processing units  to perform the texturing operations are that higher-quality anti-aliased texture methods (such as EWA) can be performed when such quality is desired. Deferred texturing lowers the cost of utilizing such higher quality methods. Additionally, the system coherently accesses texture images through the memory hierarchy. Further, programmable deferred shading (such as Renderman shaders or procedural textures) is possible by storing the required parameters during rasterization.","The system also supports ray tracing mixture of polygons and multiple volumes using the scheduling grid cells to reorder the processing to memory coherent chunks. Applications which do not require the image quality produced by ray tracing polygons, can utilize polygon rasterization hardware. External polygon rasterization is used to allow much larger polygon datasets to be rendered at much faster frame rates. The projected RGB\u03b1Z image is used to initialize the ray endpoints. Subsequently, the volume is rendered terminating the rays at the proper depth. This leverages existing geometry hardware to correctly render, in real-time, opaque polygons in harmony with volume rendering.","An architecture known as GI-Cube and methods for accelerating volumetric global illumination and standard direct volume rendering will now be described. The GI-Cube architecture shown in  can accelerate a variety of algorithms including:\n\n","In its most basic form GI-Cube is a standard volume rendering accelerator that produces images of comparable quality to Cube-4 and VolumePro. The GI-Cube architecture is part of Cube-5 architecture discussed above. The basic pipeline of GI-Cube takes volume samples along a ray using trilinear sampling of the density and gradient, performs some per-sample shading operation, and composites the shaded sample onto the ray. GI-Cube radically departs from the aforementioned architectures by generally processing rays in image order thus permitting great flexibility (e.g., perspective projection, global illumination). Preferably the processing order is not strictly an image order, but a hybrid order. That is, groups of rays are spatially partitioned into blocks, then blocks are processed one or several in parallel at a time. Rays are passed between blocks (and sometimes processors) at boundaries. At a coarse level, the processing is image order. That is, individual rays can be submitted at any time during the computation. At a medium level, the processing is object order and all the rays within a block are processed together. At a fine level the processing is image order and samples are taken along each ray in round robin fashion using a cache to exploit coherence among rays.","A Nvolume dataset is organized into cubic blocks of size nprior to rendering. The data may be stored on disk blocked or else assembled into blocks on the fly. In the examples discussed herein N=256 and n=32. The blocks are distributed among p multiple processors according to a function. For the best load balance, N is related to pn by some integer. Three possible distributions are illustrated in , , and . Referring now to , the first option uses a simple slab partition to cut the volume into p axis aligned slabs  with one slab assigned to each processor for multiprocessing which minimizes inter-processor communication at the expense of load balancing. Referring now to , the second option uses a repeating slab partition to cut the volume into slabs  of n voxels thick and assigns slab i to processor i mod p for multiprocessing which increases inter-processor communication but improves load balance. However, certain viewing directions such as the image plane oriented parallel to the slabs do not balance well. Referring now to , the third option uses a skewed block slab partition that cuts the volume into cubic blocks  and assigns cubic block (i, j, k) to processor (i+j+k) mod p for multiprocessing has the best load balance but the most inter-processor communication. Rays may pass between blocks three processors away, while the first two options always communicate locally. None of these schemes is necessary in the uniprocessor case.","To render an image, a set of image rays is generated on the interface to the GI-Cube board (described below) and clipped with the volume extents. Each ray is a data structure containing some, all, or more of the following: the (u, v) image coordinate, contribution to the image, starting point, direction, color, opacity, lifetime, generation, interaction value, and type. A typical ray data structure is illustrated in , this one comprising 32 bytes, although other arrangements and sizes are possible. Preferably rays are created by treating the intersection coordinates as texture coordinates and utilizing an incremental texture mapping scan conversion approach. The volume intersection coordinates can be generated by scan converting the six faces of the volume. Cut planes are implemented using one additional polygon scan conversion per plane. Rays are then bucketed into queues representing the volume blocks. Some portion of the queues is loaded onto the fixed size hardware queues.","Each hardware processor (pipeline)  selects a queue which contributes most toward completion. Two options include:\n\n","Preferably a volume cache  large enough to hold an entire block is used to capitalize on spatial coherence among rays. To trilinearly sample (i.e., perform a linear interpolation in three dimensions) the first ray, eight neighboring voxels need to be fetched into the cache. The direct mapped, eight-way interleaved volume cache  provides the ability to read out an entire trilinear neighborhood of voxels in each cycle. If the next ray is spatially coherent, it generally can share some (perhaps four) of the fetched voxels depending on the inter-ray spacing. Assuming a ray and sampling density of one ray sample per voxel, then processing the entire block generates only (n+1)cache misses although the total bandwidth for trilinear interpolation is eight times that amount. The supersampling along the ray increases coherence and the second ray sample in a trilinear neighborhood has a 100% hit ratio. While there are some stalls (i.e., pauses waiting for the cache to fill) at the start of processing a block, the number of voxels requested from the main volume memory never exceeds the block size. The volume cache  is configured to hold one extra slice of voxels in each dimension to account for the overlap of the trilinear neighborhood.","In addition to containing 12 bits of density, the voxels contain gradient information, a tag field for segmentation, and irradiance data which is unused for standard volume rendering as illustrated in . The gradient is pre-computed and quantized into discrete angular bins similar too the Vizard II PCI board described in -by M. Meissner et al., , pages 61-67, August 1998. However, certain datasets could yield improved results with vector quantization. The eight neighboring voxels are read from the volume cache , decoded, and interpolated. The density, three gradient components, and possibly three color components are trilinearly interpolated, and the tag field is zero-order interpolated (i.e., nearest neighbor).","The final sample is passed down the pipeline  for shading and compositing. If no color information is supplied, a transfer function is used to convert the density into color and opacity. Otherwise the transfer function only supplies the opacity. The material tag is used to provide additional dimensionality (e.g., useful coexistence of sampled and scanned datasets often requires separate tailored opacity transfer functions). The inter-sample distance d is multiplied by a random number r selected between \u00bd and 1 to provide jittering as a compromise between noise and aliasing. The opacity is modulated accordingly: \u03b1\u2032=1\u2212(1\u2212\u03b1).","Local shading is preferably performed by utilizing a reflectance map. While this technique is limited to distant light sources only, the results are adequate for most visualizations. The pipeline  next performs compositing using the over operator and stores the result back into the ray data structure which travels down the pipeline . All the while, new rays have been accepted into the top of the pipeline  resulting in the production of one new ray sample (on different rays) with each hardware cycle, neglecting any stalls due to cache misses.","The ray is next advanced by distance dr. The new start position determines the appropriate queue for the ray. First, the appropriate processor  is selected. If the ray now belongs to a neighboring processor  (see ), the ray data structure is queued for horizontal communication to the appropriate neighboring processor . If the ray has exited the volume or reached full opacity, it is queued for vertical communication with the board interface. If the ray remains in the same processor , it is further scrutinized to determine the appropriate block for subsequent queuing.","As rays are received by the board interlaced, they are assembled into a composite image. Sub-pixel sampling can be used to improve the quality of the image. In this case, sub-pixel coordinates are stored in the generated rays and the composite image is accumulated by appropriate filtering. Once complete, the final image is transferred to main memory for display on the frame buffer.","Space leaping and early ray termination accelerate rendering of most datasets. Early may termination is employed to halt processing when the ray reaches some opacity threshold. An empty flag is associated with each block because some blocks may contain no visible data. A special part of the pipeline  computes the nearest edge intersection of each ray. Based on the ray direction, axial distances to the pertinent three block faces are computed. The actual distance of the intersection is computed by dividing by the appropriate component of the ray direction. If the empty flag is set, the ray is advanced by the minimum of the three distances, but at least dr. Note that some blocks may be rendered invisible by the current transfer function although they contain nonzero data. The driver software monitors to detect this condition and set empty flags as necessary which can be done asynchronously after editing the transfer function.","A method of volume rendering with volumetric global illumination will now be described in accordance with the present invention. A number of changes are required to the above algorithm to implement volumetric global illumination. First, rays must be generalized to be either lighting rays or rendering rays. The primary difference between them is that lighting rays distribute energy along the ray while rendering rays accumulate energy. In the present invention, energy is distributed trilinearly (splatted) to the irradiance field of voxels along the ray. Second, a two-pass (bidirectional) methodology is adopted in which energy is distributed from the light sources in the first pass, and irradiance energy is gathered toward the eye point in the second pass. Third, scattering of rays are permitted within the volume (e.g., reflections, caustics).","In the first pass of global illumination, light energy is allocated among a number of rays and then traced through the scene. Instead of generating a ray from a point light source to each of the Nvoxels, rays are only fired to the face voxels of the volume while allowing these rays to automatically sample the interior voxels. Up to 6 kNrays sample the surface of the volume, where k is a super-sampling factor. A point-to-finite-area form factor based on the distance r to the light source is used to compute the energy of the light source Edistributed among the surface voxel rays:",{"@attributes":{"id":"p-0450","num":"0457"},"maths":{"@attributes":{"id":"MATH-US-00041","num":"00041"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["E","v"]},"mo":"=","mfrac":{"mrow":[{"msub":{"mi":"E","mn":"1"},"mo":["\u2062","\u2062","\u2062"],"mi":["cos","\u03b8"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}},{"mi":["k","\u03c0"],"mo":["\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}}],"msup":{"mi":"r","mn":"2"}}]}}}},"br":{}},"Area light sources are handled slightly differently. For each of the surface voxels, k random points on the area light source surface are selected. The energy is determined using the above equation multiplied by cos \u03c6, where \u03c6 is the angle between the inward volume surface normal direction and the light source normal at the random point on the light source. These lighting rays are queued on blocks and scheduled in the first pass just like rendering rays.","As the lighting rays travel through the volume they deposit energy to voxels along the ray. The reduction in radiance along a differential distance dx is given by \u03ba(x)dx where \u03ba(x) is the coefficient of extinction at point x and \u03bais the sum of the coefficients of absorption \u03baand scattering \u03ba. \u03ba(x) is determined by K\u03b1\u2032(x), where Kis a constant defined per material tag, and therefore \u03ba(x)=(1\u2212K)\u03b1\u2032(x). At each sample point along the ray, the energy Estored  in the density field of the ray data structure is split into scattered energy E=EK\u03b1\u2032(x) , absorbed energy E=E(1\u2212K)\u03b1\u2032(x) , and the transmitted final ray energy E\u2032=E(1\u2212\u03b1\u2032(x))  as shown in . Part of the ray energy is eventually stored in the volume data as a view independent estimate of the irradiance.","Two modes of global illumination are available, low albedo and high albedo. In the low albedo mode, optically thin objects (e.g., light fog) are lighted with a small number of light rays (actually bundles of rays). In the high albedo mode many rays are used to stochastically sample the complex transport space of high albedo objects. In low albedo mode, light bundles distribute part of their energy at all samples with nonzero opacity, similar to the absorption suppression method of Pattanaik et al., which is described in , by Pattanaik et al., 4(3):133-152 July-September 1993. In this mode, the exiting bundle energy is E\u2032 and the absorbed energy Eis trilinearly distributed (splatted) to the irradiance fields of the eight neighboring voxels. The ray direction is only scattered when the accumulated ray opacity computed incrementally with \u03b11\u2212(1\u2212\u03b1)(1\u2212\u03b1\u2032(x)) reaches some uniform random value which is constant per ray and is stored in the \u201cinteraction\u201d field of the ray format. In this way, the light bundle energy is continuously distributed to the voxels along the way.","In high albedo mode, scattering is an important source of illumination so many light rays are necessary to sufficiently sample the illumination transport equations. In this mode, rays model photons which carry an indivisible amount of energy which may scatter (bounce) at many difference locations before finally being absorbed. As such, they only deliver their energy once during a photon capture event (absorption). As in low albedo mode, rays interact with the medium when the accumulated opacity reaches the interaction value. However, rays either scatter or absorb depending on the scattering albedo \u03ba\/\u03baand a uniform random value. If scattering is selected, the ray direction is modified based on the material's BRDF and the accumulated opacity is set to zero. The ray then continues in a new direction having gained a generation. Whenever the photon is absorbed, the full energy Eis splatted to the irradiance fields of the neighboring eight voxels. The two methods attempt to achieve the same distribution of energy in two different ways which are tuned to the characteristics of the medium. With a large number of photons, the law of large numbers succeeds in generating a continuous distribution of illumination. The high albedo mode can also be used for shooting a large number of rendering rays. In this case, color is not accumulated continuously along the ray, but only when the ray is absorbed.","Both lighting rays and rendering rays scatter according to the phase function or bidirectional scattering function (BSDF) of the material. The tag of each voxel determines the material type. Among other things, materials define the color, the scattering constant, and the BSDF. The specification of the BSDF is flexible. Certain BSDFs are easily specified in hardware (e.g., isotropic scattering) and others (e.g., Fresnel, refraction, point sampled automobile paint) are more suitable for software implementation. Complex BSDFs are trapped by the hardware and redirected to software for further processing. The hardware supports specular reflection, dull reflection, dull scattering, isotropic scattering, and ideal diffuse reflection. All other BSDFs intercept the ray processing in hardware and pass it to the software for processing. After the software scatters the ray, it re-queues the ray on the appropriate hardware queue for further processing. Note that both lighting and rendering rays can be scattered.","Rays no longer travel in coherent groups as in orthographic volume rendering because of the apparent randomness introduced by perspective projection, the global illumination lighting method, and scattering. The block reordering and scheduling algorithm automatically attempts to maximize coherence among computations.","Referring now to , the architecture of the GI-Cube will now be described. The architecture is designed to render 256volumes in real time (30 Hz) using a single PCI board . Global illumination and other algorithms are accelerated to interactive rates, depending on the desired quality of results. The flexibility of the generalized ray tracing approach and the pipelined hardware makes this all possible.","The board  is composed of three major components. The first component is a digital signal processor (DSP)  which acts as an interface to the board and a controller of the individual processors. It handles loading the dataset, generating lighting and viewing rays, controlling the operation and I\/O of the processors, assembling the final image, and reporting the results back to the host over the PCI interface . It is directly connected to an SDRAM  for scratch memory storage and the frame buffer.","The second component is a set of hardware pipelines  called block processors as shown in . Each block processor  follows the general layout of . Each pipeline  is designed to take one sample along a ray in each hardware cycle, barring stalls due to cache misses.","The block processors  includes a queue sorter  to maintain and sort a group of fixed size hardware ray queues . As new rays are accepted from either the DSP  or neighbor processors , they must be bucketed into queues  depending on the starting point of the ray. Simple addition, shift, and modulo operations are sufficient to select the appropriate queue . Given a ray start position (x,y,z), block size of 32, volume size of 256, and the simple slab volume distribution scheme of , the internal queue number q is determined using C shift notation by:\n\n\u2032=(5)mod 2\n\n5\n\nz\u2032=z>>5\n\n=(6)+(3)+\n\nDue to the simplicity of queue selection and the potential bottleneck, the bucketing operation is over-clocked by a factor of two.\n","Embedded DRAM (eDRAM) is used to maintain ray data in the queues  since the data access is highly regular, the amount of storage is large, and the required bandwidth is high. A fixed number and size of ray queues  simplifies the bookkeeping allowing constant time access to active and sorted queues  in every cycle. For our reference design with 256voxels, blocks of , and four processors , each processor has 128 queues. With each queue  having a length of 256 rays and width of 32 bytes, the total eDRAM memory on a four processor ASIC is 4 MB. Because the sorting operation is over-clocked, up to two rays need to be written into queues  at the same time. For that reason, each queue  is implemented as a separate eDRAM so multiple queues  can be written simultaneously, unless of course both rays belong in the same queue . At the same time, the active ray queue supplies new rays to the pipeline  at the rate of one ray per cycle.","One issue which arises with fixed sized ray queues is exhausting available space. This can happen when rays converge at a point (near a light source, near the camera, or with focused caustics). When this occurs, overflow rays are returned to the DSP  over the ray bus . This does not generally hamper throughput since overflow occur when there is too much work in the ray queue . The only detriment is the consumption of resources on the DSP .","Another responsibility of the processors is to determine the active ray queue. Each ray queue  is assigned a scalar importance based on one of the following criteria: 1. The queue with the most rays; and 2. The queue with the most contribution to the image. Each of these criteria can be incrementally maintained by simply adding any incoming ray importance and subtracting any outgoing ray importance. A pipelined insertion sorter  as shown in  is used to sort these importances in hardware. Alternatively a priority heap could also be used.","The pipelined insertion sorter  having a comparison buffer  and a selected buffer . Both buffers ,  having linear storage arrangement for storing queue numbers and importance. The pipelined insertion sorter  inserts each modified importance at the top and allows it to filter down through the ranks incrementally. When a modified queue appears at the top, it immediately wipes out its old location in the ranks. That way, only one copy ever appears in the list. Each rank contains two items: the selected and the comparison. Items from above are always first placed in the comparison buffer . Then at each cycle, the two buffers ,  are compared and the larger is moved to the selected buffer . The smaller is moved to the comparison buffer  of the next lower rank. The active queue is always processed until it is emptied, so it must remain at the top rank until it becomes zero. Therefore, the importance of the active queue is set higher than the scalar importance of the remaining ray queues. Preferably the importance of the active queue is set at infinity. When the active queue empties, all lower ranks are simultaneously moved up one rank. The queue number of the active queue is used to control a multiplexer which selects the active queue output.","Referring again to , the GI-Cube architecture preferably uses RAMBUS memory (RDRAM) . The memory is generally the key component of any volume rendering architecture since that is the usual bottleneck. The GI-Cube architecture generally requires parallel, distributed memory and high bandwidth memories. Parallel, distributed memory permits size and bandwidth scalability by the simple addition of identical components. The disadvantage of distributed memory is the difficulty of sharing data for dynamic load balancing, which is not attempted in this architecture. The RAMBUS memory (RDRAM)  is used because of its high sustained bandwidth for random accesses. Alternatively, the double data rate (DDR) DRAMs could also be used. At 800 MHz, one RDRAM can supply 1.6 GB\/s bandwidth. In the GI-Cube architecture, standard volume rendering at the design point requires an average of 2.8 GB\/s and global illumination 4.6 GB\/s. The invention could also be implemented by sharing one RDRAM  between every two processors to reduce costs. The sharing would result in a noticeable degradation in performance because of the sharing increases the latency of cache misses.","Referring now to , a portion of the processor (pipeline)  having a prefetch and memory controller  and the interleaved volume cache  is shown. The prefetch and memory controller  preferably include an address decoder , a cache tags  and amiss scheduler . The interleaved volume cache  preferably include an eight-way interleaved volume memory  and an eight gradient LUT . RDRAMs  can deliver four voxels to the pipeline , which runs at about 100 MHz because RDRAMs currently operate at 800 MHz and supply 18 bits (\u00bd a voxel) every cycle. To hide latency, voxels are prefetched earlier in the pipeline  as shown in . After address decoding with an address decoder , the eight tags associated with the trilinear neighborhood are checked by a cache tags . Up to eight misses are queued by the miss scheduler  for serial RDRAM access. The Rambus ASIC cell (RAC)  streams voxel read\/write operations to and from the RDRAM . Retrieved voxels are received into a queue which feeds the volume cache .","Each of the eight banks is able to read or write one voxel per clock cycle. Up to four voxels are retrieved each cycle from the over-clocked RDRAM . As long as there are no bank contentions, all are written to their respective banks during the cycle. Referring again to , the newly retrieved voxels are routed around the cache  through a bypass  that also writes them directly to the resampling unit  to perform the trilinear interpolation using the newly retrieved voxels. To ensure that all the voxels have reached the cache , a simple counter is used that stalls each ray until the specified number of voxels have been retrieved into the cache .","Before interpolation, the gradient index of each of the eight voxels must be decoded into three components. Preferably the 11-bit gradient index is cached and eight identical gradient look-up tables (LUTs)  are then used to decode the eight indices in parallel. It would cost over 50% more storage to cache the actual gradient components (3\u00d710 bits per index).","The fourth major component of the processors  is the pipeline itself as shown in . The resampling unit  can be considered the top of the pipeline. The resampling unit  accepts one sample location along a ray and the eight nearest voxels. The resampling unit  trilinearly interpolates the density, gradient, and irradiance using seven linear interpolators per channel. The tag is nearest-neighbor sampled. The sample data is passed to the segmentation unit  which looks up the color, opacity, and shading coefficients (total of 36+16+32=84 bits) in a density+tag indexed SRAM LUT.","The segmented and classified sample is passed to the compositing unit  which randomly jitters the sample location along the ray. The sample location and image destination bits are mangled to select a random factor from an SRAM LUT representing uniformly random numbers between \u00bd and 1. The sample opacity is then modulated, using another LUT to assist in computing the power function. The opacity of the ray is then updated by using the compositing equation.","If the ray is a lighting ray, the amount of energy lost to the surrounding voxels is computed as described above. If any energy should be deposited, the amount is forwarded to the miss scheduler  of the volume cache  in order to be added to the nearest eight voxels using the weights interpolated in the resampling unit . This reduces the available bandwidth and can lead to stalls, especially in low albedo mode. When a sample borders other blocks, up to seven other blocks may contain copies of the voxels because the voxels on one edge are shared. To maintain consistency among different copies, the energy is packaged up and queued on the neighboring blocks.","A special flag in the ray is used to mark them as irradiance carriers. When the volume cache  detects an irradiance carrier, it retrieves the current irradiance for each voxel from the cache , adds to them the trilinearly interpolated energy, and writes them back to the cache  and memory. The main additions required to accommodate this read\/modify\/write behavior are a datapath from the pipeline to the volume cache , a datapath from the pipeline to the dispatcher, and a recirculation from the data cache to the miss scheduler  to write the voxels.","If the ray is a rendering ray and global illumination is turned off, a shading unit  shades the sample. Preferably the shading unit  includes a reflectance map for shading the sample. The resolution of the reflectance map is 128for each of the six faces, but discretization artifacts are minimized by using bilinear interpolation. For globally illuminated rendering rays, illumination is computed based on the BSDF. The diffuse component is estimated by using the sampled irradiance. The specular component is resolved only if the ray is scattered and happens to hit the light source, which is only feasible for area light sources. To evaluate the specular component of the Phong lighting model would require casting a ray to each light source, multiplying the number of rays and flooding the fixed length ray queues. Alternatively, the reflectance map can be loaded with the specular component of the point light sources (rather than the usual black) and the result summed into the sample color.","The shading unit  is followed by the spatting unit  and the scattering unit . The splatting unit  updates the voxel data and ray data associated with a sample location. The order of the spatting unit  and the scattering unit  on the pipeline between the queue sorter  and the shading unit  is not critical.","The final stage of the pipeline includes a scattering unit  that scatters the ray based on the BSDF. If the BSDF of the sample's material is too complex for the hardware to compute, a flag is set in the ray data structure and the ray is directed to the ray sorter to be forwarded to the DSP for further processing. In the DSP, a user-specified routine computes the new ray direction and the ray is sent back to the processors via the bus. Simple BSDF's are evaluated in hardware. For example, isotropic scattering is computed by selecting a direction \u03b4 from a random direction LUT. The random index is selected by again mangling the ray position and destination bits. Glossy scattering is performed using:",{"@attributes":{"id":"p-0476","num":"0483"},"maths":{"@attributes":{"id":"MATH-US-00042","num":"00042"},"math":{"@attributes":{"overflow":"scroll"},"mfrac":{"mrow":[{"msup":{"mi":["D","\u2032"]},"mo":"=","mrow":{"mi":["D","\u03b2\u03b4"],"mo":"+"}},{"mo":["\uf603","\uf604"],"mrow":{"mi":["D","\u03b2\u03b4"],"mo":"+"}}]}}},"br":{}},"Referring now to , the layout of the board  is much simplified by grouping all the processors  into a single ASIC. The rest of the board is composed of a DSP , some SDRAM  for the frame buffer, and preferably at least two RDRAMs  for volume memory. With this chip count, a single PCI board implementation is entirely feasible. Of course, additional RDRAMs  can be easily daisy chained to meet increased memory needs. The host is freed by the board of most rendering responsibilities. Besides modeling, manipulation, and final display, the host is largely available for other processing. The exception is when other algorithms are performed which rely on the host for most of the algorithmic work, while the board acts as a coprocessor.","The DSP  carries the bulk of the rendering management responsibilities. Prior to rendering, the DSP  coordinates loading the volume data and LUT onto the ASIC. During rendering, the DSP  continually generates new rays according to the viewing parameters and assembles the processed rays into an image in the attached SDRAM . At the end of rendering a frame, it transmits the image back to the host's main memory for display. The processors  require minimal control coordination during the actual rendering as they autonomously process and return rays. Preferably the ASIC pipeline is implemented in silicon.","The bandwidth of the PCI interface (132 MB\/s) to the board  can become a bottleneck in some cases. For usual volume rendering and global illumination at 30 Hz, it is possible to transmit 512images at 36 bits per pixel (8-8-8-12 rgb\u03b1) without saturating the bus. However, other algorithms such as volumetric texturing, which require the full participation of the host, may flood the interface. In particular, if all the rays are generated on the host, transferred over the PCI bus , and returned over the bus , flooding may occur. In this mode, the frame rate or resolution might be reduced, unless the AGP interface which provides four times the bandwidth is used instead.","The pipelines are controlled through simple logic connections. There is no need to coordinate shared activity among the processors  over the bus. Therefore, the ray bus  can be physically implemented as two unidirectional connections. The DSP  to the processors  is just a broadcast connection, while the processors  to the DSP  is a many-to-one connection. The latter can be implemented as a binary tree of merging nodes.","The size of a ray packet is 32 bytes. The width of the bus can accommodate this size since it is on-chip. The bus frequency matches the processor  frequency (100 MHz) so each processor  receives one new ray every p cycles. Asp grows beyond four, this can become a limitation, particularly during the start-up period when the queues  are first filled. To overcome this, ray differentials can be used to compress a sequence of rays into a smaller data structure. Just as the host instructs the DSP  to generate all the rays for a specific viewpoint, the DSP  can instruct the processors  to extrapolate a number of rays given the differential (e.g., in direction and destination).","The communication between processors  is also in the form of ray packets of 32 bytes. Since this communication is local and presumably on-chip, it is easily implemented. Testing with a software simulator has shown that processors  communicate with each other only 2 to 4 out of 100 cycles. Therefore, each ray packet can be broken into several smaller parts and transmitted over a narrower bus at the same speed. However, partitioning the volume with repeating slabs increases the communication to about 7% and block skewing about 20%. Block skewing resulted in a bisection bandwidth of about 12 MB\/s, while simple slabs had about 1 MB\/s or less. Tests show that the memory interface is used 37.5% of the time during rendering and 92% of the time during the lighting pass of global illumination. The increased utilization is due to the irradiance splatting operation which requires a read-modify-write cycle.","A method for performing efficient volumetric backprojection will now be described. The method is useful in global illumination where the method is used to transport illumination through the scene, starting from the light sources and propagating outward. In addition, the method is useful in reconstruction such as computed tomography. The method can be implemented on the Cube-5 architecture or on a modified Cube-4 architecture as described in , by Frank Dachille IX, Klaus Mueller, and Arie Kaufman, 2000, pp. 109-117, October 2000, which is incorporated herein by reference. The method will be now described in connection with a new algorithm for volumetric global illumination based on a sequence of backprojection iterations, and computed tomography as a basis for reconstruction.","In local illumination, the global distribution of light energy is ignored and shading calculations are performed assuming full visibility of all light sources. While this is useful as a first approximation, the incorporation of global light visibility information (shadows, one instance of global illumination) adds a great deal of intuitive information to the image. This low albedo lighting simulation has the ability to cast soft shadows by volume density objects. Generous improvements in realism are achieved by incorporating a high albedo lighting simulation.","We wish to solve the illumination transport equation for the general case of global illumination needs to be solved. The incident illumination I (\u03b3,\u03c9) in direction \u03c9 at any voxel \u03b3 can be described as\n\n(\u03b3,\u03c9)=\u222b\u222b(\u03c9,\u03c9\u2032)(\u03b3,\u03c9\u2032)\n\nwhere \u0393 is the set of all directions, V is the set of all voxels v, and f(\u03c9,\u03c9\u2032) is the phase function in directions \u03c9 and \u03c9\u2032. This means that the illumination at any voxel is dependent upon the illumination at every other voxel. Generally in practice, this integral-equation is solved by finite repeated projection of energy among voxels. This leads to a finite energy transport path, which is generally sufficient for visual fidelity.\n","To implement the method some of the same assumptions for standard radiosity are made. In particular, the voxels are generally assumed to behave as diffuse surfaces when a gradient exists. When there is no gradient (as in the case of homogeneous fog) then the voxel scatters light in all directions isotropically. The computation is organized not per pixel or per voxel, but per direction. Organizing per direction capitalizes on coherence by utilizing slice-by-slice computation.","The volumetric scene is first analyzed to determine the initial distribution of lighting energy. Preferably the direct illumination (typically the major contributor to overall intensity) is computed directly. For directional light sources a single sweep similar to , by J. Kajiya and B. Von Hwerzen, (84 ), volume 18, pages 165-174, July 1984, along one major axis is sufficient to propagate the light energy to all the voxels. For point light sources both inside and outside the volume, the light intensity is backprojected outward from the light source to every voxel using a slice-based approach. However, it has been determined that in practice it is far simpler to shoot one or more rays toward each of the Nexterior voxels of the volume and account for the inverse-square intensity falloff of each ray.","Besides the volume density array p(s), s \u03b5 R, a radiosity array I(s) and an unshot radiosity array I(s) is maintained. A transfer function converts each sample volume density \u03c1into an opacity \u03b1and color C. For many datasets, a simple linear ramp from zero opacity at density \u03c1to full opacity at \u03c1is sufficient. For CT datasets, it has been determined that it is useful to set \u03c1at about 20% density and \u03c1to full density to eliminate noise. For voxelized datasets, the full dynamic range was used. In experimentation, only a single wavelength of light with objects of a constant intensity were used. In any case, a transfer function should be chosen for the illumination transport which elucidates the features of interest, the same as in direct volume rendering. As a matter of implementation, the single density value could be replaced with pre-classified RGB\u03b1 values to support pre-segmented volumes (e.g., the visible human dataset).","In the initial sweep of direct illumination, light energy is transported in proportion to the optical path length to the light source. The radiosity deposited into each voxel along a path from the light source to s is\n\n()=\n\nwhere \u03ba(s) is the extinction coefficient at s. The extinction coefficient describes the reduction in intensity of a light beam as it passes through a differential volume element. The integral is computed incrementally along the path using standard compositing to accumulate opacity along the ray. As energy is depleted from each ray it is deposited into both the radiosity array Iand the unshot radiosity array Imodulated by the reflectivity \u03bb of the volume sample. Reflectivity dictates the percentage of light that is reflected out of the reduced light ray intensity. The extinction coefficient and reflectivity are both determined by a transfer function based on the local volume density. Note that trilinear or better interpolation should be utilized for both sampling the density \u03c1 and depositing the energy into the radiosity Iand unshot radiosity Iarrays.\n","A different approach is utilized for area light sources. The computation of the direct illumination contribution of an area light source requires integrating across the entire area for each visible voxel. The integration is postponed until the next step by summing the energy directly into the radiosity and unshot radiosity Iarrays because this is nearly as difficult as calculating the indirect illumination. If all light sources are area light sources, then the indirect passes are processed next as the initial pass can be avoided. However, the smaller the area light sources, the longer it will take to reach equilibrium. Therefore, smaller area light sources can sometimes be more efficiently computed as a small set of point lights.","In the second pass, the integration of the illumination contribution of all voxels to all other voxels is performed by a finite number of iterations. In each iteration, a random direction \u03c3 for the backprojection is selected. Note that the convergence could be improved by selecting directions using a quasi-random (e.g.,) sequence of directions rather than a uniform random sequence. An preferred method is to select points distributed on a sphere as directions.","In each iteration, slices perpendicular to the major axis nearest to the random direction \u03c3 are processed. Starting with the first slice, a ray front in the form of a 2D buffer is initialized. This buffer is used to transport energy along the rays defined by the elements and \u03c3. At each slice, the rays simultaneously accumulate and deposit energy from the neighboring voxels. The differential equation describing the transfer of energy I in a ray in direction \u03c3 over a differential length ds is:",{"@attributes":{"id":"p-0493","num":"0500"},"maths":{"@attributes":{"id":"MATH-US-00043","num":"00043"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mi":"dI"},{"mrow":{"mrow":[{"msub":{"mi":["I","u"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"s"}},{"mi":"\u03d5","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","\u03c3"],"mo":","}}}],"mo":"\u2062"}},{"mrow":{"mrow":{"mrow":{"mi":"if","mo":"\u2062","mrow":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mo":"\u2207","mi":"\u03c1"}},"mo":"\u00b7","mi":"\u03c3"}},"mo":"<","mn":"0"},"mo":","}}]},{"mtd":[{"mrow":{"mi":"_","mo":"="}},{"mrow":{"mrow":[{"msub":{"mi":["I","u"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"s"}},{"mrow":[{"mi":"\u03ba","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"s"}},{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"s"}}],"mo":"\u2062"}],"mo":"-"}},{"mrow":{"mrow":{"mrow":{"mi":"if","mo":"\u2062","mrow":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mo":"\u2207","mi":"\u03c1"}},"mo":"\u00b7","mi":"\u03c3"}},"mo":"=","mn":"0"},"mo":","}}]},{"mtd":[{"mi":"ds"},{"mrow":{"mrow":[{"mo":"-","mrow":{"mi":"\u03ba","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"s"}}},{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"s"}},{"mi":"\u03d5","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["s","\u03c3"],"mo":","}}}],"mo":["\u2062","\u2062"]}},{"mrow":{"mrow":{"mrow":{"mi":"if","mo":"\u2062","mrow":{"mrow":{"mo":["\uf603","\uf604"],"mrow":{"mo":"\u2207","mi":"\u03c1"}},"mo":"\u00b7","mi":"\u03c3"}},"mo":">","mn":"0"},"mo":","}}]}]}}},"br":{}},"In a very high resolution lighting simulation, it would be possible to purely absorb and emit light isotropically by each voxel. This is akin to using microgeometry to determine the reflectance behavior of surfaces. But it is much more efficient to compile statistics on surface reflectances and use a bidirectional reflectance distribution function (BRDF) instead to model the gross effects of the microgeometry. In the absence of surfaces (where there is a zero gradient), a simple isotropic absorption-emission model is used. But at surface boundaries, the energy transfer is allowed to only occur in one direction. The ray energy is only allowed to be deposited onto the surface if the ray is approaching the surface. Conversely, unshot radiosity is only allowed to augment the ray energy if the ray is leaving the surface. Additionally, surfaces are modeled as ideal diffuse reflectors, and therefore the angle of incidence is taken into account using the dot product. This distinction between isotropic and diffuse reflectors is automatic, in contrast to Sobierajski's method of explicitly storing two coefficients per voxel as described in , by L. Sobierajski, Ph.D. thesis, Stony Brook; NY, August 1994.","\u03b6 is used to distribute energy over several iterations. By only emitting part of the voxel radiosity in each iteration, the energy is distributed to a larger variety of voxels, leading to faster convergence. The complete algorithm for a single backprojection is given in . In the implementation in accordance with the invention, the ray buffer contains a slice-sized array of rays which are resampled for interaction with each voxel. Because of the bidirectional transference of energy between the rays and the volume, at least one of the participants must be resampled so that the exchange can take place at a specific location in space. Preferably the ray buffer is resampled because it is 2D requiring only bilinear interpolation instead of trilinear interpolation of the volume, or both.","In the procedure, energy exchange is computed one slice at a time, then the ray array is shifted along the ray direction to the next slice as indicated in . Parts of the ray buffer which move outside the volume are wrapped around to the other side and re-initialized. A modulo operation efficiently computes the wrap-around.","Clearly, the final distribution of energy will be strongly correlated to the initial chosen direction. If a certain voxel density gradient happens to be in the same direction as the initial direction \u03c3, then all of the unshot energy will be shot in the initial iteration. Two techniques are used together to reduce this effect. First, a small value of \u03b6 helps to spread out the contribution over more voxels. Second, the process is repeated many times and the result is averaged. To repeat this without using additional buffers, the total amount of energy added to the system is retained and used to normalize the individual voxel radiosity during rendering. This permits incremental refinement of the solution to include in increasing variety of directional sampling over time.","If desired the solution can be terminated early to display intermediate results because the iterative approach provides a progressive refinement. In a progressive refinement, the intermediate stages are visualized by estimating the distribution of energy throughout the scene. In order to avoid placing radiosity in the interior of solid objects the unshot radiosity is preferably not simply split equally among all the voxels. Preferably this is accomplished by proportioning the energy according to the product of density and gradient. In this way, empty voxels, which conventionally have zero density, and solid interiors, which usually have no gradient, are avoided.","The iterations are continued until convergence. Convergence is defined by the voxel-wise root-mean-square (RMS) difference between radiosity estimates \u0394i iterations apart being below some threshold \u03b4. The RMS difference is computed by the Pythagorean sum of squared differences between corresponding voxels, assuming the original volume is in the range [0,1]. Of course, termination can be accelerated by accepting a greater error tolerance and vice versa, leading to an adjustable time-quality tradeoff.","Selecting \u0394i\u226720 is used to avoid local minima in the search process. Referring now to , the logarithmic rate of convergence with t=20 and \u03b4=0.1 is demonstrated. When convergence is achieved, there is usually unshot radiosity in the scene from the last several iterations. The radiosity added in each iteration has a half-life which is data dependent. The unshot radiosity can be (1) ignored and removed from the sum of unshot radiosities, (2) distributed among the other voxels of the scene, or (3) distributed more appropriately by iterating further until some proportion \u03b5 of the total energy is dissipated. The latter is the most appropriate technique, but this choice has little effect on the final distribution after convergence.","A modified version of direct volume rendering is used to render using the radiosity-density representation. Instead of shading each sample along the ray by summing the illumination by each of the light sources, the pre-computed radiosity is used which contains the influence of both the direct and indirect light sources. The image rendering equation from point sin direction \u03c3 is then:",{"@attributes":{"id":"p-0502","num":"0509"},"maths":{"@attributes":{"id":"MATH-US-00044","num":"00044"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mrow":[{"mi":"I","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["s","o"]},"mo":",","mi":"\u03c3"}}},{"msubsup":{"mo":"\u222b","msub":{"mi":["s","o"]},"mi":"\u221e"},"mo":"\u2062","mrow":{"mrow":{"mi":"\u03c1","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"s"}},"mo":"\u2062","mi":"I"}}],"mo":"="},{"mrow":[{"mo":["(",")"],"mi":"s"},{"mo":"\u2146","mi":"s"}],"mo":["\u2062","\u2062","\u2062"],"msup":{"mi":"e","mrow":{"mo":"-","mrow":{"msubsup":{"mo":"\u222b","msub":{"mi":["s","o"]},"mi":"\u221e"},"mo":"\u2062","mrow":{"mrow":[{"mi":"\u03c1","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"t"}},{"mo":"\u2146","mi":"t"}],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}}}}}},"mstyle":{"mspace":{"@attributes":{"width":"0.2em","height":"0.2ex"}}}}],"mo":","}}}},"It has been determined that the inclusion of cos(\u03b8) factor in the integral similar too Lambert's law enhances the image contrast, emphasizes the gradient, and improves the overall appearance. \u03b8 is the angle between the viewing ray and the volume gradient. It is computed using the dot product \u2207\u03c1(s)\u00b7\u03c3 clamped to the range [0,1]. In the absence of a volume gradient a value of 1 is used in place of the dot product, for this indicates a homogenous region that emits isotropically.","A number of methods have been proposed to reconstruct the 3D shape of objects from photographic images. Kutulakos and Seitz use a technique called space carving to generate a binary representation of the objects on a discrete volume grid, which is described in , by K. Kutulakos and S. Seitz, 692, Computer Science Dept., University of Rochester, Rochester, N.Y., May 1998. It works by backprojecting the object's silhouette edges that can be detected in the images. Seitz proposed a method described in , by S. Seitz and C. Dyer, 25(3), November 1999, termed voxel coloring that works its way through the scene from front to back in layers and picks the probable color for each voxel based upon the acquired images. Both methods make a binary decision on what color and occupancy a voxel should have, which can lead to aliasing. A new approach to reconstructing a volumetric object from its backprojections is now described below.","The following observations are noted as being important criteria to reconstructing a volumetric object from its backprojections:\n\n","Reconstructing a volumetric object from its backprojections is a common procedure in the medical field. Computed tomography (CT) is routinely employed to recover a patient's interior from X-ray projections that were taken around a circular orbit around the patient. The most commonly used CT method is Filtered Backprojection (FBP), where the projections are first filtered with a high-pass filter, and then backprojected onto the volume. The high-pass filtering is necessary to avoid blurring of the reconstructed object, and the backprojection can be thought of as a simple spreading of the filtered projection image across the volume grid. The theory behind FBP requires the projection images to be spaced at equidistant orientations around the patient. The quality of the reconstruction suffers considerably when this prerequisite is not fulfilled, and also when the number of projections is small (that is why 500 and more projections are taken by medical scanners). In these scenarios, iterative techniques, such as the Simultaneous Algebraic Reconstruction Technique (SART), are more adequate. In SART, the volume is reconstructed by a sequence of projections and backprojections. The technique iteratively (1) projects an image from the volume currently being reconstructed, (2) compares it to the actual X-ray image acquired from the scanner, (3) corrects the reconstructed volume using backprojection, and (4) repeats the process until convergence.","To implement SART, a sequence of x-ray images is selected; convergence is faster if successive images are projected in approximately orthogonal directions. A relaxation factor \u03bb\u03b5[0,1] is selected to mix each voxel with its correction. For each image in the sequence, the existing volume (initially empty) is projected from the same viewpoint as the x-ray image. The true image is subtracted from the approximate image and the result scaled by \u03bb. This difference image corresponds to the correction which would fix the volume according to that viewpoint. Rays traverse the volume and deposit the correction value (either positive or negative) to the voxels along the ray. As the process is repeated, the volume converges to the original sampled volume.","CT can reconstruct three-dimensional object features of very little contrast (less than 0.5%) and with high resolution (less than 1 mm), but tomographic reconstruction is primarily used in the context of imaging with X-ray energies which are confined to hospitals and shielded industrial sites. Apart from the fact that X-rays are difficult to generate, health considerations prohibit the use of X-ray technology to scan real objects in the office, home, or laboratory, for subsequent incorporation on graphics scenes. The question is, can we use the high-fidelity properties of CT methods to reconstruct objects imaged with harmless visible light and so recover low-contrast and very detailed object features.","Since all CT methods including SART assume all objects can be perfectly penetrated by the X-ray beam, obscuration is not a problem. But, using visible wavelengths of light means that some parts of the scene may be obscured in some or all of the images. For that reason, the estimated volume usually never approaches the real volume because the interior is indeterminate. The same problem arises with reconstruction from saturated x-ray images. Furthermore, some parts of the scene may be indeterminate due to specular highlights (e.g., a mirror) or complete shadowing.","Referring now to , a virtual test setup is shown. A scene of random translucent triangles are voxelized into a reference volume. Then a virtual light source , camera , and backdrop  are positioned in the scene. The volume  is rotated on a virtual turntable  to acquire a non-uniform sequence of projections with both a white  and a black  backdrop and controllable ambient and diffuse shading. A reconstruction volume containing both color and opacity is initialized to empty. Then a number of iterations are used to converge the reconstruction volume.","In each iteration, a random source projection is selected and virally imaged. Although the opacity can be obtained with volume rendering, it is unavailable with standard image capture. Using two images, one with a white backdrop  and one with black , the opacity can be computed afterward with a straightforward derivation involving the compositing operator. Given a pixel of a photograph of the object over a white background Cand over a black background C, can be expressed in terms of the object color C, the object opacity \u03b1, and the compositing equations\n\n\u03b1+1(1\u2212\u03b1)\n\n\u03b1+0(1\u2212\u03b1)\n\nand solving for object opacity \u03b1results in get\n",{"@attributes":{"id":"p-0512","num":"0525"},"maths":[{"@attributes":{"id":"MATH-US-00045","num":"00045"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["C","o"]},"mo":"=","mfrac":{"msub":[{"mi":["C","b"]},{"mi":["\u03b1","o"]}]}}}},{"@attributes":{"id":"MATH-US-00045-2","num":"00045.2"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["C","\u03c9"]},"mo":"=","mrow":{"mrow":{"mfrac":{"msub":[{"mi":["C","b"]},{"mi":["\u03b1","o"]}]},"mo":"\u2062","msub":{"mi":["\u03b1","o"]}},"mo":["+","-"],"mn":"1","msub":{"mi":["\u03b1","o"]}}}}},{"@attributes":{"id":"MATH-US-00045-3","num":"00045.3"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":["\u03b1","o"]},"mo":"=","mrow":{"msub":[{"mi":["C","b"]},{"mi":["C","\u03c9"]}],"mo":["+","-"],"mn":"1"}}}}],"br":{}},"Advanced methods for volumetric global illustration that can be accelerated by a ray tracing architecture such as GI-Cube will now be described. These methods depart from previous techniques by processing more efficiently. Instead of iterative slice-based techniques for global illumination, the methods process a number of rays in direct proportion to the light intensity in every part of the scene","A stochastic solution method for the transport equation will now be described. There is a well-known connection between visual importance and radiance. Bi-directional methods are the most promising tool for discovering the Markov chain transport paths that connect the light source(s) with the viewpoint. The first step in the new method, is to sample these transport paths in order to gain a statistical insight into the important modes of transport. Therefore, bi-directional methods are used to generate a sample list of complete paths from the light sources to the eye. Sufficient sampling is performed to collect information to make use of importance sampling techniques which is generally less than the amount required for generating an image. This is a stochastic method which can take advantage of anisotropic multiple scattering. The method can discover specular to diffuse transport paths efficiently since it is bi-directional.","The second step is to classify the list of transport paths. The voxels are aggregated into clusters of some manageable size. Preferably the transport paths are classified according to the location of interaction points. For example, a path that made two bounces at voxel clusters Vand Vbetween the light source and the eye would be classified under LVVE, using the notation described in , by P. Heckbert, (90 ), Volume 24, pages 145-154, August 1990, which is incorporated herein by reference. The most significant transport paths are almost always the shortest which include those by direct illumination and single scattering. These transport paths will also generally occur with the greatest frequency. After all the paths are classified, they are sorted by frequency within the group.","The next step is to process each set of paths, from the most frequent to the least. Preferably an accounting is made for all the significant sources of illumination. Radiosity methods account for every possible diffuse transport path in a breadth-first manner. In the present invention, the illumination transport paths are processed in a depth-first manner. The most significant source is identified by starting with the most frequent. This is similar too the process of progressive refinement, except that we only concentrate on the most significant transport path. To process a set of paths, the amount of energy transported from the light to the eye over each image pixel is analytically determined. A variety of variance reduction techniques known in the art can be used. Suitable examples are described in , by J. Kajiya, (86 ), Volume 20, pages 143-150, August 1986, and more recently in , by E. Veach, Ph.D. thesis, Stanford Calif., December 1997, both of which are incorporated herein by reference. This technique can be compared to metropolis light transport (\u201cMLT\u201d), described in , by E. Veach and L. Guibas, 97 , pages 65-76, August 1997, the method processes a transport path while sampling variations on the path by mutating the interaction points. In MLT, there is a great deal of variance over the image because the mutations are not continual over the image. In the present invention, this is changed by deterministically varying the interaction point over the image.","With a volumetric scene representation, a handful of elements, each of equal size and shape, are first constructed. Next bi-directional path tracing is performed to discover the statistically significant sources of light transport. Preferably the bi-directional path tracing is performed while taking into account possibly specular and diffuse BSDFs at every voxel. Note that a hardware accelerated and global illumination method is ideal for computing the random walks through the volumetric scene. The typical bi-directional path tracing method can be simplified since only important transport paths are being gathered. Random suffixes of light paths are connected to random prefixes of eye paths, as in standard bi-directional path tracing. However, light paths of equal weight and eye paths of equal weight are generated. That is, all light paths have approximately equal probability and the same for eye paths. In this way, paths can be randomly selected to join to form complete light transport paths. Direct illumination will be treated separately as a special case. Therefore, light paths of length two (2) or greater are randomly joined with eye paths of length two (2) or greater. That means that all eye paths have one vertex at the light source and at least one vertex of interaction with the medium. The same applies to the eye paths. Note that if either path has three vertices, the light source or the eye point can be chosen as a fourth vertex. This yields indirect illumination.","To generate the complete light transport paths, we select two random paths and attempt to connect them via a shadow ray. This shadow ray is the integration of opacity between the path endpoints. It can be easily computed by the processing engines of this invention. Hundreds of these shadow rays can be computed simultaneously using the intelligent ray scheduling of this invention.","The present invention uses the optical depth, which is a continuous measure of the visibility between two locations. This is the same computation that is used to connect paths in , by E. Lafortune and Y. Willems, 7, pages 91-101, June 1996. Since the shadow direction is arbitrarily selected and not according to the BSDF, then the contribution of the transport path must be weighed by this weight. The paths are classified after a sufficient number of complete paths are generated.","The paths are classified by identifying each of the interaction points along the path according to the selected clusters. Therefore, each path can be classified as interacting with a short list of clusters. Paths that share the same list of clusters are grouped together into bins. The bins are then sorted by weight of contribution to the final image. In this way, the most full bin represents the general path along which most of the indirect illumination statistically occurs. By processing the bins in the order of most full to least full, a progressive method is provided that quickly converges to an approximate, importance sampled solution.","A k-d volume subdivision is used to process the complete transport path. For example, the element is first subdivided in half in one of the major axis directions (x, y, and z) and a sample is randomly taken in the unsampled half. The next time the element is subdivided, the two sub-elements are subdivided in a different major axis direction and two samples are randomly taken in the unsampled halves. The next time the element is subdivided, the four sub-elements are subdivided in the remaining major axis direction and four samples are randomly taken in the unsampled halves. The element nearest to the eye is always subdivided completely, i.e., each voxel is computed separately. The other elements are subdivided until there is no visible change in the final image. If there are two or more clusters before the final visible cluster, then the choice of which cluster to subdivide can be randomized in the same way as the randomized subdivision method presented in , by J. Kajiya, (86 ), Volume 20, pages 143-150, August 1986.","After each subdivision step, the accumulated sub-image due to the current transport path is divided by two since the number of samples will double with the new subdivision. An optical depth is retained between each pair of sub-elements which corresponds to the percentage of energy that is transmitted between the two. The optical depth does not need to be recomputed when an element outside the pair is subdivided. The optical depth is actually never recomputed, but always re-used down one level in the subdivision, and one additional optical depth is computed for the newly subdivided element.","Further re-use can be accommodated at the expense of a little extra storage. After one transport path has been completely computed, a segment of another transport path may coincide with the path. The overlapping segment can be re-used, although this implies the re-used segment must be subdivided at the same level as the previous for the similar segment. This strategy amounts to a lazy evaluation of the complete set of O(n) form factors in the context of radiosity. However, this method is view dependent and therefore not as restricted as radiosity due to the allowance for arbitrary BSDFs.","One problem so far is the computation of highly specular surfaces. A highly specular surface only contributes significantly in one single direction for any incident direction. Therefore, computing the contribution of one highly specular sub-element for all of the sub-elements further down in the transport path is prone to aliasing. One solution to this problem is to handle highly specular surfaces separately, creating a single transport path through a sub-element rather than the typical branching that occurs.","It is well known that path tracing is a more effective method than distribution ray tracing for the simple fact that more time is spent tracing important primary rays than an exponential number of secondary rays. For that reason, it is more efficient to sample the transport path by using path tracing rather than bushy ray tracing. Fortunately, the paths that determined the transport path originally can be re-used for the integration of the illumination on the last element.","Therefore, the modified algorithm is as follows:","1. Emit photons from the light source(s) into the volumetric scene according to the goiniometric distribution. All particle paths have an approximately equal probability of existence. Compute the paths of the particles using the intelligent scheduling and processing of the proposed invention.","2. Emit importons (importance particles) from the viewpoint according to the visibility distribution. All importons have an equal probability of contributing to the image. Compute the paths of the particles using the intelligent scheduling and processing of the proposed invention.","3. Randomly connect photons paths with importon paths to generate complete light transport paths of length at lease 4 (including the light source and viewpoint). Weight the composite paths by the product of the optical depth, and the BSDFs of the two connected endpoints. Generate random connections between paths on the general purpose CPU. Each random connection of paths generates a new ray computation for the invention to process.","4. Subdivide the volume into a handful of volume clusters of equal size. Classify and group the paths according to the interactions within the clusters. For example if two paths both interact with clusters 7 and 15 before reaching the viewpoint, then group the two paths together into a bin.","5. Sort the bins from largest contribution to the smallest contribution and process the bins in that order. This leads to a progressive refinement solution to indirect illumination.","6. Process each bin by selecting some proportion of the transport paths and computing analytically the contribution to each voxel of the last cluster, and finally the contribution to the final image. The number of sub-paths computed for each transport path should be proportional to the statistical contribution of the transport path (as estimated in step 5).","7. Continue processing paths until there is no further change to the final image.","Step 6 can be expanded to explain the details further. At the interaction point just before the final cluster, the original transport sub-paths in the bin correspond to a photon hit within the cluster. These photon hits can be used to estimate the indirect illumination within the final cluster. Some proportion of these photon hits is selected to compute the indirect illumination. The proportion may be smaller than the original statistical evaluation or it may be larger (requiring additional photon paths to be traced). If additional paths are required, these could be generated by a rejection method\u2014rejecting those paths that do not traverse the desired cluster path. However, this is slow. Another method is to select random voxels in each new cluster and compute the probability of transport along this path. This differs from the original method of generating equal weight (probability) particle paths. However, we already modified the probabilities during the matching procedure, so this is of no consequence.","When a set of photon hit points are identified, process each hit point using a three-point transport equation. First, convert the incoming energy at the hit point into an outgoing energy in the direction of the first voxel in the final cluster. Then compute the optical depth (using the invention to integrate the opacity along the ray) between the hit point and the first voxel and modify the weight by this quantity. Continue by using the BSDF at the first voxel to convert the energy into an energy in the direction of the viewpoint. Again, modify the weight by the optical depth with respect to the viewpoint. Finally, contribute the final weighted energy to the image.","This procedure can be improved a bit by reordering the computation. In a slightly more efficient way, some of the quantities can be computed incrementally. With a little more up-front cost in determining the proper set of rays, the final cluster can be ray casted ray-by-ray. For each ray, the optical depth is determined incrementally as in the usual ray casting method. Then, at each voxel, not all the hit points and determining the contribution from each, using the BSDFs at each interaction point and the optical depth between them.","A recursive blocking method and apparatus for storing and traversing a 3D volume data will now be described. To understand the advantages of the recursive blocking method, the traditional method of storing and traversing a 3D volume data will be briefly described.","Volume rendering 3D scalar volumes are stored as linear arrays as shown in . Referring now to , in the traditional linear volume layout all the voxels of one x-axis-aligned scan-line are first stored. The data is then stored scan-line by scan-line until one slice (xy-plane) is filled. Finally the data is stored slice by slice along the z-axis. With this approach the preferred traversal order is in storage order-scan line by scan line. In order to apply a Gau\u03b2 filter with radius  voxels, the data has to be convolved with a 5filter. This forces scan lines from 5 consecutive slices to be brought up the memory hierarchy to the CPU. For a typical 256volume, each 256slice of 8 bit scalar data has a space requirement of 2\u00b72B=64 KB. Thus, for all data touched by the filter during processing of one slice 5\u00b764 KB in the source and 64 KB in the destination volume are needed. These 384 KB are well beyond Level 1 cache sizes and even larger than many Level 2 caches on older PCs. Therefore, when the next slice is filtered the voxels which have been transferred to the cache for reading during processing of the previous slice are no longer there and need to be re-fetched. In addition, since cache sizes as well as volume sizes are usually powers of two (2), caches having a capacity less than or equal to one slice scan lines with the same offset from a slice beginning also map to the same cache line. Thus for a 5filter and the destination volume to reside in the cache collision-free, the cache would have to be six way associative. As Level 1 caches are mostly two-way associative, voxel data is constantly being replaced, even during processing of just one scan line.","Referring now to  a recursive blocking volume layout is shown for a 256volume. The recursive blocking method stores and traverses the 3D volume data such that the cache access hit ratio is near optimal. Generally a voxel with 3D position Pis stored at array offset O. The 3D position P=(x; y; z) is in integers and has the bit pattern ( . . . , x, x, x, x; . . . , y, y, y, y; . . . , z, z, z, z) while Ohas the bit pattern ( . . . , z, y, x, z,y, x, z, y, x, z, y, x). The 3D volume data is stored in a flat array of generally the same size as that which is used for the traditional linear volume layout, but the order in which the voxels are stored is different. In the recursive blocking method, the 3D volume data is recursively subdivided into preferably eight (8) octant blocks until the second lowest level it reaches is 2voxel blocks and finally a single voxel. The memory array indices of each block have the index of the voxel as shown in the exploded view of the front lower left corner in . The conversion between offset Oto 3D position Pare preferably done through bit manipulations. Alternatively they can be achieved through the use of lookup tables. To go from position to offset one can add the independent (separable) x, y, and z offsets which can be stored in a lookup table of 256 entries when processing a 256volume. This table requires 4\u00b7256B=1 KB which is less than 10% of even the smallest (16 K) Level 1 caches. The reverse conversion through lookup tables breaks the bit representation of the offset in groups that tan become the indices into position lookup tables whose entries need to be added to yield the 3D position. The data traversal is preferably done in linear storage order. Although the data layout is logically recursive, bit manipulations and lookup table computations are preferred fast and direct methods. A complete example of the recursive blocking of a volume will now be explained with reference to a 4volume.","Referring now to , a recursive blocking volume layout is shown for a 256volume. The memory array indices of each block have the index of the voxel as shown on the faces of the volume in . The memory array indices of each interior block have the index of the voxel as shown for the respective slices as shown in . Using the above equation where P=(x, y; z)=(3,1,1) and x, y, and z are in integers, the bit pattern ( . . . , x, x, x, x; . . . , y, y, y, y; . . . , z, z, z, z)=( . . . 0011; . . . 0001; . . . 0001) while Ohas the bit pattern ( . . . , z, y, x, z, y, x, z, y, x, z, y, x)=( . . . 000000001111) which equals 15 in integers. Referring now to , the lookup table for converting a 3D position to an offset is shown and includes separate offsets for each axis. The Index can be determined by the following equation:\n\nIndex=\n\nFor P=(x; y; z)=(3,1,1), the Index=LUTx[3]+LUTy[1]+LUTz[1]=9+2+4=15. For converting from the offset to the 3D position, the lookup tables in  are used to perform bit manipulations. The Lookup table for the index to position conversion breaks the bits of the index into groups. For each group there is a table which has x, y, and z positions as entries. Given an index it is broken into its bit groups and each corresponding table entry is added to yield the final x, y, and z position. The P=(x; y; z) can be determined by the following equation:\n\n=()=5-32-0[\n\nwhere A equals the integer value of the first three (3) places of the Index bit group and B equals the integer value of the second three (3) places of the Index bit group. For example, where the Index=15 integer=001111 binary, A=111 binary=7 integer and B=001 binary=1 integer. For index=15, P=(x; y; z)=LUT5-3[1]+LUT2-0[7]=(2,0,0)+(1,1,1)=(3,1,1).\n","The apparatus of the invention is an addressing unit configured with the method of the invention. The addressing unit can be, for example, any suitable computer, processor (e.g., digital signal processor, microprocessor, etc.), microcontroller, or circuit designed to be incorporated into a volume rendering system.","The present invention is designed for the use of hierarchical memory. All data paths are built to optimize the latency\/bandwidth\/cost tradeoffs. With the recursive blocking data layout and traversal, the time spent in the memory hierarchy levels close to the processing units is maximized. In the example of the 5Gau\u03b2 filter, all 125 voxels covered by the filter and even a complete 3D neighborhood of 16voxels is laid out cache-line collision free and requires only 4 KB cache space. Multiples of these neighborhoods fit easily onto each processing element. Thus, while traversing the current neighborhood, we can page in the next neighborhood to be touched by the algorithm.","Referring now to , a volume processing architecture known as EUREKA which employs a conventional polygon geometry pipeline, tightly coupled to an imagery pipeline will be described. The EUREKA architecture is similar too the Cube-5 architecture shown in  but now includes a geometry unit  for processing additional objects other than volumes as described below. The EUREKA architecture uses a three-level dynamic memory hierarchy similar to the Cube 5 architecture described above and holds volumes, partial computation and rendering results, surface objects, 2D or 3D textures (possibly mip-mapped), and images for image-based rendering. The EUREKA architecture includes a programmable processing element with embedded Level 1 memory. When arranged in parallel, the EUREKA units read data from their dedicated Level 2 memory modules, possibly exchange some data with their neighbors, process them and write back the results. The final ray value is collected through a data bus, which forwards it to the blending and warping units. A key feature of EUREKA is its ability to scale up the addition of identical pipelines and memory. The only bus on the system provides image order bandwidth and simple control information. Global ray communication required for rendering is achieved through fixed, local communication within and between pipelines.","The memory hierarchy is a cascade of memories with increasing bandwidth and decreasing size toward the PEs. The data is organized in blocks (cuboid subvolumes) allowing transfers of blocks between levels. Blocks move through the hierarchy as needed and are aligned with the scheduling grid at Level 1. Connections between neighboring EUREKA units allow high-bandwidth communication of ray information between spatially coherent cells instead of the usual up-over-down communication.","Normal volume data is read-only and thus can be discarded upon replacement in the lower levels of the memory. However, some blocks (e.g., ray data, reconstructed volume data, and global illumination data) are read-write and thus are written back up the hierarchy. EUREKA also lends itself to rendering of real-time 4D data by using a set of parallel I\/O ports connected to the Level 2 memories.","The EUREKA memory hierarchy enables efficient handling\/rendering of large datasets including those which are much larger than Level 1. Often, the entire dataset does not contribute to the final image due to empty or opaque data regions. We take advantage of this by employing space leaping and early ray termination. These interfame coherence allow the working set of blocks to remain resident in lower hierarchy levels.","A preferred embodiment of the EUREKA architecture includes standard a DRAM (e.g., SDRAM, RDRAM) to implement Level 2 memory. For example, 50 GB of bandwidth can be supplied using Rambus devices operating at 800 MHz and wide 512 bit datapath on the board. We can expect 100% bandwidth utilization of these DRAM devices because block transfers will always be larger than minimal burst lengths. Level 1 memory includes on-chip memory adjacent to the PEs. The preferred embodiment of the EUREKA architecture preferably includes embedded DRAM (eDRAM) for providing very dense storage.","The preferred embodiment of the EUREKA architecture includes an application programming interface (API) for allowing applications to access and use of the EUREKA hardware. Preferably the programming interface will also include a software development kit (SDK). Both the API and SDK are be compatible with OpenGL and Direct3D. VolVis, a comprehensive volume visualization software that provides a solid foundation for the API. Besides the meta-objects discussed above (polygon, voxel, image, and point), other commonly-used graphical objects include free-form parametric curves\/surfaces, curves\/surfaces defined by algebraic functions, free-form sculptured solids, subdivision solids CSG-based shapes, and deformable models for time-varying phenomena. The EUREKA architecture can adaptively sample aforementioned objects in an effective way and accurately convert them at the programming interface level.","EUREKA processes a set of arbitrarily positioned and oriented generalized rays one sample at a time in a round-robin fashion. To achieve spatial coherence, a set of rays are grouped together based on a regular 3D spatial partitioning. The group of rays are then processed one grid cell at a time or in parallel on multiple processing elements (PEs) similar too ray tracing of large geometric scenes. The rays consist of a data structure containing a start point, a direction, a partially composited color and opacity, and a final image destination. That is, each ray contains all the information necessary to continue processing without auxiliary information which can support arbitrary projections (e.g., perspective, fish-eye). Image rays are cast into the scheduling grid and each is stored in a queue associated with the intersected grid cell. The cells are scheduled for processing in an order that minimizes the total processing time. Scheduling heuristics are defined based on the status of the rays and ray queues. Further, the scheduler considers the data distribution throughout the memory hierarchy as well as the contribution of each cell toward completion of the image.","For ray casting, the scheduling algorithm takes advantage of the fact that the viewing and lighting rays all travel outward from the originating point. Ray tracing a cell is performed with a deep pipeline for resampling, classification, shading, compositing, and ray scattering. Usually there are more rays in the ray queue than pipeline stages, thus enabling full pipeline utilization.","The EUREKA architecture uses ray-directed rendering to efficiently combine into a common framework the following major rendering and processing paradigms: volume rendering, volumetric ray tracing with global illumination, mixing volumes with translucent polygons, texture mapping, image-based rendering, point-based rendering, tomographic reconstruction, and volume processing.","In Volumetric Global Illumination the EUREKA architecture uses a two pass approach. In the first pass, we generate an irradiance volume, a discretized approximation of the received light energy at each point in the volume. Furthermore, we permit scattering at every sample with directions based on an indexed bidirectional scattering distribution function (BSDF). In the second pass, we visualize the volume by shooting view rays and integrating the pre-computed irradiance at each voxel. In both viewing and rendering passes, rays are possibly scattered at every sample leading to a bi-directional method that automatically generates soft shadows, caustics, sub-surface scattering, and radiosity through multiple scattering.","In mixing polygons with volumes, we place them in buckets for each scheduling grid cell rather than between volume slices. When a PE is scheduled to render a cell, it receives both the volume data and polygons. The rays which are processed for that cell are, first, intersected with the polygons. The closest intersection point is stored in the ray and the ray is cast through the volume data, stopping at the polygon intersection point. Scattering (transmission or reflection) is computed and the secondary rays are similarly processed. EUREKA rasterizes polygons in the geometry pipeline to allow large polygon datasets to be rendered at high frame rates. The projected RGB\u03b1Z image is used to initialize the ray endpoints. Subsequently, the volume is rendered terminating the rays at the proper depth. This leverages existing geometry hardware to correctly render, in real-time, opaque polygons in harmony with volume rendering. Translucent polygons must be rendered in sorted order to yield correct results. The recursive ray tracing algorithm automatically handles any number of multiple translucent polygons.","Texture mapping is a widely-used technique to simulate high-quality image effects, such as surface details, lighting and shadows. The polygon engines are supplied texture mapping functionality with the imagery pipeline of EUREKA. The geometry pipeline's only responsibility will be to rasterize triangles, while the imagery pipeline will perform the texture lookups. One advantage of the programmable PEs performing the texturing operations is that higher-quality anti-aliased texture methods such as EWA can be used when desired. Deferred texturing lowers the cost of utilizing such higher quality methods.","Image-based techniques use images as rendering primitives. Rendering then becomes a resampling process, which can be efficiently implemented using a general-purpose CPU possibly with the assistance of texture-mapping hardware. Preferably EUREKA supports image-based rendering techniques with programmable PEs, blending, and warping units.","Points are represented as a smooth, radially symmetric 3D basis kernel function that can be pre-integrated into a 2D table, also called the footprint. Points can be located at arbitrary positions and can be represented as a multi-resolution hierarchy. EUREKA's ray-directed rendering approach will render points using the ray-based point-array traversal scheme of Mueller and Yagel. In that approach, rays traverse the points in volume space, computing the footprint table indices by simple differencing equations. The needed coefficients can be stored with the ray.","Tomographic reconstruction is a technique used to recover 3D volumes from a set of 2D projection images. These images can originate from an X-ray scanner as well as a photographic camera. Tomographic reconstruction consists of a series of volumetric backprojections, possibly intermixed with a series of froward projections when an iterative reconstruction algorithm is chosen. Both forward and backward projection can be efficiently implemented in EUREKA. The ray-directed approach enables the modeling of ray scattering and attenuation, which is crucial for high-fidelity reconstruction of volumes from functional modalities, such as SPECT and PET. Especially the functional modalities will benefit greatly from the tremendous speed of EUREKA which can reduce these reconstruction times by two orders of magnitude.","Volume processing includes segmentation, feature extraction, manipulation (e.g., transformation, deformation, sculpting, multi-volume or multi-field registration through warping, volumetric CSG, physics based modeling, FEM) and other non-rendering tasks which are performed on volumes. Many volume processing applications produce a new volume dataset which must be subsequently rendered. EUREKA's PEs will enable interactive processing, with sub-second feedback for combined processing and rendering.","In summary EUREKA is the first programmable scalable architecture that supports real-time, high-quality volume rendering and processing of high-resolution datasets. Furthermore, EUREKA is not just a volume rendering machine, it supports other graphics operations efficiently, such as rendering of surfaces (e.g., polygons), texture mapping, image-based rendering, point-based rendering, mixed rendering, volumetric ray tracing, tomographic reconstruction, and segmentation. It combines the efficiency of polygonal rendering, the expressive power of volume visualization, the photorealism of image-based methods and the simplicity of point-based rendering, in a single framework.","Although illustrative embodiments of the present invention have been described herein with reference to the accompanying drawings, it is to be understood that the invention is not limited to those precise embodiments, and that various other changes and modifications may be effected therein by one skilled in the art without departing from the scope or spirit of the present invention."],"GOVINT":[{},{}],"heading":["STATEMENT OF GOVERNMENT RIGHTS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":["FIG. 3","FIG. 2"]},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":["FIG. 6B","FIG. 6A"]},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":["FIG. 10","FIG. 9"]},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 18A"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 18B"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 20A"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":["FIG. 20B","FIG. 20A"]},{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 22","sup":"3 "},{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 25","sup":"3 "},{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 26"},{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 29"},{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 30"},{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0070","num":"0069"},"figref":"FIG. 32"},{"@attributes":{"id":"p-0071","num":"0070"},"figref":"FIG. 33"},{"@attributes":{"id":"p-0072","num":"0071"},"figref":"FIG. 34"},{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 35"},{"@attributes":{"id":"p-0074","num":"0073"},"figref":["FIG. 36","FIG. 35"]},{"@attributes":{"id":"p-0075","num":"0074"},"figref":"FIG. 37"},{"@attributes":{"id":"p-0076","num":"0075"},"figref":"FIG. 38"},{"@attributes":{"id":"p-0077","num":"0076"},"figref":"FIG. 39"},{"@attributes":{"id":"p-0078","num":"0077"},"figref":"FIG. 40"},{"@attributes":{"id":"p-0079","num":"0078"},"figref":"FIG. 41"},{"@attributes":{"id":"p-0080","num":"0079"},"figref":"FIG. 42"},{"@attributes":{"id":"p-0081","num":"0080"},"figref":"FIG. 43"},{"@attributes":{"id":"p-0082","num":"0081"},"figref":"FIG. 44"},{"@attributes":{"id":"p-0083","num":"0082"},"figref":"FIG. 45"},{"@attributes":{"id":"p-0084","num":"0083"},"figref":"FIG. 46"},{"@attributes":{"id":"p-0085","num":"0084"},"figref":"FIG. 47"},{"@attributes":{"id":"p-0086","num":"0085"},"figref":"FIG. 48"},{"@attributes":{"id":"p-0087","num":"0086"},"figref":"FIG. 49"},{"@attributes":{"id":"p-0088","num":"0087"},"figref":"FIG. 50"},{"@attributes":{"id":"p-0089","num":"0088"},"figref":"FIG. 51"},{"@attributes":{"id":"p-0090","num":"0089"},"figref":"FIG. 52"},{"@attributes":{"id":"p-0091","num":"0090"},"figref":"FIG. 53"},{"@attributes":{"id":"p-0092","num":"0091"},"figref":"FIG. 54"},{"@attributes":{"id":"p-0093","num":"0092"},"figref":"FIG. 55"},{"@attributes":{"id":"p-0094","num":"0093"},"figref":"FIG. 56"},{"@attributes":{"id":"p-0095","num":"0094"},"figref":"FIG. 56A"},{"@attributes":{"id":"p-0096","num":"0095"},"figref":"FIG. 56B"},{"@attributes":{"id":"p-0097","num":"0096"},"figref":["FIG. 56C","FIG. 56B"]},{"@attributes":{"id":"p-0098","num":"0097"},"figref":"FIG. 56D"},{"@attributes":{"id":"p-0099","num":"0098"},"figref":["FIG. 56E","FIG. 56D"],"b":"8"},{"@attributes":{"id":"p-0100","num":"0099"},"figref":["FIG. 56F","FIG. 56E"],"b":"5"},{"@attributes":{"id":"p-0101","num":"0100"},"figref":"FIG. 57"},{"@attributes":{"id":"p-0102","num":"0101"},"figref":"FIG. 58"},{"@attributes":{"id":"p-0103","num":"0102"},"figref":"FIG. 59"},{"@attributes":{"id":"p-0104","num":"0103"},"figref":"FIG. 60"},{"@attributes":{"id":"p-0105","num":"0104"},"figref":"FIG. 61"},{"@attributes":{"id":"p-0106","num":"0105"},"figref":"FIG. 62"},{"@attributes":{"id":"p-0107","num":"0106"},"figref":"FIG. 63"},{"@attributes":{"id":"p-0108","num":"0107"},"figref":"FIG. 64"},{"@attributes":{"id":"p-0109","num":"0108"},"figref":"FIG. 65"},{"@attributes":{"id":"p-0110","num":"0109"},"figref":"FIG. 66"},{"@attributes":{"id":"p-0111","num":"0110"},"figref":"FIG. 67"},{"@attributes":{"id":"p-0112","num":"0111"},"figref":"FIG. 68"},{"@attributes":{"id":"p-0113","num":"0112"},"figref":"FIG. 69"},{"@attributes":{"id":"p-0114","num":"0113"},"figref":"FIG. 70"},{"@attributes":{"id":"p-0115","num":"0114"},"figref":"FIG. 71"},{"@attributes":{"id":"p-0116","num":"0115"},"figref":"FIG. 72"},{"@attributes":{"id":"p-0117","num":"0116"},"figref":"FIG. 73"},{"@attributes":{"id":"p-0118","num":"0117"},"figref":"FIG. 74"},{"@attributes":{"id":"p-0119","num":"0118"},"figref":"FIG. 75"},{"@attributes":{"id":"p-0120","num":"0119"},"figref":["FIG. 76","FIG. 18"]},{"@attributes":{"id":"p-0121","num":"0120"},"figref":"FIG. 77"},{"@attributes":{"id":"p-0122","num":"0121"},"figref":"FIG. 78"},{"@attributes":{"id":"p-0123","num":"0122"},"figref":"FIG. 79"}]},"DETDESC":[{},{}]}
