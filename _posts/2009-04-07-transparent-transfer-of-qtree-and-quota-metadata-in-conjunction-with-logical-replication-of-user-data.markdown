---
title: Transparent transfer of qtree and quota metadata in conjunction with logical replication of user data
abstract: A technique that provides the ability to copy or move a volume that includes one or more quota structures, by using logical replication, where the volume and any quota structures in it are immediately accessible upon completion of the copy/move operation, and where the quota structures' usage control functionality is immediately functional and reliable upon completion of the copy/move operation. A user does not have to reinitialize the quota mechanism or invoke a quota scanner at the destination.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08078816&OS=08078816&RS=08078816
owner: NetApp, Inc.
number: 08078816
owner_city: Sunnyvale
owner_country: US
publication_date: 20090407
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["At least one embodiment of the present invention pertains to network storage systems, and more particularly, to a technique for transparent transfer of qtree and quota metadata with logical replication of user data.","A network storage controller is a processing system that is used to store and retrieve data on behalf of one or more hosts on a network. A storage server is a type of storage controller that operates on behalf of one or more clients on a network, to store and manage data in a set of mass storage devices, such as magnetic or optical storage-based disks or tapes. Some storage servers are designed to service file-level requests from hosts, as is commonly the case with file servers used in a network attached storage (NAS) environment. Other storage servers are designed to service block-level requests from hosts, as with storage servers used in a storage area network (SAN) environment. Still other storage servers are capable of servicing both file-level requests and block-level requests, as is the case with certain storage servers made by NetApp\u00ae, Inc. of Sunnyvale, Calif.","In at least one implementation, a storage server or server cluster makes data available to a client (host) system by presenting or exporting one or more volumes, or one or more logical containers of data referred to herein as quota trees (\u201cqtrees\u201d), to the client systems. A \u201cvolume\u201d is a logical data set which is an abstraction of physical storage, combining one or more physical mass storage devices (e.g., disks) or parts thereof into a single logical storage object, and which is managed as a single administrative unit, such as a single file system. A \u201cfile system\u201d is a structured (e.g., hierarchical) set of stored logical containers of data (e.g., volumes, LUNs, directories, files), which does not have to include or have its storage based on \u201cfiles\u201d per se.","From the perspective of a client system, each volume can appear to be a single disk drive. However, each volume can represent the storage space in a single physical storage device, a redundant array of independent disks (RAID) or a RAID group, an aggregate of some or all of the storage space in multiple physical storage devices, or any other suitable set of storage space. An \u201caggregate\u201d is a logical aggregation of physical storage; i.e., a logical container for a pool of storage, combining one or more physical mass storage devices (e.g., disks) or parts thereof into a single logical storage object, which contains or provides storage for one or more other logical data sets at a higher level of abstraction (e.g., volumes).","A \u201cqtree\u201d is a quota structure, which is a logical container of data that has a mechanism for controlling consumption of storage space. In certain storage servers a qtree is implemented as a top-level directory within a volume, where such directory has special properties (e.g., quota rules) for managing its storage space and\/or file consumption for one or more specific users or user groups, for example.","While qtrees are a useful mechanism for limiting storage device usage and file usage, they can present problems with regard to certain types of storage system operations, such as certain operations for mirroring, copying or moving a volume. For example, it may be desirable to have the capability to copy or move a volume from one file system to another, or from one aggregate to another. Further, it may be desirable to accomplish the operation by using logical replication of user data, as opposed to using physical replication at the block level within the file system. Logical replication is replication in which the replicated data set has the identical logical structure as the original data set but does not necessarily have the same physical structure (i.e., at the block level of a file system) as the original data set (a \u201cblock\u201d in this context is the smallest addressable unit of contiguous data used by a given file system to manipulate and transfer data, which is commonly though not necessarily 4 kbytes in size). It may be desirable to use logical replication instead of a physical block level replication, because that way a volume or other type of data set can be copied or moved notwithstanding differences in the file system formats and structures at the source and the destinations.","However, logical replication presents a problem when the volume being moved (or copied) includes one or more qtrees on which quotas are being enforced. In order to have reliable control of file or block usage in a qtree, an up-to-date accounting of file usage and block usage by the qtree needs to be maintained. Such an accounting is generated by scanning a volume that contains a qtree to determine its file and block usage, and the element which performs this scanning function in the storage server is called a quota scanner. However, if the file system or aggregate used at the destination is of a different type or format than the file system at the source, the quota accounting generated at the source may not be valid for the destination. For example, the source aggregate may be a 32-bit aggregate while the destination aggregate is a 64-bit aggregate, or vice versa. As another example, the source file system may be a FlexVol\u00ae type file system while the destination file system is a striped file system (FlexVol is a trademark of NetApp, Inc.).","One way to handle this problem would be to rescan the volume that contains the qtree after the volume move operation is complete, to generate a new quota accounting at the destination. However, it can take a long time to complete such a scan\u2014potentially hours or days, depending on the size of the volume\u2014during which the qtree's usage control functionality is not available. Consequently, the volume move operation can cause a significant and prolonged service disruption to administrative users who want to limit the qtree's block\/file usage. That is, while the data is immediately available to the client users, the file system consumption is not being controlled immediately after the move. This situation is particularly undesirable when a volume move is desired to be \u201ctransparent\u201d, i.e., where a user does not perceive any interruption in access to the volume and is not required to unmount the volume at the source and remount it at the destination.","The technique introduced here provides the ability to copy or move a volume that includes one or more quota structures, such as qtrees, by using logical replication, where the volume and any quota structures it includes are immediately accessible upon completion of the copy\/move operation, and where the quota structures' usage control functionality is immediately functional and reliable upon completion of the copy\/move operation. (In this context, a move operation can be considered just a special type of copy operation, i.e., one in which the copy at the source volume is deleted at the end of the operation. Therefore, references to \u201ccopy\u201d and \u201cmove\u201d in this document are not to be interpreted as mutually exclusive). The latest quota rules from the source volume are applied to the destination volume. The user does not have to reinitialize the quota mechanism or invoke a quota scanner at the destination after the copy\/move operation has completed. Further, a user does not have to unmount the volume at the source and remount it at the destination. As such, the entire volume copy\/move operation is transparent to the user, at least to the extent there is effectively no disruption in access to the volume and its included qtrees.","Briefly described, the technique sends at least some quota metadata from the source to the destination separately from the user data being replicated. The destination dynamically creates and updates its own quota accounting of the volume being copied, based on the user data and quota metadata it receives as part of a logical replication. In some cases this involves making corrections to the quota accounting on the destination, as in some instances the quota metadata may be received at the destination out of order relative to its corresponding user data. Nonetheless, the technique introduced here allows for such instances, such that at the end of the process, all of the quota metadata, including the quota rule, quota state and quota accounting, are current (i.e., up-to-date and correct) at the destination.","The technique in one embodiment includes a method which includes storing a data set, such as a volume, at a source in a network storage controller in a network storage system, and implementing a quota mechanism (e.g., a qtree) in the data set at the source to control usage of storage space by the data set. The quota mechanism includes a quota state, one or more quota rules and a quota accounting of the data set at the source. The method further includes executing a copy\/move operation, which includes performing a logical replication of the data set from the source to the destination, and automatically implementing the quota mechanism for the data set at the destination so that upon completion of the copy\/move operation the quota state, the quota rule and the quota accounting of the data set are in effect and up-to-date at the destination. In this context, \u201cautomatically\u201d means that the action being referred to is done without any explicit (administrative) user command\/request to perform that action.","Automatically implementing the quota mechanism for the data set at the destination can include dynamically creating and updating the destination quota accounting of the data set at the destination in response to receiving, at the destination, data of the data set at the source, as part of the logical replication of the data set (which may include one or more \u201csnapshots\u201d, i.e., persistent point-in-time images of the data set). \u201cDynamically\u201d in this context means that the creation and updating of the quota accounting at the destination occurs as the user data (blocks) and user data's metadata (inode information) are received at the destination from the source. More specifically, this operation can include adding block and file count to a new quota structure (e.g., qtree) at the destination.","In one embodiment, after the destination receives blocks of the data set from the source and updates the destination quota accounting of the data set, it then receives metadata attributes of the blocks and corrects the destination quota accounting based on the metadata attributes. Correcting the quota accounting at the destination can include debiting (subtracting) the number of blocks from the first quota structure and crediting (adding) the number of blocks to a second quota structure (e.g., qtree) associated with the data set, based on the metadata attributes.","By virtue of the use of logical replication, the file system and\/or aggregate that contain the data set at the source can be different from the file system and\/or aggregate (respectively) at the destination, without adversely affecting the move operation or the qtree functionality. For example, if a particular file consumed 100 blocks on the source file system based on its format, and if the same file were to consume 50 blocks on the destination file system, the quota accounting technique introduced here will handle this situation automatically and correctly, since the accounting is dynamic.","Other aspects of the technique will be apparent from the accompanying figures and from the detailed description which follows.","References in this specification to \u201can embodiment\u201d, \u201cone embodiment\u201d, or the like, mean that the particular feature, structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment.","System Environment",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIGS. 1 and 2","FIG. 1","FIG. 1"],"b":["104","1","104","2","102","106","104","1","104","2","102","102","108","110","112","105","212"]},"The storage server  may be, for example, one of the FAS-xxx family of storage server products available from NetApp, Inc. The client systems .-. are connected to the storage server  via the computer network , which can be a packet-switched network, for example, a local area network (LAN) or wide area network (WAN). Further, the storage server  is connected to the disks  via a switching fabric , which can be a fiber distributed data interface (FDDI) network, for example. It is noted that, within the network data storage environment, any other suitable numbers of storage servers and\/or mass storage devices, and\/or any other suitable network technologies, may be employed.","The storage server  can make some or all of the storage space on the disk(s)  available to the client systems .-. in a conventional manner. For example, each of the disks  can be implemented as an individual disk, multiple disks (e.g., a RAID group) or any other suitable mass storage device(s). The storage server  can communicate with the client systems .-. according to well-known protocols, such as the Network File System (NFS) protocol or the Common Internet File System (CIFS) protocol, to make data stored on the disks  available to users and\/or application programs. The storage server  can present or export data stored on the disk  as volumes and\/or qtrees, to each of the client systems .-..","Various functions and configuration settings of the storage server  and the mass storage subsystem  can be controlled from a management station  coupled to the network . Among other operations, a volume move such as described herein can be initiated from the management station .",{"@attributes":{"id":"p-0040","num":"0039"},"figref":["FIG. 2","FIG. 1","FIG. 2"],"b":["200","204","204","1","204","202","206","204","202","202","208","208","1","208","210","212","212","1","212","212"]},"Each of the nodes  is configured to include several modules, including an N-module , a D-module , and an M-host  (each of which can be implemented by using a separate software module) and an instance of a replicated database (RDB) . Specifically, node . includes an N-module ., a D-module ., and an M-host .; node .N includes an N-module .N, a D-module .N, and an M-host .N; and so forth. The N-modules .-.M include functionality that enables nodes .-.N, respectively, to connect to one or more of the client systems  over the network , while the D-modules .-.N provide access to the data stored on the disks .-.N, respectively. The M-hosts  provide management functions for the clustered storage server system . Accordingly, each of the server nodes  in the clustered storage server arrangement provides the functionality of a storage server.","The RDB  is a database that is replicated throughout the cluster, i.e., each node  includes an instance of the RDB . The various instances of the RDB  are updated regularly to bring them into synchronization with each other. The RDB  provides cluster-wide storage of various information used by all of the nodes , including a volume location database (VLDB) (not shown). The VLDB is a database that indicates the location within the cluster of each volume in the cluster (i.e., the owning D-module  for each volume) and is used by the N-modules  to identify the appropriate D-module  for any given volume to which access is requested.","The nodes  are interconnected by a cluster switching fabric , which can be embodied as a Gigabit Ethernet switch, for example. The N-modules  and D-modules  cooperate to provide a highly-scalable, distributed storage system architecture of a clustered computing environment implementing exemplary embodiments of the present invention. Note that while there is shown an equal number of N-modules and D-modules in , there may be differing numbers of N-modules and\/or D-modules in accordance with various embodiments of the technique described here. For example, there need not be a one-to-one correspondence between the N-modules and D-modules. As such, the description of a node  comprising one N-module and one D-module should be understood to be illustrative only.",{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 3","b":["208","301","301","320","340","370","380","390","370","208","208","270","214","216"]},"The storage controller  can be embodied as a single- or multi-processor storage system executing a storage operating system  that preferably implements a high-level module, such as a storage manager, to logically organize the information as a hierarchical structure of named directories, files and special types of files called virtual disks (hereinafter generally \u201cblocks\u201d) on the disks. Illustratively, one processor  can execute the functions of the N-module  on the node  while another processor  executes the functions of the D-module .","The memory  illustratively comprises storage locations that are addressable by the processors and adapters , ,  for storing software program code and data structures associated with the present invention. The processor  and adapters may, in turn, comprise processing elements and\/or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system , portions of which is typically resident in memory and executed by the processing elements, functionally organizes the storage controller  by (among other things) invoking storage operations in support of the storage service provided by the node . It will be apparent to those skilled in the art that other processing and memory implementations, including various computer readable storage media, may be used for storing and executing program instructions pertaining to the technique introduced here.","The network adapter  includes a plurality of ports to couple the storage controller  to one or more clients  over point-to-point links, wide area networks, virtual private networks implemented over a public network (Internet) or a shared local area network. The network adapter  thus can include the mechanical, electrical and signaling circuitry needed to connect the storage controller  to the network . Illustratively, the network  can be embodied as an Ethernet network or a Fibre Channel (FC) network. Each client  can communicate with the node  over the network  by exchanging discrete frames or packets of data according to pre-defined protocols, such as TCP\/IP.","The storage adapter  cooperates with the storage operating system  to access information requested by the clients . The information may be stored on any type of attached array of writable storage media, such as magnetic disk or tape, optical disk (e.g., CD-ROM or DVD), flash memory, solid-state disk (SSD), electronic random access memory (RAM), micro-electro mechanical and\/or any other similar media adapted to store information, including data and parity information. However, as illustratively described herein, the information is stored on disks . The storage adapter  includes a plurality of ports having input\/output (I\/O) interface circuitry that couples to the disks over an I\/O interconnect arrangement, such as a conventional high-performance, Fibre Channel (FC) link topology.","Storage of information on disks  can be implemented as one or more storage volumes that include a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number (VBN) space on the volume(s). The disks  can be organized as a RAID group. One or more RAID groups together form an aggregate. An aggregate can contain one or more volumes\/file systems.","The storage operating system  facilitates clients' access to data stored on the disks . In certain embodiments, the storage operating system  implements a write-anywhere file system that cooperates with one or more virtualization modules to \u201cvirtualize\u201d the storage space provided by disks . In certain embodiments, a storage manager  () logically organizes the information as a hierarchical structure of named directories and files on the disks . Each \u201con-disk\u201d file may be implemented as set of disk blocks configured to store information, such as data, whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module(s) allow the storage manager  to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers (LUNs).","In the illustrative embodiment, the storage operating system  is a version of the Data ONTAP\u00ae operating system available from NetApp, Inc. and the storage manager  implements the Write Anywhere File Layout (WAFL\u00ae) file system. However, other storage operating systems are capable of being enhanced for use in accordance with the principles described herein.",{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 4","b":["330","330","410","410","214","410","412","206","410","410","440"],"i":"a "},"In addition, the storage operating system  includes a set of layers organized to form a storage server  that provides data paths for accessing information stored on the disks  of the node . The storage server  also forms the D-module  in combination with underlying processing hardware. To that end, the storage server  includes a storage manager module  that manages any number of volumes , a RAID system module  and a storage driver system module . At least one of the volumes  includes at least one qtree  and at least one standard directory .","The storage manager  primarily manages a file system (or multiple file systems) and serves client-initiated read and write requests. The storage manager  includes a quota scanner , which can scan any volume  that has a qtree  and whose quota state is ON, to generate a quota accounting. More specifically, the quota scanner  counts and maintains statistics on the number of files and blocks used by each qtree in a given volume . The RAID system  manages the storage and retrieval of information to and from the volumes\/disks in accordance with a RAID redundancy protocol, such as RAID-4, RAID-5, or RAID-DP, while the disk driver system  implements a disk access protocol such as SCSI protocol or FCP.","The storage server  also includes a CF interface module to implement intra-cluster communication  with N-modules and\/or other D-modules. The CF interface modules and can cooperate to provide a single file system image across all D-modules  in the cluster. Thus, any network port of an N-module  that receives a client request can access any data container within the single file system image located on any D-module  of the cluster.","The storage server  further includes a replication module , which includes the ability to perform logical replication of any volume. In doing so, the replication module  can perform the replication operations of either the source or the destination (or as both if the source and destination are on the same D-module ).","The CF interface modules  implement the CF protocol to communicate file system commands among the modules of cluster over the cluster switching fabric  (). Such communication can be effected by a D-module exposing a CF application programming interface (API) to which an N-module (or another D-module) issues calls. To that end, a CF interface module  can be organized as a CF encoder\/decoder. The CF encoder of, e.g., CF interface on N-module  can encapsulate a CF message as (i) a local procedure call (LPC) when communicating a file system command to a D-module  residing on the same node or (ii) a remote procedure call (RPC) when communicating the command to a D-module residing on a remote node of the cluster. In either case, the CF decoder of CF interface on D-module  de-encapsulates the CF message and processes the file system command.","In operation of a node , a request from a client  is forwarded as a packet over the network  and onto the node , where it is received at the network adapter  (). A network driver of layer  processes the packet and, if appropriate, passes it on to a network protocol and file access layer for additional processing prior to forwarding to the storage manager . At that point, the storage manager  generates operations to load (retrieve) the requested data from disk  if it is not resident in memory . If the information is not in memory , the storage manager  indexes into a metadata file to access an appropriate entry and retrieve a logical VBN. The storage manager  then passes a message structure including the logical VBN to the RAID system ; the logical VBN is mapped to a disk identifier and disk block number (DBN) and sent to an appropriate driver (e.g., SCSI) of the disk driver system . The disk driver accesses the DBN from the specified disk  and loads the requested data block(s) in memory for processing by the node. Upon completion of the request, the node (and operating system) returns a reply to the client  over the network .","The data request\/response \u201cpath\u201d through the storage operating system  as described above can be implemented in general-purpose programmable hardware executing the storage operating system  as software or firmware. Alternatively, it can be implemented at least partially in specially designed hardware. That is, in an alternate embodiment of the invention, some or all of the storage operating system  is implemented as logic circuitry embodied within a field programmable gate array (FPGA) or an application specific integrated circuit (ASIC), for example.","The N-module  and D-module  can be implemented as processing hardware configured by separately-scheduled processes of storage operating system ; however, in an alternate embodiment, the modules may be implemented as processing hardware configured by code within a single operating system process. Communication between an N-module  and a D-module  is thus illustratively effected through the use of message passing between the modules although, in the case of remote communication between an N-module and D-module of different nodes, such message passing occurs over the cluster switching fabric . A known message-passing mechanism provided by the storage operating system to transfer information between modules (processes) is the Inter Process Communication (IPC) mechanism. The protocol used with the IPC mechanism is illustratively a generic file and\/or block-based \u201cagnostic\u201d CF protocol that comprises a collection of methods\/functions constituting a CF API.","Volume Copy\/Move","While the technique introduced here is described in the context of a volume move operation, it is to be understood that the technique is also potentially applicable to other types of operations that involve copying data with associated quotas, such as data protection mirroring and replication for load sharing, for example.","The technique can be used in conjunction with a so-called \u201ctransparent\u201d volume move operation, in which users do not perceive any interruption in access to the volume and a user is not required to unmount the volume at the source and remount it at the destination, that is, the volume move operation is transparent to the user. To that extent, the quota mechanism described herein serves to facilitate the transparency of a volume move operation.","During a transparent volume move operation, according to one embodiment, client write requests continue to be targeted at the source volume, while the data from the source is replicated to the destination. When the replication process gets closer in time to the active filesystem, the replication module  reaches the final stage of the move, in which the source volume is quiesced (i.e., no write operations from clients are allowed), as discussed below. During this quiesced time, write operations are failed with a special error code that will only cause the N-module that received the write request to retry the operation without sending a failure message back to the requesting client. In the final stage of the volume move, a final snapshot of the source volume is taken to capture the final changes in the active filesystem and is replicated to the destination. Once this is done, the VLDB is updated indicating that access to this volume should henceforth target the new volume. After the VLDB update the N-module which is retrying the client operation accesses the relocated volume and replies to the client. Thus, while the clients may notice a temporary delay in response (while the N-blade is retrying operations), the file system at all times is available to the clients, thereby making the volume move \u201ctransparent\u201d.","As noted above, the storage server  () includes a replication module , which can perform logical replication of a volume, either as part of a copy operation or a move operation. Replication can accomplished by communicating user data and metadata between two different D-modules  (i.e., a source D-module and a destination D-module), each of which includes a replication module . The copy or move operation may involve copying or moving a volume from one type of aggregate to a different type of aggregate (e.g., a 32-bit aggregate to a 64-bit aggregate, or vice versa). Likewise, the copy or move operation may involve copying or moving a volume from one type of file system to another type of file system (e.g., from a FlexVol file system to a striped file system, or vice versa). Note that while the technique is described in the context of a clustered system, such as shown in , it can also be used in a non-clustered system.","In accordance with the technique introduced here, logical replication is used to copy or move a volume, rather than physical replication. Logical replication is replication in which the replicated (destination) data set has the identical logical structure as the original (source) data set but does not necessarily have the same physical structure (i.e., at the block level of a file system) as the original data set. In this context, logical replication ensures only that, for corresponding data on the source and destination: 1) the mode number is the same (ii) the mode attributes are same (iii) for files, the mode's contents, i.e., level-0 block data is same, and (iv) directory entries are logically identical; other attributes may be different between the source and destination for corresponding data. This is in contrast with physical replication at the block level of the file system.","To understand the difference, first note that in certain storage systems, each data block in a volume can be represented by both a physical block identified by a physical block pointer, and a logical block identified by a corresponding logical block pointer. In the illustrative embodiments described here, a given block of data has both a logical block pointer and a physical block pointer, which refer to the same data. However, the physical block pointer indicates the actual physical location of the data block on a storage medium, whereas the logical block pointer indicates the logical position of the data block within the data set (e.g., a file) relative to other data blocks. In one embodiment, a logical block pointer is a block pointer that is used by the RAID system  () and the storage manager , for communication between those two elements; whereas a physical block pointer is a block pointer that is used by the RAID system  and the storage driver system  for communication between those two elements.","In a system such as this, with physical replication at the block level, the replication process creates a copy that has the identical structure of physical block pointers as the original data set. With logical replication, the copy has the identical structure of logical block pointers as the original data set but may (and typically does) have a different structure of physical block pointers than the original data set.","However, logical replication presents a problem when the volume being moved (or copied) includes one or more qtrees with quotas enforced. To have reliable control of file or block usage in a qtree, an up-to-date accounting of file usage and block usage by the qtree needs to be maintained. However, if the file system or aggregate used at the destination is of a different type or format than the file system at the source, the quota accounting generated at the source may not be valid for the destination.","The technique introduced here overcomes this problem, however, such that at the end of the copy operation, all of the quota metadata for any qtree in the volume being moved, including the quota state, the quota rule and the quota accounting of the data set, are in effect and up-to-date at the destination. As such, the quotas do not have to be initialized at the destination\u2014they will be enforced and current immediately upon completion of the move operation.","Briefly described, the technique sends at least some quota metadata from the source to the destination separately from the user data being replicated, although some of the quota metadata may be sent to the destination concurrently with the user data. The destination then dynamically creates and updates its own quota accounting of the volume being moved, based on the user data and quota metadata it receives during the logical replication. Here \u201cdynamically\u201d means that the operation occurs on-the-fly, i.e., as user data are being received during the logical replication. In some instances, updating the quota accounting on the destination involves making corrections to the quota accounting, since in some instances the quota metadata may be received at the destination out of order relative to its corresponding user data. For example, user data representing the contents of an inode that is part of a qtree may be received at the destination before certain attributes of the qtree to which the inode belongs are received at the destination. Nonetheless, the technique introduced here allows for such instances, such that at the end of the process, all of the quota metadata, including the quota rule, quota state and quota accounting, are current at the destination.","The technique will now be further described with reference to . As shown in , in one embodiment every directory, qtree and file in a volume is represented by a separate inode . An inode  is a metadata container that resides in special type of block. An inode  contains metadata about the storage object it represents (e.g., file, qtree or directory) and is the root of a hierarchical structure  of blocks that make up the storage object (the lowest level of such structure is the actual user data of the object). As shown in , metadata  included in an inode  can include, for example: the inode number of the object (a unique identifier of the inode); the type of storage object which the inode represents; a tree identifier (TID) which uniquely identifies the qtree with which the inode is associated, if any; a parent inode number identifying the inode of the parent entity of the inode; ownership of the storage object, including a user identifier (UID) and group identifier (GID); access permissions for the object; and size of the object. An inode  also includes a set of pointers  to lower-level blocks , in addition to the other metadata  noted above. A given storage object, such as a file, qtree or directory, is stored in the form of a hierarchical structure , which includes its inode , typically a number of indirect blocks , and a number of lowest level blocks (also called \u201cdirect blocks\u201d or \u201clevel-0\u201d blocks)  that contain the actual user data.","As shown in , the inodes of the files, qtrees and directories in a volume are stored in a special system file, i.e., an inode file . A separate inode file  is maintained for each volume. Each inode  in an inode file  is the root of a hierarchical structure  of blocks. The location of the inode file  for each volume is stored in a Volume Information (\u201cVolumeInfo\u201d) block  associated with that volume. The VolumeInfo block  is a metadata container that contains metadata that applies to the volume as a whole.","Any given volume includes both user data and metadata.  shows, for an illustrative volume  which includes at least one qtree (not shown), that the volume  includes various user data  and various metadata , where the metadata  includes quota-related metadata , and the quota-related metadata  includes various types of metadata relating to the qtree(s) and quota accounting. Specifically, the quota-related metadata  includes a quota control metafile , a quota database , a quota B-plus tree metafile , a quota names metafile , and a qtree metafile . All of the quota related metadata  will be recreated at the destination when the volume  is copied or moved from the source to the destination.","The quota control metafile  includes the quota status and a logging interval for the entire volume . The quota status specifies whether quotas are ON or OFF for the volume , and the logging interval specifies the minimum time interval by which an error message should be output to an administrative user when a quota limit is exceeded.","The quota database metafile  includes the quota rules and the quota accounting for each qtree (i.e., an accounting of the number of files and blocks associated with each qtree). Quota rules can specify limits for usage of storage devices (e.g., disks), which can include different types of limits corresponding to different levels of notification to an administrative user, such as \u201chard\u201d limits, thresholds and \u201csoft\u201d limits. Quota rules can also specify limits for file usage (e.g., hard and soft limits).","In one embodiment there are two types of quota rules: default rules and explicit rules. The target of an explicit rule can be a particular qtree, user, group or set of users. In one embodiment only one quota rule can be created for a given target. An example of an explicit rule with a qtree as its target is:\n\n","Note that the actual syntax used for a quota rule in the quota database metafile  is not germane to this description; essentially any convenient format can be used.","A default rule is a rule which applies to multiple targets. For example, an administrator can define a default tree rule that says, for any qtree created, the storage limit is 10 GB. Once a default rule is in place, any new qtree that gets created will inherit this rule, unless an explicit rule is defined that qtree. Such a rule, when inherited from the default rule, is called a \u201cderived rule\u201d and gets added to the quota database metafile  as a rule to be enforced for that target.","The quota B-plus tree metafile  allows for fast random access of records in the quota database metafile  and takes the TID attribute of a qtree root inode as its index key, outputting an identifier of a record in the quota database metafile . The quota names metafile  contains the ASCII string of the name an administrative user gives to the target of each quota rule in the quota database metafile  (where the target of a quota rule can be a qtree, user or group). The record for the associated quota rule has a pointer to the appropriate entry in the quota names metafile . The qtree metafile  includes metadata specifically about each qtree, including the TID and security style (e.g., Unix, NTFS or mixed type) of the qtree and whether opportunistic locks (\u201cop locks\u201d) are enabled for the qtree. In addition, the qtree metafile also includes the inode number of the inode of the qtree (i.e., the qtree root inode), a generation number of the qtree root inode, and a name location cookie which stores the location of the directory entry that contains the name of the qtree root inode within the root directory of the volume . The generation number is used to determine whether the file handle that the client has represents the file that the server has (it does if the generation numbers match).","As shown in , a given volume  typically includes an active file system  (which is the volume in its current state) and one or more snapshots of the volume .-.M. The active file system  includes all of the user data  and all of the metadata  of the volume . Each snapshot  also includes all of the user data  and all of the metadata  (including quota-related metadata ) of the volume, for a particular past point in time, i.e., the point in time at which the snapshot was created.","As shown in , the technique introduced here involves sending information from the source to the destination using at least two separate processes. In one embodiment the replication module  () includes a data replication engine , which implements one of these processes, and a configuration replication engine  which implements the other process. User data and associated metadata  of snapshots (not shown) of the volume  at the source  are sent to the destination  through the data replication engine , which accomplishes that using logical replication. The data replication engine  also updates the quota metadata . If the source  and destination  are associated with different D-modules, then this may be done through two instances of the data replication engine , one at the source and one at the destination.","However, quota-related metadata  are sent to the destination  by using a separate communication process, executed by the configuration replication engine , which does not involve logical replication. If the source  and destination  are associated with different D-modules, then this may be done through two instances of the configuration replication engine , one at the source and one at the destination. The configuration replication engine  can use any convenient communication protocol and\/or communication channel. Further, at least some of the quota-related metadata  are sent to the destination before the user data and associated metadata  are sent, to enable dynamic updating of the quota accounting at the destination.","In one embodiment, the data replication engine  uses four types of messages to communicate with the destination, namely, Push_Inode messages, Push_Blocks messages, Push_Holes messages and Push_Dirent messages.","A Push_Inode message is used only to send an inode. This type of message includes the inode number, generation, object type, TID, UID, GID, etc. of the inode being sent. It is used to update the quota accounting and qtree metafile at the destination.","A Push_Blocks message is used to send one or more user data blocks or an associated (non-inode) metadata blocks. This type of message includes the inode number, generation, type of inode and offset of the blocks being transmitted, as well as the actual block(s). It is used to update the quota accounting at the destination.","A Push_Holes message is used to indicate that a particular offset that was used to hold a block no longer holds a block, i.e., it is a \u201chole\u201d. This type of message includes the inode number, generation, type of inode and offset of the hole(s). It is used to update the quota accounting at the destination.","A Push_Dirent (push directory entry) message is used to give the directory entry (e.g., a file) for a particular directory inode. This type of message contains the directory's inode number and generation number and the directory entry's name, inode number and generation number. It is used to update the qtree metafile at the destination.","The message type of a given message can be identified in any convenient manner, such as by a header in the message. The specific format of these messages is not important; any convenient format can be used. If the source  and destination  are associated with different D-modules , then communication of these messages can be carried out via the CF interface modules () of those D-modules .","In one embodiment, the configuration replication engine  uses four types of messages to communicate quota metadata with the destination, namely, Qtree_Attrs messages, Quota_Status messages, Quota_Rules messages and Quota_Names messages. A Qtree_Attrs message communicates information about a qtree's security style and oplocks. A Quota_Status message communicates whether quotas are ON or OFF and the logging interval for the volume. A Quota_Rules message communicates the quota rules configured for any qtree within a volume. A Quota_Names message communicates to the destination that the destination can generate the qtree names to be populated in the quota names metafile  at the destination, as described further below.","The technique introduced here, according to one embodiment, can be summarized as follows and as illustrated in : The technique begins at  with storing a data set at a source in a network storage controller of a network storage system. The technique implements a quota mechanism in the network storage controller for the data set at the source at , to control usage of storage space by the data set. The quota mechanism includes a quota state, a quota rule and a quota accounting of the data set at the source. The technique then at  executes a copy operation to copy the data set from the source to a destination in the network storage system. The copy operation includes performing a logical replication of the data set from the source to the destination, and automatically implementing the quota mechanism for the data set at the destination so that upon completion of the copy operation, the quota state, the quota rule and the quota accounting of the data set are in effect and up-to-date at the destination.","An embodiment of the process  of copying or moving a volume is further discussed now with reference to . It is assumed that the volume to be copied\/moved includes at least one qtree with quotas enforced on the volume. It is further assumed that the volume includes at least one pre-existing snapshot, where \u201cpre-existing\u201d means that the snapshot already exists at the time the copy\/move operation is initiated. As shown in , the overall process can be considered to have three stages: a preliminary stage  which sets up the destination volume, followed by a replication stage , and then a final stage  of establishing consistency between the volume at the source and the volume at the destination (in the case of a move, the source volume is effectively deleted at the end of the process).",{"@attributes":{"id":"p-0092","num":"0092"},"figref":"FIG. 8","b":["701","801","802"]},"The snapshot created at  includes the quota metadata for the volume, including its quota state and any associated quota rules. Therefore, at  the process sends the quota state (ON or OFF) for the volume, from the snapshot generated at , to the destination. The process then creates a quota control metafile  in the destination volume and stores the quota state in that metafile at .","Next, if the source quota state is ON for the volume at , the process proceeds to -; otherwise, the first stage  ends at this point. At  the process sends the quota rule for each qtree at the source, from the snapshot generated at , to the destination. At  the process creates a quota database metafile  in the destination volume and stores the quota rules that were sent at  in that metafile. Also at  the process creates and populates a quota B-plus tree metafile  for the received quota rules at the destination. At  the process initiates operation of a quota scanner  on the volume at the destination. After  the process ends.","Communication of this metadata between the source and the destination can be done using any convenient communication protocol or channel; however, in one embodiment these steps are done using a communication process that is separate from the logical replication process. In one embodiment, the quota state and quota rules are communicated from one D-module  associated with the source to another D-module  associated with the destination.",{"@attributes":{"id":"p-0096","num":"0096"},"figref":["FIG. 9","FIG. 4"],"b":["702","702","216","216","440","216"],"i":"b "},"The following actions are performed for each pre-existing snapshot of the volume at the source. Initially, at  the process selects the first\/earliest (or next) pre-existing snapshot of the volume at the source. At  the process sends qtree configuration information for each qtree in the volume from the source to the destination. The qtree configuration information sent in this step includes the TID, security style and op locks of each qtree. At  the process uses logical replication to send user data and certain metadata of the snapshot from the source to the destination. The metadata sent during  includes the inodes of the volume's qtrees, directories and files, including inode attributes such as inode numbers, generation numbers, etc.","At  the process creates or updates the destination quota accounting in the quota database metafile of the destination volume. The quota accounting is maintained on a per-qtree basis. This operation is performed dynamically in response to the receipt of snapshot user data and metadata in , as such user data and metadata are received at the destination. Creating or updating the quota accounting is done based on the inode attributes received (e.g., TIDs) and corresponding inode attributes already stored on disk at the destination, as described further below in relation to .","At , the process populates the qtree metafile in the volume at the destination with each qtree root's inode number, generation number and name location cookie, whenever these metadata are received at the destination. Hence, this operation is also performed dynamically in response to the receipt of user data and metadata in .","After all of the snapshot user data and metadata for a given snapshot have been transferred and processed in this manner (), then at  the process updates qtree names in the quota names metafile  at the destination. This is done by the configuration replication engine  sending a Quota_Names message, from the source to the destination.","To understand operation  better, note that a derived rule for a qtree gets created on the destination when a new TID is first detected at the destination from a Push_Inode replication message (assuming there is a default rule for qtrees in place). When such a derived rule gets added to the quota database metafile , the particular record in the quota database metafile  has to point to a location within the quota names metafile  that contains the path of the affected qtree. However, this location information within the derived record is not populated during the derived rule creation on the destination as part of volume copy\/move, since the path of the qtree to which the new TID refers is not yet available. To populate this location information within the derived record, a Quota_Names message is sent from the source to the destination after the logical replication is finished for a given snapshot. At the destination, as part of this message, the process \u201cwalks\u201d through the quota database metafile  at the destination, finds the derived rules and checks whether the location information is populated and the location points to the right path of the qtree. If the location information is not populated or the path of the qtree is different, it is corrected at the destination at this time.","Referring back to , at  the process creates a corresponding snapshot in the destination volume based on the transferred user data and metadata. After all the pre-existing snapshots have been processed in this manner (, ), the replication stage  of the process ends.",{"@attributes":{"id":"p-0103","num":"0103"},"figref":"FIGS. 10A and 10B","b":"904"},"Initially, at  the destination receives a message from the source. If the message is a Push_Inode message (), the process proceeds to . At , the process compares the generation number and object type in the Push_Inode message with the generation number and type for the inode on disk which has the inode number specified in the received message. If both values match (), then at  the process decrements the quota usage appropriately for the old TID (i.e., the TID that is present in the inode on disk). Specifically, the process decrements the blocks count by the number, B, of blocks currently consumed by this inode on disk, and decrements the file count by one. Additionally, if the generation number and type match, the process increments the blocks count by B and the file count by one for the new TID (i.e., the TID that has arrived with the Push_Inode message). The process then ends.","If the generation number or type in the Push_Inode message do not match those on disk for the corresponding inode number, then from  the process branches to , where it determines whether the type indicated in the inode on disk is a valid type. For example, an inode which is either unused or represents an access control list (ACL) is considered to be invalid for purposes of quota accounting. Other types of modes (e.g., regular, directory, stream, special files) are considered to be valid. If the type is valid, then at  the process decrements the blocks count by B and decrements the file count by one for the old TID. Next, at  the process reinitializes the specified inode at the destination with the new generation number and type values, and then increments the file count for the new TID by one at . The process then ends. If the type is not valid at , the process skips from  to , described above.","If the received message is a Push_Blocks message (), then from  the process proceeds to . At , the process compares the generation number and object type in the Push_Blocks message with the generation number and type for the inode on disk which has the inode number specified in the received message. If both values match (), then at  the process increments the blocks count by B for the TID on disk. The process then ends.","If the generation number or type in the Push_Blocks message do not match those on disk for the corresponding inode number, then from  the process branches to , where it determines whether the type indicated in the inode on disk is a valid type, based on the criteria mentioned above (regarding ). If the type is valid, then at  the process decrements the blocks count by B and decrements the file count by one for the old TID. Next, at  the process reinitializes the specified inode at the destination with the new generation number and type values, and then at  increments the file count by one and increments the blocks count by the number, B, of blocks contained in the Push_Blocks message. The process then ends. If the type is not valid at , the process skips from  to , described above.","If the received message is a Push_Holes message (), then from  the process proceeds to ; otherwise, the process ends. At  the process determines whether the block on disk corresponding to the offset indicated in the Push_Holes message is used. If the outcome of  is affirmative, the process decrements the blocks count for the TID on disk for the inode, by B blocks; otherwise, the process ends.","Receiving a Push_Inode message or a Push_Dirent message at the destination during logical replication also triggers an update of the qtree metafile  at the destination.  illustrates the process at the destination of updating the qtree metafile  in response to receiving a Push_Inode message at the destination. Initially, a Push_Inode message is received at the destination at . In response, at  the process loads the \u201cold\u201d inode specified in message from disk at the destination. Next, the process determines at  whether the old inode is a qtree root. If the old inode is not a qtree root, then the process continues to , where a new inode that matches the inode contents in the Push_Inode message is created or loaded. Next, at  the process makes one or more determinations, -. Specifically, if the new inode is a directory (), and its parent information is available in the inode (), its parent is the volume's root (), and the its TID is not equal to zero (), then at  the process adds the qtree root to the qtree metafile at the destination. If any of the above-mentioned conditions is not true, then that means the inode is not a qtree root, in which case the process does nothing further (). The process then ends.","If the old inode is determined to be a qtree root at , then from  the process branches to , where it determines whether the new mode's generation number matches that of the old inode. If there is a mismatch, then the process deletes the qtree root from the qtree metafile  at the destination at , after which the process continues to , described above. If there is no mismatch at , then the process continues immediately to .",{"@attributes":{"id":"p-0111","num":"0111"},"figref":"FIG. 10D","b":["609","1051","1052","1053","1054","1055","1057","609","1058","609","1053","1055","1059"]},{"@attributes":{"id":"p-0112","num":"0112"},"figref":"FIG. 11","b":["703","702","703"]},"Next, at  the process creates a final snapshot of the volume at the source. The process then determines at  whether there any differences in certain quota metadata between this final snapshot and the snapshot that was generated at  (), i.e., the baseline quota metadata snapshot. The quota metadata that is compared in this step includes, for example, the quota limits and logging intervals for the volume. The manner in which such differences are detected is not germane to this description; any convenient method of detecting differences in data can be used.","If no differences are detected (), then the process proceeds to , as described below. If any differences are detected, then at  the process sends the differences from the source to the destination. Note that these differences can be sent using any convenient communication protocol or channel, however, in one embodiment they are sent using a protocol and process separate from the logical replication process. Next, the destination updates the appropriate quota-related metadata  at the destination, based on these differences. Finally, at  the process sends the user data and metadata of the final snapshot to the destination (through logical replication), appropriately updates the destination quota accounting, quota names metafile and qtree metafile at the destination, and creates a corresponding final snapshot of the volume at the destination, all in essentially the same way as described above ().","The techniques introduced above can be implemented by programmable circuitry programmed or configured by software and\/or firmware, or entirely by special-purpose circuitry, or in a combination of such forms. Such special-purpose circuitry (if any) can be in the form of, for example, one or more application-specific integrated circuits (ASICs), programmable logic devices (PLDs), field-programmable gate arrays (FPGAs), etc.","Software or firmware to implement the techniques introduced here may be stored on a machine-readable medium and may be executed by one or more general-purpose or special-purpose programmable microprocessors. A \u201cmachine-readable medium\u201d, as the term is used herein, includes any mechanism that can store information in a form accessible by a machine (a machine may be, for example, a computer, network device, cellular phone, personal digital assistant (PDA), manufacturing tool, any device with one or more processors, etc.). For example, a machine-accessible medium includes recordable\/non-recordable media (e.g., read-only memory (ROM); random access memory (RAM); magnetic disk storage media; optical storage media; flash memory devices; etc.), etc.","Although the present invention has been described with reference to specific exemplary embodiments, it will be recognized that the invention is not limited to the embodiments described, but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly, the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["One or more embodiments of the present invention are illustrated by way of example and not limitation in the figures of the accompanying drawings, in which like references indicate similar elements and in which:",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 6B"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 6C"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 6D"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIGS. 10A and 10B"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 10C"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 10D"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 11"}]},"DETDESC":[{},{}]}
