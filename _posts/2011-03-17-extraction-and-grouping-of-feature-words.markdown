---
title: Extraction and grouping of feature words
abstract: Various embodiments of systems and methods for extraction and grouping of feature words are described herein. Feature words are obtained from a first corpus of text bodies comprising a plurality of reviews. A second corpus is created using a combination of the obtained feature words, verbs and adjectives from the first corpus. The second corpus comprises filtered reviews and each of the filtered reviews pertains to a review. Topics are preliminarily assigned for words in the filtered reviews of the second corpus. For each of the feature words in the second corpus, a topic count is determined for every preliminarily assigned topic. After determining the topic count, one or more of the topics are finally assigned to the feature words based on a topic count value. At least one topic is presented as a group of the feature words for which the at least one topic is assigned based on the topic count value.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08484228&OS=08484228&RS=08484228
owner: SAP AG
number: 08484228
owner_city: Walldorf
owner_country: DE
publication_date: 20110317
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD","BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["The field relates generally to collection of information from a corpus of text bodies. More particularly, the field relates to techniques for extracting and grouping feature words from a corpus of text bodies.","Opinions about a variety of topics and subjects are expressed by users in electronic form, especially on the Internet. For example, users of products and services typically express their opinions on relevant websites. Opinions expressed about a product (or service) may be related to one or more features of the product. There can also be general opinions that may not be particularly related to any feature of the product. Opinions can have positive, negative, or mixed tones about products or services. Each text body of such expression of opinions written in natural languages by an individual user can be referred to as a review.","For enterprises offering products or services, analysis of user reviews can be very useful. A meaningful analysis of the reviews would help enterprises better understand users' feedback and can offer valuable insights into the performance of product and service offerings. However, there can be numerous such reviews for any given product or service, leading to vast amount of data. Reading each review is not practically feasible. Analysis of only a fraction of the reviews (e.g., by sampling) may not be indicative of the overall picture and could result in incorrect conclusions.","It would therefore be desirable to provide a meaningful analysis of user reviews.","Various embodiments of systems and methods for extraction and grouping of feature words are described herein. Feature words are obtained from a first corpus of text bodies comprising a plurality of reviews. A second corpus is created using a combination of the obtained feature words, verbs and adjectives from the first corpus. The second corpus comprises filtered reviews and each of the filtered reviews pertains to a review of the plurality of reviews. Topics are preliminarily assigned for words in the filtered reviews of the second corpus. For each of the feature words in the second corpus, a topic count is determined for every preliminarily assigned topic. After determining the topic count, one or more of the topics are finally assigned to the feature words based on a topic count value. At least one topic is presented on a user interface as a group of the feature words for which the at least one topic is assigned based on the topic count value.","These and other benefits and features of embodiments of the invention will be apparent upon consideration of the following detailed description of preferred embodiments thereof, presented in connection with the following drawings.","Embodiments of techniques for extraction and grouping of feature words are described herein. In the following description, numerous specific details are set forth to provide a thorough understanding of embodiments of the invention. One skilled in the relevant art will recognize, however, that the invention can be practiced without one or more of the specific details, or with other methods, components, materials, etc. In other instances, well-known structures, materials, or operations are not shown or described in detail to avoid obscuring aspects of the invention.","Reference throughout this specification to \u201cone embodiment\u201d, \u201cthis embodiment\u201d and similar phrases, means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Thus, the appearances of these phrases in various places throughout this specification are not necessarily all referring to the same embodiment. Furthermore, the particular features, structures, or characteristics may be combined in any suitable manner in one or more embodiments.",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1","b":"100"},"The method  is explained in reference to a non-generic corpus that includes reviews about products. However, it should be noted that the subjects are not confined to only products and services. There can be a variety of subjects on which users express their opinions and the method is applicable to all such subjects. Also, different subjects may have different feature words.","At , feature words from a first corpus of text bodies are obtained. The first corpus of text bodies is a collection of reviews in electronic form. In one embodiment, the reviews include opinions that are expressed by users on websites. Each review is a text body comprising one or more sentences in a natural language such as the English language. Feature words can be single words or compound words which are overly emphasized in a particular corpus of interest. For example, reviews about a printer product can include user opinions about aspects of the printer such as performance of the printer or problems with the printer. Feature words can include \u201cnozzle\u201d (a single word), \u201cprint head\u201d (a compound word), \u201cproblems,\u201d and \u201creplacement.\u201d These feature words are overly emphasized in that, generally, their frequency of occurrence within the first corpus is relatively high compared to other words. A detailed embodiment for obtaining the feature words from the first corpus is explained with reference to .","At , a second corpus of the text bodies is created. The second corpus is created using a combination of the feature words obtained at  and verbs and adjectives from the first corpus. In other words, the second corpus is created by retaining verbs, adjectives, and the obtained feature words from the first corpus. The second corpus is in a way a modified or processed first corpus. For example, consider that a review R in the first corpus includes a complete sentence. The same review R in the second corpus includes a combination of feature words, verbs, and adjectives. This kind of representation of a review in the second corpus is termed as a filtered review. Therefore, the second corpus includes a collection of filtered reviews.","Consider a snippet of a review R in the first corpus that reads \u201cTHIS IS A GOOD CAMERA WITH A SHARP AUTO-FOCUS. IT ALSO TAKES SHARP PICTURES.\u201d","Feature words are obtained from a plurality of such reviews. After creating the second corpus, a filtered review FR in the second corpus that corresponds to the review R can include the following sequence of words:\n\n","At , topics are preliminarily assigned to each word in the filtered reviews of the second corpus. Topic models can be used for preliminarily assigning topics to the words in the filtered reviews. A topic model is a statistical model that discovers topics occurring in a collection of text documents. In one embodiment, Latent Dirichlet Allocation (a type of topic model) is used to assign topics for words in the filtered reviews. LDA methodology is described in \u201cLatent Dirichlet Allocation\u2014David M. Blei, Andrew Y. Ng, Michael I. Jordan; 3, 2003.\u201d LDA methodology is briefly described below.","Latent Dirichlet Allocation (LDA) is a generative probabilistic model of a corpus. A corpus is a collection of documents, where each document (w) in the corpus is treated as a sequence of N words and is denoted by w=(w, w, . . . , w). LDA treats a document in a corpus as a random mixture over latent topics, where each topic is characterized by a distribution of words. LDA assumes the following generative process for each document in a corpus:\n\n","According to LDA, the dimensionality \u2018k\u2019 of the Dirichlet distribution (and thus the dimensionality of the topic variable \u2018z\u2019) is assumed to be known and fixed. Also, the word probabilities are parameterized by a \u2018k\u00d7V\u2019 matrix. \u03b2 is a \u2018k\u00d7V\u2019 matrix where \u03b2=p(w=1|z=1), which is a fixed quantity to be estimated. The parameter \u03b1 is a k-vector with components \u03b1>0 and \u2018\u03b8\u2019 is a k-dimensional Dirichlet random variable. The key inferential problem here is to estimate the hidden parameters \u03b8 and z={z, z}, i.e., to estimate p(\u03b8, z|w, \u03b1, \u03b2) for each document in the corpus. Since this probability \u2018p(\u03b8, z|w, \u03b1, \u03b2)\u2019 is intractable to compute, variational parameters \u03b3 and \u03a6 are brought in to simplify the computation, resulting in the following variational distribution:\n\n(\u03b8,|\u03b3, \u03a6)=(\u03b8|\u03b3)\u00b7\u03c0(|\u03a6())\n\nWhere \u2018\u03b3\u2019 is a Dirichlet parameter and (\u03a6, . . . , \u03a6) are multinomial parameters and the symbol \u2018\u00b7\u2019 denotes multiplication operation.\n","An optimization problem is then formulated to determine the values of the variational parameters \u2018\u03b3\u2019 and \u2018\u03a6.\u2019 Finding a tight lower bound on the log-likelihood \u201clog p(w|\u03b1,\u03b2)\u201d is equivalent to solving the following optimization problem:\n\n(\u03b3*, \u03a6*)=argmin ((\u03b8, \u03b3, \u03a6) \u2225(\u03b8, ))\n","Where \u2018argmin\u2019 stands for argument of minimum.","By solving the aforementioned optimization problem, the following equations are obtained:\n\n\u03a6=\u03b2\u00b7exp{[log(\u03b8)|\u03b3]}\u2003\u20031\n\n\u03b3=\u03b1+\u03a3\u03a6\u2003\u20032\n","Where \u2018i\u2019 is the index corresponding to the topic and \u2018n\u2019 indexes the words in a document.","In order to estimate the corpus level parameters \u03b1 and \u03b2, log likelihood of the data given by \u20181(\u03b1, \u03b2)=\u03a3log p (w|\u03b1, \u03b2)\u2019 is maximized.","Since the quantity p(w|\u03b1, \u03b2) cannot be computed tractably, a variational EM procedure is used. E-step of the EM procedure involves finding the optimal values of the variational parameters using equations 1 and 2. M-step involves maximizing the lower bound on the log likelihood with respect to the model parameters \u03b1 and \u03b2 for fixed values of variational parameters computed in the E-step. These two steps are repeated until the lower bound on the log likelihood converges. A summary of an embodiment employing LDA is shown below:\n\n","The second corpus that is created at  is used as the corpus for processing using the LDA methodology. The filtered reviews in the second corpus will form the documents w=(w, w, . . . , w) as referred to in the LDA methodology. Since the filtered reviews in the second corpus only include a combination of feature words, verbs, and adjectives, the filtered reviews in the second corpus do not include coherent sentences. The second corpus is processed using the LDA methodology to assign topics to each word in the filtered reviews. After assigning topics, each word in each of these filtered reviews of the second corpus has an associated topic. The topic identification using LDA is implicit, without naming any topic. Topics are assigned to the feature words using topic identification numbers (topic \u2018i\u2019 as mentioned in the above description of LDA).","It should be noted that alternative techniques can be used for assigning topics. Examples of such techniques include Mixture of Unigrams model, Latent Semantic Indexing (LSI), and probabilistic Latent Semantic Indexing (pLSI). However, the LDA methodology has some advantages over the above techniques. For example, in a Mixture of Unigrams model, the word distributions can be viewed as representations of topics under the assumption that each document exhibits exactly one topic. This assumption is often limiting to effectively model a large collection of documents. In contrast, the LDA methodology allows documents to exhibit multiple topics to different degrees.","The pLSI model uses a training set of documents and the model learns the topic mixtures only for those documents on which it is trained. A difficulty with pLSI is that the number of parameters which must be estimated grows linearly with the number of training documents, leading to over-fitting. LDA methodology overcomes these problems by treating the topic mixture weights as a k-parameter hidden random variable rather than a large set of individual parameters which are explicitly linked to the training set. LDA methodology is a well-defined generative model and generalizes easily to new documents. LDA methodology also does not suffer from the over-fitting issues as the pLSI model.","After , each word in each filtered review of the second corpus has a preliminarily assigned topic. Therefore, each feature word in each filtered review also has a preliminarily assigned topic. At , for each feature word in the second corpus, a topic count for every preliminarily assigned topic is determined. A topic count is an aggregate of the number of preliminary assignments of a particular topic to a particular feature word across all the filtered reviews in the second corpus. For example, consider that \u2018Topic \u2019 is assigned to \u2018Feature word \u2019 twenty times across all the filtered reviews in the second corpus. Therefore, for the \u2018Feature word ,\u2019 the topic count for \u2018Topic \u2019 is \u201c20.\u201d Similarly, topic counts for all the topics are determined for all the feature words. Finally, for each feature word, there will be a topic count for every topic. An embodiment of topic count determination is explained in reference to .","At , one or more topics are assigned to feature words based on topic count values. In one embodiment, a topic having a highest topic count value associated with a feature word is assigned to that feature word. For example, consider that for \u2018Feature word ,\u2019 the topic count values for the Topics , , and  are 20, 6, and 5, respectively. \u2018Topic \u2019 has the highest topic count value for the \u2018Feature word \u2019 and therefore is assigned to the \u2018Feature word .\u2019 In another embodiment, one or more topics are assigned based on a threshold topic count value relative to the highest topic count value. A threshold topic count value can be taken as a percentage of the highest topic count value. For example, if a highest topic count value is \u2018n\u2019 (e.g., 20) then topics having a threshold topic count value of at least 80 percent \u2018n\u2019 (i.e., 16) are assigned to the corresponding feature word.","After assigning topics to feature words based on the topic count values, topics are presented at  as a group of the feature words for which the topics are assigned. The group of feature words can be displayed on a user interface. For example, if \u2018Topic \u2019 is assigned to \u2018Feature word ,\u2019 \u2018Feature word ,\u2019 and \u2018Feature word ,\u2019 then the \u2018Topic \u2019 is presented as a group including the \u2018Feature word ,\u2019 \u2018Feature word ,\u2019 and \u2018Feature word .\u2019 The topics can be presented as a list of feature words on a user interface. As another example, consider that \u2018Topic A\u2019 and \u2018Topic B\u2019 are assigned to feature word \u2018X,\u2019 \u2018Topic A\u2019 is assigned to features words \u2018Y\u2019 and \u2018Z,\u2019 and \u2018Topic B\u2019 is assigned to feature words \u2018P,\u2019 \u2018Q,\u2019 and \u2018M.\u2019 \u2018Topic A\u2019 is then presented as a group of the feature words \u2018X,\u2019 \u2018Y,\u2019 and \u2018Z\u2019 and \u2018Topic B\u2019 is presented as a group of the feature words \u2018X,\u2019 \u2018P,\u2019 \u2018Q,\u2019 and \u2018M.\u2019","Each topic represents an implicit sense of a particular usage, purpose, or an aspect. If the first corpus includes reviews about a product such as a mobile phone, then each topic that is exposed as a group of feature words can represent an implicit sense of an aspect of the product. An aspect of a product is an abstraction comprising relations indicated by combinations of feature words. For example, a topic having feature words \u20183G,\u2019 \u2018Wireless Capability,\u2019 \u2018Qwerty Keyboard\u2019 and \u2018e-mail application\u2019 indicates an aspect \u2018e-mail capability\u2019 of a mobile phone. In another example, a topic having feature words \u2018problems,\u2019 \u2018customer support,\u2019 \u2018print head,\u2019 and \u2018nozzle\u2019 would implicitly indicate frequent co-occurrences of \u2018problems\u2019 with \u2018print head\u2019 and \u2018nozzle\u2019 of a printer product. Several such aspects can be obtained from a given non-generic corpus. A user looking at the groups of the feature words will be able to readily recognize aspects of that product.","A feature word can belong to more than one topic. For example, in a corpus including reviews on an electronic device such as a laptop or a camera, the feature word \u201cmagnesium alloy casing\u201d (a compound word) can belong to a topic indicating \u201caesthetics\u201d and a topic indicating \u201cdurability.\u201d The \u201caesthetics\u201d topic is presented as a group of feature words including \u201cmagnesium alloy casing\u201d and other feature words such as \u201clooks\u201d or \u201cdesign\u201d to which the \u201caesthetics\u201d topic is assigned. The \u201cdurability\u201d topic is presented as a group of feature words including \u201cmagnesium alloy casing\u201d and other feature words such as \u201ctoughness\u201d to which the \u201cdurability\u201d topic is assigned.",{"@attributes":{"id":"p-0041","num":"0050"},"figref":["FIG. 2","FIG. 3"],"b":["200","202"],"i":["Red Opal: Product","Feature Scoring From Reviews","Proceedings of the ","th ACM conference on Electronic commerce, "]},"At , opinion fragments are obtained from the first corpus. In one embodiment, a text analyzer tool such as BUSINESSOBJECTS\u2122 TEXT ANALYSIS (a product of SAP AG, Germany) is used to obtain opinion fragments. Opinion fragments are obtained by fragmenting sentences in the reviews based on certain rules. These rules isolate opinions fragments, i.e., a series of words, in the sentences. Consider the sentence, \u201cThe pictures are good, but the flash is horrible.\u201d An example rule to isolate the opinion fragments could be \u201cthe occurrence of the word \u2018but\u2019 preceded by an adjective and a noun and succeeded by an adjective and a noun.\u201d On applying this rule the following opinion fragments can be obtained: opinion fragment  including \u201cThe pictures are good\u201d and opinion fragment  including \u201cThe flash is horrible.\u201d Several such rules can be used to obtain opinion fragments and the above rule is only an example.","At , relative occurrences for feature words are determined based on their occurrences within the opinion fragments and within the first corpus. Specifically, for a given feature word extracted at step , the occurrence of that feature word within the opinion fragments is counted as a percentage of its total occurrence in the first corpus. This percentage is termed as the relative occurrence. Relative occurrences are determined for each feature word. In one embodiment, if a particular feature word is referred in an opinion fragment by means of Anaphora Resolution, then that feature word is considered as occurring within the opinion fragments. At , feature words having corresponding relative occurrences more than a pre-determined threshold are retained. This threshold can be a qualitative estimate based on manual inspection of the feature words. For example, the threshold can be taken as \u2018three\u2019 and feature words with relative occurrences of more than three are retained. These retained feature words are the feature words that are used for creating a second corpus (i.e., at  in ).",{"@attributes":{"id":"p-0044","num":"0053"},"figref":"FIG. 3","b":["300","302","304","302","302"],"i":["Red Opal: Product","Feature Scoring From Reviews"]},"At , frequencies of each noun word and compound noun word in the first corpus  are calculated. For example, for the noun word \u2018x\u2019 or compound noun word \u2018y,\u2019 the frequency of all occurrences in the first corpus  is calculated and denoted as \u2018nx\u2019 and \u2018ny,\u2019 respectively. At , total occurrences of each noun word and each compound noun word are calculated. Frequencies of all the noun words within the first corpus  are added to generate the total noun word count \u2018N.\u2019 Similarly, frequencies of all the compound noun words within the first corpus  are also added to generate the total compound noun count \u2018H.\u2019","At , scores are calculated for each noun word and compound noun word. For each noun word \u2018x,\u2019 a score is calculated using the following formula:\n\nScore()=[(\/())]\u2212[()\/2]\n\nWhere, ln is a natural logarithm function and the character \u2018\u00b7\u2019 is a multiplication symbol.\n","For each compound noun word \u2018y,\u2019 a score is calculated using the following formula:\n\nScore()=[(()]\u2212[()\/2]\n\nWhere, ln is a natural logarithm function and the character \u2018\u00b7\u2019 is a multiplication symbol.\n","A threshold is applied at . This threshold can be a qualitative estimate based on manual inspection of the feature words. As an example, noun words and compound noun words having a threshold score below \u2212500 (e.g. \u2212501, \u2212502.25, \u2212600, \u22121000 are all below \u2212500\u2014In Red Opal methodology only negative scores are obtained) are retained. At , noun words or compound noun words having a score more than a certain threshold value are retained as feature words. These retained feature words are the output of step  in .",{"@attributes":{"id":"p-0049","num":"0058"},"figref":"FIG. 4","b":["400","402","404","402","1","1","1","5","2","1","5","2","402","406","1","1","402","2","1","402","1","1","2","2","1","5","3","5","4","2"]},"Since the topic count value for T is \u20184\u2019 and is highest compared to other topic count values (i.e., T=2), the topic T is assigned to FW. For FW, the topic count value for T is highest \u20184\u2019 and therefore T is assigned to FW. For FW, the topic count value for T is highest \u20184\u2019 and therefore T is also assigned to FW. For FW, the topic count value for T is highest \u20184\u2019 and therefore T is also assigned to FW. The topics T and T are then presented as a group of feature words for which they are assigned, as shown in .","Referring to , the topic T  is presented as group of feature words including FW and FW. The topic T  is presented as group of feature words including FW and FW. The combination of the feature words in a topic would implicitly indicate the notion of the topic. For example, FW and FW, when viewed in combination would implicitly indicate the topic T . Depending on the subject of the first corpus, each topic represents an implicit sense of a particular usage, purpose, or an aspect.",{"@attributes":{"id":"p-0052","num":"0061"},"figref":"FIG. 6","b":["600","602","602","604","604"]},"By extracting and grouping feature words from numerous reviews related to a subject, an overview of underlying topics pertaining to the reviews is provided to an end user on a user interface. Such analysis of reviews can be very valuable. For example, if the topics indicate problems associated with a product, appropriate actions can be taken to improve that product. The method for extracting and grouping feature words has applications in several areas. For example, the method can be used to analyze customer feedback data stored within a Customer Relationship Management (CRM) system. In a web based new product offering, consumers can quickly focus on the features of their interest and analyze sentiments pertaining to those features based on grouping of the features. The method can also be used for market sentiment analysis, brand value analysis, competitive analysis, and feature-specific sentiment analysis.","Some embodiments of the invention may include the above-described methods being written as one or more software components. These components, and the functionality associated with each, may be used by client, server, distributed, or peer computer systems. These components may be written in a computer language corresponding to one or more programming languages such as, functional, declarative, procedural, object-oriented, lower level languages and the like. They may be linked to other components via various application programming interfaces and then compiled into one complete application for a server or a client. Alternatively, the components maybe implemented in server and client applications. Further, these components may be linked together via various distributed programming protocols. Some example embodiments of the invention may include remote procedure calls being used to implement one or more of these components across a distributed programming environment. For example, a logic level may reside on a first computer system that is remotely located from a second computer system containing an interface level (e.g., a graphical user interface). These first and second computer systems can be configured in a server-client, peer-to-peer, or some other configuration. The clients can vary in complexity from mobile and handheld devices, to thin clients and on to thick clients or even other servers.","The above-illustrated software components are tangibly stored on a computer readable storage medium as instructions. The term \u201ccomputer readable storage medium\u201d should be taken to include a single medium or multiple media that stores one or more sets of instructions. The term \u201ccomputer readable storage medium\u201d should be taken to include any physical article that is capable of undergoing a set of physical changes to physically store, encode, or otherwise carry a set of instructions for execution by a computer system which causes the computer system to perform any of the methods or process steps described, represented, or illustrated herein. Examples of computer readable storage media include, but are not limited to: magnetic media, such as hard disks, floppy disks, and magnetic tape; optical media such as CD-ROMs, DVDs and holographic devices; magneto-optical media; and hardware devices that are specially configured to store and execute, such as application-specific integrated circuits (\u201cASICs\u201d), programmable logic devices (\u201cPLDs\u201d) and ROM and RAM devices. Examples of computer readable instructions include machine code, such as produced by a compiler, and files containing higher-level code that are executed by a computer using an interpreter. For example, an embodiment of the invention may be implemented using Java, C++, or other object-oriented programming language and development tools. Another embodiment of the invention may be implemented in hard-wired circuitry in place of, or in combination with machine readable software instructions.",{"@attributes":{"id":"p-0056","num":"0065"},"figref":"FIG. 7","b":["700","700","705","755","700","740","755","710","715","710","715","705","715","700","725","730","700","725","730","700","735","700","750","750","700","745","700","720","760","760","760","750","760"]},"A data source is an information resource. Data sources include sources of data that enable data storage and retrieval. Data sources may include databases, such as, relational, transactional, hierarchical, multi-dimensional (e.g., OLAP), object oriented databases, and the like. Further data sources include tabular data (e.g., spreadsheets, delimited text files), data tagged with a markup language (e.g., XML data), transactional data, unstructured data (e.g., text files, screen scrapings), hierarchical data (e.g., data in a file system, XML data), files, a plurality of reports, and any other data source accessible through an established protocol, such as, Open DataBase Connectivity (ODBC), produced by an underlying software system (e.g., ERP system), and the like. Data sources may also include a data source where the data is not tangibly stored or otherwise ephemeral such as data streams, broadcast data, and the like. These data sources can include associated data foundations, semantic layers, management systems, security systems and so on.","In the above description, numerous specific details are set forth to provide a thorough understanding of embodiments of the invention. One skilled in the relevant art will recognize, however that the invention can be practiced without one or more of the specific details or with other methods, components, techniques, etc. In other instances, well-known operations or structures are not shown or described in details to avoid obscuring aspects of the invention.","Although the processes illustrated and described herein include series of steps, it will be appreciated that the different embodiments of the present invention are not limited by the illustrated ordering of steps, as some steps may occur in different orders, some concurrently with other steps apart from that shown and described herein. In addition, not all illustrated steps may be required to implement a methodology in accordance with the present invention. Moreover, it will be appreciated that the processes may be implemented in association with the apparatus and systems illustrated and described herein as well as in association with other systems not illustrated.","The above descriptions and illustrations of embodiments of the invention, including what is described in the Abstract, is not intended to be exhaustive or to limit the invention to the precise forms disclosed. While specific embodiments of, and examples for, the invention are described herein for illustrative purposes, various equivalent modifications are possible within the scope of the invention, as those skilled in the relevant art will recognize. These modifications can be made to the invention in light of the above detailed description. Rather, the scope of the invention is to be determined by the following claims, which are to be interpreted in accordance with established doctrines of claim construction."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The claims set forth the embodiments of the invention with particularity. The invention is illustrated by way of example and not by way of limitation in the figures of the accompanying drawings in which like references indicate similar elements. The embodiments of the invention, together with its advantages, may be best understood from the following detailed description taken in conjunction with the accompanying drawings.",{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0010","num":"0009"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 5","FIG. 4"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 7"}]},"DETDESC":[{},{}]}
