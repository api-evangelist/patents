---
title: Inspecting potentially interfering features in a machine vision system
abstract: A robust video tool is provided for use in a machine vision inspection system. The robust video tool comprises a region of interest, a user interface, edge detection operations, and excluded region operations that determine a set of current-feature edge points which includes edge points detected in the region of interest and excludes edge points in an excluded region. The excluded region is determined by an excluded region generator, based on at least one previously characterized feature which is a feature characterized by using a video tool that detects the edge points of, and characterizes dimensional parameters of, the previously characterized feature. Importantly, the robust video tool features and operations are configured to allow learn mode programming on a workpiece that is either properly fabricated or improperly fabricated, and the resulting program will operate reliably on a run mode workpiece that is either properly fabricated or improperly fabricated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08269830&OS=08269830&RS=08269830
owner: Mitutoyo Corporation
number: 08269830
owner_city: Kawasaki-shi
owner_country: JP
publication_date: 20110414
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENTS"],"p":["The invention relates generally to machine vision inspection systems, and more particularly to methods of inspecting potentially interfering features on a workpiece.","Precision machine vision inspection systems (or \u201cvision systems\u201d for short) can be utilized to obtain precise dimensional measurements of inspected objects and to inspect various other object characteristics. Such systems may include a computer, a camera and optical system, and a precision stage that is movable in multiple directions so as to allow the camera to scan the features of a workpiece that is being inspected. One exemplary prior art system that is commercially available is the QUICK VISION\u00ae series of PC-based vision systems and QVPAK\u00ae software available from Mitutoyo America Corporation (MAC), located in Aurora, Ill. The features and operation of the QUICK VISION\u00ae series of vision systems and the QVPAK\u00ae software are generally described, for example, in the 3, published January 2003, and the 3, published September 1996, each of which is hereby incorporated by reference in their entirety. This product, as exemplified by the QV-302 Pro model, for example, is able to use a microscope-type optical system to provide images of a workpiece at various magnifications, and move the stage as necessary to traverse the workpiece surface beyond the limits of any single video image. A single video image typically encompasses only a portion of the workpiece being observed or inspected, given the desired magnification, measurement resolution, and physical size limitations of such systems.","Machine vision inspection systems generally utilize automated video inspection. U.S. Pat. No. 6,542,180 (the '180 patent) teaches various aspects of such automated video inspection and is incorporated herein by reference in its entirety. As taught in the '180 patent, automated video inspection metrology instruments generally have a programming capability that allows an automatic inspection event sequence to be defined by the user for each particular workpiece configuration. This can be implemented by text-based programming, for example, or through a recording mode which progressively \u201clearns\u201d the inspection event sequence by storing a sequence of machine control instructions corresponding to a sequence of inspection operations performed by a user with the aid of a graphical user interface, or through a combination of both methods. Such a recording mode is often referred to as \u201clearn mode\u201d or \u201ctraining mode.\u201d Once the inspection event sequence is defined in \u201clearn mode,\u201d such a sequence can then be used to automatically acquire (and additionally analyze or inspect) images of a workpiece during \u201crun mode.\u201d","The machine control instructions including the specific inspection event sequence (i.e., how to acquire each image and how to analyze\/inspect each acquired image) are generally stored as a \u201cpart program\u201d or \u201cworkpiece program\u201d that is specific to the particular workpiece configuration. For example, a part program defines how to acquire each image, such as how to position the camera relative to the workpiece, at what lighting level, at what magnification level, etc. Further, the part program defines how to analyze\/inspect an acquired image, for example, by using one or more video tools such as edge\/boundary detection video tools.","Video tools (or \u201ctools\u201d for short) and other graphical user interface features may be used manually to accomplish manual inspection and\/or machine control operations (in \u201cmanual mode\u201d). Their set-up parameters and operation can also be recorded during learn mode, in order to create automatic inspection programs, or \u201cpart programs.\u201d Video tools may include, for example, edge\/boundary detection tools, autofocus tools, shape or pattern matching tools, dimension measuring tools, and the like.","One application for a machine vision inspection system is the inspection of features on a workpiece. In some cases, these features may be contained in different layers and\/or may interfere or have overlapping edges. One example of such a workpiece is a printed circuit board (PCB), wherein it may be desirable to measure the registration relationship between a pattern in a solder resist layer and conductive features intended to be exposed and\/or insulated by the solder resist layer. Such features may be concentric, or in some cases, fabrication errors may occur which cause the edges of some of the features to overlap or interfere. Such errors may be associated with short circuiting or other problems for the operation of the PCB. Thus, during the inspection of the PCB, it is important to be able to accurately determine if the features are properly located. However, prior art methods for automatically inspecting such features are neither easy enough to program for unskilled users of the machine vision inspection system, nor robust enough to operate properly under a wide variety of possible alignments and misalignment conditions for the related features, which may potentially not overlap, or overlap to various degrees. Improvements in programming and techniques related to accurately detecting and\/or measuring potentially overlapping or interfering features on workpieces such as PCBs, would be desirable.","This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.","A machine vision inspection system includes features for programming and techniques related to inspecting potentially interfering features on a workpiece. The machine vision inspection system may comprise: a control system comprising a memory element and an excluded region generator; an imaging system operable to provide workpiece images; a display usable to display workpiece images and user interface elements; a user interface usable to define a sequence of workpiece inspection operations including video tool operations which analyze workpiece images, detect edge points of a workpiece feature, and characterize at least the location of the workpiece feature, and a plurality of video tools including a robust video tool that performs edge detection. The robust video tool comprises a region of interest, a robust video tool interface included in the user interface, operations that detect edge points in the region of interest, and excluded region operations that analyze a workpiece image and determine a set of current-feature edge points such that the set of current-feature edge points includes edge points detected in the region of interest and does not include edge points located within an excluded region that is determined based on at least one previously characterized feature. A previously characterized feature, as disclosed herein, is defined to be a feature characterized by using one of the plurality of video tools of the machine vision inspection system to analyze a workpiece image to detect edge points of the previously characterized feature and characterize a dimensional parameter of the previously characterized feature based on the detected edge points. According to the principles of this invention, the excluded region generator is configured to determine the excluded region based on at least one previously characterized feature.","It should be appreciated that the term \u201cpreviously characterized feature\u201d is sometimes used herein as a descriptive name for a workpiece feature which is eventually used in the capacity of a \u201cpreviously characterized feature\u201d for the purposes of determining an excluded region, even when referring to that workpiece feature before it has actually been characterized. Similarly, the term \u201ccurrent feature\u201d is sometimes used herein as a descriptive name for a workpiece feature which may be subject to interference from the \u201cpreviously characterized feature\u201d when it is eventually analyzed (e.g., using a robust video tool as disclosed herein), even when referring to that workpiece feature at a time before or after it is actually analyzed.","In some embodiments, the excluded region generator is configured to determine the excluded region based on a set of excluded region parameters defined during learn mode, the set of excluded region parameters including an identification of the at least one previously characterized feature that is used to determine the excluded region. In some embodiments, the excluded region generator may be configured to automatically identify a previously characterized feature that is closest to a current instance of the robust video tool during learn mode, and automatically include an identification of that previously characterized feature in the set of excluded region parameters that is used to determine the excluded region.","In some embodiments, the user interface is configured to define the parameters of at least one instance of a video tool in order to provide a characterization of the at least one previously characterized feature using a representative workpiece during learn mode, and the robust video tool interface is configured to define the parameters of a current instance of the robust video tool which is used to determine a set of current-feature edge points for a current feature on the representative workpiece during learn mode, and the machine vision inspection system is configured to record those parameters and the defined set of excluded region parameters in a part program. The machine vision inspection system may be configured to execute operations of the part program during the run mode of operation for a run mode workpiece similar to the representative workpiece. Those operations may comprise using the recorded defined parameters to provide the characterization of the at least one previously characterized feature on the run mode workpiece, that is, to characterize the at least one feature on the run mode workpiece that corresponds to the at least one previously characterized feature on the similar representative learn mode workpiece. Those operations may further comprise using the recorded excluded region parameters to determine the excluded region based on the at least one previously characterized feature on the run mode workpiece, and using the recorded defined parameters of the current instance of the robust video tool to determine a set of current-feature edge points for the current feature on the run mode workpiece, such that the set of current-feature edge points includes edge points detected in the region of interest of the current instance of the robust video tool on the run mode workpiece and does not include edge points located within the excluded region that is determined based on the at least one previously characterized feature on the run mode workpiece.","In various embodiments the user interface is configured such that a user can select and configure the at least one instance of a video tool included in the plurality of video tools, wherein that video tool is one of a circle tool having an annular region of interest, an arc tool having an arc-shaped region of interest, and a tool that characterizes a straight edge and has a rectangular region of interest; and a user interface of that video tool includes a region of interest indicator which is superimposed on the displayed image of the representative workpiece during learn mode. The user interface of that video tool is configured by the user in order to define the parameters of an instance of that video tool in order to provide the at least one previously characterized feature using the representative workpiece during the learn mode of operation. In various embodiments, the user interface is configured such that a user can select and configure an instance of the robust video tool, wherein the robust video tool is also one of a circle tool having an annular region of interest, an arc tool having an arc-shaped region of interest, and a tool that characterizes a straight edge and has a rectangular region of interest; and the robust video tool interface includes a region of interest indicator and an excluded region indicator which are superimposed on a displayed image of the representative workpiece during learn mode. The robust video tool interface is configured by the user in order to define the parameters of the current instance of the robust video tool used to determine the set of current-feature edge points for a current feature on the representative workpiece during the learn mode of operation.","In some embodiments, the user interface comprises an excluded region parameter portion which is configured to define members of the set of excluded region parameters during the learn mode of operation of the machine vision inspection system, and the defined members include an identification of the at least one previously characterized feature used to determine the excluded region. The excluded region parameter portion may comprise an excluded feature identifier that is configured by the user to provide the identification of the at least one previously characterized feature that is used to determine the excluded region. In some embodiments, the excluded region parameter portion comprises a list of previously characterized features that is automatically generated by the control system and the excluded feature identifier comprises a selection element that is configured to select at least one member of the list to include in the identification of the at least one previously characterized feature that is used to determine the excluded region.","In some embodiments, the excluded region parameter portion is included in the robust video tool interface, and the robust video tool interface is configured to define the parameters of a current instance of the robust video tool during learn mode, including defining a current instance of the set of excluded region parameters for an excluded region corresponding specifically to the current instance of the robust video tool. In such a case, the current instance of the set of excluded region parameters includes an identification of a current instance of the at least one previously characterized feature that is used to determine the excluded region. In some such embodiments, the robust video tool interface comprises a region of interest indicator and an excluded region indicator which are displayed superimposed on an image of the representative workpiece during the learn mode of operation. The region of interest indicator may automatically change when the current instance of the set of excluded region parameters is changed. In some embodiments, the excluded region indicator is configured to be altered by a user to adjust the size of the excluded region indicator and make a corresponding change in the current instance of the set of excluded region parameters.","In some embodiments, the excluded region generator may be configured to determine the excluded region such that it encompasses at least a majority of the edge points detected while characterizing the at least one previously characterized feature that is used to determine the excluded region. In various embodiments it is preferred that the excluded region encompass all such edge points (although, in some such cases, \u201coutliers\u201d in the detected edge points may fall outside of the excluded region).","In some embodiments, a previously characterized feature may be defined as a feature characterized by using one of the plurality of video tools to perform operations comprising (a) analyzing a workpiece image to detect edge points of that feature and (b) fitting a geometric feature to the detected edge points, and (c) providing a characterization of the fitted geometric feature, that characterization comprising at least a dimensional parameter of the fitted geometric feature (e.g., the video tool may be a known circle tool, or arc tool, or a tool that fits a line to a straight edge, or the like) and the excluded region generator may be configured to determine the excluded region based on the characterization of the fitted geometric feature of at least one previously characterized feature. Since such video tools exist in known machine vision inspection systems, and there are known methods for such fitted geometric features to be recorded in memory in such systems, such embodiments of the invention may be implemented with relatively little modification of existing systems and require relatively little additional training for users in order obtain the benefits of the invention.","In some embodiments, the excluded region generator is configured to determine the excluded region by adjusting at least one of the size and the location of the fitted geometric feature and using that adjusted fitted geometric feature as a boundary of the excluded region, wherein the adjusted fitted geometric feature provides a boundary that encompasses at least a majority of the detected edge points used for fitting the geometric feature. The robust video tool may comprise a scan direction that is used in its operations that detects edge points in its region of interest, and the excluded region generator may be configured to adjust the fitted geometric feature such that the boundary moves in a direction opposite to that scan direction. Moving the boundary in this direction tends to prevent the robust video tool from including edge points of the at least one previously characterized feature in the set of current-feature edge points, and the direction may be automatically chosen, in some embodiments.","The foregoing features are particularly useful for inspecting potentially overlapping circular features (e.g., holes, copper pads, and solder resist openings) on PCBs. In such applications, the previously characterized feature is a feature characterized by using a circle tool included in the plurality of video tools, the fitted geometric feature is a circle, and the robust video tool is a circle tool. Both fluorescent and non-fluorescent imaging may be useful for inspecting features on PCBs, since the PCB may include a translucent fluorescent solder resist coating, and the different types of imaging may reveal different types of features of the solder resist, and\/or elements that it covers, more clearly. For example, different types of images may be provided as disclosed in copending and commonly assigned U.S. patent application Ser. No. 12\/904,013, which is hereby incorporated by reference in its entirety. More generally, in some embodiments, the excluded region generator is configured to determine the excluded region based on at least one previously characterized feature that is a feature characterized in a first workpiece image that is exposed using a first set of wavelengths, and the robust video tool is configured to detect the set of current-feature edge points in a second workpiece image that is exposed using a second set of wavelengths that is different than the first set of wavelengths. In some embodiments, one of the first and second sets of wavelengths includes a fluorescent wavelength emitted by the workpiece with a significant intensity, and the other of the first and second sets of wavelengths does not include the fluorescent wavelength emitted by the workpiece with a significant intensity.","In some embodiments, the robust video tool comprises an excluded region mode wherein the excluded region operations are performed, and a non-excluded region mode comprising non-excluded region operations that analyze a workpiece image and determine a set of current-feature edge points such that the set of current-feature edge points includes edge points detected in the region of interest without consideration of an excluded region. In such cases, the robust video tool interface may include an excluded region activation element which is operable by a user to determine whether or not an instance of the robust video tool operates in the excluded region mode or the non-excluded region mode. Such a versatile robust video tool simplifies training and programming for users.","It will be appreciated that the features outlined above and disclosed in greater detail below may be particularly useful for certain applications. For example, in an implementation where the workpiece is a multiple layer workpiece (e.g., a printed circuit board including a solder resist layer), edge features in the various layers may overlap, causing conventional edge detection tools to fail or mislead due to \u201cconfusion\u201d regarding which edge feature is in which layer and the inability to separate their respective edges. Conventional video tools have regions of interest that are unable to solve this problem, due to the fact that the features may truly overlap on some workpieces and not on others, such that the regions of interest cannot be programmed to reliably isolate a particular feature and operate effectively or reliably for all workpiece conditions. Similarly, known feature \u201cmasking\u201d programming methods (e.g., see U.S. Pat. No. 7,324,682, which is hereby incorporated by reference in its entirety) are taught by assuming that an occluding or interfering feature is always present, and so may not operate effectively or reliably for all workpiece conditions. It should be appreciated that for some properly fabricated workpieces, there is no occlusion or overlap of features, and thus the methods of the '682 patent cannot even be programmed in learn mode, due to the absence of interfering or occluding elements in the region of interest of a \u201ccurrent feature\u201d on a properly fabricated representative workpiece. Thus, the features associated with the present invention are more general, easier to implement, and yet more robust for certain types of applications, in comparison to the prior art. In particular, the features disclosed herein allow programming in learn mode on a representative workpiece, whether properly fabricated or improperly fabricated, and will still operate reliably in the resulting automatic part program on a run mode workpiece that is either properly fabricated or improperly fabricated.",{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 1","b":["10","10","12","14","14","16","18","22","24","26","16","10"]},"The vision measuring machine  includes a moveable workpiece stage  and an optical imaging system  which may include a zoom lens or interchangeable lenses. The zoom lens or interchangeable lenses generally provide various magnifications for the images provided by the optical imaging system . The machine vision inspection system  is generally comparable to the QUICK VISION\u00ae series of vision systems and the QVPAK\u00ae software discussed above, and similar state-of-the-art commercially available precision machine vision inspection systems. The machine vision inspection system  is also described in commonly assigned U.S. Pat. Nos. 7,454,053, and 7,324,682, and U.S. patent application Ser. Nos. 12\/343,383, filed Dec. 23, 2008, and 12\/608,943, filed Oct. 29, 2009, which are each incorporated herein by reference in their entireties.",{"@attributes":{"id":"p-0034","num":"0033"},"figref":["FIG. 2","FIG. 1"],"b":["120","200","100","120","200","200","205","220","230","230","240","210","212","210","20","205","260","250","280","286","288"]},"The optical assembly portion  is controllably movable along a Z-axis that is generally orthogonal to the X and Y axes, by using a controllable motor  that drives an actuator to move the optical assembly portion  along the Z-axis to change the focus of the image of the workpiece . The controllable motor  is connected to the input\/output interface  via a signal line .","A workpiece , or a tray or fixture holding a plurality of workpieces , which is to be imaged using the machine vision inspection system  is placed on the workpiece stage . The workpiece stage  may be controlled to move relative to the optical assembly portion , such that the interchangeable objective lens  moves between locations on a workpiece , and\/or among a plurality of workpieces . One or more of a stage light , a first coaxial light , a second coaxial light \u2032, and a surface light  (e.g., a ring light) may emit source light , , \u2032 and\/or , respectively, to illuminate the workpiece or workpieces . The light sources  and \u2032 may emit light  and \u2032 along a path including a minor , as described in greater detail below. The second coaxial light \u2032 may emit source light \u2032 which has a wavelength profile which causes certain workpiece materials (e.g., solder resist) to fluoresce. Such elements are described in more detail in copending and commonly assigned U.S. patent application Ser. No. 12\/904,013, incorporated herein by reference. It will be appreciated that the specific features and elements outlined above for the optical paths providing the source light for fluorescent and non-fluorescent imaging are exemplary only and not limiting. The source light is reflected or transmitted as workpiece light , or fluorescent workpiece light \u2032 is emitted, and the workpiece light used for imaging passes through the interchangeable objective lens  and the turret lens assembly  and is gathered by the camera system . The image of the workpiece(s) , captured by the camera system , is output on a signal line  to the control system portion . The light sources , , \u2032, and  may be connected to the control system portion  through signal lines or busses , , and , respectively. To alter the image magnification, the control system portion  may rotate the turret lens assembly  along axis  to select a turret lens, through a signal line or bus .","As shown in , in various exemplary embodiments, the control system portion  includes a controller , the input\/output interface , a memory , a workpiece program generator and executor , and a power supply portion . Each of these components, as well as the additional components described below, may be interconnected by one or more data\/control buses and\/or application programming interfaces, or by direct connections between the various elements.","The input\/output interface  includes an imaging control interface , a motion control interface , a lighting control interface , and a lens control interface . The motion control interface  may include a position control element , and a speed\/acceleration control element , although such elements may be merged and\/or indistinguishable. The lighting control interface  includes lighting control elements -, and which control, for example, the selection, power, on\/off switch, and strobe pulse timing, if applicable, for the various corresponding light sources of the machine vision inspection system . The lighting control element may control the selection, power, on\/off switch, and strobe pulse timing, if applicable, for the second coaxial light \u2032 which may excite fluorescent workpiece materials to emit fluorescent image light.","The memory  may include an image file memory portion , a previously characterized feature memory portion for recording data related to previously characterized features as outlined further below, a workpiece program memory portion  that may include one or more part programs, or the like, and a video tool portion . The video tool portion  includes video tool portion and other video tool portions (e.g., ), which determine the GUI, image processing operation, etc., for each of the corresponding video tools, and a region of interest generator that supports automatic, semi-automatic and\/or manual operations that define various ROIs that are operable in various video tools included in the video tool portion .","In the context of this disclosure, and as known by one of ordinary skill in the art, the term video tool generally refers to a relatively complex set of operations that a machine vision user can implement through a relatively simple user interface (e.g., a graphical user interface, editable parameter windows, menus, and the like), without creating the step-by-step sequence of operations included in the video tool or resorting to a generalized text-based programming language, or the like. For example, a video tool may include a complex pre-programmed set of image processing operations and computations which are applied and customized in a particular instance by adjusting a few variables or parameters that govern the operations and computations. In addition to the underlying operations and computations, the video tool comprises the user interface that allows the user to adjust those parameters for a particular instance of the video tool. For example, many machine vision video tools allow a user to configure a graphical region of interest indicator through simple \u201chandle dragging\u201d operations using a mouse, in order to define the location parameters of a subset of an image that is to be analyzed by the image processing operations of a particular instance of a video tool. Furthermore, by operating an instance of a video tool during learn mode, with a particular feature of a representative workpiece located in the region of interest, the image processing operations of the video tool may automatically extract certain representative parameters (e.g., the intensity change across an edge feature in the region of interest) that can be saved or recorded in a part program and used as limits to ensure speed or reliability when that instance of the tool (that is, its operations) is automatically run on a corresponding feature on a corresponding workpiece (e.g., an identical part or product) during run mode. This aspect of learn mode operation is referred to as \u201ctraining\u201d a particular instance of the video tool. It should be noted that the visible user interface features are sometimes referred to as the video tool, with the underlying operations being included implicitly.","It should be appreciated that in contrast to prior art methods, the features and methods disclosed herein related to \u201cexcluded region operations\u201d may be implemented using relatively simple video tools that are also useful for operations other than the excluded region operations. Furthermore, the features and methods disclosed herein are robust with respect to the relevant feature relationships (that is, potentially interfering or non-interfering features) both during learn mode and during run mode, despite the fact that they may be implemented by relatively unskilled users.","In particular, the foregoing attributes may be associated with a robust video tool and an associated excluded region generator , included in the video tool portion . Although they are described as separate elements here, in some embodiments the features and operations of the excluded region generator and the robust video tool may be merged and\/or indistinguishable. Briefly, the robust video tool includes excluded region operations that prevent a potentially interfering feature from being \u201cconfused\u201d with a feature that is intended to be inspected by an instance of the robust video tool . In some embodiments, the robust video tool may be operated in a mode that includes the excluded region operations , and at a different time operated in a mode that omits these operations. The robust video tool also includes a robust video tool interface (a user interface), which may include an excluded region parameter portion . However, in some embodiments, the excluded region parameter portion may be implemented outside of the robust video tool interface (e.g., as a separate interface element, or associated with the excluded region generator ). Features and operations associated with these elements are described in greater detail below.","In general, the memory portion  stores data usable to operate the vision system components portion  to capture or acquire an image of the workpiece  such that the acquired image of the workpiece  has desired image characteristics. The memory portion  may also contain data defining a graphical user interface operable through the input\/output interface . The memory portion  may also store inspection result data, may further store data usable to operate the machine vision inspection system  to perform various inspection and measurement operations on the acquired images (e.g., implemented, in part, as video tools), either manually or automatically, and to output the results through the input\/output interface . As disclosed herein, the memory portion  includes a previously characterized feature memory portion , which stores data related to certain \u201cpreviously characterized features\u201d (e.g., features analyzed and characterized using video tools), such that the data may be used by the excluded region generator to determine an excluded region for one or more instances of the robust video tool . In various embodiments, the previously characterized features may be labeled (e.g., named or numbered) for reference in a user interface. A previously characterized feature may be associated with (or characterized by) a particular measurement result that is stored (e.g., a circle size and location in the case of a circle feature, or a line location and angle in the case of a straight line feature). In some embodiments, a previously characterized feature may also be associated with (or characterized by) the detected edge points that are used to determine its measurement result, and\/or statistics or dimensions related to their scatter, or the like, which may be stored. However, in some embodiments, once a previously characterized feature is characterized by a measurement result and\/or uncertainty statistics or dimensions, the underlying edge point locations are deleted from memory.","The signal lines or busses , , and  of the stage light , the coaxial lights  and \u2032, and the surface light , respectively, are all connected to the input\/output interface . The signal line  from the camera system  and the signal line  from the controllable motor  are connected to the input\/output interface . In addition to carrying image data, the signal line  may carry a signal from the controller  that initiates image acquisition.","One or more display devices  (e.g., the display  of ) and one or more input devices  (e.g., the joystick , keyboard , and mouse  of ) can also be connected to the input\/output interface . The display devices  and input devices  can be used to display a user interface, which may include various graphical user interface (GUI) features that are usable to perform inspection operations, and\/or to create and\/or modify part programs, to view the images captured by the camera system , and\/or to directly control the vision system components portion .","In various exemplary embodiments, when a user utilizes the machine vision inspection system  to create a part program for the workpiece , the user generates part program instructions either by explicitly coding the instructions automatically, semi-automatically, or manually, using a workpiece programming language, and\/or by generating the instructions by operating the machine vision inspection system  in a learn mode to provide a desired image acquisition training sequence. For example, a training sequence may comprise positioning a particular workpiece feature of a representative workpiece in the field of view (FOV), setting light levels, focusing or autofocusing, acquiring an image, and providing an inspection training sequence applied to the image (e.g., using an instance of one of the video tools on that workpiece feature). The learn mode operates such that the sequence(s) are captured or recorded and converted to corresponding part program instructions. These instructions, when the part program is executed, will cause the machine vision inspection system to reproduce the trained image acquisition and inspection operations to automatically inspect that particular workpiece feature (that is the corresponding feature in the corresponding location) on a run mode workpiece or workpieces which matches the representative workpiece used when creating the part program.","These analysis and inspection methods that are used to inspect features in a workpiece image are typically embodied in the various video tools (e.g., video tools , , etc.) included in the video tool portion  of the memory , as outlined above. Many known video tools, or \u201ctools\u201d for short, are included in commercially available machine vision inspection systems, such as the QUICK VISION\u00ae series of vision systems and the associated QVPAK\u00ae software, discussed above.","It is a particular problem in general purpose machine vision inspection systems to provide methods and tools that allow relatively unskilled users to program such systems with robust inspection operations that reliably provide accurate measurements. The features and operations disclosed herein address this problem in relation to potentially interfering features on a workpiece, such as features that appear on multiple layers or surfaces of a workpiece.",{"@attributes":{"id":"p-0049","num":"0048"},"figref":["FIGS. 3A and 3B","FIGS. 3A-3C","FIGS. 3A-3C"],"b":["300","300","305","320","320","320","310","310","310","420","420","320","300","300","310","320","4","4","305","330","320","310"],"i":"roi"},"As shown in , the circle tool  is provided for determining edge points EP of the current feature E. Exemplary examples of circle and other video tools are described in more detail in U.S. Pat. Nos. 7,627,162 and 7,769,222, which are hereby incorporated by reference in their entireties. The user interface for the circle tool  includes the region of interest indicator having an inner boundary  in and an outer boundary out, a scan direction indicator , a sampling direction indicator , and an edge selector  which is located by the user to indicate a representative portion of the edge that is to be detected. The scan direction indicator  is configured to point in the direction corresponding to the sequence that is to be used for the analysis of pixel intensity that is used for detecting the edge in an image. It is generally advantageous that the scan direction proceeds from the least textured or \u201cnoisy\u201d side of an edge or boundary to the more noisy side of the edge. In this way, image noise or texture is less likely to be interpreted as the edge, in that the true edge is encountered first in the analysis sequence. The sampling direction indicator  shows the direction that the edge point detection scans proceed around the circle tool  (e.g., counter-clockwise sampling direction).","In the case shown in , the inner boundary  in may be configured by a user during learn mode such that it excludes the ideally concentric edge feature E from the region of interest . In such a case, the circle tool  is able to accurately determine the edge points EP of the current feature E without interference from the edge feature E. Known circle tools would operate with the desired result in this case. In contrast, as will be described in more detail below with respect to , in a non-ideal case where the edge feature E overlaps the current feature E, known circle tools configured and\/or trained and recorded as shown in  will fail to provide accurate results.",{"@attributes":{"id":"p-0052","num":"0051"},"figref":["FIG. 3C","FIGS. 3A and 3B","FIG. 3C"],"b":["300","305","320","305","305","310","305","420","420","320","300","310","320","310","320"],"i":"roi"},"As shown in , in this case the edge points detected at various scan locations in the region of interest detect a set of edge points that includes the proper edge points EP of the current feature E, but that also includes the improper edge points EP-X, which are edge points of the misplaced interfering feature \u2032E. A circle fit to this set of edge points will not reflect the proper size or location of the current feature E (or its associated region ), and thus will lead to improper inspection results, as is typical of known video tools in this case. Since the location of the interfering feature \u2032E is not sufficiently predictable for a range of workpieces, this problem cannot be solved by narrowing the region of interest, or other conventional means. Likewise, known region masking techniques cannot be integrated with conventional video tools by relatively unskilled users to produce the desired result. A solution to this problem is described below.","Briefly, as described in greater detail with reference to , the features and operations disclosed herein comprise using a simple video tool to characterize a potentially interfering feature, and then using a new type of robust video tool that uses an excluded region based on the previously characterized potentially interfering feature (e.g., characterized by a size and\/or location measurement of its actual geometry), to exclude edge points detected in the region of interest around a current feature that are actually edge points of the previously characterized interfering feature. Thus, only valid edge points of the current feature are ultimately identified by the robust video tool, regardless of the location of the potentially interfering features, as desired.",{"@attributes":{"id":"p-0055","num":"0054"},"figref":["FIGS. 4A and 4B","FIG. 3C","FIG. 3C","FIGS. 4A and 4B"],"b":["400","400","305","320","310","320","305","320"]},"In particular,  illustrates a region of interest of a video tool  (implied), as indicated by the ROI indicator of the video tool , which is used for edge detection and characterization of the potentially interfering feature \u2032E. In one embodiment, the video tool  may be one of a plurality of standard video tools included in a machine vision inspection system (e.g., as exemplified by the video tools -, previously outlined with reference to ). As shown in , in order to encompass all the potential locations of the potentially interfering feature \u2032E, and thereby reliably detect it for all cases (e.g., all workpieces, including the cases , and \u2033, shown in dashed outline), the region of interest must be very wide. The user may configure the ROI inner boundary  in and the ROI outer boundary out during learn mode, based on experience, or the known tolerances of the feature \u2032E, etc.","As shown in , the wide region of interest of the video tool  may encompass the feature E as well the potentially interfering feature \u2032E, which presents the possibility of the video tool  detecting edge points of the feature E erroneously, rather than the desired edge points of the feature \u2032E. However, a number of known measures may prevent this, including the following:","Firstly, if the scan direction is set from \u201cin to out\u201d (in this example), the first edge detected along the scan direction will tend to be the feature \u2032E in many cases, since it is nominally smaller than, and inside of, the feature E.","Secondly, the feature \u2032E exhibits a larger intensity transition than the feature E (in this example), and the amount of this large transition can be determined and a related large transition threshold recorded in the tool  during learn mode training. When the tool  is executed with this threshold as an acceptance criteria (a known practice) the edge points of the feature E will then fail to meet this threshold and will not be detected.","Thirdly, it will be appreciated that the effectiveness of the threshold technique outlined above may be enhanced further by using image exposure wavelengths or illumination angles that increase the intensity transition at the edge feature \u2032E and\/or decrease the intensity transition at the edge feature \u2032E. Thus, in some applications (not all), a first image (e.g., an image exposed using a first set of wavelengths) may be used to detect the potentially interfering feature (e.g., the feature \u2032E) and a second image may be used to detect a current edge feature using the robust video tool (e.g., the feature ). In some applications, the first image may be exposed using a first set of wavelengths, and the second image may be exposed using a second set of wavelengths that is different than the first set of wavelengths (e.g., one of the images may be a fluorescent image). In some applications (e.g., when one of the features is a through hole), the hole may be back lighted in one image (e.g., by a stage light) and the other feature may be lighted from the top in another image.","More generally, a user may choose any of the foregoing techniques, and\/or choose to measure the larger feature rather than the smaller feature as the previously characterized feature if it exhibits a stronger edge, or the like, in order to provide a reliably characterized previously characterized feature. In the case that a larger feature such as a large circle is used as the previously characterized feature, then it may be the area outside the circle that is the excluded region. Whether it is the area inside or outside a closed form, or the area to the lighter or darker side of a line, that is determined to be the excluded region may be governed by an excluded region polarity parameter that is governed by a default setting, or an algorithm in the excluded region generator, or a setting entered by a user.","As shown in , using appropriate techniques, the video tool  detects the proper edge points EP of the potentially interfering feature \u2032E at various scan locations in the region of interest , regardless of its actual location in the region of interest indicated by the ROI indicator . The video tool  may then fit a circle to the set of edge points of the circular edge feature, and characterize related dimensional parameters such as the size and location of the potentially interfering feature \u2032E, in order to provide a previously characterized feature that is suitable for determining an excluded region for use with a robust video tool, as described in greater detail below.",{"@attributes":{"id":"p-0063","num":"0062"},"figref":["FIG. 4B","FIG. 2","FIG. 5"],"b":["400","305","310","143","310","410","310"],"i":"er "},"For example, in some embodiments, a previously characterized feature may be characterized by a video tool analyzing a workpiece image, detecting edge points of the feature, and fitting a geometric feature to the detected edge points (e.g., a circle) and characterizing the fitted geometric feature with dimensional parameters describing its size and\/or location (e.g., a circle center location and radius, or a straight line location and angle). The characterization may be stored in the previously characterized feature memory portion , outlined with reference to . In some embodiments, the excluded region is determined (e.g., by the excluded region generator outlined with reference to ) by using the fitted geometric feature as its boundary, or adjusting at least one of the size and the location of the fitted geometric feature (e.g., based on additional parameters in a set of excluded region parameters, as outlined below with reference to ) and using that adjusted fitted geometric feature as a boundary of the excluded region, wherein the adjusted fitted geometric feature provides a boundary that encompasses a majority of the detected edge points used for fitting the geometric feature, or the like.","In one embodiment, a circle tool may provide a circularity dimension related to the scatter of the edge points that a circle is fit to for purposes of characterization and the circle may be adjusted based on the circularity dimension. In other embodiments, the excluded region is determined by forming its boundary along line or curve segments joining the detected edge points, or the like. In some embodiments, the adjustment of the size of the excluded region is based on the scan direction (e.g., as indicated by a scan direction indicator or related parameter of the robust video tool), such that the excluded region boundary moves in a direction opposite to that of the scan direction. In some embodiments, this ensures that the scan can \u201cstop\u201d at the excluded region, before detecting any erroneous edge points of a previously characterized interfering feature. However, such methods of determining the excluded region are exemplary only, not limiting. Various other ways of determining an excluded region based on a previously characterized feature may be devised by one skilled in the art based on the teachings of this disclosure. The excluded region may be determined by the excluded region generator outlined with reference to , using the characterization stored in the previously characterized feature memory portion ",{"@attributes":{"id":"p-0066","num":"0065"},"figref":["FIG. 4C","FIG. 4B","FIG. 3C","FIG. 3C"],"b":["400","305","420","420","420","420","420","420","422","428","424","420","420","420","420","420","420","320","420","310","310","310"],"i":["roi ","roi ","roi ","roi "]},"In one embodiment, the excluded region operations of the robust video tool \u2032 that prevent the set of current feature edge points from including edge points located within the excluded region may comprise analyzing the region of interest \u2032to identify a set of \u201ctemporary\u201d edge points therein (e.g., all detected edge points therein), then eliminating temporary edge points that are located in the excluded region, and using the remaining temporary edge points as the final set of detected current feature edge points. In another embodiment, the excluded region operations may comprise excluding the excluded region from the region of interest \u2032to form a modified operational region of interest, and analyzing the modified operational region of interest to determine the detected set of current feature edge points.",{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 5","b":["500","500","510","520","510","510","510","510","510","510","512","510","510","514","510","516","510","518"],"i":["a","b","c","a","c","b ","c "]},"The tabbed portion may reflect the X and Y coordinates of the center of the circle tool ROI, and the inner and outer radii of the circle tool ROI, denoted R and R respectively. These values may be determined by graphical definition using the region of interest indicator, and\/or they may be entered directly in the dialog box. The tabbed portion may also reflect the coordinates XS and YS of the edge selector.","As described in more detail in the previously incorporated '222 patent, in certain embodiments the tabbed portion may reflect the edge search parameters to be employed within the selected ROI. The tabbed portion may reflect a slope parameter type that specifies whether the edge intensity profile is to exhibit a falling slope (light to dark), a rising slope (dark to light) or any (either) slope when proceeding along the circle edge detection scan line direction indicated by the circle tool. The tabbed portion may reflect a parameter type that specifies whether the edge intensity profile corresponds to a strong edge or a weak edge. Strong edge search criteria may be more stringent to ensure higher reliability in the resulting detection. Conversely, weak edge search criteria sacrifice some reliability, in order to make it more likely that the weak edge will be identified. The tabbed portion may also reflect scan parameters that are used for identifying points along the circle edge in the ROI. Outlier removal may cause geometrically deviant points to be rejected, a scan interval value may cause points to be identified at a spacing of 1 degree, or 5 degrees, etc., and a clockwise or counterclockwise sampling direction may be specified for proceeding with the edge scans.","As also described in more detail in the '222 patent, in certain embodiments the tabbed portion may include a portion that allows selection of a type of filter that may be applied to the image data in the ROI before performing edge detection. In one specific example embodiment, the user may be allowed to select one of four filter types or no filter at all. The user may select a median, an averaging, a Gaussian or a morphology filter. The tabbed portion may also include a portion that reflects edge detection threshold values that may govern edge detection operations. Certain embodiments allow selection of either a static or a dynamic edge threshold. In one specific example embodiment, the user may specify three values TH, THR, and THS. The static threshold value TH defines the mean pixel intensity of the pixels that define an edge. The dynamic threshold value THR modifies the value THS at run time. The edge strength threshold value THS defines the minimum acceptance threshold for the difference in gray scale intensity of the pixels that define the edge of the surface. These thresholds determine whether an edge point is identified along an edge intensity scan line, or whether a scan \u201cfails.\u201d","In the embodiment shown in , the robust video tool interface  corresponds to a circle-type video tool which may be operated in either the robust \u201cexcluded region mode\u201d (that is, a mode which utilizes excluded region operations as disclosed herein), or in a known standard mode (that is, a non-excluded region mode wherein a workpiece image is analyzed and a set of edge points are determined for a current feature in the region of interest without consideration of an excluded region). In this embodiment, the tabbed portion includes an excluded region activation element  which may comprise part of the excluded region parameter portion , and which is operable by a user to determine whether or not a current instance of the video tool operates in the excluded region mode (e.g., using excluded region operations) or in the non-excluded region mode. In this particular embodiment, the excluded region activation element  comprises a pair of \u201cyes\/no\u201d radio buttons which toggle based on a user selection. In one embodiment, the excluded region parameter portion  only appears on the display when the \u201cyes\u201d radio button is selected (that is, when the robust excluded region mode operations are to be used). It will be appreciated that in other embodiments, these different \u201cmodes\u201d may be implemented using two separate circle-type video tools, instead.","In the embodiment shown in , the tabbed portion also includes an excluded feature candidate indicator  comprising a pair of \u201cyes\/no\u201d radio buttons which toggle based on a user selection in order to determine whether a feature characterized by a current instance of the tool will be included in a list of \u201cexcluded feature candidates\u201d. For example, when the robust mode is not activated in the video tool interface , a user may want to use a current instance of the video tool to characterize a feature in order to provide a previously characterized feature, as disclosed herein. In that case, it may be useful to use the indicator  to mark the current feature for inclusion in a candidate list of previously characterized features and\/or record certain learned parameters and\/or detected edge points that may not otherwise be recorded, for later use by the excluded region generator.","The excluded region parameter portion  includes an excluded region parameter dialog box  which may be configured to define the members of a current instance of a set of excluded region parameters (e.g., the parameters used by the excluded region generator) used to generate an excluded region corresponding specifically to the current instance of the robust video tool. In the embodiment shown in , the excluded region parameter dialog box  includes an identification portion , an excluded region boundary adjustment element in column , an exclusion method selection element in column , and a reverse region polarity selection element in column .","The identification portion  identifies the previously characterized feature(s) that is(are) used to determine the excluded region. In the illustrated embodiment, the identification portion  comprises a column  including a list of previously characterized features, and a column  including a corresponding list of selection elements, which are configured by the user to provide the identification of which specific previously characterized features in column  that will be used to determine the excluded region for the current instance of the robust video tool. In this particular embodiment, each selection element comprises a pair of \u201cyes\/no\u201d radio buttons which toggle based on a user selection, to indicate whether the corresponding feature in column  is used to determine the excluded region (e.g., whether it is included in the excluded region), or not. In one embodiment, the list of previously characterized features in column  is automatically generated by the control system of the machine vision inspection system. In one embodiment, the list may include all previously characterized features. In another embodiment, the list may include all previously characterized features which are also located within the current image. In another embodiment, the list may include all previously characterized features which are proximate to (e.g., within a predetermined distance of) a defined region of interest of a current instance of the robust video tool. In yet other embodiments, the robust video tool operations and\/or the excluded region generator may automatically identify the previously characterized feature which is closest to the current instance of the robust video tool (that is, closest to its region of interest) as a previously characterized feature to be included in a current set of excluded region parameters that is used to determine the excluded region for the current instance of the robust video tool. In such an embodiment, the identification portion  may be omitted.","As previously outlined, the size of the excluded region may be adjusted based on adjusting a dilation of detected edge points, or adjusting the size of a fitted geometric feature, or the like (e.g., such that the excluded region encompasses a majority of the detected edge points of the previously characterized feature). The excluded region boundary adjustment element in column  may be set by the user to govern the size of such adjustments. In the embodiment shown in , the excluded region boundary adjustment element in column  comprises a slider , which may be dragged by the user to determine the amount of adjustment. However, in other embodiments, the user may enter a number of pixels (or other units) as an adjustment amount. In other embodiments, the user may drag a handle on a displayed excluded region indicator (e.g., as shown in ), or the adjustment may be automatic (e.g., based on a default amount, or a determined scatter parameter of the previously characterized feature, or the like), and the excluded region boundary adjustment element in column  may be omitted.","As previously outlined, the excluded region operations of the robust video tool that prevent the set of current feature edge points from including edge points located within the excluded region may comprise analyzing the region of interest to identify a set of \u201ctemporary\u201d edge points therein, then eliminating temporary edge points that are located in the excluded region, and using the remaining temporary edge points as the final set of detected current feature edge points. In another embodiment, the excluded region operations may comprise excluding the excluded region from the region of interest to form a modified operational region of interest, and analyzing the modified operational region of interest to determine the detected set of current feature edge points. In one embodiment, the exclusion method selection element shown in column  may be used by the user to select which of these methods is applied. In other embodiments, the method may be predetermined and the element in column  may be omitted.","As previously outlined, a user may choose to measure a larger feature rather than a smaller feature as the previously characterized feature if it exhibits a stronger edge, or the like, in order to provide a reliably characterized previously characterized feature. In the case that a larger feature such as a large circle is used as the previously characterized feature, then it may be the area \u201coutside the circle\u201d that is the excluded region applicable to a current instance of a robust video tool. In one embodiment, the reverse region polarity selection element in column , comprising a toggle switch icon that may be clicked by a user to reverse the polarity, may be used by the user to select the \u201cpolarity\u201d of an excluded region relative to a previously characterized feature. In other embodiments, the polarity may be automatically determined based on other parameters of the current instance of the robust video tool and\/or the relevant previously characterized feature(s), and the element in column  may be omitted.","It will be appreciated that the foregoing embodiments of user interface features are exemplary only, and not limiting. Other features usable to enter and\/or alter the members of a set of excluded region parameters in the context of a robust video tool user interface will be apparent to one skilled in the art based on this disclosure.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIG. 6","FIG. 6","FIG. 4C","FIG. 5"],"b":["600","600","603","305","420","420","420","422","424","428","510","530","600","620","640","630","650"],"i":"roi"},"In various embodiments, the user may create a current instance of a robust video tool by selecting a robust video tool from a drop down menu or toolbar that displays a plurality of alternative video tools and\/or mode selection buttons, all accessed under the tools menu element . Upon such a selection, in one embodiment, the user interface may automatically display the region of interest indicator \u2032in a default position, as well as the previously described robust circle tool parameter dialog box  and the excluded region parameter dialog box , for configuring the various parameters of the selected robust video tool, as outlined above.","In various embodiments, the robust video tool interface displays the region of interest indicator \u2032and the excluded region indicator ERI of the robust video tool superimposed upon the current feature E to be inspected, as shown in the field of view window . In various embodiments, the excluded region indicator ERI may appear automatically, as determined based on the closest previously characterized feature to the current region of interest \u2032, or as soon as a previously characterized feature is selected using the appropriate columns of the excluded region parameter dialog box  (as outlined above with reference to ), or the like. In the embodiment shown in , the excluded region indicator ERI includes a size adjustment handle ERIah, which may be engaged using a mouse and dragged by the user to resize the excluded region (e.g., as generated by the excluded region generator according to principles outlined above) and define or redefine associated adjustment parameters. In various embodiments, the excluded region indicator ERI may automatically change on the display when changes are made in a current instance of the set of excluded region parameters using the excluded region parameter dialog box , or the like.","In the example shown in , the excluded region indicated by the excluded region indicator ERI corresponds to the previously characterized feature \u201cCircle\u201d selected in the excluded region parameter dialog box . As indicated by its name, Circle is a previously characterized feature that was characterized by using a circle tool included in the plurality of video tools (e.g., tools accessible using the Tools menu element ) to detect its edge points and characterize its dimensional parameters, for example. In one embodiment, as outlined above, the excluded region indicator ERI may automatically appear at the previously characterized location of the feature Circle, as soon as Circle is selected using the appropriate columns of the excluded region parameter dialog box  (as outlined above with reference to ), and the excluded region generator determines the excluded region based on the selected Circle and on related parameters defined in the excluded region parameter dialog box . In one embodiment, the stored characterization data of the feature Circle may correspond to a circle fitted to edge points of the previously characterized edge feature \u2032E (e.g., as characterized by a circle tool), and the excluded region indicator ERI may initially correspond to an excluded region determined based on that fitted circle, with a default or user entered amount of adjustment as indicated in the adjustment column of the excluded region parameter dialog box . Desirably, the amount of adjustment should be set such that any observable edge points of the previously characterized edge feature \u2032E fall within the excluded region, so that the excluded region operations of the robust video tool (e.g., the operations that determine the set of current-feature edge points EP\u2032 such that the set includes edge points detected in the region of interest \u2032and does not include edge points located within the excluded region indicated by ERI) do not allow them to be included in the set of current-feature edge points EP\u2032 of the current feature E, which is to be detected and analyzed by the current instance of the robust video tool.","As previously outlined, in various embodiments, the machine vision inspection system (e.g., the system , shown in ) comprises a learn mode of operation and a run mode of operation. Its user interface is configured to define the parameters of at least one instance of a video tool included in a plurality of video tools (e.g., the video tools -shown in ) in order to provide the at least one previously characterized feature (e.g., the feature Circle, shown in ) using a representative workpiece during the learn mode of operation. The robust video tool interface (e.g., comprising the elements , , and  shown in ) is configured to define the parameters of a current instance of the robust video tool used to determine a set of current-feature edge points for a current feature (e.g., the set of current-feature edge points EP\u2032 of the current feature E, shown in ), on the representative workpiece during the learn mode of operation. The machine vision inspection system is configured to record the parameters defined during the learn mode of operation in a part program, including the defined parameters of the at least one video tool used to provide the at least one previously characterized feature, the defined parameters of the current instance of the robust video tool, and the defined set of excluded region parameters. The machine vision inspection system is also configured to execute operations of the part program during the run mode of operation for a run mode workpiece similar to the representative workpiece, wherein the operations of the part program comprise (a) using the recorded defined parameters of the current instance of the at least one video tool to provide characterization of the at least one previously characterized feature on the run mode workpiece, (b) using the recorded defined excluded region parameters to determine the excluded region based on the at least one previously characterized feature on the run mode workpiece, and (c) using the recorded defined parameters of the current instance of the robust video tool to determine a set of current-feature edge points for the current feature on the run mode workpiece such that the set of current-feature edge points includes edge points detected in the region of interest of the current instance of the robust video tool on the run mode workpiece and does not include edge points located within the excluded region that is determined based on the at least one previously characterized feature on the run mode workpiece.","As previously indicated, the features of the present invention are more general, easier to implement, and yet more robust for certain types of applications, in comparison to the prior art. In particular, the operations and features disclosed above are video tool operations and features operable by a relatively unskilled user. Importantly, the disclosed features and operations allow programming in learn mode on a representative workpiece that is either properly fabricated or improperly fabricated, and will still operate reliably in the resulting automatic part program on a run mode workpiece that is either properly fabricated or improperly fabricated.","It should be appreciated that although the examples illustrated and discussed herein use previously characterized and current features which are circle features characterized using circle-type video tools, that similar teachings and principles may be implemented using other types of video tools, such as arc tools which use an arc shaped region of interest, or tools that characterize a straight edge and use a rectangular region of interest, or the like. Such tools are known in the art, and may be modified to add the features of a robust video tool according to this invention, based on the teachings and principles disclosed herein.","Furthermore, although the examples illustrated and discussed herein use excluded regions which are defined to correspond to a specific instance of a robust video tool, the principles disclosed herein may also be applied and\/or implemented globally. For example, in one embodiment, a set of one or more robust video tools may be defined during learn mode, wherein each tool comprises the excluded region operations outlined above. A set of previously characterized features that are used to determine the excluded region may also be defined during learn mode (e.g., in one embodiment, by implementing an element analogous to the element  shown in , in various video tools). In one embodiment, a dialog box similar to the excluded region parameter dialog box  may be used to define related excluded region parameters for each member or all of the set of previously characterized features. Then, at the end of learn mode, or another appropriate time, the excluded region generator may generate a \u201cglobal\u201d excluded region which comprises a set of excluded regions corresponding to that defined set of previously characterized features. Then each robust video tool may refer to the global excluded region when performing its excluded region operations.","While various exemplary embodiments of the present invention have been illustrated and described, numerous variations in the illustrated and described arrangements of features and sequences of operations will be apparent to one skilled in the art based on this disclosure. In other embodiments, new types of video tools specific to utilizing the excluded region methods of the present invention may be provided. Other video tool embodiments and associated graphical user interface features will be apparent to one of ordinary skill in the art having the benefit of the general teachings disclosed herein. Thus, it will be appreciated that various changes according to the teachings herein can be made to the various particular embodiments outlined above without departing from the spirit and scope of the invention."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing aspects and many of the attendant advantages of this invention will become more readily appreciated as the same become better understood by reference to the following detailed description, when taken in conjunction with the accompanying drawings, wherein:",{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":["FIG. 2","FIG. 1"]},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIGS. 3A and 3B"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 3C"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 4A","FIG. 3C"]},{"@attributes":{"id":"p-0028","num":"0027"},"figref":["FIG. 4B","FIGS. 3C and 4A"]},{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 4C","FIGS. 3C and 4A","FIG. 4B"]},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 6"}]},"DETDESC":[{},{}]}
