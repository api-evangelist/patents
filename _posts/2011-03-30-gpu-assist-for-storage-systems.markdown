---
title: GPU assist for storage systems
abstract: A method is provided for providing, with a GPU, selective cryptographic assist to data storage operations. The method is performed by a computer, the computer having a general-purpose central processing unit (CPU) and a special-purpose processor optimized for performing vector-based calculations. The method includes (a) calculating a processing load value on the CPU as the CPU performs a set of data storage operations, (b) comparing the calculated processing load value to a threshold value, (c) if the threshold value exceeds the calculated processing load, then performing cryptographic operations related to a data storage operation of the set of data storage operations on the CPU to the exclusion of the special-purpose processor, and (d) otherwise, performing the cryptographic operations related to the data storage operation on the special-purpose processor to the exclusion of the CPU. A corresponding apparatus is also provided. A method is also provided for providing GPU-assist to sequence-detection operations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08572407&OS=08572407&RS=08572407
owner: EMC Corporation
number: 08572407
owner_city: Hopkinton
owner_country: US
publication_date: 20110330
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"p":["Data storage systems often have particular processing needs. For example, some storage systems allow some or all data to be encrypted. Some storage systems provide a multipathing feature to allow data to travel over multiple paths before being written to disk. This may be done to allow for redundancy. It may also be done to increase bandwidth or to distribute load.","In some data storage systems, these particular processing needs are performed in software by a central processing unit (CPU). In particular, data encryption is often performed by a crypto kernel as part of the software used to control the storage system. Path selection for use with multipathing is also often performed by a multipathing module as part of the storage system software. It may be noted that path selection may also involve selecting a particular port of a storage device from a plurality of available ports.","Some storage systems make use of storage devices that incorporate read-ahead caches. These read-ahead caches read several blocks ahead of any given read operation in order to increase efficiency in sequential read operations. This efficiency results from minimizing the need to continually access the disk platters, since disk access operations are much more expensive than cache reads.","The above-described approach to data storage may not be entirely optimal, because some particular data storage processing needs are cycle-intensive tasks, so, in some instances, performing these operations in software on the computer system may slow down data storage operations.","In particular, data encryption can utilize a large amount of processing resources. Therefore, it is desirable to offload encryption operations from the CPU when CPU utilization is high.","Furthermore, sequential data storage operations may be hindered by not taking advantage of read-ahead caching techniques when multipathing to different ports of a storage device using distinct caches for each port. Therefore, it is desirable to perform sequence detection operations as part of the multipathing task in order to maximize read efficiency. However, since sequence detection operations are a cycle-intensive task, it is also desirable to offload these sequence detection operations from the CPU.","Embodiments of the present invention are directed to techniques for providing, with a special-purpose processor optimized for performing vector-based calculations (such as, for example, a readily-available graphics processing unit or GPU), selective cryptographic assist to data storage operations. A method is performed by a computer, the computer having a general-purpose central processing unit (CPU) and a special-purpose processor optimized for performing vector-based calculations. The method includes (a) calculating a processing load value on the CPU as the CPU performs a set of data storage operations, (b) comparing the calculated processing load value to a threshold value, (c) if the threshold value exceeds the calculated processing load, then performing cryptographic operations related to a data storage operation of the set of data storage operations on the CPU to the exclusion of the special-purpose processor, and (d) otherwise, performing the cryptographic operations related to the data storage operation on the special-purpose processor to the exclusion of the CPU. Use of this method is beneficial because it allows cryptographic operations (which are typically highly parallelized) to be performed on a GPU at the same time that the remainder of the data storage operation processing is performed on the CPU. In addition, because the offloading of cryptographic operations to the GPU is selective (based on CPU utilization), it avoids slowing down operations when CPU utilization is low (due to inefficiencies introduced by communications and coordination between the CPU and GPU).","Embodiments of the present invention are also directed to techniques for providing special assistance to sequence-detection operations in a data storage system. A method is performed by a computer, the computer having a general-purpose central processing unit (CPU) and a special-purpose processor optimized for performing vector-based calculations. The method includes (a) performing a set of data storage READ operations on the CPU, each data storage READ operation of the set of data storage operations being directed to one data storage device of a data storage system, the one data storage device having a plurality of ports, (b) performing, on the special-purpose processor, sequence detection operations associated with the set of data storage READ operations, including sending a signal to the CPU indicating whether or not a particular data storage READ operation of the set of data storage READ operations is in-sequence with previous data storage READ operations of the set of data storage READ operations, and (c) if the CPU does not receive a signal from the special-purpose processor indicating that the particular data storage READ operation is in-sequence with previous data storage READ operations, then sending the particular data storage READ operation to a port of the plurality of ports based only on load-balancing selection criteria, and (d) if the CPU receives a signal from the special-purpose processor indicating that the particular data storage READ operation is in-sequence with previous data storage READ operations, then sending the particular data storage READ operation to a same port which processed the previous data storage READ operations. Use of this method is beneficial because it allows sequence detection operations (which are typically highly vectorized) to be performed on a GPU at the same time that the remainder of the data storage operation processing is performed on the CPU. This allows multipathing to be selectively modified to allow sequential READ operations to be aggregated at a single port of a storage device, instead of distributing such sequential READ operations among different ports of the storage device, each port having a separate read-ahead cache.",{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 1","b":["30","30","30"]},"System  includes a key manager server , a host interconnect , and one or more hosts  (depicted as hosts (), (), . . . , ()). Key manager server  and hosts  connect to each other via host interconnect . The hosts  also connect to storage devices  (depicted as storage devices (), (), . . . , ()) via a storage interconnect . In some embodiments, the host interconnect  and the storage interconnect  are combined.","Each host  includes a processor (depicted as CPU ), a controller chipset , memory , and a general-purpose bus  for communicating with peripherals installed in expansions slots . Processor  may be, for example, a general purpose processor or microprocessor, a central processing unit (CPU), or a set of multiple CPUs. For the purposes of this disclosure, the term CPU  will refer to the central processing hardware of the host , whether it be a single CPU or a set of multiple CPUs operating in a symmetric or asymmetric configuration. Memory  may be made up of one or more of the following: volatile random access memory, non-volatile read-only memory, non-volatile flash memory, magnetic storage, optical storage, etc. In one embodiment, the expansion slots  are compatible with the Peripheral Component Interconnect (PCI) Express standard, as is well-known in the art. Installed in at least one expansion slot is a graphics card , having an on-board graphics processing unit (GPU) . Further details of the GPU  are provided below, in connection with . Graphics cards are typically used for performing graphics calculations and other operations associated with displaying graphics on a screen. However, in the present disclosure, the graphics card  is used for other purposes instead (although it should be understood that in some embodiments, graphics card  may also be used in the typical manner in addition to performing these additional functions). Also installed in at least one expansion slot  are one or more host bus adapters (HBAs)  (depicted as HBAs -, -, . . . , -) for connecting to storage interconnect  over redundant paths. Each HBA  connects to storage interconnect  through one or more paths.","Hosts  are computers executing applications that store data on the data storage devices . In addition to connecting to the host interconnect , each host  also connects to the storage interconnect , typically via a plurality of independent connections. In one embodiment, the hosts  employ a multipathing function which establishes and utilizes multiple paths from a given host  to a given storage device , which can provide higher performance as well as redundancy for greater availability.","The storage interconnect  can be any type of network or input\/output (I\/O) bus capable of interconnecting storage devices  with host computers . In some embodiments, the storage devices  and host  are interconnected in a manner such that, to the operating systems running on the hosts , the storage devices  appear as locally attached, but this is not required. The storage interconnect  may be a shared, public, or private network and encompasses a wide area or local area and can be implemented through any suitable combination of wired and\/or wireless communication networks. Furthermore, the storage interconnect  may include local area network (LAN), a wide area network (WAN), an intranet, the Internet, or a set of switches. For example, in one embodiment, the storage interconnect  works with Fibre Channel connectivity and is implemented in the form of a storage area network (SAN). In another embodiment, the storage interconnect  works with internet protocol (IP) connectivity and is implemented via an Internet-Small Computer System Interface (iSCSI) (e.g., for Fibre Channel). Those of skill in the art will recognize that other implementations are, of course, possible.","Storage devices  may be, in some embodiments, any sort of storage equipment capable of connecting to storage interconnect . In some embodiments, each storage device  is a disk array. Examples of disk arrays include the Symmetrix Integrated Cache Disk Array System and the CLARiiON Disk Array System, both available from EMC Corp. of Hopkinton, Mass. As is well-known in the art, a typical disk array includes a disk array controller, disk enclosures holding a plurality of disk drives, and a power supply. Each storage device  also includes one or more ports  (depicted, in the case of storage device (), as ()-, ()-, and ()-), each of which has one or more physical connections to storage interconnect . Each port also has an associated cache  (depicted, in the case of storage device (), as ()-, ()-, and ()-), such as, for example, a read-ahead cache. In some embodiments, for every READ operation directed at a port , the storage device  reads 8 or 16 blocks of data past a last block of the READ operation and deposits the read blocks in the read-ahead cache .","In operation, the hosts  execute application programs that utilize the storage devices  for non-volatile data storage. The storage interconnect  may employ a storage-oriented protocol such as iSCSI or Fibre Channel to enable block-oriented read and write commands and the accompanying data to be transferred between the hosts  and storage devices . Additionally, the system  provides selective encryption of storage data by the hosts . The key manager server  and host interconnect  provide support for the data encryption function as described in more detail below.","Key manager server  provides key manager functionality, i.e., the generation, protection, storage, replacement, and elimination of data encryption keys and related data that are used in data encryption\/decryption operations. In one embodiment, key manager server  is a server appliance. One example of a key manager server  usable in some embodiments is the RSA Key Manager appliance manufactured by EMC Corp. of Hopkinton, Mass. It should be understood that this is by way of example only; other products may also serve as the key manager server .","Key manager server  and hosts  connect to each other via host interconnect . Host interconnect  may be, for example, a network, such as a LAN or a WAN. Host interconnect  may also be realized by a collection of one or more switches interconnecting key manager server  and hosts .","As mentioned, key manager server  controls the generation, protection, storage, replacement, and elimination of data encryption keys. In particular, key manager server  creates encryption keys and corresponding key identifiers. Each key identifier is associated with a corresponding encryption key and can be used to obtain the key from the key manager server , provided that all permissions and credentials are in place.",{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 2","b":["50","50","50","61","62","61","62","46","50"]},"OS  (which contains many well-known components that are not shown or described herein) includes a file system  and a logical volume manager . OS  also includes an input\/output (I\/O) filter driver  and an HBA driver . I\/O filter driver  may be, for example, a component of the PowerPath Encryption With RSA software available from EMC Corp. of Hopkinton, Mass. I\/O filter driver  includes an OS interface , an HBA interface , and a set of common application programming interfaces (APIs) . I\/O filter driver  also includes a key controller module (KCM) or encryption manager  and one or more intermediate layers (IL) . ILs  may include, for example, one or more virtualization modules  and multipathing modules . Load analyzer , crypto kernel , and GPU interfaces ,  may also be considered to be part of I\/O filter driver . Portions of the I\/O filter driver  and the HBA driver  may also make up a storage I\/O stack . It should be understood that this arrangement is by way of example only; in some embodiments, one or more components of the storage I\/O stack  may be external to the I\/O filter driver . HBA driver  is in communication with one or more HBA  to provide communication with storage devices .","The KCM  is generally responsible for managing the data encryption aspects of operation of the host  in which it resides. In some arrangements, the KCM  may arrange for the encryption to be performed by crypto kernel . However, since crypto kernel  runs in software (running on processor ), such operation may impose a performance penalty in terms of latency and\/or throughput of data storage operations. Therefore, in some arrangements, KCM  is able to arrange for all or a portion of the encryption to be performed by the GPU . In particular, load analyzer  determines if the CPU utilization of processor  exceeds a threshold, in which case, KCM  offloads cryptographic processing to the GPU  via GPU interface . This is particularly beneficial because a GPU is designed as a highly-parallel processor, which is optimized to perform similar operations on different data in parallel, which is beneficial in the case of cryptographic processing of large segments of data.","Multipathing module  distributes data storage operations, which are each directed at a particular data storage device (), between the various ports () of that device (). Typically, multipathing module  distributes the data storage operations between the ports () using a load-balancing approach, so that each port bears an approximately equal share of the workload. Multipathing module  also distributes all data storage operations directed at a particular port ()-p between the various available paths to that port and between the various HBAs  to process the operations, also using a load-balancing approach.","In some embodiments, multipathing module  may offload certain operations to GPU  as well. In particular, since sequential READ operations (i.e., READ operations that are directed to areas of a storage device  that are closely in-sequence with each other) may benefit from the read-ahead caching provided by cache , it is beneficial to send sequential read operations across paths that terminate in the same port , rather than evenly distributing these operations across multiple ports . Doing this may require sequence-detection for READ operations. However, since sequence-detection is a cycle-intensive task, it would generally not be efficient to overburden the CPU  with performing sequence-detection. Therefore, in some embodiments, multipathing module  offloads sequence-detection operations to the GPU  via GPU interface .",{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 3","b":["58","58","58","58","16"]},"GPU  includes a moderator  which is connected to a plurality of processing elements (PEs)  over an internal bus . Each PE  includes a local moderator (LM) , a plurality of processor cores (PCs) , and a set of processing registers (PR) . GPU  also includes additional standard components, not depicted. As depicted, GPU  includes six PEs , each of which has eight PCs , which allows six simultaneous operations to be performed, each operation operating on up to eight pieces of data. It should be understood, that the depicted number of PEs  and PCs  is by example only. Any number of PEs  and PCs  may be used, such as, for example, 32 PEs , each PE  having 64 PCs . Moderator  functions to distribute data and instructions to the various PEs  and to assemble output data into meaningful form. LMs  also serve a similar function, distributing data and instructions to the local PCs  and PRs  of that PE .","Because, in the present disclosure, GPU  is used primarily for non-graphical processing, all (or most) PEs  are available for performing non-graphical operations. GPU  is utilized herein for general-purpose computing on graphics processing units (GPGPU). Therefore, GPU  preferably supports one or more GPGPU programming libraries, such as, for example, OpenCL, DirectCompute, CUDA, or Stream. Use of one of these libraries allows a programmer to easily program the GPU  to perform kernels (functions) on streams of data elements.",{"@attributes":{"id":"p-0036","num":"0035"},"figref":["FIG. 4","FIG. 1","FIG. 2"],"b":["100","100","36","36","50","100","36"],"i":"a"},"In step , load analyzer  (which runs on CPU ) calculates a processing load value (hereinafter referred to as a CPLV\u2014a calculated processing load value) of the CPU  as the CPU  performs data storage operations. The CPLV represents a percentage of the total processing capacity of the CPU  being used, as is well-known in the art. In some embodiments, step  is not performed by load analyzer , or even by any component of the storage I\/O stack  or the I\/O filter driver . Rather, in some embodiments, a module built into the OS  continually performs CPLV calculations.","In step , load analyzer  (again, running on CPU ) compares the CPLV to a threshold value T, and selectively proceeds to choose either step  or step  based on the comparison in order to maximize performance, as described in further detail below. If the threshold T exceeds the CPLV, then load analyzer proceeds to choose step . However, if the CPLV exceeds the threshold T, then load analyzer proceeds to choose step  instead.","Threshold T represents a maximum CPU utilization below which it is desirable for the CPU  to perform cryptographic processing related to the data storage operations, but above which, it is desirable for the GPU  to perform cryptographic processing related to the data storage operations. It should be understood that there is a performance penalty generated by performing the encryption on the GPU , due to overhead generated by the large amount of data necessary to transmit back-and-forth across system bus . Therefore, it is best to perform the cryptographic processing on the GPU  when the load on the CPU  would otherwise reach 100% and the performance of the data storage operations would be slowed down enough to overcome the bus overhead. Thus, the threshold T should ideally be calculated so as approximately yield that result.","In some embodiments, threshold T is a fixed value, such as 90%, while in other embodiments, threshold T is a value dependent on the speed of the CPU . For example, on a single-core 3 GHz processor, the threshold T may equal 25%, while on a quad-core 3 GHz processor, the threshold T may equal 90%. In yet another embodiment, the threshold T may be dynamically calculated based on the number of applications the CPU  is running.","It should be understood that, although step  has been depicted as being performed when T>CPLV and step  when T\u2266CPLV, in some embodiments, step  may be performed when T\u2267CPLV and step  when T<CPLV.","In step , crypto kernel  performs all of the cryptographic processing related to a data storage operation on the CPU  instead of on the GPU . Although the GPU  is much more highly parallelized than the CPU , the CPU  can, most likely, perform individual mathematical operations faster than the GPU .","In step , the GPU  performs cryptographic processing related to the data storage operation instead of the CPU  performing this cryptographic processing. It should be understood that the CPU  may perform some tasks related to the cryptographic processing (such as sending the appropriate data to the GPU  and aggregating results generated by the GPU ), but the underlying cryptographic calculations are performed on the GPU .","The GPU  is well-suited to performing cryptographic operations on data storage operations because these cryptographic operations typically involve performing independent operations on large sets of data in parallel. For example, using a block-cipher approach, since every block of data (e.g., 128 bits or 16 bytes of data) is processed independently, the parallelism of the GPU  can be exploited. In some encryption schemes, such as the well-known Advanced Encryption Standard (AES), parallelism can be exploited within blocks. For example, in AES, each block is converted into a 4\u00d74 matrix, and, for at least some operations within the cryptographic processing, individual cells (or other partitioned areas, such as rows or columns) can be independently processed according to the AES algorithm (for example, by XORing each partitioned area with an encryption key or a value derived from an encryption key). Partitioning of the data among various PEs  may be performed by moderator , and partitioning of data within each PE  may be performed by LM . Thus, in one simple example, if GPU  has 16 PEs , each of which contains 16 PCs , moderator  may break up the data into individual 128-bit blocks and send each of 16 128-bit blocks to a separate PE , allowing 16\u00d7128=2048 bits=256 bytes of data to be encrypted at once. Within each PE , LM  may partition the 128-bit block into 16 8-bit cells, each of which may be processed by a separate PC  in parallel making use of registers PR  using the same execution kernel. Alternatively, since each PC  is generally able to process 32 bits of data, moderator  may instead send four blocks of data to each PE , and then the LM  of each PE  would partition the data of each block into 4-cell groups of 32 bits each, allowing each PC  to process each 4-cell group in parallel. In this second example, 4\u00d716\u00d7128=8192 bits=1024 bytes=1 kilobyte of data may be encrypted at once. Using a more advanced GPU  would permit even larger chunks of data to be cryptographically processed at a time.","It should be understood that steps  and  may be repeated continuously, periodically, or upon certain triggers. For example, in one embodiment, step  may be performed once per storage operation, prior to engaging in that particular data storage operation. In that case, some data storage operations might utilize the GPU  for cryptographic processing, while other data storage operations might simultaneously utilize the crypto kernel  for such cryptographic processing. That could happen if, for example, at the beginning of operation 1, the CPLV is 91% (assuming a threshold of 90%), upon which the cryptographic processing for operation 1 is sent to the GPU . While the GPU  is still engaging in cryptographically processing operation 1, CPU  may begin processing data storage operation 2. CPU  would then perform steps  and  again. At that point, if the CPLV had dropped to 89%, then the CPU  would undertake cryptographic processing of operation 2 (via crypto kernel ). Thus, CPU  would be performing cryptographic processing of operation 2 at the same time that GPU  would be performing cryptographic processing of operation 1, since multiple data storage operations are performed at once in parallel","As an additional example, in another embodiment, step  may be performed at periodic intervals, such as once every 10 seconds. In that case, all data storage operations generated within the 10 seconds after a first performance of step  might be sent to the GPU  for cryptographic processing, while all data storage operations performed in the subsequent 10 seconds (performed after a second performance of step , with an opposite evaluation result) might be sent to the crypto kernel  for cryptographic processing. It should be understood that 10 seconds is given by way of example only. In some embodiments, the periodic intervals may be in the range of 1 second to 100 seconds. In some embodiments, the periodic interval is pre-selected and programmed into the software, while in other embodiments, the periodic interval is a user-selectable value at run-time.","Some embodiments are directed towards sequence detection.  depicts an example method  according to one embodiment. Method  is performed by a host , such as host (), as depicted in , with a memory  configured as in . In method , host  is able to perform data storage READ operations with GPU-assisted sequence-detection. It should be noted that, in some embodiments, data storage WRITE operations (and other non-READ operations) may not make use of this method , since they do not need to benefit from read-ahead caching. Therefore, multipathing module  (running on CPU ) is free to assign data storage WRITE operations to any port  based on availability and load-balancing criteria.","In step , CPU  performs a set of data storage READ operations, each data storage READ operation of the set of data storage operations being directed to one data storage device () of a data storage system. It should be understood that, in some embodiments, the READ operation may be specifically targeted at a particular logical disk mapped to the storage device (); it will be appreciated that the general principles apply in an equivalent manner to these embodiments. A typical data storage READ operation is associated with a block-start value and a length of the operation (in blocks). In some embodiments, a block-end value (which is equivalent to the block-start value plus the length minus 1) may replace the length. The READ operation directs the storage device () to READ from the address of the block-start value through the address of the block-end value.","In step , GPU  performs sequence detection operations associated with the set of data storage READ operations. This step is performed simultaneously with step . As part of this step, GPU interface  sends certain information about the data storage READ operations (such as a list of block start locations for the current READ operation and a set of previous READ operations directed to the same storage device ()) to GPU , GPU  performs the actual sequence-detection, and then GPU  sends a signal to the CPU  indicating whether or not a particular data storage READ operation of the set of data storage READ operations is in-sequence with previous data storage READ operations of the set of data storage READ operations. In some embodiments, the GPU  sets a flag (e.g., SeqFlag, stored in memory , but also transmitted back and forth between memory  and memory on graphics card ) associated with a particular storage device () to ON when data storage READ operations directed at that storage device () are sequential, and sets the flag to OFF when data storage READ operations directed at that storage device () are not sequential.","These sequence-detection operations are vector-based operations that take advantage of the highly-parallelized design of the GPU , which is optimized for performing vector calculations. Various vector-based sequence-detection operations (including various well-known vector-based sequence-detection operations) may be utilized. In some embodiments, a particular sequence-detection algorithm, described below, with reference to , may be used.","In step , the multipathing module  (running on CPU ) reads the value of SeqFlag for the data storage device () to which a particular data storage READ operation is directed, and if SeqFlag is OFF (i.e., the data storage READ operation is not sequential with previous data storage READ operations directed at that data storage device ()), then it proceeds to execute step , while, if SeqFlag is ON (i.e., the data storage READ operation is sequential with previous data storage READ operations directed at that data storage device ()), then it proceeds to execute step .","In step , multipathing module  directs the data storage READ operation towards any available port ()-q of the data storage device () without regard to which port ()-p processed the previous data storage READ operation directed towards that data storage device. In particular, the multipathing module chooses port ()-q based on load-balancing selection criteria, as is well-known in the art.","In step , multipathing module  directs the data storage READ operation towards whichever port ()-p of the data storage device () processed the previous data storage READ operation directed towards that data storage device (). Multipathing module  does this step without regard to load-balancing. This allows the data storage device () to take advantage of the read-ahead caching provided by cache ()-p. However, multipathing module  is still free to select any available path between the host  and that port ()-p based on load-balancing selection criteria.","Returning to step , the sequence-detection may be performed by the GPU  in the following manner. For each data storage READ operation directed to a particular data storage device (), if the SeqFlag is already set ON, then it is determined if the current data storage READ operation is in-sequence with the previous sequential READ operations. In making this determination, GPU  makes use of READ operation data from the last p (e.g., the last 12) READ operations directed to that particular data storage device () (or, instead from the past q seconds\u2014for example, 10 seconds), or since the SeqFlag was last changed from OFF to ON, whichever period is shorter. Between each READ operation to be considered (let us say that there are p such READ operations), the GPU calculates the slope of READ block-start values plotted against the successive READ operation numbers. READ block-end values are typically ignored. In one embodiment, as long as the slopes remain substantially of the same sign (positive or negative) and no slope is greater than an upper threshold value (UTV) or less than a lower threshold value (LTV), the READ operations are considered to be sequential. In one embodiment, the slopes are said to remain substantially of the same sign as long as there are no more than two inflection points (changes of sign in the slope) over any set of 5 READ operations in a row. In some embodiments, slope values are mathematically smoothed before being compared, in order to reduce the effect of outlier operations.","For example,  depicts a graph  of example data that may be used in performing method . Graph  corresponds to the data in Table 1, below.",{"@attributes":{"id":"p-0056","num":"0055"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}},{"entry":["Request # -","Time","Block","Block",{},{},{}]},{"entry":["X","Period","Start - Y","End","Y2-Y1","X2-X1","Slope"]},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["1","0","0","10",{},{},{}]},{"entry":["2","1","20","30","20.00","1","20"]},{"entry":["3","2","16","32","\u22124.00","1","\u22124"]},{"entry":["4","3","14","28","\u22122.00","1","\u22122"]},{"entry":["5","4","18","36","4.00","1","4"]},{"entry":["6","5","64","128","46.00","1","46"]},{"entry":["7","6","128","256","64.00","1","64"]},{"entry":["8","10","256","512","128.00","1","128"]},{"entry":["9","20","192","256","\u221264.00","1","\u221264"]},{"entry":["10","25","250","512","58.00","1","58"]},{"entry":["11","26","328","456","78.00","1","78"]},{"entry":["12","30","850","1024","522.00","1","522"]},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]}}]}}},"Table 1 depicts an example set of substantially sequential READ operations and associated information. As an illustration, GPU  calculates the slope between requests 4 and 5 by calculating Y2\u2212Y1, which corresponds to the difference in READ block start values between rows 4 and 5, which is 18\u221214=4. Then, GPU  is able to calculate the slope as (Y2\u2212Y1)\/(X2\u2212X1)=(18\u221214)\/(5\u22124)=4 (where the X values correspond to the request numbers and the Y values correspond to the block-start values).","Recall that, in one embodiment, the slopes are said to remain substantially of the same sign as long as there are no more than two inflection points (changes of sign in the slope) over any set of 5 READ operations in a row. Thus, in Table 1, since inflection points exist at READ requests 3, 5, 9, and 10, and since there is no group of 5 sequential READ requests in the range 1-11 that have more than 2 inflection points (and since no positive slopes are less than the UTV and no negative slopes exceed the LTV), all points in Table 1 from 1 to 11 may be considered sequential in one embodiment.","As long as the slopes remain substantially of the same sign and no slope is greater than the UTV (e.g., 64) or less than the LTV (e.g., \u2212128), the READ operations are considered to be sequential. However, in one embodiment, smoothing is used, as follows to allow for occasional discrepancies.","For example, in one embodiment, GPU  may establish a (p\u22121)x2 matrix to smooth out the results over the last p READ operations. For p=12 as in Table 1, an 11x2 matrix is used. Each row of the matrix contains two Block start values to enable computation of a local slope Y2\u2212Y1. Thus, the matrix for Table 1 would be {(0, 20), (20, 16), (16, 14), (14, 18), (18, 64), (64, 128), (128, 256), (256, 192), (192, 250), (250, 328), (328, 850)}. The slopes for each row can be calculated in parallel using the GPU , yielding slopes s, s, . . . , s. An 11x3 matrix can now be constructed with each row having the UTV, LTV, and a slope. For example, row 1 would be (UTV, LTV, s)=(64, \u2212128, 20) while row 11 would be (UTV, LTV, s)=(64, \u2212128, 522). This matrix can also be processed in parallel on the GPU  by determining if the slope falls between the LTV and UTV for each row. If it falls within the range of LTV . . . UTV, then return 0. Otherwise return 1. This yields an array of bit values. For example, in the case of Table 1, the array is {0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1}. Preliminarily, the values can be added up, and if the values sum to less than 5, the SeqFlag would be set to ON. If the values sum to at least 5, it may be determined if there are at least five 1's in a row, in which case the SeqFlag would be set to OFF. Otherwise, the SeqFlag would be set to ON.","Once the SeqFlag is set to OFF, in order to turn it back ON, the GPU  examines data since the last inflection point. Once it finds three READ requests in a row that have the same sign and fall within the constraints of the upper threshold and the lower threshold, the SeqFlag may be set back to ON. SeqFlag is typically initially set to ON.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 6B","b":["400","200","400"]},{"@attributes":{"id":"p-0063","num":"0062"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 2"},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}},{"entry":["Request # -","Time","Block","Block",{},{},{}]},{"entry":["X","Period","Start - Y","End","Y2-Y1","X2-X1","Slope"]},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"1","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"6","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"28pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["1","0","181","181",{},{},{}]},{"entry":["2","1","593","594","412.00","1","412"]},{"entry":["3","2","812","814","219.00","1","219"]},{"entry":["4","3","968","971","156.00","1","156"]},{"entry":["5","4","83","87","\u2212885.00","1","\u2212885"]},{"entry":["6","5","920","925","837.00","1","837"]},{"entry":["7","6","472","478","\u2212448.00","1","\u2212448"]},{"entry":["8","10","217","227","\u2212255.00","1","\u2212255"]},{"entry":["9","20","4","24","\u2212213.00","1","\u2212213"]},{"entry":["10","25","239","264","235.00","1","235"]},{"entry":["11","26","301","327","62.00","1","62"]},{"entry":["12","30","856","1024","555.00","1","555"]},{"entry":{"@attributes":{"namest":"1","nameend":"7","align":"center","rowsep":"1"}}}]}}]}}},"Table 2 depicts an example set of substantially non-sequential READ operations and associated information. Performing the same example approach as illustrated above with respect to Table 1, the final array would be {1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1}, which indicates non-sequentiality.","GPU  performs these sequence-detection operations by performing all of the necessary slope calculations and smoothing operations in parallel, taking advantage of the multiple PEs  and PCs  on the GPU .","While various embodiments of the invention have been particularly shown and described, it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined by the appended claims.","It should be understood that although various embodiments have been described as being methods, software embodying these methods is also included. Thus, one embodiment includes a tangible computer-readable medium (such as, for example, a hard disk, a floppy disk, an optical disk, computer memory, flash memory, etc.) programmed with instructions, which, when performed by a computer or a set of computers, cause one or more of the methods described in various embodiments to be performed. Another embodiment includes a computer which is programmed to perform one or more of the methods described in various embodiments.","Furthermore, it should be understood that all embodiments which have been described may be combined in all possible combinations with each other, except to the extent that such combinations have been explicitly excluded.","Finally, nothing in this Specification shall be construed as an admission of any sort. Even if a technique, method, apparatus, or other concept is specifically labeled as \u201cprior art\u201d or as \u201cconventional,\u201d Applicants make no admission that such technique, method, apparatus, or other concept is actually prior art under 35 U.S.C. \u00a7102, such determination being a legal determination that depends upon many factors, not all of which are known to Applicants at this time."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing and other objects, features and advantages will be apparent from the following description of particular embodiments of the invention, as illustrated in the accompanying drawings in which like reference characters refer to the same parts throughout the different views. The drawings are not necessarily to scale, emphasis instead being placed upon illustrating the principles of various embodiments of the invention.",{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 6B"}]},"DETDESC":[{},{}]}
