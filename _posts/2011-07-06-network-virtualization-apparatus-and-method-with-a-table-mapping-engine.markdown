---
title: Network virtualization apparatus and method with a table mapping engine
abstract: Some embodiments provide a virtualizer for managing a plurality of managed switching elements that forward data through a network. The virtualizer comprises a first set of tables for storing input logical forwarding plane data and a second set of tables for storing output physical control plane data. It also includes a table mapping engine for mapping the input logical forwarding plane data in the first set of tables to output physical control plane data in the second set of tables by performing a set of database join operations on the input logical forwarding plane data in the first set of tables. In some embodiments, the physical control plane data is subsequently translated into physical forwarding behaviors that direct the forwarding of data by the managed switching elements.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08717895&OS=08717895&RS=08717895
owner: Nicira, Inc.
number: 08717895
owner_city: Palo Alto
owner_country: US
publication_date: 20110706
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"p":["This application claims benefit to U.S. Provisional Patent Application 61\/361,912, filed on Jul. 6, 2010; U.S. Provisional Patent Application 61\/361,913, filed on Jul. 6, 2010; U.S. Provisional Patent Application 61\/429,753, filed on Jan. 4, 2011; U.S. Provisional Patent Application 61\/429,754, filed on Jan. 4, 2011; U.S. Provisional Patent Application 61\/466,453, filed on Mar. 22, 2011; U.S. Provisional Patent Application 61\/482,205, filed on May 3, 2011; U.S. Provisional Patent Application 61\/482,615, filed on May 4, 2011; U.S. Provisional Patent Application 61\/482,616, filed on May 4, 2011; U.S. Provisional Patent Application 61\/501,743, filed on Jun. 27, 2011; and U.S. Provisional Patent Application 61\/501,785, filed on Jun. 28, 2011. These provisional applications are incorporated herein by reference.","Many current enterprises have large and sophisticated networks comprising switches, hubs, routers, servers, workstations and other networked devices, which support a variety of connections, applications and systems. The increased sophistication of computer networking, including virtual machine migration, dynamic workloads, multi-tenancy, and customer specific quality of service and security configurations require a better paradigm for network control. Networks have traditionally been managed through low-level configuration of individual components. Network configurations often depend on the underlying network: for example, blocking a user's access with an access control list (\u201cACL\u201d) entry requires knowing the user's current IP address. More complicated tasks require more extensive network knowledge: forcing guest users' port 80 traffic to traverse an HTTP proxy requires knowing the current network topology and the location of each guest. This process is of increased difficulty where the network switching elements are shared across multiple users.","In response, there is a growing movement, driven by both industry and academia, towards a new network control paradigm called Software-Defined Networking (SDN). In the SDN paradigm, a network controller, running on one or more servers in a network, controls, maintains, and implements control logic that governs the forwarding behavior of shared network switching elements on a per user basis. Making network management decisions often requires knowledge of the network state. To facilitate management decision-making, the network controller creates and maintains a view of the network state and provides an application programming interface upon which management applications may access a view of the network state.","Three of the many challenges of large networks (including datacenters and the enterprise) are scalability, mobility, and multi-tenancy and often the approaches taken to address one hamper the other. For instance, one can easily provide network mobility for virtual machines (VMs) within an L2 domain, but L2 domains cannot scale to large sizes. Also, retaining tenant isolation greatly complicates mobility. Despite the high-level interest in SDN, no existing products have been able to satisfy all of these requirements.","Some embodiments of the invention provide a system that allows several different logical data path sets to be specified for several different users through one or more shared network infrastructure switching elements (referred to as \u201cswitching elements\u201d below). In some embodiments, the system includes a set of software tools that allows the system to accept logical data path sets from users and to configure the switching elements to implement these logical data path sets. These software tools allow the system to virtualize control of the shared switching elements and the network that is defined by the connections between these shared switching elements, in a manner that prevents the different users from viewing or controlling each other's logical data path sets (i.e., each other's switching logic) while sharing the same switching elements.","In some embodiments, one of the software tools that allows the system to virtualize control of a set of switching elements (i.e., to allow several users to share the same switching elements without viewing or controlling each other's logical data path sets) is an intermediate data storage structure that (1) stores the state of the network, (2) receives and records modifications to different parts of the network from different users, and (3), in some embodiments, provides different views of the state of the network to different users. For instance, in some embodiments, the intermediate data storage structure is a network information base (NIB) data structure that stores the state of the network that is defined by one or more switching elements. The system uses this NIB data structure as an intermediate storage structure for reading the state of the network and writing modifications to the state of the network. In some embodiments, the NIB also stores the logical configuration and the logical state for each user specified logical data path set. In these embodiments, the information in the NIB that represents the state of the actual switching elements accounts for only a subset of the total information stored in the NIB.","In some embodiments, the system has (1) a network operating system (NOS) to create and maintain the NIB storage structure, and (2) one or more applications that run on top of the NOS to specify logic for reading values from and writing values to the NIB. When the NIB is modified in order to effectuate a change in the switching logic of a switching element, the NOS of some embodiments also propagates the modification to the switching element.","The system of different embodiments uses the NIB differently to virtualize access to the shared switching elements and network. In some embodiments, the system provides different views of the NIB to different users in order to ensure that different users do not have direct view and control over each other's switching logic. For instance, in some embodiments, the NIB is a hierarchical data structure that represents different attributes of different switching elements as elements (e.g., different nodes) in a hierarchy. The NIB in some of these embodiments is a multi-layer hierarchical data structure, with each layer having a hierarchical structure and one or more elements (e.g., nodes) on each layer linked to one or more elements (e.g., nodes) on another layer. In some embodiments, the lowest layer elements correspond to the actual switching elements and their attributes, while each of the higher layer elements serve as abstractions of the actual switching elements and their attributes. As further described below, some of these higher layer elements are used in some embodiments to show different abstract switching elements and\/or switching element attributes to different users in a virtualized control system.","In some embodiments, the definition of different NIB elements at different hierarchical levels in the NIB and the definition of the links between these elements are used by the developers of the applications that run on top of the NOS in order to define the operations of these applications. For instance, in some embodiments, the developer of an application running on top of the NOS uses these definitions to enumerate how the application is to map the logical data path sets of the user to the physical switching elements of the control system. Under this approach, the developer would have to enumerate all different scenarios that the control system may encounter as well as the mapping operation of the application for each scenario. This type of network virtualization (in which different views of the NIB are provided to different users) is referred to below as Type I network virtualization.","Another type of network virtualization, which is referred to below as Type II network virtualization, does not require the application developers to have intimate knowledge of the NIB elements and the links (if any) in the NIB between these elements. Instead, this type of virtualization allows the application to simply provide user specified logical switching element attributes in the form of one or more tables, which are then mapped to NIB records by a table mapping engine. In other words, the Type II virtualized system of some embodiments accepts the logical switching element configurations (e.g., access control list table configurations, L2 table configurations, L3 table configurations, etc.) that the user defines without referencing any operational state of the switching elements in a particular network configuration. It then maps the logical switching element configurations to the switching element configurations stored in the NIB.","To perform this mapping, the system of some embodiments uses a database table mapping engine to map input tables, which are created from (1) logical switching configuration attributes, and (2) a set of properties associated with switching elements used by the system, to output tables. The content of these output tables are then transferred to the NIB elements. In some embodiments, the system uses a variation of the datalog database language, called nLog, to create the table mapping engine that maps input tables containing logical data path data and switching element attributes to the output tables. Like datalog, nLog provides a few declaratory rules and operators that allow a developer to specify different operations that are to be performed upon the occurrence of different events. In some embodiments, nLog provides a limited subset of the operators that are provided by datalog in order to increase the operational speed of nLog. For instance, in some embodiments, nLog only allows the AND operator to be used in any of the declaratory rules.","The declaratory rules and operations that are specified through nLog are then compiled into a much larger set of rules by an nLog compiler. In some embodiments, this compiler translates each rule that is meant to address an event into several sets of database join operations. Collectively the larger set of rules forms the table mapping, rules engine that is referred to below as the nLog engine. In some embodiments, the nLog virtualization engine also provides feedback (e.g., from one or more of the output tables or from NIB records that are updated to reflect values stored in the output tables) to the user in order to provide the user with state information about the logical data path set that he or she created. In this manner, the updates that the user gets are expressed in terms of the logical space that the user understand and not in terms of the underlying switching element states, which the user does not understand.","The use of nLog serves as a significant distinction between Type I virtualized control systems and Type II virtualized control systems, even for Type II systems that store user specified logical data path sets in the NIB. This is because nLog provides a machine-generated rules engine that addresses the mapping between the logical and physical domains in a more robust, comprehensive manner than the hand-coded approach used for Type I virtualized control systems. In the Type I control systems, the application developers need to have a detailed understanding of the NIB structure and need to use this detailed understanding to write code that addresses all possible conditions that the control system would encounter at runtime. On the other hand, in Type II control systems, the application developers only need to produce applications that express the user-specified logical data path sets in terms of one or more tables, which are then mapped in an automated manner to output tables and later transferred from the output tables to the NIB. This approach allows the Type II virtualized systems not to maintain the data regarding the logical data path sets in the NIB. However, some embodiments maintain this data in the NIB in order to distribute this data among other NOS instances, as further described below.","As apparent from the above discussion, the applications that run on top of a NOS instance can perform several different sets of operations in several different embodiments of the invention. Examples of such operations include providing an interface to a user to access NIB data regarding the user's switching configuration, providing different layered NIB views to different users, providing control logic for modifying the provided NIB data, providing logic for propagating received modifications to the NIB, etc.","In some embodiments, the system embeds some or all such operations in the NOS instead of including them in an application operating on top of the NOS. Alternatively, in other embodiments, the system separates some or all of these operations into different subsets of operations and then has two or more applications that operate above the NOS perform the different subsets of operations. One such system runs two applications on top of the NOS, a control application and a virtualization application. In some embodiments, the control application allows a user to specify and populate logical data path sets, while the virtualization application implements the specified logical data path sets by mapping the logical data path set to the physical switching infrastructure. In some embodiments, the virtualization application translates control application input into records that are written into the NIB, and from the NIB these records are then subsequently transferred to the switching infrastructure through the operation of the NOS. In some embodiments, the NIB stores both the logical data path set input received through the control application and the NIB records that are produced by the virtualization application.","In some embodiments, the control application can receive switching infrastructure data from the NIB. In response to this data, the control application may modify record(s) associated with one or more logical data path sets (LDPS). Any such modified LDPS record would then be translated to one or more physical switching infrastructure records by the virtualization application, which might then be transferred to the physical switching infrastructure by the NOS.","In some embodiments, the NIB stores data regarding each switching element within the network infrastructure of a system, while in other embodiments, the NIB stores state information about only switching elements at the edge of a network infrastructure. In some embodiments, edge switching elements are switching elements that have direct connections with the computing devices of the users, while non-edge switching elements only connect to edge switching elements and other non-edge switch elements.","The system of some embodiments only controls edge switches (i.e., only maintains data in the NIB regarding edge switches) for several reasons. Controlling edge switches provides the system with a sufficient mechanism for maintaining isolation between computing devices, which is needed, as opposed to maintaining isolation between all switch elements, which is not needed. The interior switches forward data packets between the switching elements. The edge switches forward data packets between computing devices and other network elements (e.g., other switching elements). Thus, the system can maintain user isolation simply by controlling the edge switching elements because the edge switching elements are the last switches in line to forward packets to hosts.","Controlling only edge switches also allows the system to be deployed independent of concerns about the hardware vendor of the non-edge switches. Deploying at the edge allows the edge switches to treat the internal nodes of the network as simply a collection of elements that moves packets without considering the hardware makeup of these internal nodes. Also, controlling only edge switches makes distributing switching logic computationally easier. Controlling only edge switches also enables non-disruptive deployment of the system. Edge switching solutions can be added as top of rack switches without disrupting the configuration of the non-edge switches.","In addition to controlling edge switches, the network control system of some embodiments also utilizes and controls non-edge switches that are inserted in the switch network hierarchy to simplify and\/or facilitate the operation of the controlled edge switches. For instance, in some embodiments, the control system requires the switches that it controls to be interconnected in a hierarchical switching architecture that has several edge switches as the leaf nodes and one or more non-edge switches as the non-leaf nodes. In some such embodiments, each edge switch connects to one or more of the non-leaf switches, and uses such non-leaf switches to facilitate its communication with other edge switches. Examples of functions that such non-leaf switches provide to facilitate such communications between edge switches in some embodiments include (1) routing of a packet with an unknown destination address (e.g., unknown MAC address) to the non-leaf switch so that this switch can route this packet to the appropriate edge switch, (2) routing a multicast or broadcast packet to the non-leaf switch so that this switch can convert this packet to a series of unicast packets to the desired destinations, (3) bridging remote managed networks that are separated by one or more networks, and (4) bridging a managed network with an unmanaged network.","Some embodiments employ one level of non-leaf (non-edge) switches that connect to edge switches and in some cases to other non-leaf switches. Other embodiments, on the other hand, employ multiple levels of non-leaf switches, with each level of non-leaf switch after the first level serving as a mechanism to facilitate communication between lower level non-leaf switches and leaf switches. In some embodiments, the non-leaf switches are software switches that are implemented by storing the switching tables in the memory of a standalone computer instead of an off the shelf switch. In some embodiments, the standalone computer may also be executing in some cases a hypervisor and one or more virtual machines on top of that hypervisor. Irrespective of the manner by which the leaf and non-leaf switches are implemented, the NIB of the control system of some embodiments stores switching state information regarding the leaf and non-leaf switches.","The above discussion relates to the control of edge switches and non-edge switches by a network control system of some embodiments. In some embodiments, edge switches and non-edge switches (leaf and non-leaf nodes) may be referred to as managed switches. This is because these switches are managed by the network control system (as opposed to unmanaged switches, which are not managed by the network control system, in the network) in order to implement logical data path sets through the managed switches.","In addition to using the NIB to store switching-element data, the virtualized network-control system of some embodiments also stores other storage structures to store data regarding the switching elements of the network. These other storage structures are secondary storage structures that supplement the storage functions of the NIB, which is the primary storage structure of the system while the system operates. In some embodiments, the primary purpose for one or more of the secondary storage structures is to back up the data in the NIB. In these or other embodiments, one or more of the secondary storage structures serve a purpose other than backing up the data in the NIB (e.g., for storing data that are not in the NIB).","In some embodiments, the NIB is stored in system memory (e.g., RAM) while the system operates. This allows for fast access of the NIB records. In some embodiments, one or more of the secondary storage structures, on the other hand, are stored on disks, or other non-volatile memories, which are slower to access. Such non-volatile disks or other non-volatile memories, however, improve the resiliency of the system as they allow the data to be stored in a persistent manner.","The system of some embodiments uses multiple types of storages in its pool of secondary storage structures. These different types of structures store different types of data, store data in different manners, and provide different query interfaces that handle different types of queries. For instance, in some embodiments, the system uses a persistent transactional database (PTD) and a hash table structure. The PTD in some embodiments is a database that is stored on disk or other non-volatile memory. In some embodiments, the PTD is a commonly available database, such as MySQL or SQLite. The PTD of some embodiments can handle complex transactional queries. As a transactional database, the PTD can undo a series of earlier query operations that it has performed as part of a transaction when one of the subsequent query operations of the transaction fails.","Moreover, some embodiments define a transactional guard processing (TGP) layer before the PTD in order to allow the PTD to execute conditional sets of database transactions. The TGP layer allows the PTD to avoid unnecessary later database operations when conditions of earlier operations are not met. The PTD in some embodiments stores the exact replica of the data that is stored in the NIB, while in other embodiments it stores only a subset of the data that is stored in the NIB. In some embodiments, some or all of the data in the NIB is stored in the PTD in order to ensure that the NIB data will not be lost in the event of a crash of the NOS or the NIB.","While the system is running, the hash table of some embodiments is not stored on a disk or other non-volatile memory. Instead, it is a storage structure that is stored in volatile system memory when the system is running. When the system is powered down, the content of the hash table is stored on a disk or any other non-volatile memory. The hash table uses hashed indices that allow it to retrieve records in response to queries. This structure combined with the hash table's placement in the system's volatile memory allows the table to be accessed very quickly. To facilitate this quick access, a simplified query interface is used in some embodiments. For instance, in some embodiments, the hash table has just two queries, a Put query for writing values to the table and a Get query for retrieving values from the table. The system of some embodiments uses the hash table to store data that the NOS needs to retrieve very quickly. Examples of such data include network entity status, statistics, state, uptime, link arrangement, and packet handling information. Furthermore, in some embodiments, the NOS uses the hash tables as a cache to store information that is repeatedly queried, such as flow entries that will be written to multiple nodes.","Using a single NOS instance to control a network can lead to scaling and reliability issues. As the number of network elements increases, the processing power and\/or memory capacity that are required by those elements will saturate a single node. Some embodiments further improve the resiliency of the control system by having multiple instances of NOS running on one or more computers, with each instance of NOS containing one or more of the secondary storage structures described above. Each instance in some embodiments not only includes a NOS instance, but also includes a virtualization application instance and\/or a control application instance. In some of these embodiments, the control and\/or virtualization applications partition the workload between the different instances in order to reduce each instance's control and\/or virtualization workload. Also, in some embodiments, the multiple instances of NOS communicate the information stored in their secondary storage layers to enable each instance of NOS to cover for the others in the event of a NOS instance failing. Moreover, some embodiments use the secondary storage layer (i.e., one or more of the secondary storages) as a channel for communicating between the different instances.","The distributed, multi-instance control system of some embodiments maintains the same switch element data records in the NIB of each instance, while in other embodiments, the system allows NIBs of different instances to store different sets of switch element data records. Some embodiments that allow different instances to store different portions of the NIB, divide the NIB into N mutually exclusive portions and store each NIB portion in one NIB of one of N controller instances, where N is an integer value greater than 1. Other embodiments divide the NIB into N portions and store different NIB portions in different controller instances, while allowing some or all of the portions to partially (but not completely) overlap with the other NIB portions.","The hash tables in the distributed control system of some embodiments form a distributed hash table (DHT), with each hash table serving as a DHT instance. In some embodiments, the DHT instances of all controller instances collectively store one set of records that is indexed based on hashed indices for quick access. These records are distributed across the different controller instances to minimize the size of the records within each instance and to allow for the size of the DHT to be increased by adding other DHT instances. According to this scheme, each DHT record is not stored in each controller instance. In fact, in some embodiments, each DHT record is stored in at most one controller instance. To improve the system's resiliency, some embodiments, however, allow one DHT record to be stored in more than one controller instance, so that in case one instance fails, the DHT records of that failed instance can be accessed from other instances. Some embodiments do not allow for replication of records across different DHT instances or allow only a small amount of such records to be replicated because the system of these embodiments stores in the DHT only the type of data that can be quickly re-generated.","The distributed control system of some embodiments replicates each NIB record in the secondary storage layer (e.g., in each PTD instance and\/or in the DHT) in order to maintain the records in the NIB in a persistent manner. For instance, in some embodiments, all the NIB records are stored in the PTD storage layer. In other embodiments, only a portion of the NIB data is replicated in the PTD storage layer. For instance, some embodiments store a subset of the NIB records in another one of the secondary storage records, such as the DHT.","By allowing different NOS instances to store the same or overlapping NIB records, and\/or secondary storage structure records, the system improves its overall resiliency by guarding against the loss of data due to the failure of any NOS or secondary storage structure instance. For instance, in some embodiments, the portion of NIB data that is replicated in the PTD (which is all of the NIB data in some embodiments or part of the NIB data in other embodiments) is replicated in the NIBs and PTDs of all controller instances, in order to protect against failures of individual controller instances (e.g., of an entire controller instance or a portion of the controller instance).","In some embodiments, each of the storages of the secondary storage layer uses a different distribution technique to improve the resiliency of a multiple NOS instance system. For instance, as mentioned above, the system of some embodiments replicates the PTD across NOS instances so that every NOS has a full copy of the PTD to enable a failed NOS instance to quickly reload its PTD from another instance. In some embodiments, the system distributes the DHT fully or with minimal overlap across multiple controller instances in order to maintain the DHT instance within each instance small. This approach also allows the size of the DHT to be increased by adding additional DHT instances, and this in turn allows the system to be more scalable.","For some or all of the communications between the distributed instances, the distributed system of some embodiments uses coordination managers (CM) in the controller instances to coordinate activities between the different controllers. Examples of such activities include writing to the NIB, writing to the PTD, writing to the DHT, controlling the switching elements, facilitating intra-controller communication related to fault tolerance of controller instances, etc.","To distribute the workload and to avoid conflicting operations from different controller instances, the distributed control system of some embodiments designates one controller instance within the system as the master of any particular NIB portion (e.g., as the master of a logical data path set) and one controller instance within the system as the master of any given switching element. Even with one master controller, a different controller instance can request changes to different NIB portions and\/or to different switching elements controlled by the master. If allowed, the master instance then effectuates this change and writes to the desired NIB portion and\/or switching element. Otherwise, the master rejects the request.","The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly, to understand all the embodiments described by this document, a full review of the Summary, Detailed Description and the Drawings is needed. Moreover, the claimed subject matters are not to be limited by the illustrative details in the Summary, Detailed Description and the Drawing, but rather are to be defined by the appended claims, because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.","In the following detailed description of the invention, numerous details, examples, and embodiments of the invention are set forth and described. However, it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.","Some embodiments of the invention provide a method that allows several different logical data path sets to be specified for several different users through one or more shared switching elements without allowing the different users to control or even view each other's switching logic. In some embodiments, the method provides a set of software tools that allows the system to accept logical data path sets from users and to configure the switching elements to implement these logical data path sets. These software tools allow the method to virtualize control of the shared switching elements and the network that is defined by the connections between these shared switching elements, in a manner that prevents the different users from viewing or controlling each other's logical data path sets while sharing the same switching elements.","In some embodiments, one of the software tools that the method provides that allows it to virtualize control of a set of switching elements (i.e., to enable the method to allow several users to share the same switching elements without viewing or controlling each other's logical data path sets) is an intermediate data storage structure that (1) stores the state of the network, (2) receives modifications to different parts of the network from different users, and (3), in some embodiments, provide different views of the state of the network to different users. For instance, in some embodiments, the intermediate data storage structure is a network information base (NIB) data structure that stores the state of the network that is defined by one or more switching elements. In some embodiments, the NIB also stores the logical configuration and the logical state for each user specified logical data path set. In these embodiments, the information in the NIB that represents the state of the actual switching elements accounts for only a subset of the total information stored in the NIB.","The method uses the NIB data structure to read the state of the network and to write modifications to the state of the network. When the data structure is modified in order to effectuate a change in the switching logic of a switching element, the method propagates the modification to the switching element.","In some embodiments, the method is employed by a virtualized network control system that (1) allows user to specify different logical data path sets, (2) maps these logical data path sets to a set of switching elements managed by the control system. In some embodiments, the network infrastructure switching elements includes virtual or physical network switches, software switches (e.g., Open vSwitch), routers, and\/or other switching devices, as well as any other network elements (such as load balancers, etc.) that establish connections between these switches, routers, and\/or other switching devices. Such switching elements (e.g., physical switching elements, such as physical switches or routers) are implemented as software switches in some embodiments. Software switches are switches that are implemented by storing the switching tables in the memory of a standalone computer instead of an off the shelf switch. In some embodiments, the standalone computer may also be executing in some cases a hypervisor and one or more virtual machines on top of that hypervisor","These switches are referred to below as managed switching elements or managed forwarding elements as they are managed by the network control system in order to implement the logical data path sets. In some embodiments described below, the control system manages these switching elements by pushing physical control plane data to them, as further described below. Switching elements generally receive data (e.g., a data packet) and perform one or more processing operations on the data, such as dropping a received data packet, passing a packet that is received from one source device to another destination device, processing the packet and then passing it a destination device, etc. In some embodiments, the physical control plane data that is pushed to a switching element is converted by the switching element (e.g., by a general purpose processor of the switching element) to physical forwarding plane data that specify how the switching element (e.g., how a specialized switching circuit of the switching element) processes data packets that it receives.","The virtualized control system of some embodiments includes (1) a network operating system (NOS) that creates and maintains the NIB storage structure, and (2) one or more applications that run on top of the NOS to specify control logic for reading values from and writing values to the NIB. The NIB of some of these embodiments serves as a communication channel between the different controller instances and, in some embodiments, a communication channel between different processing layers of a controller instance.","Several examples of such systems are described below in Section I. Section II then describes the software architecture of a NOS instance. Section III further describes the control data pipeline of some embodiments of the invention. Section IV next describes how some embodiments perform the virtualization operations that map user specified input to data tuples in the NIB. Then, Section V describes operations the control application of some embodiments performs. Finally, Section VI describes the computer systems and processes used to implement some embodiments of the invention.","I. Virtualized Control System",{"@attributes":{"id":"p-0079","num":"0078"},"figref":"FIG. 1","b":"100"},"As shown in , the system  includes one or more switching elements , a network operating system , a network information base , and one or more applications . The switching elements include N switching devices (where N is a number equal to 1 or greater) that form the network infrastructure switching elements of the system . In some embodiments, the network infrastructure switching elements includes virtual or physical network switches, software switches (e.g., Open vSwitch), routers, and\/or other switching devices, as well as any other network elements (such as load balancers, etc.) that establish connections between these switches, routers, and\/or other switching devices. All such network infrastructure switching elements are referred to below as switching elements or forwarding elements.","The virtual or physical switching devices  typically include control switching logic  and forwarding switching logic . In some embodiments, a switch's control logic  specifies (1) the rules that are to be applied to incoming packets, (2) the packets that will be discarded, and (3) the packet processing methods that will be applied to incoming packets. The virtual or physical switching elements  use the control logic  to populate tables governing the forwarding logic . The forwarding logic  performs lookup operations on incoming packets and forwards the incoming packets to destination addresses.","As further shown in , the system  includes one or more applications  through which switching logic (i.e., sets of logical data paths) is specified for one or more users (e.g., by one or more administrators or users). The network operating system (NOS)  serves as a communication interface between (1) the switching elements  that perform the physical switching for any one user, and (2) the applications  that are used to specify switching logic for the users. In this manner, the application logic determines the desired network behavior while the NOS merely provides the primitives needed to access the appropriate network state. In some embodiments, the NOS  provides a set of Application Programming Interfaces (API) that provides the applications  programmatic access to the network switching elements  (e.g., access to read and write the configuration of network switching elements). In some embodiments, this API set is data-centric and is designed around a view of the switching infrastructure, allowing control applications to read and write state to any element in the network.","To provide the applications  programmatic access to the switching elements, the NOS  needs to be able to control the switching elements  itself. The NOS uses different techniques in different embodiments to control the switching elements. In some embodiments, the NOS can specify both control and forwarding switching logic  and  of the switching elements. In other embodiments, the NOS  controls only the control switching logic  of the switching elements, as shown in . In some of these embodiments, the NOS  manages the control switching logic  of a switching element through a commonly known switch-access interface that specifies a set of APIs for allowing an external application (such as a network operating system) to control the control plane functionality of a switching element. Two examples of such known switch-access interfaces are the OpenFlow interface and the Open Virtual Switch interface, which are respectively described in the following two papers: McKeown, N. (2008). (which can be retrieved from the Internet), and Pettit, J. (2010). (which can be retrieved from the Internet). These two papers are incorporated herein by reference.",{"@attributes":{"id":"p-0084","num":"0083"},"figref":"FIG. 1","b":["135","125"]},"In order to define the control switching logic  for physical switching elements, the NOS of some embodiments uses the Open Virtual Switch protocol to create one or more control tables within the control plane of a switch element. The control plane is typically created and executed by a general purpose CPU of the switching element. Once the system has created the control table(s), the system then writes flow entries to the control table(s) using the OpenFlow protocol. The general purpose CPU of the physical switching element uses its internal logic to convert entries written to the control table(s) to populate one or more forwarding tables in the forwarding plane of the switch element. The forwarding tables are created and executed typically by a specialized switching chip of the switching element. Through its execution of the flow entries within the forwarding tables, the switching chip of the switching element can process and route packets of data that it receives.","To enable the programmatic access of the applications  to the switching elements , the NOS also creates the network information base (NIB) . The NIB is a data structure in which the NOS stores a copy of the switch-element states tracked by NOS. The NIB of some embodiments is a graph of all physical or virtual switch elements and their interconnections within a physical network topology and their forwarding tables. For instance, in some embodiments, each switching element within the network infrastructure is represented by one or more data objects in the NIB. However, in other embodiments, the NIB stores state information about only some of the switching elements. For example, as further described below, the NIB in some embodiments only keeps track of switching elements at the edge of a network infrastructure. In yet other embodiments, the NIB stores state information about edge switching elements in a network as well as some non-edge switching elements in the network that facilitate communication between the edge switching elements. In some embodiments, the NIB also stores the logical configuration and the logical state for each user specified logical data path set. In these embodiments, the information in the NIB that represents the state of the actual switching elements accounts for only a subset of the total information stored in the NIB.","In some embodiments, the NIB  is the heart of the NOS control model in the virtualized network system . Under one approach, applications control the network by reading from and writing to the NIB. Specifically, in some embodiments, the application control logic can (1) read the current state associated with network entity objects in the NIB, (2) alter the network state by operating on these objects, and (3) register for notifications of state changes to these objects. Under this model, when an application  needs to modify a record in a table (e.g., a control plane flow table) of a switching element , the application  first uses the NOS' APIs to write to one or more objects in the NIB that represent the table in the NIB. The NOS then acting as the switching element's controller propagates this change to the switching element's table.",{"@attributes":{"id":"p-0088","num":"0087"},"figref":["FIG. 2","FIG. 2"],"b":["110","205","215","210","205","230","235","240","220","225","230","235","220"]},"Next, in the third stage, the NOS uses the set of switch-access APIs to write a new set of values into the switch. In some embodiments, the NIB performs a translation operation that modifies the format of the records before writing these records into the NIB. These operations are pictorially illustrated in  by showing the values d, e, f translated into d\u2032, e\u2032, f\u2032, and the writing of these new values into the switch . Alternatively, in some embodiments, one or more sets of values are kept identically in the NIB and the switching element, which thereby causes the NOS  to write the NIB values directly to the switch  unchanged.","In yet other embodiments, the NOS' translation operation might modify the set of values in the NIB (e.g., the values d, e, f) into a different set of values with fewer values (e.g., values x and y, where x and y might be a subset of d, e, and f, or completely different) or additional values (e.g., the w, x, y, z, where w, x, y, and z might be a super set of all or some of d, e, and f, or completely different). The NOS in these embodiments would then write this modified set of values (e.g., values x and y, or values w, x, y and z into the switching element).","The fourth stage finally shows the switch  after the old values a, b, and c have been replaced in the switch control record  with the values d\u2032, e\u2032, and f\u2032. Again, in the example shown in , the NOS of some embodiments propagates NIB records to the switches as modified versions of the records were written to the NIB. In other embodiments, the NOS applies processing (e.g., data transformation) to the NIB records before the NOS propagates the NIB records to the switches, and such processing changes the format, content and quantity of data written to the switches.","A. Different NIB Views","In some embodiments, the virtualized system  of  provides different views of the NIB to different users in order (1) to ensure that different users do not have direct view and control over each other's switching logic and (2) to provide each user with a view of the switching logic at an abstraction level that is desired by the user. For instance, in some embodiments, the NIB is a hierarchical data structure that represents different attributes of different switching elements as elements (e.g., different nodes) in a hierarchy. The NIB in some of these embodiments is a multi-layer hierarchical data structure, with each layer having a hierarchical structure and one or more elements (e.g., nodes) on each layer linked to one or more elements (e.g., nodes) on another layer. In some embodiments, the lowest layer elements correspond to the actual switching elements and their attributes, while each of the higher layer elements serves as abstractions of the actual switching elements and their attributes. As further described below, some of these higher layer elements are used in some embodiments to show different abstract switching elements and\/or switching element attributes to different users in a virtualized control system. In other words, the NOS of some embodiments generates the multi-layer, hierarchical NIB data structure, and the NOS or an application that runs on top of the NOS shows different users different views of different parts of the hierarchical levels and\/or layers, in order to provide the different users with virtualized access to the shared switching elements and network.",{"@attributes":{"id":"p-0094","num":"0093"},"figref":["FIG. 3","FIG. 1","FIG. 1"],"b":["300","300","100","300","105","105","120","105"],"i":["a","d "]},"In system , the NIB  stores sets of data records for each of the switching elements -. In some embodiments, a system administrator can access these four sets of data through an application  that interfaces with the NOS. However, other users that are not system administrators do not have access to all of the four sets of records in the NIB, because some switch logic records in the NIB might relate to the logical switching configuration of other users.","Instead, each non system-administrator user can only view and modify the switching element records in the NIB that relate to the logical switching configuration of the user.  illustrates this limited view by showing the application  providing a first layered NIB view  to a first user  and a second layered NIB view  to a second user . The first layered NIB view  shows the first user data records regarding the configuration of the shared switching elements -for implementing the first user's switching logic and the state of this configuration. The second layered NIB view  shows the second user data records regarding the configuration of the shared switching elements -for implementing the second user's switching logic and the state of this configuration. In viewing their own logical switching configuration, neither user can view the other user's logical switching configuration.","In some embodiments, each user's NIB view is a higher level NIB view that represents an abstraction of the lowest level NIB view that correlates to the actual network infrastructure that is formed by the switching elements -. For instance, as shown in , the first user's layered NIB view  shows two switches that implement the first user's logical switching configuration, while the second user's layered NIB view  shows one switch that implements the second user's logical switching configuration. This could be the case even if either user's switching configuration uses all four switching elements -. However, under this approach, the first user perceives that his computing devices are interconnected by two switching elements, while the second user perceives that her computing devices are interconnected by one switching element.","The first layered NIB view is a reflection of a first set of data records  that the application  allows the first user to access from the NIB, while the second layered NIB view is a representation of a second set of data records  that the application  allows the second user to access from the NIB. In some embodiments, the application  retrieves the two sets of data records  and  from the NIB and maintains these records locally, as shown in . In other embodiments, however, the application does not maintain these two sets of data records locally. Instead, in these other embodiments, the application simply provides the users with an interface to access the limited set of first and second data records from the NIB . Also, in other embodiments, the system  does not provide switching element abstractions in the higher layered NIB views  and  that it provides to the users. Rather, it simply provides views to the limited first and second set of data records  and  from the NIB.","Irrespective of whether the application maintains a local copy of the first and second data records or whether the application only provides the switching element abstractions in its higher layered NIB views, the application  serves as an interface through which each user can view and modify the user's logical switching configuration, without being able to view or modify the other user's logical switching configuration. Through the set of APIs provided by the NOS , the application  propagates to the NIB  changes that a user makes to the logical switching configuration view that the user receives from the application. The propagation of these changes entails the transferring, and in some cases of some embodiments, the transformation, of the high level data entered by a user for a higher level NIB view to lower level data that is to be written to lower level NIB data that is stored by the NOS.","In the system  of , the application  can perform several different sets of operations in several different embodiments of the invention, as discussed above. Examples of such operations include providing an interface to a user to access NIB data regarding the user's logical switching configuration, providing different layered NIB views to different users, providing control logic for modifying the provided NIB data, providing logic for propagating received modifications to the NIB structure stored by the NOS, etc.","The system of some embodiments embeds all such operations in the NOS  instead of in the application  operating on top of the NOS. Alternatively, in other embodiments the system separates these operations into several applications that operate above the NOS.  illustrates a virtualized system that employs several such applications. Specifically, this figure illustrates a virtualized system  that is similar to the virtualized system  of , except that the operations of the application  in the system  have been divided into two sets of operations, one that is performed by a control application  and one that is performed by a virtualization application .","In some embodiments, the virtualization application  interfaces with the NOS  to provide different views of different NIB records to different users through the control application . The control application  also provides the control logic for allowing a user to specify different operations with respect to the limited NIB records\/views provided by the virtualization application. Examples of such operations can be read operations from the NIB or write operations to the NIB. The virtualization application then translates these operations into operations that access the NIB. In translating these operations, the virtualization application in some embodiments also transfers and\/or transforms the data that are expressed in terms of the higher level NIB records\/views to data that are expressed in terms of lower level NIB records.","Even though  shows just one control application and one virtualization application being used for the two users, the system  in other embodiments employs two control applications and\/or two virtualization applications for the two different users. Similarly, even though several of the above-described figures show one or more applications operating on a single NOS instance, other embodiments provide several different NOS instances on top of each of which, one or more applications can execute. Several such embodiments will be further described below.","B. Type I versus Type II Virtualized System","Different embodiments of the invention use different types of virtualization applications. One type of virtualization application exposes the definition of different elements at different hierarchical levels in the NIB and the definition of the links between these elements to the control applications that run on top of the NOS and the virtualization application in order to allow the control application to define its operations by reference to these definitions. For instance, in some embodiments, the developer of the control application running on top of the virtualization application uses these definitions to enumerate how the application is to map the logical data path sets of the user to the physical switching elements of the control system. Under this approach, the developer would have to enumerate all different scenarios that the control system may encounter and the mapping operation of the application for each scenario. This type of virtualization is referred to below as Type I network virtualization.","Another type of network virtualization, which is referred to below as Type II network virtualization, does not require the application developers to have intimate knowledge of the NIB elements and the links in the NIB between these elements. Instead, this type of virtualization allows the application to simply provide user specified switching element attributes in the form of one or more tables, which are then mapped to NIB records by a table mapping engine. In other words, the Type II virtualized system of some embodiments accepts switching element configurations (e.g., access control list table configurations, L2 table configurations, L3 table configurations, etc.) that the user defines without referencing any operational state of the switching elements in a particular network configuration. It then maps the user-specified switching element configurations to the switching element configurations stored in the NIB.",{"@attributes":{"id":"p-0107","num":"0106"},"figref":["FIG. 5","FIG. 3","FIG. 4"],"b":["300","400","500","110","105","105","400","500","520","525","110","520","525"],"i":["a","d"]},"More specifically, the control application  allows (1) a user to specify abstract switching element configurations, which the virtualization application  then maps to the data records in the NIB, and (2) the user to view the state of the abstract switching element configurations. In some embodiments, the control application  uses a network template library  to allow a user to specify a set of logical data paths by specifying one or more switch element attributes (i.e., one or more switch element configurations). In the example shown in , the network template library includes several types of tables that a switching element may include. In this example, the user has interfaced with the control application  to specify an L2 table , an L3 table , and an access control list (ACL) table . These three tables specify a logical data path set  for the user. In specifying these tables, the user simply specifies desired switch configuration records for one or more abstract, logical switching elements. When specifying these records, the user of the system  does not have any understanding of the switching elements -employed by the system or any data regarding these switching elements from the NIB . The only switch-element specific data that the user of the system  receives is the data from the network template library, which specifies the types of network elements that the user can define in the abstract, which the system can then process.","While the example in  shows the user specifying ACL table, one of ordinary skill in the art will realize that the system of some embodiments does not provide such specific switch table attributes in the library . For instance, in some embodiments, the switch-element abstractions provided by the library  are generic switch tables and do not relate to any specific switching element table, component and\/or architecture. In these embodiments, the control application  enables the user to create generic switch configurations for a generic set of one or more tables. Accordingly, the abstraction level of the switch-element attributes that the control application  allows the user to create is different in different embodiments.","Irrespective of the abstraction level of the switch-element attributes produced through the control logic application, the virtualization application  performs a mapping operation that maps the specified switch-element attributes (e.g., the specific or generic switch table records) to records in the NIB. In some embodiments, the virtualization application translates control application input into one or more NIB records . The virtualization application then writes the resulting NIB records  to the NIB through the API set provided by NOS. From the NIB, these records are then subsequently transferred to the switching infrastructure through the operation of the NOS. In some embodiments, the NIB stores both the logical data path set input received through the control application as well as the NIB records that are produced by the virtualization application.","In some embodiments, the control application can receive switching infrastructure data from the NIB. In response to this data, the control application may modify record(s) associated with one or more logical data path sets (LDPS). Any such modified LDPS record would then be translated to one or more physical switching infrastructure records by the virtualization application, which might then be transferred to the physical switching infrastructure by the NOS.","To map the control application input to physical switching infrastructure attributes for storage in the NIB, the virtualization application of some embodiments uses a database table mapping engine to map input tables to output tables. These input tables are created from (1) the control-application specified input tables, and (2) a set of properties associated with switching elements used by the system. The content of these output tables are then transferred to the NIB elements.","Some embodiments use a variation of the datalog database language to allow application developers to create the table mapping engine for the virtualization application, and thereby to specify the manner by which the virtualization application maps logical data path sets to the controlled physical switching infrastructure. This variation of the datalog database language is referred to below as nLog. Like datalog, nLog provides a few declaratory rules and operators that allow a developer to specify different operations that are to be performed upon the occurrence of different events. In some embodiments, nLog provides a limited subset of the operators that are provided by datalog in order to increase the operational speed of nLog. For instance, in some embodiments, nLog only allows the AND operator to be used in any of the declaratory rules.","The declaratory rules and operations that are specified through nLog are then compiled into a much larger set of rules by an nLog compiler. In some embodiments, this compiler translates each rule that is meant to address an event into several sets of database join operations. Collectively the larger set of rules forms the table mapping, rules engine that is referred to below as the nLog engine. The nLog mapping techniques of some embodiments is further described below.","In some embodiments, the nLog virtualization engine provides feedback (e.g., from one or more of the output tables or from NIB records that are updated to reflect values stored in the output tables) to the user in order to provide the user with state information about the logical data path set that he or she created. In this manner, the updates that the user gets are expressed in terms of the logical space that the user understands and not in terms of the underlying switching element states, which is the user does not understand.","The use of nLog serves as a significant distinction between Type I virtualized control systems and Type II virtualized control systems, even for Type II systems that store user specified logical data path sets in the NIB. This is because nLog provides a machine-generated rules engine that addresses the mapping between the logical and physical domains in a more robust, comprehensive manner than the hand-coded approach used for Type I virtualized control systems. In the Type I control systems, the application developers need to have a detailed understanding of the NIB structure and need to use this detailed understanding to write code that addresses all possible conditions that the control system would encounter at runtime. On the other hand, in Type II control systems, the application developers only need to produce applications that express the user-specified logical data path sets in terms of one or more tables, which are then automatically mapped to output tables whose content are in turn transferred to the NIB. This approach allows the Type II virtualized systems not to maintain the data regarding the logical data path sets in the NIB. However, some embodiments maintain this data in the NIB in order to distribute this data among other NOS instances, as further described below.","C. Edge and Non-Edge Switch Controls","As mentioned above, the NIB in some embodiments stores data regarding each switching element within the network infrastructure of a system, while in other embodiments, the NIB stores state information about only switching elements at the edge of a network infrastructure.  illustrate an example that differentiates the two differing approaches. Specifically,  illustrates the switch infrastructure of a multi-tenant server hosting system. In this system, six switching elements are employed to interconnect six computing devices of two users A and B. Four of these switches - are edge switches that have direct connections with the computing devices - of the users A and B, while two of the switches  and  are interior switches (i.e., non-edge switches) that interconnect the edge switches and connect to each other.",{"@attributes":{"id":"p-0119","num":"0118"},"figref":["FIG. 7","FIG. 7"],"b":["700","605","620","700","110","115","605","620","705","110","605","620","610","645","615","650","605","620","610","615"]},"The system of some embodiments only controls edge switches (i.e., only maintains data in the NIB regarding edge switches) for several reasons. Controlling edge switches provides the system with a sufficient mechanism for maintaining isolation between computing devices, which is needed, as opposed to maintaining isolation between all switch elements, which is not needed. The interior switches forward data packets between switching elements. The edge switches forward data packets between computing devices and other network elements (e.g., other switching elements). Thus, the system can maintain user isolation simply by controlling the edge switch because the edge switch is the last switch in line to forward packets to a host.","Controlling only edge switches also allows the system to be deployed independent of concerns about the hardware vendor of the non-edge switches, because deploying at the edge allows the edge switches to treat the internal nodes of the network as simply a collection of elements that moves packets without considering the hardware makeup of these internal nodes. Also, controlling only edge switches makes distributing switching logic computationally easier. Controlling only edge switches also enables non-disruptive deployment of the system because edge-switching solutions can be added as top of rack switches without disrupting the configuration of the non-edge switches.","In addition to controlling edge switches, the network control system of some embodiments also utilizes and controls non-edge switches that are inserted in the switch network hierarchy to simplify and\/or facilitate the operation of the controlled edge switches. For instance, in some embodiments, the control system requires the switches that it controls to be interconnected in a hierarchical switching architecture that has several edge switches as the leaf nodes and one or more non-edge switches as the non-leaf nodes. In some such embodiments, each edge switch connects to one or more of the non-leaf switches, and uses such non-leaf switches to facilitate its communication with other edge switches. Examples of functions that a non-leaf switch of some embodiments may provide to facilitate such communications between edge switch in some embodiments include (1) routing of a packet with an unknown destination address (e.g., unknown MAC address) to the non-leaf switch so that this switch can route this packet to the appropriate edge switch, (2) routing a multicast or broadcast packet to the non-leaf switch so that this switch can convert this packet to a series of unicast packets to the desired destinations, (3) bridging remote managed networks that are separated by one or more networks, and (4) bridging a managed network with an unmanaged network.","Some embodiments employ one level of non-leaf (non-edge) switches that connect to edge switches and in some cases to other non-leaf switches. Other embodiments, on the other hand, employ multiple levels of non-leaf switches, with each level of non-leaf switch after the first level serving as a mechanism to facilitate communication between lower level non-leaf switches and leaf switches. In some embodiments, the non-leaf switches are software switches that are implemented by storing the switching tables in the memory of a standalone computer instead of an off the shelf switch. In some embodiments, the standalone computer may also be executing in some cases a hypervisor and one or more virtual machines on top of that hypervisor. Irrespective of the manner by which the leaf and non-leaf switches are implemented, the NIB of the control system of some embodiments stores switching state information regarding the leaf and non-leaf switches.","The above discussion relates to the control of edge switches and non-edge switches by a network control system of some embodiments. In some embodiments, edge switches and non-edge switches (leaf and non-leaf nodes) may be referred to as managed switches. This is because these switches are managed by the network control system (as opposed to unmanaged switches, which are not managed by the network control system, in the network) in order to implement logical data path sets through the managed switches.","D. Secondary Storage Structure","In addition to using the NIB to store switching-element data, the virtualized network-control system of some embodiments also stores other storage structures to store data regarding the switching elements of the network. These other storage structures are secondary storage structures that supplement the storage functions of the NIB, which is the primary storage structure of the system while the system operates. In some embodiments, the primary purpose for one or more of the secondary storage structures is to back up the data in the NIB. In these or other embodiments, one or more of the secondary storage structures serves a purpose other than backing up the data in the NIB (e.g., for storing data that are not in the NIB).","In some embodiments, the NIB is stored in system memory (e.g., RAM) while the system operates. This allows for the fast access of the NIB records. In some embodiments, one or more of the secondary storage structures, on the other hand, are stored on disk or other non-volatile memories that are slower to access. Such non-volatile disk or other storages, however, improve the resiliency of the system as they allow the data to be stored in a persistent manner.",{"@attributes":{"id":"p-0128","num":"0127"},"figref":["FIG. 8","FIGS. 4 and 5"],"b":["800","400","500","805","810","815","820"]},"In some embodiments, the PTD  is a database that is stored on disk or other non-volatile memory. In some embodiments, the PTD is a commonly available database, such as MySQL or SQLite. The PTD of some embodiments can handle complex transactional queries. As a transactional database, the PTD can undo a series of prior query operations that it has performed as part of a transaction when one of the subsequent query operations of the transaction fails. Moreover, some embodiments define a transactional guard processing (TGP) layer before the PTD in order to allow the PTD to execute conditional sets of database transactions. The TGP layer allows the PTD to avoid unnecessary later database operations when conditions of earlier operations are not met.","The PTD in some embodiments stores the exact replica of the data that are stored in the NIB, while in other embodiments it stores only a subset of the data that are stored in the NIB. Some or all of the data in the NIB are stored in the PTD in order to ensure that the NIB data will not be lost in the event of a crash of the NOS or the NIB.","The PNTD  is another persistent database that is stored on disk or other non-volatile memory. Some embodiments use this database to store data (e.g., statistics, computations, etc.) regarding one or more switch element attributes or operations. For instance, this database is used in some embodiment to store the number of packets routed through a particular port of a particular switching element. Other examples of types of data stored in the database  include error messages, log files, warning messages, and billing data. Also, in some embodiments, the PNTD stores the results of operations performed by the application(s)  running on top of the NOS, while the PTD and hash table store only values generated by the NOS.","The PNTD in some embodiments has a database query manager that can process database queries, but as it is not a transactional database, this query manager cannot handle complex conditional transactional queries. In some embodiments, accesses to the PNTD are faster than accesses to the PTD but slower than accesses to the hash table .","Unlike the databases  and , the hash table  is not a database that is stored on disk or other non-volatile memory. Instead, it is a storage structure that is stored in volatile system memory (e.g., RAM). It uses hashing techniques that use hashed indices to quickly identify records that are stored in the table. This structure combined with the hash table's placement in the system memory allows this table to be accessed very quickly. To facilitate this quick access, a simplified query interface is used in some embodiments. For instance, in some embodiments, the hash table has just two queries: a Put query for writing values to the table and a Get query for retrieving values from the table. Some embodiments use the hash table to store data that change quickly. Examples of such quick-changing data include network entity status, statistics, state, uptime, link arrangement, and packet handling information. Furthermore, in some embodiments, the NOS uses the hash tables as a cache to store information that is repeatedly queried for, such as flow entries that will be written to multiple nodes. Some embodiments employ a hash structure in the NIB in order to quickly access records in the NIB. Accordingly, in some of these embodiments, the hash table  is part of the NIB data structure.","The PTD and the PNTD improve the resiliency of the NOS system by preserving network data on hard disks. If a NOS system fails, network configuration data will be preserved on disk in the PTD and log file information will be preserved on disk in the PNTD.","E. Multi-Instance Control System","Using a single NOS instance to control a network can lead to scaling and reliability issues. As the number of network elements increases, the processing power and\/or memory capacity that are required by those elements will saturate a single node. Some embodiments further improve the resiliency of the control system by having multiple instances of NOS running on one or more computers, with each instance of NOS containing one or more of the secondary storage structures described above. The control applications in some embodiments partition the workload between the different instances in order to reduce each instance's workload. Also, in some embodiments, the multiple instances of NOS communicate the information stored in their storage layers to enable each instance of NOS to cover for the others in the event of a NOS instance failing.",{"@attributes":{"id":"p-0137","num":"0136"},"figref":"FIG. 9","b":["900","990","905","910","915","900"]},"As shown in , each instance includes a NOS , a virtualization application , one or more control applications , and a coordination manager (CM) . For the embodiments illustrated in this figure, each NOS in the system  is shown to include a NIB  and three secondary storage structures, i.e., a PTD , a distributed hash table (DHT) instance , and a persistent non-transaction database (PNTD) . Other embodiments may not tightly couple the NIB and\/or each of the secondary storage structures within the NOS. Also, other embodiments might not include each of the three secondary storage structures (i.e., the PTD, DHT instance, and PNTD) in each instance , , or . For example, one NOS instance  may have all three data structures whereas another NOS instance may only have the DHT instance.","In some embodiments, the system  maintains the same switch element data records in the NIB of each instance, while in other embodiments, the system  allows NIBs of different instances to store different sets of switch element data records.  illustrate three different approaches that different embodiments employ to maintain the NIB records. In each of these three examples, two instances  and  are used to manage several switching elements having numerous attributes that are stored collectively in the NIB instances. This collection of the switch element data in the NIB instances is referred to as the global NIB data structure  in .",{"@attributes":{"id":"p-0140","num":"0139"},"figref":["FIG. 10","FIG. 11","FIG. 12"],"b":["1015","1005","1010","1015","1020","1025","1020","1005","1025","1010","1015","1030","1035","1005","1030","1010","1035"]},"The system  of some embodiments also replicates each NIB record in each instance in the PTD  of that instance in order to maintain the records of the NIB in a persistent manner. Also, in some embodiments, the system  replicates each NIB record in the PTDs of all the controller instances , , or , in order to protect against failures of individual controller instances (e.g., of an entire controller instance or a portion of the controller instance). Other embodiments, however, do not replicate each NIB record in each PTD and\/or do not replicate the PTD records across all the PTDs. For instance, some embodiments only replicate a part but not all of the NIB data records of one controller instance in the PTD storage layer of that controller instance, and then replicate only this replicated portion of the NIB in all of the NIBs and PTDs of all other controller instances. Some embodiments also store a subset of the NIB records in another one of the secondary storage records, such as the DHT instance .","In some embodiments, the DHT instances (DHTI)  of all controller instances collectively store one set of records that are indexed based on hashed indices for quick access. These records are distributed across the different controller instances to minimize the size of the records within each instance and to allow the size of the DHT to be increased by adding additional DHT instances. According to this scheme, one DHT record is not stored in each controller instance. In fact, in some embodiments, each DHT record is stored in at most one controller instance. To improve the system's resiliency, some embodiments, however, allow one DHT record to be stored in more than one controller instance, so that in case one DHT record is no longer accessible because of one instance failure, that DHT record can be accessed from another instance. Some embodiments store in the DHT only the type of data that can be quickly re-generated, and therefore do not allow for replication of records across different DHT instances or allow only a small amount of such records to be replicated.","The PNTD  is another distributed data structure of the system  of some embodiments. For example, in some embodiments, each instance's PNTD stores the records generated by the NOS  or applications  or  of that instance or another instance. Each instance's PNTD records can be locally accessed or remotely accessed by other controller instances whenever the controller instances need these records. This distributed nature of the PNTD allows the PNTD to be scalable as additional controller instances are added to the control system . In other words, addition of other controller instances increases the overall size of the PNTD storage layer.","The PNTD in some embodiments is replicated partially across different instances. In other embodiments, the PNTD is replicated fully across different instances. Also, in some embodiments, the PNTD  within each instance is accessible only by the application(s) that run on top of the NOS of that instance. In other embodiments, the NOS can also access (e.g., read and\/or write) to the PNTD . In yet other embodiments, the PNTD  of one instance is only accessible by the NOS of that instance.","By allowing different NOS instances to store the same or overlapping NIB records, and\/or secondary storage structure records, the system improves its overall resiliency by guarding against the loss of data due to the failure of any NOS or secondary storage structure instance. In some embodiments, each of the three storages of the secondary storage layer uses a different distribution technique to improve the resiliency of a multiple NOS instance system. For instance, as mentioned above, the system  of some embodiments replicates the PTD across NOS instances so that every NOS has a full copy of the PTD to enable a failed NOS instance to quickly reload its PTD from another instance. In some embodiments, the system  distributes the PNTD with overlapping distributions of data across the NOS instances to reduce the damage of a failure. The system  in some embodiments also distributes the DHT fully or with minimal overlap across multiple controller instances in order to maintain the DHT instance within each instance small and to allow the size of the DHT to be increased by adding additional DHT instances.","For some or all of the communications between the distributed instances, the system  uses the CMs . The CM  in each instance allows the instance to coordinate certain activities with the other instances. Different embodiments use the CM to coordinate the different sets of activities between the instances. Examples of such activities include writing to the NIB, writing to the PTD, writing to the DHT, controlling the switching elements, facilitating intra-controller communication related to fault tolerance of controller instances, etc.","As mentioned above, different controller instances of the system  can control the operations of the same switching elements or of different switching elements. By distributing the control of these operations over several instances, the system can more easily scale up to handle additional switching elements. Specifically, the system can distribute the management of different switching elements and\/or different portions of the NIB to different NOS instances in order to enjoy the benefit of efficiencies that can be realized by using multiple NOS instances. In such a distributed system, each NOS instance can have a reduced number of switches or reduce portion of the NIB under management, thereby reducing the number of computations each controller needs to perform to distribute flow entries across the switches and\/or to manage the NIB. In other embodiments, the use of multiple NOS instances enables the creation of a scale-out network management system. The computation of how best to distribute network flow tables in large networks is a CPU intensive task. By splitting the processing over NOS instances, the system  can use a set of more numerous but less powerful computer systems to create a scale-out network management system capable of handling large networks.","To distribute the workload and to avoid conflicting operations from different controller instances, the system  of some embodiments designates one controller instance (e.g., ) within the system  as the master of any particular NIB portion and\/or any given switching element (e.g., ). Even with one master controller, different controller instance (e.g.,  and ) can request changes to different NIB portions and\/or to different switching elements (e.g., ) controlled by the master (e.g., ). If allowed, the master instance then effectuates this change and writes to the desired NIB portion and\/or switching element. Otherwise, the master rejects the request.",{"@attributes":{"id":"p-0149","num":"0148"},"figref":["FIG. 13","FIG. 9"],"b":["1300","900","1305","1310","1","2","3","1315","1320","1325","1330","1355","1360","1305","1310","1345","1350"]},"In the example illustrated in , both control applications  and  of both controllers  and  can modify records of the switching element S for both users A and B, but only controller  is the master of this switching element. This example illustrates two different scenarios. The first scenario involves the controller  updating the record S in switching element S for the user B. The second scenario involves the controller  updating the records S in switching element S after the control application  updates a NIB record S for switching element S and user A in NIB . In the example illustrated in , this update is routed from NIB  of the controller  to the NIB  of the controller , and subsequently routed to switching element S.","Different embodiments use different techniques to propagate changes to the NIB  of controller instance  to the NIB  of the controller instance . For instance, to propagate changes, the system  in some embodiments uses the secondary storage structures (not shown) of the controller instances  and . More generally, the distributed control system of some embodiments uses the secondary storage structures as communication channels between the different controller instances. Because of the differing properties of the secondary storage structures, these structures provide the controller instances with different mechanisms for communicating with each other. For instance, in some embodiments, different DHT instances can be different, and each DHT instance is used as a bulletin board for one or more instances to store data so that they or other instances can retrieve this data later. In some of these embodiments, the PTDs are replicated across all instances, and some or all of the NIB changes are pushed from one controller instance to another through the PTD storage layer. Accordingly, in the example illustrated in , the change to the NIB  could be replicated to the PTD of the controller , and from there it could be replicated in the PTD of the controller  and the NIB .","Instead of propagating the NIB changes through the secondary storages, the system  of some embodiments uses other techniques to change the record S in the switch S in response to the request from control application . For instance, to propagate this update, the NOS  of the controller  in some embodiments sends an update command to the NOS  of the controller  (with the requisite NIB update parameters that identify the record and one or more new values for the record) to direct the NOS  to modify the record in the NIB  or in the switch S. In response, the NOS  would make the changes to the NIB  and the switch S (if such a change is allowed). After this change, the controller instance  would change the corresponding record in its NIB  once it receives notification (from controller  or through another notification mechanism) that the record in the NIB  and\/or switch S has changed.","Other variations to the sequence of operations shown in  could exist because some embodiments designate one controller instance as a master of a portion of the NIB, in addition to designating a controller instance as a master of a switching element. In some embodiments, different controller instances can be masters of a switch and a corresponding record for that switch in the NIB, while other embodiments require the controller instance to be master of the switch and all records for that switch in the NIB.","In the embodiments where the system  allows for the designation of masters for switching elements and NIB records, the example illustrated in  illustrates a case where the controller instance  is the master of the NIB record S, while the controller instance  is the master for the switch S. If a controller instance other than the controller instance  and  was the master of the NIB record S, then the request for the NIB record modification from the control application  would have had to be propagated to this other controller instance. This other controller instance would then modify the NIB record and this modification would then cause the NIB , the NIB  and the switch S to update their records once the controller instances  and  are notified of this modification through any number of mechanisms that would propagate this modification to the controller instances  and .","In other embodiments, the controller instance  might be the master of the NIB record S, or the controller instance might be the master of switch S and all the records of its NIB. In these embodiments, the request for the NIB record modification from the control application  would have to be propagated to the controller instance , which would then modify the records in the NIB  and the switch S. Once this modification is made, the NIB  would modify its record S once the controller instance  is notified of this modification through any number of mechanisms that would propagate this modification to the controller instance .","As mentioned above, different embodiments employ different techniques to facilitate communication between different controller instances. In addition, different embodiments implement the controller instances differently. For instance, in some embodiments, the stack of the control application(s) (e.g.,  or  in ), the virtualization application (e.g.,  or ), and the NOS (e.g.,  or ) is installed and runs on a single computer. Also, in some embodiments, multiple controller instances can be installed and run in parallel on a single computer. In some embodiments, a controller instance can also have its stack of components divided amongst several computers. For example, within one instance, the control application (e.g.,  or ) can be on a first physical or virtual computer, the virtualization application (e.g.,  or ) can be on a second physical or virtual computer, and the NOS (e.g.,  or ) can be on a third physical or virtual computer.",{"@attributes":{"id":"p-0157","num":"0156"},"figref":["FIG. 14","FIG. 9","FIG. 14"],"b":["1400","1400","900","1405","1410","1415","1400"]},"Also, like the control system , each controller instance includes a NOS , a virtualization application , one or more control applications , and a coordination manager (CM) . Each NOS in the system  includes a NIB  and at least two secondary storage structures, e.g., a distributed hash table (DHT)  and a PNTD .","However, as illustrated in , the control system  has several additional and\/or different features than the control system . These features include a NIB notification module , NIB transfer modules , a CM interface , PTD triggers , DHT triggers , and master\/slave PTDs \/.","In some embodiments, the notification module  in each controller instance allows applications (e.g., a control application) that run on top of the NOS to register for callbacks when changes occur within the NIB. This module in some embodiments has two components, which include a notification processor and a notification registry. The notification registry stores the list of applications that need to be notified for each NIB record that the module  tracks, while the notification processor reviews the registry and processes the notifications upon detecting a change in a NIB record that it tracks. The notification module as well as its notification registry and notification processor are a conceptual representation of the NIB-application layer notification components of some embodiments, as the system of these embodiments provides a separate notification function and registry within each NIB object that can be tracked by the application layer.","The transfer modules  include one or more modules that allow data to be exchanged between the NIB  on one hand, and the PTD or DHT storage layers in each controller instance on the other hand. In some embodiments, the transfer modules  include an import module for importing changes from the PTD\/DHT storage layers into the NIB, and an export module for exporting changes in the NIB to the PTD\/DHT storage layers.","Unlike the control system  that has the same type of PTD in each instance, the control system  only has PTDs in some of the NOS instances, and of these PTDs, one of them serves as master PTD , while the rest serve as slave PTDs . In some embodiments, NIB changes within a controller instance that has a slave PTD are first propagated to the master PTD , which then direct the controller instance's slave PTD to record the NIB change. The master PTD  similarly receives NIB changes from controller instances that do not have either master or slave PTDs.","In the control system , the coordination manager  includes the CM interface  to facilitate communication between the NIB storage layer and the PTD storage layer. The CM interface also maintains the PTD trigger list , which identifies the modules of the system  to callback whenever the CM interface  is notified of a PTD record change. A similar trigger list  for handling DHT callbacks is maintained by the DHT instance . The CM  also has a DHT range identifier (not shown) that allows the DHT instances of different controller instances to store different DHT records in different DHT instances.","Also, in the control system , the PNTD is not placed underneath the NIB storage layer. This placement is to signify that the PNTD in the control system  does not exchange data directly with the NIB storage layer, but rather is accessible solely by the application(s) (e.g., the control application) running on top of the NOS  as well as other applications of other controller instances. This placement is in contrast to the placement of the PTD storage layer \/ and DHT storage layers , which are shown to be underneath the NIB storage layer because the PTD and DHT are not directly accessible by the application(s) running on top of the NOS . Rather, in the control system , data are exchanged between the NIB storage layer and the PTD\/DHT storage layers of the same or different instances.","The control system  uses the PTD, DHT and PNTD storage layers to facilitate communication between the different controller instances. In some embodiments, each of the three storages of the secondary storage layer uses a different storage and distribution technique to improve the resiliency of the distributed, multi-instance system . For instance, the system  of some embodiments replicates the PTD across NOS instances so that every NOS has a full copy of the PTD to enable a failed NOS instance to quickly reload its PTD from another instance. On the other hand, the system  in some embodiments distributes the PNTD with partial overlapping distributions of data across the NOS instances to reduce the damage of a failure. Similarly, the system  in some embodiments distributes the DHT fully or with minimal overlap across multiple controller instances in order to maintain the DHT instance within each instance small. Also, using this approach, allows the system to increase the size of the DHT by adding additional DHT instances in order to make the system more scalable.","One of the advantages of this system is that it can be configured in any number of ways. In some embodiments, this system provides great flexibility to specify the configurations for the components of the system in order to customize its storage and data distribution scheme to achieve the best tradeoff of scalability and speed on one hand, and reliability and consistency on the other hand. Attributes of the storage structures that affect scalability, speed, reliability and consistency considerations include the speed of the storage (e.g., RAM versus disk access speed), the reliability of the storage (e.g., persistent non-volatile storage of disk versus volatile storage of RAM), the query interface of the storage (e.g., simple Put\/Get query interface of DHT versus more robust transactional database queries of PTD in some embodiments), and the number of points of failures in the system (e.g., a single point of failure for a DHT record versus multiple points of failure for a PTD record in some embodiments).","Through the configurations of its components, the system can be configured (1) on how to distribute the data records between the NIB and the secondary storage structures within one instance (e.g., which secondary storage should store which NIB record), (2) on how to distribute the data records between the NIBs of different instances (e.g., which NIB records should be replicated across different controller instances), (3) on how to distribute the data records between the secondary storage structures within one instance (e.g., which secondary storage records contain which records), (4) on how to distribute the data records between the secondary storage structures of different instances (e.g., which secondary storage records are replicated across different controller instances), (5) on how to distribute secondary storage instances across controller instances (e.g., whether to put a PTD, a DHT, or a Stats database instances within each controller or whether to put different subset of these storages within different instances), and (6) on how to replicate data records in the distributed secondary storage structures (e.g., whether to replicated PTD fully across all instances, whether to replicate some or all DHT records across more than one instance, etc.). The system also allows the coordination between the different controller instances as to the master control over different switching elements or different portions of the NIB to be configured differently. In some embodiments, some or all of these configurations can be specified by applications (e.g., a control application or a virtualization application) that run on top of the NOS.","In some embodiments, as noted above, the CMs facilitate intra-controller communication related to fault tolerance of controller instances. For instance, the CMs implement the intra-controller communication through the secondary storage layers described above. A controller instance in the control system may fail due to any number of reasons. (e.g., hardware failure, software failure, network failure, etc.). Different embodiments may use different techniques for determining whether a controller instance has failed. In some embodiments, Paxos protocol is used to determine whether a controller instance in the control system has failed. While some of these embodiments may use Apache Zookeeper to implement the Paxos protocols, other embodiments may implement Paxos protocol in other ways.","Some embodiments of the CM  may utilize defined timeouts to determine whether a controller instance has failed. For instance, if a CM of a controller instance does not respond to a communication (e.g., sent from another CM of another controller instance in the control system) within an amount of time (i.e., a defined timeout amount), the non-responsive controller instance is determined to have failed. Other techniques may be utilized to determine whether a controller instance has failed in other embodiments.","When a master controller instance fails, a new master for the logical data path sets and the switching elements needs to be determined. Some embodiments of the CM  make such determination by performing a master election process that elects a master controller instance (e.g., for partitioning management of logical data path sets and\/or partitioning management of switching elements). The CM  of some embodiments may perform a master election process for electing a new master controller instance for both the logical data path sets and the switching elements of which the failed controller instance was a master. However, the CM  of other embodiments may perform (1) a master election process for electing a new master controller instance for the logical data path sets of which the failed controller instance was a master and (2) another master election process for electing a new master controller instance for the switching elements of which the failed controller instance was a master. In these cases, the CM  may determine two different controller instances as new controller instances: one for the logical data path sets of which the failed controller instance was a master and another for the switching elements of which the failed controller instance was a master.","In some embodiments, the master election process is further for partitioning management of logical data path sets and\/or management of switching elements when a controller instance is added to the control system. In particular, some embodiments of the CM  perform the master election process when the control system  detects a change in membership of the controller instances in the control system . For instance, the CM  may perform the master election process to redistribute a portion of the management of the logical data path sets and\/or the management of the switching elements from the existing controller instances to the new controller instance when the control system  detects that a new network controller has been added to the control system . However, in other embodiments, redistribution of a portion of the management of the logical data path sets and\/or the management of the switching elements from the existing controller instances to the new controller instance does not occur when the control system  detects that a new network controller has been added to the control system . Instead, the control system  in these embodiments assigns unassigned logical data path sets and\/or switching elements (e.g., new logical data path sets and\/or switching elements or logical data path sets and\/or switching elements from a failed network controller) to the new controller instance when the control system  detects the unassigned logical data path sets and\/or switching elements.","II. Single NOS Instance",{"@attributes":{"id":"p-0172","num":"0171"},"figref":"FIG. 15","b":["1500","1400","1500"]},"Also, in some embodiments, the NOS instance  provides multiple methods for applications to gain access to network entities. For instance, in some embodiments, it maintains an index of all of its entities based on the entity identifier, allowing for direct querying of a specific entity. The NOS instance of some embodiments also supports registration for notifications on state changes or the addition\/deletion of an entity. In some embodiments, the applications may further extend the querying capabilities by listening for notifications of entity arrival and maintaining their own indices. In some embodiments, the control for a typical application is fairly straightforward. It can register to be notified on some state change (e.g., the addition of new switches and ports), and once notified, it can manipulate the network state by modifying the NIB data tuple(s) (e.g., key-value pairs) of the affected entities.","As shown in , the NOS  includes an application interface , a notification processor , a notification registry , a NIB , a hash table , a NOS controller , a switch controller , transfer modules , a CM , a PTD , a CM interface , a PNTD , a DHT instance , switch interface , and NIB request list .","The application interface  is a conceptual illustration of the interface between the NOS and the applications (e.g., control and virtualization applications) that can run on top of the NOS. The interface  includes the NOS APIs that the applications (e.g., control or virtualization application) running on top of the NOS use to communicate with the NOS. In some embodiments, these communications include registrations for receiving notifications of certain changes in the NIB , queries to read certain NIB attributes, queries to write to certain NIB attributes, requests to create or destroy NIB entities, instructions for configuring the NOS instance (e.g., instructions regarding how to import or export state), requests to import or export entities on demand, and requests to synchronize NIB entities with switching elements or other NOS instances.","The switch interface  is a conceptual illustration of the interface between the NOS and the switching elements that run below the NOS instance . In some embodiments, the NOS accesses the switching elements by using the OpenFlow or OVS APIs provided by the switching elements. Accordingly, in some embodiments, the switch interface  includes the set of APIs provided by the OpenFlow and\/or OVS protocols.","The NIB  is the data storage structure that stores data regarding the switching elements that the NOS instance  is controlling. In some embodiments, the NIB just stores data attributes regarding these switching elements, while in other embodiments, the NIB also stores data attributes for the logical data path sets defined by the user. Also, in some embodiments, the NIB is a hierarchical object data structure (such as the ones described above) in which some or all of the NIB objects not only include data attributes (e.g., data tuples regarding the switching elements) but also include functions to perform certain functionalities of the NIB. For these embodiments, one or more of the NOS functionalities that are shown in modular form in  are conceptual representations of the functions performed by the NIB objects.","The hash table  is a table that stores a hash value for each NIB object and a reference to each NIB object. Specifically, each time an object is created in the NIB, the object's identifier is hashed to generate a hash value, and this hash value is stored in the hash table along with a reference (e.g., a pointer) to the object. The hash table  is used to quickly access an object in the NIB each time a data attribute or function of the object is requested (e.g., by an application or secondary storage). Upon receiving such requests, the NIB hashes the identifier of the requested object to generate a hash value, and then uses that hash value to quickly identify in the hash table a reference to the object in the NIB. In some cases, a request for a NIB object might not provide the identity of the NIB object but instead might be based on non-entity name keys (e.g., might be a request for all entities that have a particular port). For these cases, the NIB includes an iterator that iterates through all entities looking for the key specified in the request.","The notification processor  interacts with the application interface  to receive NIB notification registrations from applications running on top of the NOS and other modules of the NOS (e.g., such as an export module within the transfer modules ). Upon receiving these registrations, the notification processor  stores notification requests in the notification registry  that identifies each requesting party and the NIB data tuple(s) that the requesting party is tracking","As mentioned above, the system of some embodiments embeds in each NIB object a function for handling notification registrations for changes in the value(s) of that NIB object. For these embodiments, the notification processor  is a conceptual illustration of the amalgamation of all the NIB object notification functions. Other embodiments, however, do not provide notification functions in some or all of the NIB objects. The NOS of some of these embodiments therefore provides an actual separate module to serve as the notification processor for some or all of the NIB objects.","When some or all of the NIB objects have notification functions in some embodiments, the notification registry for such NIB objects are typically kept with the objects themselves. Accordingly, for some of these embodiments, the notification registry  is a conceptual illustration of the amalgamation of the different sets of registered requestors maintained by the NIB objects. Alternatively, when some or all of the NIB objects do not have notification functions and notification services are needed for these objects, some embodiments use a separate notification registry  for the notification processor  to use to keep track of the notification requests for such objects.","The notification process serves as only one manner for accessing the data in the NIB. Other mechanisms are needed in some embodiments for accessing the NIB. For instance, the secondary storage structures (e.g., the PTD  and the DHT instance ) also need to be able to import data from and export data to the NIB. For these operations, the NOS  uses the transfer modules  to exchange data between the NIB and the secondary storage structure.","In some embodiments, the transfer modules include a NIB import module and a NIB export module. These two modules in some embodiments are configured through the NOS controller , which processes configuration instructions that it receives through the interfaces  from the applications above the NOS. The NOS controller  also performs several other operations. As with the notification processor, some or all of the operations performed by the NOS controller are performed by one or more functions of NIB objects, in some of the embodiments that implement one or more of the NOS  operations through the NIB object functions. Accordingly, for these embodiments, the NOS controller  is a conceptual amalgamation of several NOS operations, some of which are performed by NIB object functions.","Other than configuration requests, the NOS controller  of some embodiments handles some of the other types of requests directed at the NOS instance . Examples of such other requests include queries to read certain NIB attributes, queries to write to certain NIB attributes, requests to create or destroy NIB entities, requests to import or export entities on demand, and requests to synchronize NIB entities with switching elements or other NOS instances.","In some embodiments, the NOS controller stores requests to change the NIB on the NIB request list . Like the notification registry, the NIB request list in some embodiments is a conceptual representation of a set of distributed requests that are stored in a distributed manner with the objects in the NIB. Alternatively, for embodiments in which some or all of the NIB objects do not maintain their modification requests locally, the request list is a separate list maintained by the NOS . The system of some of these embodiments that maintains the request list as a separate list, stores this list in the NIB in order to allow for its replication across the different controller instances through the PTD storage layer and\/or the DHT storage layer. This replication allows the distributed controller instances to process in a uniform manner a request that is received from an application operating on one of the controller instances.","Synchronization requests are used to maintain consistency in NIB data in some embodiments that employ multiple NIB instances in a distributed control system. For instance, the NIB of some embodiments provides a mechanism to request and release exclusive access to the NIB data structure of the local instance. As such, an application running on top of the NOS instance(s) is only assured that no other thread is updating the NIB within the same controller instance. The application therefore needs to implement mechanisms external to the NIB to coordinate an effort with other controller instances to control access to the NIB. In some embodiments, this coordination is static and requires control logic involvement during failure conditions.","Also, in some embodiments, all NIB operations are asynchronous, meaning that updating a network entity only guarantees that the update will eventually be pushed to the corresponding switching element and\/or other NOS instances. While this has the potential to simplify the application logic and make multiple modifications more efficient, often it is useful to know when an update has successfully completed. For instance, to minimize disruption to network traffic, the application logic of some embodiments requires the updating of forwarding state on multiple switches to happen in a particular order (to minimize, for example, packet drops). For this purpose, the API of some embodiments provides the synchronization request primitive that calls back one or more applications running on top of the NOS once the state has been pushed for an entity. After receiving the callback, the control application of some embodiments will then inspect the content of the NIB and determine whether its state is still as originally intended. Alternatively, in some embodiments, the control application can simply rely on NIB notifications to react to failures in modifications as they would react to any other network state changes.","The NOS controller  is also responsible for pushing the changes in its corresponding NIB to switching elements for which the NOS  is the master. To facilitate writing such data to the switching element, the NOS controller  uses the switch controller . It also uses the switch controller  to read values from a switching element. To access a switching element, the switch controller  uses the switch interface , which as mentioned above uses OpenFlow or OVS, or other known set of APIs in some embodiments.","Like the PTD and DHT storage structures  and  of the control system  of , the PTD and DHT storage structures  and  of  interface with the NIB and not the application layer. In other words, some embodiments only limit PTD and DHT layers to communicate between the NIB layer and these two storage layers, and to communicate between the PTD\/DHT storages of one instance and PTD\/DHT storages of other instances. Other embodiments, however, allow the application layer (e.g., the control application) within one instance to access the PTD and DHT storages directly or through the transfer modules . These embodiments might provide PTD and DHT access handles (e.g., APIs to DHT, PTD or CM interface) as part of the application interface , or might provide handles to the transfer modules that interact with the PTD layer (e.g., the CM interface ) and DHT layers, so that the applications can directly interact with the PTD and DHT storage layers.","Also, like structures  and , the PTD  and DHT instance  have corresponding lists of triggers that are respectively maintained in the CM interface  and the DHT instance . Also, like the PNTD  of the control system , the PNTD  of  does not interface with the NIB . Instead, it interfaces with the application layer through the application interface . Through this interface, the applications running on top of the NOS can store data in and retrieve data from the PNTD. Also, applications of other controller instances can access the PNTD , as shown in .","III. Control Data Pipeline",{"@attributes":{"id":"p-0191","num":"0190"},"figref":"FIG. 16","b":["1600","1625","1605","1610","1615","1605","1610","1610"]},"As shown in , the control application  in some embodiments has two logical planes  and  that can be used to express the input and output to this application. In some embodiments, the first logical plane  is a logical control plane that includes a collection of higher-level constructs that allow the control application and its users to specify one or more logical data path sets within the logical control plane for one or more users. The second logical plane  in some embodiments is the logical forwarding plane, which represents the logical data path sets of the users in a format that can be processed by the virtualization application . In this manner, the two logical planes  and  are virtualization space analogs of the control and forwarding planes  and  that are typically can be found in a typical managed switch , as shown in .","In some embodiments, the control application  defines and exposes the logical control plane constructs with which the application itself or users of the application define different logical data path sets within the logical control plane. For instance, in some embodiments, the logical control plane data  includes logical ACL data, etc. Some of this data (e.g., logical ACL data) can be specified by the user, while other such data (e.g., the logical L2 or L3 records) are generated by the control application and may not be specified by the user. In some embodiments, the control application  generates and\/or specifies such data in response to certain changes to the NIB (which indicate changes to the managed switches and the managed data path sets) that the control application  detects.","In some embodiments, the logical control plane data (i.e., the LDPS data that is expressed in terms of the control plane constructs) can be initially specified without consideration of current operational data from the managed switches and without consideration of the manner by which this control plane data will be translated to physical control plane data. For instance, the logical control plane data might specify control data for one logical switch that connects five computers, even though this control plane data might later be translated to physical control data for three managed switches that implement the desired switching between the five computers.","The control application includes a set of modules for converting any logical data path set within the logical control plane to a logical data path set in the logical forwarding plane . In some embodiments, the control application  uses the nLog table mapping engine to perform this conversion. The control application's use of the nLog table mapping engine to perform this conversion is further described below and is also further described in U.S. patent application Ser. No. 13\/177,532, entitled \u201cNetwork Control Apparatus and Method\u201d, filed concurrently with this application, now published as US2013\/0058339, which is incorporated by reference in this application. The control application also includes a set of modules for pushing the LDPS from the logical forwarding plane  of the control application  to a logical forwarding plane  of the virtualization application .","The logical forwarding plane  includes one or more logical data path sets of one or more users. The logical forwarding plane  in some embodiments includes logical forwarding data for one or more logical data path sets of one or more users. Some of this data is pushed to the logical forwarding plane  by the control application, while other such data are pushed to the logical forwarding plane by the virtualization application detecting events in the NIB  as further described below for some embodiments.","In addition to the logical forwarding plane , the virtualization application  includes the physical control plane . The physical control plane  includes one or more physical control path sets of one or more users. The virtualization application includes a set of modules for converting any LDPS within the logical forwarding plane  to a physical control data path set in the physical control plane . In some embodiments, the virtualization application  uses the nLog table mapping engine to perform this conversion. The virtualization application also includes a set of modules (not shown) for pushing the physical control plane data from the physical control plane  of the virtualization application  into the NIB  of the NOS .","From the NIB, the physical control plane data is later pushed into the managed switch , as shown in . As mentioned above, the physical control plane data in some instances of some embodiments is pushed to the managed switch by the NOS of the same controller instance that has the control application  and virtualization application , but in other instance is pushed to the managed switch by the NOS of another controller instance (not shown). The managed switch  then converts this physical control plane data to physical forwarding plane data that specifies the forwarding behavior of the managed switch.","In some embodiments, the physical control plane data that is propagated to the managed switch  allows this switch to perform the logical data processing on data packets that it processes in order to effectuate the processing of the logical data path sets specified by the control application. In some such embodiments, physical control planes include control plane data for operating in the physical domain and control plane data for operating in the logical domain. In other words, the physical control planes of these embodiments include control plane data for processing network data (e.g., packets) through managed switches to implement physical switching and control plane data for processing network data through managed switches in order to implement the logical switching. In this manner, the physical control plane facilitates implementing logical switches across managed switches. The use of the propagated physical control plane to implement logical data processing in the managed switches is further described in U.S. patent application Ser. No. 13\/177,535, entitled \u201cHierarchical Managed Switch Architecture,\u201d filed concurrently herewith, now published as US2013\/0058250. US2013\/0058250 is incorporated by reference in this application.","In addition to pushing physical control plane data to the NIB , the control and virtualization applications  and  also store logical control plane data and logical forwarding plane data in the NIB. These embodiments store such data in the NIB for a variety of reasons. For instance, in some embodiments, the NIB  serves as a medium for communications between different controller instances, and the storage of such data in the NOB facilitates the relaying of such data across different controller instances.",{"@attributes":{"id":"p-0201","num":"0200"},"figref":"FIG. 16","b":["1600","1625","1600","1605","1610","1605","1610","1605","1610"]},{"@attributes":{"id":"p-0202","num":"0201"},"figref":["FIG. 17","FIG. 16"],"b":["1605","1610","1615","1705","1710","1715"]},{"@attributes":{"id":"p-0203","num":"0202"},"figref":["FIG. 17","FIG. 17"],"b":["1770","1705"]},"Each logical switch has two logical planes  and  that can be used to express the input and output to the logical switch. In some embodiments, the logical plane  is a logical control plane (denoted by \u201cLCP\u201d in the figure) that includes a collection of higher-level constructs that allow the control application layer and its user to specify one or more logical data path sets within the logical control plane for the user. The second logical plane  in some embodiments is the logical forwarding plane (denoted by \u201cLFP\u201d in the figure), which represents the logical data path sets of the user in a format that can be processed by the virtualization application layer . Because of these two logical planes  and , the logical switches appear as virtualization space analogs of the control and forwarding planes  and  that typically can be found in managed switches, as shown in .","This figure then illustrates that through the virtualization application layer  and the NOS layer , the logical switches  can be implemented in three managed switches . The number of logical switches  may be less or more than three. That is, the number of logical switches  in some embodiments does not have to match to the number of managed switches that implement the logical switches. To implement the logical switches  in the three managed switches, the virtualization application layer  converts the logical forwarding plane data of the logical switches into physical control plane data, and the NOS layer  pushes this data to the managed switches . As mentioned above, the pushed physical control plane data allows the managed switches to perform physical switching operations in both the physical and logical data processing domains.","IV. Virtualization Application","As mentioned above, the virtualization application of some embodiments specifies the manner by which different LDPS' of different users of a network control system can be implemented by the switching elements managed by the network control system. In some embodiments, the virtualization application specifies the implementation of the LDPS' within the managed switching element infrastructure by performing conversion operations. These conversion operations convert the LDPS' data records (also called data tuples below) to the control data records (e.g., physical control plane data) that are initially stored within the managed switching elements and then used by the switching elements to produce forwarding plane data (e.g., flow entries) for defining forwarding behaviors of the switches. The conversion operations also produce other data (e.g., in tables) that specify network constructs (e.g., tunnels, queues, queue collections, etc.) that should be defined within and between the managed switching elements. As described in the above-mentioned U.S. patent application Ser. No. 13\/177,535, now published as US2013\/0058250, entitled \u201cHierarchical Managed Switch Architecture,\u201d the network constructs also include managed software switching elements that are dynamically deployed or pre-configured managed software switching elements that are dynamically added to the set of managed switching elements.",{"@attributes":{"id":"p-0207","num":"0206"},"figref":["FIG. 18","FIG. 18"],"b":["1800","1800","1805","1800"]},"At , the process  then performs a filtering operation to determine whether this instance of the virtualization application is responsible for the input event data. As described above, several instances of the virtualization application may operate in parallel to control multiple sets of logical data paths in some embodiments. In these embodiments, each virtualization application uses the filtering operation to filter out input data that does not relate to the virtualization application's logical data path set. To perform this filtering operation, the virtualization application of some embodiments includes a filter module. This module in some embodiments is a standalone module, while in other embodiments it is implemented by a table mapping engine (e.g., implemented by the join operations performed by the table mapping engine) that maps records between input tables and output tables of the virtualization application, as further described below.","Next, at , the process determines whether the filtering operation has filtered out the received input event data. The filtering operation filters out the input event data in some embodiments when the input event data does not fall within one of the logical data path sets that are the responsibility of the virtualization application. When the process determines (at ) that the filtering operation has filtered out the input event the process ends. Otherwise, the process  transitions to .","At , a converter of the virtualization application generates one or more sets of data tuples based on the received input event data. In some embodiments, the converter is a table mapping engine that performs a series of table mapping operations on the input event data to map the input event data to other data tuples. As mentioned above, this table mapping engine also performs the filtering operation in some embodiments. One example of such a table mapping engine is an nLog table-mapping engine which will be described bellow.","In some embodiments, the data tuples that the process  generates may include data (e.g., physical control plane data) that the process has to push down to the NIB. Accordingly, at , the process publishes to the NIB any data tuples that it has generated if such publication is necessary. After , the process ends.","The virtualization application in some embodiments performs its mapping operations by using the nLog table mapping engine, which, as described above, is a variation of the datalog table mapping technique. Datalog is used in the field of database management to map one set of tables to another set of tables. Datalog is not a suitable tool for performing table mapping operations in a virtualization application of a network control system as its current implementations are often slow. Accordingly, the nLog engine of some embodiments is custom designed to operate quickly so that it can perform the real time mapping of the LDPS data tuples to the data tuples of the managed switching elements. This custom design is based on several custom design choices. For instance, some embodiments compile the nLog table mapping engine from a set of high level declaratory rules that are expressed by an application developer (e.g., by a developer of a control application). In some of these embodiments, one custom design choice that is made for the nLog engine is to allow the application developer to use only the AND operator to express the declaratory rules. By preventing the developer from using other operators (such as ORs, XORs, etc.), these embodiments ensure that the resulting rules of the nLog engine are expressed in terms of AND operations that are faster to execute at run time.","Another custom design choice relates to the join operations performed by the nLog engine. Join operations are common database operations for creating association between records of different tables. In some embodiments, the nLog engine limits its join operations to inner join operations (also called as internal join operations) because performing outer join operations (also called as external join operations) can be time consuming and therefore impractical for real time operation of the engine.","Yet another custom design choice is to implement the nLog engine as a distributed table mapping engine that is executed by several different virtualization applications. Some embodiments implement the nLog engine in a distributed manner by partitioning management of logical data path sets. Each logical data path set includes logical data paths that are specified for a single user of the control system. Partitioning management of the logical data path sets involves specifying for each particular logical data path set only one controller instance as the instance responsible for specifying the NIB records associated with that particular logical data path set. For instance, when the control system uses three switching elements to specify five logical data path sets for five different users with two different controller instances, one controller instance can be the master for NIB records relating to two of the logical data path sets while the other controller instance can be the master for the NIB records for the other three logical data path sets. Partitioning management of logical data path sets ensures that conflicting values for the same logical data path sets are not written to the NIB by two different controller instances, and thereby alleviates the applications running on top of NOS from guarding against the writing of such conflicting values.","Partitioning management of the LDPS' also assigns in some embodiments the table mapping operations for each LDPS to the nLog engine of the controller instance responsible for the LDPS. The distribution of the nLog table mapping operations across several nLog instances reduces the load on each nLog instance and thereby increases the speed by which each nLog instance can complete its mapping operations. Also, this distribution reduces the memory size requirement on each machine that executes a controller instance. As further described below, some embodiments partition the nLog table mapping operations across the different instances by designating the first join operation that is performed by each nLog instance to be based on the LDPS parameter. This designation ensures that each nLog instance's join operations fail and terminate immediately when the instance has started a set of join operations that relate to a LDPS that is not managed by the nLog instance.","A more detailed example of the nLog mapping engine and the virtualization application is described in sub-sections A-E below. Sub-section A initially describes the software architecture of the virtualization application of some embodiments. Sub-section B then describes further the parallel, distributed management of the LDPS. Sub-section C next describes one manner for designing the nLog mapping engine. Sub-section D then describes the nLog engine's table mapping operations in response to an external event from the NIB or an internal event that is generated by the nLog engine. Lastly, sub-section E provides code-based examples that describe how a portion of the nLog engine is specified and how this portion later operates to perform its mapping operations.","A. Architecture",{"@attributes":{"id":"p-0218","num":"0217"},"figref":["FIG. 19","FIG. 14"],"b":["1900","1900","1430","1900","1905","1965","1960","1905","1905","1900","1960","1955","1965"]},"As shown in , the virtualization application  includes a set of rule-engine input tables , a set of function and constant tables , a query manager , a rules engine , a set of rule-engine output tables , a NIB monitor , a NIB publisher , and a compiler . The compiler  is one component of the application that operates at a different instance in time than the application's other components. The compiler operates when a developer needs to specify the rules engine for a particular control application and\/or virtualized environment, whereas the rest of the application's modules operate at run time when the application interfaces with the control application and the NOS to deploy and monitor logical data path sets specified by one or more users.","In some embodiments, the compiler  takes a relatively small set (e.g., few hundred lines) of declarative instructions  that are specified in a declarative language and converts these into a large set (e.g., thousands of lines) of code that specify the operation of the rules engine , which performs the application's table mapping as further described below. As such, the compiler greatly simplifies the virtualization application developer's process of defining and updating the virtualization application. This is because the compiler allows the developer to use a high level programming language that allows a compact definition of the virtualization application's complex mapping operation and to subsequently update this mapping operation in response to any number of changes (e.g., changes in the networking functions supported by the virtualization application, changes to desired behavior of the virtualization application, etc.).","In some embodiments, the rule-engine (RE) input tables  include tables with logical data and\/or switching configurations (e.g., access control list configurations, private virtual network configurations, port security configurations, etc.) specified by the user and\/or the virtualization application. They also include in some embodiments tables that contain physical data (i.e., non-logical data) from the switching elements managed by the virtualized control system. In some embodiments, such physical data includes data regarding the managed switching elements (e.g., physical control plane data) and other data regarding network configuration employed by the virtualized control system to deploy the different LDPS' of the different users.","The RE input tables  are partially populated by the LDPS data (e.g., by logical forwarding plane data) provided by the control application . The control application generates part of the LDPS data based on user input regarding the logical data path sets. It also generates part of the LDPS data by monitoring the NIB to identify changes in the managed switching element infrastructure that would require modification to the LDPS data. The control application's generation of LDPS data based on the monitoring of the NIB is further described below and further described in the above-mentioned U.S. patent application Ser. No. 13\/177,532, now published as US2013\/0058339, entitled \u201cNetwork Control Apparatus and Method.\u201d In addition to the control application , the NIB monitor  partially populates the RE input tables  with some or all of the data that the NIB monitor collects from the NIB . The operation of the NIB monitor will be further described below.","In addition to the RE input tables , the virtualization application  includes other miscellaneous tables  that the rules engine  uses to gather inputs for its table mapping operations. These tables  include constant tables that store defined values for constants that the rules engine  needs to perform its table mapping operations. For instance, constant tables  may include a constant \u201czero\u201d that is defined as the value 0, a constant \u201cdispatch_port_no\u201d as the value 4000, a constant \u201cbroadcast_MAC_addr\u201d as the value 0xFF:FF:FF:FF:FF:FF. (A dispatch port in some embodiments is a port that specifies that the managed switch should reprocess the packet based on another flow entry. Examples of such dispatch ports are provided in the above-mentioned U.S. patent application Ser. No. 13\/177,535, now published as US2013\/0058250, entitled \u201cHierarchical Managed Switch Architecture.\u201d)","When the rules engine  references constants, the corresponding value defined for the constants are actually retrieved and used. In addition, the values defined for constants in the constant table  may be modified and\/or updated. In this manner, the constant table  provides the ability to modify the value defined for constants that the rules engine  references without the need to rewrite or recompile code that specifies the operation of the rules engine .","The tables  further include function tables that store functions that the rules engine  needs to use to calculate values needed to populate the output tables . One example of such a function is a hash function that the rules engine uses to compute hash values for distributing DHT operations as well as load balancing traffic between lower level switches and higher level switches in a hierarchical switching architecture. U.S. patent application Ser. No. 13\/177,529, now published as US2013\/0058356, entitled \u201cMethod and Apparatus for Using a Network Information Base to Control a Plurality of Shared Network Infrastructure Switching Elements,\u201d and filed concurrently with the present application, describes the use of hash tables for distributing DHT operations, while the above-identified U.S. patent application Ser. No. 13\/177,535, now published as US2013\/0058250, entitled \u201cHierarchical Managed Switch Architecture,\u201d describes the use of hash tables to load balance traffic in a hierarchical switching architecture. U.S. patent application Ser. No. 13\/177,529, now published as US2013\/0058356, entitled \u201cMethod and Apparatus for Using a Network Information Base to Control a Plurality of Shared Network Infrastructure Switching Elements,\u201d and filed concurrently with the present application is incorporated herein by reference. U.S. patent application Ser. No. 13\/177,529, now published as US2013\/0058356, entitled \u201cMethod and Apparatus for Using a Network Information Base to Control a Plurality of Shared Network Infrastructure Switching Elements,\u201d also described the above-mentioned request list processing, which allows one control instance to request modifications to a LDPS managed by another controller instance.","The rules engine  performs table mapping operations that specify one manner for implementing the LDPS' within the managed switching element infrastructure. Whenever one of the RE input tables is modified, the rules engine performs a set of table mapping operations that may result in the modification of one or more data tuples in one or more RE output tables. The modification of the output table data tuples, in turn, may cause the NIB to be modified in order to establish and\/or modify the implementation of a particular user's LDPS in the managed switching element infrastructure.","As shown in , the rules engine  includes an event processor , several query plans , and a table processor . In some embodiments, each query plan is a set of join operations that are to be performed upon the occurrence of a modification to one of the RE input table. Such a modification is referred to below as an input table event. As further described below, each query plan is generated by the compiler  from one declaratory rule in the set of declarations . In some embodiments, the query plans are defined by using the nLog declaratory language.","In some embodiments, the compiler  does not just statically generate query plans but rather dynamically generates query plans based on performance data it gathers. The complier  in these embodiments generates an initial set of query plans and let the rules engine operate with the initial set of query plans. The virtualization application gathers the performance data or receives performance feedbacks (e.g., from the rules engine). Based on this data, the compiler is modified so that the virtualization application or a user of this application can have the modified compiler modify the query plans while the rules engine is not operating or during the operation of the rules engine.","For instance, the order of the join operations in a query plan may result in different execution times depending on the number of tables the rules engine has to select to perform each join operation. The compiler in these embodiments can be re-specified in order to re-order the join operations in a particular query plan when a certain order of the join operations in the particular query plan has resulted in a long execution time to perform the join operations.","The event processor  of the rules engine  detects the occurrence of each input table event. The event processor of different embodiments detects the occurrence of an input table event differently. In some embodiments, the event processor registers for callbacks with the RE input tables for notification of changes to the records of the RE input tables. In such embodiments, the event processor  detects an input table event when it receives notification from a RE input table that one of its records has changed.","In response to a detected input table event, the event processor  (1) selects the appropriate query plan for the detected table event, and (2) directs the table processor  to execute the query plan. To execute the query plan, the table processor  in some embodiments performs the join operations specified by the query plan to produce one or more records that represent one or more sets of data values from one or more input and miscellaneous tables  and . The table processor  of some embodiments then (1) performs a select operation to select a subset of the data values from the record(s) produced by the join operations, and (2) writes the selected subset of data values in one or more RE output tables .","In some embodiments, the RE output tables  store both logical and physical network element data attributes. The tables  are called RE output tables as they store the output of the table mapping operations of the rules engine . In some embodiments, the RE output tables can be grouped in several different categories. For instance, in some embodiments, these tables can be RE input tables and\/or virtualization-application (VA) output tables. A table is a RE input table when a change in the table causes the rules engine to detect an input event that requires the execution of a query plan. A RE output table  can also be a RE input table  that generates an event that causes the rules engine to perform another query plan after it is modified by the rules engine. Such an event is referred to as an internal input event, and it is to be contrasted with an external input event, which is an event that is caused by a RE input table modification made by the control application  or the NIB monitor .","A table is a virtualization-application output table when a change in the table causes the NIB publisher  to publish a change to the NIB , as further described below. As shown in , a table in the RE output tables  can be a RE input table , a VA output table , or both a RE input table  and a VA output table .","The NIB publisher  detects changes to the VA output tables  of the RE output tables . The NIB publisher of different embodiments detects the occurrence of a VA output table event differently. In some embodiments, the NIB publisher registers for callbacks with the VA output tables for notification of changes to the records of the VA output tables. In such embodiments, the NIB publisher  detects an output table event when it receives notification from a VA output table that one of its records has changed.","In response to a detected output table event, the NIB publisher  takes each modified data tuple in the modified VA output tables and propagates this modified data tuple into the NIB  through the APIs provided by the NOS . After a new data tuple is propagated to the NIB by the NIB publisher , the NOS  propagates, if needed, a NIB data tuple that was modified because of the propagated VA output table data tuple to one or more of the managed switching elements. In doing this, the NOS completes the deployment of the LDPS (e.g., one or more logical switching configurations) to one or more managed switching elements as specified by the NIB records.","As the VA output tables store both logical and physical network element data attributes in some embodiments, the NIB  in some embodiments stores both logical and physical network element attributes that are identical or derived from the logical and physical network element data attributes in the output tables . In other embodiments, however, the NIB only stores physical network element attributes that are identical or derived from the physical network element data attributes in the output tables .","The NIB monitor  interfaces with the NIB  to receive notifications regarding changes to the NIB. The NIB monitor of different embodiments detects the occurrence of a change in the NIB differently. In some embodiments, the NIB monitor registers for callbacks with the NIB for notification of changes to one or more records in the NIB. In such embodiments, the NIB monitor  detects NIB change event when it receives notification from the NIB that one of its records has changed. In response to a detected NIB change event, the NIB monitor  may modify one or more RE input tables , which, in turn, may cause one or more RE input table event to occur that then initiates the execution of one or more query plans by the rules engine. In other words, the NIB monitor writes some or all of the information that it receives from the NIB into the input tables , so that the state and configuration of the managed switching elements can be accounted for while generating the NIB data tuples through the mapping operations. Each time the managed switching configuration or underlying managed switching element state changes, the NIB monitor  may update the input table records  so that the generated NIB data tuples can be updated to reflect the modified switching configuration or underlying switching element state.","In some embodiments, the NIB monitor  is a collection of input objects (or functions) associated with the RE input tables. Each input object in some embodiments is associated with one RE input table and is responsible for modifying its associated input table in response to a change in the NIB. Each input object in some embodiments registers with one or more NIB objects for callback notifications upon the occurrence of changes to the NIB object(s). Similarly, in some embodiments, the NIB publisher  is a collection of output objects (or functions) associated with the VA output tables. Each output object in some embodiments is associated with one VA output table and is responsible for propagating changes in its associated output table to the NIB. As such, in some embodiments, the NIB monitor is a conceptual representation of the input and output objects that register with the NIB for callbacks.","The query manager  interfaces with the control application  to receive queries regarding LDPS data. As shown in , the manager  of some embodiments also interfaces with the NIB  in order to query the NIB to provide the control application state information regarding the network elements in the LDPS' for the different user. In other embodiments, however, the query manager  queries the output tables  to obtain LDPS data for the control application.","B. Designing the nLog Table Mapping Engine","In some embodiments, the virtualization application  uses a variation of the datalog database language, called nLog, to create the table mapping engine that maps input tables containing logical data path data and switching element attributes to the output tables. Like datalog, nLog provides a few declaratory rules and operators that allow a developer to specify different operations that are to be performed upon the occurrence of different events. In some embodiments, nLog provides a smaller subset of the operators that are provided by datalog in order to increase the operational speed of nLog. For instance, in some embodiments, nLog only allows the AND operator to be used in any of the declaratory rules.","The declaratory rules and operations that are specified through nLog are then compiled into a much larger set of rules by an nLog compiler. In some embodiments, this compiler translates each rule that is meant to respond to an event into several sets of database join operations. Collectively the larger set of rules forms the table mapping, rules engine that is referred to below as the nLog engine.",{"@attributes":{"id":"p-0243","num":"0242"},"figref":"FIG. 21","b":["2100","1925","1900","2105","2110","2105","1905","1900"]},"One example  of such a rule is illustrated in . This example is a multi-conditional rule that specifies that an Action X has to be taken if four conditions A, B, C, and D are true. The expression of each condition as true in this example is not meant to convey that all embodiments express each condition for each rule as True or False. For some embodiments, this expression is meant to convey the concept of the existence of a condition, which may or may not be true. For example, in some such embodiments, the condition \u201cA=True\u201d might be expressed as \u201cIs variable Z=A?\u201d In other words, A in this example is the value of a parameter Z, and the condition is true when Z has a value A.","Irrespective of how the conditions are expressed, a multi-conditional rule in some embodiments specifies the taking of an action when certain conditions in the network are met. Examples of such actions include creation or deletion of new packet flow entries, creation or deletion of new network constructs, modification to use of existing network constructs, etc. In the virtualization application  these actions are often implemented by the rules engine  by creating, deleting, or modifying records in the output tables, which are then propagated to the NIB by the NIB publisher .","As shown in , the multi-conditional rule  uses only the AND operator to express the rule. In other words, each of the conditions A, B, C and D has to be true before the Action X is to be taken. In some embodiments, the declaration toolkit  only allows the developers to utilize the AND operator because excluding the other operators (such as ORs, XORs, etc.) that are allowed by datalog allows nLog to operate faster than datalog.","The compiler  converts each rule specified by the declaration toolkit  into a query plan  of the rules engine.  illustrates the creation of three query plans -for three rules -. Each query plan includes one or more sets of join operations. Each set of join operations specifies one or more join operations that are to be performed upon the occurrence of a particular event in a particular RE input table, where the particular event might correspond to the addition, deletion or modification of an entry in the particular RE input table.","In some embodiments, the compiler  converts each multi-conditional rule into several sets of join operations, with each set of join operations being specified for execution upon the detection of the occurrence of one of the conditions. Under this approach, the event for which the set of join operations is specified is one of the conditions of the multi-conditional rule. Given that the multi-conditional rule has multiple conditions, the compiler in these embodiments specifies multiple sets of join operations to address the occurrence of each of the conditions.",{"@attributes":{"id":"p-0249","num":"0248"},"figref":"FIG. 21","b":["2115","2120","2125","2130","2135","2140"],"i":"a"},"These four sets of operations collectively represent the query plan that the rules engine  performs upon the occurrence of a RE input table event relating to any of the parameters A, B, C, or D. When the input table event relates to one of these parameters (e.g., parameter B) but one of the other parameters (e.g., parameters A, C, and D) is not true, then the set of join operations fails and no output table is modified. But, when the input table event relates to one of these parameters (e.g., parameter B) and all of the other parameters (e.g., parameters A, C, and D) are true, then the set of join operations does not fail and an output table is modified to perform the action X. In some embodiments, these join operations are internal join operations. In the example illustrated in , each set of join operations terminates with a select command that selects entries in the record(s) resulting from the set of join operations to output to one or more output tables.","To implement the nLog engine in a distributed manner, some embodiments partition management of logical data path sets by assigning the management of each logical data path set to one controller instance. This partition management of the LDPS is also referred to as serialization of management of the LDPS. The rules engine  of some embodiments implements this partitioned management of the LDPS by having a join to the LDPS entry be the first join in each set of join operations that is not triggered by an event in a LDPS input table.",{"@attributes":{"id":"p-0252","num":"0251"},"figref":["FIG. 22","FIG. 21"],"b":["2115","2220","2220","2220","2210","2115","2115","2105","2120","2125","2130","2135","2140","2115","2220","2230","2235","2240","2245","2115"],"i":["a ","a","a","c ","a","c ","a ","a","a ","a. "]},"The four sets of join operations , ,  and  are operational sets that are each to be performed upon the occurrence of one of the conditions A, B, C, and D. The first join operations in each of these four sets , ,  and  is a join with the LDPS table managed by the virtualization application instance. Accordingly, even when the input table event relates to one of these four parameters (e.g., parameter B) and all of the other parameters (e.g., parameters A, C, and D) are true, the set of join operations may fail if the event has occurred for a LDPS that is not managed by this virtualization application instance. The set of join operations does not fail and an output table is modified to perform the desire action only when (1) the input table event relates to one of these four parameters (e.g., parameter B), all of the other parameters (e.g., parameters A, C, and D) are true, and (3) the event relates to a LDPS that is managed by this virtualization application instance. Sub-section D below further describes how the insertion of the join operation to the LDPS table allows the virtualization application to partition management of the LDPS'.","C. Table Mapping Operations Upon Occurrence of Event",{"@attributes":{"id":"p-0255","num":"0254"},"figref":"FIG. 23","b":["2300","1900","1905","1950"]},"As shown in , the process  initially detects (at ) a change in a RE input table . In some embodiments, the event processor  is the module that detects this change. Next, at , the process  identifies the query plan associated with the detected RE input table event. As mentioned above, each query plan in some embodiments specifies a set of join operations that are to be performed upon the occurrence of an input table event. In some embodiments, the event processor  is also the module that performs this operation (i.e., is the module that identifies the query plan).","At , the process  executes the query plan for the detected input table event. In some embodiments, the event processor  directs the table processor  to execute the query plan. To execute a query plan that is specified in terms of a set of join operations, the table processor  in some embodiments performs the set of join operations specified by the query plan to produce one or more records that represent one or more sets of data values from one or more input and miscellaneous tables  and .",{"@attributes":{"id":"p-0258","num":"0257"},"figref":"FIG. 24","b":["2405","2410","2415","2410","2415","2420","2425","2430"]},"Two records in two tables \u201cmatch\u201d when values of a common key (e.g., a primary key and a foreign key) that the two tables share are the same, in some embodiments. In the example in , the records  and  in tables  and  match because the values C in these records match. Similarly, the records  and  in tables  and  match because the values F in these records match. Finally, the records  and  in tables  and  match because the values R in these records match. The joining of the records , , , and  results in the combined record . In the example shown in , the result of a join operation between two tables (e.g., tables  and ) is a single record (e.g., ABCDFGH). However, in some cases, the result of a join operation between two tables may be multiple records.","Even though in the example illustrated in  a record is produced as the result of the set of join operations, the set of join operations in some cases might result in a null record. For instance, as further described in sub-section D below, a null record results when the set of join operations terminates on the first join because the detected event relates to a LDPS not managed by a particular instance of the virtualization application. Accordingly, at , the process determines whether the query plan has failed (e.g., whether the set of join operations resulted in a null record). If so, the process ends. In some embodiments, the operation  is implicitly performed by the table processor when it terminates its operations upon the failure of one of the join operations.","When the process  determines (at ) that the query plan has not failed, it stores (at ) the output resulting from the execution of the query plan in one or more of the output tables. In some embodiments, the table processor  performs this operation by (1) performing a select operation to select a subset of the data values from the record(s) produced by the join operations, and (2) writing the selected subset of data values in one or more RE output tables .  illustrates an example of this selection operation. Specifically, it illustrates the selection of values B, F, P and S from the combined record  and the writing of these values into a record  of an output table .","As mentioned above, the RE output tables can be categorized in some embodiments as (1) a RE input table only, (2) a VA output table only, or (3) both a RE input table and a VA output table. When the execution of the query plan results in the modification a VA output table, the process  publishes (at ) the changes to this output table to the NIB. In some embodiments, the NIB publisher  detects changes to the VA output tables  of the RE output tables , and in response, it propagates the modified data tuple in the modified VA output table into the NIB  through the APIs provided by the NOS . After a new data tuple is propagated to the NIB by the NIB monitor, the NOS  propagates, if needed, a NIB data tuple that was modified because on the propagated VA output table data tuple to one or more of the managed switching elements. In doing this, the NOS completes the deployment of the LDPS (e.g., one or more logical switching configurations) to one or more managed switching elements as specified by the NIB records.","At , the process determines whether the execution of the query plan resulted in the modification of the RE input table. This operation is implicitly performed in some embodiments when the event processor  determines that the output table that was modified previously at  modified a RE input table. As mentioned above, a RE output table  can also be a RE input table  that generates an event that causes the rules engine to perform another query plan after it is modified by the rules engine. Such an event is referred to as an internal input event, and it is to be contrasted with an external input event, which is an event that is caused by a RE input table modification made by the control application  or the NIB monitor . When the process determines (at ) that an internal input event was created, it returns to  to perform operations - for this new internal input event. The process terminates when it determines (at ) that the execution of the query plan at  did not result in an internal input event.","One of ordinary skill in the art will recognize that process  is a conceptual representation of the operations used to map a change in one or more input tables to one or more output tables. The specific operations of process  may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations, and different specific operations may be performed in different embodiments. For instance, the process  in some embodiments batches up a set of changes in RE input tables  and identifies (at ) a query plan associated with the set of detected RE input table events. The process in these embodiments executes (at ) the query plan for the whole set of the RE input table events rather than for a single RE input table event. Batching up the RE input table events in some embodiments results in better performance of the table mapping operations. For example, batching the RE input table events improves performance because it reduces the number of instance that the process  will produce additional RE input table events that would cause it to start another iteration of itself.","D. Parallel, Distributed Management of LDPS\u2032","As mentioned above, some embodiments implement the nLog engine as a distributed table mapping engine that is executed by different virtualization applications of different controller instances. To implement the nLog engine in a distributed manner, some embodiments partition the management of the logical data path sets by specifying for each particular logical data path set only one controller instance as the instance responsible for specifying the NIB records associated with that particular logical data path set. Partitioning the management of the LDPS' also assigns in some embodiments the table mapping operations for each LDPS to the nLog engine of the controller instance responsible for the LDPS.","As described above by reference to , some embodiments partition the nLog table mapping operations across the different instances by designating the first join operation that is performed by each nLog instance to be based on the LDPS parameter. This designation ensures that each nLog instance's join operations fail and terminate immediately when the instance has started a set of join operations that relate to a LDPS that is not managed by the nLog instance.",{"@attributes":{"id":"p-0268","num":"0267"},"figref":"FIG. 25","b":["2505","2510","2515","2520","2525","2530","2510","2515","2505","2520"]},"In the example illustrated in , the two query plans  and  are not executed because an input table event A has occurred for a LDPS  and these two plans are not associated with such an event. Instead, the two query plans  and  are executed because they are associated with the input table event A that has occurred. As shown in this figure, the occurrence of this event results in two sets of join operations being performed to execute the two query plans  and . The first set of join operations  for the query plan  fails because the query plan  is specified for a LDPS , which is a LDPS not managed by the virtualization application instance . This set of join operations fails on the first join operation  because it is a join with the LDPS table, which for the virtualization application instance  does not contain a record for the LDPS . In some embodiments, even though the first join operation  has failed, the remaining join operations (not shown) of the query plan  will still be performed and fail. In other embodiments, the remaining join operations of the query plan  will not be performed as shown.","The second set of join operations  does not fail, however, because it is for the LDPS , which is a LDPS managed by the virtualization application instance  and therefore has a record in the LDPS table of this application instance. This set of join operations has four stages that each performs one join operation. Also, as shown in , the set of join operations terminates with a selection operation that selects a portion of the combined record produced through the join operations.","The distribution of the nLog table mapping operations across several nLog instances reduces the load on each nLog instance and thereby increases the speed by which each nLog instance can complete its mapping operations.  illustrates an example that describes this reduction in workload. Specifically, it illustrates an example where two controller instances  and  are responsible for the virtualization application functionality of two different LDPS' A and B for different tenants A and B of a multi-tenant computing environment. The two controller instances manage two sets of managed switches  and . Each of the two sets of managed switches manages a set of machines  or , which may be host machines running on dedicated machines, or may be virtual machines running on shared machines.","In four stages, this figure illustrates the results of the table mapping operations that are performed by the virtualization applications of these two different controller instances. The first stage  shows that no machines have been deployed in the managed system for either tenant A or tenant B. The second stage  shows the computing environment with several machines that have been deployed for tenant A in the two sets of machines  and . It also shows the VA output table  of the virtualization application of the controller instance  with flow entries for the LDPS A that were specified by this instance's virtualization application. The second stage further shows the NIB  of the controller instance  containing the flow entries for the LDPS A. At this stage, the NIB  also contains LDPS data relating to LDPS A in some embodiments, but this data is not shown in .","The third stage  in  shows that the flow entries for the LDPS A have migrated to the NIB  of the controller instance . This migration occurs because of the NIB replication across the controller instances. Also, this replication causes LDPS data relating to LDPS A to be copied to the NIB . The third stage  further shows the computing environment with several machines that have been deployed for tenant B in the two sets of machines  and . It also shows the VA output table  of the virtualization application of the controller instance  with flow entries for the LDPS B that were specified by this instance's virtualization application. The third stage further shows the NIB  of the controller instance  containing the flow entries for the LDPS B. At this stage, the NIB  also contains LDPS data relating to LDPS B in some embodiments, but this data is not shown in .","The fourth stage  shows that the flow entries for the LDPS B have migrated to the NIB  of the controller instance . This migration occurs because of the NIB replication across the controller instances. This replication also causes LDPS data relating to LDPS B to be copied to the NIB . As shown at the stage , the NIBs  and  have LDPS data relating to both LDPS A and LDPS B. However, the VA output tables of one controller instance do not store flow entries for the LDPS of another controller instance. That is, in this example, the VA output tables  of controller instance A do not store the flow entries for the LDPS B and the VA output tables  of controller instance B do not store the flow entries for the LDPS A. This depiction is meant to illustrate that some embodiments partition the storage of the logical state data across several controller instances. This allows these embodiments to keep the size of tables (e.g., the input or output tables) small in order to increase the speed by which each nLog instance can complete its mapping operations as described above.","While the input and output tables of each controller instance in some embodiments only store or practically only store logical state data for only the LDPS' for which the controller instance is responsible, the NIB for each controller instance in some of these embodiments contains all or practically all of the logical state data (e.g., except some logical port statistics that are stored in the DHTs of controller instances that are not replicated across) for all LDPS of all controller instances. However, other embodiments will partition the logical state data for the LDPS's across the NIBs of different controller instances.","E. NIB Monitor",{"@attributes":{"id":"p-0277","num":"0276"},"figref":["FIG. 27","FIG. 27"],"b":["1950","1900","1905","1920","1915","1940","1935","1925"]},"As described above, the NIB monitor  interfaces with the NIB  to receive notifications regarding changes to the NIB . In the examples described above, the NIB monitor  may modify one or more RE input tables  when it receives a record change notification from the NIB. The rules engine  then performs a series of mapping operations to map the modified RE input tables to the RE output tables , which may include RE input tables , VA output tables  of  and tables that serve as both RE input tables and VA output tables.","In addition to modifying the RE input tables , the NIB monitor  of some embodiments may also modify one or more VA output tables when it receives a NIB change notification. That is, the NIB monitor in these embodiments directly modifies the VA output tables for some NIB change notifications, such as notification relating to some of the changes to the state and configuration of the managed switching elements. By directly writing such data to the VA output tables, the NIB  keeps the VA output tables updated with the current state and configuration of the managed switching elements.",{"@attributes":{"id":"p-0280","num":"0279"},"figref":"FIG. 27","b":["1945","2710","2715","2710","2715","2710","1925","2710","1925","1910","2715","1960"]},"As shown, the virtualization application  conceptually includes a difference assessor . The difference assessor  detects a change in the first representation  or in the second representation . A change in the first representation may occur when the rules engine  puts the result of its mapping operations in the VA output tables  of . A change in the second representation may occur when the NIB updates the VA output tables  when it is notified of a change in the NIB. Upon detecting a change in the VA output tables, the difference assessor  in some embodiments examines both the first representation  and the second representation  to find out the difference, if any, between these two representations.","When there is no difference between these two representations, the difference assessor  takes no further action because the current state and configuration of the managed switching elements are already what they should be. However, when there is a difference, the different assessor  may have the NIB publisher  publish the difference (e.g., data tuples) to the NIB . When the NIB publisher published the difference to the NIB , the difference will be propagated to the managed switching elements by the NOS  and the state and configuration of the managed switching elements will be the state and configuration specified by the first representation . Also, when the difference assessor detects a difference between the two representations, the difference assessor in some embodiments may call the input tables of the control or virtualization application to initiate additional table mapping operations to reconcile the difference between the desired and current values. Alternatively, in other embodiments, the NIB monitor will end up updating the input tables based on the changes in the NIB at the same time it updates the output tables and these will trigger the nLog operations that might update the output table.","In some embodiments, the virtualization application  does not store the desired and current representations  and  of the physical control plane data, and does not use a difference assessor  to assess whether two corresponding representations are identical. Instead, the virtualization application  stores each set of physical control plane data in a format that identifies differences between the desired data value and the current data value. When the difference between the desired and current values is significant, the virtualization application  of some embodiments may have the NIB publisher push a data tuple change to the NIB, or may call the input tables of the control or virtualization application to initiate additional table mapping operations to reconcile the difference between the desired and current values.","The operation of the virtualization application  will now be described with an example network event (i.e., a change in the network switching elements). In this example, the switching elements managed by the virtualization application include a pool node. A pool node and other network constructs associated with the pool node are described in the above-identified U.S. patent application Ser. No. 13\/177,535, now published as US2013\/0058250, entitled \u201cHierarchical Managed Switch Architecture.\u201d The pool node has a root bridge and a patch bridge. The pool node shuts down for some reason (e.g., by hardware or software failure) and the two bridges of the pool node get shut down together with the pool node. This results in an update in the NIB  which will indicate the pool node and its bridges being down. The NIB monitor  in this example is then notified of this update in the NIB and subsequently writes information to both the RE input tables  and the VA output tables (specifically, the second representation ). The rules engine  performs mapping operations upon detecting the change in the RE input tables  but the mapping result (i.e., the first representation ) in this example will not change the desired data value regarding the pool node and its bridges. That is, the desired data value would still indicate that the pool node and the two bridges should exist in the configuration of the system. The second representation  would also indicate the presence of the pool node and its bridges in the configuration.","The NOS  then restarts the pool node but the root bridge and the patch bridge do not come back up in this example. The NIB  will indicate that the pool node is back up and the NIB monitor  updates the RE input tables  and the second representation  of the VA output tables accordingly. The rules engine  performs mapping operations on the RE input tables  but the resulting desired data value would still not change because there was no change as to the existence of the pool node in the configuration of the system. However, the current data value in the second representation  would indicate at this point that the pool node has come back up but not the bridges. The difference assessor  detects the changes in the first and second representations and compares the desired and current data values regarding the pool node and its bridges. The difference assessor  determines the difference, which is the existence of the two bridges in the pool node. The difference assessor  notifies the NIB publisher  of this difference. The NIB publisher  publishes this difference in the NIB . The NOS  propagates this information to the pool node so that the pool node creates the root and patch bridges in it.","F. Code Based Examples","1. nLog and Datalog Declarations","As discussed above, nLog in some embodiments is a variation of the datalog database language for declaring logic that controls how the virtualization application  will map input tables to the output tables by performing mapping operations on the information contained in the input tables. In some embodiments, the input tables contain information comprising logical data path sets and switching element attributes. In some embodiments, the output tables contain physical control plane data. In the Type I virtualized control system in some embodiments, declaring logic for governing how a control application requires extensive use cases and conditions in C++ or an equivalent programmatic language. In some embodiments, the nLog language expresses the same control logic as a programming language such as C++, but in high-level declarations that are easily modified and extended. In some embodiments of the invention, the nLog declarations are compiled into a code in a programming language for use by a run time process by the nLog compiler.","As discussed above, nLog is a variation of the Datalog language selected for its speed and applicability to distributing logical data path sets. The Datalog language is a query and rule language for deductive databases. The Datalog language is stated using rule declarations. In some embodiments, nLog uses the \u201cJOIN\u201d, \u201cSELECT\u201d, and \u201cAND\u201d operators from Datalog. \u201cJOIN\u201d performs an inner join of tables on a specified join-predicate. Inner join creates a new result table by combining column values of two tables based on the specified join-predicate. Inner join compares each row of the joined tables to find all pairs of rows that satisfy the join-predicate. When the join-predicate is satisfied, column values for each matched pair of rows of the joined table are combined into a result row. \u201cSELECT\u201d returns column or row entries from tables based on a specified result set. \u201cAND\u201d returns true where all inputs to the \u201cAND\u201d function are true. This set of operators is sufficient to implement the distribution of logical data path sets of some embodiments of the invention. Other operators, such as the \u201cXOR\u201d and \u201cOR\u201d operators, are not used for some embodiments of nLog because (1) their implementation may complicate the development of the nLog compiler, (2) their function can be emulated using only the \u201cAND\u201d operator, and\/or (3) their performance is less than that of the \u201cJOIN\u201d, \u201cSELECT\u201d, and \u201cAND\u201d operators.","In some embodiments nLog uses the syntactic structure of Datalog rule declarations. For instance, nLog rule declarations are of the form \u201c<head>:-<body1>, <body2>.\u201d The rule declaration \u201c<head>:-<body1>, <body2>\u201d can be understood as \u201c<head> if it is known that <body1> and <body2>.\u201d The \u201c:-\u201d symbol demarcates the head from the body of a rule declaration. The text prior to the \u201c:-\u201d symbol is the head of a rule declaration. The text following the \u201c:-\u201d symbol is the body of a rule declaration. The head of the rule declaration receives parameters and defines the action the rules engine will take when the conditions specified in the body clauses of the rule declaration are satisfied. One of the ordinary skill in the art will realize that the rule declarations do not have to be in this specific form. That is, any equivalent forms may be used to define these rule declarations.","Similar to the head of a rule declaration, the body contains actions and parameters. In the previous example, clauses <body1> and <body2> comprised the body of the rule declaration. As shown in the previous example, a rule declaration may have more than one body clause. However, clauses in the body portion of a rule declaration are not used to create, modify, or destroy network constructs. Rather, in some embodiments, clauses in the body portion of a rule declaration are used as conditions for performing the action of the head portion of a rule declaration.","The head and body clauses of rule declarations can accept parameters. Parameters are inputs to the actions contained in either the head or the body clauses of rules declarations. For example, the parameters of a rule declaration that creates a new network construct typically indicate the location and identity of the new network construct to be created. Parameters will be described in more detail in the example rule declaration described below.","2. Pool Node Rule Declaration","In order to illustrate the structure of an nLog rule declaration of some embodiments of the invention, an example nLog rule declaration will be discussed in detail. This rule declaration will be referred to as the \u201cpool_node\u201d rule declaration.",{"@attributes":{"id":"p-0295","num":"0294"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"pool_node(zone_id, phys_chassis_id) :-"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"log_zone(_, zone_id, _),"]},{"entry":[{},"stt_connector(zone_id, phys_chassis_id, _),"]},{"entry":[{},"phys_chassis_forwarding_enabled(phys_chassis_id),"]},{"entry":[{},"phys_chassis_connected(phys_chassis_id);"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"In some embodiments, STT stands for stateless transport layer tunneling. Tunneling is encapsulating messages formatted in one communication protocol (i.e., messages having a header of one protocol) with a header of another communication protocol. The messages are encapsulated at one end of a communication link that uses the latter protocol and de-capsulated at another end of the communication link. This form of tunnel is described in U.S. Provisional Patent Applications 61\/471,897, 61\/479,680, and 61\/501,433.","The \u201cpool_node\u201d rule declaration specifies when the rules engine of a virtualization application should create a pool node. A rules engine can support multiple rule declarations indicating when to create a pool node, but this discussion will be limited to this specific example. The \u201cpool_node\u201d rule declaration can be understood as: \u201ccreate a pool node in this zone and this chassis if it is known that this chassis in this zone supports STT connectors, has forwarding enabled, is in a valid logical zone, and is connected.\u201d A pool node is a hierarchical switching structure that assists managed switching elements in forwarding packets. A zone is a logical zone containing logical switches, pool nodes, managed switching elements, and extenders. An extender is a hierarchical switching structure that assists pool nodes in forwarding packets to remote locations or unmanaged switching elements. A chassis is a node, an x86 box, a hypervisor, a pool node, or an extender. A node is switch that can forward packets. A pool node, a zone, an extender, a chassis, and a node are described in detail in the above-identified U.S. patent application Ser. No. 13\/177,535, now published as US2013\/0058250, entitled \u201cHierarchical Managed Switch Architecture.\u201d","The head of the \u201cpool_node\u201d rule declaration above reads \u201cpool_node(zone_id, phys_chassis_id)\u201d. The head of the declaration is the action preceding the \u201c:-\u201d symbol. The head indicates that this rule, if it is known that all the clauses of the body are true, will call for the creation of a pool node. The head contains the parameters \u201czone_id\u201d and \u201cphys_chassis_id\u201d. \u201cZone_id\u201d corresponds to the logical zone of the pool node that will be created if the conditions of the body of the rule declaration are satisfied. \u201cChassis_id\u201d corresponds to the machine that will host the pool node that will be created if the conditions of the body of the rule declaration are satisfied. The parameters for the head and the parameters for the body clauses are the same. For example, the \u201czone_id\u201d parameter in the \u201cpool_node\u201d action in the head of the rule declaration is the same \u201czone_id\u201d in the \u201cstt_connector\u201d clause in the body of the rule declaration.","The body of the \u201cpool_node\u201d declaration above is all the clauses following the \u201c:-\u201d symbol. The \u201cpool_node\u201d declaration above contains four clauses: \u201clog_zone\u201d, \u201cstt_connector\u201d, \u201cphys_chassis forwarding_enabled\u201d, and \u201cphys_chassis_connected\u201d. The body clause \u201clog_zone(_, zone_id,)\u201d tests whether the zone indicated by \u201czone_id\u201d is a valid logical zone. The underscores before and after \u201czone_id,\u201d indicate masked parameters that are not relevant to the \u201cpool_node\u201d declaration. The body clause \u201cstt_connector(zone_id, phys_chassis_id,)\u201d tests whether the zone indicated by \u201czone_id\u201d and the physical chassis indicated by \u201cphys_chassis_id\u201d supports STT tunnels. The underscore after \u201cphys_chassis_id,\u201d indicates a masked parameter that is not relevant to the \u201cpool_node\u201d declaration. The body clause \u201cphys_chassis forwarding_enabled(phys_chassis_id)\u201d tests whether the physical chassis indicated by \u201cphys_chassis_id\u201d has forwarding_enabled. The body clause \u201cphys_chassis_connected(phys_chassis_id)\u201d tests whether the physical chassis indicated by \u201cphys_chassis_id\u201d is connected to the network. Once compiled, the \u201cpool_node\u201d declaration above will define how the rules engine will react to events corresponding to \u201clog_zone\u201d, \u201cstt_connector\u201d, \u201cphys_chassis_forwarding_enabled\u201d, and \u201cphys_chassis_connected\u201d. Based on the C++ code compiled from the \u201cpool_node\u201d rule declaration, the rules engine may order the creation of a new pool node.","3. Compiling Rule Declarations","As discussed above, in some embodiments nLog rule declarations  are compiled into C++ code using the nLog compiler . The nLog compiler  produces C++ code that may be executed by the rules engine . The rules engine  executes C++ code to map input tables containing logical data path sets and switching element attributes to the output tables containing physical control plane data by performing mapping operations on logical data path sets. Compilation of rule declarations by the nLog compiler takes place at developer time. Execution of compiled rule declarations by the rules engine takes place at run time when the network controller instance is managing the managed switching elements. The nLog compiler reduces the complexity of writing code to define how the rules engine  will distribute logical data path sets by allowing the distribution logic to be declared in the higher-level nLog language instead of C++.","In some embodiments, four sets of rule declarations of the form <head>:-<body1>, <body2>, <body3>, <body4> can produce hundreds of lines (e.g., 700 lines) of C++ code once compiled. Each <bodyX> clause is compiled into a query plan defining how the rules engine will handle an event associated with the body clause. The \u201cpool_node\u201d rule declaration above will compile into four query plans, one query plan for each clause in the body of the \u201cpool_node\u201d rule declaration. Each body clause action will serve as an event that is associated with one of the compiled query plans.","In some embodiments, a query plan defines the operations the rules engine will perform when the rules engine detects an event associated with the query plan. When the rules engine receives an event, the rules engine queries the input tables  based on the query plan or query plans associated with the received event.","As discussed above, events are network occurrences that have a corresponding query plan in the rules engine in some embodiments. Events indicate that one of the body clauses of the rule declaration from which the query plan was compiled is true. Examples of events include a dispatch port being enabled, a physical chassis being connected to the network, or STT tunnel being enabled on a chassis. When such an event occurs, the rules engine computes how logical data path sets and switching element attributes in the input tables will be distributed as physical control plane data in the output tables.","One event may trigger multiple query plans compiled from multiple rule declarations. For example, the \u201cdispatch_port_req\u201d rule declaration includes a body clause \u201cphys_chassis_connected\u201d and the \u201cpool_node\u201d rule declaration includes a body clause \u201cphys_chassis_connected\u201d. If a rules engine contains the compiled query plans for both the \u201cdispatch_port_req\u201d and \u201cpool_node\u201d rule declarations, and if the rules engine receives an event associated with \u201cphys_chassis_connected\u201d, then the rules engine will execute the query plans for \u201cphys_chassis_connected\u201d compiled from both the \u201cdispatch_port_req\u201d and \u201cpool_node\u201d rule declarations.","In some embodiments, a query plan consists of N\u22121 JOIN operations and one SELECTION operation, where N is equal to the number of body clauses in the rule declaration from which the query plan was compiled. If a rules engine receives an event associated with a query plan, the query plan will join the input tables  of the N\u22121 body clauses of the rule declaration from which the query plan was compiled. Then the query plan will select the head action parameters from the joined tables. If the JOIN operations pass an AND test, and the SELECTION operation yields valid values, then the rules engine will execute the head action.","4. Compiled \u201cPool_Node\u201d Query Plan","The query plans compiled from the \u201cpool_node\u201d rule declaration above will now be discussed as a specific example of a set of query plans. In the discussion below, the rules engine has the query plans compiled from the \u201cpool_node\u201d rule declaration.","The \u201cpool_node\u201d rule declaration will compile to four query plans. Each of the four query plans consists of three JOIN operations and one SELECTION operation. If the rules engine receives an event corresponding to \u201clog_zone C, zone_id,)\u201d, \u201cstt_connector (zone_id, phys_chassis_id, _)\u201d, \u201cphys_chassis_forwarding_enabled (phys_chassis_id)\u201d, or \u201cphys_chassis_connected (phys_chassis_id)\u201d, then the rules engine will run the associated query plan. A \u201clog_zone (_, zone_id, _)\u201d event occurs when a logical zone is instantiated. An \u201cstt_connector (zone_id, phys_chassis_id, _)\u201d event occurs when STT tunneling is enabled on a physical chassis for a specific zone. A \u201cphys_chassis_forwarding_enabled (phys_chassis_id)\u201d event occurs when a physical chassis enables forwarding. A \u201cphys_chassis_connected (phys_chassis_id)\u201d event occurs when a physical chassis is connected to the network. The compiled \u201cphys_chassis_connected (phys_chassis_id)\u201d query plan will now be shown:",{"@attributes":{"id":"p-0310","num":"0309"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"static void get_plan12(Runner& runner, Runner::Queries* queries) {"},{"entry":"\/\/ global_plan_id: 0x846fd20b1488481b"},{"entry":"\/\/ head: pool_node(V(zone_id), V(phys_chassis_id))"},{"entry":"\/\/ rank: 2"},{"entry":"\/\/ event: phys_chassis_connected(V(phys_chassis_id))"},{"entry":"\/\/ query_plan:"},{"entry":"\/\/ 0) 100000, phys_chassis_forwarding_enabled(V(phys_chassis_id)),"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"245pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\/\/","Join, mapping: [(0, 0)] new columns: [ ] new mappings: [ ],"]},{"entry":["\/\/","drop_columns: [ ], index: phys_chassis_forwarding_enabled(0)"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"\/\/ 1) 100001, stt_connector(V(zone_id), V(phys_chassis_id),"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"245pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["\/\/","V(_)), Join, mapping: [(1, 0)] new columns: [u\u2018zone_id\u2019] new mappings:"]},{"entry":["\/\/","[0], drop_columns: [ ], index: stt_connector(1)"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"\/\/ 2) 100000, log_zone(V(_), V(zone_id), V(_)), Join, mapping:"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"245pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["\/\/","[(1, 1)] new columns: [ ] new mappings: [ ], drop_columns: [ ], index:"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"231pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"\/\/","log_zone(1)"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"\/\/ 3) 100000, pool_node(V(zone_id), V(phys_chassis_id)),"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"231pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"\/\/","Selection, mapping: [(0, 1), (1, 0)] new columns: [ ] new mappings: [ ],"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"1","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"245pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":["\/\/","drop_columns: [ ], index: None"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\u2002Operators operators;"},{"entry":"\u2002append_operator(runner, JOIN,"},{"entry":"\u20031, ((const int[ ][2]) {{0, 0}}),"},{"entry":"\u20030, ((const int[ ]) {\u22121}),"},{"entry":"\u20030, ((const int[ ]) {\u22121}),"},{"entry":"\u20030, ((const int[ ][2]) {{\u22121,\u22121}}),"},{"entry":"\u2003\u201cphys_chassis_forwarding_enabled\u201d,"},{"entry":"runner.index(\u201cphys_chassis_forwarding_enabled_0\u201d), &operators);"},{"entry":"\u2002append_operator(runner, JOIN,"},{"entry":"\u20031, ((const int[ ][2]) {{1, 0}}),"},{"entry":"\u20031, ((const int[ ]) {0}),"},{"entry":"\u20030, ((const int[ ]) {\u22121}),"},{"entry":"\u20030, ((const int[ ][2]) {{\u22121,\u22121}}),"},{"entry":"\u2003\u201cstt_connector\u201d, runner.index(\u201cstt_connector_1\u201d), &operators);"},{"entry":"\u2002append_operator(runner, JOIN,"},{"entry":"\u20031, ((const int[ ][2]) {{1, 1}}),"},{"entry":"\u20030, ((const int[ ]) {\u22121}),"},{"entry":"\u20030, ((const int[ ]) {\u22121}),"},{"entry":"\u20030, ((const int[ ][2]) {{\u22121,\u22121}}),"},{"entry":"\u2003\u201clog_zone\u201d, runner.index(\u201clog_zone_1\u201d), &operators);"},{"entry":"\u2002append_operator(runner, SELECTION,"},{"entry":"\u20032, ((const int[ ][2]) {{0, 1},{1, 0}}),"},{"entry":"\u20030, ((const int[ ]) {\u22121}),"},{"entry":"\u20030, ((const int[ ]) {\u22121}),"},{"entry":"\u20030, ((const int[ ][2]) {{\u22121,\u22121}}),"},{"entry":"\u2003\u201cpool_node\u201d, NULL, &operators);"},{"entry":"\u2002queries\u2212>push_back(new Query("},{"entry":"\u2003\u2002runner.table(\u201cpool_node\u201d),"},{"entry":"\u2003\u2002runner.table(\u201cphys_chassis_connected\u201d),"},{"entry":"\u2003\u2002operators,"},{"entry":"\u2003\u20020x846fd20b1488481b));"},{"entry":"}"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"If a \u201cphys_chassis_connected\u201d event occurs, then the rules engine will access the query plan for \u201cphys_chassis_connected\u201d shown above. The access occurs on the first line where the code says \u201cstatic void get_plan12(Runner& runner, Runner::Queries* queries)\u201d. In this case, a \u201cphys_chassis_connected\u201d event is associated with \u201cplan12\u201d. The fifth line, \u201c\/\/ event: phys_chassis_connected(V(phys_chassis_id))\u201d is a comment that indicates that this plan is associated with the \u201cphys_chassis_connected\u201d event.","Below the commented portion, the section starting with \u201cOperators operators;\u201d indicates that three JOIN operations and one SELECTION operation will be appended to a runner object. The runner object is indicated to the left of each JOIN or SELECTION operator where the code says \u201cappend_operator(runner, JOIN,\u201d or \u201cappend_operator(runner, SELECTION,\u201d. After all operations have been appended, the runner object executes the JOIN operations and the SELECTION operation. The lines \u201cqueries-> push back(newQuery(runner.table(\u201cpool node\u201d), runner.table(\u201cphys_chassis_connected\u201d), operators\u201d indicate that the runner object will be pushed as a new query. Execution of the JOIN and SELECTION operations occurs after the runner object has been pushed as a new query.","The first three operators are JOIN operations on various columns and indices. The fourth operator is a SELECTION operation. The first JOIN operator joins columns [0,0] on the index \u201cphys_chassis_forwarding_enabled(0)\u201d. The second JOIN operator joins columns [1,0] on the index \u201cstt_connector(1)\u201d. The third JOIN operator joins columns [1,1] on the index \u201clog_zone(1)\u201d. The SELECTION operator selects columns [01],[1,0] on no index. This query plan will be executed in the rules engine of the virtualization application processes notification from a network operating system. During execution the rules engine will access the input tables  to retrieve the information necessary to perform the JOIN and SELECTION operations as described above.","5. Rules Engine Execution of \u201cPool_Node\u201d Query Plan","In order to illustrate the operation of the rules engine, a specific example of an execution of a query plan will now be discussed using the \u201cphys_chassis_connected(phys_chassis_id)\u201d query plan shown above. The execution process begins when the event processor  receives notification of a physical chassis being connected. Upon notification of a physical chassis being connected, the event processor  looks up the query plans  associated with a physical chassis being connected. In some embodiments, multiple query plans may exist for a single event and those multiple query plans would all be executed. In this case, only a single query plan \u201cphys_chassis_connected(phys_chassis_id)\u201d will be found. The event processor  then executes the query plan. The process begins with appending JOIN operators and SELECT operators to a runner object. This can be seen in the code above where the code has 3 \u201cappend_operator(runner, JOIN,\u201d blocks followed by a \u201cappend_operator(runner,SELECTION\u201d block. The event processor  executes each operation in the order in which it was appended to the runner object. The JOIN operations will perform inner join operations on the input tables  associated with the body clauses on predicates based on the values shown in each \u201cappend_operator(runner, JOIN,\u201d code block. The SELECTION operator will select on columns to retrieve a result set, if any, based on the values shown in the \u201cappend_operator(runner,SELECTION\u201d code block.","Once all operators of a query plan are appended to the runner object, the event processor  sends the runner object to the table processor  to execute the operations appended to the runner object. The JOIN and SELECTION operations take place on data tuples in the input tables , and in some embodiments, on function and constant tables  that are not contained in the input tables . If the JOIN and SELECTION operations are successful, then the table processor  will return a \u201cphys_chassis_id\u201d and a \u201czone_id\u201d to the event processor  to be used in creating a new pool node. If any of the JOIN and SELECTION operations fail, then a NULL result will be returned in all parameters and the operations terminated in some embodiments. A NULL result in any parameter will result in a failure of the query plan. Upon successful return of parameters \u201cphys_chassis_id\u201d and \u201czone_id\u201d, the event processor  will submit the parameters to the input tables  as output entries.","6. Input Tables","As discussed above, input tables  contain information that the rules engine maps to the output tables as physical control plane data. In some embodiments, the information contained in the input tables  includes: logical data path sets, data corresponding to external events, and supplemental network data used by the rules engine during computation of the distribution of logical data path sets. The input tables  receive notifications from the network operating system regarding state changes in the network. The input tables  also translate those notifications from the network operating system into data tuples and events to be used by the rules engine to compute how to distribute input table information across the output tables.","In some embodiments, input tables  share a basic set of methods and attributes. The basic set of attributes includes: an entity input table, an export interface, and an export registry for the export interface. The basic set of methods includes: methods for accessing the export registry through the export interface, input table instantiation functions, and registration and un-registration methods for the namesake entity of the input table class. E.g., the physical forwarding engine input table class has methods for registering and un-registering physical forwarding engines.","In some embodiments, input tables  are programmatic objects stored as a database in computer memory that correspond to types of logical and physical network elements. Examples of input table classes include: physical forwarding engine input tables, logical forwarding engine input tables, logical port input tables, and physical port input tables.","7. Physical Forwarding Engine Input Table","The physical forwarding engine input table class stores data tuples regarding physical forwarding engines that the rules engine uses in performing mapping operations. In some embodiments, physical forwarding engine input tables store data tuples corresponding to physical switches, software switches, and virtual switches. To illustrate the operation of the input tables , the physical forwarding engine input table class will be discussed in detail as an example. The physical forwarding engine input table class contains the following member classes: entity input table and forwarding engine export interface. The physical forwarding engine input table class contains the following attributes: a name, an export registry for the export interface, and a network ID. The entity input table receives entity data from the NIB and uses the export interface to load the export registry with data received from the NIB through the NIB monitor . The entity input table also alerts the rules engine of changes to the data tuples in the input tables . The export registry contains data tuples to be used by the rules engine. The entity input table is defined as an attribute of the class PhysicalForwardingEngineInputTable:",{"@attributes":{"id":"p-0323","num":"0322"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"21pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"196pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"class PhysicalForwardingEngineInputTable :"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"public EntityInputTable,"]},{"entry":[{},"public ForwardingEngineExportInterface {"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"public:PhysicalForwardingEngineInputTable("]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"154pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"const std::string& name,"]},{"entry":[{},"ForwardingEngineExportRegistry* fe_reg,"]},{"entry":[{},"const NetworkId& physical_network);"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"168pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"void start( );"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"182pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"..."]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"The entity input table is implemented in physical forwarding engine input table.cc:",{"@attributes":{"id":"p-0325","num":"0324"},"tables":{"@attributes":{"id":"TABLE-US-00004","num":"00004"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"PhysicalForwardingEngineInputTable::"]},{"entry":[{},"PhysicalForwardingEngineInputTable("]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"42pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"175pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"const std::string& name,"]},{"entry":[{},"ForwardingEngineExportRegistry* fe_registry,"]},{"entry":[{},"const NetworkId& physical_network)"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"189pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},": EntityInputTable(name), fe_registry_(fe_registry),"]},{"entry":[{},"physical_network_id_(physical_network) { }"]},{"entry":[{},"..."]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}]}}]}}},"The entity input table receives notifications and data tuples from the NIB monitor . The entity input table uses the export interface to load the export registry with data tuples received from the NIB monitor as shown above. For example, if the NIB monitor  sends a notification that a forwarding engine has a new port, then the entity input table will use the \u201cPortAdd\u201d export interface method to add a port to the export registry.","The export interface defines the methods of changing the values of the data in the export registry. These methods include: register forwarding engine, un-register forwarding engine, change in connection state of forwarding engine, data path change, address add, address remove, port add, port remove, forwarding table add, forwarding table remove, forwarding engine bind, forwarding engine unbind, transmit buffer, transmit packet, receive buffer, and receive packet. The export registry contains the data associated with the export interface.","The forwarding engine input table class further includes functions to instantiate a physical forwarding input table object's export registry, start a physical forwarding input table object, register a forwarding engine, and unregister a forwarding engine.","Returning to the \u201cPortAdd\u201d example above, after the entity input table has used the export interface to load the export registry with a new port, the entity input table notifies the rules engine of the new port. The rules engine handles this notification by looking up a query plan to handle a \u201cPortAdd\u201d event. If a rule declaration has been compiled into a query plan to handle \u201cPortAdd\u201d, then the rules engine will execute the associated query plan or plans. However, in some embodiments, not all changes to the input table export registry will have an associated query plan in the rules engine. For example, if the rules engine developer did not provide a rule declaration for handling \u201cPortAdd\u201d, then the rules engine would not respond to notification of a \u201cPortAdd\u201d event.","8. Physical Forwarding Engine Output Table","As discussed above, the output tables  store data tuples to be distributed across the NIB. In some embodiments, the output tables  also reconcile differences between the output tables  and the NIB by re-pushing state information to the NIB when the NIB does not match data in the output tables . For example, if a pool node creation was pushed to the NIB by the output tables , and the pool node creation failed, then the output tables  would push the pool node creation again. In some embodiments, the output tables  also register the input tables  and the output tables  for notification of state changes in the NIB.","To illustrate the operation of the output tables , the physical forwarding engine output table class will be discussed in detail as an example. The physical forwarding engine output table class stores data tuples regarding physical forwarding engines to be distributed across the NIB. The physical forwarding engine output table class \u201cPhysicalForwardingEngineOutputTable\u201d can export data tuples for the following forwarding engines: forwarding engine chassis, forwarding nodes, and forwarding engines. The physical forwarding engine output table class contains the following member classes: external output table (\u201cpublic ExternalOutputTable\u201d), chassis export interface (\u201cpublic ChassisExportInterface\u201d), node export interface (\u201cpublic NodeExportInterface\u201d), Forwarding Engine export interface (\u201cForwardingEngineExportInterface\u201d), and attribute reference manager (\u201cAttributeRefManager\u201d). The physical forwarding engine output table class contains the following attributes: table name (\u201ctable_name\u201d), engine name (\u201cengine_name\u201d), NIB (\u201cNIB* nib\u201d), chassis export registry (\u201cChassisExportRegistry* chassis_registry\u201d), node export registry (\u201cNodeExportRegistry* node_registry\u201d), Forwarding Engine export registry (\u201cForwardingEngineExportRegistry* engine_registry\u201d), network ID (\u201cconst NetworkId& physical_network\u201d), controller ID (\u201cconst UUID& controller_id\u201d), and an OpenFlow configuration exporter to Open Virtual Switch databases (\u201cOFConfigExportOVSDB* exporter\u201d).","In order to elucidate the member classes and attributes of the physical forwarding engine output table class, an example of a physical forwarding engine output table object will be described below in conjunction with its code:",{"@attributes":{"id":"p-0334","num":"0333"},"tables":{"@attributes":{"id":"TABLE-US-00005","num":"00005"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"class PhysicalForwardingEngineOutputTable"},{"entry":"\u2002: public ExternalOutputTable<EntityId, ForwardingEngine>,"},{"entry":"\u2003public ChassisExportInterface,"},{"entry":"\u2003public NodeExportInterface,"},{"entry":"\u2003public ForwardingEngineExportInterface,"},{"entry":"\u2003public AttributeRefManager<ForwardingEngine> {"},{"entry":"\u2002public:"},{"entry":"\u2002PhysicalForwardingEngineOutputTable(const std::string& table_name,"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"203pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"const std::string& engine_name,"]},{"entry":[{},"NIB* nib,"]},{"entry":[{},"ChassisExportRegistry* chassis_registry,"]},{"entry":[{},"NodeExportRegistry* node_registry,"]},{"entry":[{},"ForwardingEngineExportRegistry* engine_registry,"]},{"entry":[{},"const NetworkId& physical_network,"]},{"entry":[{},"const UUID& controller_id,"]},{"entry":[{},"OFConfigExportOVSDB* exporter);"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\u2002void start( );"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"The attributes of the physical forwarding engine output table class define a physical forwarding engine output table object. The physical forwarding engine output table object pushes state changes to objects in the NIB according to the output of the rules engine. The physical forwarding engine output table object is further for monitoring the NIB for inconsistencies with the output of the rules engine.","The attributes that define a physical forwarding engine output table object can be seen above below \u201cPhysicalForwardingEngineOutputTable\u201d. The table name (\u201ctable_name\u201d) defines the name of the object in order to identify the table. The engine name (\u201cengine_name\u201d) defines which physical forwarding engine in the output table object is to be populated by the object. The NIB pointer (\u201cNIB* nib\u201d) defines what NIB the object will export data tuples to. In some embodiments, there are multiple NIBs.","The export registry pointers are pointers to registry objects to the NIB identified by the NIB pointer. The pointers serve as a means to pass NIB state data back to the physical forwarding engine output table object. When the output of the rules engine does not match the NIB state data in the export registries, the physical forwarding engine output table object will push the output of the rules engine down to the NIB to correct the inconsistency. The physical forwarding engine output table object is notified of changes to NIB state using events as described above. In this manner, the physical forwarding engine output table object enforces the physical network configuration defined by the output of the rules engine.","The chassis export registry pointer (\u201cChassisExportRegistry* chassis_registry\u201d) points to a NIB object that contains data tuples for physical chassis. In some embodiments, changes in the operational status of a chassis constitute an event for the chassis export registry. The node export registry pointer (\u201cNodeExportRegistry* node_registry\u201d) points to a NIB object that contains data tuples for nodes. The Forwarding Engine export registry pointer (\u201cForwardingEngineExportRegistry* engine_registry\u201d) points to a NIB object that contains the data tuples the object stores for forwarding engines. In some embodiments, changes in the state of the forwarding engines constitute events for the Forwarding Engine export registry.","The network ID (\u201cconst NetworkId& physical_network\u201d) defines the identity of a group of physical network items in the NIB that is relevant to the physical forwarding engine output table object. In some embodiments, the group of items relevant to the physical forwarding engine output table object is the entirety of the physical forwarding elements of the network. In some embodiments of the NIB, the NIB stores network IDs that identify groups of logical forwarding elements.","The controller ID (\u201cconst UUID& controller_id\u201d) defines what controller instance the object is exporting from. In some embodiments, controller instances are grouped into clusters to pool processing power. In those embodiments, the controller ID identifies which controller in the controller cluster has made changes to NIB data. The OpenFlow configuration exporter pointer (\u201cOFConfigExportOVSDB* exporter\u201d) points to the OVS database exporter object in the NIB that the exports NIB state data to network elements.","The object's member classes serve as the object's means of receiving information from the NIB, storing the information in the object's registries, and sending data tuples to the output tables. The attribute reference manager (\u201cAttributeRefManager\u201d) receives information from the NIB and writes data tuples to the export registries using the export interfaces. The export interfaces (\u201cChassisExportInterface\u201d, \u201cNodeExportInterface\u201d, and \u201cForwardingEngineExportInterface\u201d) provide interfaces to allow the registries to be written to by the attribute reference manager. The external output table (\u201cExternalOutputTable\u201d) sends data tuples to the output tables from the export registries using the exporter handle described above. The export interfaces are shown in this code sample:",{"@attributes":{"id":"p-0342","num":"0341"},"tables":{"@attributes":{"id":"TABLE-US-00006","num":"00006"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0","pgwide":"1"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}}],"thead":{"row":{"entry":[{},{"@attributes":{"namest":"offset","nameend":"1","align":"center","rowsep":"1"}}]}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\/\/ ChassisExportInterface implementation."]},{"entry":[{},"\u2002void Register(const EntityPtr<Chassis>&) { }"]},{"entry":[{},"\u2002void Unregister(const EntityPtr<Chassis>&) { }"]},{"entry":[{},"\u2002void ConnectionStateChange(const EntityPtr<Chassis>&,"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"245pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"\u2002const Chassis::ConnectionState&) { }"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u2002void AddressAdd(const EntityPtr<Chassis>&, const Address&) { }"]},{"entry":[{},"\u2002void AddressRemove(const EntityPtr<Chassis>&, const Address&) { }"]},{"entry":[{},"\u2002void ForwardingEngineAdd(const EntityPtr<Chassis>&,"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"245pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"const EntityPtr<ForwardingEngine>&);"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"\u2002void ForwardingEngineRemove(const EntityPtr<Chassis>&,"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"245pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":{"entry":[{},"\u2002const EntityPtr<ForwardingEngine>&);"]}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u2002\/\/ NodeExportInterface implementation."]},{"entry":[{},"\u2002void Register(const EntityPtr<Node>&) { }"]},{"entry":[{},"\u2002void Unregister(const EntityPtr<Node>&) { }"]},{"entry":[{},"\u2002void AddressAdd(const EntityPtr<Node>&, const Address&) { }"]},{"entry":[{},"\u2002void AddressRemove(const EntityPtr<Node>&, const Address&) { }"]},{"entry":[{},"\u2002void PortAdd(const EntityPtr<Node>&, const EntityPtr<Port>&) { }"]},{"entry":[{},"\u2002void PortRemove(const EntityPtr<Node>&, const EntityPtr<Port>&) { }"]},{"entry":[{},"\u2002void UpdateStatistics(const EntityPtr<Node>&) { }"]},{"entry":[{},"\u2002void StatisticsUpdated(const EntityPtr<Node>&) { }"]},{"entry":[{},"\u2002void NameChange(const EntityPtr<Node>&, const std::string&);"]},{"entry":[{},"\u2002\/\/ ForwardingEngineExportInterface implementation"]},{"entry":[{},"\u2002void Register(const EntityPtr<ForwardingEngine>&);"]},{"entry":[{},"\u2002void Unregister(const EntityPtr<ForwardingEngine>&);"]},{"entry":[{},"\u2002void SubtypeChange(const EntityPtr<ForwardingEngine>&, const"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"273pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":"NetEntity::TypeIdParamType) { }"}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"2"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"259pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"\u2002void NameChange(const EntityPtr<ForwardingEngine>&,const std::string&) { }"]},{"entry":[{},"\u2002void NetworkUUIDsChange(const EntityPtr<ForwardingEngine>&, const std::string&)"]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"273pt","align":"left"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"{ }"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}]}}},"void BridgeIDChange(const EntityPtr<ForwardingEngine>&, const std::string&) { }","There are three types of export interfaces. The export interfaces types include a physical forwarding chassis interface, a physical forwarding nodes interface, and a Forwarding Engine interface. The physical forwarding chassis interface consists of functions for changing the state of forwarding chassis in the output tables. The physical forwarding nodes interface consists of functions for changing the state of physical forwarding nodes in the output tables. In some embodiments, physical forwarding chassis and physical forwarding nodes include: x86 boxes, hypervisor boxes, hosts, pool nodes, extenders, and transport nodes. The Forwarding Engine interface consists of functions for changing the state of forwarding engines in the output tables. In some embodiments, forwarding engines (i.e., forwarding elements) include: physical switches, software switches, and virtual switches. The export interfaces provide a means of setting export registry data. The export interfaces support methods such as \u201cregister\u201d, \u201cunregister\u201d, \u201cAddressAdd\u201d and \u201cNameChange\u201d. After data tuples are added to the export registries through the export interfaces, the external output table brings in said data tuples. The external output table's implementation can be seen in this code snippet:",{"@attributes":{"id":"p-0345","num":"0344"},"tables":{"@attributes":{"id":"TABLE-US-00007","num":"00007"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"left"}},"thead":{"row":{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":"\/\/ Implementations for the abstract methods of the ExternalOutputTable."},{"entry":"Tuple key(const Tuple& tuple);"},{"entry":"key_type create(const Tuple& value, bool& error);"},{"entry":"key_type modify(const key_type& id, const Tuple& value, bool& error);"},{"entry":"bool destroy(const key_type& id);"},{"entry":"void check_validity(const key_type& id);"},{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}}]}}}}},"As shown above, the external output table entries can be created, modified, destroyed, or checked the validity of The output tables receive information from the NIB through the attribute reference manager. The attribute reference manager stores that information as data tuples in the export registries using the export interfaces, and the data tuples from the NIB is pushed to the external output table.","V. Control Application","As mentioned above, the control application of some embodiments converts control data records (also called data tuples) to forwarding plane data records (e.g., logical forwarding plane data) by performing conversion operations. Specifically, in some embodiments, the control application populates the logical data path tables (e.g., the logical forwarding tables) that are created by the virtualization application with logical data path sets.",{"@attributes":{"id":"p-0348","num":"0347"},"figref":"FIG. 28","b":"2800"},"As shown in , the process  initially receives (at ) data regarding an input event. The input event data may be logical data supplied by the user in some embodiments. As described in the above-identified U.S. patent application Ser. No. 13\/177,532, now published as US2013\/0058339, entitled \u201cNetwork Control Apparatus and Method,\u201d some embodiments provide the user with an interface that the user can use to specify input event data. An example of user-supplied data could be logical control plane data including access control list data for a logical switch that the user manages. The input event data may also be logical forwarding plane data that the control application generates in some embodiments from the logical control plane data. The input event data in some embodiments may also be physical forwarding plane data or physical control plane data received from the NIB. In some embodiments, the process  receives the physical forwarding data from a NIB monitor that monitors the NIB to detect a change in the NIB that reflects a change in one or more managed switching elements.","At , the process  then performs a filtering operation to determine whether this instance of the control application is responsible for the input event data. As described above, several instances of the control application may operate in parallel to control multiple sets of logical data paths in some embodiments. In these embodiments, each control application uses the filtering operation to filter out input data that does not relate to the control application's logical data path set. To perform this filtering operation, the control application of some embodiments includes a filter module. This module in some embodiments is a standalone module, while in other embodiments it is implemented by a table mapping engine (e.g., implemented by the join operations performed by the table mapping engine) that maps records between input tables and output tables of the virtualization application, as further described below.","Next, at , the process determines whether the filtering operation has filtered out the input event data. The filtering operation filters out the input event data in some embodiments when the input event data does not fall within one of the logical data path sets that are the responsibility of the control application. When the process determines (at ) that the filtering operation has filtered out the input event data, the process ends. Otherwise, the process  transitions to .","At , a converter of the virtualization application generates one or more sets of data tuples based on the received input event data. In some embodiments, the converter is a table mapping engine that performs a series of table mapping operations on the input event data to map the input event data to other data tuples. As mentioned above, this table mapping engine also performs the filtering operation in some embodiments. One example of such a table mapping engine is an nLog table-mapping engine which will be described bellow.","In some embodiments, the data tuples that the process  generates may include data (e.g., logical forwarding plane data) that the process has to push down to the NIB. Accordingly, at , the process publishes to the NIB any data tuples that it has generated if such publication is necessary. After , the process ends.","The control application in some embodiments performs its mapping operations by using the nLog table mapping engine, which, as described above, is a variation of the datalog table mapping technique.  illustrates a control application  of some embodiments of the invention. This application  uses an nLog table mapping engine to map input tables that contain input data tuples to LDPS data tuples. This application resides on top of a virtualization application  that receives the LDPS data tuples from the control application . The virtualization application  maps the LDPS data tuples to data tuples for defining managed switching elements, attributes of the managed switching elements, and flow entries for the managed switching elements. The virtual application  resides on top of a NOS  that contains a NIB  that stores the data tuples generated by the virtualization application .","More specifically, the control application  allows different users to define different logical data path sets (LDPS), which specify the desired switching configurations of the users. The control application  also reacts to changes in the NIB to modify the LDPS'. The virtualization application  through its mapping operations converts each of the LDPS of each user into a set of data tuples to populate to the NIB. The virtualization application  then populates the NIB  with the generated sets of data tuples. When the NOS  subsequently propagates the NIB data tuples for a particular user to the switching element(s), the NOS completes the deployment of the user's switching configuration to one or more switching elements. In some embodiments, the control application is executed on the same machine with the virtualization application and the NOS. However, the control application, the virtualization application, and the NOS do not have to run on the same machine in other embodiments. That is, one of these applications or each of these applications may run on a different computer.","As shown in , the control application  includes a set of rule-engine input tables , a set of function and constant tables , a query manager , a rules engine , a set of rule-engine output tables , a NIB monitor , a publisher , and a compiler . The compiler  is one component of the application that operates at a different instance in time than the application's other components. The compiler operates when a developer needs to specify the rules engine for a particular control application and\/or virtualized environment, whereas the rest of the application's modules operate at run time when the application interfaces with the control application and the NOS to deploy and monitor logical data path sets specified by one or more users.","In some embodiments, the compiler  takes a relatively small set (e.g., few hundred lines) of declarative instructions  that are specified in a declarative language and converts these into a large set (e.g., thousands of lines) of code that specify the operation of the rules engine , which performs the application's table mapping. As such, the compiler greatly simplifies the control application developer's process of defining and updating the control application. This is because the compiler allows the developer to use a high level programming language that allows a compact definition of the control application's complex mapping operation and to subsequently update this mapping operation in response to any number of changes (e.g., changes in the logical networking functions supported by the control application, changes to desired behavior of the control application, etc.).","In some embodiments, the rule-engine (RE) input tables  include tables with logical data and\/or switching configurations (e.g., access control list configurations, private virtual network configurations, port security configurations, etc.) specified by the user and\/or the control application. They also include in some embodiments tables that contain physical data (i.e., non-logical data) from the switching elements managed by the virtualized control system. In some embodiments, such physical data includes data regarding the managed switching elements (e.g., physical control plane data) and other data regarding network configuration employed by the virtualized control system to deploy the different LDPS' of the different users.","The RE input tables  are partially populated by the LDPS data (e.g., logical control plane data) provided by the user. It also generates part of the LDPS data (e.g., logical forwarding plane data) and physical (i.e., non-logical) data (e.g., physical control plane data) by monitoring the NIB to identify changes in the managed switching element infrastructure that would require modification to the LDPS data and\/or the physical data.","In addition to the RE input tables , the control application  includes other miscellaneous tables  that the rules engine  uses to gather inputs for its table mapping operations. These tables  include constant tables that store defined values for constants that the rules engine  needs to perform its table mapping operations.","When the rules engine  references constants, the corresponding value defined for the constants are actually retrieved and used. In addition, the values defined for constants in the constant table  may be modified and\/or updated. In this manner, the constant table  provides the ability to modify the value defined for constants that the rules engine  references without the need to rewrite or recompile code that specifies the operation of the rules engine .","The tables  further include function tables that store functions that the rules engine  needs to use to calculate values needed to populate the output tables . One example of such a function is a hash function that the rules engine uses to compute hash values for distributing distributed hash table (DHT) operations as well as load balancing traffic between lower level switches and higher level switches in a hierarchical switching architecture.","The rules engine  performs table mapping operations that specify one manner for converting any logical data path set within the logical control plane to a logical data path set in the logical forwarding plane. Whenever one of the rule-engine (RE) input tables is modified, the rules engine performs a set of table mapping operations that may result in the modification of one or more data tuples in one or more RE output tables. The modification of the output table data tuples, in turn, through the virtualization application , may cause the NIB to be modified in order to establish and\/or modify the implementation of a particular user's LDPS in the managed switching element infrastructure.","As shown in , the rules engine  includes an event processor , several query plans , and a table processor . Each query plan is a set of rules that specify a set of join operations that are to be performed upon the occurrence of a modification to one of the RE input table. Such a modification is referred to below as an input table event. As described above, each query plan is generated by the compiler  from one declaratory rule in the set of declarations . In some embodiments, the query plans are defined by using the nLog declaratory language.","In some embodiments, the compiler  does not just statically generate query plans but rather dynamically generates query plans based on performance data it gathers. The complier  in these embodiments generates an initial set of query plans and let the rules engine operate with the initial set of query plans. The control application gathers the performance data or receives performance feedbacks (e.g., from the rules engine). Based on this data, the compiler is modified so that the control application or a user of this application can have the modified compiler modify the query plans while the rules engine is not operating or during the operation of the rules engine.","For instance, the order of the join operations in a query plan may result in different execution times depending on the number of tables the rules engine has to select to perform each join operation. The compiler in these embodiments can be re-specified in order to re-order the join operations in a particular query plan when a certain order of the join operations in the particular query plan has resulted in a long execution time to perform the join operations.","The event processor  of the rules engine  detects the occurrence of each input table event. The event processor of different embodiments detects the occurrence of an input table event differently. In some embodiments, the event processor registers for callbacks with the RE input tables for notification of changes to the records of the RE input tables. In such embodiments, the event processor  detects an input table event when it receives notification from a RE input table that one of its records has changed.","In response to a detected input table event, the event processor  (1) selects the appropriate query plan for the detected table event, and (2) directs the table processor  to execute the query plan. To execute the query plan, the table processor  in some embodiments performs the join operations specified by the query plan to produce one or more records that represent one or more sets of data values from one or more input and miscellaneous tables  and . The table processor  of some embodiments then (1) performs a select operation to select a subset of the data values from the record(s) produced by the join operations, and (2) writes the selected subset of data values in one or more RE output tables .","In some embodiments, the RE output tables  store both logical and physical network element data attributes. The tables  are called RE output tables as they store the output of the table mapping operations of the rules engine . In some embodiments, the RE output tables can be grouped in several different categories. For instance, in some embodiments, these tables can be RE input tables and\/or control-application (CA) output tables. A table is a RE input table when a change in the table causes the rules engine to detect an input event that requires the execution of a query plan. A RE output table  can also be a RE input table  that generates an event that causes the rules engine to perform another query plan. Such an event is referred to as an internal input event, and it is to be contrasted with an external input event, which is an event that is caused by a RE input table modification made by the control application  or the NIB monitor .","A table is a control-application output table when a change in the table causes the publisher  to publish a change to the virtualization application  and\/or to the NIB, as further described below. A table in the RE output tables  can be a RE input table, a CA output table, or both a RE input table and a CA output table.","The publisher  detects changes to the CA output tables of the RE output tables . The publisher of different embodiments detects the occurrence of a CA output table event differently. In some embodiments, the publisher registers for callbacks with the CA output tables for notification of changes to the records of the CA output tables. In such embodiments, the publisher  detects an output table event when it receives notification from a CA output table that one of its records has changed.","In response to a detected output table event, the publisher  takes some or all of modified data tuples in the modified CA output tables and propagates this modified data tuple(s) to the input tables (not shown) of the virtualization application . In some embodiments, instead of the publisher  pushing the data tuples to the virtualization application, the virtualization application  pulls the data tuples from the CA output tables  into the input tables of the virtualization application. Alternatively, in some embodiments, the publisher  publishes changes to the modified CA output tables to the NIB, and the virtualization application  retrieves these changes from the NIB and based on them, modifies its input tables. In some embodiments, the CA output tables  of the control application  and the input tables of the virtualization  may be identical. In yet other embodiments, the control and virtualization applications use one set of tables, so that the CA output tables are essentially VA input tables.","Moreover, the publisher  in some embodiments takes some or all of modified data tuples in the modified CA output tables and propagates this modified data tuple into the NIB  through the APIs provided by the NOS . Also, the publisher may push down logical data (e.g., logical control plane data, logical forwarding plane data, etc.) processed and maintained by the control application  to the NIB . This is because, in some embodiments, the NIB  serves as a medium for all communications between the control application, the virtualization application, and the NOS of different controller instances as described above.","As the CA output tables store both logical and physical network element data attributes in some embodiments, the NIB  in some embodiments stores both logical and physical network element attributes that are identical or derived by the virtualization application  from the logical and physical network element data attributes in the output tables . In other embodiments, however, the NIB only stores physical network element attributes that are identical or derived by the virtualization application  from the physical network element data attributes in the output tables .","The NIB monitor  interfaces with the NIB  to receive notifications regarding changes to the NIB. The NIB monitor of different embodiments detects the occurrence of a change in the NIB differently. In some embodiments, the NIB monitor registers for callbacks with the NIB for notification of changes to one or more records in the NIB. In such embodiments, the NIB monitor  detects NIB change event when it receives notification from the NIB that one of its records has changed. In response to a detected NIB change event, the NIB monitor  may modify one or more RE input tables , which, in turn, may cause one or more RE input table event to occur that then initiates the execution of one or more query plans by the rules engine. In other words, the NIB monitor writes some or all of the information that it receives from the NIB into the input tables , so that the state and configuration of the managed switching elements can be accounted for while generating the NIB data tuples through the mapping operations. Each time the managed switching configuration or underlying managed switching element state changes, the NIB monitor  may update the input table records  so that the generated NIB data tuples can be updated to reflect the modified switching configuration or underlying switching element state.","In some embodiments, the NIB monitor  is a collection of input objects (or functions) associated with the RE input tables. Each input object in some embodiments is associated with one RE input table and is responsible for modifying its associated RE input table in response to a change in the NIB. Each input object in some embodiments registers with one or more NIB objects for callback notifications upon the occurrence of changes to the NIB object(s). Similarly, in some embodiments, the publisher  is a collection of output objects (or functions) associated with the CA output tables. Each output object in some embodiments is associated with one CA output table and is responsible for propagating changes in its associated output table to the virtualization application . As such, in some embodiments, the NIB monitor is a conceptual representation of the input and output objects that register with the NIB for callbacks.","The query manager  interfaces with the control application  to receive queries regarding LDPS data. As shown in , the query manager  of some embodiments also interfaces with the NIB  in order to query the NIB to provide the control application state information regarding the network elements in the LDPS' for the different user. In other embodiments, however, the query manager  queries the output tables  to obtain LDPS data for the control application.",{"@attributes":{"id":"p-0378","num":"0377"},"figref":"FIG. 30","b":["3005","3010","3015","3020"]},"This figure further shows the NIB publisher  receiving some or all of the input table records and publishing these records to the NIB . As further shown, the NIB monitor  receives NIB change notifications (e.g., notifications of managed switch changes detected by the NOS or notification of NIB changes pushed by the other controller instances) in some embodiments and in response to such notifications it may update input and output tables  and  of the control application. In some embodiments, the NIB monitor  may write LCP, LFP, or PCP data to the input tables  in response to NIB modification notifications.","The bottom half of  also illustrates the table mapping operations of the virtualization application . As shown, the virtualization application's input tables  store logical forwarding plane data and physical control plane data, as the collection of all these data along with data in the constant and function tables (not shown) is used by the virtualization application's nLog engine  in some embodiments to generate physical control plane data from the input logical forwarding plane data.","This figure further shows the NIB publisher  receiving some or all of the input table records and publishing these records to the NIB . As further shown, the NIB monitor  receives NIB change notifications in some embodiments and in response to such notifications it may update input and output tables  and  of the virtualization application. In some embodiments, the NIB monitor  may write LFP or PCP data to the input tables  in response to NIB modification notifications.","As mentioned above, some of the logical or physical data that a NIB monitor pushes to the input tables of the control or virtualization application relates to data that is generated by other controller instances and passed to the NIB monitor's particular NIB (e.g., through the secondary storage layer). For instance, in some embodiments, the logical data regarding logical constructs that relates to multiple LDPS' might change, and the NIB monitor may write this change to the input tables. Another example of such logical data that is produced by another controller instance in a multi controller instance environment occurs when a user provides logical control plane data for a LDPS on a first controller instance that is not responsible for the LDPS. This change requests is added to the NIB request list (such as the NIB request list  described above by reference to ) of the first controller instance. This request list is then propagated across the NIBs of other controller instances by replication processes described above. The NOS of a second controller instance, which is the master of the LDPS, eventually makes the change to the NIB of the second controller instance based on the propagated request list. The NIB monitor of the second controller instance then writes the change to the one of the application's input tables (e.g., the control application's input table). Accordingly, in such cases, the logical data that the NIB monitor writes to the input tables in some cases may originate from the NIB of another controller instance.","As mentioned above, the control application  and the virtualization application  are two separate applications that operate on the same machine or different machines in some embodiments. Other embodiments, however, implement these two applications as two modules of one integrated application, with the control application module  generating LDPS in the logical forwarding plane and the virtualization application generating physical data path sets in the physical control plane.","Still other embodiments integrate the control and virtualization operations of these two applications within one integrated application, without separating these operations into two separate modules.  illustrates an example of such an integrated application . This application  uses an nLog table mapping engine  to map data from an input set of tables  to an output set of tables , which like the above described embodiments, may include one or more tables in the input set of tables. The input set of tables in this integrated application may include LCP data that need to be mapped to LFP data, or it may include LFP data that need to be mapped to PCP data.","In this integrated control and virtualization application , a NIB publisher  publishes input table records and output table records to the NIB . A NIB monitor  then receives notification of changes from the NIB  (e.g., managed switch changes detected by the NOS or NIB changes pushed by the other controller instances), and for some notifications (e.g., those relating to the LDPS' for which the application is the master), pushes changes to the input and\/or tables  and .","VI. Electronic System","Many of the above-described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium (also referred to as computer readable medium). When these instructions are executed by one or more processing unit(s) (e.g., one or more processors, cores of processors, or other processing units), they cause the processing unit(s) to perform the actions indicated in the instructions. Examples of computer readable media include, but are not limited to, CD-ROMs, flash drives, RAM chips, hard drives, EPROMs, etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.","In this specification, the term \u201csoftware\u201d is meant to include firmware residing in read-only memory or applications stored in magnetic storage, which can be read into memory for processing by a processor. Also, in some embodiments, multiple software inventions can be implemented as sub-parts of a larger program while remaining distinct software inventions. In some embodiments, multiple software inventions can also be implemented as separate programs. Finally, any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments, the software programs, when installed to operate on one or more electronic systems, define one or more specific machine implementations that execute and perform the operations of the software programs.",{"@attributes":{"id":"p-0388","num":"0387"},"figref":"FIG. 32","b":["3200","3200","3200","3200","3205","3210","3225","3230","3235","3240","3245"]},"The bus  collectively represents all system, peripheral, and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance, the bus  communicatively connects the processing unit(s)  with the read-only memory , the system memory , and the permanent storage device .","From these various memory units, the processing unit(s)  retrieve instructions to execute and data to process in order to execute the processes of the invention. The processing unit(s) may be a single processor or a multi-core processor in different embodiments.","The read-only-memory (ROM)  stores static data and instructions that are needed by the processing unit(s)  and other modules of the electronic system. The permanent storage device , on the other hand, is a read-and-write memory device. This device is a non-volatile memory unit that stores instructions and data even when the electronic system  is off. Some embodiments of the invention use a mass-storage device (such as a magnetic or optical disk and its corresponding disk drive) as the permanent storage device .","Other embodiments use a removable storage device (such as a floppy disk, flash drive, etc.) as the permanent storage device. Like the permanent storage device , the system memory  is a read-and-write memory device. However, unlike storage device , the system memory is a volatile read-and-write memory, such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments, the invention's processes are stored in the system memory , the permanent storage device , and\/or the read-only memory . For example, the various memory units include instructions for implementing the processes in accordance with some embodiments. From these various memory units, the processing unit(s)  retrieve instructions to execute and data to process in order to execute the processes of some embodiments.","The bus  also connects to the input and output devices  and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices  include alphanumeric keyboards and pointing devices (also called \u201ccursor control devices\u201d). The output devices  display images generated by the electronic system. The output devices include printers and display devices, such as cathode ray tubes (CRT) or liquid crystal displays (LCD). Some embodiments include devices such as a touchscreen that function as both input and output devices.","Finally, as shown in , bus  also couples electronic system  to a network  through a network adapter (not shown). In this manner, the computer can be a part of a network of computers (such as a local area network (\u201cLAN\u201d), a wide area network (\u201cWAN\u201d), or an Intranet, or a network of networks, such as the Internet. Any or all components of electronic system  may be used in conjunction with the invention.","Some embodiments include electronic components, such as microprocessors, storage and memory that store computer program instructions in a machine-readable or computer-readable medium (alternatively referred to as computer-readable storage media, machine-readable media, or machine-readable storage media). Some examples of such computer-readable media include RAM, ROM, read-only compact discs (CD-ROM), recordable compact discs (CD-R), rewritable compact discs (CD-RW), read-only digital versatile discs (e.g., DVD-ROM, dual-layer DVD-ROM), a variety of recordable\/rewritable DVDs (e.g., DVD-RAM, DVD-RW, DVD+RW, etc.), flash memory (e.g., SD cards, mini-SD cards, micro-SD cards, etc.), magnetic and\/or solid state hard drives, read-only and recordable Blu-Ray\u00ae discs, ultra density optical discs, any other optical or magnetic media, and floppy disks. The computer-readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code, such as is produced by a compiler, and files including higher-level code that are executed by a computer, an electronic component, or a microprocessor using an interpreter.","While the above discussion primarily refers to microprocessor or multi-core processors that execute software, some embodiments are performed by one or more integrated circuits, such as application specific integrated circuits (ASICs) or field programmable gate arrays (FPGAs). In some embodiments, such integrated circuits execute instructions that are stored on the circuit itself.","As used in this specification, the terms \u201ccomputer\u201d, \u201cserver\u201d, \u201cprocessor\u201d, and \u201cmemory\u201d all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification, the terms display or displaying means displaying on an electronic device. As used in this specification, the terms \u201ccomputer readable medium,\u201d \u201ccomputer readable media,\u201d and \u201cmachine readable medium\u201d are entirely restricted to tangible, physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals, wired download signals, and any other ephemeral signals.","While the invention has been described with reference to numerous specific details, one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. For instance, a number of the figures (including ,  and ) conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations, and different specific operations may be performed in different embodiments. Furthermore, the process could be implemented using several sub-processes, or as part of a larger macro process.","Also, several embodiments were described above in which a user provides logical data path sets in terms of logical control plane data. In other embodiments, however, a user may provide logical data path sets in terms of logical forwarding plane data. In addition, several embodiments were described above in which a controller instance provides physical control plane data to a switching element in order to manage the switching element. In other embodiments, however, the controller instance may provide the switching element with physical forwarding plane data. In such embodiments, the NIB would store physical forwarding plane data and the virtualization application would generate such data.","Furthermore, in several examples above, a user specifies one or more logic switches. In some embodiments, the user can provide physical switch configurations along with such logic switch configurations. Also, even though controller instances are described that in some embodiments are individually formed by several application layers that execute on one computing device, one of ordinary skill will realize that such instances are formed by dedicated computing devices or other machines in some embodiments that perform one or more layers of their operations.","Also, several examples described above show that a logical data path set is associated with one user. One of the ordinary skill in the art will recognize that then a user may be associated with one or more sets of logical data paths in some embodiments. That is, the relationship between a logical data path set is not always a one-to-one relationship as a user may be associated with multiple logical data path sets. Thus, one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details."],"BRFSUM":[{},{}],"heading":["BACKGROUND","BRIEF SUMMARY","DETAILED DESCRIPTION"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The novel features of the invention are set forth in the appended claims. However, for purposes of explanation, several embodiments of the invention are set forth in the following figures.",{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0043","num":"0042"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0044","num":"0043"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0045","num":"0044"},"figref":"FIG. 7"},{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0047","num":"0046"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0048","num":"0047"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0049","num":"0048"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0051","num":"0050"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0052","num":"0051"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0053","num":"0052"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0054","num":"0053"},"figref":"FIG. 16"},{"@attributes":{"id":"p-0055","num":"0054"},"figref":"FIG. 17"},{"@attributes":{"id":"p-0056","num":"0055"},"figref":"FIG. 18"},{"@attributes":{"id":"p-0057","num":"0056"},"figref":"FIG. 19"},{"@attributes":{"id":"p-0058","num":"0057"},"figref":"FIG. 20"},{"@attributes":{"id":"p-0059","num":"0058"},"figref":"FIG. 21"},{"@attributes":{"id":"p-0060","num":"0059"},"figref":"FIG. 22"},{"@attributes":{"id":"p-0061","num":"0060"},"figref":"FIG. 23"},{"@attributes":{"id":"p-0062","num":"0061"},"figref":"FIG. 24"},{"@attributes":{"id":"p-0063","num":"0062"},"figref":"FIG. 25"},{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIGS. 26"},{"@attributes":{"id":"p-0065","num":"0064"},"figref":"FIG. 27"},{"@attributes":{"id":"p-0066","num":"0065"},"figref":"FIG. 28"},{"@attributes":{"id":"p-0067","num":"0066"},"figref":"FIG. 29"},{"@attributes":{"id":"p-0068","num":"0067"},"figref":"FIG. 30"},{"@attributes":{"id":"p-0069","num":"0068"},"figref":"FIG. 31"},{"@attributes":{"id":"p-0070","num":"0069"},"figref":"FIG. 32"}]},"DETDESC":[{},{}]}
