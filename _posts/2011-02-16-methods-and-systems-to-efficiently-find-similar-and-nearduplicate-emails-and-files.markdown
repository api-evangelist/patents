---
title: Methods and systems to efficiently find similar and near-duplicate emails and files
abstract: A set of trigrams can be generated for each document in a plurality of documents processed by an e-discovery system. Each trigram in the set of trigrams for a given document is a sequence of three terms in the given document. A set of trigrams for each similar document is then determined based on the set of trigrams for the original document. To facilitate identification of the similar documents, a full text index is then generated for the plurality of documents and the set of trigrams for each document are indexed into the full text index, as individual terms. Queries can be generated into the full text index based on trigrams of a document to determine other similar or near-duplicate documents. After a set of potentially similar documents are identified, a separate distance criteria can be applied to evaluate the level of similarity between the two documents in an efficient way.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09275129&OS=09275129&RS=09275129
owner: Symantec Corporation
number: 09275129
owner_city: Mountain View
owner_country: US
publication_date: 20110216
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["CROSS-REFERENCES TO RELATED APPLICATIONS","BACKGROUND OF THE INVENTION","BRIEF SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE INVENTION"],"p":["This application is related to commonly owned U.S. Pat. No. 7,657,603 granted Feb. 2, 2010 based on U.S. patent application Ser. No. 11\/457,241, filed Jul. 13, 2006 and entitled \u201cMethods and Systems of Electronic Message Derivation,\u201d which is hereby incorporated by reference for all purposes.","This application is related to commonly owned U.S. Pat. No. 7,593,995 granted Sep. 22, 2009 based on U.S. patent application Ser. No. 11\/457,317, filed Jul. 13, 2006 and entitled \u201cMethods and Systems of Electronic Message Threading and Ranking,\u201d which is hereby incorporated by reference for all purposes.","This application is related to commonly owned and co-pending U.S. patent application Ser. No. 11\/657,398, filed Jan. 23, 2007 and entitled \u201cMethods and Systems of Electronic Message Threading and Ranking,\u201d which is a continuation of U.S. patent application Ser. No. 11\/457,317 and which also claims the benefit of U.S. Provisional Application No. 60\/761,501, filed Jan. 23, 2006 and entitled \u201cIncremental E-Mail Crawling and Indexing Methods and Apparatus\u201d and U.S. Provisional Application No. 60\/761,679, filed Jan. 23, 2006 and entitled \u201cSystem, Method, and User Interface for Distributed E-Mail Analysis,\u201d which are hereby incorporated by reference for all purposes.","This disclosure relates generally to techniques for processing electronic messages and documents. More particularly, the disclosure provides techniques to efficiently find similar and near-duplication electronic messages and files.","Collaboration using electronic messaging, such as email and instant messaging is becoming increasingly ubiquitous. Many users and organizations have transitioned to \u201cpaperless\u201d offices, where information and documents are communicated almost exclusively using electronic messaging. Also, \u201cpaper\u201d based documents can be scanned and converted to electronic files using OCR (Optical character recognition). As a result, users and organizations are also now expending time and money to sort and archive increasing volumes of digital documents and data.","At the same time, state and federal regulators such as the Federal Energy Regulatory Commission (FERC), the Securities and Exchange Commission (SEC), and the Food and Drug Administration (FDA) have become increasingly aggressive in enforcing regulations requiring storage, analysis, and reporting of information based on electronic messages. Additionally, criminal cases and civil litigation frequently employ electronic discovery techniques, in addition to traditional discovery methods, to discover information from electronic documents and messages.","One problem with electronically storing information is that complying with disclosure requirements or reporting requirements is difficult because of the large amounts of data that may accumulate. As broadband connections to the Internet are common in most homes and businesses, emails frequently include one or more multi-megabyte attachments. Moreover, these emails and attachments are increasingly of diverse and propriety formats, making later access to data difficult without the required software.","Another problem is that disclosure requirements or reporting requirements do not simply require that the electronic message be preserved and then disclosed. Often, the disclosure requirements or reporting requirements are more focused toward the disclosure or report on information about the electronic message, such as who had access to sensitive data referred to in the contents of a particular electronic message. Some companies have teams of employees spending days and weeks reviewing emails in order to respond to regulatory audits and investigations. For these reasons, the inventors believe that users and organizations need electronic message analysis solutions to help lower costs in disclosing and\/or reporting information related to electronic messaging and other electronically stored information.","In electronic discovery, whether it is for early case assessment or for improving speed and accuracy of review, it is critically important to identify as many responsive documents as is possible. Unlike typical web search engine technologies which focuses on identifying only a handful of most relevant documents, electronic discovery invariably is about minimizing the risks of overlooking relevant documents and minimizing expenses. This shifts the technical challenge from optimizing precision (finding only relevant documents) into one of increasing recall (finding most of the relevant documents). One aspect of message analysis is the ability to find similar or near duplicate documents. Once similar\/near duplicate documents are found for given document, user can perform desired action on that bulk of documents and thereby save time.","Accordingly, what is desired is to solve problems relating to finding similar and near-duplicate documents, some of which may be discussed herein. Additionally, what is desired is to reduce drawbacks related to finding similar and near-duplicate documents, some of which may be discussed herein.","The following portion of this disclosure presents a simplified summary of one or more innovations, embodiments, and\/or examples found within this disclosure for at least the purpose of providing a basic understanding of the subject matter. This summary does not attempt to provide an extensive overview of any particular embodiment or example. Additionally, this summary is not intended to identify key\/critical elements of an embodiment or example or to delineate the scope of the subject matter of this disclosure. Accordingly, one purpose of this summary may be to present some innovations, embodiments, and\/or examples found within this disclosure in a simplified form as a prelude to a more detailed description presented later.","Finding similar or near-duplicate documents a given document is a well known problem with various solutions such as extracting feature vectors for each document and comparing feature vectors. Such approaches typically require comparing the feature vectors of all the documents in the corpus with the given document to find similar or near duplicate documents. In various embodiments, techniques are provided for reducing the space in which such comparison is required. Thus, in one aspect, the need to fetch each of the feature vectors of every document and then compare it with the feature vector of the given document can be eliminated or reduced.","In one embodiment, a method for generating document signatures can include receiving, at one or more computer systems, a plurality of documents. Each document in the plurality of documents can include a plurality of terms. A set of trigrams can be generated for each document in the plurality of documents. Each trigram in the set of trigrams for a given document in the plurality of documents may be a sequence in the given document of terms (e.g., three consecutive terms) in the plurality of terms of the given document. Thus, a document presents, for example, a three-term sliding window whose trigrams are assembled into a collection of trigrams along with the number of occurrences of each trigram.","In a further aspect, another set of trigrams may then be determined for each document in the plurality of documents based on the above set of trigrams for the document and criteria for filtering similarity between documents. The criteria may be user-specified. A second set of trigrams for a given document in the plurality of documents may be a subset of a first set of trigrams for the given document and may have one or more trigrams that satisfy the any specified filter criteria.","In further embodiments, the second set of trigrams for each document in the plurality of documents may be stored in a storage device as offsets into the document for each term of each trigram in the second set of trigrams. In other embodiments, the actual trigrams (and terms) may be stored. In other embodiments, a full text index may be generated for the plurality of documents. The second set of trigrams for each document in the plurality of documents may then be indexed into the full text index.","In various embodiments, a first document may be received and a set of trigrams associated with the first document may be determined. A query into the full text index for the plurality of documents indexing the second set of trigrams for each document in the plurality of documents may be generated based of the set of trigrams associated with the first document. A first set of documents in the plurality of documents may then be determined in response to executing the query on the full text index indexing the second set of trigrams for each document in the plurality of documents. Accordingly, the query into the full text index may reduce the space for which comparison between features vectors is required.","In various embodiments, an actual similarity between a document and the potentially documents may be measured using a mathematical formula involving the trigram-based feature vectors. In one embodiment, the Hamming Distance between the two trigram sets can be useful. This measures the minimum number of substitutions required to change one string into the other, or the number of errors that transformed one string into the other. In another embodiment, the Cosine Similarity measurement of two feature vectors in a term-space model can be utilized. Such distance computations are expensive, so it is important to reduce the number of comparisons between feature vectors of documents. The current embodiment's full-text index query achieves this purpose.","In some embodiments, the threshold filtering criteria needs to be dynamically changeable. To facilitate this, it is important to retrieve the smallest set of potentially similar documents, so the feature vector comparisons can be computed and stored in temporary memory for the users to apply dynamic filters.","In some embodiments, it is required to identify both a set of near-duplicate documents, as well as a set of parent documents. An example is an email containing a plurality of attachments. In the event the emails are not similar but the attachments are near-duplicate based on the similarity threshold criteria, it may be required to identify the parent emails where such attachments are found. To do this, the trigram feature vector of every attachment needs to be processed independently.","In further embodiments, the space may be further reduced by determining the first set of documents based on second filter criteria. The first set of documents may be determined as a subset of a second set of documents returned in response to executing the query based on a cosine similarity between the first document and each document in the second set of documents.","In some embodiments, one or more user interfaces may be generated and configured for displaying information identifying selected ones of the plurality of documents as substantially similar to a first document selected via the one or more user interfaces in response to a query into the full text index for the plurality of documents indexing the second set of trigrams for each document in the plurality of documents based of a set of trigrams associated with the first document. In one aspect, an attachment match bitmap may be generated identifying which parts of selected ones of the plurality of documents correspond to an actual match with a first document. The one or more user interfaces may be configured for displaying which parts of the selected ones of the plurality of documents correspond to an actual match with the first document.","In another aspect, determining the second set of trigrams for each document in the plurality of documents based on the first set of trigrams for the document and first filter criteria may include filtering the first set of trigrams for each document to determine the second set of trigrams for the document as a predetermined number of most frequently occurring trigrams. A frequency table may be generated which indicates the frequency each trigram occurs in a document.","In at least one embodiment, a non-transitory computer-readable medium is provided storing computer-executable code for generating document signatures. The computer-readable medium can include code for receiving a plurality of documents, each document in the plurality of documents having a plurality of terms, code for generating a first set of trigrams for each document in the plurality of documents, each trigram in the first set of trigrams for a given document in the plurality of documents being a sequence in the given document of three terms in the plurality of terms of the given document, code for determining a second set of trigrams for each document in the plurality of documents based on the first set of trigrams for the document and first filter criteria, the second set of trigrams for a given document in the plurality of documents being a subset of the first set of trigrams for the given document and having one or more trigrams that satisfy the first filter criteria, and code for storing the second set of trigrams for each document in the plurality of documents as offsets into the document for each term of each trigram in the second set of trigrams.","In yet another embodiment, a e-discovery system can include a processor and a memory in communication with the processor and configured to store processor-executable instructions which configured the processor to receive a plurality of documents, each document in the plurality of documents having a plurality of terms, generate a first set of trigrams for each document in the plurality of documents, each trigram in the first set of trigrams for a given document in the plurality of documents being a sequence in the given document of three terms in the plurality of terms of the given document, determine a second set of trigrams for each document in the plurality of documents based on the first set of trigrams for the document and first filter criteria, the second set of trigrams for a given document in the plurality of documents being a subset of the first set of trigrams for the given document and having one or more trigrams that satisfy the first filter criteria, store the second set of trigrams for each document in the plurality of documents as offsets into the document for each term of each trigram in the second set of trigrams, generate, with the one or more processors associated with the one or more computer systems, a full text index for the plurality of documents, index the second set of trigrams for each document in the plurality of documents into the full text index, receive a first document, determine a set of trigrams associated with the first document, generate a query into the full text index for the plurality of documents based of the set of trigrams associated with the first document, determine a first set of documents in the plurality of documents in response to executing the query on the full text index, and generating one or more user interfaces configured for displaying information identifying selected ones of the first set of documents as substantially similar to the first document.","A further understanding of the nature of and equivalents to the subject matter of this disclosure (as well as any inherent or express advantages and improvements provided) should be realized in addition to the above section by reference to the remaining portions of this disclosure, any accompanying drawings, and the claims.","Finding similar or near-duplicate documents a given document is a well known problem with various solutions involving extracting feature vectors for each document and comparing feature vectors. In some approaches, documents are analyzed to extract certain feature vectors which are then compared with the feature vectors of all the documents in the corpus to find similar or near duplicate documents. In various embodiments, techniques are provided for reducing the space in which such comparison is required. Thus, in one aspect, the need to fetch each the feature vectors of every document and then compare it with the feature vector of the given document can be eliminated or reduced.","In various embodiments, a set of trigrams can be generated for each document in a plurality of documents processed by an e-discovery system. Each trigram in the set of trigrams for a given document is a sequence of three consecutive terms, with a sliding window of terms of the given document. A second set of trigrams for each document is then determined based on the first set of trigrams for the document and first filter criteria. The second set of trigrams for a given document is a subset of the first set of trigrams for the given document and includes one or more trigrams that satisfy the first filter criteria. A full text index is then generated for the plurality of documents and the second set of trigrams for each document are indexed into the full text index. The filtering is helpful to reduce the size of full text index storage space. Queries can be generated into the full text index based on trigrams of a document to determine other similar or near-duplicate documents. Thus, the space is reduced for which a feature vector comparison is required.","The embodiments discussed herein are illustrative of one or more examples of the present invention. As these embodiments of the present invention are described with reference to illustrations, various modifications or adaptations of the methods and\/or specific structures described may become apparent to those skilled in the art. All such modifications, adaptations, or variations that rely upon the teachings of the present invention, and through which these teachings have advanced the art, are considered to be within the scope of the present invention. Hence, the present descriptions and drawings should not be considered in a limiting sense, as it is understood that the present invention is in no way limited to only the embodiments illustrated.","Processing of Electronic Messages",{"@attributes":{"id":"p-0046","num":"0045"},"figref":"FIG. 1","b":["100","100","105","110","115","120","125","130","135","140","145","150","155","105","160","165","170","175","180","185","190","195","115","117","155","157"]},"Master index  can include hardware and\/or software elements that provide indexing of information associated with electronic documents, such as word processing files, presentation files, databases, e-mail message and attachments, instant messaging (IM) messages, Short Message Service (SMS) messages, Multimedia Message Service (MMS), OCRed files, or the like. Master index  may be embodied as one or more flat files, databases, data marts, data warehouses, and other repositories of data. Although the disclosure references specific examples using e-mail messages, the disclosure should not be considered as limited to only e-mail message or electronic messages only. The disclosure is applicable to other types of electronic documents as discussed above.","In various embodiments, master index  provides indexes configured for accessing and retrieving a variety of content, metadata, and attributes associated with electronic documents processed by processing system . For example, e-mail tables  can include hardware and\/or software elements that index information associated with e-mail messages processed by processing system . E-mail full text index  can include hardware and\/or software elements that provide a full text index of e-mail messages processed by processing system . The full text index may be an inverted index that enables fast searching of contents (e.g., headers and body), metadata, and attachments of e-mail messages processed by processing system .","Topic tables  can include hardware and\/or software elements that index e-mails and topics, concepts, or categories. Topic tables  may store relationships between predetermined, user-defined, or automatically derived topics, concepts, or categories and e-mail messages processed by processing system . In another aspect, topic table  may store relationships between related, similar, and near-duplicate e-mail messages. Cluster full text index  can include hardware and\/or software elements that provide a full text index of e-mail messages that have a cluster relationship. A cluster relationship may be defined by relationships based on statistical analysis of noun phrases, linguistic analysis, semantic analysis, or the like. Clusters of e-mail messages having close relationships satisfying predetermined criteria may be associated with topics in topic tables .","Distribution list full text index  can include hardware and\/or software elements that provide a full text index of e-mail messages associated with a distribution or conversation, such as mailing list or e-mail chain. Participant tables  can include hardware and\/or software elements that index information related to participants of a distribution or conversation (e.g., To-recipients, CC-recipients, BCC-recipients, etc.). Dimension tables  and fact tables  can include hardware and\/or software elements that index information facilitating further processing of e-mail messages processed by processing system , such as data warehouse processing, post-processing analytics, or the like.","MAPI module  is linked to e-mail servers  and to duplicate eliminator . MAPI module  can include hardware and\/or software elements configured for communicating with data repositories, such as e-mail servers . In this example, MAPI module  may interface directly with e-mail server  using one or more application programming interfaces (APIs). MAPI module  may incorporate or implement other interfaces, protocols, etc. for facilitating communication with a particular data repository. E-mail servers  can include hardware and\/or software elements that provide electronic messaging services, such as e-mail transport, storage, and retrieval. One example of mail server  is a computer system running Microsoft Exchange Server 2000 from Microsoft Corporation of Redmond, Wash. Mail server  may include other mail transport agents, mail user agents, and the like. E-mail messages may be stored on mail server  in a file, such as an Outlook PST file, a database, or the like.","Duplicate eliminator  can include hardware and\/or software elements that detect and eliminate redundant and\/or duplicative information from data repositories. In one example of operation, MAPI module  may retrieve e-mail messages from e-mail servers  in order to \u201ccrawl\u201d e-mail servers  to request e-mail messages. Duplicate eliminator  may filter redundant and\/or duplicate e-mail messages received from e-mail servers .","For example, a user A of mail server  may have sent an e-mail message addressed to user B and to user C. When duplicate eliminator  received e-mail messages obtained from mailboxes on mail server  for users A, B, and C, user A's mailbox contains the e-mail message as sent to user B and user C. Additionally, both user B's and user C's mailbox contains the respective user's copy of the e-mail message as received from user A. Duplicate eliminator potentially receives at least three copies of the e-mail message.","Duplicate eliminator  may determine two MD5 checksums for each e-mail message to \u201cidentify\u201d the e-mail message. Duplicate eliminator  may compute the MD5 checksums in response to message attribute data associated with an e-mail message, such as a sender e-mail address or sender identifier, sorted To-recipient e-mail addresses or To-recipient identifiers, sent time, alpha-numeric contents of subject, and the body text (e.g., body text size, contents of the body text, etc.). Other information not included in the e-mail message but associated with the message attribute data may also be used to compute the MD5 checksums. Other types of integrity, detection, and authenticity algorithms, such as cyclical redundancy checks (CRCs), hashes, and the like, may be used in addition to or in the alternative to the MD5 checksum.","In one example, a first \u201cstrict\u201d MD5 checksum can be computed that is unique and represents an exact match of a processed e-mail message. A second \u201crelaxed\u201d MD5 checksum can be computed that is non-unique or semi-unique. Duplicate eliminator  may compute a relaxed MD5 checksum using a portion or subset of the message attribute data used to compute the strict MD5 checksum. When duplicate eliminator receives a new e-mail, the new e-mail message may be processed (e.g., address normalization and cleansing) and a strict MD5 checksum may be computed and compared with previously computed strict MD5 checksums to determine whether the new e-mail message is unique. If the strict MD5 checksum for the new e-mail message is different, duplicate eliminator  then computes a relaxed MD5 checksum for the new e-mail message and compares the relaxed MD5 checksum to previously computed relaxed MD5 checksums.","If the relaxed MD5 checksum for the new e-mail message is different, then the new-e-mail address is not a duplicate. If the relaxed MD5 checksum for the new e-mail message is the same as one or more previously computed relaxed MD5 checksums, duplicate eliminator  may apply one or more rules or policies to further eliminate possible duplicate e-mail messages. The rules or polices may be based on time differences, header processing, and the like, and also the addition of trailing content, such as disclaimers, names of attachment files, and the like.","Buffer manager  is linked to duplicate eliminator  and indexer . Buffer manager  can include hardware and\/or software elements that manage data communications. Buffer manager  may buffer or otherwise manage production and consumption of e-mail messages retrieved while \u201ccrawling\u201d data repositories. In one embodiment, buffer manager  may create batches of e-mail messages. In one aspect, batching the e-mail messages may allow indexer  to apply batch-processing techniques to message attribute data associated with a batch of e-mail messages. Buffer manager  may create batches of 10, 50, or 100 e-mail messages.","Indexer  is linked to master index . Indexer  can include hardware and\/or software elements that index electronic documents. Indexer  may include functionality for decomposing documents into constituent parts and populating master index . For example, indexer  may process an e-mail message to parse header and body fields to retrieve message content and generate metadata associated with the e-mail message. Indexer  may further perform other types of processing, such as surface processing, statistical processing, linguistic processing, semantic processing, or the like.","Advantageously, electronic document processing system  can provide a user or organization with access to indexed electronically stored information to assist in reporting requirements or gathering information for the purposes of electronic discovery and information governance. After \u201ccrawling\u201d data repositories to retrieve documents and the like, processing system  can automatically process and index the retrieved information. Processing system  can then allow the user or organization to readily and quickly search and query the processed information for a variety of purposes. Processing system  further provides other post-processing features to enhance the discovery and presentation of relevant information to the user or organization.","For example, thread analyzer  is linked to master index . Thread analyzer  can include hardware and\/or software elements that organize documents into one or more discussions or conversations. An e-mail thread can be a series or sequence of one or more e-mail messages that form a logical discussion or communication. E-mail messages within an e-mail thread may be related by sender address, recipient address, topic, and time. E-mail messages may further be related based on forwarding replies, CC-recipients, BCC-recipients, and the like. Thread analyzer  may determined groups of documents that are related to a discussion or conversation as well as determine orderings or position of e-mail messages in e-mail threads.","In another example, topic classifier  is linked to master index . Topic classifier  can include hardware and\/or software elements that determine topics, concepts, or categories for an electronic document. Topic classifier  may determine a topic of an e-mail message based on the subject header or in response to the content of the body of an e-mail message. Topic classifier  may further determine a topic of an e-mail message based on statistical, linguistic, or semantic analysis. Topic classifier  may associate an e-mail message with a given topic, classifier, and\/or category. The topic may be predefined, user-defined, or automatically created based on based on statistical, linguistic, or semantic analysis.","In another example, analytics ETL module  is linked to master index . Analytics ETL module  can include hardware and\/or software elements that provide an interface accessing master index . In one example, analytics ETL module  provides an interface for importing and\/or extracting data between master index  and one or more external data sources. Analytics ETL module  may provide an interface for transforming data in master index  (e.g., cleansing, aggregation, summarization, integration, etc.) and loading the data into some form of data warehouse for further analysis and processing.","In yet another example, directory interface  is linked to master index  and directory servers . Directory interface  can include hardware and\/or software elements that access information stored in a directory. A directory can include any database of information associated with objects, such as users or computer hosts. In various embodiments, directory servers  include one or more directory servers (e.g., directory server ) running Active Directory by Microsoft Corporation of Redmond, Wash. In other embodiments, other types of directory servers and\/or services may be used such as Lightweight Directory Access Protocol (LDAP) servers, Identity Management servers, and the like. In various embodiments, examples of information stored in directory servers  can include \u201corganizational\u201d or \u201ccorporate\u201d data, such as department identifiers associated with a user or computer host, a group identifier associated with a user, a corporate or departmental title associated with a user, telephone and address information, and security information.",{"@attributes":{"id":"p-0064","num":"0063"},"figref":"FIG. 2","b":["200","200","202","204","206","208","210","202","204","206","208","210","202","204","206"]},"Portal  includes software elements for accessing and presenting information provided by the indexer . In this example, the portal  includes web applications  communicatively coupled to information gathering and presentation resources, such as a Java Server Page (JSP) module , a query engine , a query optimization module , an analytics module , and a domain templates module .","Indexer  includes software elements for processing and storing e-mail messages. The indexer  includes metadata , full text indices , thread analysis , group effects , and topics .","Crawler  includes software elements for retrieving e-mail messages from an e-mail repository. Some examples of an e-mail repository are an e-mail server (e.g., one of the mail servers  of ), a Post Office Protocol (POP) enabled computer server, an Internet Message Access Protocol (IMAP) enabled computer server, and files, such as PST files, UNIX style maildirs\/mbox formats, and the like. In this example, the crawler  includes Reference Information Storage System (RISS) module , Enterprise Vault Software (EV) module , MAPI module , PST module , Directory Services (ADS) module , and a Microsoft Exchange Server Mailbox Merge Wizard (ExMerge) module .","Accordingly, software components  can provide a user or organization with access to indexed electronically stored information to assist in reporting requirements or gathering information for the purposes of electronic discovery and information governance. After \u201ccrawling\u201d electronically stored information to retrieve documents and the like, software components  can automatically process and index the retrieved information. Software components  can then allow the user or organization to readily and quickly search and query the processed information for a variety of purposes.","Recall of Processed Documents","Early assessment is a growingly important phase of e-discovery and information governance during which complete scope and extent of relevant information in a collection is often unknown. Traditional keyword and Boolean searches often play a big part in an assessment, but they are not always enough to hone in on the specifics of a case. In one aspect, electronic document processing system  can offer an additional approach to improve the recall of related and relevant documents based on statistical and linguistic analysis of document content.","In various embodiments, processing system  may be used for electronic discovery. Electronic discovery almost always involves searching for relevant and responsive documents. One or more technologies may be applied for the task. Keyword based search has been one traditional method of searching, but its limitations have been well understood and documented [1]. At their most basic level, concept search technologies are designed to overcome some limitations of keyword search.","When applied to document discovery, traditional Boolean keyword search often results in sets of documents that include non-relevant items (false positives) or that exclude relevant terms (false negatives). This is primarily due to the effects of synonymy (different words with similar meanings) or polysemy (same word with multiple meanings). For polysemes, an important characteristic requirement is that they share the same etymology but their usage has evolved it into different meanings. Moreover, there are also situations where words that do not share the same etymology have different meanings (e.g., river bank vs. financial bank), in which case they are classified as homonyms. In addition to the above word forms, unstructured text content, and especially written text in emails and instant messages contain user-created code words, proper name equivalents, contextually defined substitutes, and prepositional references etc., that mask the document from being indentified using Boolean keyword search. Even simple misspellings, typos and OCR scanning errors can make it difficult to locate relevant documents.","Also common is an inherent desire of speakers to use a language that is most suited from the perspective that is convenient for the speaker. This can be illustrated using the event which the victim's side called the event in question an \u201caccident\u201d or a \u201cdisaster\u201d while the plaintiff's side called it an \u201cevent\u201d, \u201csituation\u201d, \u201cincident\u201d, \u201cproblem\u201d, \u201cdifficulty\u201d, etc. The combination of human emotion, language variation, and assumed context makes the challenge of retrieving these documents purely on the basis of Boolean keyword searches a nearly impossible task.","Concept based searching is a very different type of search when compared to Boolean keyword search. The input to concept searching is one or more words that allow the investigator or user to express a concept. The search system is then responsible for identifying other documents that belong to the same concept. All concept searching technologies attempt to retrieve documents that belong to a concept (reduce false negatives and improve recall) while at the same time not retrieve irrelevant documents (reduce false positives and increase precision).","Thus, concept search, as applied to electronic discovery, is a search using meaning or semantics. While it is very intuitive in evoking a human reaction, expressing meaning as input to a system and applying that as a search that retrieves relevant documents is something that requires a formal model. Technologies that attempt to do this formalize both the input request and the model of storing and retrieving potentially relevant documents in a mathematical form. There are several technologies available for such treatment, with two broad initial approaches.","First are unsupervised learning systems. These systems convert input text into a semantic model, typically by employing a mathematical analysis technique over a representation called vector space model. This model captures a statistical signature of a document, its terms and their occurrences. A matrix derived from the corpus is then analyzed using a Matrix decomposition technique. These systems are unsupervised in the sense that they do not require a training set where data is pre-classified into concepts or topics. Also, such systems do not use ontology or any classification hierarchy and rely purely on the statistical patterns of terms in documents.","These systems generally derive their semantics through a representation of co-occurrence of terms. One primary consideration is maintaining this co-occurrence in a form that reduces impact of noise terms while capturing the essential elements of a document. For example, a document about an automobile launch may contain terms about automobiles, their marketing activity, public relations etc., but may have a few terms related to the month, location and attendees, along with frequently occurring terms such as pronouns and prepositions. Such terms do not define the concept automobile, so their impact in the definition must be reduced. To achieve such end result, unsupervised learning systems represent the matrix of document-terms and perform a mathematical transformation called dimensionality reduction.","First are supervised learning systems. In the supervised learning model, an entirely different approach is taken. A main requirement in this model is supplying a previously established collection of documents that constitutes a training set. The training set contains several examples of documents belonging to specific concepts. The learning algorithm analyzes these documents and builds a model, which can then be applied to other documents to see if they belong to one of the several concepts that is present in the original training set. Thus, concept searching task becomes a concept learning task that may use one of the following techniques: Decision Trees, Na\u00efve Bayesian Classifier, and Support Vector Machines.","While supervised learning is an effective approach during document review, its usage in the context of searching has significant limitations. In many situations, a training set that covers all possible outcomes is unavailable and it is difficult to locate exemplar documents. Also, when the number of outcomes is very large and unknown, such methods are known to produce poor results.","As noted earlier, concept searching techniques are most applicable when they can reveal semantic meanings of a corpus without a supervised learning phase. One method includes Singular Value Decomposition (SVD) also is known with Latent Semantic Indexing (LSI). LSI is one of the most well-known approaches to semantic evaluation of documents. This was first advanced at Bell Labs (1985) and later developed by many information retrieval researchers [3].","The essence of the approach is to build a complete term-document matrix, which captures all the documents and the words present in each document. Typical representation is to build an N\u00d7M matrix where the N rows are the documents, and M columns are the terms in the corpus. Each cell in this matrix represents the frequency of occurrence of the term at the \u201ccolumn\u201d in the document \u201crow\u201d.","Such a matrix is often very large\u2014document collections in the millions and terms reaching tens of millions are not uncommon. Once such a matrix is built, the mathematical technique known as SVD reduces the dimensionality of the matrix into a smaller size. This process reduces the size of the matrix and captures the essence of each document by the most important terms that co-occur in a document. In the process, the dimensionally reduced space represents the \u201cconcepts\u201d that reflect the conceptual contexts in which the terms appear.","Another method includes Principal Component Analysis (PCA) which is very similar to latent semantic analysis in that a set of highly correlated artifacts of words and documents in which they appear is translated into a combination of the smallest set of uncorrelated factors. These factors are the principal items of interest in defining the documents, and are determined using a SVD technique. The mathematical treatment, application and results are similar to LSI. A variation on this, called Independent Component Analysis (ICA) is a technique that works well with data of limited variability. However, in the context of electronic discovery documents where data varies widely, this results in poor performance.","Yet another method includes Non-negative Matrix Factorization (NMF) which is most useful for classification and text clustering where a large collection of documents are forced into a small set of clusters. NMF constructs a document-term matrix similar to LSI and includes the word frequency of each term. This is factored into a term-feature and feature-document matrix, with the features automatically derived from the document collection. The process also constructs data clusters of related documents as part of the mathematical reduction. An example of this research takes the Enron email corpus and classifies the data using NMF into 50 clusters [2].","Latent Dirichlet Allocation (LDA) is a technique that combines elements of Bayesian learning and probabilistic latent semantic indexing. In this sense, it relies on a subset of documents pre-classified into a training set, and unclassified documents are classified into concepts based on a combination of models from the training set [10].","Although theoretically attractive and experimentally successful, word space models are plagued with efficiency and scalability problems. This is especially true when the models are faced with real-world applications and large scale data sets. The source of these problems is the high dimensionality of the context vectors, which is a direct function of the size of the data. If document-based co-occurrences is used, the dimensionality equals the number of documents in the collection. If word-based co-occurrences is used, the dimensionality equals the vocabulary, which tends to be even bigger than the number of documents. This means that the co-occurrence matrix will soon become computationally intractable when the vocabulary and the document collections grow.","Nearly all the technologies build a word space by building a word-document matrix with each row representing a document and column representing a word. Each cell in such a matrix represents the frequency of occurrence of the word in that document. All these technologies suffer from a memory space challenge, as these matrices grow to very large sizes. Although many cells are sparse, the initial matrix is so large that it is not possible to accommodate the computational needs of large electronic discovery collections. Any attempt to reduce this size to a manageable size is likely to inadvertently drop potentially responsive documents. Another problem with all of these methods is that they require the entire semantic space to be constructed ahead of time, and are unable to accommodate new data that would be brought in for analysis. In most electronic discovery projects, it is routine that some part of the data is brought in as a first loading batch, and once review is started, additional batches are processed.","In various embodiments, a semantic space is generated using a variation of Reflective Random Indexing (RRI) [4, 5, 6]. In one aspect, a semantic vector space model is provided to achieve the same dimensionality reduction espoused by LSI, without requiring the mathematically complex and intensive SVD and related matrix methods. In some embodiment, a set of term vectors and a set of document vectors are created. These vectors may be built using a scan of the document and term space with several data normalization steps. A semantic space build may occur seamlessly without any user intervention, such as during indexing or analytics processing as discussed above. Case data collection may then ready for culling, early case assessment (ECA), search, review, production, or the like.","Generation of a Semantic Space","In various embodiments, processing system  generates a semantic space with semantic vectors as term vectors and document vectors. Processing system  may generate a term vector for each term in a corpus of information. Processing system  then may generate a document vector for each document in the corpus. As noted earlier, one primary characteristic of the semantic space is a dimensionality reduction of a term-document matrix. Each row in a term-document matrix represents all documents in which a term appears. Each column in the matrix represents all terms that a document contains. Therefore, semantic relatedness may be expressed in the connectedness of each matrix cell. For example, two documents that share the same set of terms may be connected through a direct connection. It is also possible for two documents to be connected using an indirect reference.",{"@attributes":{"id":"p-0089","num":"0088"},"figref":["FIG. 3","FIG. 1"],"b":["300","310","310","310","310","320","310","330","340"]},"Semantic vector analysis module  includes hardware and\/or software elements configured for generating semantic space . For example, semantic vector analysis module  may identify terms found in each document of a corpus and all the documents in which a term is found. Semantic vector analysis module  then may build both term-to-term (e.g., term vectors ) and term-to-document (e.g., document vectors ) vector projections in semantic space . For example, semantic vector analysis module  may examine subject, body, quoted text regions of email messages indexed in document full text index  and content regions of the email messages indexed in document full text index .","Table 1 below illustrates a term document matrix for fifteen terms and six documents that may be generated by semantic vector analysis module . There are several terms related to another through a direct connection\u2014\u201cinvestments\u201d and \u201cmanhattan\u201d for example, through the term \u201cdiamond\u201d. Indirect connections are further evident between terms such as \u201cpoker\u201d and \u201cinvestments.\u201d",{"@attributes":{"id":"p-0092","num":"0091"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"7"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"42pt","align":"center"}}],"thead":{"row":[{"entry":[{},"TABLE 1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]},{"entry":[{},"d1","d2","d3","d4","d5","d6"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"6","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"8"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"49pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"6","colwidth":"14pt","align":"center"}},{"@attributes":{"colname":"7","colwidth":"42pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"diamond","1","1",{},"1",{},"1"]},{"entry":[{},"investments","1"]},{"entry":[{},"fund",{},{},"1"]},{"entry":[{},"apple",{},{},"1"]},{"entry":[{},"hedge",{},{},"1"]},{"entry":[{},"manhattan",{},"1"]},{"entry":[{},"poker",{},{},{},"1","1"]},{"entry":[{},"hand",{},{},{},"1"]},{"entry":[{},"ace",{},{},{},"1"]},{"entry":[{},"baseball",{},{},{},{},{},"1"]},{"entry":[{},"yankees",{},{},{},{},{},"1"]},{"entry":[{},"office",{},"1"]},{"entry":[{},"stock",{},{},"1"]},{"entry":[{},"table",{},{},{},{},"1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"7","align":"center","rowsep":"1"}}]}]}}]}}},"As can be observed, the above term-document matrix is a very sparse matrix. This can grow to very large sizes for most document analysis cases. In various embodiments, dimensionality reduction can be applied to reduce the sparse matrix into a manageable size. This achieves two purposes. First, it enables large cases to be processed in currently available computing platforms. Second, and more importantly, it captures the semantic relatedness in a mathematical model.","To further improve the quality of semantic vectors, semantic vector analysis module  may apply certain filters. In one example, semantic vector analysis module  may apply one or more rules to remove terms with low Inverse Document Frequency (IDF). Terms with a low IDF may include terms that are very common among a large number of documents which may not be very helpful in describing the semantic content of documents. In another example, semantic vector analysis module  may apply one or more rules to remove terms with very low global term frequency (TF). Terms less than a small number of global TF also may not help, since they are limited to just a few documents. In a still further example, semantic vector analysis module  may apply one or more rules to remove terms with language specific characters or unusual characters as these also may not be effective in defining a concept.","In various embodiments, in building semantic spaces, semantic vector analysis module  may retain original terms without any stemming applied. In one aspect, by not requiring stemming, performance of building these vector spaces may be helped in that the process is not impacted by language identification and language stemming performance. In further embodiments, document full text index  may include a variety of partitions. Semantic vector analysis module  may process each partition independently and\/or separately. One positive outcome of per-partition vector space is that the semantic vector building phase scales linearly by the number of partitions in the index. One negative outcome is that search results need to be merged, and clustering of documents may produce multiple clusters, one for each partition. An alternative design would be to build a single vector space for all terms and documents put together.",{"@attributes":{"id":"p-0096","num":"0095"},"figref":["FIG. 4","FIG. 3"],"b":["350","350","410","420","410","420","350","430","440","1","2","3","450","460"]},"In further embodiments, vector assignments may initially be chosen essentially at random.  is a block diagram illustrating randomly generated document vectors of semantic space  according to Table 1 in one embodiment according to the present invention. One or more vectors may be derived from a random starting point. The vectors may then be refined through training cycles or through other incremental processing.  is a block diagram illustrating document vectors of a semantic space as initialized using a variation of Reflective Random Indexing in one embodiment according to the present invention. In this example, a document vector for each document in a corpus of information may be assigned a series of sequence of random values (i.e., a document vector is assigned a random collection of 200 float values). Specific randomly chosen numbers may be at assigned at each position. In some aspect, the actual numbers assigned are not important as is selecting a unique or semi-unique random pattern for each document.","In some embodiment, after initializing each document vector of a document in a corpus to random values, each document vector represents an initial signature of the document. Term vectors can then be computed by iterating through all the terms of the documents in the corpus. For each term of a given document, processing system  can examine all documents in which the term appears. As an example, the word \u201cdiamond\u201d appears in Table 1 in documents d, d, d, and d. Each corresponding document vector then can be merged into a term vector for the word \u201cdiamond.\u201d In one aspect, the merging of the document vectors uses the initially determined random values of the document vector corresponding to a term and scales the values by the frequency of the term in each document as in equation (1):",{"@attributes":{"id":"p-0099","num":"0098"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"t","mrow":{"mi":["i","j"],"mo":","}},"mo":"=","mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"k","mo":"=","mn":"0"},"mi":"l"},"mo":"\u2062","mrow":{"msub":[{"mi":["n","k"]},{"mi":"d","mrow":{"mi":["k","j"],"mo":","}}],"mo":"\u2062"}}}},{"mrow":{"mo":["(",")"],"mn":"1"}}]}}}}},"Each term k's frequency in the document nweighs in for each document vector's position. Thus, this operation projects all the documents that a term appears in, and condenses it into the dimensions allocated for that term. As is evident, this operation is a fast scan of all terms and their document positions. Using various indexing schemes (e.g., Lucene API TermEnum and TermDocs), a collection of term vectors can be derived very easily.","Once term vectors are computed, these term vectors can be projected on to document vectors. For example, processing system  may then compute new document vectors from the term vectors, replacing the former random assignments with a new vector. For example, processing system  computes a new document vector for each document by examining all the terms of the document and merging all the term vectors into a new document vector (e.g., using a vector sum). The merging of term vectors may take into account the frequency of each term in the document and the frequency of the document in the entire corpus.","In some embodiments, the above process constitutes a single training cycle. Processing system  may repeat the process for a second cycle allowing the vectors to converge to a more stable point. Specifically, term vectors can again be computed by iterating through all the terms of the document. For each term of a given document, processing system  can examine all documents in which the term appears. Each corresponding document vectors then can be merged into the term vector. In this aspect, the merging of the document vectors uses term vectors determined in each previous training cycle.","Accordingly, by constructing a semantic vector space for a corpus of documents, processing system  can generate an output space that captures most if not all essential co-occurrence patterns embodied in the corpus. Thus, each term vector maintains most if not all of the documents in which a corresponding term appears and each document vector maintains most if not all of the terms present in a corresponding document. Together, a co-occurrence matrix derives the semantic position of the documents in the corpus.  is graph  illustrating semantic space  generated according to Table 1 in one embodiment of the present invention.",{"@attributes":{"id":"p-0104","num":"0103"},"figref":["FIGS. 7A and 7B","FIGS. 7A and 7B","FIGS. 7A and 7B","FIG. 7A"],"b":["700","700","700","705"]},"In step , a set of random document vectors are received. As discussed above, in various embodiments, a set of random document vectors may be generated with each document of a corpus using Random Indexing. The general idea behind word space models is to use distributional statistics to generate high-dimensional vector spaces, in which words are represented by context vectors whose relative directions are assumed to indicate semantic similarity. This assumption is motivated by the distributional hypothesis, which states that words with similar meanings tend to occur in similar contexts. According to this hypothesis, if two words are observed that constantly occur with the same contexts, it may be assuming that the two words mean similar things. The two words not need to occur with each other; only that the words co-occur with the same other words. In standard word space methodology, a high-dimensional vector space is produced by collecting the data in a co-occurrence matrix F, such that each row Fw represents a unique word w and each column Fc represents a context c, typically a multi-word segment such as a document, or another word. In the former case, where the columns represents documents, the matrix may be a words-by-documents matrix. In the latter case where the columns represents words, the matrix may be called a words-by-words matrix. LSA is an example of a word space model that uses document-based co-occurrences.","The cells Fwc of a co-occurrence matrix record the frequency of co-occurrence of word w and document or word c. As an example, for document-based co-occurrences, and if a given word is observed three times in a given document in a corpus, a 3 may be entered in a corresponding cell in the co-occurrence matrix. By the same token, for word-based co-occurrences, and if two given words are observed to occur close to each other five times in a corpus, a 7 may be entered in a corresponding cell of the co-occurrence matrix. Frequency counts are usually normalized and weighted in order to reduce the effects of high frequency words, and, in case document-based co-occurrences are used, to compensate for differences in document size.","The point of the co-occurrence matrix is that the rows Fw effectively constitute vectors in a high-dimensional space, such that the elements of the vectors are (normalized) frequency counts, and the dimensionality of the space is determined by the number of columns in the matrix, which is identical to the number of contexts (i.e. words or documents) in a corpus. We call the vectors context or document vectors, since they represent the contexts or documents in which words have occurred. In effect, the context or document vectors are representations of the distributional profiles of words, which means that a distributional similarity may be defined between words in terms of vector similarity. By virtue of the distributional hypothesis, this makes it very straight-forward to compute semantic similarity between words, such that a comparison is made between context vectors using any of a wide range of possible vector similarity measures, such as the cosine of the angles between the vectors, or the City-Block metric.","Although theoretically attractive and experimentally successful, word space models are plagued with efficiency and scalability problems. This is especially true when the models are faced with real-world applications and largescale data sets. One source of these problems is the high dimensionality of context or document vectors, which is a direct function of the size of the data. For document-based co-occurrences, the dimensionality equals the number of documents in the collection, and for word-based co-occurrences, the dimensionality equals the vocabulary, which tends to be even bigger than the number of documents. This means that the co-occurrence matrix will soon become computationally intractable when the vocabulary and the document collection grow. Another problem with the co-occurrence matrix is that a majority of the cells in the matrix will be zero due to the sparse data problem. That is, only a fraction of the co-occurrence events that are possible in the co-occurrence matrix will actually occur, regardless of the size of the data. A tiny amount of the words in language are distributionally promiscuous; the vast majority of words only occur in a very limited set of contexts. In a typical co-occurrence matrix, more than 99% of the entries are zero.","In order to counter problems with very high dimensionality and data sparseness, most well-known and successful models, like LSA, use statistical dimension reduction techniques. Standard LSA uses truncated Singular Value Decomposition (SVD), which is a matrix factorization technique that can be used to decompose and approximate a matrix, so that the resulting matrix has much fewer columns\u2014typically only a couple of hundred\u2014and is much denser. It should be noted that SVD is not the only way to achieve this result. There are a number of related dimension reduction techniques that are used in word space research (e.g. principal component analysis and independent component analysis), and they all share the same basic methodology: first sample the data in a standard co-occurrence matrix, and then transform it into a much smaller and denser representation.","There are (at least) three reasons to avoid using dimension reduction techniques of this type:","Dimension reduction techniques such as SVD tend to be computationally very costly, with regards to both memory consumption and execution time. For many applications, and especially for large vocabularies and large document collections, it is not practically feasible to compute an SVD.","Dimension reduction is typically a one-time operation, which means that the entire process of first constructing the co-occurrence matrix and then transforming it has to be done from scratch, every time new data is encountered. The inability to add new data to the model is a serious deficiency, as many applications require the possibility to easily update the model.","Most importantly, these dimension reduction techniques fail to avoid the initial huge co-occurrence matrix. On the contrary, they require initial sampling of the entire data. There are two problems with this. First, it is the initial co-occurrence matrix that is computationally cumbersome. In order to make the models efficient and scalable, this step should be avoided, rather than handled by ad hoc solutions. Second, initial sampling of the entire data means that there can be no intermediary results. It is only after both constructing and transforming the co-occurrence matrix that any processing can begin.","As an alternative to LSA-like models that first construct a huge co-occurrence matrix and then use a separate dimension reduction phase, processing system  may use an incremental word space model called Random Indexing, based on Pentti Kanerva's work on sparse distributed representations. The basic idea is to accumulate context vectors based on the occurrence of words in contexts. This technique can be used with any type of linguistic context, is inherently incremental, and does not require a separate dimension reduction phase.","In some embodiments, a Random Indexing technique can be described as a two-step operation:","First, each context (e.g. each document or each word) in a corpus of information is assigned a unique and randomly generated representation called an index vector. These index vectors are sparse, high-dimensional, and ternary, which means that their dimensionality (d) is on the order of thousands, and that they consist of a small number of randomly distributed +1 s and \u22121 s, with the rest of the elements of the vectors set to 0.","Then, context vectors are produced by scanning through the text, and each time a word occurs in a context (e.g. in a document, or within a sliding context window), that context's d-dimensional index vector is added to the context vector for the word in question. Words are thus represented by d-dimensional context vectors that are effectively the sum of the words' contexts.","In the Random Indexing approach, a standard co-occurrence matrix F of order w\u00d7c is produced by using unary index vectors of the same dimensionality c as the number of contexts, and then the resulting context vectors are collected in a matrix. Such unary index vectors would consist of a single 1 in a different position for each context, and would thus be orthogonal. By contrast, the d-dimensional random index vectors are only nearly orthogonal.","In step , a first term for which to generate a term vector is selected. In step , all documents in which the term appears are determined. All documents in which a term appears may be obtained using one or more indexes as discussed above. In step , a first document in which the term appears is selected.","In step , frequency of the term in the document is determined. In step , a document vector of the selected document is added to a term vector for the selected term. The document vector may be scaled by the term frequency.",{"@attributes":{"id":"p-0121","num":"0120"},"figref":["FIG. 7A","FIG. 7B","FIG. 7A"],"b":["740","745","730","750"]},"In step , a determination is made whether any terms remain for which to generate a term vector. If one or more terms remain, in step , the next term is selected. Processing is then repeated for the next term in step  of . If no terms remain for which to generate a term vector,  ends in step .",{"@attributes":{"id":"p-0123","num":"0122"},"figref":["FIGS. 8A and 8B","FIGS. 8A and 8B","FIGS. 8A and 8B","FIG. 8A"],"b":["800","800","800","805"]},"In step , a set of empty document vectors are received. In various embodiments, all previous document vectors generated using Random Indexing are initialized to zero. In step , a first document for which to generate a document vector is selected. In step , all terms that appear in the selected document are determined.","In step , a first term that appears in the document is selected. In step , frequency of the term in the document is determined. In step , frequency of the term in a corpus (e.g., one that includes the selected document) is determined. In step , a term vector of the selected term is added to a document vector for the selected document. The term vector may be scaled by the determined document term frequency and the determined corpus term frequency.",{"@attributes":{"id":"p-0126","num":"0125"},"figref":["FIG. 8A","FIG. 8B","FIG. 8A"],"b":["845","850","830","855"]},"In step , a determination is made whether any documents remain for which to generate a document vector. If one or more documents remain, in step , the next document is selected. Processing is then repeated for the next document in step  of . If no documents remain for which to generate a document vector,  ends in step .","In various embodiments, the processing of , B, A, and B may constitute one training cycle as exemplified in . The steps may be repeated as many cycles as needed. Typically, two training cycles may be sufficient to get a good representation of term and document spaces. In another aspect, processing system  may start with existing document vectors and term vectors and add new terms\/document into it. In such a case, the additional documents simply add to the semantic space by reinforcing existing term and document vectors.","In some embodiments, processing system  may further capture positional information of terms. System  may use positional information and build a term and its neighbors as a way of limiting how a term's proximity defines its context. Accordingly, during indexing, processing system  may further capturing special positional information. Processing system  then may build a term-to-term projection based on the positional information.","Accordingly, in various embodiments, a semantic space may be build using a linear scan of terms, followed by a scan of documents. In contrast to LSA and other dimensionality reduction techniques, processing system  requires much less memory and CPU resources for semantic space construction. This is primarily because matrix operations such as singular value decomposition (SVD) are computationally intensive, and requires both the initial term-document matrix and intermediate matrices to be manipulated in memory. In contrast, semantic vectors can be built for a portion of the term space with a portion of the index. It is also possible to scale simply by employing persistence to disk at appropriate batching levels, thus scaling to unlimited term and document collections. Additionally, in other aspects, processing system  may more easily parallelize vector space building for distribution across multiple systems. This allows parallel computation of the space, allowing for a distributed algorithm to work on multiple term-document spaces simultaneously. This can dramatically increase the availability of concept search capabilities to very large matters, and within time constraints that are typically associated with large electronic discovery projects.","Moreover, processing system  may build a semantic space incrementally, as new batches of data are received, without having to build the entire space from scratch. This is a very common scenario in electronic discovery, as an initial batch of document review needs to proceed before all batches are collected. It is also fairly common for the scope of electronic discovery to increase after early case assessment. Finally, processing system  may be tune a semantic space using parameter selection such as dimension selection, similarity function selection and selection of term-term vs. term-document projections. These capabilities allow electronic discovery project teams to weigh the costs of computational resources against the scope of documents to be retrieved by the search. If a matter requires a very narrow interpretation of relevance, the concept search algorithm can be tuned and iterated rapidly. Like other statistical methods, semantic spaces retain their ability to work with a corpus including multiple languages, multiple data types, encoding types etc., which is a key requirement for e-discovery. This is because processing system  does not rely on linguistic priming or linguistic rules for its operation.","Resource requirements for building a semantic vector space is an important consideration. Time and space complexity of semantic space algorithms can be evaluated as a function of corpus size, both from the initial construction phase and for follow-on search and retrievals. Performance measurements for both aspects were characterized for four different corpora, as indicated in Table 2.",{"@attributes":{"id":"p-0133","num":"0132"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"center"}}],"thead":{"row":[{"entry":"TABLE 2"},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}},{"entry":[{},{},{},"EDRM",{}]},{"entry":[{},{},{},"Enron ","TREC"]},{"entry":[{},"Demo","Reuters","Data","Tobacco"]},{"entry":["Corpus","case","Collection","set","Corpus"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"1","colwidth":"98pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"21pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"28pt","align":"char","char":"."}},{"@attributes":{"colname":"5","colwidth":"35pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["Number of PST Files","12","\u2014","171","\u2014"]},{"entry":["Number of Emails","19302","\u2014","428072","\u2014"]},{"entry":["Number of Attachments and\/or","2917","21578","305508","6,270,345"]},{"entry":["Files",{},{},{},{}]},{"entry":["Number of Term Vectors (email)","49210","\u2014","251110","\u2014"]},{"entry":["Number of Document Vectors","17261","\u2014","402607","\u2014"]},{"entry":["(email)",{},{},{},{}]},{"entry":["Number of Term Vectors","57964","63210","189911","3,276,880"]},{"entry":["(attachments)",{},{},{},{}]},{"entry":["Number of Doc Vectors","2153","21578","305508","6,134,210"]},{"entry":["(attachments)",{},{},{},{}]},{"entry":["Number of Clusters (email)","542","\u2014","3996","\u2014"]},{"entry":["Number of Clusters (attachments)","105","134","2856","210,789"]},{"entry":{"@attributes":{"namest":"1","nameend":"5","align":"center","rowsep":"1"}}}]}}]}}},"As can be observed, term vectors and document vectors vary based on the characteristics of the data. While the number of document vectors closely tracks the number of documents, the number of term vectors grows more slowly. This is the case even for OCR-error prone ESI collections, where the term vector growth moderated as new documents were added to the corpus.","In some aspects, space complexity of a semantic space model is linear with respect to the input size. Partitioning of a problem across certain term boundaries and persisting the term and document vectors can provide for increased scalability. For example, a 4 million document collection with 20 million terms, processing system  may break apart the term collection into 20 sub-spaces of a million terms each. Since term vector stores do not rely on other term vectors\u2014they only rely on document vectors, the space can be partitioned effectively. For the above case, processing system  may implement scaling by sharding the terms in a multi-pass algorithm. Since both the semantic space construction and its use during concept search are scalable by use of external disk-resident structures, memory requirements are modest. One implementation of the algorithm requires memory space for tracking one million term and document vectors, which is about 2 GB, for a semantic vector dimension of 200.","Time for semantic space construction is linear on the number of terms and documents. For a very large corpus, the space construction requires periodic persistence of partially constructed term and document vectors and their clusters. A typical configuration persists term vectors for each million terms, and documents at each million documents. As an example, the NIST TextRetrieval Conference (TREC) Legal Track supplied tobacco corpus would require 4 term sub-space constructions, with six document partitions, yielding 24 data persistence invocations. If we consider the number of training cycles, each training cycle repeats the same processes. As an example, the TREC tobacco corpus with two training cycles involves 48 persistence invocations. For a corpus of this size, persistence adds about 30 seconds for each invocation.",{"@attributes":{"id":"p-0137","num":"0136"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"center"}}],"thead":{"row":[{"entry":[{},"TABLE 3"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},{},"Vector","Cluster"]},{"entry":[{},{},"Construction","Construction"]},{"entry":[{},"Performance Item","(minutes)","(minutes)"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":{"entry":{}}}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"91pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"70pt","align":"char","char":"."}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Demo case","2","1"]},{"entry":[{},"Reuters-21578 Collection","1","1"]},{"entry":[{},"EDRM Enron data set","40","15"]},{"entry":[{},"TREC Tobacco Corpus","490","380"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}}},"Table 3 provides measurements that were taken on a commodity Dell PowerEdge R system, with two Quad Xeon 4500 processors at 2.1 GHz CPU and 32 GB amount of memory.","Partitioning and Clustering","In still further embodiments, processing system  may partition or cluster document and\/or term vector spaces.  is an illustration of semantic space  of  divided into one or more clusters in one embodiment according to the present invention. Processing system  may perform text clustering based on semantic vectors to generate one or more cluster representing \u201cconcepts\u201d or \u201ctopics.\u201d For example processing system  may identify document clusters based on document vectors and create concepts or topics from these document clusters. Naming of concepts or topics may be based on frequent terms appearing in members of the cluster.","One valuable usage item is the centroid of each cluster, which is a centroid representation of all cluster members. For example, cluster  is associated with centroid  which is a vector representing the \u201ccenter\u201d of all documents that are members of cluster . All documents that are members of cluster  also all within radius . Radius  may be a maximum radius around centroid  that encompasses the document vectors of all documents that are members of cluster .","In some aspects, a clustering algorithm may be used that constructs hierarchical clusters.  is an illustration of one or more cluster hierarchies in one embodiment according to the present invention. In one example, processing system  may incorporate an algorithm for hierarchical kMeans clustering where a cosine distance metric is used for similarity. For each cluster, processing system  may determine a centroid and its max radius (e.g., determined as a cosine distance from the centroid). All vectors that belong to a cluster fall within the max radius. In this example, cluster  includes set  of sub-clusters. Each sub-cluster may have a centroid and maximum radius. Accordingly, obtaining one topic\/cluster may further show all sub-clusters in a hierarchy. Further, naming of topics may be based on frequent terms appearing in the sub-clusters below a cluster as well as the members of the cluster. Thus, processing system  may implement one or more clustering algorithms that divide data into meaningful sub-groups (clusters) so that intra-cluster similarity is maximized while inter-cluster similarity is minimized. Some techniques are further discussed in relation to the clustering research package from University of Minnesota called CLUTO.","Accordingly, in some aspects, processing system  allows visualization and other treatment (such as tagging) of like vectors (as a single concept or topic). Processing system  provides for narrowing\/partitioning search result spaces into smaller more manageable spaces, navigation paths through a large collection of documents, and the discovery of other documents through membership in clusters. Processing system  may further cluster key-vector pairs and store the pairs using an indexing method that facilitates vector comparisons for items only within a specific set of clusters as further discussed below. Cluster-ordered index has the benefit that given an object, its cluster can be identified quickly using an index.",{"@attributes":{"id":"p-0143","num":"0142"},"figref":["FIG. 10","FIG. 10","FIG. 10"],"b":["1000","1000","1000","1010"]},"In step , all objects in a semantic space are initialized to belong to a single cluster. In step , a branching parameter is determined. In some embodiments, a branching parameter or k-value may be indicative of a number of branches. Values such as 10-way branching may be used. In step , objects are clustered into sub-clusters based on the branching parameter. For example, processing system  may use kMeans clustering to generate a number of sub-clusters. In another example, processing system  may use agglomerative hierarchical clustering to build a hierarchy from individual elements by progressively merging clusters.","In some embodiments, a single invocation of kMeans clustering by processing system  may include allocating a number of cluster mappings to be same size as the number of objects. Cluster centroids are allocated to be the number of clusters, each of a predetermined number of dimensions. Cluster mappings are then initialized randomly where each object vector is assigned to a random cluster. Processing system  then may iterate as many times as needed computing new mappings. Processing system  then may initialize each cluster centroid vector to zero. For each object, processing system  retrieves its vector and adds it to the cluster's centroid vector. After all objects are processed, processing system  then normalizes the centroid vector.","Processing system  thereafter computes new mappings for each object vector, based on the nearest centroid vector (i.e., the centroid that is closest to it) and changes the cluster for that object if its centroid is not its current mapping. By tracking the number of cluster mapping changes, and if there are any changes, processing system  continues to iterate for new mappings.","In step , sub-clusters are split that satisfy splitting conditions. For example, for any sub-cluster that qualifies for splitting, processing system  may recursively invoke clustering on that sub-cluster. Continuing the example above, if a member cluster member was re-assigned to a child sub-cluster, remove it from the current cluster. Processing system  stops when there are no new clusters generated.  ends in step .","In various embodiments, processing system  may utilize measurement criteria for determining whether to split a sub-cluster. For example, for a cluster in question, processing system  may determine the centroid and its vector. For all vectors that belong to that cluster, processing system  may determine a cosine distance of the vector from the centroid. If the combined normalized distance for all vectors is below a certain threshold value, processing system  may determine that the cluster should not be split further. If the distance is above a certain value, processing system  may split the cluster further.","In various embodiments, processing system  implements clustering using a variety of forms. For example, processing system  may use Cluster Mapping where a mapping parallels all root-level objects that were clustered. The mapping maps a document or term vector at the root level to the cluster it belongs. In another example, processing system  may use Cluster Partition where all object members that belong to a cluster and sub-cluster members are contained at the child level. In another example, each cluster level contains a centroid, which represents the centroid of all the members and clusters that belong under it.","In further embodiments, processing system  may cluster search results into a set of clusters. For search results that satisfy certain conditions (such as being limited to around 100K objects), processing system  may build document vectors from term vectors and build a new document vector space specific to only the search results. Processing system  may further take only the search results and build a new hierarchical cluster to cluster results. This has the advantage that any incrementally added documents (i.e., those that were not part of the corpus used to build the term or document vectors) can be part of the search results.","Signature and N-Gram Building","Finding similar or near-duplicate documents of a given document is a well known problem. Various solutions have been attempted, for example, involving extracting feature vectors for each document and comparing feature vectors using cosine or other type similarity function. These types of approaches require comparing feature vectors of all documents in a corpus with a given document to find similar or near duplicate documents.","In various embodiments, processing system  can reduce the space in which such comparisons are required. Accordingly, in one aspect, processing system  may eliminate the need to fetch feature vectors of every document in a corpus and then compare it with the feature vector of a given document under consideration.",{"@attributes":{"id":"p-0153","num":"0152"},"figref":["FIG. 11","FIG. 11","FIG. 11"],"b":["1100","1100","1100","1110"]},"In step , a document is received. As discussed above, a document may be embodied as any form of electronically stored information. For example, a document may be an email message, an instant message, a data packet, a file stored on a file system, a character or binary large object (CLOB or BLOB) storing in a database, or the like. The document may further be associated with metadata.","In step , the document is tokenized. For example, content of the document or other associated metadata may be tokenized. Tokenization may occur using tokenizer to extract individual terms or sequences of terms. In one example, tokenization may occur using a whitespace or language specific tokenizer. In another example, a tokenizer may remove special characters from terms. In yet another example, noise words may be removed, such as stop words and terms that may be deemed unlikely to have linguistic or contextual meaning. In yet another example, completely numeric-only terms may be removed. In yet another example, terms with length out of acceptable length range are also removed. All these kinds of tokenization techniques help reduce the number of terms to be further processed for overlapping window and also help in reducing the size of corresponding full text index.","In step , one or more trigrams are generated for the document. In general, an N-gram can include one or more characters, symbols, words, terms, phrases, or the like. For example, during tokenization, individual words or terms may be extracted from the document. In one aspect, overlapping subsequences of three words or terms may be determined. These overlapping subsequences of three words or terms are referred to as trigrams.","In various embodiments, frequency of the occurrence of each trigram extracted from the document may also be determined. A frequency table may be retrained for each unique trigram and the number of occurrences for a given document. Other metadata may be associated with each unique trigram, such as contextual rating, appropriateness ratings, linguistic usage information, organizational usage information, or the like.","In some embodiments, offsets in the document of the determined overlapping subsequences of three words or terms representing a trigram may be stored in a storage device or database instead of storing the actual trigrams. In one aspect, storing the offsets can provide for more efficient use of memory, both for storage and later retrieval.","In step , relevant trigrams are determined to represent the document. Relevant trigrams may be selected based on rules or other criteria. For example, the top 20-30 most frequent trigrams may be selected to represent the document. In another example, a score for each tri-gram may be determined by considering its frequency and the document length. Trigrams having a score satisfying some predetermined criteria or threshold may be used to represent the document. In yet another embodiment, certain terms may be weighted to influence the overall score of a trigram, such as adding to or subtracting from the overall score. In any event, the relevant trigrams then may become a signature\/feature vector of the document.","In step , any relevant trigrams representing the document are indexed with a full text index associated with the document. For example, a set of relevant trigrams are indexed in a full text index so that they can be searched by keyword. In one aspect, a signature\/feature vector of a document can be indexed in a stored region in encoded hex string format. Each trigram can then be indexed as individual uni-gram tokens in the full text index so that those can be searched by keyword.","In another aspect, each trigram may be indexed as terms next to each other. However, in some embodiments, an artificial distance may be created between two trigrams, such as to avoid accidental matches. For example, supposed trigram of a given document includes the terms \u201capple orange banana\u201d and trigram of the document includes the terms \u201cpeach apricot lemon.\u201d A query for the trigram \u201corange banana peach\u201d should not return the given document if an artificial distance is created between the trigram and trigram.  ends in step .","Searching for Signatures","Accordingly, by indexing relevant trigrams with a full text index of a document, system  can avoid comparing the feature vectors of all documents to find similar documents in various embodiments.",{"@attributes":{"id":"p-0163","num":"0162"},"figref":["FIG. 12","FIG. 12","FIG. 12"],"b":["1200","1200","1200","1210"]},"In step , a feature vector of a document is received. For example, an e-discovery investigator or other user interacting with one or more user interfaces associated with system  may select a document for which to find similar or near-duplicate documents. System  may retrieve the documents previously generated feature vector or generate a feature vector for the document as discussed above with respect to .","In step , a query is generated based on the feature vector of the document. In one embodiment, a Boolean OR query may be constructed of all the tri-grams in a document's feature vector. The query may be executed on the stored region that contains feature vectors of all the documents. For example, if a document's feature vectors contains trigram \u201capple orange banana\u201d and trigram \u201cpeach apricot lemon,\u201d a query may be generated as \u201capple orange banana\u201d OR \u201cpeach apricot lemon.\u201d","In step , results of the query are received from the full text index. In various embodiments, the query is facilitated by the full text indexed such that the full text engine finds documents whose feature vectors contain at least one trigram of the query as opposed to merely finding individual terms.","In step , relevant documents are determined from the results. System  may go through these results and pick the top N documents, for example, based on a predetermined number or a document score cut-off. Other criteria may be used to determine which of the documents are relevant.","In step , the feature vector of the document is then compared with feature vectors of the relevant documents from the results. System  may retrieve and compare the feature vectors of the top N documents using cosine similarity to find actual matches. In various embodiments, documents containing more trigram matches may be ranked and scored higher. A similarity score threshold can be configurable by a user to provide more or less documents determined to be similar. Accordingly, system  allows a user to control the space in which feature vector comparison will be carried out.  ends in step .","Presenting Results","In some embodiments, when dealing with emails for example, it can be important to find matching attachments and emails. However, an attachment without the context of the email may not present the full context of the result. System  can find the similar or near duplicate documents regardless of the type (emails or attachments). Additionally, when system  presents the results of a search, system  may map attachments to corresponding emails, while maintaining information about the actual attachment ids in a bitmap.","In some embodiments, a presentation layer may present matching documents as emails as top level entities. System  may use an attachment match bitmap to present exactly which part(s) of the emails (email body or specific attachment) corresponds to an actual match. Thus, system  may provide all the information about the match without losing the context. System  may further list down the unique similar uni-grams between the given and resultant document.","Hardware Overview",{"@attributes":{"id":"p-0171","num":"0170"},"figref":["FIG. 13","FIG. 13"],"b":"1300"},"Computer system  can include hardware and\/or software elements configured for performing logic operations and calculations, input\/output operations, machine communications, or the like. Computer system  may include familiar computer components, such as one or more one or more data processors or central processing units (CPUs) , one or more graphics processors or graphical processing units (GPUs) , memory subsystem , storage subsystem , one or more input\/output (I\/O) interfaces , communications interface , or the like. Computer system  can include system bus  interconnecting the above components and providing functionality, such connectivity and inter-device communication. Computer system  may be embodied as a computing device, such as a personal computer (PC), a workstation, a mini-computer, a mainframe, a cluster or farm of computing devices, a laptop, a notebook, a netbook, a PDA, a smartphone, a consumer electronic device, a gaming console, or the like.","The one or more data processors or central processing units (CPUs)  can include hardware and\/or software elements configured for executing logic or program code or for providing application-specific functionality. Some examples of CPU(s)  can include one or more microprocessors (e.g., single core and multi-core) or micro-controllers. CPUs  may include 4-bit, 8-bit, 12-bit, 16-bit, 32-bit, 64-bit, or the like architectures with similar or divergent internal and external instruction and data designs. CPUs  may further include a single core or multiple cores. Commercially available processors may include those provided by Intel of Santa Clara, Calif. (e.g., x86, x8664, PENTIUM, CELERON, CORE, CORE 2, CORE ix, ITANIUM, XEON, etc.), by Advanced Micro Devices of Sunnyvale, Calif. (e.g., x86, AMD64, ATHLON, DURON, TURION, ATHLON XP\/64, OPTERON, PHENOM, etc). Commercially available processors may further include those conforming to the Advanced RISC Machine (ARM) architecture (e.g., ARMv7-9), POWER and POWERPC architecture, CELL architecture, and or the like. CPU(s)  may also include one or more field-gate programmable arrays (FPGAs), application-specific integrated circuits (ASICs), or other microcontrollers. The one or more data processors or central processing units (CPUs)  may include any number of registers, logic units, arithmetic units, caches, memory interfaces, or the like. The one or more data processors or central processing units (CPUs)  may further be integrated, irremovably or moveably, into one or more motherboards or daughter boards.","The one or more graphics processor or graphical processing units (GPUs)  can include hardware and\/or software elements configured for executing logic or program code associated with graphics or for providing graphics-specific functionality. GPUs  may include any conventional graphics processing unit, such as those provided by conventional video cards. Some examples of GPUs are commercially available from NVIDIA, ATI, and other vendors. The one or more graphics processors or graphical processing units (GPUs)  may include any number of registers, logic units, arithmetic units, caches, memory interfaces, or the like. The one or more data processors or central processing units (CPUs)  may further be integrated, irremovably or moveably, into one or more motherboards or daughter boards that include dedicated video memories, frame buffers, or the like.","Memory subsystem  can include hardware and\/or software elements configured for storing information. Memory subsystem  may store information using machine-readable articles, information storage devices, or computer-readable storage media. Some examples of these articles used by memory subsystem  can include random access memories (RAM), read-only-memories (ROMS), volatile memories, non-volatile memories, and other semiconductor memories. In various embodiments, memory subsystem  can include semantic analysis data and program code .","Storage subsystem  can include hardware and\/or software elements configured for storing information. Storage subsystem  may store information using machine-readable articles, information storage devices, or computer-readable storage media. Storage subsystem  may store information using storage media . Some examples of storage media  used by storage subsystem  can include floppy disks, hard disks, optical storage media such as CD-ROMS, DVDs and bar codes, removable storage devices, networked storage devices, or the like. In some embodiments, all or part of semantic analysis data and program code  may be stored using storage subsystem .","In various embodiments, computer system  may include one or more hypervisors or operating systems, such as WINDOWS, WINDOWS NT, WINDOWS XP, VISTA, WINDOWS 13 or the like from Microsoft of Redmond, Wash., Mac OS or Mac OS X from Apple Inc. of Cupertino, Calif., SOLARIS from Sun Microsystems, LINUX, UNIX, and other UNIX-based or UNIX-like operating systems. Computer system  may also include one or more applications configured to execute, perform, or otherwise implement techniques disclosed herein. These applications may be embodied as semantic analysis data and program code . Additionally, computer programs, executable computer code, human-readable source code, or the like, and data may be stored in memory subsystem  and\/or storage subsystem .","The one or more input\/output (I\/O) interfaces  can include hardware and\/or software elements configured for performing I\/O operations. One or more input devices  and\/or one or more output devices  may be communicatively coupled to the one or more I\/O interfaces .","The one or more input devices  can include hardware and\/or software elements configured for receiving information from one or more sources for computer system . Some examples of the one or more input devices  may include a computer mouse, a trackball, a track pad, a joystick, a wireless remote, a drawing tablet, a voice command system, an eye tracking system, external storage systems, a monitor appropriately configured as a touch screen, a communications interface appropriately configured as a transceiver, or the like. In various embodiments, the one or more input devices  may allow a user of computer system  to interact with one or more non-graphical or graphical user interfaces to enter a comment, select objects, icons, text, user interface widgets, or other user interface elements that appear on a monitor\/display device via a command, a click of a button, or the like.","The one or more output devices  can include hardware and\/or software elements configured for outputting information to one or more destinations for computer system . Some examples of the one or more output devices  can include a printer, a fax, a feedback device for a mouse or joystick, external storage systems, a monitor or other display device, a communications interface appropriately configured as a transceiver, or the like. The one or more output devices  may allow a user of computer system  to view objects, icons, text, user interface widgets, or other user interface elements.","A display device or monitor may be used with computer system  and can include hardware and\/or software elements configured for displaying information. Some examples include familiar display devices, such as a television monitor, a cathode ray tube (CRT), a liquid crystal display (LCD), or the like.","Communications interface  can include hardware and\/or software elements configured for performing communications operations, including sending and receiving data. Some examples of communications interface  may include a network communications interface, an external bus interface, an Ethernet card, a modem (telephone, satellite, cable, ISDN), (asynchronous) digital subscriber line (DSL) unit, FireWire interface, USB interface, or the like. For example, communications interface  may be coupled to communications network\/external bus , such as a computer network, to a FireWire bus, a USB hub, or the like. In other embodiments, communications interface  may be physically integrated as hardware on a motherboard or daughter board of computer system , may be implemented as a software program, or the like, or may be implemented as a combination thereof.","In various embodiments, computer system  may include software that enables communications over a network, such as a local area network or the Internet, using one or more communications protocols, such as the HTTP, TCP\/IP, RTP\/RTSP protocols, or the like. In some embodiments, other communications software and\/or transfer protocols may also be used, for example IPX, UDP or the like, for communicating with hosts over the network or with a device directly connected to computer system .","As suggested,  is merely representative of a general-purpose computer system appropriately configured or specific data processing device capable of implementing or incorporating various embodiments of an invention presented within this disclosure. Many other hardware and\/or software configurations may be apparent to the skilled artisan which are suitable for use in implementing an invention presented within this disclosure or with various embodiments of an invention presented within this disclosure. For example, a computer system or data processing device may include desktop, portable, rack-mounted, or tablet configurations. Additionally, a computer system or information processing device may include a series of networked computers or clusters\/grids of parallel processing devices. In still other embodiments, a computer system or information processing device may perform techniques described above as implemented upon a chip or an auxiliary processing board.","Various embodiments of any of one or more inventions whose teachings may be presented within this disclosure can be implemented in the form of logic in software, firmware, hardware, or a combination thereof. The logic may be stored in or on a machine-accessible memory, a machine-readable article, a tangible computer-readable medium, a computer-readable storage medium, or other computer\/machine-readable media as a set of instructions adapted to direct a central processing unit (CPU or processor) of a logic machine to perform a set of steps that may be disclosed in various embodiments of an invention presented within this disclosure. The logic may form part of a software program or computer program product as code modules become operational with a processor of a computer system or an information-processing device when executed to perform a method or process in various embodiments of an invention presented within this disclosure. Based on this disclosure and the teachings provided herein, a person of ordinary skill in the art will appreciate other ways, variations, modifications, alternatives, and\/or methods for implementing in software, firmware, hardware, or combinations thereof any of the disclosed operations or functionalities of various embodiments of one or more of the presented inventions.","The disclosed examples, implementations, and various embodiments of any one of those inventions whose teachings may be presented within this disclosure are merely illustrative to convey with reasonable clarity to those skilled in the art the teachings of this disclosure. As these implementations and embodiments may be described with reference to exemplary illustrations or specific figures, various modifications or adaptations of the methods and\/or specific structures described can become apparent to those skilled in the art. All such modifications, adaptations, or variations that rely upon this disclosure and these teachings found herein, and through which the teachings have advanced the art, are to be considered within the scope of the one or more inventions whose teachings may be presented within this disclosure. Hence, the present descriptions and drawings should not be considered in a limiting sense, as it is understood that an invention presented within a disclosure is in no way limited to those embodiments specifically illustrated.","Accordingly, the above description and any accompanying drawings, illustrations, and figures are intended to be illustrative but not restrictive. The scope of any invention presented within this disclosure should, therefore, be determined not with simple reference to the above description and those embodiments shown in the figures, but instead should be determined with reference to the pending claims along with their full scope or equivalents."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["In order to reasonably describe and illustrate those innovations, embodiments, and\/or examples found within this disclosure, reference may be made to one or more accompanying drawings. The additional details or examples used to describe the one or more accompanying drawings should not be considered as limitations to the scope of any of the claimed inventions, any of the presently described embodiments and\/or examples, or the presently understood best mode of any innovations presented within this disclosure.",{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0032","num":"0031"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0034","num":"0033"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIGS. 7A and 7B"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIGS. 8A and 8B"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 9A"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 9B"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0040","num":"0039"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0041","num":"0040"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0042","num":"0041"},"figref":"FIG. 13"}]},"DETDESC":[{},{}]}
