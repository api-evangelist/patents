---
title: Multi-resolution hidden markov model using class specific features
abstract: A method for classifying data includes selecting an elemental size and features for the data that are representative of possible subclasses. Resolution widths are selected in conjunction with these features. Models associated with symbols are developed from these resolution widths and features. Data is compared with these models to give a likelihood that the model applies. The best model is determined and a signal is provided related to the symbol associated with the best model.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08200489&OS=08200489&RS=08200489
owner: The United States of America as represented by the Secretary of the Navy
number: 08200489
owner_city: Washington
owner_country: US
publication_date: 20090129
---

{"@attributes":{"id":"description"},"GOVINT":[{},{}],"heading":["STATEMENT OF GOVERNMENT INTEREST","CROSS-REFERENCE TO RELATED PATENT APPLICATIONS","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAIL DESCRIPTION OF THE INVENTION"],"p":["The invention described herein may be manufactured and used by or for the Government of the United States of America for governmental purposes without the payment of any royalties thereon or therefor.","None.","(1) Field of the Invention","The present invention generally relates to a signal analysis method that uses class specific features and different size analysis windows for recognizing phenomena in the signal.","(2) Description of the Prior Art","Characterizing an input signal is a common problem in many fields. In sonar and radar, it is often desirable to separate natural sources from manmade sources. This method also has application with geological survey signals and non-time series signals such as images. It can be applied to any one dimensional signal.","In speech recognition, it is desirable to recognize phonemes so that speech can be converted into text. Rabiner in \u201cA Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,\u201d Proceedings of IEEE, Vol. 77, No. 2, (February 1989) provides background teaching a method for voice recognition using hidden Markov models. Three common hidden Markov model problems are given. The first is computing the probability of a given observation sequence from the model. This measures the matching between the observed sequence and the model. The second problem is in choosing the best state sequence for a given model and observation sequence. The solution of this problem uncovers the sequence of symbols. The third problem is refining model parameters to maximize the probability that an observation is characterized by a model. Resolution of this problem optimizes the model parameters to best describe an observed sequence.","Because the na\u00efve solution to computing observation sequence probability is numerically intensive, a well-known forward procedure has been developed for efficiently calculating the probability of a given observation sequence from the model. In this method, a forward probability vector is calculated for each time in the observational period. This is calculated from the state probabilities, the observational symbol probabilities and the state transition probabilities. The calculation greatly reduces the number of calculations by using the lattice structure of the hidden Markov model.","In a typical automatic speech recognition (ASR) system, such as that taught by Rabiner, the speech signal is divided into equal-sized segments, from which features (usually cepstral coefficients) are extracted. The probabilistic model is usually a Hidden Markov Model (HMM). The process of characterizing the data is divided into two separate stages. In the first stage, it is necessary to extract useful information from the raw input data in a compact form that is useable by automatic recognition algorithms. This usually means that the data is divided into segments, and information in the form of features is extracted from each segment. In the second stage, an algorithm, usually a probabilistic model such as a hidden Markov model, decides which type of signal is present in each segment, or combines segments to recognize signals spanning multiple segments.","One problem with such a two-stage approach is the necessity of making difficult compromises in the first (feature extraction) stage. The size of the segments and the type of features must be consistent, even if the potential signal types are not. In view of this, existing systems aren't very good at processing complex signals where the segment sizes vary.","A good example of a complex signal is human speech where a typical vowel is consistent over time-durations as long as 60 milliseconds and is rich in spectral detail, whereas a \u201cT-burst\u201d, a component of the sound of the consonant \u201cT\u201d, has a duration as short as a few milliseconds and has little spectral detail. In spite of this, a single segment size and feature extractor is used for both. The typical analysis window for human speech is about 30 milliseconds (effectively 16 milliseconds after applying a window function), which is a compromise. It can be regarded as a poor compromise because it is too long to observe the occurrence of the \u201cT-burst\u201d and too short to fully characterize the spectral detail of the vowel.","The reason that the compromise is needed at all is because of the way decisions are made between competing signal hypotheses. These decisions are primarily made using a common feature set. For example, in order to decide between signal type A and B, the system needs to first train itself on the patterns observed for signal types A and B at the output of a feature extractor. Then, in the testing phase, the pattern observed using exactly the same feature extractor is compared with the learned patterns for signal types A and B, and a decision is made. Prior to the introduction of the class-specific method (CSM), classical decision theory did not consider the problem of deciding between signal types when patterns were learned using different segment sizes and different features extractors for each signal type. This is unfortunate because the segment size and feature type that is best to characterize each signal type may differ. Accordingly, there is a need for a method that can use different features and segment sizes, yet at the same time make optimal statistical decisions.","Several techniques related the use of signal-dependent or class-dependent features for classification are taught in the prior art, yet only those methods related to the class-specific method or CSM, are general in nature, and derived from a theory rooted in the classical optimum Bayesian classifier. CSM is covered in U.S. Pat. No. 6,535,641, \u201cClass-Specific Classifier\u201d, and augmented by the probability density function (PDF) projection theorem (PPT) which is disclosed in Baggenstoss, \u201cThe PDF Projection Theorem and the Class-Specific Method\u201d, IEEE Transactions on Signal Processing, Vol. 51, No. 3 (March 2003). The probability density function projection theorem eliminates the need for sufficient statistics and allows the use of class-dependent reference hypotheses, improving the performance of any classification system using class-dependent features. U.S. Pat. No. 6,466,908, entitled \u201cSystem and Method for Training a Class-specific Hidden Markov Model Using a Modified Baum-Welch Algorithm\u201d alleviates the need for a common feature set in a HMM.","Accordingly, there is provided a method for classifying data includes selecting an elemental size and features for the data that are representative of possible subclasses. Resolution widths are selected in conjunction with these features. Models associated with symbols are developed from these resolution widths and features. Data is compared with these models to give a likelihood that the model applies. The best model is determined and a signal is provided related to the symbol associated with the best model.","The method of the current invention receives data and determines the most likely signal hypothesis or symbol based on the most probable multi-resolution hidden Markov model. The symbol can be a word or phoneme in speech recognition. Unlike prior art hidden Markov model analysis, this method uses a multi-resolution hidden Markov model which combines the outputs from a variety of analysis window resolutions and feature types into a probabilistic model.","A glossary of terms and symbols is provided below to avoid confusion.",{"@attributes":{"id":"p-0024","num":"0023"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"3"},"colspec":[{"@attributes":{"colname":"1","colwidth":"63pt","align":"left"}},{"@attributes":{"colname":"2","colwidth":"35pt","align":"left"}},{"@attributes":{"colname":"3","colwidth":"119pt","align":"left"}}],"thead":{"row":[{"entry":"TABLE 1"},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}},{"entry":["Term","Symbol","Description"]},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":["elemental segment","x","A uniform segment of raw data of"]},{"entry":[{},{},"length T samples starting at sample"]},{"entry":[{},{},"1 + (t \u2212 1)T. The finest subdivision of"]},{"entry":[{},{},"data that is considered."]},{"entry":[{},"t","Elemental segment index variable,"]},{"entry":[{},{},"takes values from 1 to N"]},{"entry":[{},"T","The base segmentation size (number"]},{"entry":[{},{},"of samples) of the elemental"]},{"entry":[{},{},"segments."]},{"entry":["Analysis window","x","Definition 1: A non-elemental"]},{"entry":[{},{},"segment of raw data of length KT"]},{"entry":[{},{},"samples starting at sample 1 + (t \u2212 1)T."]},{"entry":[{},{},"Definition 2: The set of K elemental"]},{"entry":[{},{},"segments,contained'in the analysis"]},{"entry":[{},{},"window."]},{"entry":["Signal class","m","One of a set of M possible data"]},{"entry":[{},{},"phenomena (called a state in"]},{"entry":[{},{},"standard hidden Markov model"]},{"entry":[{},{},"analysis)."]},{"entry":["Analysis window","K","The set of analysis window sizes"]},{"entry":["set",{},"available to signal class m, in"]},{"entry":[{},{},"increments of elemental segment size"]},{"entry":[{},{},"of T samples."]},{"entry":[{},{},"Example: K= {12,8,4}"]},{"entry":["Entry flags","E","Entry flags associated with the"]},{"entry":[{},{},"analysis window set K."]},{"entry":[{},{},"Example E= {1,1,0}."]},{"entry":["State partition",{},"A section of wait states in the"]},{"entry":[{},{},"expanded state trellis associated"]},{"entry":[{},{},"with a given signal class m and"]},{"entry":[{},{},"segment size k."]},{"entry":["Wait state","w","A discrete state in the expanded"]},{"entry":[{},{},"state trellis used to count out the"]},{"entry":[{},{},"number of time steps in a partition."]},{"entry":["Segment","L","Equal to log p(x|m)"]},{"entry":["likelihood",{},{}]},{"entry":"function"},{"entry":{"@attributes":{"namest":"1","nameend":"3","align":"center","rowsep":"1"}}}]}}}}},"For background purposes and to aid in the discussion,  shows the standard hidden Markov model (HMM) which uses uniform segmentation of the time-series and common feature set. In a standard HMM, a time-series signal is segmented for processing using a pre-determined uniform segmentation with overlap, and a common set of features is extracted from the segments. The underlying assumption is that the signal may be in one of M possible states, and that the state of the signal, which we are not able to observe directly, changes at increments of time co-incident with the centers of the overlapped segments.",{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 1","b":["10","12","12","14","14","14","14","16"]},"Also in the FIG. are seen a set of processing windows  shown as rectangles. The rectangles  illustrate how each processing window is aligned to the input time-series . In the standard HMM, feature are extracted from each processing window  from the segment of input data  that is aligned with each window . The features from each processing window  are assumed to represent the characteristics of the input data  at a time coincident with the center of the processing window. As is the usual practice, the processing windows  are shifted in such a way that there is a certain amount of overlap between adjacent windows . In the FIG., there is a 50 percent overlap. The use of overlapping windows  is in part to improve the chance that events observed in the data will be aligned with at least one processing window .","In  at the very bottom, there is a standard HMM state trellis . In trellis , we see two potential state transition paths (solid  and dotted ). Paths  and  represent possible paths that could be guessed from the observations. Note that the time resolution of the potential paths  and  agrees with the time resolution afforded by the shifted processing windows . We can see here the compromise that is made necessary by the standard HMM. If the processing windows  are too large, the ability to observe short events (such as the noise burst C) and fast changes in system state is diminished. Even with a reduction in shifting and an increase in overlap, the ability of the standard HMM to observe fast changes in system state is not improved. If the processing windows  are too short, the feature extraction necessary to optimally detect long events such as a weak sine wave is diminished.","Now referring to , a representation of the ideal situation, the available processing windows  are aligned with the events in the data. Unfortunately, the ideal set of processing windows  can be created only if we know the actual state transitions. Trying every possible combination of processing windows  is infeasible because this would involve computing every window size at every shift and testing all combinations. Accordingly, it is necessary to somewhat reduce the number of potential window sizes.",{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIG. 3","b":["28","28","28"]},"In , we show one of the segmentations possible by selecting processing windows  available in . Because of the reduced set of windows , the noise segment of duration k=9 (region B) needs to be represented by a combination of windows  having length k=3 and k=6, which is a good compromise.","Note that the amount of processing is still large because even very large processing windows need to be calculated at every shift, which is highly redundant. Note, however, that efficient processing can be used to take advantage of the redundancy. Efficient processing of this type has been described in U.S. Pat. No. 6,907,367, entitled \u201cTime Series Segmentation\u201d. We note that even if the processing problem can be solved, there are still two problems remaining. First, there is the issue of searching the enormous space of all possible segmentations of the input data. Second, there is the problem of comparing competing segmentations when the features are extracted from different segment sizes and potentially different feature types. This issue does not occur in the standard HMM because all segment sizes are the same and the same features are extracted from the segments, so simple likelihood comparisons are possible. We now describe how an embodiment of the multi-resolution hidden Markov model (MRHMM) is able to solve both of these problems and do it efficiently.","The MRHMM can be described as resulting from a sequence of three modifications of the HMM. In the first modification, we employ the probability density function projection theorem (PPT) to convert from using feature likelihood functions to raw (input) data likelihood functions. By comparing raw data likelihood functions, we are able to compare competing segmentations fairly. In the second modification, we constrain the dwell times of each system state. We do this by expanding the state trellis to include \u201cwait states\u201d that count out prescribed number of time steps. Once the system enters a given state, it must remain in that state for a fixed number of time steps before it can exit. In the third modification, we utilize \u201cpartial PDF\u201d values so that the likelihood values are referred to time steps at the system resolution of T samples. In addition to the three primary modifications, there are a number of improvements including efficient implementation and minimum description length (MDL) compensation.","According to the first modification, we employ the PPT to operate in the raw data domain. This allows us to compare the raw data likelihood functions for two potential segmentations. Let q represent a potential segmentation and signal subclass assignment. In other words, q defines not only the length and positions of the segments, but the signal subclass present in each segment. The segmentation in  will serve as an illustrative example. The raw data likelihood function given the segmentation q in  is\n\n()=log (2)+log (1)+log (1)+log (3)+log (1)=\u2003\u2003(1)\n\nwhere X is the entire input time series, and xis the data segment of length K time steps starting at the t-th time step. Note that the total likelihood function given the segmentation equals the sum of the individual segment likelihood functions, which is denoted by L=log p(x|m), and is due to the assumption of conditional independence of the segments.\n","By applying the PDF projection theorem (PPT) we are able to calculate Lfrom the feature likelihood functions. Accordingly, it is assumed that a different feature extractor is available to characterize each signal subclass. Let zbe the feature vector extracted from segment xusing the feature extractor designed for signal subclass m. Using results from Baggenstoss, The PDF Projection Theorem and the Class-Specific Method, IEEE Transactions on Signal Processing, Vol. 51, No. 3 (March 2003), we can write:",{"@attributes":{"id":"p-0036","num":"0035"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","t","K"]},"mo":"\u2758","mi":"m"}}},{"mfrac":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","t","K"]},"mo":"\u2758","msub":{"mi":"H","mrow":{"mn":"0","mo":",","mi":"m"}}}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["z","K"],"mrow":{"mi":["t","m"],"mo":","}},"mo":"\u2758","msub":{"mi":"H","mrow":{"mn":"0","mo":",","mi":"m"}}}}}]},"mo":"\u2062","mrow":{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["z","K"],"mrow":{"mi":["t","m"],"mo":","}},"mo":"\u2758","mi":"m"}}}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"2"}}]}}}},"br":{},"sub":["0,m ","t","t,m","t","t,m"],"sup":["K ","K ","K","K"]},{"@attributes":{"id":"p-0037","num":"0036"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msup":{"mi":["J","m"]},"mo":"\u2061","mrow":{"mo":["(",")"],"msubsup":{"mi":["x","t","K"]}}},"mo":"=","mfrac":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["x","t","K"]},"mo":"\u2758","msub":{"mi":"H","mrow":{"mn":"0","mo":",","mi":"m"}}}}},{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msubsup":{"mi":["z","K"],"mrow":{"mi":["t","m"],"mo":","}},"mo":"\u2758","msub":{"mi":"H","mrow":{"mn":"0","mo":",","mi":"m"}}}}}]}}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}},"br":{},"sub":["t","t,m"],"sup":["K ","K"]},"The second modification involves constraining state transitions to account for states of longer duration.  shows an example state trellis . State trellis  has M states at NT times represented by circles. A first path  and a second path  are shown through state trellis . In this model, it is required that whenever entering state , the state must remain in state  for 4 time steps (4T samples) before having the opportunity to transition. This constraint may be imposed by expanding the set of states to include \u201cwait\u201d states and using a structured state transition matrix. In the matrix below, associated with , the value at a position represents the probability of transition into that state. \u201c1\u201d is a certainty, \u201c0\u201d is an impossibility, and \u201cX\u201d a probability between a certainty and impossibility.",{"@attributes":{"id":"p-0039","num":"0038"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msup":{"mi":["A","e"]},"mo":"=","mrow":{"mo":["[","]"],"mtable":{"mtr":[{"mtd":[{"mi":"X"},{"mi":"X"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"1"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mn":"0"}]},{"mtd":[{"mi":"X"},{"mi":"X"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"}]},{"mtd":[{"mi":"X"},{"mi":"X"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"}]},{"mtd":[{"mi":"X"},{"mi":"X"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"}]},{"mtd":[{"mi":"X"},{"mi":"X"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"}]},{"mtd":[{"mi":"X"},{"mi":"X"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"}]},{"mtd":[{"mi":"X"},{"mi":"X"},{"mn":"0"},{"mn":"0"},{"mn":"0"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"},{"mi":"X"}]}]}}}},{"mrow":{"mo":["(",")"],"mn":"4"}}]}}}},"br":{},"sup":"e "},"A potential source of confusion exists for the concept \u201cstate\u201d because in the MRHMM there are three potential meanings of state (subclass, partition, wait state), while in the HMM there is but one. In the HMM, the concept of \u201cstate\u201d is synonymous with signal subclass. In the MRHMM, each signal subclass is associated with more than one state partition, each with a different duration constraint as defined by the size of the wait state partition.","The dwell-time constraints of a given signal subclass m are represented by the set K. For example if K={12,8,4}, signal subclass m is associated with wait-state partitions of size k=12, k=8, and k=4. This also means that the processing windows of sizes 12, 8, and 4 (times T samples) are available for signal subclass m. In addition to constraining the size of the available segments, we can also control the starting segment size. Corresponding to set Kis the entry flag, for example, E={1,1,0}. This indicates that signal subclass m is allowed to begin with a segment size of 12 or 8, but not 4. This effectively sets a minimum length of the dwell time in signal subclass m. The total dwell time, thus has a minimum of 8 time steps (8T samples), and the full length of stay in signal subclass m must be composed of segments selected from the set k=12, 8 and 4.","For the situation shown in , the following constraints are proposed:",{"@attributes":{"id":"p-0043","num":"0042"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"28pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"84pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"21pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"63pt","align":"center"}}],"thead":{"row":[{"entry":[{},"TABLE 2"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]},{"entry":[{},"Class","Name","K","E"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"1","Noise","12,6,3","1,1,0"]},{"entry":[{},"2","Sine Wave","12,6","1,0"]},{"entry":[{},"3","Noise Burst","3","1"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}}}},"The third modification required for a MRHMM is referencing likelihood values to the base time resolution of T samples. In effect, we \u201cpretend\u201d that a uniform segmentation of T samples was used so we need a likelihood value for each T-sample segment for a given segmentation. For a segment of K time steps, we assume that the log-likelihood in each time step is 1\/K times the log-likelihood of the length-K segment. For example,\n\n=log ()\u2003\u2003(5)\n\nwhich is the log-likelihood of a length k=6 segment, is broken into 6 equal terms, each equal to \u2159Lwhich, if added up are equal to L. Each of the terms is assumed to be the log-likelihood of one of the 6 T-sample segments contained within the segment. We call these \u201cpartial PDF values\u201d because they are not actual PDF (likelihood function) values, and only have a physical meaning when added up across a full segment.\n","If we consider all the segment likelihood values Lthat need to be calculated, we have one for each segment size, at each delay, and for each signal subclass. We then need to expand this number further because each likelihood value appears in K partial PDF values. It can be seen that there is a partial PDF value for each wait state at each incremental time step. If we let Nbe the total number of wait states, we have a partial PDF matrix of Nwait states by N time steps. The previous example (table 2) has 63 wait states.","The result of the last modification is to create what appears to be the state likelihood values for a standard HMM operating with Nstates on a T-sample regular time update. Let \u03c0be the length-Nprior probabilities and let Abe the Nby Nstate transition matrix defined for the expanded state trellis of wait-states (We will explain later how these are obtained). We can, in principle, apply the standard forward procedure, as discussed in Rabiner and previously described, to these log probabilities as though they were an HMM operating at the fine T-sample time resolution. We could also apply the forward-backward procedure to obtain the a posteriori wait-state probabilities \u03b3, which is the probability that wait state w was in effect at incremental time t, given all the data.","At first glance, it appears that this is not valid because the partial PDF values are not proper observation likelihoods. However, the log-likelihood of the raw data conditioned on knowing the segmentation may be thought of as the accumulation of log-likelihood on the path through the trellis. Because of the dwell-time constraint imposed by the special form of the state transition matrix, any paths through the expanded trellis that accumulate partial PDF values that do not add up to full segments are weighted by a zero probability. So, upon closer inspection the application of the standard HMM forward procedure to the expanded state trellis is indeed a valid algorithm, provided the dwell-time constraints are consistent with the partial PDF values.","Application of the forward procedure as modified herein calculates the probability that a multi-resolution hidden Markov model fits the data. Each of these multi-resolution hidden Markov model is associated with a symbol, word or other information about the associated data. The multi-resolution hidden Markov model having the highest likelihood can be used as the most likely explanation for the received data. Thus, the associated symbol can be given to a user as the most likely interpretation. In a speech recognition application, this symbol could be a word. In a geophysical analysis application this symbol could be an indication of a geological structure. In sonar analysis the symbol could be a ship identification or ship class. This symbol can then be provided to a user or another system for action.","The following provides a discussion of how the forward procedure from the prior art is adapted to MRHMMs. The probability density function (otherwise known as likelihood function) of the raw time-series data given the state path q through the expanded trellis is defined as p(X|q). The forward procedure integrates the PDF over the paths weighted by the probability of the path:",{"@attributes":{"id":"p-0050","num":"0049"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"mrow":[{"mi":"L","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"X"}},{"munder":{"mo":"\u2211","mrow":{"mi":["q","Q"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["X","q"],"mo":"\u2758"}}},{"mi":"P","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"q"}}],"mo":"\u2062"}}],"mo":"="},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}},"br":{},"sub":["w,t","m,t "]},"Another improvement can be made to the method of calculating conditional likelihood functions. We previously gave an example of the calculation of the total likelihood for a given path L(X|q). We described how L(X|q) is made up of the sum of the likelihood values of the segments that constitute path q, for example L(X|q)=L+L+L+L+L. A problem that often occurs in the field is the issue of over-parameterization. This is the phenomenon by which the total likelihood function always increases as we break the data up into more parts and assign more parameters to describe the data. The negative aspect of this effect is that when we compare two paths based on their likelihood values alone, the one with more segments (and therefore more parameters, i.e. extracted feature values) is more likely to win the comparison. This is a well-studied problem called model order selection.","Model order selection is not a problem with ordinary hidden Markov models because ordinary hidden Markov models use the same features and segment sizes. In the model comparison field, different models can be compared by assigning a penalty function to the likelihood values depending on the number of parameters (total dimension of the extracted feature values). One known penalty function is the minimum description length (MDL) penalty which equals",{"@attributes":{"id":"p-0053","num":"0052"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":"-","mfrac":{"mi":"p","mn":"2"}},{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mi":"N"}}],"mo":"\u2062"}}},"br":{}},{"@attributes":{"id":"p-0054","num":"0053"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mi":"p","mrow":{"mn":"2","mo":"\u2062","mi":"k"}},"mo":"\u2062","mrow":{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["K","max"]},"mo":"\u2062","mi":"T"}}}}}},"br":{}},{"@attributes":{"id":"p-0055","num":"0054"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mfrac":{"mn":"1","mi":"k"},"mo":"\u2062","msubsup":{"mi":["L","K"],"mrow":{"mi":["t","m"],"mo":","}}}}},"br":{},"sub":["max ","max "]},{"@attributes":{"id":"p-0056","num":"0055"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mo":"-","mfrac":{"mn":["1","2"]}},{"mi":"log","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["K","max"]},"mo":"\u2062","mi":"T"}}}],"mo":"\u2062"}}},"br":{}},"The described method requires further modifications for efficient calculation. The number of wait states in the expanded hidden Markov model problem can be very large. The forward and backward procedures have a complexity of the order of the square of the number of states. Thus, an efficient implementation of the forward and backward procedures and Baum-Welch algorithm may be needed that takes advantage of the redundancies in the expanded problem. With some thought, it is not difficult to program the forward and backward procedures so as not to explicitly increment each wait state and not to require the availability of the partial PDF values, relying only on the full segment PDF values. We have obtained a processing time reduction factor of 42 with a problem that had 9 states and expanded to 274 wait states. The two algorithms were tested to produce the same results within machine precision.","Training the MRHMM in order to arrive at a set of parameters is another problem resolved by this method. These parameters include the state transition matrix probabilities and the parameters of the feature PDFs. One method of training a standard hidden Markov model is accomplished by the Baum Welch re-estimation algorithm, which is a special case of an expectation-maximization (EM) algorithm. The part of the Baum Welch algorithm that re-estimates the state probability density functions is simply a weighted maximum likelihood update. That is, it attempts to maximize the weighted log-likelihood function of each state. The weight changes at each time update and is equal to \u03b3, the a posteriori probability (given all the data) that signal subclass m is in effect at time t. The values of \u03b3, are obtained as explained above. The re-estimation of the parameters of the segment feature likelihood functions log p(z|m) follows the procedure outlined in Rabiner for the standard HMM. Only a slight modification to the standard re-estimation procedure is required. The modification is necessary because there is a segment likelihood function for each segment size associated with signal subclass m. If \u03b3indicates that signal subclass m is likely to be present at time t, this does not necessarily mean that all segment sizes associated with signal subclass m are likely. For example, if we are near the end of the probable dwell time in signal subclass m, it is not likely that a large segment could be starting. To account for this issue, the feature zis weighted by the smallest value of \u03b3within the range of times t that span the segment from which zwas obtained. This process we call \u201cganging\u201d because we effectively gang together the training of all the segment sizes associated with signal subclass m.","In summary, one basic method of developing a multi-resolution hidden Markov Model using class specific features is provided. During a definitional phase, one must enumerate the subclasses m. An elemental segment size T, the finest subdivision of data, must be chosen. (This elemental segment size can be based on classification needs or physical system constraints such as processing speed or memory capacity).","In continuing to develop the hidden Markov models, there is an initialization phase. Let Abe the expanded state transition matrix incorporating wait states. While in principal it is possible to estimate Ausing the standard Baum-Welch algorithm applied to the expanded trellis, it requires a large number of parameters to estimate. One way to reduce the number of parameters to estimate is to base the expanded state transition matrix on a hypothesized M-by-M elemental transition matrix Abased on the transition of signal subclasses at the elemental segment rate. Ais then created from A by the following method. If the wait state of interest, w, is not the last wait state in a partition, then A=1. For each subclass pair i, j where i\u2260j, let w be the last wait state of any partition corresponding to signal subclass i and let w\u2032 be the first wait state of a partition corresponding to signal subclass j that has an entry flag of 1. In this case, we let A=A. For each signal subclass i, let w be the last wait state of any partition corresponding to signal subclass i and let w\u2032 be the first wait state of any partition also corresponding to subclass i. We let A=\u03b1A, where index m runs over partitions corresponding to subclass i. In order that Ais a proper state transition matrix, we need that",{"@attributes":{"id":"p-0061","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"munder":{"mo":"\u2211","mi":"m"},"mo":"\u2062","msub":{"mi":["\u03b1","m"]}},"mo":"=","mn":"1."}}},"br":{}},"State prior probabilities must also be initialized. \u03c0is the subclass prior probability, and \u03c0is the expanded state prior probability function. When wait state w is the first wait state of partition corresponding to signal subclass I (with a non-zero entry flag), then \u03c0=\u03c0. Feature probability density functions can be initialized utilizing training data from each subclass."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The foregoing features of the present invention will become more readily apparent and may be understood by referring to the following detailed description of an illustrative embodiment of the present invention, taken in conjunction with the accompanying drawings, in which:",{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
