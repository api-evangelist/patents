---
title: System and method for computing the visual profile of a place
abstract: A system and method for computing a place profile are disclosed. The method includes providing a geographical definition of a place, retrieving a set of images based on the geographical place definition. With a classifier, image-level statistics for the retrieved images are generated. The classifier has been trained to generate image-level statistics for a finite set of classes, such as different activities. The image-level statistics are aggregated to generate a place profile for the defined place which may be displayed to a user who has provided information for generating the geographical definition or used in an application such as a recommender system or to generate a personal profile for the user.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09298982&OS=09298982&RS=09298982
owner: XEROX CORPORATION
number: 09298982
owner_city: Norwalk
owner_country: US
publication_date: 20110726
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","INCORPORATION BY REFERENCE","BRIEF DESCRIPTION","DETAILED DESCRIPTION"],"p":["The exemplary embodiment relates generally to automated systems and methods for profiling and finds particular application in connection with profiling of geographical locations based on image data.","The widespread use of digital cameras and other devices adapted to capturing images, such as mobile phones, has lead to the public availability of large numbers of images which are often tagged with geographical location information in the form of geo-tags and keyword information. Digital images are often shared among users in large databases of visual data, for example, via community photograph collections such as the Flickr\u00ae collection. A user interested in photographs of a particular city, such as London, for example, could enter a keyword search for \u201cLondon\u201d and be presented with a large number of images. Despite the abundance of publicly-accessible image data, the information is not organized in a way which allows a user to get a representative view of a place of interest.","The exemplary embodiment provides a system and an automated method for computing a visual profile of a place.","The following references, the disclosures of which are incorporated herein by reference in their entireties, are mentioned:","U.S. patent application Ser. No. 13\/050,587, filed on Mar. 17, 2011, entitled SYSTEM AND METHOD FOR ADVERTISING USING IMAGE SEARCH AND CLASSIFICATION, by Craig Saunders and Nicolas Guerin, discloses computing the profile of a person based on categorization of images in the person's collection.","The following relate generally to visual classification and image retrieval methods: U.S. Pub. Nos. 20030021481; 2007005356; 20070258648; 20080069456; 20080240572; 20080317358; 20090144033; 20100040285; 20100092084; 20100098343; 20100191743; 20100189354; 20100318477; 20110040711; 20110026831; 20110091105; U.S. application Ser. No. 12\/693,795, filed on Jan. 26, 2010, entitled A SYSTEM FOR CREATIVE IMAGE NAVIGATION AND EXPLORATION, by Sandra Skaff, et al.; U.S. application Ser. No. 12\/859,898, filed on Aug. 20, 2010, entitled LARGE SCALE IMAGE CLASSIFICATION, by Florent Perronnin, et al.; U.S. application Ser. No. 12\/890,789, filed on Sep. 27, 2010, entitled IMAGE CLASSIFICATION EMPLOYING IMAGE VECTORS COMPRESSED USING VECTOR QUANTIZATION, by Jorge S\u00e1nchez, et al., Perronnin, F., Dance, C., \u201cFisher Kernels on Visual Vocabularies for Image Categorization,\u201d in Proc. of the IEEE Conf on Computer Vision and Pattern Recognition (CVPR), Minneapolis, Minn., USA (June 2007); Yan-Tao Zheng, Ming Zhao, Yang Song, H. Adam, U. Buddemeier, A. Bissacco, F. Brucher, Tat-Seng Chua, and H. Neven, \u201cTour the World: Building a web-scale landmark recognition engine,\u201d IEEE Computer Society Conference, 2009; Herve Jegou, Matthijs Douze, and Cordelia Schmid, \u201cImproving Bag-Of-Features for Large Scale Image Search,\u201d in IJCV, 2010; F. Perronnin, J. S\u00e1nchez, and T. Mensink, \u201cImproving the fisher kernel for large-scale image classification,\u201d in ECCV 2010; Jorge S\u00e1nchez and Florent Perronnin, \u201cHigh-dimensional signature compression for large-scale image classification,\u201d in CVPR, 2011.","In accordance with one aspect of the exemplary embodiment, a method for computing a place profile includes providing a geographical definition of a place and retrieving a set of images based on the place definition. Retrieved images in the set are classified with a classifier. The classifier has been trained to generate image-level statistics for a finite set of classes. The method further includes aggregating the image-level statistics to generate a place profile for the defined place. One or more of the steps of the method may be performed with a computer processor.","In accordance with another aspect of the exemplary embodiment, a system for computing a place profile includes a place definition component for generating a definition of a place, based on information received from a user. An image selection component retrieves a set of images corresponding to the defined place from an associated database of images, the images being tagged with geoposition information. An image classification trained on a finite set of classes classifies the retrieved images to generate image-level statistics. A profile component aggregates the image-level statistics to generate a place profile for the geographically-defined place. The components of the system may be implemented by a processor and\/or stored in non-transitory memory.","In another aspect of the exemplary embodiment, a method for generating a visual representation of a place profile includes, for a predefined set of activity classes, training a classifier on training images labeled according to the activity classes in the set of classes. A geographical definition of a place is generated, based on information supplied by a user. A set of images is retrieved from an image database based on the place definition and on geoposition information associated with images in the database. With the trained classifier, image-level statistics are computed for the retrieved images in the set of retrieved images. The image-level statistics are aggregated to generate a place profile for the defined place and a visual representation of the place profile is generated for display to a user. One or more of the steps of the method may be performed with a computer processor.","Aspects of the exemplary embodiment relate to a system and method for automatically computing a visual profile of a place (\u201cplace profile\u201d) for a given place using repositories of geo-tagged visual data (e.g., photographic images and\/or videos). In particular, statistics are computed with respect to a given taxonomy of classes. Aspects of the exemplary embodiment also relate to a place profile computed from geo-tagged images using image classification and to a user interface which allows a user to query the system about a particular place of interest and to visualize the results. Various applications of such a place profile are also disclosed herein, such as recommender systems and targeted advertising.","The exemplary place profile is a statistical description of a given place in terms of a set of classes. This profile of a given place (e.g. a region, a city, or a district) is computed automatically, with respect to the pre-defined taxonomy of classes. Typical classes (categories) correspond to activities, such as tourist activities (e.g., types of activities that tourists may be interested in when visiting a new place). The method can leverage the substantial amount of geo-tagged data available on the Internet.","The geo-tagged image data may be specifically generated for the task. However, by using geo-tagged photo collections which have not been specifically designed for the place profile generation, the activity-based profile of a place based on these photo collections reflects what people actually did and found to be of interest, rather than what a professional photographer may expect to be of interest.","The exemplary method uses supervised learning to train a classifier to assign activity classes to unlabeled images.","The exemplary visual place profile may complement other sources of information, e.g., profiles based on textual information. However, a significant advantage of the exemplary visual place profile is that it can make use of geo-tagged images while text is generally not geo-tagged.","With reference to , a computer-implemented system  for generating a profile for a place is illustrated. The system  may be implemented in one or more computing devices and the exemplary arrangement of computing devices is intended to be illustrative only. The system  includes a server computer  which communicates with a client computing device  via a wired or wireless connection , such as a local area network or wide area network, such as the Internet. A user operating on the client device  can enter a query  concerning a geographical place of interest and view a visual profile  of the place through a graphical user interface (GUI) generated by the system , alone or in cooperation with the client device , e.g., using a web browser  operating on the client device .","The exemplary visual place profile  includes class scores for each of a plurality of predefined classes, the class scores being based on the automatic classification of a set of photographic images which are geographically associated with a place of interest. In general, the visual profile  can include non-zero class scores for a plurality of the predefined classes, e.g. as a histogram of class scores generated through aggregation of image-level statistics, as explained in greater detail below.","The exemplary system  has access to a database  of images. The image database  includes a large collection of geographically labeled (geo-tagged) images . Each geo-tagged image  in the database  includes image data  for an array of pixels forming an image as well as metadata  associated with the image data, which provides information on a geographical location in which the image was captured, e.g., according to coordinates of longitude and latitude. Typically, each geo-tag  includes geo-positional (GPS) information which is acquired automatically at the time of image capture by a GPS component  of an image capture device , such as a camera, mobile phone, laptop, or the like. The information  may be in the form of exchangeable image file format (EXIF) data in the case of image files from cameras, smartphones the like. However, it is also contemplated that the geo-tag  could be manually created by a user, for example, using an external GPS device to obtain the information when the camera  is not equipped to do so.","The exemplary digital images  are photographic images, such as individual photographic images (photos), video images (videos), and combinations thereof which may be multicolor (color) and\/or monochrome (black and white) images. The images  may be stored in any convenient file format, such as JPEG, Graphics Interchange Format (GIF), JBIG, Windows Bitmap Format (BMP), Tagged Image File Format (TIFF), or the like. The image data  may include colorant values, such as grayscale values, for each of a set of color separations, such as RGB, or be expressed in another other color space in which different colors can be represented. In general, \u201cgrayscale\u201d refers to the optical density value of any single color channel, however expressed (RGB, YCbCr, etc.).","The database  may be stored remotely from the computer , e.g., on a remote server  hosted by an image sharing website. The computer  has access to the database  via a wired or wireless link , analogous to link .","Computer  includes main memory  which stores software instructions  for performing the exemplary method described with reference to , and a processor , in communication with the memory , for executing the instructions . The processor , such as the computer's CPU, may also control the operation of computer . Data memory , separate from or integral with the main memory , stores data used in computing a place profile . The computer  includes one or more network interface (input\/output I\/O) components ,  for enabling the server to communicate with external devices , , via the network , . Hardware components , , , ,  of the system  may communicate via a data\\control bus .","Memory  may be resident on the computer  or on a linked computing device(s). Client device  and remote server  may be similarly configured to the illustrated computer , except as noted, i.e., with a respective processor, memory, and a network interface. Client device  also includes a display device , such as a computer monitor or LCD screen, and one or more user input devices , such as a keyboard, keypad, cursor control device, touch screen, or the like, for inputting commands to the client device processor. The client device  may be a specific or general purpose computer, such as a PC, such as a desktop, a laptop, tablet, or palmtop computer, a portable digital assistant (PDA), mobile phone, or other computing device having an associated display. In some embodiments, the system  may be resident wholly or partly on client device .","The digital processor  can be variously embodied, such as by a single-core processor, a dual-core processor (or more generally by a multiple-core processor), a digital processor and cooperating math coprocessor, a digital controller, or the like. In general, any device, capable of implementing a finite state machine that is in turn capable of implementing the flowchart shown in , can be used as the processor. The memory or memories ,  may represent any type of non-transitory computer readable medium such as random access memory (RAM), read only memory (ROM), magnetic disk or tape, optical disk, flash memory, or holographic memory. In one embodiment, the memory ,  comprises a combination of random access memory and read only memory. Exemplary memory ,  stores instructions for performing the exemplary method and data used and generated therein, as well operating instructions for operating the computer .","The term \u201csoftware,\u201d as used herein, is intended to encompass any collection or set of instructions executable by a computer or other digital system so as to configure the computer or other digital system to perform the task that is the intent of the software. The term \u201csoftware\u201d as used herein is intended to encompass such instructions stored in storage medium such as RAM, a hard disk, optical disk, or so forth, and is also intended to encompass so-called \u201cfirmware\u201d that is software stored on a ROM or so forth. Such software may be organized in various ways, and may include software components organized as libraries, Internet-based programs stored on a remote server or so forth, source code, interpretive code, object code, directly executable code, and so forth. It is contemplated that the software may invoke system-level code or calls to other software residing on a server or other location to perform certain functions.","As described in further detail below with reference to the exemplary method, the system includes various software components , , , , , ,  for implementing the method, including a classifier training component , for definition and training of a taxonomy of classes. For example, the classifier training component  takes as input training data , which includes a set of geo-tagged images labeled according to class, and learns parameters of a classifier , based on the training data . The classifier  can be a set of trained class models, one for each class (or a single multi-class classification model). A signature generation component  generates representations of images  in a suitable format for use in training the class models  and for generating representations of images . A place definition component  generates a definition  of a place under consideration, e.g., based on input geographical coordinates of latitude and longitude. An image selection component  retrieves a set  of the geo-tagged images  from the database , based on the place definition . An image classification component  applies the trained class model(s)  to the retrieved images  and outputs image-level statistics. A profile component  aggregates the image-level statistics into a place-level profile , which can be presented to a user and\/or used to generate other information, such as advertizing content, or the like. An interface generator  generates a graphical user interface for display to a user on the display device .","Components , , , , , , , of the system may be implemented in hardware or a combination of hardware and software. As will be appreciated, the components need not all reside on the same device . Additionally, when the class models  have been trained, component  may no longer be needed and can be omitted. In other embodiments, the models  may be trained with a separate computer system. Additionally, rather than retrieving images  in their entirety, the image signatures of the images  in the database  may be pre-computed, such that the \u201cretrieved images\u201d  may simply be retrieved image signatures for the respective images  and the images' corresponding geo-tags .","With reference also to , a flow chart illustrating a computer-implemented method for generating a place profile is shown. The method begins at S and may proceed as follows.","At S, a taxonomy of classes is defined and a classifier (multi-class or set of class models ) is trained for these classes using labeled training data .","At S, a geographical definition of a given place is generated or otherwise provided.","At S, a set  of images corresponding to the place definition generated at S is retrieved from database  and may be stored in local memory .","At S, the retrieved images  are classified (using their image signatures as input to the classifier ) to provide image-level statistics with respect to the given taxonomy of classes.","At S, the image-level statistics output as S are aggregated to generate a place-level profile .","At S, the place profile  or a representation thereof is output, e.g., to client device .","At S, the place profile  may be used as information in an application such as for generating advertizing content or a recommender system.","The method ends at S.","Further details of the system and method will now be described.","1) Definition and Training of a Taxonomy of Classes (S)","The exemplary method relies on the pre-definition of a finite set of two or more classes. Exemplary classes can correspond loosely to activities, such as activities related to man-made structures, e.g., \u201carchitecture,\u201d \u201cvisiting museums\u201d; sports-related activities, such as \u201cclimbing,\u201d \u201cskiing,\u201d \u201cswimming,\u201d \u201ctennis\u201d; specific location-related activities, such as \u201clandmark sightseeing,\u201d \u201cbeach,\u201d \u201cmountains\u201d; and general-interest activities such as \u201cfood,\u201d \u201cmusic,\u201d and the like. For some applications there may be at least 10, 50, or at least 200 of such classes, such as up to 1000 classes, or more. However, it is to be appreciated that the granularity of the classes may depend on the application. For some applications, for example, a class such as \u201carchitecture\u201d may be sufficient but for other applications, a more refined taxonomy, e.g., church, castle, temple, stadium, and so forth may be more appropriate. Additionally, classes may be grouped hierarchically, so that, for example, where there are only a few images in the classes that form a group, a higher level class may be presented.","As will be appreciated, the above classes are exemplary only and the number and type of classes can be varied to suit the application. The taxonomy can be defined by a specific user as a preliminary step (e.g., the user is allowed to select from a large number of predefined classes for which labeled training images are available) or a predefined set of classes may be developed by considering the type of activities in which users in general may be interested.","A labeled set of images  is used to train the corresponding class models . The exemplary classifier models(s) may be linear or nonlinear, and may be trained with any suitable classification training algorithm, such as sparse logistic regression, na\u00efve Bayes, linear discriminant analysis, support vector machines, linear regression, or any other suitable machine learning method.","A quantitative image representation or \u201csignature\u201d is derived from each training image , which together with the each training image's manually applied label(s) serve as the input to the class model, during training. The exemplary image signatures are vectorial representations, which can be of a fixed length and which are derived by computing, for each of one or more low-level local feature types, a set of statistics for patches of the image and then aggregating these statistics into an image-level representation.","In this approach, the image is divided into patches (regions), a representative vector is constructed for each patch, and these results are concatenated or otherwise combined to generate the signature. The use of patches distributed across the image ensures that the quantitative representation is representative of the image as a whole while also including components representing smaller portions of the image. In some approaches, the patches may be of varying sizes and may overlap in order to provide components representing different objects (e.g., faces, animals, sky, and the like) having different size scales and located at different, and possibly overlapping, places in the image.","The patches can be obtained by image segmentation, by applying specific interest point detectors, by considering a regular grid, or simply by random sampling of image patches. For example, at least about 100 patches are extracted from each region. More generally, over the image as a whole, at least 1000 and in some cases, at least 10,000 patches may be extracted. The number of patches can be up to 100,000 or more, depending on the size of the image file.","The low level features which are extracted from the patches are typically quantitative values that summarize or characterize aspects of the respective patch, such as spatial frequency content, an average intensity, color characteristics (in the case of color images), gradient values, and\/or other characteristic values. In some embodiments, at least about fifty low level features are extracted from each patch, which may be concatenated or otherwise combined to generate a descriptor of the patch. However, the number of features that can be extracted is not limited to any particular number or type of features for example, 1000, 10,000, or 100,000 low level features could be extracted depending on computational capabilities. In the exemplary embodiment, the low level features include local (e.g., pixel) color statistics, and texture. For color statistics, local RGB statistics (e.g., mean and standard deviation) may be computed. For texture, gradient orientations (representing a change in color) may be computed for each patch as a histogram to generate gradient feature descriptors, such as Scale Invariant Feature Transform (SIFT) descriptors (SIFT-like features). In the exemplary embodiment two (or more) types of low level features, such as color and texture, are separately extracted and a high level representation of the patch or image is based on a combination (e.g., a sum or a concatenation) of two descriptors, one for each feature type.","In the exemplary embodiment, SIFT descriptors, as described by Lowe, in \u201cObject Recognition From Local Scale-Invariant Features,\u201d International Conference on Computer Vision (ICCV), 1999, are computed on each patch. SIFT descriptors are multi-image representations of an image neighborhood, such as Gaussian derivatives computed at, for example, eight orientation planes over a four-by-four grid of spatial locations, giving a 128-dimensional vector (that is, 128 features per features vector in these embodiments). Other descriptors or feature extraction algorithms may be employed to extract features from the patches. Examples of some other suitable descriptors are set forth by K. Mikolajczyk and C. Schmid, in \u201cA Performance Evaluation Of Local Descriptors,\u201d Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), Madison, Wis., USA, June 2003, which is incorporated in its entirety by reference.","In one illustrative example employing SIFT features, the features are extracted from 32\u00d732 pixel patches on regular grids (every 16 pixels) at five scales, to provide 128 dimensional SIFT descriptors. Other suitable features include simple 96-dimensional color features in which a patch is subdivided into 4\u00d74 sub-regions and in each sub-region the mean and standard deviation are computed for the three R, G and B channels. The number of features is optionally reduced, e.g. to 64 dimensions, using Principal Component Analysis (PCA).","For each type of low-level local feature, a set of statistics is computed for each patch in the form of a local descriptor. The statistics are aggregated to generate a region level or image-level representation. For computational efficiency reasons, two techniques for generating image representations which model the distribution of feature sets using fixed-length vectorial representations can be employed: the bag-of-visual-words (BOV) and the Fisher vector (FV) (or Fisher kernel (FK)).","In the \u201cbag-of-visual-words\u201d (BOV) method, the \u201cbag of words\u201d is suitably a vector or other representation whose components indicate occurrence frequencies (or counts) of \u201cvisual words\u201d in the image. In such \u201cbag of visual words\u201d representations, each visual word in a visual vocabulary corresponds to a grouping of typical low-level features. The Fisher kernel (FK) is a generic framework which combines the benefits of generative and discriminative approaches for image classification. In the context of image classification, the FK has been used to extend the bag-of-visual-words (BOV) representation by going beyond count statistics. See, e.g. Perronnin et al., \u201cFisher kernels on visual vocabularies for image categorization\u201d in CVPR (2007).","Descriptions of such methods for computing image signatures are provided in the above-mentioned list of references, incorporated by reference herein. By way of example, the method and system described in Jorge S\u00e1nchez and Florent Perronnin, \u201cHigh-dimensional signature compression for large-scale image classification,\u201d in CVPR, 2011, and in copending U.S. application Ser. No. 12\/890,789, entitled IMAGE CLASSIFICATION EMPLOYING IMAGE VECTORS COMPRESSED USING VECTOR QUANTIZATION, by Jorge S\u00e1nchez, et al. may be employed (both of these references will be referred to herein as \u201cS\u00e1nchez\u201d). This image classification technology is particularly suited to a large-scale classifier where large numbers of images may be used in training and\/or classification.","For example, the classifier training component  and\/or image classification component  includes or accesses the signature generator  for generating image signatures of training images  and retrieved images . The signature generator  includes a patch extractor, which extracts and analyzes the content-related low level features of patches of the image , such as shape, texture, color, or the like. The extracted descriptors (such as vectors) from each patch are concatenated and optionally reduced in dimensionality, to form a features vector. The feature vectors of an image are assigned to clusters. For example, a visual vocabulary is previously obtained by clustering low-level features extracted from training images, using for instance K-means. Each patch vector is then assigned to a nearest cluster and a histogram of the assignments can be generated. In other approaches, a probabilistic framework is employed. For example, it is assumed that there exists an underlying generative model, such as a Gaussian Mixture Model (GMM), from which all the vectors are emitted. In this case, the visual vocabulary can be estimated using the Expectation-Maximization (EM) algorithm. In either case, each visual word in the vocabulary corresponds to a grouping of typical low-level features. The visual words may each correspond (approximately) to a mid-level image feature such as a type of visual (rather than digital) object (e.g., ball or sphere, rod or shaft, etc.), characteristic background (e.g., starlit sky, blue sky, grass field, etc.), or the like. Given an image to be assigned an image signature, each extracted feature vector is assigned to its closest visual word in the previously trained vocabulary or to all visual words in a probabilistic manner in the case of a stochastic model. A histogram is computed by accumulating the occurrences of each visual word. This is the image's signature.","Typically, each class model  has adjustable parameters whose values are determined by training on the labeled training set. The objective of the training is to select the adjustable parameters such that the output of the classifiers substantially agrees with the classification labels assigned to the images of the training set. The exemplary class models  are binary classifiers which are each trained on a set of positive samples for the class and a set of negative samples, the positive samples being the signatures of images labeled with the respective class of images and the negative samples being the signatures of images not labeled with the respective class of images (which may be drawn at random from the remainder of the training set). The trained class models  are then used by the image classification component  to assign an unlabeled, retrieved image  to one or a subset (less than all) of the classes, or assign it probabilistically over all classes, based on its computed image signature. A soft (probabilistic) classification can be converted to a hard classification by thresholding. For example, the trained class models may each assign a score based on how well an image signature of the retrieved image matches the model.","The local descriptors extracted from the patches can comprise SIFT descriptors. See, e.g. Lowe, \u201cDistinctive image features from scale-invariant keypoints,\u201d IJCV vol. 60 (2004). In one illustrative example employing SIFT features, the features are extracted from 32\u00d732 pixel patches on regular grids (every 16 pixels) at five scales, using 128-D SIFT descriptors. Other suitable features include simple 96-D color features in which a patch is subdivided into 4\u00d74 sub-regions and in each sub-region the mean and standard deviation are computed for the three R, G and B channels. The number of features is optionally reduced, e.g. to 64 dimensions, using Principal Component Analysis (PCA). These are merely illustrative examples, and additional and\/or other local descriptors can be used.","In the exemplary embodiment, an image vector is computed for each image (or for each of a plurality of image regions) based on the extracted local descriptors of that image (or image region). Advantageously, partitioning the image into regions retains spatial location information in the image, for example, faces and sky are typically found in the top portion of an image.","In some illustrative examples, a Fisher vector is computed for each image region by modeling the extracted local descriptors of the image region using a mixture model to generate a corresponding image region vector having vector elements that are indicative of parameters of mixture model components of the mixture model representing the extracted local descriptors of the image region. The exemplary mixture model is a Gaussian mixture model (GMM) comprising set of Gaussian functions (Gaussians) to which weights are assigned in the parameter training, each Gaussian represented by its mean vector, and covariance matrix. It is assumed that the covariance matrices are diagonal. See, e.g., Perronnin et al., \u201cFisher kernels on visual vocabularies for image categorization\u201d in CVPR (2007). Where Fisher vectors are computed for image regions, these can be concatenated to form the final image vector representing the image. Methods for computing Fisher vectors are more fully described in S\u00e1nchez.","The trained GMM is intended to describe the content of any image within a range of interest (for example, any color photograph if the range of interest is color photographs; or, any image of a black and white document if the range of interest is black and white documents, or so forth).","The image signature (vector) representing the image  is substantially more compact than the image  itself, where compactness or size is measured by the amount of memory or storage occupied by the image vector or image. However, the image vector can still be relatively large. By way of example, in some suitable embodiments: the GMM includes 256 Gaussian components; the descriptors have dimensionality D=64; and partitioning is employed with the number of image regions being R=4. If the Fisher vector of includes gradients computed for each Gaussian mean \u03bcand for each Gaussian covariance \u03c3, but not for each Gaussian weight \u03c9, then the number of gradients P computed per Gaussian component is P=2D=128 gradients. In this case the Fisher vector has dimensionality E=N\u00d7P\u00d7R=256\u00d7128\u00d74=131,072 dimensions. If four-byte floating point arithmetic is used to represent the dimensions, then the Fisher vector for the single image occupies about 0.5 megabyte.","A dimensionality reduction method such as PCA can be used to reduce dimensionality. Compressing the image vectors in this way does not always result in retention of the most useful information. A vector quantization algorithm as disclosed in S\u00e1nchez can alternatively be used to reduce the information stored. In this approach, a set of standard vectors in the vector space is defined and a given image vector is compressed by (1) identifying the closest standard vector and (2) representing the image vector by a short \u201ccodeword\u201d representing that closest standard vector. A \u201ccodebook\u201d provides the associations between the codes and the corresponding standard vectors. Training of the class models  on such compressed vectors can be performed as described in S\u00e1nchez.","2) Geographical Definition of a Place (S)","In this step, a geographical region of interest (i.e., a place) is defined. In the exemplary embodiment, the user may install an application on the client device which allows the user to interact with the system . As illustrated in , a user is provided with a user interface  through which a place of interest in selected. The user may be provided with a menu which displays various options for specifying a geographical region of interest. The exemplary user interface includes a visual representation  of a geographical region which is displayed on the display device  of the client device . The visual representation  may be a map, a satellite view, simplified representation of a region, e.g., pinpointing major towns and cities, or combination thereof. For example, as shown in , the user is presented with a map  via the user interface  and asked to click on a place of interest, e.g., by manipulation of a displayed cursor  with the cursor control device . The user may move the map  to display a different general location using conventional navigation tools, such as up\/down\/left\/right arrows and\/or an enlarge\/reduce slider.","The user may be requested to click on a point location  of interest. In some embodiments (), the user may be asked to specify a place radius r, e.g., in miles or kilometers, or subdivisions thereof around the center . A circle of radius r is then displayed on the map and the user may enlarge or reduce the radius using the cursor control device or other input device, and in some cases, move its center . The area bounded by the boundary circle  of radius r is then input to the system  as the user's selection of the place of interest.","In another embodiment (), the user may be provided with the option to generate a free-form place boundary . The user hand-draws the boundary  for the region of interest on a map , e.g., by moving the cursor  or with a finger on a touch screen.","In yet another embodiment, a user may enter the name of the place of interest, such as \u201cLondon,\u201d \u201cLoire Valley,\u201d or the like, or may select from a menu which includes a list of places. In another embodiment, a user may enter the geographical coordinates of a place of interest, e.g., using decimal degrees with negative numbers for South and West, such as \u201c42.3126, \u2212108.4264\u201d.","The information on the user's selection is received by the place definition component  of the system and stored in memory  during processing.","Various methods for defining the place, based on the user's selection, are contemplated. In some embodiments, the place is defined directly by its geographical coordinates, e.g., center and a radius r. r may be predefined, such as 1 km, user-selected, as described above, or computed as a function of the number of images in the locality e.g., to ensure at least a threshold minimum number of retrieved images or no more than a threshold maximum number of retrieved images.","In some cases, the geographical coordinates of famous places (especially cities) may be retrieved from an information source, such as from the Internet, e.g. from websites such as www.maplandia.com. Thus, for example, if a latitude and longitude are entered, the system retrieves the maximum and minimum values of latitude and longitude stored for the recognized place which is centered approximately at or includes the selected latitude and longitude.","In the case where a user defines a free-form place boundary , the place definition may be computed by partitioning the map region into an array of cells  and for each cell which is wholly within the boundary , or a majority of it is within the boundary, considering the cell as part of the place (as shown by the symbol \u201cX\u201d in ).","The output of S is a set of geographical coordinates which serve as a geographical definition of the place. These coordinates may be in the form of minimum and maximum longitude and latitude values to define a rectangle in GPS space, for one or more cells . The values may be defined in decimal degrees from, for example, \u221290.000000 to +90.000000 for latitude and \u2212180.000000 to +180.000000 for longitude, or other number of decimal places, where, for example, the maximum and minimum values for each of longitude and latitude differ by at least 0.000001 or 0.0001 decimal degrees, e.g., up to a maximum of 1 or 5 degrees. Or the geographical place definition may be a single value for each of longitude and latitude with a radius of, for example, at least 0.01 Km, or 0.1 Km, or 1 Km and may be up to 10 Km or 100 km, i.e., defining a circle in GPS space.","3) Image Gathering (S)","A large database  of geo-tagged images is available to the system . The database may be accessed remotely from an external server  and\/or it may be resident on the computer , or even on the client device . A combination of databases  may be accessed. The methods disclosed herein are particularly suited to photographs  stored by a variety of people on online image sharing sites , such as the Flickr\u2122 (www.flickr.com), Picasa\u2122 (www.picasa.google.com) and Panoramio\u2122 (www.panoramio.com) websites, or on social networking websites, such as Facebook\u2122, LinkedIn\u2122, or MySpace\u2122, to the extent the photographs on such websites can be accessed. These websites contain millions of geo-tagged images. The method is not limited to such websites as there are many web 2.0 photo-sharing websites which could be used or any other collection of geo-tagged images may be used. Some of these photo sharing sites enable searching for images based on their geographic coordinates. For example, www.flickr.com provides an application programming interface (API) which enables users to download images in terms of the minimum and the maximum of the longitude and the latitude (i.e., a bounding box).","Using the geographical coordinates output from S, a set  of images  is downloaded. The set may be quite large, e.g., at least 100 or more images are retrieved. If the geographical coordinates of S correspond to a complex region (as in ), the system  can approximate the region as a set of non-overlapping cells  (where each cell corresponds to a primitive shape such as a small square) and run multiple queries on the server , e.g., one query for each cell.","With photographs stored in JPEG file format, the geo-tag information  is typically embedded in the metadata (e.g., stored in Exchangeable image file format (EXIF) or Extensible Metadata Platform (XMP) format). Latitude and longitude are stored in units of degrees with four or more decimal places.","For each retrieved image in set , additional meta-data can be collected, such as the user identifier, if it is available. This information can be used in S to smooth the results to account for a disproportionately large number of images attributed to a single user identifier.","4) Classification of Retrieved Images (S)","The image classification step includes classifying the images  retrieved at S with respect to the taxonomy of classes defined and trained in S. For each image , a signature is generated by the signature generator , using the same method as for the training images . The signature is input to each of the class models  which each output a score Sfor the corresponding class C. The image classification component  may output the one (or more) most likely class C for each of the image  or all the classes whose score Sexceeds a given confidence threshold \u03b8(along with their confidence). For example, if the class model outputs a score in the range [0,1] and the confidence threshold is 0.6, an image with a score for the class of 0.7 may be tagged with the class label and with a confidence value Vbased on the score S. The confidence value Vmay be the same as the score S, i.e., 0.7 in this case. It is possible for an image not to be assigned to any class if none of the class scores Sexceeds the preselected confidence threshold \u03b8for the class.","As will be appreciated, the larger the number of considered classes, the higher the probability of assigning incorrect class tags to an image. This problem can be mitigated by setting the confidence threshold \u03b8to achieve a target classification accuracy (e.g., by using a new set of labeled training images to test the results of each trained class model). The confidence threshold \u03b8can additionally or alternatively be set as a function of the number of images downloaded at S. Indeed, if the number of downloaded images is large, then a high confidence threshold \u03b8value can be set (and therefore a large number of low-confidence tags is discarded), while still having enough tags available to compute meaningful statistics at S. For example, only the downloaded images with the tags having the top K confidence Vmay be retained from the set  for computing the place profile.","Another method for reducing the chance of an incorrect tag being assigned to an image is to consider whether geographically proximate retrieved images in the set  have the same tag and filter out tags that are determined to be outliers on this basis. This is based on the assumption that if an image corresponds to a popular (probable) class C, then other images with the same class tag should be found in its spatial neighborhood (i.e., within a small sub-region of the defined place). For example, for each class C, a tag verification process based on density estimation is performed. Assume that N images with index i=1 . . . N were labeled with a given class tag C and that the 2D coordinates (in terms of longitude and latitude) of these images are denoted z. A kernel k(.,.) (such as a Gaussian kernel) is placed around each of these N images and, at a location z, the following estimate of density Dcan be computed:\n\n=\u03a3()\n","If the local density Dfor a given image is lower than a threshold \u03b8, then the tag C for this image is removed. The bandwidth of the kernel and the threshold \u03b8reflect the level of confidence in the number of images of a given activity which should have been taken within a given radius. These parameters can be learned automatically on a per-class basis (e.g., through grid search since there are only two parameters (the kernel bandwidth and threshold \u03b8, mentioned above) to achieve a target classification accuracy or coverage.","The remaining tags\/class scores\/confidence values, after any filtering, constitute the image-level statistics to be input to the profile component , which aggregates the scores according to a predetermined aggregating function.","5) Score Aggregation (S)","A profile  of the determined place is computed at S. This may include aggregating the image-level statistics computed at S into a place-level representation . In one embodiment, the profile may be generated by counting, for each class C, the number of images Nwith the associated tag. The counts can be weighted by the associated confidence values V. A histogram can then be generated which represents the optionally weighted counts for all the classes. This histogram representation can be L1 normalized to account for the number of retrieved (or labeled) images: each histogram bin thus represents the proportion of images (C) assigned to each activity. Cis referred to herein as the class or activity score ().","It is to be appreciated that a class may have a very high score Cbecause a single (or few) enthusiastic tourist(s) took a large number of pictures pertaining to that class (i.e., are disproportionately represented in the retrieved images). However, in general, a user will be interested in activities which were of interest to many other users. Therefore, rather than counting the number of images, the number of users who took pictures pertaining to a given activity (as represented by the user ID in the image metadata) can be counted and used as the class score C. In this case, C. does not take into account the number of individual images of a given class taken by the same user and therefore better reflects the class\/activity popularity. In other embodiments, the class score may be a function of both the number of images and number of user IDs.","For some applications, users may not be interested in the proportion of images (and\/or users) related to a given class\/activity in the absolute but rather, relative to other places. Therefore, the class score Cof a given activity for a given place can be turned into a relative value, e.g., between 0 and 100, which reflects the rank of this place with respect to other comparable places for the considered class. As \u201ccomparable\u201d places, the system may only compare, for example, cities of a given size\/population with other cities of the same approximate size\/population. For example the class score computed as above could be expressed as a ratio, e.g., a percentage, of the average (e.g., median or mean) score for that class from a set of place profiles. Similarly, each class score can be turned into a binary value: 0 if the score of a class for a place is lower than the average (e.g., median or mean) score for this class as estimated on a set of places and 1 if the score is higher than the average.","In some embodiments, the system  may consider only the top ranked or a few of the top ranked classes as representative of the place profile.","The place profile  may be output in numerical form, e.g., as a vector in which each element of the vector corresponds to a class and in general at least two of the elements are non-zero.","In some embodiments, a visual representation  of the place profile  can be generated for display to a user via the GUI. The profile  of a geographical place may thus be displayed or otherwise output as any one or more of:","a) a histogram of activity scores, as illustrated, for example, in  (e.g., showing only the most popular classes),","b) a ranked list of classes, e.g., including only the most popular classes in order of popularity with the highest score being assigned a rank of 1, and the other scores numbered sequentially in order of decreasing score from 2 to n, or as an unordered list,","c) a list of those classes which exceed the median score for the class as determined for a set of places, optionally ranked based on the class scores, or in any other suitable manner which is representative of the class-based statistics for multiple retrieved images.","In some embodiments, the representation may be interactive. For example, if the user clicks on the area of the display screen  showing, for example, the \u201ctransport\u201d score, images in the set of images  which have been tagged with the transport class by the classifier  may be retrieved from memory  and displayed to the user via the GUI.","6) Applications","Various applications are contemplated for the exemplary place profile (S). The following are intended as exemplary only.","A. Recommender Systems","In one embodiment, the place profile  is used in recommendation systems. For example, if a person has a profile which describes his or her interests (with respect to the classes\/activities defined at S), then the system can compute a match between the personal profile and a set of place profiles and subsequently return the places with the highest match. To build a personal profile, a user may be asked to enter explicitly his\/her interests through an interface. Alternatively, the personal profile may be computed automatically from the person's own images, as disclosed for example, in U.S. patent application Ser. No. 13\/050,587, incorporated herein by reference. As disclosed therein, the generating of a personal profile includes categorizing the set of the user's images into a finite set of image categories (here, activity classes), based on image content of the images in the set (e.g., using an image signature, computed as described above), using a classifier\/class models, which as\/have been trained on the finite set of image categories, such as models . The user's images may be retrieved, for example, from the client computing device , from a user's page of a social networking site, and\/or by querying database  by user ID. The match between the user profile and each of a set of place profiles can be computed based on a subset of classes, e.g., the top class(es) in the user profile and the top class(es) in the place profiles, or by computation of a suitable distance metric, such as the Earth Mover's distance, Euclidean distance, chidistance, Manhattan distance, or other suitable distance metric. The system may output the place profile having the highest computed similarity\/least distance to the user's profile or a small subset of the set of place profiles based on the computed matches.","The recommendation of a place could additionally or alternatively be based on past experiences. For example, a user may enter in the system a place or places that the user enjoyed and may query the system for similar places. In this embodiment, the system compares the place profile of the place with precomputed profiles of a set of other places to find similar places, based on their profiles.","B. Advertising","The place profile may be used in generation of targeted advertisements. For example, a set of different advertisements are associated with each of the classes\/activities defined at S. Then, if a person queries for a place (for instance, in a web search engine), an advertisement could be triggered if the corresponding class score for that place exceeds a given threshold. For example, a sporting goods seller may have advertisements for skis and bicycles. If the queried place has a higher score for skiing than cycling in its place profile and\/or the skiing score exceeds a given threshold, the advertisement related to skis is automatically output. The advertisement may be displayed to the user automatically, e.g., as a banner, pop-up, sidebar, or the like. Also, an advertisement could be placed automatically next to the text in a web-page if it refers to a given place.","C. Personal Profile","Place profiles  may be used to compute a personal profile. For example, the person submits a list of places that he likes and the system generates a place profile  for each place. The place profiles can then be aggregated (e.g., simply averaged, for each class) to form a personal profile. If the user has ranked or provided rating scores for the places, the person profile may be computed by performing a regression from the place profiles to the rating scores.","The method illustrated in  may be implemented in a computer program product that may be executed on a computer. The computer program product may comprise a non-transitory computer-readable recording medium on which a control program is recorded (stored), such as a disk, hard drive, or the like. Common forms of non-transitory computer-readable media include, for example, floppy disks, flexible disks, hard disks, magnetic tape, or any other magnetic storage medium, CD-ROM, DVD, or any other optical medium, a RAM, a PROM, an EPROM, a FLASH-EPROM, or other memory chip or cartridge, or any other tangible medium from which a computer can read and use.","Alternatively, the method may be implemented in transitory media, such as a transmittable carrier wave in which the control program is embodied as a data signal using transmission media, such as acoustic or light waves, such as those generated during radio wave and infrared data communications, and the like.","The exemplary method may be implemented on one or more general purpose computers, special purpose computer(s), a programmed microprocessor or microcontroller and peripheral integrated circuit elements, an ASIC or other integrated circuit, a digital signal processor, a hardwired electronic or logic circuit such as a discrete element circuit, a programmable logic device such as a PLD, PLA, FPGA, Graphical card CPU (GPU), or PAL, or the like. In general, any device, capable of implementing a finite state machine that is in turn capable of implementing the flowchart shown in , can be used to implement the method for generating a place profile.","The exemplary system and method have several advantages over existing methods and systems for recommending places and activities. Existing systems tend to be manually intensive and do not make use of the content of images and\/or image classification technologies.","Several references have proposed to mine automatically popular landmarks from collections of geo-tagged images. See, e.g., Y.-T. Zheng, et al., \u201cTour the world: building a web-scale landmark recognition engine,\u201d CVPR 2009, and S. Gammeter, et al., \u201cI know what you did last summer: object-level auto-annotation of holiday snaps,\u201d ICCV 2009. The exemplary system, rather than automatically discovering popular landmarks (unsupervised learning), is able to profile places according to a set of learned classifiers (a supervised learning approach), which can provide complementary information to such systems.","It will be appreciated that variants of the above-disclosed and other features and functions, or alternatives thereof, may be combined into many other different systems or applications. Various presently unforeseen or unanticipated alternatives, modifications, variations or improvements therein may be subsequently made by those skilled in the art which are also intended to be encompassed by the following claims."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":["FIG. 3","FIG. 1"]},{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 4","FIG. 3"]},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
