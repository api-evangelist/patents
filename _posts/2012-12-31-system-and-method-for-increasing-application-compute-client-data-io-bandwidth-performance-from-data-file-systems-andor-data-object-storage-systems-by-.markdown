---
title: System and method for increasing application compute client data I/O bandwidth performance from data file systems and/or data object storage systems by hosting/bundling all of the data file system storage servers and/or data object storage system servers in the same common global shared memory compute system as the application compute clients
abstract: Compute client processes are currently limited to a small percentage of the data I/O bandwidth available from a parallel file system's total aggregate I/O bandwidth. I/O bandwidth is limited by the pathways linking the parallel file system's data storage servers to the clients process's computer system and the number of stops the data makes in route before arriving in the client's memory space. Both of these limitations are overcome by hosting the entire set of file system storage servers or object storage servers within the same common global shared memory, parallel computer system, as the requesting client process. The data moves once, in parallel, from the storage devices, directly to the storage servers memory, which is the same memory address space as the compute client. This provides the compute client with low-latency access to the “Entire Aggregate I/O Bandwidth” provided by all of the File System Data Storage Servers.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09152603&OS=09152603&RS=09152603
owner: 
number: 09152603
owner_city: 
owner_country: 
publication_date: 20121231
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS-REFERENCE TO RELATED APPLICATIONS","FEDERALLY SPONSORED RESEARCH","REFERENCES CITED","U.S. PATENT DOCUMENTS","BACKGROUND OF THE INVENTION","BRIEF SUMMARY OF THE INVENTION","Advantages","REFERENCE NUMERALS","DETAILED DESCRIPTION OF A SAMPLE EMBODIMENT","SUMMARY","OPERATION"],"p":["This application claims the benefit of provisional patent application Ser. No. 61\/582,342, entitled \u201cMethod for increasing File System I\/O Bandwidth for a File Systems' Client by Hosting\/Bundling all of the File Systems' Data Servers in the same Shared Memory Compute System with the specific File Systems' Client,\u201d filed Dec. 31, 2011, by the present inventor.","Not Applicable.","Not Applicable.","U.S. Pat. No. 6,105,122 9\/2000 P. Keith Muller et al","U.S. Pat. No. 7,721,009 B2 5\/2010 Moreira et al.","1. Field of the Invention","This invention is directed to the field of large data input\/output (I\/O) transfer into, and out of, large compute systems and large data storage systems. It is particularly directed towards reducing the data access latency times and increasing the data I\/O transfer rates for compute clients processing extremely large data sets within large, shared-memory compute systems.","2. Description of Prior Art","Demand is growing to find solutions to the \u201cbig data\u201d challenge faced by government and business, for example, in areas such as fraud detection, remote sensor data analysis, financial modeling, social network analysis, and risk analysis. Many data intensive computing applications need to process an enormous amount of data in a timely manner, and if too much time is spent loading a required data set into a compute client's memory, then the entire production workflow will be delayed. The same logic holds for processes that are generating very large data sets. The newly generated large data sets need to be quickly made available for the next step in the larger production workflow. To support such an environment, a supercomputer architecture is required that incorporates hundreds or thousands of CPU cores for handling multiple processing threads and supersized memories to hold more of the very large data sets. Simply put, the more data the supercomputer can process at any given time, the quicker the results are presented to decision makers. When it comes to mission-critical pattern discovery problems in government and business, whoever obtains the answers first is one step ahead of their adversary or competitor.","The challenges to meeting \u201cbig data\u201d demands are many:","The compute hardware needs to be able to scale up to an extraordinary level\u2014thousands of CPU cores and multiple terabytes of memory\u2014which is far beyond the capacity of any commodity X86 server. Computer software needs to be able to take advantage of the scaled-up hardware platform. And, an extremely efficient data access input\/output system that can quickly load the \u201cbig data\u201d into system memory and just as quickly store the processed data onto reliable media is needed to complete the system.","The ability to build larger and larger compute systems has been facilitated by advances in massively parallel compute system techniques, algorithmic resource orchestration techniques (Map Reduce and others), low-cost\/high-performance CPU's, and the advent of both open-source and proprietary Parallel File Systems. Parallel File Systems such as \u201cLustre\u201d [See \u201cLustre scalable storage\u201d Copyright 2006 Cluster File Systems \u2014Rights owned by ORACLE Corp.], and IBM's GPFS (General Parallel File System) [Refer to F. B. Schmuck and R. L Haskin, GPFS: a shared-disk file system for large computing clusters, in proceedings of Conference of Files and Storage Technologies (FAST'02), 2002] have allowed these newer and larger parallel compute systems to scale up to higher and higher numbers of compute nodes and compute-cores and still have enough File System I\/O Bandwidth to allow for very good compute performance on many classes of applications.","Parallel File Systems generate very high \u201cTotal I\/O\u201d Bandwidth levels by utilizing multiple \u201cFile System Data Storage Servers\u201d like the dual socket server  in  (Lustre uses \u201cObject Storage Servers (OSS's) and IBM's GPFS uses NSD Servers (Network Shared Disk Servers)). Each File System Data Storage Server (OSS or NSD Server) is attached to one or more \u201cData Storage Devices,\u201d like  in , via a storage network fabric  that in many HPC environments is implemented using Infiniband (IB) switches and (IB) Host Channel Adapters (HCA's) . The \u201cData Storage Device\u201d  often consists of a \u201cStorage Controller\u201d , and a High-Performance or High-Capacity Disk Storage Devices  or Solid-State Disk (SSD) Storage Devices (many new systems are using SSDs in combination with SATA or SAS Disks to balance total I\/O operations with total storage capacity). Sometimes \u201cFile System Data Storage Servers,\u201d like  in , are paired with Storage Subsystems, like  in , that are implemented as raw \u201cTrays-of-Disks\u201d without Disk-Controllers. These Trays-of-Disks are referred to as JBOD's (just a Bunch Of Disks), and require additional Data-Block management software to run within the \u201cFile System Data Storage Server\u201d  to accomplish the functionality required for the \u201cJBOD\u201d to serve as a suitable Storage Device . The \u201cZetta File System,\u201d or ZFS [open source licensed By Oracle Corp.], is one candidate application for handling JBOD's and is being utilized by the Lawrence Livermore National Labs (LLNL) team, in combination with the Lustre File System as a part of their \u201cNext-Generation\u201d Sequoia Super Computer installation which hopes to have a \u201cTotal File System I\/O Bandwidth of between 500 GB\/s and 1000 GB\/s.","The data blocks in the Parallel File System are \u201cStriped\u201d or spread across multiple \u201cFile System Data Storage Servers,\u201d like the  grouping of , and their associated collective sets of Storage Devices  in . The sequence of events for a compute client to receive the data stored in the Parallel File Systems after a client data request is:","1\u2014The data blocks are copied from \u201cDisk Storage Devices \u2014 data storage locations\u201d and into \u201cStorage Controllers 107 memory\u2014Data Movement #1,","2\u2014The data blocks are then copied from \u201cStorage Controller  memory\u201d into \u201cFile System Data Storage Servers  memory\u201d\u2014Data Movement #2,","3\u2014The data blocks are then copied from all of the \u201cFile System Data Storage Servers  memory\u201d into the client - memory space\u2014Data Movement #3.","Moving the data blocks three times before they can be utilized is very inefficient, but for many parallel applications the inefficiency is accepted as current practice and the job schedules are allocated by compute time required as well as data I\/O transfer time. All of these \u201cData Movement processes\u201d are running in parallel, simultaneously across all of the File System Data Storage Servers and their respective Storage Controllers and Disk Storage Devices. The aggregate data I\/O speed for the entire File System is a function of the available bandwidth for all of the File System Data Storage Servers operating in unison. The grouping of File System Data Storage Servers  in  has the potential I\/O bandwidth of 8 GB\/s because each of the 4 servers can maintain 2 GB\/s and the set of 2 disk Storage Devices,  in , can each maintain 4 GB\/s.","The total I\/O performance of the Parallel File Systems can be scaled up by adding more File System Data Storage Servers, see  in , and more Disk Storage Devices,  in . The additional server and storage resources allow the example Parallel File System in  to provide an aggregate potential data I\/O bandwidth of 20 GB\/s instead of the original potential of I\/O speed of 8 GB\/s. The combination of adding more Storage Servers and more Storage Devices creates a very high Total potential I\/O Bandwidth solution for use by the massively parallel compute systems and their applications.","The use of Parallel File Systems, like Lustre and GPFS, for massively parallel compute systems has worked well because each compute node in the massively parallel compute system typically needs to use only a small portion of the I\/O Bandwidth to accomplish its compute tasks and to deliver intermediate or final results. The High Speed Parallel File Systems in use at Large Supercomputing Centers are used to take very rapid \u201cSnap-shots\u201d of intermediate compute results for complex compute jobs that can sometimes take days or weeks to complete. The snapshots are called \u201cCheck Points\u201d and they require very High Total I\/O Bandwidth since each of the many thousands of compute nodes are sending copies of their intermediate compute results out for analysis and algorithmic tuning.","One of the HPC systems at Oak Ridge National Laboratory, Jaguar, has over 26,000 compute nodes with approximately 180,000 compute-cores, and its Lustre Parallel File Systems operate at 240 GB\/s and 44 GB\/s. That meant that each \u201cFile System Client\u201d, like - in , on a small compute node, could \u201cSimultaneously Share\u201d about 9-12 MB\/s on average of the \u201cTotal File System I\/O,\u201d when all of the many thousands of compute nodes were working together in parallel on one large problem. The peak File System I\/O performance for a single specific \u201cFile System Client,\u201d like - in , on a Jaguar compute node was just over 1.25 GB\/s. This means that the peak \u201cFile System Client\u201d I\/O Bandwidth performance available for a single node on Jaguar is 0.5% of the \u201cTotal I\/O\u201d Bandwidth available from the entire File System. This level of I\/O performance works very well for the large scale scientific simulation problems that have been tailored to work on the massively scaled parallel compute systems like \u201cjaguar\u201d with it's thousands of separate compute nodes that typically begin processing jobs with a few small-sized data sets for each compute node. But there are many additional types of compute problems that require the manipulation and processing of very large data sets that must first be loaded into the memory space of one compute system before processing can begin.","Many US Government projects have requirements to rapidly access and process large numbers of files in the 5 GB to 15 GB range. There are many fielded systems are producing data sets at a rate of 1 Tera Byte (TB) or more per hour, and there are several ongoing projects that produce data sets that range from 4 to 6 TB in size. The users of these large, Multi-TB data sets would like to have the ability to rapidly load and process entire multi-TB files in \u201cLarge Shared Memory\u201d compute systems, like  in , and utilize the compute systems 100's or 1000's of compute-cores to reduce the \u201cRaw multi-TB\u201d data sets into useful, user friendly, result sets. The task of crafting algorithms to process these Very Large Multi-TB data sets is much easier to accomplish if the entire data set can fit within the internal memory address space of a large \u201cCommon Global Shared Memory\u201d compute system and be acted on by all of the 100's or 1000's of compute-cores resident with the single large \u201cCommon Global Shared Memory\u201d compute system.","An example of using a \u201cCommon Global Shared Memory\u201d compute system in combination with a typical Parallel File System to process very large data sets can be further examined by using the performance characteristics of Oak Ridge's \u201cJaguar\u201d Supercomputer. Jaguar's \u201cPeak I\/O Performance\u201d for a single compute node was 1.25 GB\/s. Using that I\/O Bandwidth value for the I\/O performance of a single \u201cFile System Client\u201d within a \u201cLarge Shared Memory\u201d compute system, such as  in  or , results in a 15 GB file loading into the \u201cLarge Shared Memory\u201d Compute system in just 12 seconds. An acceptable data I\/O transfer time for many situations. But the time required to load a 6 TB file from a modular storage device or a current implementation of a typical Parallel File System,  and  and  from , into the same large \u201cCommon Global Shared Memory\u201d compute system, - in  of , would be at least 4,800 seconds! Having 100's or 1000's of processing cores waiting over 4,000 seconds for a data set to load into a system like  in  or , would be a terrible waste of an expensive compute resource.","The \u201cPeak I\/O bandwidth\u201d available for a specific application compute client from the total aggregate I\/O provided by the Parallel File Systems' entire collection of \u201cFile System's Data Storage Servers,\u201d like  and  added together in , can be limited by many of the physical and logical elements that interconnect a typical Lustre or GPFS Parallel File System implementation.","A significant implementation element that can limit the \u201cI\/O Bandwidth Performance\u201d for a specific \u201cFile Systems' Client\u201d is the total number, and bandwidth capacity, of the physical or logical I\/O network pathways available to link the specific \u201cApplication Compute Client\u201d with the \u201cData Storage Servers\u201d of the Parallel File System. The designs and implementations of \u201cApplication Compute Clients\u201d will vary across the range of commercially available Parallel File Systems. The number of I\/O network pathways supported by a specific \u201cApplication compute client\u201d implementation, combined with the I\/O bandwidth available for \u201cApplication compute client\u201d use within each supported I\/O network pathway, will significantly affect the total I\/O Bandwidth for the specific \u201cFile System Client.\u201d","The physical linkage between the \u201cApplication compute client\u201d processes - and the collective set of \u201cFile System Data Storage Servers\u201d  and , is depicted in  with the HCA's  supporting the external I\/O connectivity from the - Client within the large \u201cCommon Global Memory Compute System,\u201d , and the Infiniband Switch  that in-turn provides the I\/O network pathway connectivity to the entire collection of \u201cFile System Data Storage Servers\u201d  and  in .","One specific current example of how a Parallel File System's design and implementation of its \u201cApplication compute client\u201d can limit the total amount of File System I\/O Bandwidth available to a specific \u201cApplication compute client\u201d is Lustre's current Client design and implementation. As currently implemented, a Lustre Client can have one physical Infiniband connection. This means that a QDR (Quad Data Rate=4 GB\/s) or DDR (Double Data Rate=2 GB\/s) IB connection between a Lustre File System Client - and HCA  in  over to the  IB switch, would be the only I\/O pathway between the Lustre File System Client and the collection of \u201cFile System Data Storage Servers\u201d (OSS's for Lustre). Current Lustre Client instances utilizing a single QDR IB connection as its I\/O network pathway to the \u201cFile System Data Storage Servers\u201d have been able to achieve sustained I\/O rates of up to 3.2 GB\/s after considerable tuning adjustments were made, and while the Lustre Client was servicing several separate processes within a \u201cLarge Shared Memory\u201d compute system with data blocks from several unique files that were striped across all of the \u201cFile System Data Storage Servers\u201d (OSS servers) within the specific Lustre File System.","A redesign and re-implementation of the Lustre Client software would be required to permit the Lustre \u201cFile System Client\u201d process to utilize two or more physical IB connections as its I\/O network pathway to the \u201cFile System Data Storage Servers.\u201d An implementation that allowed a Lustre File System Client to utilize two QDR IB connections would potentially be able to achieve I\/O bandwidth rates up to a maximum of 6 or 7 GB\/s, but even this level of File System Client I\/O Bandwidth performance is still a very small percentage of the total I\/O Bandwidth available from Parallel File Systems that can achieve 100's of GB\/s of total I\/O Bandwidth. If the File System Client - in  of  was able to operate at 6 GB\/s of I\/O Bandwidth, it would still be utilizing less than \u2153 of the total potential File System I\/O of 20 GB\/s derived from the Parallel File System's \u201cFile System Data Storage Servers\u201d in  and  in .","Existing Parallel File Systems (like Lustre and GPFS) perform well at distributing a small share of the total file system I\/O to the thousands of compute nodes in today's large supercomputing compute clusters. However, for cases where very large data files need to be quickly loaded into a single, multi-processor compute node, the existing Parallel File Systems are not capable of providing more than a very small percentage of the total I\/O Bandwidth to an individual compute client. A more efficient use of the Large Shared Memory Compute System,  in , would be had if there were a way for all of the Parallel File Systems aggregate I\/O to be available for the compute client or clients within the Large Shared Memory Compute system.","Therefore, there is a need for a solution to improve the \u201cPeak I\/O Bandwidth Performance\u201d for a single \u201cFile System Client,\u201d - in , in a large \u201cCommon Global Shared Memory\u201d Compute System,  in .","In accordance with one embodiment of the invention, a system and method for implementing improved compute client data I\/O performance from a Parallel File System comprises a Large Shared Memory Compute System with access to multiple Data Storage Devices, having an allocated sub-set of the internal compute resources assigned to perform all Parallel File System data access processes within each of their own process allocations of shared-memory segments, for data block access and data sharing, and then the internally hosted compute clients, within the Large Shared Memory Compute System, would benefit from the full aggregate data I\/O bandwidth available from the combination of all the Data Storage Devices and Parallel File System processes by being able to utilize Direct Memory Access (DMA) to access all of the shared-memory segments containing the requested data blocks that were placed into the shared memory space of the Large Shared Memory Compute System at the access speed of the bandwidth of the memory system within the Large Shared Memory Compute System. The data movement from storage to client would be accomplished in one step, since the client and the Parallel File Systems processes would be operating within the one shared-memory space. The idea is to completely avoid the complexity and performance impact typically required when switching between data movement between various separate compute nodes, I\/O nodes, File System nodes, and across message passing interfaces over compute cluster interconnects. All of the desired data would move in parallel between data storage and compute memory, or in parallel between compute memory and data storage, without any time-consuming, extraneous stopovers in-route.","Compute clients within the Large Shared Memory Compute System described above would not be limited to a small percentage of the I\/O derived from the aggregate of File System Servers or Data Storage Servers. And data access latency would be improved since the data would only have to be moved from storage directly into the memory space of the Large Shared Memory Compute System where the client would have direct DMA access to it. If a client within the Large Shared Memory Compute System requested access to a specific 20 GB data set, contained within the Parallel File System storage, and each of the ten File System Server processes,  of , hosted within the Large Shared Memory Compute System,  of , were each capable of receiving 2 GB\/sec of data blocks from the interconnected Data Storage Devices, and if each of the processes were to fetch 2 GB of data from the Data Storage Devices and place their 2 GB portions of data into the specifically allocated shared-memory segments, then the requesting client within the Large Shared Memory Compute System would have Direct Memory Access (DMA) to all ten of the 2 GB portions of data blocks, the entire 20 GB data set, after approximately 1 second. A data I\/O bandwidth rate of 20 GB\/sec is a large improvement over the Prior Art implementations that would be limited to transferring data from the externally hosted File Systems Servers via the external interconnect fabric that typically allows for a maximum data transfer of 1-3 GB\/sec. Even if the 3 GB\/sec transfer rate was achievable, the time to transfer a 20 GB file would be over six times slower at 6.6 seconds compared to 1 second. The typical Prior Art Parallel File System configuration would also have the data moving two or three times before reaching the memory space of the compute client. The improved method reduces the data movement to two moves, if a data storage controller is utilized, or down to a single data movement, if the File System Data Servers are performing the data storage controller functions (as in the case of a system utilizing the ZFS file system for data storage management within the data storage devices).","The data I\/O performance of this new configuration, of Bundling all of the parallel File System Data Servers within the same Large Shared Memory compute systems as the compute clients, would be very close to the combined I\/O bandwidth performance of all of the back-end data storage systems or all of the combined I\/O bandwidth performance of the entire set of File Systems Data Servers within the Large Shared Memory Compute System.",{"@attributes":{"id":"p-0040","num":"0000"},"ul":{"@attributes":{"id":"ul0001","list-style":"none"},"li":[{"@attributes":{"id":"ul0001-0001","num":"0039"},"b":"100","figref":"FIG. 1"},{"@attributes":{"id":"ul0001-0002","num":"0040"},"b":["101","1"]},{"@attributes":{"id":"ul0001-0003","num":"0041"},"b":["101","2"]},{"@attributes":{"id":"ul0001-0004","num":"0042"},"b":["102","1"]},{"@attributes":{"id":"ul0001-0005","num":"0043"},"b":["102","2"]},{"@attributes":{"id":"ul0001-0006","num":"0044"},"b":"103"},{"@attributes":{"id":"ul0001-0007","num":"0045"},"b":"104"},{"@attributes":{"id":"ul0001-0008","num":"0046"},"b":"105"},{"@attributes":{"id":"ul0001-0009","num":"0047"},"b":"106"},{"@attributes":{"id":"ul0001-0010","num":"0048"},"b":"107"},{"@attributes":{"id":"ul0001-0011","num":"0049"},"b":"108"},{"@attributes":{"id":"ul0001-0012","num":"0050"},"b":"109"},{"@attributes":{"id":"ul0001-0013","num":"0051"},"b":"110"},{"@attributes":{"id":"ul0001-0014","num":"0052"},"b":"111"},{"@attributes":{"id":"ul0001-0015","num":"0053"},"b":"112"},{"@attributes":{"id":"ul0001-0016","num":"0054"},"b":"113"},{"@attributes":{"id":"ul0001-0017","num":"0055"},"b":"114"},{"@attributes":{"id":"ul0001-0018","num":"0056"},"b":"115"},{"@attributes":{"id":"ul0001-0019","num":"0057"},"b":["200","2"]},{"@attributes":{"id":"ul0001-0020","num":"0058"},"b":"201"},{"@attributes":{"id":"ul0001-0021","num":"0059"},"b":["202","2"]},{"@attributes":{"id":"ul0001-0022","num":"0060"},"b":["300","3"]},{"@attributes":{"id":"ul0001-0023","num":"0061"},"b":"301"},{"@attributes":{"id":"ul0001-0024","num":"0062"},"b":"302"},{"@attributes":{"id":"ul0001-0025","num":"0063"},"b":"303"},{"@attributes":{"id":"ul0001-0026","num":"0064"},"b":"304"},{"@attributes":{"id":"ul0001-0027","num":"0065"},"b":"401"},{"@attributes":{"id":"ul0001-0028","num":"0066"},"b":["402","401","403"]},{"@attributes":{"id":"ul0001-0029","num":"0067"},"b":"403"},{"@attributes":{"id":"ul0001-0030","num":"0068"},"b":["501","401","501"]}]}},"A system and method for increasing the application compute client data I\/O bandwidth performance from data file systems or data object storage systems can be achieved by hosting\/bundling all of the data file system storage servers or data object storage system servers for the entire parallel file system, in the same common global shared memory compute system as the application compute clients.","An embodiment of this novel re-arrangement of the existing elements of a typical Parallel File System is depicted in . Item  in  shows the \u201cBundled File System Data Storage Servers\u201d within the large \u201cCommon global Shared Memory\u201d compute system . This new method changes the pathway linking the \u201cFile System Client\u201d and the \u201cFile System Data Storage Servers\u201d from being an external I\/O network pathway like  in , to a Direct Memory Access (DMA) pathway depicted as  within  from . The memory bandwidth of the large \u201cCommon Global Shared Memory\u201d compute system now serves as the pathway between the \u201cFile System Client\u201d -, and the entire collection of \u201cFile System Data Storage Servers\u201d  that generate all of the aggregate I\/O bandwidth for the entire Parallel File System. The \u201cApplication Compute Client\u201d shown as - in  can potentially access the full 20 GB\/s of \u201cParallel File System\u201d I\/O Bandwidth generated by the Hosted Bundle of \u201cFile System Data Storage Servers\u201d  via a Direct Memory Access (DMA)  within the memory subsystem  that is implemented within the large \u201cCommon Global Shared Memory\u201d compute system  shown in .","The memory I\/O bandwidth performance of the memory Subsystem  within the large \u201cCommon Global Shared Memory\u201d compute system  now becomes the largest limiting factor in determining the Peak I\/O bandwidth available for the \u201cFile System Client\u201d - operating within the large \u201cCommon Global Shared Memory\u201d compute system .","A nice feature of this novel arrangement of \u201cParallel File System\u201d elements, is that external \u201cFile System Clients,\u201d like - in , can still access the entire \u201cParallel File System\u201d via the same methods and processes that were used before the \u201cFile System Data Storage Servers\u201d  were Bundled into . The Hosted\/Bundled \u201cFile System Data Storage Servers\u201d  are still connected to IB switch  via , and they are still connected to the Storage Subsystems via . The I\/O Bandwidth available for external \u201cFile System Clients\u201d like - remains the same, as it was when the \u201cFile System Data Storage Servers\u201d were in their original separate server instances like  and  in .","The efficient hosting the Parallel File Systems' full complement of File System Data Storage Servers in the same \u201cCommon Global Shared Memory\u201d compute system as the specific File System Client process would require the careful integration of the following major sub elements of the large \u201cCommon Global Shared-Memory\u201d compute system:\n\n","This method would allow a specific application compute client to access the \u201cEntire Aggregate Bandwidth\u201d provided by all of the File System Data Storage Servers processes within the Parallel File System up to the bandwidth limit of the \u201cCommon Global Shared-Memory\u201d compute system. File System Clients external to the \u201cHosted File System Servers\u201d that are residing within the \u201cCommon Global Shared-Memory\u201d compute system would still be subject to the same \u201cChannel I\/O link\u201d limiting factors as the Prior Art implementations, but would still have shared access to the parallel file System or object storage system. Processes running within the \u201cCommon Global Shared Memory\u201d compute system would be serviced by the Internal File System I\/O Clients via Common Global shared memory access and DMA procedures and would have full I\/O access up to the limit of bandwidth of the Common Global Shared Memory compute system.","Having very fast I\/O bandwidth for application client's within a large common global shared memory machine will be very useful for a variety of applications. In the earlier case, where a 6 Tera Byte data file needed to be transferred from a modular storage device into an analysis environment, the previously accepted performance would effect the transfer at 1.25 GB\/sec for a total of 4,800 seconds. With all of the data storage servers working in parallel,  in , within the same common global shared memory compute environment, the transfer to compute client - in  could approach 20 GB\/sec and would be complete in approximately 300 seconds! In order to achieve those transfer rates, the data sets need to be presented in formats compatible with the parallel file systems being used in the common global shared memory compute system. One embodiment of such a configuration is depicted in .","The data storage devices are configured in modular transportable assemblies per items  and  in , that can be quickly connected or dis-connected via the  and  modular cable assemblies. When a modularized data storage device is being prepped for transport, an orderly shutdown sequence is initiated. Upon completion of the shut down sequence, the specifics about the particular file system or data object system will have been written into a designated startup\/shutdown region of the modularized data storage device. Upon re-coupling the transported modularized data storage device, , to an alternate \u201cCommon Global Shared Memory\u201d compute system, the configuration of the data file system or data object storage systems information will be detected from the designated startup\/shutdown region of storage. At that time, the alternate system will re-allocate compute elements within the Common Global Shared Memory compute environment to be used as an additional set of File System Data Storage Servers for the data files system or data object storage system. When the file system or data object storage system of the modularized data storage device, , is up and running, a high-speed transfer of the data resident within the modularized data module into the Common Global Shared Memory can occur. In the embodiment depicted in , the contents of the  data module could be transferred to the  data module at an aggregate rate approaching 20 GB\/sec. A much better transfer profile than if a typical transfer was utilized using standard configurations.","Another beneficial advantage of hosting\/bundling all of the data storage servers within the same \u201cCommon Global Shared Memory,\u201d is that the data elements themselves are available to the application compute clients with reduced latency. In a typical Parallel Data Object file system, like Lustre or IBM's GPFS file system, the data elements are \u201cphysically\/logically\u201d moved to an application compute client in the following sequence:\n\n","In this example, data element \u201cAlpha\u201d was moved three times before getting to the application compute element. In the proposed situation, where all of the file system data servers or object storage servers are all part of the same \u201cCommon Global Shared Memory,\u201d the data movement sequence would be as follows:\n\n","In the case where the data storage subsystem is a JBOD collection of disks or Solid State Disk drives, without a storage controller, the number of data moves would be reduced down to one! Data element Alpha would be moved from a disk or SSD, and placed directly into the memory address space allocated by the file system storage server, where it would be available for use by an application compute client via direct memory access.","The system and method of Hosting\/Bundling all of the files system data servers in the same common global shared memory compute system provides for increased aggregate I\/O bandwidth performance with the added benefit of reducing the latency times for accessing specific data sets. The prior art systems and methods utilized various schemes for parallel computing and parallel I\/O, and all made reference to various connectivity configurations, but none of them included the incorporation of all file system or data storage system computation within the same system common global shared memory as the application compute client.","A single file system or data object storage application compute client process in a large compute system (hundreds to thousands of compute cores), or a small compute system (a few dozen compute cores) is typically limited to sharing only a small portion of the total aggregate data I\/O bandwidth available in today's modern data file system's or data object storage's systems total I\/O bandwidth. Many of today's very high bandwidth data file systems or object storage systems derive their total \u201cfile system\u201d I\/O bandwidth by aggregating data I\/O access from multiple, physical file system data storage servers or data object storage servers that are themselves orchestrating and conveying data I\/O access from their physically or logically attached \u201cback-end\u201d data storage devices and systems. The percentage share of total file system or data object storage I\/O bandwidth available for a single file system client is dropping as current and planned implementations of modern data file systems and data object storage systems grow in total aggregate I\/O bandwidth from 200-300 Giga Bytes\/sec to well over 1 Tera Bytes\/sec. The I\/O bandwidth available for a specific file system application compute client from the entire aggregate set of file system's data storage servers or data object storage servers are limited by many factors. A primary factor for limiting file system or data object storage client I\/O is the number of \u201cclient I\/O network pathways\u201d that are supported by the specific file system's design, and the bandwidth capacity the of the I\/O network pathways that are linking the multiple file system data storage servers to the specific file system client process.","Performance for typical file system or data storage systems client I\/O will range from approx. 1.5 GB\/sec to 3.0 GB\/sec. Even 3.0 GB\/sec is not a very large share of client I\/O bandwidth performance when compared to aggregate file system I\/O bandwidth figures that are now exceeding 1,000 GB\/sec. This I\/O client bandwidth limitation can be alleviated by physically or logically hosting the entire set of file system data storage servers or data object storage servers within the same common global shared memory, parallel compute system, as the specific application compute client process.","The efficient hosting of the data file systems' storage servers or data object storage servers full complement of file system data storage servers in the same \u201cCommon Global Shared Memory\u201d compute system as the specific application compute client process would require the careful integration of the following major sub elements of the \u201cCommon Global Shared Memory\u201d compute system:\n\n","This system and method would allow a specific application compute client to access the \u201cEntire Aggregate Bandwidth\u201d provided by all of the File System Data Storage Servers processes up to the bandwidth limit of the \u201ccommon Global Shared Memory\u201d compute system. File System Clients external to the \u201cHosted File System Servers Shared-Memory compute system\u201d would still be subject to the same \u201cChannel I\/O link\u201d limiting factors, but would still have shared access to the hosted File Systems. Processes running within the \u201cCommon Global Shared Memory compute system would be serviced by internal File System Clients via shared-memory access and would have full I\/O access up to the limit of bandwidth of the Common Global Shared Memory compute system with the additional benefit of minimizing data access latency due to the reduced number of data movements.","The operation of the data file systems' or data object storage systems' \u201cHosted\/Bundled\u201d data storage servers, is the same as if they were implemented in physically separate servers. Lustre OSS servers still stripe data blocks across the storage subsystems and storage devices under the direction of the file system application compute clients. The data I\/O performance and operation of the specific application compute client file system I\/O client or data object storage client, operating within the \u201cCommon Global Shared Memory\u201d compute system is increased since it is no longer limited by an Infiniband or fibrechannel connection. It has access to the entire I\/O Bandwidth of the entire parallel file system aggregated from all of the OSS servers, or NSD servers in the case of IBM's GPFS."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"DRAWINGS","p":[{"@attributes":{"id":"p-0035","num":"0034"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0036","num":"0035"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0037","num":"0036"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0038","num":"0037"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0039","num":"0038"},"figref":"FIG. 5"}]},"DETDESC":[{},{}]}
