---
title: Data mover permitting data transfer without transferring data between application and operating system
abstract: In a computer system with a disk array that has physical storage devices arranged as logical storage units and is capable of carrying out hardware storage operations on a per logical storage unit basis, data movement operations can be carried out on a per-file basis. A data mover software component for use in a computer or storage system enables cloning and initialization of data to provide high data throughput without moving the data between the kernel and application levels.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09454368&OS=09454368&RS=09454368
owner: VMware, Inc.
number: 09454368
owner_city: Palo Alto
owner_country: US
publication_date: 20090121
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"p":["This application is related to U.S. patent application Ser. No. 12\/129,323, entitled \u201cOFFLOADING STORAGE OPERATIONS TO STORAGE HARDWARE,\u201d filed on May 29, 2008, U.S. patent application Ser. No. 12\/129,376, entitled \u201cOFFLOADING STORAGE OPERATIONS TO STORAGE HARDWARE USING A THIRD PARTY SERVER,\u201d filed on May 29, 2008, and U.S. patent application Ser. No. 12\/129,409, entitled \u201cOFFLOADING STORAGE OPERATIONS TO STORAGE HARDWARE USING A SWITCH,\u201d all three of these applications are being incorporated herein by reference.","Copying, moving, and initializing large quantities of data, e.g., 10 or more gigabytes, stored on enterprise storage systems is a common operation. These operations tend to require a long time to complete and impose significant overhead on the enterprise storage systems configured to support databases, email servers, and backup processes. The overhead involved in data copying includes multiple context switches, double copying of data between the kernel and application program, cache pollution, and scheduling of synchronous operations. Consequently, performing the data transfer for a large copy, move, or initialization operation prevents system resources from being used by other more critical tasks. This may limit the performance and scalability of the enterprise storage systems.","Enterprise storage systems employ disk arrays that are physically independent enclosures containing a disk array controller, a disk cache and multiple physical disk drives. The disk array controller manages the physical disk drives and exposes them to connected computer systems as logical data storage units, each identified by a logical unit number (LUN), and enable storage operations such as cloning, snapshotting, mirroring and replication to be carried out on the data storage units using storage hardware.","Computer systems that employ disk arrays are typically configured with a file system that executes a logical volume manager. The logical volume manager is a software or firmware component that organizes a plurality of data storage units into a logical volume. The logical volume is available in the form of a logical device with a contiguous address space on which individual files of a file system are laid out. Logical volume manager and the organization of filed on this logical volume is controlled by the file system. As a result, disk arrays do not know how individual files are laid out on the data storage units. Therefore, a disk array cannot invoke its hardware to carry out storage operations such as cloning, snapshotting, mirroring and replication on a per-file basis.","One possible solution for carrying out storage operations in a disk array on a per-file basis is to add storage metadata in data structures managed by the disk array. Disk arrays, however, are provided by a number of different vendors and storage metadata varies by vendor. This solution is not attractive because the file system would then need to be customized for each different vendor. For this reason, copying (cloning), moving and initialization of files have been typically carried out using software techniques through traditional standard file system calls.","One or more embodiments of the invention provide a data mover implemented in software for efficient cloning and initialization of data for use in a computer or storage system. The data mover can be used for zeroing file blocks and cloning file blocks with reduced host computer, memory, and input\/output overhead compared with conventional software techniques.","One embodiment of the invention provides a method of carrying out data movement operations in a computer system including a host computer connected to a storage system having storage devices. The method includes receiving a data movement instruction, from an application program executing on the host computer, specifying a source data location in the storage devices and a destination data location in the storage devices and transferring source data from the source data location to the destination data location at an operating system level of the host computer without transferring the source data between the application program and the operating system level.","Another embodiment of the invention provides a computer system having a host computer, and a storage system, connected to the host computer, having storage devices that are presented to the host computer as one or more logical storage units. The host computer is configured to execute an application program and an operating system including a data mover. The application program issues a data movement instruction specifying a source data location in the storage devices and a destination data location in the storage devices and the data mover transfers source data from the source data location to the destination data location within the operating system without transferring the source data between the application program and the operating system.","A computer readable storage medium, according to an embodiment of the invention, has stored therein a data movement instruction to be executed in a storage processor of a storage system that has storage devices that are represented as logical storage units. The data movement instruction comprises a source data location in the storage devices and a destination data location in the storage devices, wherein execution of the data movement instruction transfers source data from the source data location to the destination data location at an operating system level of a host computer without transferring the source data between an application program that issues the data movement instruction and the operating system level.",{"@attributes":{"id":"p-0027","num":"0026"},"figref":["FIG. 1A","FIG. 1A","FIG. 1A"],"b":["208","180","208","180","208","180","208","190","188","189","183","208","185","106","197","106","180","208","190","197"]},"In some embodiments, switch  is omitted; storage system  resides in a separate data center from third party server ; and third party server  communicates with storage system  via out of band path  and NIC  (network interface card) installed in storage system . In accordance with one or more further embodiments, additional switches  and storage systems  may be included in a system with one or more storage systems  residing in different data centers.",{"@attributes":{"id":"p-0029","num":"0028"},"figref":["FIG. 1B","FIG. 1B"],"b":["100","102","100","104","100","106","106","106","100","101","100","106"]},"In storage system , storage system manager , which represents one or more programmed storage processors, serves as a communication agent (to the outside world) for storage system , and implements a virtualization of physical, typically disk drive-based storage units, referred to in  as spindles -, that reside in storage system . Spindles -are collectively referred to herein as spindles . From a logical perspective, each of these spindles can be thought of as a sequential array of fixed sized extents . Storage system manager  abstracts away complexities of targeting read and write operations to addresses of the actual spindles and extents of the disk drives by exposing to computer system  an ability to view the aggregate physical storage space provided by the disk drives as a contiguous logical storage space that may be divided into a set of virtual SCSI devices known as LUNs (Logical Units) -. The virtualization of spindles -into such a contiguous logical storage space of LUNs -can provide a more efficient utilization of the aggregate physical storage space that is represented by an address space of a logical volume. Storage system manager  exposes to computer system  an ability to transmit data transfer and control operations to storage system  at a LUN \u201cblock\u201d level, where a block is a particular contiguous region in a particular LUN. For example, a LUN block may be represented as <LUN ID, offset, length> and computer system  may transmit to storage system  a read or write operation for block <LUN ID, offset, length> in the form of a SCSI operation. The LUN identifier (LUN ID) is a unique, SCSI protocol compliant, identifier value that is retrievable in response to a standard SCSI Inquiry command.","Storage system manager  maintains metadata  that includes a mapping (hereinafter, also referred to as an extent-mapping) for each of LUNs -to an ordered list of extents, wherein each such extent can be identified as a spindle-extent pair <spindle #, extent #> and may therefore be located in any of the various spindles -. As such, whenever storage system manager  receives a LUN block operation from computer system , it is able to utilize the extent-map of the LUN to resolve the block into an appropriate list of extents located in various spindles -upon which the operation is performed. Those with ordinary skill in the art will recognize that, while specific storage system manager implementation details and terminology may differ as between different storage device manufacturers, the desired consistent result is that the externally visible LUNs implement the expected semantics (in this example, SCSI semantics) needed to respond to and complete initiated transactions.","When storage system  is a NAS device, storage system manager  exposes to computer system  an ability to transmit data transfer and control operations to storage system  at the file level. In contrast with SAN storage, LUNs -are managed within the NAS device. Storage system manager  manipulates files (performs I\/O for files using block addresses, change file length and attributes, and the like) stored on the NAS device using file handles. When storage system manager  receives a file operation from computer system , it finds the location of the files being operated on within spindles -using the filehandle specified by the file operation and performs the operation.","Returning to computer system , operating system  is installed on top of hardware platform  and it supports execution of applications . Examples of operating system  may be Microsoft Windows\u00ae, Linux\u00ae, Netware-based operating systems or any other operating system known to those with ordinary skill in the art. Users may interact with computer system  through a user interface  such as a graphical user interface or a command based shell, while executing applications  may access computing resources of computer system  that are managed by operating system kernel  through kernel application programming interface (API) . Kernel  provides process, memory and device management to enable various executing applications  to share limited resources of computer system . For example, file system calls initiated by applications  through kernel API  are routed to file system . File system , in turn, converts the file system operations to volume block operations, and provides the volume block operations to logical volume manager . File system , in general, manages creation, use, and deletion of files stored on storage system  through the LUN abstraction discussed previously. Logical volume manager  translates the volume block operations for execution by storage system , and issues raw SCSI operations (or operations from any other appropriate hardware connection interface standard protocol known to those with ordinary skill in the art, including IDE, ATA, SAS and SATA) to device access layer  based on the LUN block operations.","A data mover  software component is configured to perform data movement operations without moving data between operating system kernel  and applications , as described in conjunction with . Applications  may set or change data movement policies that are used by data mover , initiate data movement operations (clone, initialization, and the like) using ioctl commands, and query data movement results synchronously or asynchronously. Data mover  manager performs buffer allocation, pipelining, and transient error recovery on behalf of applications .","Device access layer  discovers storage system , and applies command queuing and scheduling policies to the raw SCSI operations. Device driver  understands the input\/output interface of HBAs  interfacing with storage system , and sends the raw SCSI operations from device access layer  to HBAs  to be forwarded to storage system . As previously discussed, storage system manager  of storage system  receives the raw SCSI operations (i.e., LUN block level operations), and resolves them into the appropriate extents within the spindles of the disk array that are operated upon.","Instances arise during the operation of computer system  where files on file system  cannot ultimately be stored in contiguous blocks of LUNs -presented to computer system  by storage system . While there may be enough blocks of free storage space in the aggregate among various LUNs -to store such files, such blocks are neither large enough nor contiguous and may be dispersed across different LUNs. In such instances, files may need to be segmented into multiple component parts at the file system level, LUN level, and the spindle-extent level (as further detailed in ), such that the file components are stored across different blocks of different LUNs. Due to this segmentation, operations on such files, such as read and write operations, also need to be broken up into separate block level LUN operations (i.e., raw LUN block level SCSI operations) when transmitted to storage system  thereby increasing the resources used by computer system  to communicate with storage system  (e.g., CPU cycles, DMA buffers, SCSI commands in the HBA queue, etc.).","One example of an environment that deals with significantly large files or collections of files where the foregoing segmentation may occur is server virtualization. As further discussed below, virtualization systems expose the concept of a \u201cvirtual disk\u201d which is implemented as a collection of files stored on a file system.  is a functional block diagram of a virtualized computer system with a connected storage system, in which one or more embodiments of the invention may be practiced. Similar to computer system  of , computer system  may be constructed on a conventional, typically server-class, hardware platform . As shown in , computer system  includes HBAs  and NIC  that enable computer system  to connect to storage system . As further shown in , virtual machine (VMKernel) operating system  is installed on top of hardware platform  and it supports virtual machine execution space  within which multiple virtual machines (VMs) -may be concurrently instantiated and executed. Each such virtual machine -implements a virtual hardware (HW) platform  that supports the installation of a guest operating system  which is capable of executing applications . Similar to operating system  of , examples of a guest operating system  may be Microsoft Windows\u00ae, Linux\u00ae, Netware-based operating systems or any other operating system known to those with ordinary skill in the art. In each instance, guest operating system  includes a native file system layer (not shown), for example, either an NTFS or an ext3 type file system layer. These file system layers interface with virtual hardware platforms  to access, from the perspective of guest operating systems , a data storage HBA, which in reality, is virtual HBA  implemented by virtual hardware platform  that provides the appearance of disk storage support (in reality, virtual disks or virtual disks -) to enable execution of guest operating system  transparent to the virtualization of the system hardware. Virtual disks -may appear to support, from the perspective of guest operating system , the SCSI standard for connecting to the virtual machine or any other appropriate hardware connection interface standard known to those with ordinary skill in the art, including IDE, ATA, SAS and SATA.","Although, from the perspective of guest operating systems , file system calls initiated by such guest operating systems  to implement file system-related data transfer and control operations appear to be routed to virtual disks -for final execution, in reality, such calls are processed and passed through virtual HBA  to adjunct virtual machine monitor (VMM) layers -that implement the virtual system support needed to coordinate operation with virtual machine kernel . In particular, host bus emulator  functionally enables the data transfer and control operations to be correctly handled by virtual machine kernel  which ultimately passes such operations through its various layers to true HBAs  or NIC  that connect to storage system . Assuming a SCSI supported virtual device implementation (although those with ordinary skill in the art will recognize the option of using other hardware interface standards), SCSI virtualization layer  of virtual machine kernel  receives a data transfer and control operation (in the form of SCSI commands) from VMM layers -, and converts them into file system operations that are understood by virtual machine file system (VMFS) . SCSI virtualization layer  then issues these file system operations to VMFS . VMFS, in turn, converts the file system operations to volume block operations, and provides the volume block operations to logical volume manager . Logical volume manager (LVM)  is typically implemented as an intermediate layer between the driver and conventional operating system file system layers, and supports volume oriented virtualization and management of the LUNs accessible through HBAs  and NIC . As previously described, multiple LUNs, such as LUNs -can be gathered and managed together as a volume under the control of logical volume manager  for presentation to and use by VMFS  as an integral LUN.","VMFS , in general, manages creation, use, and deletion of files stored on storage system  through the LUN abstraction discussed previously. Clustered file systems, such as VMFS , are described in patent application Ser. No. 10\/773,613 that is titled, \u201cMULTIPLE CONCURRENT ACCESS TO A FILE SYSTEM\u201d filed Feb. 4, 2004. Logical volume manager  issues raw SCSI operations to device access layer  based on the LUN block operations. A data mover  software component performs the operations previously described in conjunction with data mover . Device access layer  discovers storage system , and applies command queuing and scheduling policies to the raw SCSI operations. Device driver  understands the input\/output interface of HBAs  and NIC  interfacing with storage system , and sends the raw SCSI operations from device access layer  to HBAs  or NIC  to be forwarded to storage system . As previously discussed, storage system manager  of storage system  receives the raw SCSI operations (i.e., LUN block level operations) and resolves them into the appropriate extents within the spindles of the disk array that are operated upon.",{"@attributes":{"id":"p-0040","num":"0039"},"figref":["FIG. 3","FIG. 2","FIG. 3"],"b":["222","230"],"sub":"A "},"The virtual disk is allocated by VMFS  as a series of segments -in logical address space, VMFS volume , that is managed by VMFS . Each segment -is a contiguous region in VMFS volume , where VMFS  has been constructed by an administrator of the system by allocating a set of LUNs -available from storage system's  set of LUNs -. As previously discussed in the context of , each contiguous region of a file segment that is also contiguous on one of the allocated LUNs, is considered a LUN \u201cblock\u201d  that can be represented as <LUN ID, offset, length>. As shown in , different LUN blocks  corresponding to a portion of a file segment may be of different lengths depending on how big the file segment is and what part of that file segment actually corresponds to a contiguous region of an allocated LUN. Therefore, a file may have one or more segments, and a segment may be composed of one or more blocks from one or more LUNs. In the illustrated example, file segment has 2 LUN blocks, file segment has 3 LUN blocks, file segment has 4 LUN blocks, and file segment has 1 LUN block. As shown in , file segments in VMFS volume  are converted into LUN blocks by lines connecting file segments  to LUN blocks  in LUNs  where LUNs  represent the LUN address space. When storage system  is a NAS device, the file segments are managed within the NAS device.","By resolving all file segments -making up virtual disk into an ordered list of their corresponding LUN blocks (in the case of , for a total of 10 blocks), VMFS  creates a \u201cblocklist\u201d (e.g., a list of <LUN ID, offset, length>) which is representative of virtual disk in LUN block form. As previously discussed in the context of , storage system  can utilize the extent maps for LUNs -to resolve each of the LUN blocks in the blacklist into its corresponding list of <spindle #, extent #> pairs (spindle-extent pairs) within spindles -. As shown in , LUN blocks  are converted into spindle-extent pairs by lines connecting LUN blocks  within LUNs  to extents within spindles . Extents  within spindle are explicitly labeled in . Extents within other spindles  are not labeled in . Those with ordinary skill in the art will recognize that, although  has been discussed in the context of a virtualized system in which a virtual disk is allocated into file segments, non-virtualized systems similar to that of  may also have files stored in its file system that exhibit similar types of segmentation into LUN blocks.","As previously discussed, storage devices such as storage system  typically expose LUN block level operations to computer systems communicating with it. For example, a standard raw SCSI read or write operation requires a LUN identifier, logical block address, and transfer length (i.e., similar to the <LUN ID, offset, length> encoding described herein). As such, in order to perform operations on files such as virtual disk that are managed at VMFS  file system level, standard raw SCSI operations need to be separately applied to each of the 10 blocks in virtual disk's blocklist. Each I\/O communication (e.g., transmission of a raw SCSI operation) by computer system  with storage system  can take up significant computing resources such as CPU cycles, DMA buffers, and SCSI commands in an HBA queue.","By exposing LUN blocklist level primitives to the set of operations available to computer systems communicating with storage system , disk array vendors provide computer systems an ability to offload resource intensive communication with a disk array into the disk array itself. The disk array can then leverage any proprietary hardware optimizations that may be available internally thereto. In one embodiment, such blocklist level primitives may be embedded in a command descriptor block (CDB) in a pre-existing standard command of the communication interface protocol between the computer system and disk array or, alternatively, may be added as an additional command to the set of standard commands. For example, for SCSI supported interactions between a computer system and a disk array, certain blocklist level primitives may be embedded into the CDB of SCSI's pre-existing WRITE BUFFER command, while other blocklist level primitives may require the addition of a new SCSI level command (e.g., with its own CDB) to augment SCSI's current commands. The following discussion presents three possible blocklist level primitives supported by storage system  (i.e., \u201czero\u201d for zeroing out files, \u201cclone\u201d for cloning files and \u201cdelete\u201d for deleting files). Data mover  and  may be used to improve the data throughput of the zero and clone primitives or to perform copy (clone) and initialization (zero) operations initiated by applications . These three blocklist level primitives are in the general form: operator (sourceblocklist, destinationblocklist, context identifier), and may be utilized to offload atomic components of larger composite virtual machine operations to the disk array. However, those with ordinary skill in the art will appreciate that other additional and alternative blocklist level primitives may be supported by the disk array without departing from the spirit and scope of the claimed invention.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 4A","FIG. 4A","FIG. 1B","FIG. 2"],"b":["121","114","121","114","233","208","121","405","114","405","410","405","114","116","121","405","114"]},"Operating system kernel  includes file system , move interface , buffer cache , and data mover . Move interface  interfaces between file system  and data mover  when a move call is received from application . Buffer cache  caches data that is read by operating system kernel . When a conventional move operation is performed, without the use of data mover , buffer cache  stores source data that is read from a source file and transferred from buffer cache  to application . Buffer cache  also stores destination data that is received from application  to be written to a destination file. For a single conventional copy operation, the same data is transferred between application  and buffer cache  twice. The first transfer moves the source data from buffer cache  to application  and the second transfer moves the destination data (copy of the transferred source data) from application  to buffer cache . Additionally, the source data is copied within memory allocated to application  to produce the destination data, using more memory and memory bandwidth.","System performance is reduced since the data transfers within the memory allocated to application  and between buffer cache  and application  are performed using non-pipelined synchronous system calls. Therefore, operating system kernel  and application  are occupied performing the data transfers instead of other functions. Since the data transfers are typically moving large quantities of data, e.g., 10 or more gigabytes, operating system kernel  performs context switches to perform other operations and respond to requests from other applications. The context switches consume additional processing cycles and contribute to cache pollution of the CPU's memory management unit (MMU), resulting in further performance degradation of the system.","In contrast, when data mover  is used to transfer the data, the source and destination data is retained within operating system kernel  and the costly transfers between application  and buffer cache  are not needed. Using data mover  to copy the data from a source to a destination frees operating system kernel  to perform other operations and increases the available bandwidth between application  and operating system kernel . As shown in , a data mover buffer  stores a copy of the source data. Note that a copy of the source data is not transferred between application  and buffer cache , in contrast with the previously described conventional move operation.","Data mover  includes a data mover frontend  and a data mover backend . Data mover frontend  is configured to receive the move or init command, divide source data into task fragments, create data movement contexts (e.g., threads, bottom halves or processes\u2014these are things known to a person with ordinary skill in the art of operating systems), maintain the data movement state, implement data movement directives, and return the move status to application . When application  uses a synchronous protocol, application  waits for the move operation to complete. When application  uses an asynchronous protocol, data mover frontend  notifies application  following completion of the move operation. Data mover backend  is configured to allocate and deallocate data mover buffers, such as data mover buffers , , report error messages to data mover frontend , and request tasks from data mover frontend .",{"@attributes":{"id":"p-0050","num":"0049"},"figref":"FIG. 4B","b":["121","233","405","405","1","1","2","2","1","1","2","2"]},"At step , the file system within operating system kernel  or VMKernel  receives a move data request from application . In a particular embodiment that implements virtualization, VMFS  in VMKernel  may receive a request to zero out a file such as virtual disk (e.g., to preserve VM isolation). The file system resolves the file into its component file segments at step , where <fileid, offset, length>+ in step  represents a list of <fileid, offset, length> file segments. Fileid is a unique identifier that distinguishes segments associated with one file from segments associated with another file. At step , VMKernel  or operating system kernel  resolves the file segments into logical extents. At step , VMKernel  or operating system kernel  sanitizes the arguments provided with the move command by walking through the multiple extents. At step , VMKernel  or operating system kernel  resolves each of the logical extents into a corresponding list of LUN blocks <LUN ID, offset, length>+. At step , VMKernel  or operating system kernel  consolidates these lists of LUN blocks into a sourceblocklist, the ordered list LUN blocks representing the relevant file. A sourceblocklist or destination block list may include attributes such as a flag indicating that the block is \u201czeroed\u201d meaning that when the data is read, it should be read as zeros. The flag is an optimization that allows for the data to be cleared without actually writing the data.","At step , data mover  or  determines the number of worker threads needed to perform the move operation. The worker threads will execute in parallel to perform the move operation. At step , data mover  or  determines the transfer size of a task and, as needed, divides the request into multiple tasks. The move data is divided into multiple tasks that are no larger than the transfer size, as described in detail in conjunction with . At step , data mover  or  distributes the tasks to the worker threads, as described in detail in conjunction with , to complete execution of the move command.","At step , data mover  or  reports the move status to application . In synchronous mode application  waits for the move to complete. When application  operates in an asynchronous mode, data mover  or  notifies application  that the move has been initiated. Application  then periodically polls data mover  or  to determine whether or not the move status indicates that the move is complete. Other information provided to application  may include reported errors, the quantity (bytes) of data read, and the quantity (bytes) of data written. Application  may use that information to measure the transfer efficiency of storage system  and determine the quantity of data to move using move commands. When application  enables optimizations, described in conjunction with , less data may be read and\/or written than is specified by the move command. In those cases, the transfers are more efficient and the quantity of data specified by the move commands may be increased by application  to reduce the frequency of issuing the move commands. Lowering the frequency of the commands may further improve data movement performance since more data can be moved between context switches. Additionally, the task size may be dynamically increased or decreased by data mover  based on the amount of time a worker thread uses to process each task to balance the workload and improve the transfer efficiency.",{"@attributes":{"id":"p-0054","num":"0053"},"figref":["FIG. 4C","FIG. 4B"],"b":["414","421","420","121"]},"At step  data mover frontend  determines if the length equals zero, and, if so, then all of the tasks have been generated for the move command and data mover  proceeds to step . Otherwise, at step  data mover frontend  determines the longest contiguous extent for srcOffset and dstOffset. The first time that steps  and  are completed, the srcOffset, dstOffset, and length values match the values specified for the move command. As each task is generated, the srcOffset, dstOffset, and length values are updated.","At step  data mover frontend  determines the task size as the minimum of the length, length of the source extent, and length of the destination extent. At step  data mover frontend  outputs a task by writing the task to the output array. At step  data mover frontend  updates the length, srcOffset, and dstOffset by the task size that was determined at step  and returns to step . The task size is subtracted from length and srcOffset and dstOffset are updated to indicate the next location to read from or write to.",{"@attributes":{"id":"p-0057","num":"0056"},"figref":["FIG. 4D","FIG. 4B","FIG. 4D"],"b":["416","416","416","416","416"]},"At step , a worker thread of data mover backend  requests a task from data mover frontend . At step , the worker thread enters a READ processing state. At step , the worker thread reads the source data and stores the source data in a data mover buffer, e.g., data mover buffer  or . At step , when the read is complete, the worker threads performs a registered callback operation to notify data mover backend  that the read operation was performed. At step , data mover backend  determines whether or not the read operation was successful.","If the read operation was successful, then the worker thread proceeds to step B. If the read operation was not successful, then at step , data mover backend  determines if the worker thread should retry the read operation. Data mover backend  makes such a determination using the movement directives provided to data mover . The movement directives may specify a maximum retry count and\/or a retry frequency as a parameter to provide a particular quality of service (QOS). If data mover backend  determines that the read operation should be retried, then the worker thread returns to step . Otherwise, data mover backend  proceeds to step  to determine whether or not another task should be processed.","At step , the worker thread enters a WRITE processing state. At step , the worker thread reads the source data from the data mover buffer and stores the source data in another data mover buffer for output to the destination storage device specified by the move command, e.g., data mover buffer  or . At step , when the write is complete, i.e., the data mover buffer has been transferred to the destination storage device, the worker thread performs a register callback operation to notify data mover backend  that the write operation was performed. At step , data mover backend  determines whether or not the write operation was successful.","If the write operation was successful, then the worker thread proceeds to step . If the write operation was not successful, then at step , data mover backend  determines if the worker thread should retry the write operation. Data mover backend  makes such a determination using the movement directives provided to data mover . The movement directives may specify a maximum retry count and\/or a retry frequency to provide a particular quality of service (QOS). If data mover backend  determines that the write operation should be retried, then the worker thread returns to step . Otherwise, data mover backend  proceeds to step  to determine whether or not another task should be processed.",{"@attributes":{"id":"p-0062","num":"0061"},"figref":["FIG. 4E","FIG. 4D"],"b":["416","444","425","425","456"]},"If, at step  data mover backend  determines that the LUN ID for the sourceblocklist is not null, then in step  data mover backend  determines if the zero attribute flag for the sourceblocklist is set, indicating that the data for the sourceblocklist should be read as zeros. When the zero attribute flag is set, data mover backend  proceeds to step  and sets the write destination zeros flag. Otherwise, the worker thread completes steps  and . At step , the worker thread determines if the source data is all zeros, and, if so, then at step  the worker thread sets the write destination zeros flag. Otherwise, the worker thread proceeds to step  to complete the write operation.","When a conventional copy operation is used to move data, the data is not necessarily examined to determine whether or not the data is all zeros. By examining the data as it is read, data mover  may optimize the transfer of data by reducing the amount of data that is read and written, and improve the data transfer throughput. Each optimization may be separately enabled using the movement directives.",{"@attributes":{"id":"p-0065","num":"0064"},"figref":["FIG. 4F","FIG. 4D"],"b":["416","464","425","466","121","118","468","425","470","487","489","490","492","466","489"]},"Data mover  or  avoids unnecessary transfers of data between operating system kernel  or VMKernel , respectively, by transfering data within operating system kernel  or VMKernel . Data mover  or  is able to throttle the available storage bandwidth by controlling the SCSI request queues and performing optimizations to reduce unecessary copies. The number of context switches and cache pollution may be reduced with the improved transfer efficiency. The data mover  or  can be used for zeroing file blocks and cloning file blocks with reduced host computer, memory, and input\/output overhead compared with conventional software techniques, such as application-based data movement that transfers the data through memory allocated to the application program.",{"@attributes":{"id":"p-0067","num":"0066"},"figref":["FIG. 5A","FIG. 5A"],"b":["222","302","121"],"sub":"A "},"At step , the file system within VMKernel  of the operating system receives a request to zero out a file. For example, in a particular embodiment that implements virtualization, VMFS  in VMKernel  may receive a request to zero out a file such as virtual disk (e.g., to preserve VM isolation). The file system resolves the file into its component file segments at step , where <fileid, offset, length>+ in step  represents a list of <fileid, offset, length> file segments. Fileid is a unique identifier that distinguishes segments associated with one file from segments associated with another file. At step , VMKernel  resolves the file segments into logical extents. At step , VMKernel  resolves each of the logical extents into a corresponding list of LUN blocks <LUN ID, offset, length>+. At step , VMKernel  consolidates these lists of LUN blocks into a sourceblocklist, the ordered list LUN blocks representing the relevant file. At step , VMKernel  generates a new zero blocklist primitive containing the sourceblocklist, and embeds it into the CDB of the standard SCSI command WRITE BUFFER. At step , VMKernel  issues the WRITE BUFFER command to the disk array. At decision step , if the disk array supports the new zero blocklist primitive, then, at step , internal disk array mechanisms translate the sourceblocklist to corresponding spindle-extents, and write zeroes into the extents representing the relevant file.","At decision step , if storage system  does not support the new zero blocklist primitive, then, at step , for each block <LUN ID, offset, length> in the sourceblocklist, VMKernel  generates a ioctl move data command with a null source LUN ID to data mover . At step , storage system  receives the write command, internally translates the LUN block into the appropriate spindle-extents, and writes zeroes into the extent representing the block. At decision step , VMKernel  determines if zeroes should be written for another block in the sourceblocklist and if so, steps  and  are repeated to generate and issue ioctl move data commands for another block to data mover . When all of the blocks have been processed, VMKernel  proceeds to step , and execution is complete. Those with ordinary skill in the art will recognize that different functional components or layers of VMKernel  may implement steps  to . For example, in an embodiment that implements virtualization, VMFS  layer of VMKernel  may perform steps  to  of resolving a file into segments and then into logical extents. Logical volume manager  may perform steps  to  of generating the LUN block operations, logical volume manager  of VMKernel  may convert the sourceblocklist into the raw SCSI WRITE BUFFER operation at step , and device access layer  of VMKernel  ultimately transmits the WRITE BUFFER operation at step  to storage system .",{"@attributes":{"id":"p-0070","num":"0069"},"figref":["FIG. 5B","FIG. 3"],"b":["152","300","106","152","300","156","156","152","106","106","106","106"],"sub":["B","B","D ","E "]},"Zeroed extents , , , , , and  that correspond to segment within spindles , , and are shown in . Metadata  is configured to store an extent map including the virtual LUN (assuming that each spindle extent is 64 Kbyte in size) to spindle-extent pair mapping as shown in TABLE 1, where s, s, and s may each correspond to one of spindles -. Although each spindle extent is shown as 64 Kbytes, other sizes may be used for the spindle extents. The zeroed extents may be unmapped from their respective extent maps by updating metadata . Metadata  is updated to indicate that those extents are zeroed (without necessarily writing zeroes) and proprietary mechanisms may be employed to lazily zero out requested extents using a background process, even for non-thin-provisioned LUNs. For example, a flag in metadata  for each spindle extent corresponding to segment , where the flag indicates that the extent should effectively be presented as zeroes to the user. Techniques for performing lazy zeroing are described in patent application Ser. No. 12\/050,805 that is titled, \u201cINITIALIZING FILE DATA BLOCKS\u201d filed Mar. 18, 2008.",{"@attributes":{"id":"p-0072","num":"0071"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE 1"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"Extent Map"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"4"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"70pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"91pt","align":"left"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"LUN offset",{},"Metadata\/configuration"]},{"entry":[{},"(Kbyte)","<spindle, extent>","information"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]},{"entry":[{},"\u20030","<s2, e3>","Zeroed, thin-provisioned"]},{"entry":[{},"\u200264","<s1, e1>","Clone of <s2, e1>"]},{"entry":[{},"128","<s3, e1>","Zeroed, thin-provisioned"]},{"entry":[{},"192","<s2, e3>","free"]},{"entry":[{},". . .",". . .",". . ."]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"3","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0073","num":"0072"},"figref":"FIG. 6A","b":["222","302","600","602","603","230"],"sub":"A "},"At step , logical volume manager  resolves each of the logical extents for each of file A and file B into their corresponding lists of LUN blocks <LUN ID, offset, length>+. At step , logical volume manager  consolidates these lists of LUN blocks into a sourceblocklist and a destinationblocklist for file A and file B, respectively, which are the ordered list LUN blocks representing the respective files. At step , VMKernel  generates the new clone blocklist primitive containing the sourceblocklist and destinationblocklist, and embeds it into the CDB of the standard SCSI command WRITE BUFFER. At step , VMKernel  issues the SCSI command to storage system . At decision step , if storage system  supports the new clone blocklist primitive, then, at step , internal disk array mechanisms clone the destinationblocklist's list of extents with sourceblocklist's list of extents (including utilizing any hardware optimizations within storage system  such as \u201ccopy-on-write\u201d techniques).","If, however, at decision step , storage system  does not support the new clone blocklist primitive, then, at step , VMKernel  issues the move data command to data mover  for execution as previously described. Storage system  receives a write command, internally translates the LUN block into the appropriate spindle-extents, and writes the destination extent representing the block. At decision step , VMKernel  determines if more blocks in sourceblocklist remain to be cloned and if so, step  is repeated to generate and issue a data move command for another block. When all of the blocks have been processed the clone operation is complete. Those with ordinary skill in the art will recognize that different functional components or layers of VMKernel  may implement steps  to . For example, in an embodiment that implements virtualization, VMFS  layer of VMKernel  may perform steps - of generating the LUN block operations, logical volume manager  of VMKernel  may create the sourceblocklist and destinationblocklist at steps - and convert it into the raw SCSI XCOPY operation at step , and device access layer  of VMKernel  ultimately transmits the XCOPY operation at step  to storage system .",{"@attributes":{"id":"p-0076","num":"0075"},"figref":["FIG. 6B","FIG. 3"],"b":["152","300","222","300","222","106","601","605","607","152","152","300","609","611","152","300","156","156","156","152","106","155","609","106","155","605","121","114","405"],"sub":["A ","A ","E ","B","A ","G ","A ","M ","E ","D","E","G "]},"By exposing file administrative level operations such as zero and clone to the set of file operations available to computer systems communicating with a NAS based storage device, storage vendors provide computer systems an ability to offload resource intensive communication with the file storage into the NAS device itself, which can then leverage any proprietary hardware optimizations that may be available internally to the NAS device. In one embodiment, file level primitives may be accessed as (I\/O control) commands using a pre-existing standard command of the communication interface protocol between the computer system and NAS device or, alternatively, may be added as an additional commands to the set of standard commands. The following discussion presents three possible file level primitives supported by a NAS based storage system  (i.e., \u201czero\u201d for zeroing out files and \u201cclone\u201d for cloning files, and \u201cdelete\u201d for deleting files). These file level primitives may be utilized to offload atomic components of larger composite virtual machine operations to the storage system. However, those with ordinary skill in the art will appreciate that other additional and alternative blocklist level primitives may be supported by the storage system  without departing from the spirit and scope of the claimed invention.",{"@attributes":{"id":"p-0078","num":"0077"},"figref":["FIG. 7A","FIG. 7A"],"b":["121","702","208","706","208","708","208","710","208","712","714","716","208","121"]},"Those with ordinary skill in the art will recognize that different functional components or layers of the kernel may implement steps  to . Conventional NAS devices may be configured to write zeroes to blocks to perform administrative operations, however that functionality is not available to users of the NAS device, such as VMs . Without the ZERO_BLOCKS command VMs  transfer zeroes to the NAS device to write zeroes to the blocks corresponding to a file. In some cases, for example when a two terabyte virtual disk is used, as many as two terabytes of zeroes are transferred to the NAS device compared with transferring 20 bytes of parameters using the ZERO_BLOCKS command in order to offload the storage operation from computer system  to the NAS device, e.g., storage system . Additionally, any administrative optimizations that are provided by the NAS device may also be leveraged through the ZERO_BLOCKS command. For example, particular NAS devices may be configured to not store zeroes at the time of the command is received.",{"@attributes":{"id":"p-0080","num":"0079"},"figref":["FIG. 7B","FIG. 7B"],"b":["722","208","726","208","728","208","730","208","732","734","736","208","121"]},"Those with ordinary skill in the art will recognize that the foregoing discussions as well as  are merely exemplary and that alternative blocklist and file level primitives may be implemented without departing from the spirit and scope of the claimed invention. Furthermore, while this discussion has focused upon transmitting blocklist level primitives where the blocklist is representative of an entire file on the file system, those with ordinary skill in the art will recognize that alternative embodiments may work with smaller blacklists, such as blacklists at the file segment level. For example, in the case of zeroing out virtual disk in , an alternative file segment blocklist level embodiment would require 4 instances of issuing the zero blocklist primitive to storage system  (i.e., one for each of the file segments -) in comparison to a single instance of the zero blocklist primitive containing a blocklist comprising a consolidation of the 4 smaller blacklists for the 4 file segments -.","The primitives discussed above can be used to build hardware-assisted data protection (e.g., snapshotting, cloning, mirroring and replication) and other file management commands that operate at the file level and leverage the disk array's internal capabilities. A snapshot of a virtual disk is a copy of the virtual disk as it existed at a given point in time (i.e. a previous version of a virtual disk). A virtualized system such as , may use the zero primitive of  for (a) cloning operations for eager-zeroed virtual disks, (b) initializing new file blocks in thin-provisioned virtual disks, (c) initializing previously unwritten blocks for zeroed virtual disks, and (d) integrating thin-provisioned and zeroed virtual disk formats with the disk array's hardware-based thin-provisioning. Similarly, embedding blocklists within the clone primitive as depicted in  and the CLONE_BLOCKS file primitive of  may be utilized for (a) instant provisioning of virtual disks and (b) snapshotting of virtual disks.","For example, using blacklists as discussed in  or the CLONE_BLOCKS command as discussed in  enables a virtualized system to provide instant provisioning of virtual disks in the order of a few milliseconds or seconds in comparison to a few minutes or hours without the combined use of blacklists and WRITE_BUFFER or XCOPY. Instant provisioning involves making a full copy of a template virtual disk during the process of creating or provisioning a new virtual machine within a computer system. Because virtual disks are represented as significantly large files on the filesystem, performing continual standard SCSI READ and WRITE operations at a LUN block level, including use of read and write buffers within the computer system, takes up significant time and resources. By converting the files into blacklists and utilizing the WRITE_BUFFER or XCOPY SCSI command, the effort to perform the cloning can be offloaded to the hardware of the storage system itself.","The detailed description provided herein with reference to  relates to a virtualized computer system. However, those of ordinary skill in the art will recognize that even non-virtualized computer systems may benefit from such blocklist level primitives\u2014any files existing at the file system level (i.e., not necessarily representative of virtual LUNs) of any computer system may take advantage of such blocklist level primitives. Similarly, while the foregoing discussion has utilized the SCSI interface as a primary example of protocol communication between the disk array and computer system, those with ordinary skill in the art will also appreciate that other communication protocols may be utilized without departing from the spirit and scope of the claimed invention. In particular, as described in conjunction with , a NAS device that provides file level access to storage through protocols such as NFS (in contrast to a SAN disk array supporting SCSI), rather than embedding blocklist primitives into the CDB of pre-existing SCSI commands, similarly functional blocklist primitives may be developed as ioctl control functions for NFS's standard ioctl operation.","The various embodiments described herein may employ various computer-implemented operations involving data stored in computer systems. For example, these operations may require physical manipulation of physical quantities usually, though not necessarily, these quantities may take the form of electrical or magnetic signals where they, or representations of them, are capable of being stored, transferred, combined, compared, or otherwise manipulated. Further, such manipulations are often referred to in terms, such as producing, identifying, determining, or comparing. Any operations described herein that form part of one or more embodiments of the invention may be useful machine operations. In addition, one or more embodiments of the invention also relate to a device or an apparatus for performing these operations. The apparatus may be specially constructed for specific required purposes, or it may be a general purpose computer selectively activated or configured by a computer program stored in the computer. In particular, various general purpose machines may be used with computer programs written in accordance with the teachings herein, or it may be more convenient to construct a more specialized apparatus to perform the required operations.","The various embodiments described herein may be practiced with other computer system configurations including hand-held devices, microprocessor systems, microprocessor-based or programmable consumer electronics, minicomputers, mainframe computers, and the like.","One or more embodiments of the present invention may be implemented as one or more computer programs or as one or more computer program modules embodied in one or more computer readable media. The term computer readable medium refers to any data storage device that can store data which can thereafter be input to a computer system. Computer readable media may be based on technology for embodying computer programs in a manner that enables them to be read by a computer. Examples of a computer readable medium include a hard drive, network attached storage (NAS), read-only memory, random-access memory (e.g., a flash memory device), a CD (Compact Discs) CD-ROM, a CD-R, or a CD-RW, a DVD (Digital Versatile Disc), a magnetic tape, and other optical and non-optical data storage devices. The computer readable medium can also be distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion.","Although one or more embodiments of the present invention have been described in some detail for clarity of understanding, it will be apparent that certain changes and modifications may be made within the scope of the claims. Accordingly, the described embodiments are to be considered as illustrative and not restrictive, and the scope of the claims is not to be limited to details given herein, but may be modified within the scope and equivalents of the claims. In the claims, elements and\/or steps do not imply any particular order of operation, unless explicitly stated in the claims.","In addition, while described virtualization methods have generally assumed that virtual machines present interfaces consistent with a particular hardware system, persons of ordinary skill in the art will recognize that the methods described may be used in conjunction with virtualizations that do not correspond directly to any particular hardware system. Virtualization systems in accordance with the various embodiments, implemented as hosted embodiments, non-hosted embodiments, or as embodiments that tend to blur distinctions between the two, are all envisioned. Furthermore, various virtualization operations may be wholly or partially implemented in hardware. For example, a hardware implementation may employ a look-up table for modification of storage access requests to secure non-disk data.","Many variations, modifications, additions, and improvements are possible, regardless the degree of virtualization. The virtualization software can therefore include components of a host, console, or guest operating system that performs virtualization functions. Plural instances may be provided for components, operations or structures described herein as a single instance. Finally, boundaries between various components, operations and data stores are somewhat arbitrary, and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention(s). In general, structures and functionality presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements may fall within the scope of the appended claims(s)."],"heading":["BACKGROUND","SUMMARY","DETAILED DESCRIPTION"],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":[{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1A"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 1B"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":["FIG. 3","FIG. 2"]},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 4A"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4B"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 4C","FIG. 4B"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":["FIG. 4D","FIG. 4B"]},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 4E","FIG. 4D"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 4F","FIG. 4D"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 5B"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 6B"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 7A"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 7B"}]},"DETDESC":[{},{}]}
