---
title: System for multiresolution analysis assisted reinforcement learning approach to run-by-run control
abstract: A new multiresolution analysis (wavelet) assisted reinforcement learning (RL) based control strategy that can effectively deal with both multiscale disturbances in processes and the lack of process models. The application of wavelet aided RL based controller represents a paradigm shift in the control of large scale stochastic dynamic systems of which the control problem is a subset. The control strategy is termed a WRL-RbR controller. The WRL-RbR controller is tested on a multiple-input-multiple-output (MIMO) Chemical Mechanical Planarization (CMP) process of wafer fabrication for which process model is available. Results show that the RL controller outperforms EWMA based controllers for low autocorrelation. The new controller also performs quite well for strongly autocorrelated processes for which the EWMA controllers are known to fail. Convergence analysis of the new breed of WRL-RbR controller is presented. Further enhancement of the controller to deal with model free processes and for inputs coming from spatially distributed environments are also addressed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07672739&OS=07672739&RS=07672739
owner: University of South Florida
number: 07672739
owner_city: Tampa
owner_country: US
publication_date: 20060811
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["CROSS REFERENCE TO RELATED APPLICATIONS","STATEMENT OF GOVERNMENT INTEREST","FIELD OF INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT","WRL-RbR: A WAVELET MODULATED REINFORCEMENT LEARNING CONTROL","REFERENCES"],"p":["This application claims priority to currently pending U.S. Provisional Patent Application 60\/707,243, entitled, \u201cA Machine Learning Approach to Run by Run Control Using Wavelet Modulated Sensor Data\u201d, filed Aug. 11, 2005, the contents of which are herein incorporated by reference.","This invention was made with government support under grant No. DM10330145 awarded bythe National Science Foundation. The government has certain rights in the invention.","This invention relates to controllers for manufacturing processes. More specifically, this invention relates to a system for multiresolution analysis assisted reinforcement learning approach to run-by-run control.","In recent years, run-by run (RbR) control mechanism has emerged as an useful tool for keeping complex semiconductor manufacturing processes on target during repeated short production runs. Many types of RbR controllers exist in the literature of which the exponentially weighted moving average (EWMA) controller is widely used in the industry. However, EWMA controllers are known to have several limitations. For example, in the presence of multiscale disturbances and lack of accurate process models, the performance of EWMA controller deteriorates and often fails to control the process. Also control of complex manufacturing processes requires sensing of multiple parameters that may be spatially distributed. New control strategies that can successfully use spatially distributed sensor data are required.","Run-by-Run (RbR) process control is a combination of Statistical Process Control (SPC) and Engineering Process Control (EPC). The set points of the automatic PID controllers, which control a process during a run, generally change from one run to the other to account for process disturbances. RbR controllers perform the critical function of obtaining the set point for each new run. The design of a RbR control system primarily consists of two steps\u2014process modeling, and online model tuning and control. Process modeling is done offline using techniques like response surface methods and ordinary least squares estimation. Online model tuning and control is achieved by the combination of offset prediction using a filter, and recipe generation based on a process model (control law). This approach to RbR process control has many limitations that need to be addressed in order to increase its viability to distributed sensing environments. For example, many process controllers rely on good process models that are seldom available for large scale nonlinear systems made up of many interacting subsystems. Even when good (often complex) models are available, the issue becomes the speed of execution of the control algorithms during online applications, which ultimately forces model simplification and resultant suboptimal control. Also the processes are often plagued with multiscale (multiple freq.) noise, which, if not precisely removed, leads to serious lack of controller efficiency.","A new multiresolution analysis (wavelet) assisted reinforcement learning (RL) based control strategy that can effectively deal with both multiscale disturbances in processes and the lack of process models. The application of wavelet aided RL based controller represents a paradigm shift in the control of large scale stochastic dynamic systems of which the control problem is a subset. The control strategy is termed a WRL-RbR controller. The WRL-RbR controller is tested on a multiple-input-multiple-output (MIMO) Chemical Mechanical Planarization (CMP) process of wafer fabrication for which process model is available. Results show that the RL controller outperforms EWMA based controllers for low autocorrelation. The new controller also performs quite well for strongly autocorrelated processes for which the EWMA controllers are known to fail. Convergence analysis of the new breed of WRL-RbR controller is presented. Further enhancement of the controller to deal with model free processes and for inputs coming from spatially distributed environments are also addressed.","The limitations of prior art controllers can be addressed through a multiresolution analysis (wavelet) assisted learning based controller, which is built on strong mathematical foundations of wavelet analysis and approximate dynamic programming (ADP), and is an excellent way to obtain optimal or near-optimal control of many complex systems. This wavelet intertwined learning approach has certain unique advantages. One of the advantages is their flexibility in choosing optimal or near-optimal control action from a large action space. Other advantages include faster convergence of the expected value of the process on to target, and lower variance of the process outputs. Moreover, unlike traditional process controllers, they are capable of performing in the absence of process models and are thus suitable for large scale systems.","This work was motivated by the need to develop an intelligent and efficient RbR process controller, especially for the control of processes with short production runs as in the case of semiconductor manufacturing industry. A controller that is presented here is capable of generating optimal control actions in the presence of multiple time-frequency disturbances, and allows the use of realistic (often complex) process models without sacrificing robustness and speed of execution. Performance measures such as reduction of variability in process output and control recipe, minimization of initial bias, and ability to control processes with high autocorrelations are shown to be superior in comparison to the commercially available EWMA controllers. The WRL-RbR controller is very generic, and can be easily extended to processes with drifts and sudden shifts in the mean and variance. The viability of extending the controller to distributed input parameter sensing environments including those for which process models are not available is also addressed.","According to one aspect of the present invention there is provided a run-by-run controller for controlling output variability in a manufacturing process run. The controller includes a wavelet modulator module to generate a wavelet reconstructed signal (f) from the process output (y) for a run t, a process model module to generate a predicted model output (\u0177) for a run t, an error predictor module to predict a forecast offset (a) using the input E=f\u2212\u0177; and a recipe generator module to generate a control recipe (u) by applying the forecast offset (a), wherein the control recipe is passed to a PID controller as a set-point for the next run and to the process model module to predict the next process output at run t+1.","According to one aspect of the present invention there is provided a method of performing run-by-run control to control output variability in a manufacturing process run. The method includes the steps of generating a wavelet reconstructed signal (f) from the process output (y) for a run t, generating a predicted model output (\u0177) for a run t using a control recipe (u), predicting a forecast offset (a) using the input E=f\u2212\u0177, generating a control recipe (u) by applying the forecast offset (a), wherein the control recipe is passed to a PID controller set-point for the next run and to the process model module to predict the next process output at run t+1 and passing the control recipe (u) to a PID controller as a set-point for the next run and to the process model module to predict the next process output at run t+1. In certain aspects of the present invention the manufacturing process is a MIMO process. In yet other aspect of the present invention the manufacturing process is a SISO process.","A new multiresolution analysis (wavelet) assisted reinforcement learning (RL) based control strategy that can effectively deal with both multiscale disturbances in processes and the lack of process models. The application of wavelet aided RL based controller represents a paradigm shift in the control of large scale stochastic dynamic systems of which the control problem is a subset. The control strategy is termed a WRL-RbR controller. The WRL-RbR controller is tested on a multiple-input-multiple-output (MIMO) Chemical Mechanical Planarization (CMP) process of wafer fabrication for which process model is available. Results show that the RL controller outperforms EWMA based controllers for low autocorrelation. The new controller also performs quite well for strongly autocorrelated processes for which the EWMA controllers are known to fail. Convergence analysis of the new breed of WRL-RbR controller is presented. Further enhancement of the controller to deal with model free processes and for inputs coming from spatially distributed environments are also addressed.","Among the process control literature for stochastic systems with short production runs, a commonly used control is the RbR controller. Some of the major RbR algorithms include EWMA control [1], which is a minimum variance controller for linear and autoregressive processes, optimizing adaptive quality control (OAQC) [2] which uses Kalman filtering, and model predictive R2R control (MPR2RC) [3] in which the control action is based on minimizing an objective function such as mean square deviation from target. Comparative studies between the above types of controllers indicate that in the absence of measurement time delays, EWMA, OAQC and MPR2RC algorithms perform nearly identically [4] and [5]. Also, among the above controllers, the EWMA controller has been most extensively researched and widely used to perform RbR control [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], and [17].","Consider a SISO process\n\n+noise,\u2003\u2003(1)\n\nwhere t is the index denoting the run number, yis the process output after run t, \u03b3 denotes the offset, \u03b7 represents the gain, and urepresents the input before run t. To account for process dynamics, the RbR controllers assume that the intercept \u03b3 varies with time [1]. This is incorporated by considering the prediction model for the process to be\n\n\u2003\u2003(2)\n\nfor which the corresponding control action is given by\n",{"@attributes":{"id":"p-0025","num":"0024"},"maths":{"@attributes":{"id":"MATH-US-00001","num":"00001"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"msub":{"mi":["u","t"]},"mo":"=","mfrac":{"mrow":{"mi":"T","mo":"-","msub":{"mi":"a","mrow":{"mi":"t","mo":"-","mn":"1"}}},"mi":"b"}},"mo":","}},{"mrow":{"mo":["(",")"],"mn":"3"}}]}}}},"br":[{},{}],"sub":["t\u22121 ","t\u22121","t ","t","t","t","t\u22121","t\u22121"],"in-line-formulae":[{},{}],"i":["a","y","\u2212bu","a"]},"Some of the primary drawbacks of controllers listed above include (1) dependence on good process models, (2) control actions limited by fixed filtering parameters as in EWMA, (3) inability to handle large perturbations of the system, (4) dependence on multiple filtering steps to compensate for drifts and autocorrelation, (5) inability to deal with the presence of multiscale noise, and (6) inability to scale up to large real world systems.","A control strategy is basically the prediction of forecast offset a, which in turn decides the value of the recipe uas per the predicted model (2). Hence, the performance of a control strategy greatly depends on its ability to accurately predict a. At every step of the RbR control, the number of possible choices for forecast offset at could be infinite. The key is to develop a strategy for predicting the best value of at for the given process output. The accuracy of the prediction process in conventional controllers such as the EWMA suffers from two aspects. These include 1) multiscale noises that mask the true process deviations, which are used in the prediction process, and 2) the use of a fixed filtering strategy as given by (4) limits the action choices. A wavelet interfaced machine learning based approach for predicting acould provide the ability to extract the true process, and thus predict the correct offset, and also evaluate a wide range of control choices in order to adopt the best one as explained below.","In most real world applications, inherent process variations, instead of being white noise with single scale (frequency), are often multiscale with different features localized in time and frequency. Thus, the true process outputs ycould be masked by the presence of these multiscale noises. Some examples of multiscale noise include vibrations and other disturbances captured by the sensors, noise added by the sensing circuit, measurement noise, and radio-frequency interference noise. It is beneficial if a controller could be presented with a true process output with only its significant features and without the multiscale noise. This could be accomplished through denoising of multiscale noise via a wavelet based multiresolution thresholding approach. The wavelet methods provide excellent time-frequency localized information, i.e. they analyze time and frequency localized features of the sensor data simultaneously with high resolution. They also posses the unique capability of representing long signals in relatively few wavelet coefficients (data compression). The wavelet based multiresolution approach has the ability to eliminate noise from the process output signal while retaining significant process features arising from disturbances such as trends, shifts, and autocorrelation [18]. Other denoising techniques such as short time Fourier transform (STFT) and other time or frequency only based approaches are known to be inferior to the wavelet based approach in dealing with multiscale signals due to following reasons. The conventional time domain analysis methods, which are sensitive to impulsive oscillations, have limited utility in extracting hidden patterns and frequency related information in these signals [19] and [20]. This problem is partially overcome by spectral (frequency) analysis such as Fourier transform, the power spectral density, and the coherence function analysis. However, many spectral methods rely on the implicit fundamental assumption of signals being periodic and stationary, and are also inefficient in extracting time related features. This problem has been addressed to a large extent through the use of time-frequency based STFT methods. However, this method uses a fixed tiling scheme, i.e., it maintains a constant aspect ratio (the width of the time window to the width of the frequency band) throughout the analysis (). As a result, one must choose multiple window widths to analyze different data features localized in time and frequency domains in order to determine the suitable width of the time window. STFT is also inefficient in resolving short time phenomena associated with high frequencies since it has a limited choice of wave forms [21]. In recent years, another time-frequency (or time-scale) method known as wavelet based multiresolution analysis have gained popularity in the analysis of both stationary and nonstationary signals. These methods provide excellent time-frequency localized information, which is achieved by varying the aspect ratio as shown in . This means that multiple frequency bands can be analyzed simultaneously in the form of details and approximations plotted over time, as described in the next section. Hence, different time and frequency localized features are revealed simultaneously with high resolution. This scheme is more adaptable (compared to STFT) to signals with short time features occurring at higher frequencies.","Though an exact mathematical analysis of the effects of multiscale noise on performance of EWMA controllers is not available, some experimental studies conducted by us show that EWMA controllers attempt to compensate for multiscale noise through higher variations of the control recipe (u). However, this in turn results in higher variations of the process output. It is also noted that, if the expected value of the process is on target and the process is subjected to variations, for which there are no assignable causes, the controller need not compensate for such variations, and hence the recipe should remain constant. In fact, an attempt to compensate for such variations from chance causes (noise) not only increases the variations of ubut also increases the variations of the process output y. A controller is maintained in place in anticipation of disturbances, such as mean and variance shift, trend, and autocorrelation, resulting from assignable causes. As a result, in the absence of disturbances, controllers continue to unduly compensate for process dynamics due to noise. Also EWMA is a static control strategy where the control is guided by the chosen \u03bb value as shown in (4). Thus EWMA controllers do not offer the flexibility of a having a wide variety of control choices. The above difficulties can be well addressed by a learning based intelligent control approach. Such an approach is developed in this research and is presented next.","A new control strategy is thus presented, named wavelet modulated reinforcement learning run by run control (WRL-RbR), that benefits from both wavelet based multiresolution denoising and reinforcement learning, as discussed above, and thus alleviates many of the shortcomings of EWMA controllers.",{"@attributes":{"id":"p-0031","num":"0030"},"figref":"FIG. 2","sub":["t ","t","t","t","t ","t ","t\u22121 "]},"A. Wavelet Assisted Multiscale Denoising","The wavelet based multiscale denoising renders many advantages that a controller can benefit from. One of these advantages is the detection of deterministic trends in the original signal. This can be achieved by monitoring the slope information in the approximation coefficients of the decomposition step. This information on the trend can be used as additional information for the controller to develop trend compensation strategies. Another advantage of wavelet analysis is the protection it offers against sudden spikes in the original signal which can result in oscillations in the control.","Conceptually, multiscale denoising can be explained using the analogy of nonparametric regression in which a signal fis extracted from a noisy data yas\n\n+noise,\u2003\u2003(5)\n\nwhere noiseis the noise removed by the wavelet analysis procedure described below. The wavelet analysis consists of three steps: 1) decomposition of the signal using orthogonal wavelets into wavelet coefficients, 2) thresholding of the wavelet coefficients, and 3) reconstruction of the signal into the time domain. The basic idea behind signal decomposition with wavelets is that the signal can be separated into its constituent elements through fast wavelet transform (FWT). A more detailed theory on multiresolution analysis can be found in [22]. In our method we used Daubechies [23] 4order wavelet basis function. Our choice of the basis function was motivated by the following properties: 1) It has orthogonal basis with a compact support. 2) The coefficients of the basis function add up to the square root of 2, and their sum of squares is unity; this property is critical for perfect reconstruction. 3) The coefficients are orthogonal to their double shifts. 4) The frequency responses has a double zero (produces 2 vanishing moments) at the highest frequency \u03c9=\u03c0, which provides maximum flatness. 5) With downsampling by 2, this basis function yields a halfband filter. It is to be noted that the choice of the basis function is dependent on the nature of the signal arising from a given application.\n","Thresholding of the wavelet coefficients d(j is the scale and k is the translation index) help to extract the significant coefficients. This is accomplished by using the Donoho's threshold rule [24]. This threshold rule is also called visual shrink or \u2018VisuShrink\u2019 method, in which a universal scale-dependent threshold tis proposed. The significant wavelet coefficients that fall outside of the threshold limits are then extracted by applying either soft or hard thresholding. WRL-RbR controller developed here uses soft thresholding. It is important to select the number of levels of decomposition and the thresholding values in such as way that excessive smoothing of the features of the original signal is prevented. A good review of various thresholding methods and a guideline for choosing the best method is available in [25] and [26]. Reconstruction of the signal in the time domain from the thresholded wavelet coefficients is achieved through inverse wavelet transforms. The reconstructed signal is denoted as f.","B. Process Model","Process models relate the controllable inputs uto the quality characteristic of interest \u0177. Primarily, the prediction models are obtained from offline analysis through least squares regression, response surface methods, or through a design of experiments method. It is to be noted that, real world systems requiring distributed sensing are often complex and have large number of response and input variables. Models of such systems are highly non-linear. However, in practice complex non-linear models are not used in actual process control. This is because complex models often lack speed of execution during on-line model evaluation, and also introduce additional measurement delays since many of the response factors can only be measured off-line. This retards the feedback needed in generating control recipes for the next run. In essence, execution speed is emphasized over model accuracy, which promotes the use of simplified linear models [27]. The WRL-RbR strategy allows the use of more accurate complex models. This is because the control strategy is developed offline and hence requires no online model evaluation during its application.","C. RL Based Error Prediction","A machine learning approach can be used for the task of offset (a) prediction. The evolution of error E=f\u2212\u0177, (a random variable) during the process runs is modeled as a Markov chain. The decision to predict the process offset at after each process run based on the error process Eis modeled as a Markov decision process (MDP). For the purpose of solving the MDP, it is necessary to discretize Eand a. Due to the large number of state and action combinations tuple (E, a), the Markov decision model is solved using a machine learning (reinforcement learning, in particular) approach. We first present a formal description of the MDP model and then discuss the RL approach to solve the model.","1) MDP Model of the RbR Control: Assume that all random variables and processes are defined on the probability space (\u03a9, F, P). The system state at the end of the trun is defined as the difference between the process output and the model predicted output (E=f\u2212\u0177). Let E={E:t=0, 1, 2, 3 . . . } be the system state process. Since, it can be easily argued that Eis dependent only on E, the random process E is a Markov chain.","Since the state transitions are guided by a decision process, where a decision maker selects an action (offset) from a finite set of actions at the end of each run, the combined system state process and the decision process becomes a Markov decision process. The transition probability in a MDP can be represented as p(x, d, q), for transition from state x to state q under action d. Let \u03b5 denote the system state space, i.e., the set of all possible values of E. Then the control system can be stated as follows. For any given x element \u03b5 at run t, there is an action selected such that the expected value of the process yat run t+1 is maintained at target T. In the context of RbR control, the action at run t is to predict the offset awhich is then used to obtain the value of recipe u. Theoretically, the action space for the predicted offset could range from a large negative number to a large positive number. However, in practice, for a non-diverging process, the action space is quite small, which can be discretized to a finite number of actions. We denote the action space as A. Several measures of performance such as discounted reward, average reward, and total reward can be used to solve a MDP. Reward is defined r(x, d, q) for taking action d in state x at any run t+1 that results in a transition to state q, as the actual error E=f\u2212\u0177resulting from the action. Since the objective of the MDP is to develop an action strategy that minimizes the actual error, an average reward is adopted as the measure of performance. In the next subsection, the specifics of the offset prediction methodology using a RL based stochastic approximation scheme is provided.","2) Reinforcement Learning: RL is a simulation-based method for solving MDPs, which is rooted in the Bellman [28] equation, and uses the principle of stochastic approximation (e.g. Robbins-Monro method [29]). Bellman's optimality equation for average reward says that there exists a \u03c1* and R* that satisfies the following equation:",{"@attributes":{"id":"p-0043","num":"0042"},"maths":{"@attributes":{"id":"MATH-US-00002","num":"00002"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"msup":{"mi":"R","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"x"}},{"munder":{"mi":"min","mrow":{"mi":["d","A"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d"],"mo":","}}},{"munder":{"mo":"\u2211","mrow":{"mi":["q","\u025b"],"mo":"\u2208"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","q"],"mo":[",",","]}}},{"msup":{"mi":"R","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mi":"q"}}],"mo":"\u2062"}}],"mo":["-","+"],"msup":{"mi":"\u03c1","mo":"*"}}}}],"mo":"="}},{"mrow":{"mo":["(",")"],"mn":"6"}}]}}}},"br":{}},{"@attributes":{"id":"p-0044","num":"0043"},"maths":{"@attributes":{"id":"MATH-US-00003","num":"00003"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mi":"\u03c1","mo":"=","mrow":{"munder":{"mi":"lim","mrow":{"mi":["N","\u221e"],"mo":"\u2192"}},"mo":"\u2062","mrow":{"mfrac":{"mn":"1","mi":"N"},"mo":["\u2062","\u2062"],"mi":"E","mrow":{"mo":["{","}"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"N"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","t"]}}}}}}}}},{"mrow":{"mo":["(",")"],"mn":"7"}}]},{"mtd":[{"mrow":{"mi":"R","mo":"=","mrow":{"mi":"E","mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"munderover":{"mo":"\u2211","mrow":{"mi":"t","mo":"=","mn":"1"},"mi":"\u221e"},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mo":["[","]"],"mrow":{"mrow":{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["X","t"]}}},"mo":"-","mi":"\u03c1"}}}}}}},{"mrow":{"mo":["(",")"],"mn":"8"}}]}]}}},"br":{},"sub":"t"},"The above optimality equation can be solved using the relative value iteration (RVI) algorithm as given in [30]. However, the RVI needs the transition probabilities p(x, d, q), which are often, for real life problems, impossible to obtain. An alternative to RVI is asynchronous updating of the R-values through Robbins-Monro (RM) stochastic approximation approach, in which the expected value component \u03a3 p(x, d, q)R*(q) in (6) can be replaced by a sample value of R(q) obtained through simulation. The WRL-RbR algorithm is a two-time scale version of the above learning based stochastic approximation scheme, which learns p and uses it to learn R*(x, d) for all x an element of \u03b5 and d an element of A. Convergent average reward RL algorithms (R-learning) can be found in [31], and [32]. The strategy adopted in R-Learning is to obtain the R-values, one for each state-action pair. After the learning is complete, the action with the highest (for maximization) or lowest (for minimization) R-value for a state constitutes the optimal action. Particularly in control problems, reinforcement learning has significant advantages as follows: 1) it can learn arbitrary objective functions, 2) there is no requirement to provide training examples, 3) they are more robust for naturally distributed system because multiple RL agents can be made to work together toward a common objective, 4) it can deal with the \u2018curse of modeling\u2019 in complex systems by using simulation models instead of exact analytical models that are often difficult to obtain, and 5) can incorporate function approximation techniques in order to further alleviate the \u2018curse of dimensionality\u2019 issues.","The Bellman's equation given in (6) can be rewritten in terms of values for every state-action combination as follows. At the end of the trun (decision epoch) the system state is Et=x an element of \u03b5. Bellman's theory of stochastic dynamic programming says that the optimal values for each state-action pair (x, d) can be obtained by solving the average reward optimality equation",{"@attributes":{"id":"p-0047","num":"0046"},"maths":{"@attributes":{"id":"MATH-US-00004","num":"00004"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mo":"\u2003","mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":[{"msup":{"mi":"R","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d"],"mo":","}}},{"mrow":[{"mo":["[","]"],"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":["q","\u025b"],"mo":"\u2208"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","q"],"mo":[",",","]}}},{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","q"],"mo":[",",","]}}}],"mo":"\u2062"}}},{"mrow":[{"mo":["[","]"],"mrow":{"munder":{"mo":"\u2211","mrow":{"mi":["j","\u025b"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","j"],"mo":[",",","]}}},{"munder":{"mi":"min","mrow":{"mi":["d","A"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"msup":{"mi":"R","mo":"*"},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["j","d"],"mo":","}}}}],"mo":"\u2062"}}},{"mo":"\u2200","mi":"x"}],"mo":"\u2062"}],"mo":["-","+"],"msup":{"mi":"\u03c1","mo":"*"}}],"mo":"="},{"mo":"\u2200","mrow":{"mi":"d","mo":"."}}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"9"}}]}}}}}},"A two-time scale version of the learning based approach that we have adopted to solve the optimal values for each state-action combination R*(x, d) is as follows.",{"@attributes":{"id":"p-0049","num":"0048"},"maths":{"@attributes":{"id":"MATH-US-00005","num":"00005"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"mrow":[{"msub":{"mi":"R","mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d"],"mo":","}}},{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":["\u03b1","t"]}}},{"msub":{"mi":["R","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d"],"mo":","}}}],"mo":"\u2062"},{"mrow":[{"msub":{"mi":["\u03b1","t"]},"mo":["[","]"],"mrow":{"mrow":[{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","q"],"mo":[",",","]}}},{"munder":{"mi":"min","mrow":{"mi":["b","A"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"msub":{"mi":["R","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["q","b"],"mo":","}}}}],"mo":["-","+"],"msub":{"mi":["\u03c1","t"]}}},{"mo":"\u2200","mi":"x"}],"mo":"\u2062"}],"mo":"+"}],"mo":"\u2190"},{"mo":"\u2200","mi":"d"}],"mo":[",",","]}},{"mrow":{"mo":["(",")"],"mn":"10"}}]},{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"3.9em","height":"3.9ex"}}},"mo":"\u2062","mrow":{"msub":{"mi":"\u03c1","mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":["\u03b2","t"]}}},"mo":"\u2062","msub":{"mi":["\u03c1","t"]}},{"mrow":{"msub":{"mi":["\u03b2","t"]},"mo":"\u2061","mrow":{"mo":["[","]"],"mfrac":{"mrow":{"mrow":[{"msub":[{"mi":["\u03c1","t"]},{"mi":["T","t"]}],"mo":"\u2062"},{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","q"],"mo":[",",","]}}}],"mo":"+"},"msub":{"mi":"T","mrow":{"mi":"t","mo":"+","mn":"1"}}}}},"mo":"."}],"mo":"+"}}}},{"mrow":{"mo":["(",")"],"mn":"11"}}]}]}}}},"In the above equations, t denotes the step index in the learning process (run number in the context of control), \u03b1and \u03b2are learning parameters, which take values (0, 1), and Tis the cumulative time till the tlearning step.","The learning parameters \u03b1and \u03b2are both decayed by the following rule.",{"@attributes":{"id":"p-0052","num":"0051"},"maths":{"@attributes":{"id":"MATH-US-00006","num":"00006"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mrow":{"msub":[{"mi":["\u03b1","t"]},{"mi":["\u03b2","t"]}],"mo":"\u2062"},"mo":"=","mfrac":{"mrow":[{"msub":[{"mi":"\u03b1","mn":"0"},{"mi":"\u03b2","mn":"0"}],"mo":","},{"mn":"1","mo":"+","mi":"z"}]}},{"mi":"z","mo":"=","mfrac":{"msup":{"mi":"t","mn":"2"},"mrow":{"mi":["K","t"],"mo":"+"}}}],"mo":[",",","]}},{"mrow":{"mo":["(",")"],"mn":"12"}}]}}}},"br":[{},{}],"in-line-formulae":[{},{}],"i":["R","x,d","R","x,d"],"sub":["t+1","t"]},"At the beginning of the learning process, the R-values are initialized to zeros. When the process enters a state for the first time, the action is chosen randomly since the R-values for all actions are zero initially. In order to allow for effective learning in the early learning stages, instead of the greedy action the decision maker with probability Pchooses from other actions. The choice among the other actions is made by generating a random number from a uniform distribution. The above procedure is commonly referred to in literature as exploration. The value of p(called the exploration probability) is decayed faster than the learning parameters using equation (12). Storing of the R-values for each state-action combination often presents a computational challenge for large scale systems with numerous state-action combinations. One approach is to represent the R-values of subsets of state-action space as functions instead of storing R-values for each individual state-action combination, a method known as function approximation. Recently, a diffusion wavelet based function approximation scheme has been presented to the literature [33], [34], and [35].","D. Recipe Generation","Once learning is completed, the R-values provide the optimal action choice for each state. At any run t, as the process enters a state, the action d corresponding to the lowest nonzero absolute R-value indicates the predicted forecast offset a. This is used in the calculation of the recipe u. In what follows we present the steps of the WRL-RbR algorithm in the implementation phase.","V. WRL-RbR Algorithm\n\n",{"@attributes":{"id":"p-0057","num":"0060"},"maths":{"@attributes":{"id":"MATH-US-00007","num":"00007"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"mrow":[{"msub":{"mi":"R","mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["E","t"]},{"mi":["a","t"]}],"mo":"\u2062"}}},{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":["\u03b1","t"]}}},{"msub":{"mi":["R","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["E","t"]},{"mi":["a","t"]}],"mo":","}}}],"mo":"\u2062"},{"mrow":[{"msub":{"mi":["\u03b1","t"]},"mo":"\u2061","mrow":{"mo":["[","]"],"mrow":{"mrow":[{"mrow":{"mrow":[{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["E","t"]},{"mi":["a","t"]},{"mi":"E","mrow":{"mi":"t","mo":"+","mn":"1"}}],"mo":[",",","]}}},{"munder":{"mi":"min","mrow":{"mi":["b","A"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"msub":{"mi":["R","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"E","mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":",","mi":"b"}}}}],"mo":["-","+"],"msub":{"mi":["\u03c1","t"]}},"mo":":","mi":"b"},{"msub":{"mi":"argmin","mrow":{"mi":["c","A"],"mo":"\u2208"}},"mo":["\u2062","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"4.2em","height":"4.2ex"}}}],"mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["R","t"]},"mo":"\u2062","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"E","mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":",","mi":"c"}}}},{"mrow":{"msub":{"mi":["R","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"E","mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":",","mi":"c"}}},"mo":"\u2260","mn":"0"}],"mo":":"}}}],"mo":"="}}},{"mo":"\u2200","msub":{"mi":["E","t"]}}],"mo":"\u2062"}],"mo":"+"}],"mo":"\u2190"},{"mo":"\u2200","msub":{"mi":["a","t"]}}],"mo":","}},{"mrow":{"mo":["(",")"],"mn":"14"}}]},{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}},"mo":"\u2062","mrow":{"mrow":[{"mrow":{"munder":{"mi":"min","mrow":{"mi":["b","A"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"msub":{"mi":["R","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"E","mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":",","mi":"b"}}}},"mo":":","mi":"b"},{"msub":{"mi":"argmin","mrow":{"mi":["c","A"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mo":["{","}"],"mrow":{"mrow":[{"mo":["\uf603","\uf604"],"mrow":{"msub":{"mi":["R","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"E","mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":",","mi":"c"}}}},{"mrow":{"msub":{"mi":["R","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":"E","mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":",","mi":"c"}}},"mo":"\u2260","mn":"0"}],"mo":[":","\u2062","\u2062"],"mstyle":[{"mtext":{}},{"mspace":{"@attributes":{"width":"4.2em","height":"4.2ex"}}}]}}}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"15"}}]}]}}}},"indicates that afor any state E, the greedy action b for which the absolute non-zero R-value that is closest to zero should be chosen. The optimal average reward \u03c1is updated as follows.",{"@attributes":{"id":"p-0059","num":"0062"},"maths":{"@attributes":{"id":"MATH-US-00008","num":"00008"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"msub":{"mi":"\u03c1","mrow":{"mi":"t","mo":"+","mn":"1"}},"mo":"=","mrow":{"mrow":[{"mrow":{"mo":["(",")"],"mrow":{"mn":"1","mo":"-","msub":{"mi":["\u03b2","t"]}}},"mo":"\u2062","msub":{"mi":["\u03c1","t"]}},{"mrow":{"msub":{"mi":["\u03b2","t"]},"mo":"\u2061","mrow":{"mo":["[","]"],"mfrac":{"mrow":{"mrow":[{"msub":[{"mi":["\u03c1","t"]},{"mi":["T","t"]}],"mo":"\u2062"},{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":[{"mi":["E","t"]},{"mi":["a","t"]},{"mi":"E","mrow":{"mi":"t","mo":"+","mn":"1"}}],"mo":[",",","]}}}],"mo":"+"},"msub":{"mi":"T","mrow":{"mi":"t","mo":"+","mn":"1"}}}}},"mo":"."}],"mo":"+"}}},{"mrow":{"mo":["(",")"],"mn":"16"}}]}}}}},"Learnt Stage: Using Eidentify the state x of the process. The forecast offset afor this state is now obtained from the R-value matrix by choosing the action that corresponds to the minimum of the absolute non-zero R-value for that state.\n\n","VI. Analysis for Convergence of the WRL-RBR Controller","In the interests of brevity, the complete proof of convergence of the RL scheme adopted for WRL-RbR is not presented here. The numerical results presented in Section VII provide additional evidence of the controller's convergence in terms of the boundedness of the process output and its expected value being on target. These conditions are necessary to ensure stability of the controller. In what follows, it is shown that the WRL-RbR algorithm converges, and yields R(x,d) cvalues that give optimal process control strategy. The optimal process control strategy ensures that the expected value of the process output ycoincide with the target T, and also that the y's are bounded.","It is first shown that the approximation schemes in the algorithm use transformation that are of the form presented in [38] and track ordinary differential equations (ODEs). The ODE framework based on convergence analysis presented in [39] is then used to show the convergence of the WRL-RbR algorithm.","Define the transformations as follows.",{"@attributes":{"id":"p-0065","num":"0069"},"maths":{"@attributes":{"id":"MATH-US-00009","num":"00009"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":[{"mtd":[{"mrow":{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"H","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["R","t"]}}}},{"mo":["(",")"],"mrow":{"mi":["x","d"],"mo":","}}],"mo":"\u2062"},{"munder":{"mo":"\u2211","mrow":{"mi":["q","\u025b"],"mo":"\u2208"}},"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","q"],"mo":[",",","]}}},{"mrow":[{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","q"],"mo":[",",","]}}},{"munder":{"mi":"min","mrow":{"mi":["b","A"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"msub":{"mi":["R","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["q","b"],"mo":","}}}}],"mo":["-","+"],"msup":{"mi":"\u03c1","mo":"*"}}],"mo":"["}}],"mo":["=","\u2062"],"mi":{}}},{"mrow":{"mo":["(",")"],"mn":"17"}}]},{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}},"mo":"\u2062","mrow":{"mrow":{"mrow":[{"mrow":[{"mo":["(",")"],"mrow":{"msub":{"mi":"H","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["R","t"]}}}},{"mo":["(",")"],"mrow":{"mi":["x","d"],"mo":","}}],"mo":"\u2062"},{"mo":["[","]"],"mrow":{"mrow":[{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","q"],"mo":[",",","]}}},{"munder":{"mi":"min","mrow":{"mi":["b","A"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"msub":{"mi":["R","t"]},"mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["q","b"],"mo":","}}}}],"mo":["-","+"],"msup":{"mi":"\u03c1","mo":"*"}}}],"mo":["=","\u2062"],"mi":{}},"mo":","}}},{"mrow":{"mo":["(",")"],"mn":"18"}}]},{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}},"mo":"\u2062","mrow":{"mrow":{"mrow":[{"msub":{"mi":"F","mn":"1"},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u03c1","t"]}}},{"munder":{"mo":"\u2211","mrow":{"mi":["q","\u025b"],"mo":"\u2208"}},"mo":"\u2062","mrow":{"mrow":[{"mi":"p","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","q"],"mo":[",",","]}}},{"mo":["[","]"],"mfrac":{"mrow":{"mrow":[{"msub":[{"mi":["\u03c1","t"]},{"mi":["T","t"]}],"mo":"\u2062"},{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","q"],"mo":[",",","]}}}],"mo":"+"},"msub":{"mi":"T","mrow":{"mi":"t","mo":"+","mn":"1"}}}}],"mo":"\u2061"}}],"mo":"="},"mo":","}}},{"mrow":{"mo":["(",")"],"mn":"19"}}]},{"mtd":[{"mrow":{"mstyle":{"mspace":{"@attributes":{"width":"4.4em","height":"4.4ex"}}},"mo":"\u2062","mrow":{"mrow":[{"msub":{"mi":"F","mn":"2"},"mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u03c1","t"]}}},{"mrow":{"mo":["[","]"],"mfrac":{"mrow":{"mrow":[{"msub":[{"mi":["\u03c1","t"]},{"mi":["T","t"]}],"mo":"\u2062"},{"mi":"r","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"mi":["x","d","q"],"mo":[",",","]}}}],"mo":"+"},"msub":{"mi":"T","mrow":{"mi":"t","mo":"+","mn":"1"}}}},"mo":"."}],"mo":"="}}},{"mrow":{"mo":["(",")"],"mn":"20"}}]}]}}}},"Also define errors \u03c9and \u03c9as:\n\n\u03c9=(())()\u2212(())(),\u2003\u2003(21)\n\n\u03c9(\u03c1)\u2212(\u03c1).\u2003\u2003(22)\n","The first of the two-time scale approximation equation (10) can now be written as:\n\n()=()+\u03b1((),\u03c1)+\u03c9],\u2003\u2003(23)\n\nwhere:\n\n()=()\u2212.\u2003\u2003(24)\n","As in [39], it can be shown that (23) yields an ODE of the form:",{"@attributes":{"id":"p-0069","num":"0073"},"maths":{"@attributes":{"id":"MATH-US-00010","num":"00010"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":"\u2146","msub":{"mi":["R","t"]}},{"mo":"\u2146","mi":"\u03c4"}]},"mo":"=","mrow":{"mrow":{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["R","t"]},"mo":"\u2062","mi":"\u03c1"}}},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"25"}}]}}}},"br":[{},{},{},{},{}],"in-line-formulae":[{},{},{},{}],"sub":["t+1","t","t","t","2","t","1","t","t"],"i":["[g","g","F"],"sup":"t"},{"@attributes":{"id":"p-0070","num":"0074"},"maths":{"@attributes":{"id":"MATH-US-00011","num":"00011"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":"\u2146","msub":{"mi":["\u03c1","t"]}},{"mo":"\u2146","mi":"\u03c4"}]},"mo":"=","mrow":{"mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u03c1","t"]}}},"mo":"."}}},{"mrow":{"mo":["(",")"],"mn":"28"}}]}}}}},"A. Assumptions","1) Assumption 1: The functions h and g, defined in (25) and (28), are Lipschitz continuous. This is true because the mappings (H(R)) and F(\u03c1) are linear everywhere as can be see from (17) and (19).","2) Assumption 2: Each state-action pair is visited after a finite time interval. This assumption is satisfied by running simulation for an arbitrarily long period of time until the condition |R(x, d)\u2212R(x, d)|<\u03b5 is ensured for every state-action pair that is visited. However, some remote state-action pairs are rarely visited or none at all even after substantial exploration. Such state-action pairs that are not visited too \u2018often do not impact quality of the decision.","3) Assumption 3: The step size \u03b1and \u03b2are small, which can be ensured by appropriately selecting the parameter values. The nature of R-learning is such that the reward values are updated asynchronously (one state-action pair updated in each iteration of the learning process). In order to obtain convergence to the same reward values as in the case of synchronous algorithms (where rewards for all states are updated simultaneously, i.e., in dynamic programming using transition probabilities), it is necessary to maintain small values of learning parameters \u03b1and \u03b2. The \u03b1and \u03b2values are chosen very small in order to allow slow learning and corresponding convergence. Large values of \u03b1and \u03b2could cause R-values to oscillate and not converge.","4) Assumption 4: The learning parameters must satisfy the following condition:",{"@attributes":{"id":"p-0076","num":"0080"},"maths":{"@attributes":{"id":"MATH-US-00012","num":"00012"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":{"munder":{"mi":"lim","mrow":{"mi":["t","\u221e"],"mo":"\u2192"}},"mo":"\u2062","mrow":{"mi":"sup","mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.3em","height":"0.3ex"}}},"mfrac":{"msub":[{"mi":["\u03b2","t"]},{"mi":["\u03b1","t"]}]}}},"mo":"=","mn":"0."}},{"mrow":{"mo":["(",")"],"mn":"29"}}]}}}}},"The interpretation of this assumption is that the rate of decay for learning parameter fit is faster than (a). This is achieved by fixing the starting values of both \u03b1and \u03b2as 0.01 and 0.001 respectively (Section VII A). This assumption is very crucial for these schemes to work. It says that the second iteration (Equation 16) is much slower than the first (Equation 14) because of its smaller step-size. This implies that the fast iteration in R sees the slower iteration in \u03c1 as a constant and hence converges, while the slower iteration sees the faster iteration as having converged [38] and [40]. The limiting behavior of the slower iteration is given by the ODE in Assumption 8 while that of the faster one is given by that in Assumption 7. Assumptions 2, 3, and 4 place restrictions on the learning process.","5) Assumption 5: The iterates Rand \u03c1are bounded. From the definition of the gain (7) it implies that the expected value of r() is also bounded. Since at any time t the expected reward r({dot over ( )})=E=f\u2212\u0177(see definition of Ein Section IV), it implies that the process output yis bounded. This implies that both Rand \u03c1are bounded.","6) Assumption 6: The expected value of the error terms in Equation (21) and (22) are 0 and their variances are bounded. This condition is satisfied because it can be seen from the definition of these terms that the error represents the difference between the sample and a conditional mean. By martingale convergence theory, the conditional mean tends to 0 as the number of samples tends to infinity. As per Assumption 5, iterates are bounded. This implies that the right side of (23) and (26) are bounded, which ensures that the variance of the error terms \u03c91 and \u03c92 are bounded.","7) Assumption 7. The ODE:",{"@attributes":{"id":"p-0081","num":"0085"},"maths":{"@attributes":{"id":"MATH-US-00013","num":"00013"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":"\u2146","msub":{"mi":["R","t"]}},{"mo":"\u2146","mi":"\u03c4"}]},"mo":"=","mrow":{"mi":"h","mo":"\u2061","mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["R","t"]},"mo":",","mi":"\u03c1"}}}}},{"mrow":{"mo":"\u2200","mi":"\u03c1"}}]}}},{"mrow":{"mo":["(",")"],"mn":"30"}}]}}}}},"has an asymptotically stable critical point G(\u03c1), which is unique such that the map G is Lipschitz continuous. This assumption is satisfied because of the following reason. For a fixed \u03c1, the mapping H1 (R) (17) is non-expansive with respect to the max norm [39]. Borkar and Soumyanath [40] show that for non-expansive mappings that does not need a contraction property, the above ODE converges to an asymptotically stable critical point R. The Lipschitz continuity of R can be proved by the fact that the components of the R vector (8) are Lipschitz continuous in \u03c1 [41].","8) Assumption 8: The ODE:",{"@attributes":{"id":"p-0084","num":"0088"},"maths":{"@attributes":{"id":"MATH-US-00014","num":"00014"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mfrac":{"mrow":[{"mo":"\u2146","msub":{"mi":["\u03c1","t"]}},{"mo":"\u2146","mi":"\u03c4"}]},"mo":"=","mrow":{"mi":"g","mo":"\u2061","mrow":{"mo":["(",")"],"msub":{"mi":["\u03c1","t"]}}}}},{"mrow":{"mo":["(",")"],"mn":"31"}}]}}}}},"has a global asymptotically stable critical point \u03c1*, which is unique. This is due to the fact that as the R-values stabilize, the policy becomes stationary. For a given stationary policy, the average reward is a finite constant and is also Lipschitz continuos [41]. Thus, the solution to the above ODE converges to the average reward, which is the global asymptotically stable critical point \u03c1*.","In the case of the WRL-RbR controller, the long run average reward value \u03c1* converges to 0. This can be verified from the definition of the gain in (7) and the fact that r({dot over ( )})=E=f\u2212\u0177. This implies that the expected value of r({dot over ( )})=0, since by definition, they are process deviations from target. The above convergence result of \u03c1*=0 and Equation (2) together show that E(y) converges to target T.","B. Optimality of the Control Policies","In the context of WRL-RbR controller, it is necessary to show that the control policy to which the algorithm converges is indeed optimal. To do this it is sufficient to show that the Rvalues converge to their optimal values. This is accomplished in two stages. First, for the MDP case, it is shown that the Bellman's transformation for value iteration and the relative value iteration (RVI) lead to the same policy. Since the value iteration has been demonstrated to yield optimal policies, it is concluded that the policies of the RVI are also optimal.","It is argued in [391 the approximations (23) and (26) converge to optimal values. Since this discussion on optimality is general and independent of the problem context, it is not reproduced here. The R-values obtained from (14) is the same as that obtained from (23). Thus, the WRL-RbR controller is optimal.","VII. Performance Analysis","The performance of WRL-RbR controller was tested on both SISO and MIMO processes. Processes with varying degrees of autocorrelation were studied as numerical examples. The results obtained from the WRL-RbR based strategies were compared with the EWMA based strategies.","A. WRL-RbR Controller Performance for a SISO Process","We consider an autocorrelated process as given in [8].\n\n,\u2003\u2003(32)\n\nwhere N=\u03c9N+\u03b5\u2212c\u03b5is the ARMA(1,1) process for the error, and \u03b5is white noise with U(\u22121, 1) distribution. The autocorrelation parameters are \u03c6 for the process output, and c and \u03c9 for the noise. The initial process parameter values used are as follows: \u03b3=2.0, \u03b7=2.0, u=5.0, \u03c9=1.0, c=0.7. This means that Nfollows an IMA(1,1) process (i.e. an ARMA(1,1) process with \u03c9=1.0). The output autocorrelation parameter \u03c6 was varied between 0.1 and 0.96. The smoothing constant for the EWMA equation (\u03bb) was fixed at 0.1. This value of (\u03bb) is the same as those used in [8] and [1]. The process target value was fixed at T=10. The above process with its parameters was simulated using MATLAB for 200 runs and 50 replications.\n","For the wavelet analysis, we chose Daubechies [23] 4order wavelet because of its well known stability properties [36]. Also, we chose a dyadic window length of sixteen, which allows up to four levels of decomposition. The number of levels was fixed based on the application at hand and the speed of execution of the online algorithm. The learning parameters \u03b1and \u03b2and were initialized at 0.01 and 0.001, respectively. The exploration parameter was initialized at 0.5. The constant K in the decay equations for the learning parameters was maintained at 5\u00d710and for the exploration parameter was kept at 1\u00d710. The error state space had 4001 states, each having a range of 0.1, starting at \u2212200 until 200. The action space consisted of values from \u22125 to 15 in steps of 0.1. This resulted in 201 possible actions for each state.","The process was first simulated as is with no additional changes to either its mean or its variance. The R-values were learnt for all state and action combinations. Once learning was completed offline, the learnt phase was implemented online. The WRL-RbR and EWMA controllers were applied to assess their abilities in bringing the process from start to a stable operating condition. The mean square deviation (MSD) from target of the process under both control strategies were obtained for the first 200 runs.",{"@attributes":{"id":"p-0096","num":"0100"},"figref":["FIGS. 4 and 5","FIG. 4","FIG. 5"]},"A comparison of the mean square deviation (MSD) from target is presented in Table I. The MSD is calculated as follows.",{"@attributes":{"id":"p-0098","num":"0102"},"maths":{"@attributes":{"id":"MATH-US-00015","num":"00015"},"math":{"@attributes":{"overflow":"scroll"},"mtable":{"mtr":{"mtd":[{"mrow":{"mrow":[{"mi":"MSD","mo":"=","mfrac":{"mrow":{"mo":"\u2211","msup":{"mrow":{"mo":["(",")"],"mrow":{"msub":{"mi":["y","t"]},"mo":"-","mi":"T"}},"mn":"2"}},"mi":"n"}},{"mi":"t","mo":"=","mn":"0"},{"mi":["\u2026","n"],"mo":["\u2062","\u2062"],"mstyle":{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}}],"mo":[",",",",",",",",",",","],"mn":["1","2","3"]}},{"mrow":{"mo":["(",")"],"mn":"33"}}]}}}},"br":{}},{"@attributes":{"id":"p-0099","num":"0103"},"tables":{"@attributes":{"id":"TABLE-US-00001","num":"00001"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE I"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"MEAN SQUARE DEVIATION FORM TARGET (SISO PROCESS)"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"77pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Autocorrelation","EWMA","WRL-RbR","% Decrease in MSD"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"char","char":"."}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"77pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"0.1","0.63","0.42","33"]},{"entry":[{},"0.9","33.2","1.8","95"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},"B. WRL-RbR Controller Performance for a MIMO Process","The sample MIMO process adopted for study in this section is a CMP process, which is an essential step in semiconductor wafer fabrication [10], [11]. Wafer polishing that is accomplished using CMP is a nanoscale manufacturing process. The CMP task has been made more challenging in recent years due to the complex wafer topographies, and the introduction of copper (instead of aluminum) and low-k dielectrics.  shows the schematic of a CMP setup, which synergistically combines both tribological (abrasion) and chemical (etching) effects to achieve planarization.","1) CMP Modeling: As with any manufacturing operation, the CMP process falls victim to many known and unknown disturbances that affect its controlled operation. Variations among incoming wafers, process temperatures, polishing byproducts on the pad, mechanical tolerances caused by wear, and polishing consumables (slurry and pads) contribute to disturbances in the polishing process. Virtually all CMP processes, therefore, update polishing tool recipes either automatically or manually to compensate for such disturbances.","The CMP process used is a linear model consisting of two-output and four-input CMP process. The two outputs are the material removal rate (Y) and, within-wafer non-uniformity (Y). The four controllable inputs are: plate speed (U), back pressure (U), polishing downforce (U), and the profile of the conditioning system (U). The process equations are:\n\n=1563.5+159.3()\u221238.2()+178.9()+24.9()+\u03b5,\u2003\u2003(34)\n\n=254+32.6()+113.2()+32.6()+37.1()+\u03b5,\u2003\u2003(35)\n\n",{"@attributes":{"id":"p-0104","num":"0110"},"maths":{"@attributes":{"id":"MATH-US-00016","num":"00016"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":{"mi":"T","mo":"=","mrow":{"mrow":[{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mn":"2000"}},{"mtd":{"mn":"100"}}]}},{"mi":["target","values","for","the","responses","Y"],"mo":["\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062","\u2062"],"mstyle":[{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}},{"mspace":{"@attributes":{"width":"0.8em","height":"0.8ex"}}}]}],"mo":"="}},"mo":","}}},"ul":{"@attributes":{"id":"ul0007","list-style":"none"},"li":{"@attributes":{"id":"ul0007-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0008","list-style":"none"},"li":"\u2003estimated gain"}}}},{"@attributes":{"id":"p-0105","num":"0112"},"maths":{"@attributes":{"id":"MATH-US-00017","num":"00017"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"mrow":[{"mi":"B","mo":"=","mrow":{"mo":["(",")"],"mtable":{"mtr":[{"mtd":[{"mn":"150"},{"mrow":{"mo":"-","mn":"40"}},{"mn":"180"},{"mn":"25"}]},{"mtd":[{"mn":"30"},{"mn":"100"},{"mn":"30"},{"mn":"35"}]}]}}},{"mi":"\u03bc","mo":"=","mn":"0.001"}],"mo":[",",","]}}},"ul":{"@attributes":{"id":"ul0009","list-style":"none"},"li":{"@attributes":{"id":"ul0009-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0010","list-style":"none"},"li":"\u2003and forecast error values"}}}},{"@attributes":{"id":"p-0106","num":"0114"},"maths":{"@attributes":{"id":"MATH-US-00018","num":"00018"},"math":{"@attributes":{"overflow":"scroll"},"mrow":{"msub":{"mi":"A","mn":"0"},"mo":"=","mrow":{"mrow":{"mo":["(",")"],"mtable":{"mtr":[{"mtd":{"mn":"1600"}},{"mtd":{"mn":"250"}}]}},"mo":"."}}}}},"The above process was simulated using MATLAB for 100 runs and 50 replications. The error in both Yand Ywere discretized into 21 states, each having a range of 10.0, starting at \u2212100 until 100. Hence, the state space had (21) 441 states. The action space consisted of values from 0.5 to 1.5 in steps of 0.1 for U, \u22120.5 to \u22121.5 in steps of 0.1 for U, 0.85 to 1.85 in steps of 0.1 for Uand \u22120.55 to \u22120.05 in steps of 0.05 for U. This resulted in (11) 14641 possible actions for each state. Performance of both EWMA and RL strategies were compared. Similar to the SISO case, we chose the Daubechies fourth order wavelet and, the decomposition level up to four levels for the WRL-RbR strategies. Mean square deviation and standard deviation performances are shown in Table II and Table III for both types of controllers.  show the output plots for Yand Yfor both EWMA and WRL-RbR strategies. Clearly, performance of WRL-RbR is far superior to that of EWMA controller.",{"@attributes":{"id":"p-0108","num":"0116"},"tables":{"@attributes":{"id":"TABLE-US-00002","num":"00002"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE II"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"MEAN SQUARE DEVIATION FORM TARGET"},{"entry":"(MIMO PROCESS) (\u00d710)"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"91pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Output","EWMA","WRL-RbR","% Decrease in MSD"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"49pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"91pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Y","0.52","0.22","58"]},{"entry":[{},"Y","0.12","0.045","63"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},{"@attributes":{"id":"p-0109","num":"0117"},"tables":{"@attributes":{"id":"TABLE-US-00003","num":"00003"},"table":{"@attributes":{"frame":"none","colsep":"0","rowsep":"0"},"tgroup":[{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"1"},"colspec":{"@attributes":{"colname":"1","colwidth":"217pt","align":"center"}},"thead":{"row":{"entry":"TABLE III"}},"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":{"@attributes":{"namest":"1","nameend":"1","align":"center","rowsep":"1"}}},{"entry":"STANDARD DEVIATIONS (MIMO PROCESS)"}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"center"}},{"@attributes":{"colname":"4","colwidth":"98pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Output","EWMA","WRL-RbR","% Decrease in Std. Dev"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}},{"@attributes":{"align":"left","colsep":"0","rowsep":"0","cols":"5"},"colspec":[{"@attributes":{"colname":"offset","colwidth":"14pt","align":"left"}},{"@attributes":{"colname":"1","colwidth":"28pt","align":"center"}},{"@attributes":{"colname":"2","colwidth":"42pt","align":"center"}},{"@attributes":{"colname":"3","colwidth":"35pt","align":"char","char":"."}},{"@attributes":{"colname":"4","colwidth":"98pt","align":"center"}}],"tbody":{"@attributes":{"valign":"top"},"row":[{"entry":[{},"Y","71.23","35.2","51"]},{"entry":[{},"Y","34.89","16.91","52"]},{"entry":[{},{"@attributes":{"namest":"offset","nameend":"4","align":"center","rowsep":"1"}}]}]}}]}}},"VIII. Learning Based Controller in a Model Free Environment","A strategy is presented for extending the WRL-RbR controller to work in a model free environment, which is critical for systems requiring distributed sensing. Most real world systems are complex and large, and they seldom have models that accurately describe the relationship between output parameters and the controllable inputs. A conceptual framework of a model free RbR control is given in . The control laws are learnt through simulation and are continuously improved during real time implementation. The unique advantage of model free approaches is the ability to factor into the study many other parameters some of which could be nonstationary, for which it is very difficult to develop a mathematical model. The application of model free WRL-RbR control in a CMP process which could serve as a test bed for a distributed sensing application is provided.","A. Design of a Controller for Distributed Sensing","The CMP process is influenced by various factors such as plate speed, back pressure, polishing downforce, the profile of the conditioning system, slurry properties (abrasive concentration, and size), incoming wafer thickness, pattern density of circuits, and dynamic wear of the polishing pad. Several outputs that are monitored are the material removal rate, within-wafer non-uniformity, between wafer non-uniformity, acoustic emission (AE), coefficient of friction (CoF), and thickness of the wafer. Ideally, one would monitor all the above inputs and outputs via distributed sensors. However, due to lack of accurate process models that link the outputs to the controllable inputs, and also due to speed of execution issues, simple linear models are often used. A model free learning approach would make it viable to control the CMP process using the above parameters. The wavelet analysis also provides a means of using nonstationary signals like the AE and CoF in control. This is due to the fact that wavelet analysis produces detail coefficients that are stationary surrogates of nonstationary signals. Also the pattern recognition feature of wavelet can be used to obtain information on trend\/shift\/variance of the process, which can be used by the RL controller to provide accurate compensation. Our research in WRL-RbR controllers for a large scale distributed environment is on going and results have shown unprecedented potential to extend this technology to other distributed systems. The results presented serve as a proof of concept for the new breed of learning based WRL-RbR strategy.","IX. Conclusions","RbR controllers have been applied to processes where online parameter estimation and control are necessary due to the short and repetitive nature of those processes. Presented is a novel control strategy, which has high potential in controlling many process applications. The control problem is cast in the framework of probabilistic dynamic decision making problems for which the solution strategy is built on the mathematical foundations of multiresolution analysis, dynamic programming, and machine learning. The strategy was tested on problems that were studied before using the EWMA strategy for autocorrelated SISO and MIMO systems, and the results obtained were compared with them. It is observed that RL based strategy outperforms the EWMA based strategies by providing better convergence and stability in terms of lower error variances, and lower initial bias for a wide range of autocorrelation values. The wavelet filtering of the process output enhances the quality of the data through denoising and results in extraction of the significant features of the data on which the controllers take action. Further research is underway in developing other WRL-RbR control strategies, which incorporates wavelet based analysis to detect drifts and sudden shifts in the process, and scale up the controller for large scale distributed sensing environments with hierarchical structures.","[1] A. Ingolfsson and E. Sachs, \u201cStability and sensitivity of an ewma controller,\u201d , vol. 25, no. 4, pp. 271-287, 1993.","[2] B. Del Castillo and J. Yeh, \u201cAn adaptive optimizing quality controller for linear and nonlinear semiconductor processes,\u201d , vol. 11, no. 2, pp. 285-295, 1998.","[3] W. J. Campbell, \u201cModel predictive run-to-run control of chemical mechanical planarization\u201d Ph.D. dissertation, University of Texas at Austin, 1999.","[4] Z. Ning, J. R. Moyne, T. Smith, D. Boning, B. D. Castillo, J. Y. Yeh, and A. Hurwitz, \u201cA comparative analysis of run-to-run control algorithms in the semiconductor manufacturing industry.\u201d in . IEEE\/SEMI, 1996, pp. 375-381.","[5] K. Chamness, G. Cherry, R. Good, and S. J. Qin, \u201cComparison of r2r control algorithms for the cmp with measurement delays.\u201d in ., Banff, Canada, 2001.","[6] E. Sachs, A. Hu, and A. Ingolfsson, \u201cRun by run process control: Combining spc and feedback control,\u201d ., vol. 8, pp. 26-43, 1995.","[7] S. W. Butler and J. A. Stefani, \u201cSupervisory run-to-run control of a polysilicon gate etch using in situ ellipsometry,\u201d ., vol. 7, pp. 193-201, 1994.","[8] E. Del Castillo and A. M. Hurwitz, \u201cRun-to-run process control: Literature review and extensions,\u201d , vol. 29, no. 2, pp. 184-196, 1997.","[9] T. H. Smith and D. S. Boning, \u201cArtificial neural network exponentially weighted moving average control for semiconductor processes,\u201d , vol. 15, no. 3, pp. 1377-1384, 1997.","[10] E. Del Castillo and R. Rajagopal, \u201cA multivariate double ewma process adjustment scheme for drifting processes,\u201d , vol. 34, no. 12, pp. 1055-1068, 2002.","[11] R. Rajagopal and B. Del Castillo, \u201cAn analysis and mimo extension of a double ewma run-to-run controller for non-squared systems:\u2019 , vol. 10, no. 4, pp. 417-428, 2003.","[12] S. K. S. Fan, B. C. Jiang, C. H. Jen, and C. C. Wang, \u201cSISO run-to-run feedback controller using triple EWMA smoothing foe semiconductor manufacturing processes,\u201d ., vol. 40, no. 13, pp. 3093-3120, 2002.","[13] S. T. Tseng, A. B. Yeh, F. Tsung, and Y. Y. Chan, \u201cA study of variable EWMA controller,\u201d , vol. 16, no. 4, pp. 633-643, 2003.","[14] N. S. Patel and S. T. Jenkins, \u201cAdaptive optimization of run-by-run controllers,\u201d , vol. 13, no. 1, pp. 97-107, 2000.","[15] C. T. Su and C. C. Hsu, \u201cOn-line tuning of a single ewma controller based on the neural technique,\u201d ., vol. 42, no. I 1, pp. 2163-2178, 2004.","[16] D. Shi and F. Tsung, \u201cModeling and diagnosis of feedback-controlled process using dynamic PCA and neural networks,\u201d ., vol. 41, no. 2, pp. 365-379, 2003.","[17] E. Del Castillo, \u201cLong run transient analysis of a double EWMA feedback controller,\u201d , vol. 31, pp. 1157-1169, 1999.","[18] R. Ganesan, T. K. Das, and V. Venkataraman, \u201cWavelet based multiscale statistical process monitoring\u2014A literature review,\u201d , vol. 36, no. 9, pp. 787-806, 2004.","[19] A. Terchi and Y. H. J. Au, \u201cAcoustic emission signal processing:\u2019 , vol. 34, pp. 240-244, 2001.","[20] I. N. Tansel, C. Mekdesi, O. Rodriguez, and B. Uragun, \u201cMonitoring microdrilling operations with wavelets,\u201d , pp. 151-163, 1992.","[21] X. Li, \u201cA brief review: Acoustic emission method for tool wear monitoring during turning,\u201d , vol. 42, pp. 157-165, 2002.","[22] G. Strang and T. Nguyen, . Wellesley MA: Wellesley Cambridge Press, 1996.","[23] I. Daubechies, . Philadelphia: SIAM, 1992.","[24] D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard, \u201cWavelet shrinkage: Asymptopia? (with discussion),\u201d , vol. 57, no. 2, pp. 301-369, 1995.","[25] F. Abramovich and Y. Benjamini, \u201cThresholding of wavelet coefficients as multiple hypothesis testing procedure:\u2019 in , ser. Lecture Notes in Statistics, A. Antoniadis and G. Oppenheim, Eds. New York: Springer-Verlag, 1995, vol. 103, pp. 5-14.","[26] M. Neumann and R. V. Sachs, \u201cWavelet thresholding: Beyond the Gaussian iid situation,\u201d in , ser. Lecture Notes in Statistics, A. Antoniadis and G. Oppenheim, Eds. New York: Springer-Verlag, 1995, vol. 103, pp. 301-329.","[27] A. J. Toprac, H. Luna, B. Withers, M. Bedrin, and S. Toy, \u201cDeveloping and implementing an advanced cmp run- to-run controller,\u201d 2003, available URL: http:\/\/www.micromagazine.comlarchive\/03\/08\/toprac.html.","[28] R. Bellman, \u201cThe theory of dynamic programming,\u201d ., vol. 60, pp. 503-516, 1954.","[29] H. Robbins and S. Monro, \u201cA stochastic approximation method,\u201d ., vol. 22, pp. 400-407, 1951.","[30] M. L. Puterman, in . Wiley Interscience, New York, 1994.","[31] J. Abounadi, \u201cStochastic approximation for non-expansive maps: Application to q-learning algorithms,\u201d Ph.D. dissertation, MIT, MA, February 1998.","[32] A. Gosavi, \u201cAn algorithm for solving semi-markov decision problem using reinforcement learning: Convergence analysis and numerical results,\u201d Ph.D. dissertation, 1998, IMSE Dept., University of South Florida, Tampa, Fla.","[33] J. Bremer, R. Coifinan, M. Maggioni, and A. Szlam, \u201cDiffusion wavelet packets,\u201d Yale University, Tech. Rep. YALE\/DCS\/TR-1 304, 2004, to appear in AppI. Comp. Harm. Anal.","[34] S. Mahadevan and M. Maggioni, \u201cValue function approximation using diffusion wavelets and laplacian eigenfunctions,\u201d University of Massachusetts, Department of Computer Science, Technical Report TR 2005-38 2005.","[35] R. Coifinan and M. Maggioni, \u201cDiffusion wavelets,\u201d Yale University, Tech. Rep. YALEIDCS\/TR-1303, 2004, to appear in Appi. Comp. Harm. Anal.","[36] R. Ganesan, T. K. Das, A. K. Sikder, and A. Kumar, \u201cWavelet based identification of delamination of low-k dielectric layers in a copper damascene CMP process,\u201d , vol. 16, no. 4, pp. 677-685, 2003.","[37] B. R. Bakshi, \u201cMultiscale statistical process control and model-based denoising,\u201d in , ser. Data Handling in Science and Technology, B. Walczak, Ed. P.O. Box 211, 1000 AE Amsterdam, Netherlands: Elsevier, 2000, vol. 22, ch. 17, pp. 411-436.","[38] V. S. Borkar, \u201cStochastic approximation with two-time scales,\u201d , vol. 29, pp. 291-294, 1997.","[39] A. Gosavi, \u201cEuropean journal of operational research,\u201d -, vol. 155, pp. 654-674, 2004.","[40] V. S. Borkar and K. Soumyanath, \u201cAn analog scheme for fixed point computation, part I: Theory,\u201d , vol. 44, pp. 351-354, 1997.","[41] D. Bertsekas and J. Tsitsiklis, in -. Athena Scientific, Belmont, Mass., 1995.","The disclosure of all publications cited above are expressly incorporated herein by reference, each in its entirety, to the same extent as if each were incorporated by reference individually.","It will be seen that the advantages set forth above, and those made apparent from the foregoing description, are efficiently attained and since certain changes may be made in the above construction without departing from the scope of the invention, it is intended that all matters contained in the foregoing description or shown in the accompanying drawings shall be interpreted as illustrative and not in a limiting sense.","It is also to be understood that the following claims are intended to cover all of the generic and specific features of the invention herein described, and all statements of the scope of the invention which, as a matter of language, might be said to fall therebetween. Now that the invention has been described,"],"GOVINT":[{},{}],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["For a fuller understanding of the invention, reference should be made to the following detailed description, taken in connection with the accompanying drawings, in which:",{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 4"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":"FIG. 5"},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":"FIG. 7","sub":"1 "},{"@attributes":{"id":"p-0020","num":"0019"},"figref":"FIG. 8","sub":"2 "},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 9"}]},"DETDESC":[{},{}]}
