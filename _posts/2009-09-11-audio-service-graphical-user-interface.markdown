---
title: Audio service graphical user interface
abstract: Techniques for providing a graphical user interface for an audio channel include receiving data that indicates multiple audio contents for audio presentation at a first node. A graphical user interface is formed, which displays a time sequence of the plurality of audio contents for presentation, and which associates with each audio content one or more selectable operations on the audio content. Presentation of the graphical user interface is caused at a second node. In response to causing presentation of the graphical user interface, data is received that indicates an associated operation on audio content based on input from the second node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09262120&OS=09262120&RS=09262120
owner: Nokia Technologies Oy
number: 09262120
owner_city: Espoo
owner_country: FI
publication_date: 20090911
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["BACKGROUND","SOME EXAMPLE EMBODIMENTS","DESCRIPTION OF SOME EMBODIMENTS"],"p":["Network service providers and device manufacturers are continually challenged to deliver value and convenience to consumers by, for example, providing compelling network services and devices for wireless links such as cellular transmissions. Most services involve the customer\/user interacting with a device that has a visual display and a pad of multiple software or hardware keys to press, or both. By their nature, these devices require the user's eyes gaze on the device, at least for a short time, and one or more of the user's hands press the appropriate hard or soft keys. This can divert the user from other actions the user may be performing, such as operating equipment, driving, cooking, administering care to one or more persons, among thousands of other daily tasks.","Therefore, there is a need for delivering network services through an audio interface unit with little or no involvement of the user's eyes and hands; and it is desirable both to easily control the audio interface and share the audio interface with other users of a social network","According to one embodiment, a method comprises causing at least in part a receiving of data that indicates one or more audio contents for audio presentation at a first node. A graphical user interface is caused at least in part to be formed, which displays a time sequence of the audio contents for presentation at the first node, and which associates with each audio content one or more selectable operations on the audio content. Presentation is initiated of the graphical user interface at a second node. In response to causing the presentation of the graphical user interface, data is received that indicates an associated operation on audio content based on input from the second node.","According to another embodiment, a computer-readable storage medium carrying one or more sequences of one or more instructions which, when executed by one or more processors, cause an apparatus, at least in part, to receive data that indicates one or more audio contents for audio presentation at a first node. The apparatus is further caused, at least in part, to form a graphical user interface. The graphical user interface displays a time sequence of the audio contents for presentation, and associates with each audio content one or more selectable operations on the audio content. The apparatus is further caused, at least in part, to present the graphical user interface at a second node. In response to causing the presentation of the graphical user interface, the apparatus is caused to receive data that indicates an associated operation on audio content based on input from the second node.","According to another embodiment, an apparatus comprises means for receiving data that indicates one or more audio contents for audio presentation at a first node. The apparatus also comprises means for forming a graphical user interface. The graphical user interface displays a time sequence of the audio contents for presentation, and associates with each audio content one or more selectable operations on the audio content. The apparatus also includes means for causing presentation of the graphical user interface at a second node. The apparatus includes means for receiving data that indicates an associated operation on audio content based on input from the second node, in response to causing presentation of the graphical user interface.","According to another embodiment, a method comprises facilitating access to, including granting access rights for, a user interface configured to receive data that indicates one or more audio contents for audio presentation at a first node. The method further comprises facilitating access to, including granting access rights for, a graphical user interface that displays a time sequence of the audio contents for presentation, and associates with each audio content one or more selectable operations on the audio content.","According to another embodiment, an apparatus includes at least one processor and at least one memory including computer instructions. The at least one memory and computer instructions are configured to, with the at least one processor, cause the apparatus at least to cause receiving of data that indicates one or more audio contents for audio presentation at a first node and cause forming of a graphical user interface. The graphical user interface displays a time sequence of the audio contents for presentation, and associates with each audio content one or more selectable operations on the audio content. The at least one memory and computer instructions are further configured to, with the at least one processor, cause the apparatus at least to cause presentation of the graphical user interface at a second node; and in response to causing the presentation of the graphical user interface, cause receiving of data that indicates an associated operation on audio content based on input from the second node.","Still other aspects, features, and advantages of the invention are readily apparent from the following detailed description, simply by illustrating a number of particular embodiments and implementations, including the best mode contemplated for carrying out the invention. The invention is also capable of other and different embodiments, and its several details can be modified in various obvious respects, all without departing from the spirit and scope of the invention. Accordingly, the drawings and description are to be regarded as illustrative in nature, and not as restrictive.","A method and apparatus for providing network services through an audio interface unit are disclosed. In the following description, for the purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of some embodiments of the invention. It is apparent, however, to one skilled in the art that the embodiments of the invention may be practiced without these specific details or with an equivalent arrangement. In other instances, well-known structures and devices are shown in block diagram form in order to avoid unnecessarily obscuring the embodiments of the invention.","Although various embodiments are described with respect to an audio interface unit with a full cellular communications engine and no keypad or visual display, it is contemplated that the approach described herein may be used with other wireless receivers and transceivers, including transceivers for Institute of Electrical & Electronics Engineers (IEEE) 802.11 standards for carrying out wireless local area network (WLAN) computer communication in the 2.4, 3.6 and 5 gigaHertz (GHz) frequency bands (1 GHz=10cycles per second, also called Hertz), transceivers for IEEE 802.15 as a standardization of Bluetooth wireless specification for wireless personal area networks (WPAN), and receivers for radio signals, such as amplitude modulated (AM) signals and frequency modulated (FM) signals in various radio frequency bands, including broadcast radio bands, television audio bands, and satellite radio bands and in devices that include a keypad or a visual display or both.",{"@attributes":{"id":"p-0033","num":"0032"},"figref":"FIG. 1","b":"100"},"To address this problem, system  of  introduces the capability for a user  to interact with a network without involving cables or diverting the user's eyes or hands from other tasks. Although user  is depicted for purposes of illustration, user  is not part of system . The system  allows the user  to wear an unobtrusive audio interface unit  and interact with one or more network services (e.g., social network service ) through one or more wireless links (e.g., wireless link , and wirelesses link , collectively referenced hereinafter as wireless links ), by listening to audio as output of the system and speaking as input to the system. Listening and speaking to receive and give information is not only natural and easy, but also is usually performed hands free and eyes free. Thus, the user can enjoy one or more network services while still productively and safely performing other daily tasks. Because the connection to the network is wireless, the user is unconstrained by cables while performing these other tasks. In embodiments in which the audio interface unit is simple, it can be manufactured inexpensively and can be made to be unobtrusive. An unobtrusive audio interface unit can be worn constantly by a user (e.g., tucked in clothing), so that the user  is continually available via the audio interface unit . This enables the easy and rapid delivery of a wide array of network services, as described in more detail below.","As shown in , the system  comprises an audio interface unit  and user equipment (UE) , both having connectivity to a personal audio host  and thence to a network service, such as social network service , via a communication network . By way of example, the communication network  of system  includes one or more networks such as a data network (not shown), a wireless network (not shown), a telephony network (not shown), or any combination thereof. It is contemplated that the data network may be any local area network (LAN), metropolitan area network (MAN), wide area network (WAN), a public data network (e.g., the Internet), or any other suitable packet-switched network, such as a commercially owned, proprietary packet-switched network, e.g., a proprietary cable or fiber-optic network. In addition, the wireless network may be, for example, a cellular network and may employ various technologies including enhanced data rates for global evolution (EDGE), general packet radio service (GPRS), global system for mobile communications (GSM), Internet protocol multimedia subsystem (IMS), universal mobile telecommunications system (UMTS), etc., as well as any other suitable wireless medium, e.g., microwave access (WiMAX), Long Term Evolution (LTE) networks, code division multiple access (CDMA), wideband code division multiple access (WCDMA), wireless fidelity (WiFi), satellite, mobile ad-hoc network (MANET), and the like. A network node (or simply \u201cnode\u201d) is any device that communicates with any other device in network .","The UE  is any type of mobile terminal, fixed terminal, or portable terminal including a mobile handset, station, unit, device, multimedia tablet, Internet node, communicator, desktop computer, laptop computer, Personal Digital Assistants (PDAs), or any combination thereof. It is also contemplated that the UE  can support any type of interface to the user (such as \u201cwearable\u201d circuitry, etc.).","The audio interface unit  is a much trimmed down piece of user equipment with primarily audio input from, and audio output to, user . Example components of the audio interface unit  are described in more detail below with reference to . It is also contemplated that the audio interface unit  comprises \u201cwearable\u201d circuitry. In the illustrated embodiments, a portable audio source\/output , such as a portable Moving Picture Experts Group Audio Layer 3 (MP3) player, as a local audio source is connected by audio cable  to the audio interface unit . In some embodiments, the audio source\/output  is an audio output device, such as a set of one or more speakers in the user's home or car or other facility. In some embodiments, both an auxiliary audio input and auxiliary audio output are connected to audio interface unit  by two or more separate audio cables ","By way of example, the UE  and audio interface unit  communicate with each other and other components of the communication network  using well known, new or still developing protocols. In this context, a protocol includes a set of rules defining how the network nodes within the communication network  interact with each other based on information sent over the communication links. The protocols are effective at different layers of operation within each node, from generating and receiving physical signals of various types, to selecting a link for transferring those signals, to the format of information indicated by those signals, to identifying which software application executing on a computer system sends or receives the information. The conceptually different layers of protocols for exchanging information over a network are described in the Open Systems Interconnection (OSI) Reference Model.","Communications between the network nodes are typically effected by exchanging discrete packets of data. Each packet typically comprises (1) header information associated with a particular protocol, and (2) payload information that follows the header information and contains information that may be processed independently of that particular protocol. In some protocols, the packet includes (3) trailer information following the payload and indicating the end of the payload information. The header includes information such as the source of the packet, its destination, the length of the payload, and other properties used by the protocol. Often, the data in the payload for the particular protocol includes a header and payload for a different protocol associated with a different, higher layer of the OSI Reference Model. The header for a particular protocol typically indicates a type for the next protocol contained in its payload. The higher layer protocol is said to be encapsulated in the lower layer protocol. The headers included in a packet traversing multiple heterogeneous networks, such as the Internet, typically include a physical (layer 1) header, a data-link (layer 2) header, an internetwork (layer 3) header and a transport (layer 4) header, and various application headers (layer 5, layer 6 and layer 7) as defined by the OSI Reference Model.","Processes executing on various devices, such as audio interface unit  and personal audio host , often communicate using the client-server model of network communications. The client-server model of computer process interaction is widely known and used. According to the client-server model, a client process sends a message including a request to a server process, and the server process responds by providing a service. The server process may also return a message with a response to the client process. Often the client process and server process execute on different computer devices, called hosts, and communicate via a network using one or more protocols for network communications. The term \u201cserver\u201d is conventionally used to refer to the process that provides the service, or the host on which the process operates. Similarly, the term \u201cclient\u201d is conventionally used to refer to the process that makes the request, or the host on which the process operates. As used herein, the terms \u201cclient\u201d and \u201cserver\u201d refer to the processes, rather than the hosts, unless otherwise clear from the context. In addition, the process performed by a server can be broken up to run as multiple processes on multiple hosts (sometimes called tiers) for reasons that include reliability, scalability, and redundancy, among others. A well known client process available on most nodes connected to a communications network is a World Wide Web client (called a \u201cweb browser,\u201d or simply \u201cbrowser\u201d) that interacts through messages formatted according to the hypertext transfer protocol (HTTP) with any of a large number of servers called World Wide Web (WWW) servers that provide web pages.","In the illustrated embodiment, the UE  includes a browser  for interacting with WWW servers included in the social network service module  on one or more social network server hosts  and other service modules on other hosts. The illustrated embodiment includes a personal audio service module  on personal audio host . The personal audio service module  includes a Web server for interacting with browser  and also an audio server for interacting with a personal audio client  executing on the audio interface unit . The personal audio service  is configured to deliver audio data to the audio interface unit . In some embodiments, at least some of the audio data is based on data provided by other servers on the network, such as social network service . In the illustrated embodiment, the personal audio service  is configured for a particular user  by Web pages delivered to browser , for example to specify a particular audio interface unit  and what services are to be delivered as audio data to that unit. After configuration, user  input is received at personal audio service  from personal audio client  based on spoken words of user , and selected network services content is delivered from the personal audio service  to user  through audio data sent to personal audio client .","Many services are available to the user  of audio interface unit  through the personal audio service  via network , including social network service  on one or more social network server hosts . In the illustrated embodiment, the social network service  has access to database  that includes one or more data structures, such as user profiles data structure  that includes a contact book data structure . Information about each user who subscribes to the social network service  is stored in the user profiles data structure , and the telephone number, cell phone, number, email address or other network addresses, or some combination, of one or more persons whom the user contacts are stored in the contact book data structure .","In some embodiments, the audio interface unit  connects directly to network  via wireless link (e.g., via a cellular telephone engine or a WLAN interface to a network access point). Any telephone technology may be used, including standard definition (SD) and high definition (HD) audio over wired and wireless telephones. In some embodiments, the audio interface unit  connects to network  indirectly, through UE  (e.g., a cell phone or laptop computer) via wireless link (e.g., a WPAN interface to a cell phone or laptop). Network link  may be a wired or wireless link, or some combination. In some embodiments in which audio interface unit relies on wireless link , a personal audio agent process  executes on the UE  to transfer data packets between the audio interface unit  sent by personal audio client  and the personal audio service , and to convert other data received at UE  to audio data for presentation to user  by personal audio client .","Although various hosts and processes and data structures are depicted in  and arranged in a particular way for purposes of illustration, in other embodiments, more or fewer hosts, processes and data structures are involved, or one or more of them, or portions thereof, are arranged in a different way.",{"@attributes":{"id":"p-0045","num":"0044"},"figref":["FIG. 2","FIG. 1","FIG. 9"],"b":["200","200","160","200"]},"In the illustrated embodiment, the audio interface unit  includes circuitry housing , stereo headset cables and (collectively referenced hereinafter as stereo cables ), stereo speakers and configured to be worn in the ear of the user with in-ear detector (collectively referenced hereinafter as stereo earbud speakers ), controller , and audio input cable .","In the illustrated embodiment, the stereo earbuds  include in-ear detectors that can detect whether the earbuds are positioned within an ear of a user. Any in-ear detectors known in the art may be used, including detectors based on motion sensors, heart-pulse sensors, light sensors, or temperature sensors, or some combination, among others. In some embodiments the earbuds do not include in-ear detectors. In some embodiments, one or both earbuds  include a microphone, such as microphone , to pick up spoken sounds from the user. In some embodiments, stereo cables  and earbuds  are replaced by a single cable and earbud for a monaural audio interface.","The controller  includes an activation button  and a volume control element . In some embodiments, the controller  includes a microphone instead of or in addition to the microphone in one or more earbuds  or microphone in circuitry housing . In some embodiments, the controller  is integrated with the circuitry housing .","The activation button  is depressed by the user when the user wants sounds made by the user to be processed by the audio interface unit . Depressing the activation button to speak is effectively the same as turning the microphone on, wherever the microphone is located. In some embodiments, the button is depressed for the entire time the user wants the user's sounds to be processed; and is released when processing of those sounds is to cease. In some embodiments, the activation button  is depressed once to activate the microphone and a second time to turn it off. Some audio feedback is used in some of these embodiments to allow the user to know which action resulted from depressing the activation button .","In some embodiment with an in-ear detector and a microphone in the earbud , the activation button  is omitted and the microphone is activated when the earbud is out and the sound level at the microphone in the earbud is above some threshold that is easily obtained when held to the user's lips while the user is speaking and which rules out background noise in the vicinity of the user.","An advantage of having the user depress the activation button  or take the earbud with microphone out and hold that earbud near the user's mouth is that persons in sight of the user are notified that the user is busy speaking and, thus, is not to be disturbed.","In some embodiments, the user does not need to depress the activation button  or hold an earbud with microphone ; instead the microphone is always active but ignores all sounds until the user speaks a particular word or phrase, such as \u201cMike On,\u201d that indicates the following sounds are to be processed by the unit , and speaks a different word or phrase, such as \u201cMike Off,\u201d that indicates the following sounds are not to be processed by the unit . Some audio feedback is available to determine if the microphone is being processed or not, such as responding to a spoken word or phrase, such as \u201cMike,\u201d with the current state \u201cMike on\u201d or \u201cMike off.\u201d An advantage of the spoken activation of the microphone is that the unit  can be operated completely hands-free so as not to interfere with any other task the user might be performing.","In some embodiments, the activation button doubles as a power-on\/power-off switch, e.g., as indicated by a single depression to turn the unit on when the unit is off and by a quick succession of multiple depressions to turn off a unit that is on. In some embodiments, a separate power-on\/power-off button (not shown) is included, e.g., on circuitry housing .","The volume control  is a toggle button or wheel used to increase or decrease the volume of sound in the earbuds . Any volume control known in the art may be used. In some embodiments the volume is controlled by the spoken word, while the sounds from the microphone are being processed, such as \u201cVolume up\u201d and \u201cVolume down\u201d and the volume control  is omitted. However, since volume of earbud speakers is changed infrequently, using a volume control  on occasion usually does not interfere with hands-free operation while performing another task.","The circuitry housing  includes wireless transceiver , a radio receiver , a text-audio processor , an audio mixer module , and an on-board media player . In some embodiments, the circuitry housing  includes a microphone ","The wireless transceiver  is any combined electromagnetic (em) wave transmitter and receiver known in the art that can be used to communicate with a network, such as network . An example transceiver includes multiple components of the mobile terminal depicted in  and described in more detail below with reference to that figure. In some embodiments, the audio interface unit  is passive when in wireless mode, and only a wireless receiver is included.","In some embodiments, wireless transceiver  is a full cellular engine as used to communicate with cellular base stations miles away. In some embodiments, wireless transceiver  is a WLAN interface for communicating with a network access point (e.g., \u201chot spot\u201d) hundreds of feet away. In some embodiments, wireless transceiver  is a WPAN interface for communicating with a network device, such as a SD or HD audio cell phone or laptop computer, with a relatively short distance (e.g., a few feet away). In some embodiments, the wireless transceiver  includes multiple transceivers, such as several of those transceivers described above.","In the illustrated embodiment, the audio interface unit includes several components for providing audio content to be played in earbuds , including radio receiver , on-board media player , and audio input cable . The radio receiver  provides audio content from broadcast radio or television or police band or other bands, alone or in some combination. On-board media player , such as a player for data formatted according to Moving Picture Experts Group Audio Layer 3 (MP3), provides audio from data files stored in memory (such as memory  on chipset  described below with reference to ). These data files may be acquired from a remote source through a WPAN or WLAN or cellular interface in wireless transceiver . Audio input cable  includes audio jack  that can be connected to a local audio source, such as a separate local MP3 player. In such embodiments, the audio interface unit  is essentially a multi-functional headset for listening to the local audio source along with other functions. In some embodiments, the audio input cable  is omitted. In some embodiments, the circuitry housing  includes a female jack  into which is plugged a separate audio output device, such as a set of one or more speakers in the user's home or car or other facility.","In the illustrated embodiment, the circuitry housing  includes a text-audio processor  for converting text to audio (speech) or audio to text or both. Thus content delivered as text, such as via wireless transceiver , can be converted to audio for playing through earbuds . Similarly, the user's spoken words received from one or more microphones , , (collectively referenced hereinafter as microphones ) can be converted to text for transmission through wireless transceiver  to a network service. In some embodiments, the text-audio processor  is omitted and text-audio conversion is performed at a remote device and only audio data is exchanged through wireless transceiver . In some embodiments, the text-audio processor  is simplified for converting only a few key commands from speech to text or text to speech or both. By using a limited set of key commands of distinctly different sounds, a simple text-audio processor  can perform quickly with few errors and little power consumption.","In the illustrated embodiment, the circuitry housing  includes an audio mixer module , implemented in hardware or software, for directing audio from one or more sources to one or more earbuds . For example, in some embodiments, left and right stereo content are delivered to different earbuds when both are determined to be in the user's ears. However, if only one earbud is in an ear of the user, both left and right stereo content are delivered to the one earbud that is in the user's ear. Similarly, in some embodiments, when audio data is received through wireless transceiver  while local content is being played, the audio mixer module  causes the local content to be interrupted and the audio data from the wireless transceiver to be played instead. In some embodiments, if both earbuds are in place in the user's ears, the local content is mixed into one earbud and the audio data from the wireless transceiver  is output to the other earbud. In some embodiments, the selection to interrupt or mix the audio sources is based on spoken words of the user or preferences set when the audio interface unit is configured, as described in more detail below.",{"@attributes":{"id":"p-0061","num":"0060"},"figref":["FIG. 3","FIG. 3"],"b":["160","350","350","360","370","380","390"]},"For purposes of illustration, it is assumed that the microphone is activated by depressing the activation button  while the unit is to process the incoming sounds; and the activation button is released when sounds picked up by the microphone are not to be processed. It is further assumed for purposes of illustration that both earbuds are in place in the corresponding ears of the user. It is further assumed for purposes of illustration that the user had previously subscribed, using browser  on UE  to interact with the personal audio service , for telephone call forwarding to the audio interface unit  and internet newsfeed to the unit .","At the beginning of the interval, the microphone is activated as indicated by the button signal portion , and the user speaks a command picked up as microphone signal portion  that indicates to play an audio source, e.g., \u201cplay FM radio,\u201d or \u201cplay local source,\u201d or \u201cplay stored track X\u201d (where X is a number or name identifier for the local audio file of interest), or \u201cplay internet newsfeed.\u201d For purposes of illustration, it is assumed that the user has asked to play a stereo source, such as stored track X.","In response to the spoken command in microphone signal , the audio interface unit  outputs the stereo source to the two earbuds as left earbud signal  and right earbud signal  that cause left and right earbuds to play left source and right source respectively.","When a SD or HD audio telephone call is received (e.g., is forwarded from a cell phone or land line to the personal audio service ) for the user, an alert sound is issued at the audio interface unit , e.g., as left earbud signal portion  indicating a telephone call alert. For example, in various embodiments, the personal audio service  receives the call and encodes an alert sound in one or more data packets and sends the data packets to personal audio client  through wireless link or indirectly through personal audio agent  over wireless link . The client  causes the alert to be mixed in to the left or right earbud signals, or both. In some embodiments, personal audio service  just sends data indicating an incoming call; and the personal audio client  causes the audio interface unit  to generate the alert sound internally as call alert signal portion . In some embodiments, the stereo source is interrupted by the audio mixer module  so that the alert signal portion  can be easily noticed by the user. In the illustrated embodiment, the audio mixer module  is configured to mix the left and right source and continue to present them in the right earbud as right earbud signal portion , while the call alert signal in left earbud signal portion  is presented alone to the left earbud. This way, the user's enjoyment of the stereo source is less interrupted, in case the user prefers the source to the telephone call.","The call alert left ear signal portion  initiates an alert context time window of opportunity indicated by time interval  in which microphone signals (or activation button signals) are interpreted in the context of the call alert. Only sounds that are associated with actions appropriate for responding to a call alert are tested for by the audio-text processor  or the remote personal audio service , such as \u201canswer,\u201d \u201cignore,\u201d \u201cidentify.\u201d Having this limited context-sensitive vocabulary greatly simplifies the processing, thus reducing computational resource demands on the audio interface unit  or remote host , or both, and reducing error rates. In some embodiments, the activation button signal can be used, without the microphone signal, to represent one of the responses, indicated for example by the number or duration of depressions of the button, or by timing a depression during or shortly after a prompt is presented as voice in the earbuds). In some of these embodiments, no speech input is required to use the audio interface unit.","In the illustrated embodiment, the user responds by activating the microphone as indicated by activation button signal portion  and speaks a command to ignore the call, represented as microphone signal portion  indicating an ignore command. As a result, the call is not put through to the audio interface unit . It is assumed for purposes of illustration that the caller leaves a message with the user's voice mail system. Also as a result of the ignore command, the response to the call alert is concluded and the left and right sources for the stereo source are returned to the corresponding earbuds, as left earbud signal portion  and right earbud signal portion , respectively.","At a later time, the user decides to listen to the user's voicemail. The user activates the microphone as indicated by activation button signal portion  and speaks a command to play voicemail, represented as microphone signal portion  indicating a play voicemail command. As a result, audio data representing the user's voicemail is forwarded to the audio interface unit. In some embodiments, the text-audio processor  interprets the microphone signal portion  as the play voicemail command and sends a message to the personal audio service  to provide the voicemail data. In other embodiments, the microphone signal portion  is simply encoded as data, placed in one or more data packets, and forwarded to the personal audio service  that does the interpretation.","In either case, audio data is received from the voicemail system through the personal audio service  at the personal audio client  as data packets of encoded audio data, as a result of the microphone signal portion  indicating the play voicemail command spoken by the user. The audio mixer module  causes the audio represented by the audio data to be presented in one or more earbuds. In the illustrated embodiment, the voicemail audio signal is presented as left earbud signal portion  indicating the voicemail audio and the right earbud signal is interrupted. In some embodiments, the stereo source is paused (i.e., time shifted) until the voicemail audio is completed. In some embodiments, the stereo source that would have been played in this interval is simply lost.","When the voicemail signal is complete, the audio mixer module  restarts the left and right sources of the stereo source as left earbud signal portion  and right earbud signal portion , respectively.","Thus, as depicted in , a variety of network services, such as media playing, internet newsfeeds, SD or HD audio telephone calls and voicemail are delivered to a user through the unobtrusive, frequently worn, audio interface unit . In other embodiments, other alerts and audio sources are involved. Other audio sources include internet newsfeeds (including sports or entertainment news), web content (often converted from text to speech), streaming audio, broadcast radio, and custom audio channels designed by one or more users, among others. Other alerts include breaking news alerts, text and voice message arrival, social network status change, and user-set alarms and appointment reminders, among others.","In some embodiments, the audio interface unit includes a data communications bus, such as bus  of chipset  as depicted in , and a processor, such as processor  in chipset , or other logic encoded in tangible media as described with reference to . The tangible media is configured either in hardware or with software instructions in memory, such as memory  on chipset , to determine, based on spoken sounds of a user of the apparatus received at a microphone in communication with the tangible media through the data communications bus, whether to present audio data received from a different apparatus. The processor is also configured to initiate presentation of the received audio data at a speaker in communication with the tangible media through the data communications bus, if it is determined to present the received audio data.",{"@attributes":{"id":"p-0073","num":"0072"},"figref":["FIG. 4A","FIG. 9","FIG. 4","FIG. 4","FIG. 4B","FIG. 5A","FIG. 5B","FIG. 7","FIG. 11"],"b":["400","161","160","400"]},"In step , stored preferences and alert conditions are retrieved from persistent memory on the audio interface unit . Preferences include values for parameters that describe optional functionality for the unit , such as how to mix different simultaneous audio sources, which earbud to use for alerts when both are available, how to respond to one or more earbuds not in an ear, what words to use for different actions, what words to use in different alert contexts, what network address to use for the personal audio service , names for different audio sources, names for different contacts. Parameters for alert conditions indicate what sounds to use for breaking news, social network contact status changes, text message, SD or HD audio phone calls, voice messages, reminders, and different priorities for different alerts. In some embodiments, the audio interface unit  does not include persistent memory for these preferences and step  is omitted.","In step , a query message is sent to the personal audio service  for changes in preferences and alert conditions. In some embodiments, the audio interface unit  does not include persistent memory for these preferences and step  includes obtaining all current values for preferences and alert conditions.","In step , it is determined which earbuds are in place in the user's ears. For example, in-ear detectors are interrogated to determine if each earbud is in place in a user's ear.","In step  a branch point is reached based on the number of earbuds detected to be in place in a user's ear. If no earbud is in place in the user's ear, then the audio interface unit is in offline mode, and a message is sent to the personal audio service  that the particular audio interface unit  is in offline mode.","In step , it is determined if an alert conditions is satisfied, e.g., a breaking news alert is received at the audio interface unit . In some embodiments, the user initiates the alert, e.g., by stating the word \u201cplay,\u201d which is desirable to follow, in some embodiments, by some identifier for the content to be played. If so, then in step  it is determined whether the audio interface unit is in offline mode. If so, then in step  instead of presenting the alert at an earbud, the alert is filtered and, if the alert passes the filter, the filtered alert is stored. The stored alerts are presented to the user when the user next inserts an earbud, as describe below with reference to step . Alerts are filtered to remove alerts that are not meaningfully presented later, such as an alert that it is 5 PM or an alert that a particular expected event or broadcast program is starting. Control then passes back to step  to determine which earbuds are currently in an ear of the user. In some embodiments, alerts and other audio content are determined by the remote personal audio service ; and step , step  and step  are omitted.","If it is determined in step  that one earbud is in place in the user's ear, then the audio interface unit is in alert mode, capable of receiving alerts; and a message is sent, in step , to the personal audio service  that the particular audio interface  unit is in alert mode.","If it is determined in step  that two earbuds are in place in the user's ears, then the audio interface unit is in media mode, capable of listening to stereo media or both media and alerts simultaneously; and a message is sent to the personal audio service  that the particular audio interface  unit is in media mode (step ).","In step , it is determined whether there are stored alerts. If so, then in step  the stored alerts are presented in one or more earbuds in place in the user's ear. In some embodiments, alerts and other audio content are determined by the remote personal audio service ; and step  and step  are omitted.","In step , it is determined whether there is an activation button or microphone signal or both. If so, then in step  an action to take is determined and the action is performed based on the signal and the alert or media mode of the audio interface unit. For example, a particular audio source is played, or a particular alert is responded to based on the spoken word of the user, or a phone call to a particular contact is initiated. In some embodiments, the action is determined at the text-audio processor , or performed by the audio interface unit , or both. In some embodiments the button or microphone signal is transmitted to the personal audio service , and the action is determined and performed there. In some embodiments the action is determined at the text-audio processor ; and that action is indicated in data sent to the personal audio service , where the action is performed.","In step , it is determined whether there is an audio source to play, such as broadcast radio program, a local audio source, a stream of data packets with audio codec for SD or HD audio, e.g., from a news feed, or text to speech conversion of web page content. If so, then in step , the audio source is presented at one or more in-ear earbuds by the audio mixer module .","In step , as described above, it is determined whether alert conditions are satisfied, e.g., whether an alert is received from the personal audio service . If so, and if the audio interface unit  is not in offline mode as determined in step , then in step  an audio alert is presented in one or more in-ear earbuds. For example the audio mixer module  interrupts the audio source to present the alert in one or both in-ear earbuds. In some embodiments, the user initiates the alert, e.g., by stating the word \u201cplay,\u201d which it is desirable to follow, in some embodiments, by some identifier for the content to be played. In some of these embodiments, step  is omitted. In step , the user is prompted for input in response for the alert; and the alert context time window of opportunity is initiated. Control passes to step  to process any user spoken response to the alert, e.g., received as microphone and activation button signals. In some embodiments, the prompts include an audio invitation to say one or more of the limited vocabulary commands associated with the alert. In some embodiments, the user is assumed to know the limited vocabulary responses, and step  is omitted.","In some embodiments, the alerts (and any prompts) are included in the audio data received from the remote personal audio service  through the wireless transceiver  and played in step ; so steps , ,  and  are omitted.","If it is determined in step  that there is not an alert condition, or if step  is omitted, then control passes to step . In step , it is determined whether there is a change in the in-ear earbuds (e.g., an in-ear earbud is removed or an out of ear earbud is placed in the user's ear). If so, the process continues at step . If not, then in step  it is determined whether the user is done with the device, e.g., by speaking the phrase \u201cunit off,\u201d or \u201cDone.\u201d If so, then the process ends. Otherwise, the process continues at step , described above.","Thus, the audio interface unit  is capable of presenting network service data as audio in one or more earbuds and responding based on user sounds spoken into a microphone. In the illustrated embodiment, the audio interface unit  determines, based on data received from an in-ear detector in communication with a data communications bus, whether the earbud speaker is in place in an ear of the user. If the speaker is determined not in place in the ear of the user, then the audio interface unit  terminates presentation of the received audio data at the speaker.","The audio interface unit , in some embodiments, determines whether to present the audio data by sending data indicating the spoken word to a remote service and receiving, from the remote service, data indicating whether to initiate presentation of the audio data. In some embodiments, the data indicating whether to initiate presentation of the audio data is the audio data to be presented, itself. In some embodiments, the determination whether to present the audio data further comprises converting the spoken word to text in a speech to text module of the text-audio processor and determining whether to initiate presentation of the audio data based on the text. In some embodiments, the initiation of the presentation of the received audio data at the speaker further comprises converting audio data received as text from the different apparatus to speech in a text to speech module of the text-audio processor.","In some embodiments, a memory in communication with a data communications bus includes data indicating a limited vocabulary of text for the speech to text module, wherein the limited vocabulary represents a limited set of verbal commands to which the apparatus responds. In some embodiments, the apparatus is small enough to be hidden in an article of clothing worn by the user. In some embodiments, a single button indicates a context sensitive user response to the presentation of the received audio data at the speaker.",{"@attributes":{"id":"p-0090","num":"0089"},"figref":["FIG. 4B","FIG. 9","FIG. 8","FIG. 10"],"b":["450","143","160","145","101","450"]},"In step , the audio interface units in range over wireless link are determined. In the illustrated embodiment, it is determined that the audio interface unit  is in range over wireless link . In step , a connection is established with the personal audio client  on the audio interface unit  in range.","In step , it is determined whether a message is received for a personal audio service (e.g., service ) from a personal audio client (e.g., client ). If so then in step  the message is forwarded to the personal audio service (e.g., service ).","In step , it is determined whether a phone call is received for a user of the audio interface unit in range. For example, if the user has not indicated to the personal audio service  to direct all phone calls to the service, and the audio interface unit does not have a full cellular engine, then it is possible that the user receives a cellular telephone call on UE . That call is recognized by the personal audio agent in step .","If such a call is received, then in step , a phone call alert is forwarded to the personal audio client on the audio interface unit to be presented in one or more in-ear earbuds. In some embodiments, in which the audio interface unit includes a full cellular engine, or in which all calls are forwarded to the personal audio service , step  and step  are omitted.","In step  it is determined whether audio data for an audio channel is received in one or more data packets from a personal audio service (e.g., service ) for a personal audio client (e.g., client ) on an in-range audio interface unit. If so, then in step  the audio channel data is forwarded to the personal audio client (e.g., client ).","In step , it is determined whether the process is done, e.g., by the audio interface unit (e.g., unit ) moving out of range, or by receiving an end of session message from the personal audio service (e.g., service ), or by receiving an offline message from the personal audio client (e.g., client ). If so, then the process ends. If not, then step  and following steps are repeated.",{"@attributes":{"id":"p-0097","num":"0096"},"figref":["FIG. 5A","FIG. 9","FIG. 8","FIG. 5A"],"b":["500","143","140","500","160","101"]},{"@attributes":{"id":"p-0098","num":"0097"},"figref":"FIG. 6A","b":["630","630","635","632","634","636","638","630","161","109","639","133","105"]},"The web user interface module  interacts with the web browser (e.g., browser ) to allow the user to specify what content and notifications (also called alerts herein) to present through the personal audio client as output of a speaker (e.g., one or more earbuds ) and under what conditions. Thus web user interface  facilitates access to, including granting access rights for, a user interface configured to receive first data that indicates a first set of one or more sources of content for presentation to a user, and to receive second data that indicates a second set of zero or more time-sensitive alerts for presentation to the user. Details about the functions provided by web user interface  are more fully described below with reference to steps  through  of  and in . In brief, the web user interface module  is a web accessible component of the personal audio service where the user can: (1) manage services and feeds for the user's own channel of audio; (2) set rules to filter and prioritize content delivery; and (3) visualize the information flow. The data provided through web user interface  is used to control the data acquired by the time-based input module ; and the way that data is arranged in time by organization module . In some embodiments, the visualization provided by the web interface module  includes a graphical time line, as described in more detail below with reference to  and . In some embodiments, the web interface module  includes a graphical user interface that includes one or more active areas associated with one or more components of the timeline, as described in more detail below.","The time-based input module , acquires the content used to populate one or more channels defined by the user. Sources of content for presentation include one or more of voice calls, short message service (SMS) text messages (including TWITTER\u2122), instant messaging (IM) text messages, electronic mail text messages, Really Simple Syndication (RSS) feeds, status or other communications of different users who are associated with the user in a social network service (such as social networks that indicate what a friend associated with the user is doing and where a friend is located), broadcast programs, world wide web pages on the internet, streaming media, music, television broadcasting, radio broadcasting, games, or other applications shared across a network, including any news, radio, communications, calendar events, transportation (e.g., traffic advisory, next scheduled bus), television show, and sports score update, among others. This content is acquired by one or more modules included in the time-based input module such as an RSS aggregator module , an application programming interface (API) module for one or more network applications, and a received calls module for SD or HD audio calls forwarded to the personal audio service , e.g., from one or more land lines, pagers, cell phones etc. associated with the user.","The RSS aggregation module regularly collects any kind of time based content, e.g., email, twitter, speaking clock, news, calendar, traffic, calls, SMS, radio schedules, radio broadcasts, in addition to anything that can be encoded in RSS feeds. The received calls module enables cellular communications, such as voice and data following the GSM\/3G protocol to be exchanged with the audio interface unit through the personal audio client .","The application programming interface (API) module provides access to the personal audio service from other applications, such as the social network service, e.g., through the timeline graphical user interface, described in more detail below. In some embodiments, the interaction is used to affect what content and alerts are included in the audio channel, or to add browsers or other audio interface units to the recipients of the audio channel content.","In the illustrated embodiment, the time-based input module  also includes a received sounds module for sounds detected at a microphone  on an audio interface unit  and passed to the personal audio service module  by the personal audio client .","Some of the time-based input is classified as a time-sensitive alert or notification that allows the user to respond optionally, e.g., a notification of an incoming voice call that the user can choose to take immediately or bounce to a voicemail service. The time-sensitive alerts includes at least one of a notification of an incoming voice call, a notification of incoming text (SMS, IM, email, TWITTER\u2122), a notification of incoming invitation to listen to an audio stream of a different user, a notification of breaking news, a notification of a busy voice call, a notification of a change in a status of a different user who is associated with the user in a social network service, a notification of a broadcast program, notification of an internet prompt, a reminder set previously by the user, or a request to authenticate the user, among others.","The event cache  stores the received content temporarily for a time that is appropriate to the particular content by default or based on user input to the web user interface module  or some combination. Some events associated with received content, such as time and type and name of content, or data flagged by a user, are stored permanently in an event log by the event cache module , either by default or based on user input to the web user interface module , or time-based input by the user through received sounds module , or some combination. In some embodiments, the event log is searchable, with or without a permanent index. In some embodiments, temporarily cached content is also searchable. Searching is performed in response to a verbal command from the user delivered through received sounds module , as described in more detail below, with reference to , or through a timeline graphical user interface, as described below with reference to  and  through .","The organization module  filters and prioritizes and schedules delivery of the content and alerts based on defaults or values provided by the user through the web user interface , or API , or some combination. The organization module  uses rules-based processing to filter and prioritize content, e.g., don't interrupt user with any news content between 8 AM and LOAM, or block calls from a particular number. The organization module  decides the relative importance of content and when to deliver it. If there are multiple instances of the same kind of content, e.g., 15 emails, then these are grouped together and delivered appropriately. The organized content is passed onto the delivery module .","The delivery module  takes content and optimizes it for difference devices and services. In the illustrated embodiment, the delivery module  includes a voice to text module , an API for external network applications, a text to voice module , and a cellular delivery module . API module delivers some content or sounds received in module to an application program or server or client somewhere on the network, as encoded audio or text in data packets exchanged using any known network protocol. For example, in some embodiments, the API module is configured to deliver text or audio or both to a web browser, as indicated by the dotted arrow to browser  for the mobile terminal, and browser  for some other device, such as a device belonging to a friend of the user on a social network. In some embodiments, the API delivers an icon to be presented in a different network application, e.g., a social network application; and, module responds to selection of the icon with one or more choices to deliver audio from the user's audio channel or deliver text, such as transcribed voice or the user's recorded log of channel events. For some applications or clients (e.g., for user input to network services , e.g., in response to a prompt from an internet service) voice content or microphone sounds received in module are first converted to text in the voice to text module . The voice to text module also provides additional services like: call transcriptions, voice mail transcriptions, and note to self, among others. Cellular delivery module delivers some content or sounds received in module to a cellular terminal, as audio using a cellular telephone protocol, such as GSM\/3G. For some applications, text content is first converted to voice in the text to voice module , e.g., for delivery to an audio interface unit, e.g. audio interface unit  of the user through the personal audio client , or the audio interface unit of a friend of the user on a social network.","Referring again to , in step , a logon request is received from user equipment (UE). For example an HTTP request is received from browser  on UE  based on input provided by user . In some embodiments, step  includes authenticating a user as a subscriber or registering a user as a new subscriber, as is well known in the art. In step , a user interface, such as a web page, is generated for the user to specify audio preferences and alert conditions to be used for an audio interface unit of the user (e.g., audio interface unit  of user ). In some embodiments, step  includes step  to include a graphical user interface that includes a timeline that depicts the sequence of audio contents, as described in more detail below with reference to . In step , the interface is sent to the user equipment.",{"@attributes":{"id":"p-0109","num":"0108"},"figref":["FIG. 6B","FIG. 5"],"b":["600","600","600","635","109","101","507"]},"The Hello page  includes options for the user to select from a variety of network services that can be delivered to the user's audio interface unit . For example, the left panel  indicates the user may select from several personal audio service options listed as \u201cHello channel,\u201d \u201cCalls,\u201d \u201cMessages,\u201d \u201cNotes,\u201d \u201cMarked,\u201d and \u201cService Notes.\u201d These options refer to actions taken entirely by the personal audio service  on behalf of a particular user. In addition, the user can indicate other network entities to communicate with, through personal audio service  and the audio interface unit , such as \u201cContacts,\u201d \u201cServices,\u201d and \u201cDevices.\u201d These options refer to actions taken by third party entities other than the personal audio service  and personal audio client . Contacts involve others who may communicate with the user through phone calls, emails, text messages and other protocols that do not necessarily involve an audio interface unit . Services are provided by service providers on the internet and one or more phone networks, including a cellular telephone network or HD audio telephony. Devices involve personal area network devices that could serve as the audio interface unit  or with which the audio interface unit  could potentially communicate via the Bluetooth protocol. The user navigates the items of the Hello page to determine what services to obtain from the personal audio service  and how the personal audio service  is to interact with these other entities to deliver audio to the device serving as the audio interface unit . In some embodiments, a timeline graphical user interface  is included, as described in more detail below.","Any audio and text data may be channeled to and from the audio interface unit  by the personal audio service  and the personal audio client . Text provided by services is converted by the personal audio service  to audio (speech). In the illustrated embodiment, the third party services that can be selected to be channeled through the personal audio service  to the audio interface unit  are indicated by lines through and include voice calls , voice messaging , reminders , note taking , news alerts , search engines , bulk short message service (SMS) protocol messaging such as TWITTER\u2122, social network services such as FACEBOOK\u2122, playlist services such as LASTFM\u2122, sports feed services such as ESPN GAMEPLAN\u2122, and cooking services . In the illustrated embodiment, the user has selected some of these services by marking an associated checkbox  (indicted by the x in the box to the left of the name of the third party service). When one of the third party services is highlighted, any sub-options are also presented. For example, the voice calling service includes sub-options  for selecting a directory as a source of phone numbers to call, as well as options  to select favorites, add a directory and upgrade service.","Referring again to , in step , it is determined whether a response has been received from a user, e.g., whether an HTTP message is received indicating one or more services or sub-options have been selected. If so, then in step  the audio preferences and alert conditions for the user are updated based on the response. For example, in step unique identifier for the audio interface unit  is indicated in a user response and associated with a registered user. In some embodiments, step  includes step  to operate on audio content, e.g. to update contents or delivery device or alerts, alone or in some combination, based on user response indicated in the timeline graphical user interface included in the browser, as described in more detail below with reference to . In step , it is determined if the interaction with the user is done, e.g., the user has logged off or the session has timed out. If not, control passes back to step  and following to generate and send an updated interface, such as an updated web page. If a response is not received then, in step , it is determined if the interaction is done, e.g., the session has timed out.","The Hello channel option presents a web page that displays the event log for a particular channel defined by the user.  is a diagram of another example user interface  utilized in a portion of the process of , according to an embodiment. Page  depicts the event log for one of the user's channels, as indicated by the \u201cHello channel\u201d option highlighted in panel . In some embodiments the events are indicated by symbols in the timeline graphical user interface , as described in more detail below. The page  shows today's date in field , and various events in fields through from most recent to oldest (today's entries shaded), along with corresponding times in column , type of event in column . Options column  allows the user to view more about the event, to mark the event for easy access or to delete the event from the log. In the illustrated embodiment, the events include a reminder to watch program A , a reminder to pick up person A , a call to person B , a weekly meeting , a lunch with person C , a manually selected entry , a call with person D , a game between team A and Team B , a previous reminder to record the game , lunch with person E , a message from person F , a tweet from person G , and an email from person H ",{"@attributes":{"id":"p-0114","num":"0113"},"figref":["FIG. 5B","FIG. 5A"],"b":["530","530","511","530"]},"In step , the user is prompted for and responses are received from the user for data that indicates expressions to be used to indicate allowed actions. The actions are fixed by the module; but the expressions used to indicate those actions may be set by the user to account for different cultures and languages. Example allowed actions, described in more detail below with reference to  through , include ANSWER, IGNORE, RECORD, NOTE, TRANSCRIBE, INVITE, ACCEPT, SEND, CALL, TEXT, EMAIL, STATUS, MORE, START, PAUSE, STOP, REPEAT, TUNE-IN, SLOW, MIKE, among others. For purposes of illustration, it is assumed herein that the expressions are the same as the associated actions. In some embodiments, synonyms for the terms defined in this step are learned by the personal audio service , as described in more detail below. Any method may be used to receive this data. For example, in various embodiments, the data is included as a default value in software instructions, is received as manual input from a user or service administrator on the local or a remote node, is retrieved from a local file or database, or is sent from a different node on the network, either in response to a query or unsolicited, or the data is received using some combination of these methods.","In step , the user is prompted for or data is received or both, for data that indicates one or more devices the user employs to get or send audio data, or both. Again, any method may be used to receive this data. For example, during step  the user provides a unique identifier for the audio interface unit (e.g., unit ) or cell phone (e.g., UE ), such as a serial number or media access control (MAC) number, that the user will employ to access the personal audio service .","In step , the user is prompted for or data is received or both, for data that indicates a channel identifier. Again, any method may be used to receive this data. This data is used to distinguish between multiple channels that a user may define. For example, the user may indicate a channel ID of \u201cMusic\u201d or \u201cnews\u201d or \u201cOne\u201d or \u201cTwo.\u201d In steps  through , data is received that indicates what constitutes example content and alerts for the channel identified in step . In step , it is determined whether there is another channel to be defined. If so, control passes back to step  and following for the next channel. If not, then process  (for step ) is finished.","In step , the user is prompted for or data is received or both, for data that indicates voice call handling, priority and alert tones. The data received in this step indicates, for example, which phone numbers associated with the user are to be routed through the personal audio service, and at what time intervals, a source of contact names and phone numbers, phone number of contacts to block, phone numbers of contacts to give expedited treatment, and different tones for contacts in the regular and expedited categories, and different tones for incoming calls and voice messages, among other properties for handling voice calls.","In step , the user is prompted for or data is received or both, for data that indicates text-based message handling, priority and alert tones. The data received in this step indicates, for example, which text-based messages are to be passed through the personal audio service and the user's network address for those messages, such as SMS messages, TWITTER\u2122, instant messaging for one or more instant messaging accounts, emails for one or more email accounts, and at what time intervals. This data also indicates a source of contact names and addresses, addresses of contacts to block, addresses of contacts to give expedited treatment, and different tones for contacts in the regular and expedited categories, and different tones for different kinds of text-based messaging.","In step , the user is prompted for or data is received or both, for data that indicates one or more other network services, such as RSS feeds on traffic, weather, news, politics, entertainment, and other network services such as navigation, media steaming, and social networks. The data also indicates time intervals, if any, for featuring one or more of the network services, e.g., news before noon, entertainment after noon, social network in the evening.","In step , the user is prompted for or data is received or both, for data that indicates how to deliver alerts, e.g., alerts in only one ear if two earbuds are in place, leaving any other audio in the other ear. This allows the user to apply the natural ability for ignoring some conversations in the user's vicinity to ignore the alert and continue to enjoy the audio program. Other alternatives include, for example, alerts in one or both in-ear earbuds and pause the audio or skip the audio during the interval the alert is in effect, alerts for voice ahead of alerts for text-messages, and clustering rather than individual alerts for the same type of notification, e.g., \u201c15 new emails\u201d instead of \u201cemail from person A at 10 AM, email from person B at 10.35 AM, . . . \u201d.","In step , the user is prompted for or data is received or both, for data that indicates manually entered reminders from the user, e.g., wake up at 6:45 AM, game starts in half hour at 7:15 PM, game starts at 7:45 PM, and make restaurant reservation 5:05 PM.\u201d","In step , the user is prompted for or data is received or both, for data that indicates what speech to transcribe to text (limited by what is legal in the user's local jurisdiction), e.g., user's side of voice calls, both sides of voice calls, other person side of voice calls from work numbers, and all sounds from user's microphone for a particular time interval.","In step , the user is prompted for or data is received or both, for data that indicates what audio or text to publish for other users to access and what alerts, if any, to include. Thus, a user can publish the channel identified in step  (e.g., the \u201cMusic\u201d channel) for use by other users of the system (e.g., all the user's friends on a social network). Similarly, the user can publish the text generated from voice calls with work phone numbers for access by one or more other specified colleagues at work.","The above steps are based on interactions between the personal user service  and a browser on a conventional device with visual display and keyboard of multiple keys, such as browser  on UE . The following steps, in contrast, are based on interactions between the personal user service  and a personal audio client  on an audio interface unit  or other device serving as such, which responds to user input including voice commands.","Referring again to , in step  it is determined whether the audio interface unit is offline. For example, if no message has been received from the unit for an extended time, indicating the unit may be powered off, then it is determined in step  that the audio interface unit  is offline. As another example, a message is received from the personal audio client  that the unit is offline based on the message sent in step , because no earbud speaker was detected in position in either of the user's ears.","If it is determined in step  that the audio interface unit  is offline, then, in step  it is determined whether there is an alert condition. If not, then step  is repeated. If so, then, in step , data indicating filtered alerts are stored. As described above, with reference to step , alerts that have no meaning when delayed are filtered out; and the filtered alerts are those that still have meaning at a later time. The filtered alerts are stored for delayed delivery. Control passes back to step .","If it is determined in step  that the audio interface unit  is online, then in step  the personal audio service  requests or otherwise receives data indicated by the user's audio preferences and alert conditions. For example, the personal audio service  sends requests that indicate phone calls for the user's cell phone or land line or both are to be forwarded to the personal audio service  to be processed. Similarly, the personal audio service  requests any Really Simple Syndication (RSS) feeds, such as an internet news feed, indicated by the user in responses received in step . In an illustrated embodiment, step  is performed by the time-based input module .","In step , one or more audio channels are constructed for the user based on the audio preferences and received data. For example, the user may have defined via responses in step  a first channel for music from a particular playlist in the user's profile on the social network. Similarly, the user may have defined via responses in step  a second channel for an RSS feed from a particular news feed, e.g., sports, with interruptions for breaking news from another news source, e.g., world politics, and interruption for regular weather updates on the half hour, and to publish this channel so that other contacts of the user on the social network can also select the same channel to be presented at their devices, including their audio interface devices. In step , for this example, audio streams for both audio channels are constructed. In an illustrated embodiment, step  is performed by caching content and logging events by event cache module . In some embodiments, step  includes step  to store the sequence of contents for a graphical user interface, and to include content or alerts from a friend of the user on a social network, such as received in response to a timeline graphical user interface accessed through the friend's social network page, as described in more detail below.","In step , it is determined whether any alert conditions are satisfied, based on the alert conditions defined in one or more user responses during step . If so, then in step  the alerts are added to one or more channels depending on the channel definitions given by the user in responses received in step . For example, if there are any stored filtered alerts from step  that have not yet been delivered, these alerts are added to one or more of the channels. For example, if the user has defined the first channel such that it should be interrupted in one ear only by any alerts, with a higher priority for alerts related to changes in status of contacts in a social network than to breaking news alerts and a highest priority for alerts for incoming voice calls, the stored and new alerts are presented in that order on the first channel. Similarly, the user may have defined a different priority of alerts for the second channel, and the stored and new alerts are added to the second channel with that different priority. In some embodiments, alerts are not added to a published channel delivered to another user unless the user defining the channel indicates those alerts are to be published also. In an illustrated embodiment, steps  and  are performed by organization module . In some embodiments, step  includes step  to add alerts indicated by a friend of the user on a social network, as described in more detail in a alter section.","After any alerts are added, or if there are no alerts, then control passes to step . In step , the audio from the selected channel with any embedded alerts are sent to the personal audio client  over a wireless link to be presented in one or more earbuds in place in a user's ear. For example, the audio is encoded as data and delivered in one or more data packets to the personal audio client  on audio interface unit  of user . In some embodiments, the data packets with the audio data travel through wireless link directly from a cell phone network, or a wide area network (WAN), or wireless local area network (WLAN). In some embodiments, the data packets with the audio data travel indirectly through personal audio agent process  on UE  and thence through wireless link in a wireless personal area network (WPLAN) to personal audio client . In an illustrated embodiment, step  is performed by delivery module . In some embodiments, the audio data is sent to a browser for presentation to the user during step . In some embodiments, step  includes step  to send the audio data to the browser or audio interface unit of a friend of the user on a social network.","In step , it is determined if a user response message is received from the personal audio client  of user . In an illustrated embodiment, step  is performed by received sounds module . If so, in step  an action is determined based on the response received and the action is performed. In some embodiments, the response received from the personal audio client is text converted from spoken sounds by the text-audio processor of the personal audio client. In some embodiments, the response received from the personal audio client  is coded audio that represents the actual sounds picked up the microphone of the audio interface unit  and placed in the response message and sent by the personal audio client . In an illustrated embodiment, step  is performed by organization module  or delivery module , or some combination.","The action determined and performed in step  is based on the user response in the message received. Thus, if the response indicates the user spoke the word \u201cvoicemail\u201d, then the voicemail is contacted to obtain any voice messages, which are then encoded in messages and sent to the personal audio client  for presentation in one or more in-ear earbuds of the user. Similarly, if the response indicates the user spoke the word \u201cChannel Two\u201d, then this is determined in step  and in step , when next executed, the second channel is sent to the personal audio client  instead of the first channel.","In step , it is determined if the personal audio service is done with the current user, e.g., the user has gone offline by turning off the audio interface unit  or removing all earbuds. If not, control passes back to step  and following steps to update the graphical user interface and request and receive the data indicated by the updates.",{"@attributes":{"id":"p-0135","num":"0134"},"figref":["FIG. 7","FIG. 5A"],"b":["700","700","527","500","236"]},"In step  data is received that indicates the current alert and time that the alert was issued. For example, in some embodiments this data is retrieved from memory where the information is stored during step . In step , the user audio is received, e.g., as encoded audio in one or more data packets.","In step , it is determined whether the user audio was spoken within a time window of opportunity associated with the alert, e.g. within 3 seconds of the time the user received the tone and any message associated with the alert, or within 5 seconds of the user uttering a word that set a window of opportunity for responding to a limited vocabulary. In some embodiments, the duration of the window of opportunity is set by the user in interactions with the web user interface . If so, then the user audio is interpreted in the context of a limited vocabulary of allowed actions following that particular kind of alert, as described below with respect to steps  through . If not, then the user audio is interpreted in a broader context, e.g., with a larger vocabulary of allowed actions, as described below with respect to steps  through .","In step , the sound made by the user is learned in the context of the current alert, e.g., the sound is recorded in association with the current alert. In some embodiments, step  includes determining the number of times the user made a similar sound, and if the number exceeds a threshold and the sound does not convert to a word in the limited vocabulary then determining if the sound corresponds to a synonym for one of the words of the limited vocabulary. This determination may be made in any manner, e.g., by checking a thesaurus database, or by generating voice that asks the user to identify which allowed action the sound corresponds to, or by recording the user response to a prompt issued in step  when a match is not obtained. Thus the process  learns user preferences for synonyms for the limited vocabulary representing the allowed actions. Thus, the system learns what kind of new vocabulary is desirable; can know how the user usually answers to certain friends; and that way can interpret and learn the words based on communication practices within a social networking context for the user or the friend. So with step  together with step , instead of using a pre-set vocabulary, the user can record the user's own voice commands. In some embodiments, step  is omitted.","In step , the sound is compared to the limited vocabulary representing the allowed actions for the current alert, e.g., by converting to text and comparing the text to the stored terms (derived from step ) for the allowed actions. In step , it is determined if there is a match. If not, then in step  the user is prompted to indicate an allowed action by sending audio to the user that presents voice derived from the text for one or more of the allowed actions and the start of the window of opportunity for the alert is re-set. A new response from the user is then received, eventually, in step . If there is a match determined in step , then in step  the personal audio service acts on the alert based on the match. Example alerts, limited vocabularies for matches and resulting actions are described in more detail below with reference to  through . In step , it is determined whether conditions are satisfied for storing the action in the permanent log. If not, control passes back to step , described above. If so, then in step  the action is also recorded in the permanent log.","If it is determined, in step , that the user audio was not spoken within a time window of opportunity associated with the alert, then the audio is interpreted in a broader context. In step , the sound made by the user is learned in the context of the current presented audio, e.g., the sound is recorded in association with silence or a media stream or a broadcast sporting event. In some embodiments, step  includes determining the number of times the user made a similar sound, and if the number exceeds a threshold and the sound does not convert to a word in the broader vocabulary then determining if the sound corresponds to a synonym for one of the words of the broader vocabulary. This determination may be made in any manner, e.g., by checking a thesaurus database, or by generating voice that asks the user to identify which allowed action the sound corresponds to. Thus the process  learns user preferences for synonyms for the broader vocabulary representing the allowed actions for silence or a presented audio stream. In some embodiments, step  is omitted.","In step , the sound is compared to the broader vocabulary representing the allowed actions not associated with an alert, e.g., by converting to text and comparing the text to the stored terms (derived from step ) for the allowed actions, or by comparing the user audio with stored voiceprints of the limited vocabulary. In step , it is determined if there is a match. If not, then in step  the user is prompted to indicate an allowed action by sending audio to the user that presents voice derived from the text for one or more of the allowed actions. A new response from the user is then received, eventually, in step . If there is a match determined in step , then in step  the personal audio service acts based on the match. Example limited vocabularies for matches and resulting actions are described in more detail below with reference to  for general actions and  for actions related to currently presented audio. In step , it is determined whether conditions are satisfied for storing the action in the permanent log. If not, then in step  it is determined if conditions are satisfied for terminating the process. If conditions are satisfied for storing the action, then in step  the action is also recorded in the permanent log. If it is determined, in step , that conditions are satisfied for terminating the process, then the process ends. Otherwise control passes back to step , described above.",{"@attributes":{"id":"p-0142","num":"0141"},"figref":"FIG. 11","b":["1103","517","1","2","2","3","3","4","5","6","7","8","1105"]},{"@attributes":{"id":"p-0143","num":"0142"},"figref":["FIG. 12A","FIG. 12C"],"b":["1201","1201","1202","1203","1205","1202","1211","1213","1215","1217","1219","1205","1201","1202"]},"Returning again to , in step , the audio channel timeline GUI is pushed to the user's audio service page, such as during step  described above, e.g., as timeline GUI  depicted above. In step , it is determined whether the user has activated an item in the GUI, e.g., has selected a pull down menu item associated with one of the icons. If so, then in step , an operation on the audio channel is performed based on the user response. For example, the selected past content is converted to text or indexed or published, or a new content or alert is added to the channel for rendering in the future.","In step , a notice is sent to the social network service\/server that the user has a particular active audio channel for sharing with friends, e.g., in a message that indicates the user and an audio channel identifier. In some embodiments in which the audio channel is not shared with friends, step  and following steps are omitted. In some embodiments, the notice indicates a select group of social network friends with whom the audio channel can be shared and presented. The social network is configured to indicate to the friends that the audio channel is available for presentation to the user.",{"@attributes":{"id":"p-0146","num":"0145"},"figref":["FIG. 12B","FIG. 12B"],"b":["1250","1250","1252","1252","1250","1113","1254","1254","1254","1250","1252","1254","1254","1250"]},"Referring again to , in step , it is determined if a request for the audio channel timeline GUI is received from a friend of the user in the social network. For example, as a result of the friend selecting the audio channel icon , a message is received from the social network service in step  requesting the audio channel timeline GUI of the user on behalf of the friend. It is assumed for purposes of illustration that the request includes a network address and port for the friend's browser where the GUI is to be presented as a part of the friend's homepage. If not, control passes back to step . If a request is received, however, then in step , the audio channel timeline GUI is sent to and presented on the friend's browser displaying the social network page of the friend.","In step  it is determined whether the friend has activated an item in the GUI, e.g., has selected a pull down menu item associated with one of the icons. If so, then in step , an operation on the audio channel is performed based on the user response. For example, the selected past content is converted to text or indexed or published, or a new content or alert is added to the channel for rendering in the future.","In step  it is determined, whether the friend has indicated a desire to listen to content from the user's audio channel, e.g., to join in listening to the channel as the user is hearing it. In some embodiments, the data indicating this desire is received in a request\/message that indicates the destination for the audio content, e.g., the friend's browser or the friend's audio interface unit. If so, then in step , the browser or audio interface unit of the friend is added to the delivery schedule to receive the audio content, e.g., from the delivery module .","In step , it is determined whether interactions with the audio channel are complete. If so, the process ends. Otherwise control passes back to step  to push an updated GUI to the user's audio service page, and following steps.","Additionally, the friend can amend one or more content items to the future part of the user's audio channel timeline GUI  by mouse or voice commands so that an icon is presented for each of the content items on the timeline. Further, the icon is updated to all other presentations of the user's audio channel timeline on other friends' and user's own browsers. The icon is associated with information\/metadata on the content item, a content item file and\/or information how\/where to access the content item file on a network.",{"@attributes":{"id":"p-0152","num":"0151"},"figref":"FIG. 12C","b":["1220","1120","1222","1223"]},"In the illustrated embodiment, each icon for content includes one or more of a pressure trace, a symbol or label, or a relationship, alone or in some combination. A pressure trace emulates a trace recorded by a pressure sensor as acoustic waves of varying amplitude pass by the sensor and extends for the duration of the audio content. In some embodiments, the trace is based on an actual pressure trace of the source audio indicated. In the illustrated embodiment, the trace is just an arbitrary representation to suggest audio data. In other embodiments, a simple rectangular box is used to indicate the audio data for the temporal duration of the content. Audio content that is available as sound or transcript is indicated by a colored pressure trace (or rectangle) with different colors indicating different types of audio. In the illustrated embodiment, color A (e.g., red) indicates telephone calls, including SD and HD audio telephone calls, and different color B (e.g., orange) indicates text to speech. Unavailable audio, e.g., copyrighted material that can't be reproduced directly, such as songs and broadcast audio, is indicated by a grey pressure trace. Alerts of different types are represented by a short duration pressure trace colored to indicate the type of alert, e.g., blue for broadcast content, brown for song, purple for text reminders.","The source or destination of the audio data is indicated by symbols. For example, text derived from the audio, such as speech-to-text or song title and artist, is represented by a derived text symbol , e.g., a rectangle with parallel horizontal lines. A message, such as a text message, email or instant message is represented by an envelope . An audio file containing all or a portion of the audio content is represented by a reel to reel symbol . A broadcast source is represented by a scene mark symbol , a song by a slurred note symbol , and a reminder by a reminder text box  containing the reminder as short text.","A relationship between the pressure trace and the source or destination symbol is represented by an arrow or bracket, which indicates relations  in . An incoming message or alert from a friend is indicated by an arrow from the symbol to the pressure trace. An alert or saved audio provided by the owner of the audio channel is represented by an upward arrow. A section of audio converted to text or audio file is indicated by a bracket.","At least some pixels of each icon form an active area where pointer device actions are used to indicate operations on the content, such as play, convert to text, index, join, publish, etc.","For purposes of illustration, it is assumed that timeline GUI  represents all content delivered to a user on one audio channel defined and owned by that user. The user may choose to publish all or a part of the audio channel; and only the published portion is included as a timeline GUI presented to a friend of the user on a social network. A user may define multiple audio channels, e.g., for different themes, such as music, news, business, and a different timeline GUI is generated for each audio channel. A different portion of each may be published for use in the timeline GUI sent to a friend. A user may present multiple timelines of multiple friends, e.g., on the user's browser's home web page or a social network web page.",{"@attributes":{"id":"p-0158","num":"0157"},"figref":"FIG. 12D","b":["1260","1260"]},"The GUI  includes a time axis  with later time to the right of earlier time. The time axis is divided into horizontal intervals ; and some or all intervals are labeled with an interval label . In the illustrated embodiment, each horizontal interval  is the same length and represents the same temporal duration of one quarter hour (15 minutes). Intervals that begin on the hour are labeled with an interval label  indicating the hour of the day.","Details of audio content within a horizontal interval  are indicated by zero or more icons . Multiple icons are stacked within an interval to form a column of icons, e.g., column  of icons. To allow stacking, each icon has a limited vertical interval . In the illustrated embodiment, the vertical interval  of each icon is fixed. In some embodiments, the vertical interval represents any content no matter how large is the duration of the content within the quarter hour. In some embodiments, the vertical interval represents only audio content of duration up to a particular threshold duration, e.g., one minute in the illustrated embodiment. Audio content that persists longer than the threshold duration is represented by an additional icon for each additional threshold duration. More than 15 icons can be stacked if enough icons represent content that persists less than one minute, since each icon has a minimum height of the vertical interval . In some embodiments, the icon representing audio content being rendered currently, e.g. at the current time, is indicated by an animated icon  that drops onto the column of icons. The animated icon  is also used in some embodiments as the current time icon. The icons  of past audio content are positioned along time axis  to the left of, or below, the icon  of the current audio content. The icons  of future\/planned audio content are positioned along time axis  to the right of the icon  of the current audio content.","In the illustrated embodiment, each icon  for audio content includes a uniform sized box. Audio content that is available as sound or transcript is indicated by a box with different colors indicating different types of audio. In the illustrated embodiment, uncolored icons represent content selected or reminders produced by the user, and colored icons represent special events or interactions with others. For example, color A (e.g., orange) icons A indicate reminders provided by a friend of the user; color B (e.g., red) icons B indicate text messages, such as TWITTER messages, from a friend; color C (e.g., green) icons C indicate audio messages, such as a cellular telephone call with a friend; and color D (e.g., blue) icons D indicate converted and indexed audio messages, such as snippets of a phone call that includes a website or other network or geographic address or phone number.","At least some pixels of each icon form an active area where pointer device actions or voice command, or some combination, are used to indicate operations on the content, such as play, convert to text, index, join, publish, or others described above.","As described above for timeline GUI , timeline GUI  represents all content delivered to a user on one audio channel defined and owned by that user. The user may choose to publish all or a part of the audio channel; and only the published portion is included as a timeline GUI presented to a friend of the user on a social network. A user may define multiple audio channels, e.g., for different themes, such as music, news, business, and a different timeline GUI is generated for each audio channel. A different portion of each may be published for use in the timeline GUI sent to a friend. A user may present multiple timelines of multiple friends, e.g., on the user's browser's home web page or a social network web page.",{"@attributes":{"id":"p-0164","num":"0163"},"figref":["FIGS. 13A-13C","FIGS. 13A-13C"]},{"@attributes":{"id":"p-0165","num":"0164"},"figref":"FIG. 13A","b":["1310","1319","1311","1312","1313","1314","1315","1316","1317","1311","1311"],"i":["a ","b ","c "]},"The content ID field  holds data that indicates a unique identification for the audio content represented by the timeline component. The start time field  holds data that indicates the time the content started on the audio channel in the past, or is scheduled to start in the future. The duration field  holds data that indicates the time after the start that the content ends on the audio channel. The type field  holds data that indicates the type of content (e.g., telephone call, music, broadcast, alert, email converted to speech, etc.). This information is used to determine the icon, color or symbol to use to represent the content on the graphical user interface. The set operations field  holds data that indicates the operations already performed on the audio content, such as the conversion of audio to text, or text to speech, indexing, publishing, etc.) This information is used to determine the symbol or relationship to use to represent the content on the graphical user interface. The allowed operations field  holds data that indicates the operations that may still be performed on the audio content (such as the conversion of audio to text, or text to speech, indexing, publishing, etc.). This is used to determine what menu options, for example, to present to a user who positions a pointing device over the icon representing the audio content.",{"@attributes":{"id":"p-0167","num":"0166"},"figref":"FIG. 13B","b":["1320","1322","1327","1322","1312","1327","1320","1322"]},{"@attributes":{"id":"p-0168","num":"0167"},"figref":"FIG. 13C","b":["1330","1331","1332","1337","1331","1332","1312","1337","1330","1332"]},"The processes described herein for providing network services at an audio interface unit may be advantageously implemented via software, hardware (e.g., general processor, Digital Signal Processing (DSP) chip, an Application Specific Integrated Circuit (ASIC), Field Programmable Gate Arrays (FPGAs), etc.), firmware or a combination thereof. Such exemplary hardware for performing the described functions is detailed below.",{"@attributes":{"id":"p-0170","num":"0169"},"figref":"FIG. 8","b":["800","800","810","800","800"]},"A bus  includes one or more parallel conductors of information so that information is transferred quickly among devices coupled to the bus . One or more processors  for processing information are coupled with the bus .","A processor  performs a set of operations on information as specified by computer program code related to providing network services through an audio interface unit. The computer program code is a set of instructions or statements providing instructions for the operation of the processor and\/or the computer system to perform specified functions. The code, for example, may be written in a computer programming language that is compiled into a native instruction set of the processor. The code may also be written directly using the native instruction set (e.g., machine language). The set of operations include bringing information in from the bus  and placing information on the bus . The set of operations also typically include comparing two or more units of information, shifting positions of units of information, and combining two or more units of information, such as by addition or multiplication or logical operations like OR, exclusive OR (XOR), and AND. Each operation of the set of operations that can be performed by the processor is represented to the processor by information called instructions, such as an operation code of one or more digits. A sequence of operations to be executed by the processor , such as a sequence of operation codes, constitute processor instructions, also called computer system instructions or, simply, computer instructions. Processors may be implemented as mechanical, electrical, magnetic, optical, chemical or quantum components, among others, alone or in combination.","Computer system  also includes a memory  coupled to bus . The memory , such as a random access memory (RAM) or other dynamic storage device, stores information including processor instructions for at least some steps for providing network services through an audio interface unit. Dynamic memory allows information stored therein to be changed by the computer system . RAM allows a unit of information stored at a location called a memory address to be stored and retrieved independently of information at neighboring addresses. The memory  is also used by the processor  to store temporary values during execution of processor instructions. The computer system  also includes a read only memory (ROM)  or other static storage device coupled to the bus  for storing static information, including instructions, that is not changed by the computer system . Some memory is composed of volatile storage that loses the information stored thereon when power is lost. Also coupled to bus  is a non-volatile (persistent) storage device , such as a magnetic disk, optical disk or flash card, for storing information, including instructions, that persists even when the computer system  is turned off or otherwise loses power.","Information, including instructions for at least some steps for providing network services through an audio interface unit is provided to the bus  for use by the processor from an external input device , such as a keyboard containing alphanumeric keys operated by a human user, or a sensor. A sensor detects conditions in its vicinity and transforms those detections into physical expression compatible with the measurable phenomenon used to represent information in computer system . Other external devices coupled to bus , used primarily for interacting with humans, include a display device , such as a cathode ray tube (CRT) or a liquid crystal display (LCD), or plasma screen or printer for presenting text or images, and a pointing device , such as a mouse or a trackball or cursor direction keys, or motion sensor, for controlling a position of a small cursor image presented on the display  and issuing commands associated with graphical elements presented on the display . In some embodiments, for example, in embodiments in which the computer system  performs all functions automatically without human input, one or more of external input device , display device  and pointing device  is omitted.","In the illustrated embodiment, special purpose hardware, such as an application specific integrated circuit (ASIC) , is coupled to bus . The special purpose hardware is configured to perform operations not performed by processor  quickly enough for special purposes. Examples of application specific ICs include graphics accelerator cards for generating images for display , cryptographic boards for encrypting and decrypting messages sent over a network, speech recognition, and interfaces to special external devices, such as robotic arms and medical scanning equipment that repeatedly perform some complex sequence of operations that are more efficiently implemented in hardware.","Computer system  also includes one or more instances of a communications interface  coupled to bus . Communication interface  provides a one-way or two-way communication coupling to a variety of external devices that operate with their own processors, such as printers, scanners and external disks. In general the coupling is with a network link  that is connected to a local network  to which a variety of external devices with their own processors are connected. For example, communication interface  may be a parallel port or a serial port or a universal serial bus (USB) port on a personal computer. In some embodiments, communications interface  is an integrated services digital network (ISDN) card or a digital subscriber line (DSL) card or a telephone modem that provides an information communication connection to a corresponding type of telephone line. In some embodiments, a communication interface  is a cable modem that converts signals on bus  into signals for a communication connection over a coaxial cable or into optical signals for a communication connection over a fiber optic cable. As another example, communications interface  may be a local area network (LAN) card to provide a data communication connection to a compatible LAN, such as Ethernet. Wireless links may also be implemented. For wireless links, the communications interface  sends or receives or both sends and receives electrical, acoustic or electromagnetic signals, including infrared and optical signals, that carry information streams, such as digital data. For example, in wireless handheld devices, such as mobile telephones like cell phones, the communications interface  includes a radio band electromagnetic transmitter and receiver called a radio transceiver. In certain embodiments, the communications interface  enables connection to the communication network  for providing network services directly to an audio interface unit  or indirectly through the UE .","The term computer-readable medium is used herein to refer to any medium that participates in providing information to processor , including instructions for execution. Such a medium may take many forms, including, but not limited to, non-volatile media, volatile media and transmission media. Non-volatile media include, for example, optical or magnetic disks, such as storage device . Volatile media include, for example, dynamic memory . Transmission media include, for example, coaxial cables, copper wire, fiber optic cables, and carrier waves that travel through space without wires or cables, such as acoustic waves and electromagnetic waves, including radio, optical and infrared waves. Signals include man-made transient variations in amplitude, frequency, phase, polarization or other physical properties transmitted through the transmission media. Common forms of computer-readable media include, for example, a floppy disk, a flexible disk, hard disk, magnetic tape, any other magnetic medium, a CD-ROM, CDRW, DVD, any other optical medium, punch cards, paper tape, optical mark sheets, any other physical medium with patterns of holes or other optically recognizable indicia, a RAM, a PROM, an EPROM, a FLASH-EPROM, any other memory chip or cartridge, a carrier wave, or any other medium from which a computer can read. The term computer-readable storage medium is used herein to refer to any computer-readable medium except transmission media.","Logic encoded in one or more tangible media includes one or both of processor instructions on a computer-readable storage media and special purpose hardware, such as ASIC .","Network link  typically provides information communication using transmission media through one or more networks to other devices that use or process the information. For example, network link  may provide a connection through local network  to a host computer  or to equipment  operated by an Internet Service Provider (ISP). ISP equipment  in turn provides data communication services through the public, world-wide packet-switching communication network of networks now commonly referred to as the Internet . A computer called a server host  connected to the Internet hosts a process that provides a service in response to information received over the Internet. For example, server host  hosts a process that provides information representing video data for presentation at display .","At least some embodiments of the invention are related to the use of computer system  for implementing some or all of the techniques described herein. According to one embodiment of the invention, those techniques are performed by computer system  in response to processor  executing one or more sequences of one or more processor instructions contained in memory . Such instructions, also called computer instructions, software and program code, may be read into memory  from another computer-readable medium such as storage device  or network link . Execution of the sequences of instructions contained in memory  causes processor  to perform one or more of the method steps described herein. In alternative embodiments, hardware, such as ASIC , may be used in place of or in combination with software to implement the invention. Thus, embodiments of the invention are not limited to any specific combination of hardware and software, unless otherwise explicitly stated herein.","The signals transmitted over network link  and other networks through communications interface , carry information to and from computer system . Computer system  can send and receive information, including program code, through the networks ,  among others, through network link  and communications interface . In an example using the Internet , a server host  transmits program code for a particular application, requested by a message sent from computer , through Internet , ISP equipment , local network  and communications interface . The received code may be executed by processor  as it is received, or may be stored in memory  or in storage device  or other non-volatile storage for later execution, or both. In this manner, computer system  may obtain application program code in the form of signals on a carrier wave.","Various forms of computer readable media may be involved in carrying one or more sequence of instructions or data or both to processor  for execution. For example, instructions and data may initially be carried on a magnetic disk of a remote computer such as host . The remote computer loads the instructions and data into its dynamic memory and sends the instructions and data over a telephone line using a modem. A modem local to the computer system  receives the instructions and data on a telephone line and uses an infra-red transmitter to convert the instructions and data to a signal on an infra-red carrier wave serving as the network link . An infrared detector serving as communications interface  receives the instructions and data carried in the infrared signal and places information representing the instructions and data onto bus . Bus  carries the information to memory  from which processor  retrieves and executes the instructions using some of the data sent with the instructions. The instructions and data received in memory  may optionally be stored on storage device , either before or after execution by the processor .",{"@attributes":{"id":"p-0183","num":"0182"},"figref":["FIG. 9","FIG. 8"],"b":["900","900","900"]},"In one embodiment, the chip set  includes a communication mechanism such as a bus  for passing information among the components of the chip set . A processor  has connectivity to the bus  to execute instructions and process information stored in, for example, a memory . The processor  may include one or more processing cores with each core configured to perform independently. A multi-core processor enables multiprocessing within a single physical package. Examples of a multi-core processor include two, four, eight, or greater numbers of processing cores. Alternatively or in addition, the processor  may include one or more microprocessors configured in tandem via the bus  to enable independent execution of instructions, pipelining, and multithreading. The processor  may also be accompanied with one or more specialized components to perform certain processing functions and tasks such as one or more digital signal processors (DSP) , or one or more application-specific integrated circuits (ASIC) . A DSP  typically is configured to process real-world signals (e.g., sound) in real time independently of the processor . Similarly, an ASIC  can be configured to performed specialized functions not easily performed by a general purposed processor. Other specialized components to aid in performing the inventive functions described herein include one or more field programmable gate arrays (FPGA) (not shown), one or more controllers (not shown), or one or more other special-purpose computer chips.","The processor  and accompanying components have connectivity to the memory  via the bus . The memory  includes both dynamic memory (e.g., RAM, magnetic disk, writable optical disk, etc.) and static memory (e.g., ROM, CD-ROM, etc.) for storing executable instructions that when executed perform one or more of the inventive steps described herein to provide network services through an audio interface unit The memory  also stores the data associated with or generated by the execution of the inventive steps.",{"@attributes":{"id":"p-0186","num":"0185"},"figref":["FIG. 10","FIG. 1"],"b":"1000"},"Pertinent internal components of the telephone include a Main Control Unit (MCU) , a Digital Signal Processor (DSP) , and a receiver\/transmitter unit including a microphone gain control unit and a speaker gain control unit. A main display unit  provides a display to the user in support of various applications and mobile terminal functions that perform or support the steps of configuring the server for the audio interface unit. The display unit  includes display circuitry configured to display at least a portion of a user interface of the mobile terminal (e.g., mobile telephone). Additionally, the display unit  and display circuitry are configured to facilitate user control of at least some functions of the mobile terminal An audio function circuitry  includes a microphone  and microphone amplifier that amplifies the speech signal output from the microphone . The amplified speech signal output from the microphone  is fed to a coder\/decoder (CODEC) .","A radio section  amplifies power and converts frequency in order to communicate with a base station, which is included in a mobile communication system, via antenna . The power amplifier (PA)  and the transmitter\/modulation circuitry are operationally responsive to the MCU , with an output from the PA  coupled to the duplexer  or circulator or antenna switch, as known in the art. The PA  also couples to a battery interface and power control unit .","In use, a user of mobile terminal  speaks into the microphone  and his or her voice along with any detected background noise is converted into an analog voltage. The analog voltage is then converted into a digital signal through the Analog to Digital Converter (ADC) . The control unit  routes the digital signal into the DSP  for processing therein, such as speech encoding, channel encoding, encrypting, and interleaving. In one embodiment, the processed voice signals are encoded, by units not separately shown, using a cellular transmission protocol such as global evolution (EDGE), general packet radio service (GPRS), global system for mobile communications (GSM), Internet protocol multimedia subsystem (IMS), universal mobile telecommunications system (UMTS), etc., as well as any other suitable wireless medium, e.g., microwave access (WiMAX), Long Term Evolution (LTE) networks, code division multiple access (CDMA), wideband code division multiple access (WCDMA), wireless fidelity (WiFi), satellite, and the like.","The encoded signals are then routed to an equalizer  for compensation of any frequency-dependent impairments that occur during transmission though the air such as phase and amplitude distortion. After equalizing the bit stream, the modulator  combines the signal with a RF signal generated in the RF interface . The modulator  generates a sine wave by way of frequency or phase modulation. In order to prepare the signal for transmission, an up-converter  combines the sine wave output from the modulator  with another sine wave generated by a synthesizer  to achieve the desired frequency of transmission. The signal is then sent through a PA  to increase the signal to an appropriate power level. In practical systems, the PA  acts as a variable gain amplifier whose gain is controlled by the DSP  from information received from a network base station. The signal is then filtered within the duplexer  and optionally sent to an antenna coupler  to match impedances to provide maximum power transfer. Finally, the signal is transmitted via antenna  to a local base station. An automatic gain control (AGC) can be supplied to control the gain of the final stages of the receiver. The signals may be forwarded from there to a remote telephone which may be another cellular telephone, other mobile phone or a land-line connected to a Public Switched Telephone Network (PSTN), or other telephony networks.","Voice signals transmitted to the mobile terminal  are received via antenna  and immediately amplified by a low noise amplifier (LNA) . A down-converter  lowers the carrier frequency while the demodulator  strips away the RF leaving only a digital bit stream. The signal then goes through the equalizer  and is processed by the DSP . A Digital to Analog Converter (DAC)  converts the signal and the resulting output is transmitted to the user through the speaker , all under control of a Main Control Unit (MCU) \u2014which can be implemented as a Central Processing Unit (CPU) (not shown).","The MCU  receives various signals including input signals from the keyboard . The keyboard  and\/or the MCU  in combination with other user input components (e.g., the microphone ) comprise a user interface circuitry for managing user input. The MCU  runs a user interface software to facilitate user control of at least some functions of the mobile terminal  to support providing network services through an audio interface unit The MCU  also delivers a display command and a switch command to the display  and to the speech output switching controller, respectively. Further, the MCU  exchanges information with the DSP  and can access an optionally incorporated SIM card  and a memory . In addition, the MCU  executes various control functions required of the terminal. The DSP  may, depending upon the implementation, perform any of a variety of conventional digital processing functions on the voice signals. Additionally, DSP  determines the background noise level of the local environment from the signals detected by microphone  and sets the gain of microphone  to a level selected to compensate for the natural tendency of the user of the mobile terminal .","The CODEC  includes the ADC  and DAC . The memory  stores various data including call incoming tone data and is capable of storing other data including music data received via, e.g., the global Internet. The software module could reside in RAM memory, flash memory, registers, or any other form of writable storage medium known in the art. The memory device  may be, but not limited to, a single memory, CD, DVD, ROM, RAM, EEPROM, optical storage, or any other non-volatile storage medium capable of storing digital data.","An optionally incorporated SIM card  carries, for instance, important information, such as the cellular phone number, the carrier supplying service, subscription details, and security information. The SIM card  serves primarily to identify the mobile terminal  on a radio network. The card  also contains a memory for storing a personal telephone number registry, text messages, and user specific mobile terminal settings.","While the invention has been described in connection with a number of embodiments and implementations, the invention is not so limited but covers various obvious modifications and equivalent arrangements, which fall within the purview of the appended claims. Although features of the invention are expressed in certain combinations among the claims, it is contemplated that these features can be arranged in any combination and order."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["The embodiments of the invention are illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings:",{"@attributes":{"id":"p-0011","num":"0010"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0012","num":"0011"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0013","num":"0012"},"figref":"FIG. 3"},{"@attributes":{"id":"p-0014","num":"0013"},"figref":"FIG. 4A"},{"@attributes":{"id":"p-0015","num":"0014"},"figref":"FIG. 4B"},{"@attributes":{"id":"p-0016","num":"0015"},"figref":"FIG. 5A"},{"@attributes":{"id":"p-0017","num":"0016"},"figref":["FIG. 5B","FIG. 5A"]},{"@attributes":{"id":"p-0018","num":"0017"},"figref":"FIG. 6A"},{"@attributes":{"id":"p-0019","num":"0018"},"figref":["FIG. 6B","FIG. 5A"]},{"@attributes":{"id":"p-0020","num":"0019"},"figref":["FIG. 6C","FIG. 5A"]},{"@attributes":{"id":"p-0021","num":"0020"},"figref":"FIG. 7","b":"700"},{"@attributes":{"id":"p-0022","num":"0021"},"figref":"FIG. 8"},{"@attributes":{"id":"p-0023","num":"0022"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0024","num":"0023"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0025","num":"0024"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0026","num":"0025"},"figref":"FIG. 12A"},{"@attributes":{"id":"p-0027","num":"0026"},"figref":"FIG. 12B"},{"@attributes":{"id":"p-0028","num":"0027"},"figref":"FIG. 12C"},{"@attributes":{"id":"p-0029","num":"0028"},"figref":"FIG. 12D"},{"@attributes":{"id":"p-0030","num":"0029"},"figref":"FIGS. 13A-13C"}]},"DETDESC":[{},{}]}
