---
title: Method and apparatus for improving human-machine dialogs using language models learned automatically from personalized data
abstract: A speech-based processing system includes a database of PIM data of a user, a set of language models, a learning unit, a recognition server, and a speech application. The learning unit uses a language model learning algorithm to provide language models based on the PIM data. The recognition server recognizes an utterance of the user by using one of the language models. The speech application identifies and accesses a subset of the PIM data specified by the utterance by using the recognition result. The language model learning algorithm may use grammar induction and/or or may train statistical language models based on the PIM data. The language model learning algorithm may be applied to generate language models periodically or on-the-fly during a session with the user.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07072838&OS=07072838&RS=07072838
owner: Nuance Communications, Inc.
number: 07072838
owner_city: Burlington
owner_country: US
publication_date: 20010320
---

{"@attributes":{"id":"description"},"BRFSUM":[{},{}],"heading":["FIELD OF THE INVENTION","BACKGROUND OF THE INVENTION","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION"],"p":["The present invention pertains to automatic speech recognition and related technology for allowing dialogs between humans and machines. More particularly, the present invention relates to using personalized data to improve dialogs between a human and a machine.","Advances in automatic speech recognition technology are providing more rich and meaningful dialogs between humans and machines in a rapidly-increasing number of applications. Many applications seek to allow rich, meaningful, \u201copen\u201d dialogs in an effort to make dialogs more efficient. \u201cOpen\u201d dialogs are dialogs in which the recognition system does not strictly limit what the speaker may say. Open dialogs, however, can be lengthy, tedious and error-prone, due at least in part to imperfect speech recognition accuracy.","Poor recognition accuracy in open dialogs can result from a variety of factors. One common factor is the fact that speakers typically convey information to the recognition system over a lossy speech channel, such as the public switched telephone network. Recognition accuracy also tends to depend on the quality with which expected utterances are modeled; yet speaker utterances can be difficult to predict, especially in applications that are large and open. Further, modeling for open dialogs typically requires a massive amount of training data across a very large number of speakers. As a result, the training process can be difficult and costly.","On the other hand, some applications have sought to simplify dialogs. For example, certain voice portals have provided the ability to derive very simple grammars from an address book. Typically, these grammars are constrained to people's names and addresses for use in voice-activated dialing. These applications generally are not very powerful and are limited in their applicability.","The present invention includes a method and apparatus for facilitating speech recognition. An automated language model learning process is used to acquire a set of language models based on PIM data associated with a user. An utterance by the user is recognized by using one of the language models, and the recognized utterance is used to identify and access a subset of the PIM data.","Other features of the present invention will be apparent from the accompanying drawings and from the detailed description which follows.","As described in greater detail below, a method and apparatus for improving dialogs between a human and a machine by using language models learned automatically from personalized data are described. Note that in this description, references to \u201cone embodiment\u201d or \u201can embodiment\u201d mean that the feature being referred to is included in at least one embodiment of the present invention. Further, separate references to \u201cone embodiment\u201d in this description do not necessarily refer to the same embodiment; however, neither are such embodiments mutually exclusive, unless so stated and except as will be readily apparent to those skilled in the art. Thus, the present invention can include any variety of combinations and\/or integrations of the embodiments described herein.","The technique described herein revolves around the concept of making a personal information manager (PIM) the hub of a speech application. Accordingly, as described in greater detail below, a speech-based processing system according to the present invention includes a database of PIM data of a user, a set of language models, a language model learning\/acquisition unit, a language model look-up unit, a recognition server, and a speech application. The learning unit uses a language model learning algorithm to learn language models automatically from the PIM data. During an interaction with the user, the recognition server recognizes a particular utterance of the user by using one of the language models. The speech application uses the recognition result to identify and access a portion of the PIM data specified by the utterance. For example, the utterance \u201cMike's address\u201d might be recognized using a language model learned from the PIM data and then used as a simple query to look up the address of \u201cMike Smith\u201d in the user's personal address book. The language models may be, for example, speech recognition grammars and\/or statistical language models. Hence, the language model learning algorithm may, for example, use grammar induction and\/or train statistical language models, based on the PIM data. In this manner, language models may be acquired periodically, at specified times, or on-the-fly during a session with the user.","The technique described herein promotes optimized dialogs by allowing the speaker to make short references to personal information, rather than having to speak the information in its entirety. Thus, information that is known and central to a user does not have to be conveyed over a speech channel, but rather can be referred to very simply by name or short description, looked up in the underlying PIM database, and passed on to the application that is being used. The technique also promotes improved recognition accuracy by better modeling what is expected to be said. Further, by using readily available information (PIM data), the described technique avoids the traditional problems of requiring massive amounts of training data.","This technique simplifies the problem of data acquisition by focusing on data that is readily available: the personal data in a user's PIM. In addition, this approach can facilitate short dictation tasks corresponding to PIM entry. Dictation traditionally requires a relatively large amount of data. However, by focusing on dictation of entries in the PIM domain, the amount of data required can be reduced to what is readily available. This approach also allows searches through large amounts of personal data to facilitate the location and retrieval of specific entries quickly on demand (e.g., obtaining access to specific e-mail messages or calendar entries in mobile environments).","Examples of the potential uses of this technique include:","speaking the names of people or places in an address book as a source or destination for obtaining driving directions;","speaking the names of people or places to obtain geographically-centered information (e.g., movie listings in an area, traffic reports, or weather);","speaking key words in e-mail subject lines, folder names, etc. to search for particular e-mail messages;","speaking short descriptions of events to search for specific calendar entries (e.g., \u201cWhere is my meeting with Joe Smith?\u201d or \u201cWhen is my next SpeechObjects meeting?\u201d);","dictating calendar entries based on the types of tasks a user typically has (e.g., \u201cweekly SpeechObjects meeting, \u201cone-on-one meeting with Joe Smith\u201d,); and","dictating \u201cto do\u201d tasks (e.g., \u201cpay electric bills\u201d).","Certain prior approaches only gather raw, simple name or address grammars from an address book to facilitate very simple applications, such as a voice activated dialing. In contrast, the present invention makes use of grammar induction and\/or statistical language model learning\/training techniques to enable larger, cross-domain applications that can utilize all PIM data in order to optimize dialogs. In addition, the present invention is designed to be built on top of standard APIs, in contrast with prior approaches.","Refer now to the Figure, which illustrates a system that implements this technique. Note that the Figure shows a logical configuration of the system and is not intended to imply any particular physical architecture. The physical architecture is unimportant for purposes of understanding the present invention. Note also that the illustrated components may be distributed in essentially any manner over one or more networks, such as the Internet, one or more local area networks (LANs), wide area networks (WANs), or a combination thereof. The illustrated system includes a speech application , a database  containing PIM data associated with a particular user, a recognition server , a recognition client , a language model (LM) learning unit  (hereinafter simply \u201clearning unit\u201d), a language model compilation server , a database  containing language models, and a language model lookup unit (hereinafter simply \u201clookup unit\u201d) . The system also includes an audio interface  with the user, and one or more other databases, represented by database , storing various other types of data, such as described below. The learning unit  and the lookup unit  may be packaged together in a language model server , as shown in the Figure, although such an implementation is not necessary.","It will be recognized that many of these components can be implemented in software, particularly the speech application , the recognition client , the recognition server , the learning unit , the language model lookup unit , and the compilation server . Accordingly, a software implementation is henceforth assumed in this description to facilitate description. All of the described components may be implemented using conventional hardware, which may include one or more conventional personal computers (PCs), workstations, and\/or hand-held computing devices such as personal digital assistants (PDAs) and cellular telephones. However, it is also contemplated that any of the described components may alternatively be implemented, either partially or entirely, in special-purpose hardwired circuitry.","The PIM database  may be maintained in a conventional Internet portal site, such as Yahoo! or Excite, hereinafter referred to as the \u201cend integrator\u201d. In fact, any or all of the illustrated components may be maintained and operated by a single enterprise, which may be the end integrator.","The data in the PIM database  may include, for example, a user's address book, calendar entries, to do list, or e-mail messages, or any combination thereof. It is contemplated that the system will include a separate set of language models and PIM data for each of multiple users having access to the speech application . However, the operation of the system will be described herein in terms of one user to simplify explanation. Similarly, the architecture of the illustrated system is highly scalable, in that it may include two or more of any of the illustrated components, to provide improved operation of many users.","The speech application  may be any application which makes use of recognized speech of a user to perform its intended operations. The speech application  may be based on the use of, for example, Voice XML (extensible markup language) or Nuance SpeechObjects (provided by Nuance Communications of Menlo Park, Calif.). Note that the present invention facilitates and encourages the creation of new types of speech applications designed to make use of references to PIM data.","The recognition client  handles interactions between the user, the speech application, and the recognition server. The recognition client  handles audio input and output and, if appropriate for the implementation, supports basic telephony control. The recognition client  may also perform endpointing of the user' speech received via the audio interface . The audio output capability of the recognition client  supports the playback of prerecorded prompts to the user via audio interface . The recognition client  may be, for example, the recognition client of the Nuance Speech Recognition System Version 7.0 (\u201cNuance 7.0\u201d), available from Nuance Communications of Menlo Park, Calif.","The audio interface  may be, for example, a conventional telephony connection. Alternatively, the audio interface may be an Internet Protocol (IP) telephony connection, a local microphone input, or any other type of audio connection.","The recognition server  performs speech recognition and natural language understanding of endpointed speech received from the recognition client . Optionally, the recognition server  may also perform speaker verification. The recognition server  may be, for example, the recognition server of Nuance 7.0. To recognize speech and return the natural language interpretation of the spoken utterance, the recognition server  uses a set of language models learned from the PIM data, which are stored in the language models database . The language models may be, for example, speech recognition grammars and\/or statistical language models. The recognition server  also uses a dictionary model and a set of acoustic models, user preferences, settings and other data, collectively represented by database , the nature of which will be recognized those skilled in the art.","In accordance with the present invention, the learning unit  automatically learns, acquires and\/or trains (hereinafter simply \u201clearns\u201d) language models from the PIM data. The language models may be learned on a periodic basis, at specified times, or on-the-fly (as needed) during a session with the user. The learning process may be initiated when a user registers himself with the end integrator or (if different) the speech application. The learning unit  may apply, for example, a grammar induction algorithm to induce grammars based on the PIM data, or it may apply an algorithm for training statistical language models from the PIM data. Grammar induction and training of statistical language models are known to those skilled in the art of automatic speech recognition. Grammar induction is described in, for example, \u201cAutomatic Grammar Induction from Semantic Parsing,\u201d Master's Degree thesis of Debajit Ghosh, Massachusetts Institute of Technology, June 1998. The training of statistical language models is described in, for example, Frederick Jelinek, \u201cStatistical Methods for Speech Recognition, MIT Press, Cambridge, Mass. (1999).","In one embodiment, the learning unit includes a number (N) of plug-in software libraries (\u201cplug-ins\u201d) - through -N. Each of the plug-ins -x contains heuristics tailored for learning language models to be used in accessing a specific type of PIM data (e.g., a personal name, a calendar entry, or an e-mail subject line).","In one embodiment, the language models are dynamic grammars. Dynamic grammars are grammars that can be created and\/or compiled at runtime. The compilation server  is used to convert grammars from their source code (e.g., a grammar scripting language) into a standard node array format used by the recognition server ; this process may be performed dynamically at runtime. The compilation server  may be as provided in Nuance 7.0, for example. Note, however, that the compilation server  is an optional component, which may be omitted from embodiments which do not use dynamic grammars. Note also that the language models can be simply stored as one or more files, i.e., they do not have to be stored in a database.","When the user invokes the speech application , the lookup unit  is triggered by the speech application  to identify and retrieve the appropriate language model and to provide the language model to the recognition server . To trigger this operation, the speech application  indicates to the lookup unit  the name of the current user and the type of language model (e.g., to do list) needed for the current task.","As noted above, the learning unit  and the lookup unit  may be packaged together in a single platform, i.e. a language model server , as shown, although such implementation is optional. As one possible alternative, the functionality of the lookup unit  may be incorporated into the speech application  instead of embodying it as a separate component.","To support the above described functionality, the speech application  and the learning unit  are written to the same set of application programming interfaces (APIs) for accessing PIM data. These APIs define the interfaces and utility classes to be used in learning the language models from the PIM data, selecting a language model during a user interaction, and accessing the PIM data based on the recognized speech. The APIs may include, for example, the \u201cjavax.pim.*\u201d APIs (e.g., javax.pim.addressbook, javax.pim.calendar, and javax.pim.database) described in the JavaPhone API Specification version 1.0, Mar. 22, 2000, and the \u201cjavax.mail.*\u201d APIs described in the JavaMail API Design Specification, version 1.1, August 1998, both of which are available from Sun Microsystems of Palo Alto, Calif. Thus, the language models are automatically learned from the PIM data through this set of APIs, and through the same set of APIs, the PIM data is accessed and used by the speech application . In addition, the lookup unit  is also written to this set of APIs.","In alternative embodiments, the PIM data may be provided in the form of extensible markup language (XML) documents, as specified by a set of XML document type definitions (DTDs). In such embodiments, the speech application  and the learning unit  do not need to comply with a common set of language-specific APIs. This would allow data to be stored in one standard format that is independent of programming language, such that other parties can access and interpret the data. In either case, the DTDs or the common set of code-based APIs are essentially used to implement a programmatic \u201ccontract\u201d between the application developers, end integrators, and speech experts.","Thus, a method and apparatus for improving dialogs between a human and a machine by using language models learned automatically from personalized data have been described. Although the present invention has been described with reference to specific exemplary embodiments, it will be evident that various modifications and changes may be made to these embodiments without departing from the broader spirit and scope of the invention as set forth in the claims. Accordingly, the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense."],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWING","p":"The present invention is illustrated by way of example and not limitation in the Figure of the accompanying drawing, in which like references indicate similar elements, and which illustrates a system for carrying out dialogs between a human and a machine by using language models learned automatically from personal information manager (PIM) data."},"DETDESC":[{},{}]}
