---
title: Managing a codec engine for memory compression/decompression operations using a data movement engine
abstract: A system and method for managing a functional unit in a system using a data movement engine. An exemplary system may comprise a CPU coupled to a memory controller. The memory controller may include or couple to a data movement engine (DME). The memory controller may in turn couple to a system memory or other device which includes at least one functional unit. The DME may operate to transfer data to/from the system memory and/or the functional unit, as described herein. In one embodiment, the DME may also include multiple DME channels or multiple DME contexts. The DME may operate to direct the functional unit to perform operations on data in the system memory. For example, the DME may read source data from the system memory, the DME may then write the source data to the functional unit, the functional unit may operate on the data to produce modified data, the DME may then read the modified data from the functional unit, and the DME may then write the modified data to a destination in the system memory. Thus the DME may direct the functional unit to perform an operation on data in system memory using four data movement operations. The DME may also perform various other data movement operations in the computer system, e.g., data movement operations that are not involved with operation of the functional unit.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07089391&OS=07089391&RS=07089391
owner: Quickshift, Inc.
number: 07089391
owner_city: Austin
owner_country: US
publication_date: 20020823
---

{"@attributes":{"id":"description"},"RELAPP":[{},{}],"heading":["PRIORITY DATA","CONTINUATION DATA","FIELD OF THE INVENTION","DESCRIPTION OF THE RELATED ART","SUMMARY OF THE INVENTION","DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT","INCORPORATION BY REFERENCE"],"p":["This application claims benefit of priority of U.S. provisional patent application Ser. No. 60\/314,538 titled \u201cSystem and Method for Managing a Codec Engine for Memory Compression\/Decompression Operations Using a Data Movement Engine\u201d and filed on Aug. 23, 2001.","This application is a continuation-in-part of U.S. patent application Ser. No. 09\/550,380 titled \u201cParallel Compression\/Decompression System and Method for Implementation of In-Memory Compressed Cache Improving Storage Density and Access Speed for Industry Standard Memory Subsystems and In-Line Memory Modules\u201d filed on Apr. 14, 2000, which is hereby incorporated by reference in its entirety as though fully and completely set forth herein.","The present invention relates to computer systems and\/or memory systems, and more particularly to a system which uses a data movement engine (DME) to manage a compression\/decompression (codec) engine, wherein the codec engine may be embedded on industry standard memory modules, in a memory controller, or other computer system components. The present invention further relates to a system which includes a data movement engine (DME) to manage a functional unit in a memory system, e.g., comprised on a memory module.","System memory architectures and modules have remained relatively unchanged for many years. While memory density has increased and the cost per storage bit has decreased over time, there has not been a significant improvement to the effective operation of the memory subsystem using non-memory devices located within such memory subsystems. The majority of computing systems presently use industry standard in-line modules. These modules house multiple DRAM memory devices for easy upgrade, configuration, and improved density per area.","Software-implemented compression and decompression technologies have also been used to reduce the size of data stored on the disk subsystem or in the system memory data. Current compressed data storage implementations use the system's CPU executing a software program to compress information for storage on disk. However, a software solution typically uses too many CPU compute cycles to operate both compression and decompression in the present application(s). This compute cycle problem increases as applications increase in size and complexity. In addition, there has been no general-purpose use of compression and decompression for in-memory system data. Prior art systems have been specific to certain data types. Thus, software compression has been used, but this technique limits CPU performance and has restricted use to certain data types.","Similar problems exist for programs that require multiple applications of software threads to operate in parallel. Software compression does not address heavy loaded or multithreaded applications, which require high CPU throughput. Other hardware compression solutions have not focused on \u201cin-memory\u201d data (data which reside in the active portion of the memory and software hierarchy). These solutions have typically been I\/O data compression devices located away from the system memory or memory subsystem. In addition, the usage of hardware compression has been restricted to slow, serial input and output devices usually located at the I\/O subsystem.","Mainframe computers have used data compression for acceleration and reduction of storage space for years. These systems require high dollar compression modules located away from the system memory and do not compress in-memory data in the same memory subsystem for improved performance. Such high dollar compression subsystems use multiple separate engines running in parallel to achieve compression speeds at super computer rates. Multiple separate, serial compression and decompression engines running in parallel are cost prohibitive for general use servers, workstations, desktops, or mobile units. Lower cost semiconductor devices have been developed that use compression hardware as well. The main difference is that these devices do not operate fast enough to run at memory speed and thus lack the necessary performance for in-memory data. Such compression hardware devices are limited to serial operation at compression rates that work for slow I\/O devices such as tape backup units. The problem with such I\/O compression devices, other than tape backup units, is that portions of the data to compress are often too small of a block size to effectively see the benefits of compression. This is especially true in disk and network subsystems. To operate hardware compression on in-memory data at memory bus speeds requires over an order of magnitude more speed than present day state-of-the-art compression hardware.","Prior Art Computer System Architecture",{"@attributes":{"id":"p-0009","num":"0008"},"figref":"FIG. 1","b":["100","200","300","100","1","120","2","130","110","135","100","200","135","200","300","200","210","220","230","232","232","620","200","300","235","300","310","320","330","100","200","300"]},"The prior art drawing of  also illustrates the software operating system . The typical operating system (OS) comprises multiple blocks.  shows a few of the relevant OS blocks, including the virtual memory manager (VMM) , file system , and disk drivers .","The operation of prior art systems for storage and retrieval of active and non-active pages from either the system memory or the disk is now described for reference. Again referring to the prior art system of , the VMM  is responsible for allocation of active pages and reallocation of inactive pages. The VMM  defines page fault boundaries  separating the active pages  and the inactive pages  located in both the system memory subsystem  and disk subsystem . An active page may be defined as an area or page of memory, typically 4096 bytes, which is actively used by the CPU during application execution. Active pages reside between or within system memory or CPU cache memory. An inactive page may be defined as an area or page of memory, typically 4096 bytes, which is not directly accessed by the CPU for application execution. Inactive pages may reside in the system memory, or may be stored locally or on networks on storage media such as disks. The page fault boundary  is dynamically allocated during run time operation to provide the best performance and operation as defined by many industry standard algorithms such as the LRU\/LFU lazy replacement algorithm for page swapping to disk. As applications grow, consuming more system memory than the actual available memory space, the page fault boundaries  are redefined to store more inactive pages  in the disk subsystem  or across networks. Thus, the VMM  is responsible for the placement of the page fault boundary  and the determination of active pages  and inactive pages , which reside in memory and on the disk subsystem .","The file system software , among other tasks, and along with the disk drivers , are responsible for the effective movement of inactive pages between the memory subsystem  and the disk subsystem . The file system software  may have an interface which is called by the VMM  software for the task of data movement to and from the computer disk and network subsystems. The file system  software maintains file allocation tables and bookkeeping to locate inactive pages that have been written to disk. In order for the file system to operate, the file system calls the software disk drivers  for DMA control of data movement and physical disk control. Instructions are programmed into the disk controller  of the disk subsystem  by the file system  software. Thus, when application data exceeds the available system memory space, the VMM  allocates and reallocates active and inactive pages for best operation of application data and instructs the file system  to instruct the disk driver  to carry out the DMA operation and page movement tasks.","For the purpose of this disclosure, it is helpful to understand the relative read and write time requirements for CPU read and write operation to or from each of the subsystems , , and . For example, for the CPU subsystem , a read and write operation to or from the L or L cache memory is on the order of tens of nanoseconds. A CPU  read\/write from\/to the memory subsystem  is on the order of hundreds of nanoseconds. A CPU read or write and\/or a memory controller DMA read or write to the disk subsystem  is on the order of milliseconds. To move a page (typically 4096 bytes) from the inactive page  area to the active page  by the CPU  typically requires 3 \u03bcs for the page fault software plus 7 \u03bcs for the data move, or 10 \u03bcs of overhead. For the DMA controller, typically located in the memory controller , to read or DMA a page from disk cache  requires about 1 ms, while movement of a page to physical disk requires about 10 ms. Thus, the data transfer time from disk subsystem  to memory subsystem  is about three orders of magnitude longer than from memory subsystem  to CPU subsystem  L\/L cache \/ memory. This represents an area of desired improvement. In addition, the speed of CPU reads\/writes to and from the memory subsystem  is also an area of desired improvement.","Certain prior art systems utilize multiple compression and decompression devices to achieve faster compression rates for I\/O data sent and stored on disk. No prior art currently exists which uses in-line memory compression technology at the memory interface or on memory modules to achieve improved system performance. Therefore, a new system and method is desired to improve overall memory performance, including a reduction in the effective page swap time overhead as seen in present day computing systems. The present invention addresses these problems in a unique and novel hardware and software architecture.","One embodiment of the invention comprises a system and method for managing a functional unit in a system using a data movement engine.","An exemplary system may comprise a CPU coupled to a memory controller. The memory controller may include or couple to a data movement engine (DME). The memory controller may in turn couple to a system memory, which may comprise one or more memory modules. The system memory may also include at least one functional unit. The functional unit may be comprised on a memory module of the system memory or may be comprised in other portions of the system memory. In one embodiment, the functional unit may be comprised on the computer motherboard and positioned between the memory controller and the memory modules or system memory. In one embodiment, a plurality of functional units may be comprised in the system memory. For example, each memory module may include a functional unit. The functional unit may also be comprised on another device, such as a network device, graphics device, etc.","The memory controller may couple to an expansion bus such as a PCI bus. Various devices may be coupled to the expansion bus such as a non-volatile memory, a video device and an I\/O device such as a network interface card.","The memory controller may be comprised in computer chipset logic, such as North Bridge and\/or South Bridge logic, or future implementations of chipset logic. The DME engine may be comprised in the chipset logic, and is preferably comprised in or closely coupled to the memory controller. In one embodiment, the DME may be external to, and possibly closely coupled to the memory controller or the chipset logic. For example, the DME may be comprised on the computer motherboard coupled to the chipset logic.","The DME may be any of various types of data movement engines. In one embodiment, the DME may include functionality of a DMA (direct memory access) engine, or an enhanced DMA engine. The DME may operate to transfer data to\/from the system memory and\/or the functional unit, as described herein. In one embodiment, the DME may also include multiple DME channels or multiple DME contexts.","The DME may operate to direct the functional unit to perform operations on data in the system memory. For example, the DME may read source data from the system memory, the DME may then write the source data to the functional unit, the functional unit may operate on the data to produce modified data, the DME may then read the modified data from the functional unit, and the DME may then write the modified data to a destination in the system memory. Thus the DME may direct the functional unit to perform an operation on data in system memory using four data movement operations.","The DME may also perform various other data movement operations in the computer system, e.g., data movement operations that are not involved with operation of the functional unit. For example, the DME may perform data movement operations between the system memory and the non-volatile memory, between the system memory and the I\/O device, between the CPU and any of various devices, etc. In general, the DME may be operable to transfer or move data between any two devices in the computer system. The DME may operate to perform data movement operations under control of (at the direction of) software executing on the CPU. The DME may also operate to perform data movement operations independently of the CPU, thus allowing the CPU to perform processing operations in parallel or concurrently with the data movement operations of the DME.","The functional unit may be any of various types of processing or functional devices. In the preferred embodiment of the invention, the functional unit is a compression\/decompression (codec) engine for compressing and\/or decompressing data written to\/from the system memory, or other devices. In an alternate embodiment, the functional unit may be an encryption\/decryption engine for encrypting\/decrypting data as it is transferred to\/from the system memory. In another embodiment, the functional unit may comprise a digital signal processor (DSP) for performing signal processing functions on data. In another embodiment, the functional unit may include several functions. In other words, the system may include multiple functional units. For example, the system memory may include both a codec engine and an encryption\/decryption engine.","The functional unit may be comprised in any of various components of the computer system. For example, the functional unit may be comprised in the system memory (e.g., on a memory module), in the memory controller, on an I\/O device, such as a network interface card, on a video device, or other location. The methods described herein for configuring and operating the DME to read\/write data from\/to the functional unit to accomplish various operations may be performed regardless of the location of the functional unit.","In one embodiment, the functional unit is a compression engine and\/or decompression engine comprised in the system memory, e.g., on a memory module. In this embodiment, the CPU may configure the DME for compression and\/or decompression operations.","For a compression or decompression operation, the CPU may create one or more commands which specify compression or decompression, respectively, of first data. Each of the commands may comprise at least one source address of the first data and at least one destination address. The CPU may create these commands in the system memory, or may provide these commands directly to registers in the DME. The CPU may create the one or more commands in response to software execution. For example, the CPU may create the one or more commands in response to a virtual memory manager (VMM) selecting the first data for removal from the system memory, for restoration to the system memory, or for in-memory compression or decompression. The CPU may also create the one or more commands in response to a software driver executing on the CPU. The DME may then access the one or more commands from system memory and execute the commands.","In a compression operation, during execution of each command, the DME may read first data from the system memory and write the first data to the compression engine. The compression engine included on the at least one memory module may then compress the first data to form first compressed data. The DME may then read the first compressed data from the at least one memory module and write the first compressed data to the system memory, e.g., to a compressed portion of system memory. Alternatively, the DME may write the first compressed data to the nonvolatile memory. As another operation, after writing the first compressed data to the system memory, the DME may transfer the first compressed data from the system memory to the nonvolatile memory, which involves the DME reading the first compressed data from the system memory and writing the first compressed data to the nonvolatile memory.","In a decompression operation, during execution of each command, the DME may read first compressed data from the system memory and write the first compressed data to the at least one memory module. The decompression engine included on the at least one memory module may then decompress the first compressed data to form the first data and may transfer the first data to the system memory, e.g., to an uncompressed portion of system memory. Alternatively, the DME may write the first data to the nonvolatile memory. As another operation, after writing the first data to the system memory, the DME may transfer the first data from the system memory to the nonvolatile memory, which involves the DME reading the first data from the system memory and writing the first data to the nonvolatile memory.","As another operation, the decompression engine included on the at least one memory module may decompress first compressed data to form the first data and the DME may transfer the first data from the system memory to the nonvolatile memory. The first compressed data may then be removed from the system memory. Alternatively, the compression engine included on the at least one memory module may compress first data to form first compressed data and the DME may transfer the first compressed data from the system memory to the nonvolatile memory. The first data may then be removed from the system memory.","The removal of data (compressed or uncompressed) from the system memory may be performed based on certain metrics. For example, the CPU may calculate a usage rate of the compressed data and compare the usage rate of the compressed data to a minimum usage rate. The compressed data is removed from the system memory when the usage rate of the compressed data being below the minimum usage rate.","In one embodiment, the compression engine comprises an input buffer and an output buffer. The DNE operates to write data to be compressed to the input buffer. The compression engine compresses the data and stores the resultant compressed data in the output buffer. The DME may then read the resultant compressed data from the output buffer. The decompression engine may also comprise an input buffer and an output buffer. The DME operates to write data to be decompressed to the input buffer. The decompression engine decompresses the data and stores the resultant uncompressed data in the output buffer. The DME may then read the resultant uncompressed data from the output buffer.","In one embodiment, the system further includes a virtual memory manager (VMM), a compressed memory manager for managing the first compressed portion of the system memory, and a driver for managing the data movement engine;\n\n","In one embodiment, the compression engine is a parallel compression engine which operates to analyze and compress a plurality of symbols from the first data in parallel to form the first compressed data. The parallel compression engine may operate to compare each of a plurality of symbols with each of a plurality of entries in a history table concurrently.","In one embodiment, the functional unit is comprised in the memory controller (or in the chipset logic). The functional unit may be coupled closely to the DME, which also may be comprised in the memory controller (or in the chipset logic). The placement of the functional unit proximate to the DME may operate to simplify data movement operations between the DME and the functional unit and\/or system memory. For example, the DME may read source data from the system memory, the source data may be received into the functional unit in the memory controller, the functional unit may operate on the data to produce modified data, and then the DME may then write the modified data back to the system memory. Thus the DME may direct the functional unit to perform an operation on data in system memory using only two data movement operations.","In one embodiment, multiple memory channels are coupled between the memory controller and the system memory and are used for concurrent data movement operations of reads and writes between the DME and the system memory and\/or the functional unit. In one embodiment, the DME may also include multiple DME channels or multiple DME contexts. The multiple DME channels or multiple DME contexts may be used in conjunction with the multiple memory channels.","In one embodiment, the system of  may include at least one memory channel between the memory controller and the system memory and at least one memory channel between the memory controller and the functional unit. In another embodiment, the system may include a plurality of memory channels between the memory controller and the system memory and a plurality of memory channels between the memory controller and the functional unit.","Thus, in one embodiment, the DME may be operable to read source data from the system memory concurrently with writing source data to the functional unit. The DME may also be operable to read modified data from the functional unit concurrently with writing modified data to the system memory. The DME may also be operable to write source data to the functional unit concurrently with reading modified data from the functional unit. Thus, in one embodiment, the DME may be operable to perform 2, 3, or 4 data movement operations concurrently or in parallel. The DME may also be operable to perform other data movement operations involving other devices concurrently or in parallel with data movement operations involving the functional unit.","In one embodiment, the DME may be operable to read source data from the system memory into the functional unit concurrently with writing modified data from the functional unit to the system memory. Thus, in one embodiment, the DME may be operable to perform 2 (or more) data movement operations concurrently or in parallel. The DME may also be operable to perform other data movement operations involving other devices concurrently or in parallel with data movement operations involving the functional unit.","Various other embodiments are contemplated which involve one or more DMEs and one or more functional units, wherein the DMEs and functional units may be distributed among any of various locations in the system. For example, the system may comprise a plurality of DMEs which communicate with a single functional unit. Alternatively, the system may comprise a single DME which communicates with a plurality of functional units. As another example, the system may comprise a plurality of DMEs which communicate with a plurality of functional units.","U.S. provisional patent application Serial No. 60,314,538 titled \u201cSystem and Method for Managing a Codec Engine for Memory Compression\/Decompression Operations Using a Data Movement Engine\u201d and filed on Aug. 23, 2001, is hereby incorporated by reference in its entirety as though fully and completely set forth herein.","U.S. patent application Ser. No. 09\/550,380 titled \u201cParallel Compression\/Decompression System and Method for Implementation of In-Memory Compressed Cache Improving Storage Density and Access Speed for Industry Standard Memory Subsystems and In-Line Memory Modules\u201d filed on Apr. 14, 2000, is hereby incorporated by reference in its entirety as though fully and completely set forth herein.","Bandwidth Reducing Memory Controller Including Scalable Embedded Parallel Data Compression and Decompression Engines\u201d whose inventors are Thomas A. Dye, Manuel J. Alvarez II and Peter Geiger and was filed on Jan. 29, 1999, is hereby incorporated by reference in its entirety as though fully and completely set forth herein.","U.S. Pat. No. 6,208,273 titled \u201cSystem and Method for Performing Scalable Embedded Parallel Data Compression\u201d whose inventors are Thomas A. Dye, Manuel J. Alvarez II and Peter Geiger and was filed on Oct. 20, 1999, is hereby incorporated by reference in its entirety as though fully and completely set forth herein.","U.S. patent application Ser. No. 09\/491,343 titled \u201cSystem and Method for Performing Scalable Embedded Parallel Data Decompression\u201d whose inventors are Thomas A. Dye, Manuel J. Alvarez II and Peter Geiger and was filed on Jan. 26, 2000, is hereby incorporated by reference in its entirety as though fully and completely set forth herein.","U.S. Pat. No. 6,002,411 titled \u201cIntegrated Video and Memory Controller with Data Processing and Graphical Processing Capabilities\u201d whose inventor is Thomas A. Dye and which was issued Dec. 14, 1999, is hereby incorporated by reference in its entirety as though fully and completely set forth herein.","U.S. patent application Ser. No. 08\/916,464 titled \u201cMemory Controller Including Embedded Data Compression and Decompression Engines\u201d whose inventor is Thomas A. Dye and which was filed Aug. 8, 1997, is hereby incorporated by reference in its entirety as though fully and completely set forth herein.","Computer Architecture of the Preferred Embodiment","FIG. \u2014Exemplary Computer System",{"@attributes":{"id":"p-0063","num":"0066"},"figref":"FIG. 2"},"FIGS. -: Exemplary Computer Block Diagrams",{"@attributes":{"id":"p-0064","num":"0067"},"figref":["FIGS. 3-6","FIGS. 3-6"]},{"@attributes":{"id":"p-0065","num":"0068"},"figref":"FIG. 3A","b":["100","211","211","204","211","218","218","218","216","216","218"]},"In one embodiment, a plurality of functional units  may be comprised in the system memory . For example, each memory module may include a functional unit .","The memory controller  may couple to an expansion bus such as a PCI bus . Various devices may be coupled to the expansion bus such as a non-volatile memory , a video device and an I\/O device such as a network interface card.","The memory controller  may be comprised in computer chipset logic, such as North Bridge and\/or South Bridge logic, or future implementations of chipset logic. The DME engine  may be comprised in the chipset logic, and is preferably comprised in or closely coupled to the memory controller . In one embodiment, the DME may be external to, and possibly closely coupled to the memory controller  or the chipset logic. For example, the DME  may be comprised on the computer motherboard coupled to the chipset logic.","The DME  may be any of various types of data movement engines. In one embodiment, the DME  may include functionality of a DMA (direct memory access) engine, or an enhanced DMA engine. The DME  may operate to transfer data to\/from the system memory  and\/or the functional unit , as described in detail below. In one embodiment, the DME  may also include multiple DME channels or multiple DME contexts.","The DME  may operate to direct the functional unit  to perform operations on data in the system memory . For example, the DME  may read source data from the system memory , the DME  may then write the source data to the functional unit , the functional unit  may operate on the data to produce modified data, the DME  may then read the modified data from the functional unit , and the DME  may then write the modified data to a destination in the system memory . Thus the DME  may direct the functional unit  to perform an operation on data in system memory  using four data movement operations.","The DME  may also perform various other data movement operations in the computer system, e.g., data movement operations that are not involved with operation of the functional unit. For example, the DME may perform data movement operations between the system memory  and the non-volatile memory , between the system memory  and the I\/O device, between the CPU  and any of various devices, etc. In general, the DME  may be operable to transfer or move data between any two devices in the computer system. The DME  may operate to perform data movement operations under control of (at the direction of) software executing on the CPU. The DME  may operate to perform data movement operations independently of the CPU , thus allowing the CPU  to perform processing operations in parallel or concurrently with the data movement operations of the DME .","The functional unit  may be any of various types of processing or functional devices. In the preferred embodiment of the invention, the functional unit  is a compression\/decompression (codec) engine for compressing and\/or decompressing data written to\/from the system memory , or other devices. In an alternate embodiment, the functional unit  may be an encryption\/decryption engine for encrypting\/decrypting data as it is transferred to\/from the system memory. In another embodiment, the functional unit  may comprise a digital signal processor (DSP) for performing signal processing functions on data. In another embodiment, the functional unit  may include several functions. In other words, the system may include multiple functional units. For example, the system memory  may include both a codec engine and an encryption\/decryption engine.","The functional unit  may be comprised in any of various components of the computer system. In the embodiment shown in , the functional unit  is comprised in the system memory . In another embodiment, the functional unit  may be comprised in the memory controller . In another embodiment, the functional unit  may be comprised on an I\/O device, such as a network interface card. In another embodiment, the functional unit  may be comprised in a video device. The methods described herein for configuring and operating the DME  to read\/write data from\/to the functional unit  to accomplish various operations may also be performed where the functional unit is comprised on an I\/O device such as a network interface device. Thus, in one embodiment, the functional unit  comprises a codec engine (or an encryption\/decryption unit) comprised on a network interface card of the computer system.",{"@attributes":{"id":"p-0074","num":"0077"},"figref":["FIG. 3B","FIG. 3B"],"b":["3","216","211","216","204","216","204","204","216","218"]},"For example, the DME  may read source data from the system memory , the source data may be received into the functional unit  in the memory controller A, the functional unit  may operate on the data to produce modified data, and then the DME  may then write the modified data back to the system memory A. Thus the DME  may direct the functional unit  to perform an operation on data in system memory  using only two data movement operations.",{"@attributes":{"id":"p-0076","num":"0079"},"figref":["FIGS. 4A and 4B","FIGS. 3A and 3B"],"b":["211","218","204","218","216","204"]},"In one embodiment, the system of  may include at least one memory channel between the memory controller  and the system memory  and at least one memory channel between the memory controller  and the functional unit . In another embodiment, the system of  may include a plurality of memory channels between the memory controller  and the system memory  and a plurality of memory channels between the memory controller  and the functional unit .","Thus, in the embodiment of , the DME  may be operable to read source data from the system memory  concurrently with writing source data to the functional unit . The DME  may also be operable to read modified data from the functional unit  concurrently with writing modified data to the system memory . The DME  may also be operable to write source data to the functional unit  concurrently with reading modified data from the functional unit . Thus, in the embodiment of , the DME  may be operable to perform 2, 3, or 4 data movement operations concurrently or in parallel. The DME  may also be operable to perform other data movement operations involving other devices concurrently or in parallel with data movement operations involving the functional unit .","In the embodiment of , the DME  may be operable to read source data from the system memory  into the functional unit  concurrently with writing modified data from the functional unit  to the system memory . Thus, in the embodiment of , the DME  may be operable to perform 2 (or more) data movement operations concurrently or in parallel. The DME  may also be operable to perform other data movement operations involving other devices concurrently or in parallel with data movement operations involving the functional unit .","Various other embodiments are contemplated which involve one or more DMEs  and one or more functional units , wherein the DMEs  and functional units  may be distributed among any of various locations in the system. For example, the system may comprise a plurality of DMEs  which communicate with a single functional unit . Alternatively, the system may comprise a single DME  which communicates with a plurality of functional units . As another example, the the system may comprise a plurality of DMEs  which communicate with a plurality of functional units .",{"@attributes":{"id":"p-0081","num":"0084"},"figref":["FIGS. 5A and 5B","FIGS. 5A and 5B"],"b":["3","3","216","250"]},"In the embodiment of , the DME  may operate to direct the codec engine  to perform operations on data in the system memory . For example, in a data compression operation, the DME  may read source data from the system memory , the DME  may then write the source data to the codec engine , the codec engine  may compress the data to produce compressed data, the DME  may then read the compressed data from the codec engine , and the DME  may then write the compressed data to a destination in the system memory . Thus the DME  may direct the codec engine  to compress data in system memory  using four data movement operations. For example, in a data decompression operation, the DME  may read compressed source data from the system memory , the DME  may then write the compressed source data to the codec engine , the codec engine  may decompress the compressed data to produce uncompressed data, the DME  may then read the uncompressed data from the codec engine , and the DME  may then write the uncompressed data to a destination in the system memory . Thus the DME  may direct the codec engine  to decompress data in system memory  using four data movement operations.","In the embodiment of , the placement of the codec engine  proximate to the DME  may operate to simplify data movement operations between the DME  and the codec engine  and\/or system memory . For example, in a data compression operation, the DME  may read source data from the system memory , the source data may be received into the codec engine  in the memory controller A, the codec engine  may compress the data to produce compressed data, and the DME  may then write the compressed data back to the system memory A. Thus the DME  may direct the codec engine  to compress data in system memory  using only two data movement operations. For example, in a data decompression operation, the DME  may read compressed source data from the system memory , the compressed source data may be received into the codec engine  in the memory controller A, the codec engine  may decompress the compressed source data to produce uncompressed data, and the DME  may then write the uncompressed data back to the system memory A. Thus the DME  may direct the codec engine  to decompress data in system memory  using only two data movement operations.","The following describes embodiments of the present invention where the functional unit  is a codec engine  for compressing and\/or decompressing data in (or to\/from) the system memory  (e.g., for compressing and\/or decompressing data being transferred between the system memory  and other devices in the computer system). However, the methods and operations described herein may also be applied where the functional unit  performs different operations, such as encryption and decryption, signal processing operations and other types of operations.","FIG. \u2014Computer System with Compressed Cache",{"@attributes":{"id":"p-0085","num":"0088"},"figref":["FIG. 6","FIG. 6","FIG. 6"],"b":["100","200","300","100","200","300","100","110","1","120","2","130","200","211","218","200","204","204","211","218","220","230","240","240","200"]},"Software executing on the CPU  may allocate a compressed cache (CC) . The CC  may be allocated within the normal memory map of the computer system. Compressed pages may be stored in the CC . The CC  may be allocated within system memory (also referred to as main memory or RAM) comprised of one or more volatile memory devices such as C-DIMMs (described below), DIMMs, SIMs, SDDIMMs, RIMMs, or other types of memory modules. Pages are generally 4096 bytes. In alternate embodiments, page sizes can be any size as desired by the operating system software. Instead of swapping inactive pages to the nonvolatile memory, the system and method may operate to store inactive pages in a compressed format in the CC . In addition, pages from the CC , which are maintained in compressed format, can be moved to disk or network in such format for future data storage, retrieval, or transmission over LANs or WANs. Thus, a second order benefit is achieved by storage of compressed pages in the I\/O subsystem  instead of non-compressed pages.","In one embodiment, three software programs or modules may be used in the system: the compressed cache manager (CCM), the compressed disk manager (CDM) and the C-DIMM device driver (DDD) (FIG. ). In addition, these programs may be used individually or in combination as required by operating system software or application software.","In one embodiment, the CCM, CDM and DDD may be stand-alone software programs that operate independently of the Virtual Memory Manager (VMM). In this embodiment, the CCM and CDM may operate to direct the DME , in conjunction with codec engine , to compress pages being managed by the VMM, cache or store the compressed pages in a compressed cache and\/or CPAT cache, and decompress and write pages to system memory being managed by the VMM in response to read requests from the VMM. These various operations may be performed substantially invisibly to the VMM. In other words, the VMM may have no knowledge of or control over the compression, storing, and decompression of pages.","The CCM and CDM may receive I\/O requests from the VMM, examine the I\/O requests, and direct the DME , possibly in conjunction with codec engine , to perform various operations in response to the I\/O requests. To the VMM, it appears that invalidated pages are written to nonvolatile storage and requested pages are read from nonvolatile storage and returned to system memory as in prior art systems.","In other contemplated embodiments, one or more of the operations of the CCM and CDM software programs may be integrated into a virtual memory manager (VMM). In these embodiments, an enhanced VMM may directly instruct or control the DME , in conjunction with codec engine . The enhanced VMM may directly instruct or control the DME  to initiate the compression and caching or storing of pages to a compressed cache and\/or CPAT cache, and possibly to initiate the decompression and reading back into system memory of previously cached or stored pages. In one contemplated embodiment, the CCM and CDM modules may not be necessary, as all of their operations are fully incorporated in an enhanced VMM.","In various embodiments, the DME  and memory controller  may be integrated into the processor subsystem . In other embodiments, the VMM may execute partially or completely in the memory subsystem .","FIG. \u2014A Computer System with a C-DIMM",{"@attributes":{"id":"p-0092","num":"0095"},"figref":["FIG. 7","FIG. 6","FIG. 7","FIGS. 3A"],"b":["550","4","5","200","211","550"]},"The C-DIMM devices  and the DIMM modules may be partitioned by the system software into active pages , inactive pages , and compressed pages , each of which make up components of the total system memory.","The Compression enabled Dual-Inline-Memory Module (C-DIMM ) includes a codec engine, shown as compactor chip . In the preferred embodiment, the C-DIMM  includes parallel compression and decompression technology which operates to increase the performance or reduce the cost of computers, including servers and workstations, by increasing the effective size and\/or speed of memory. In one embodiment, the compactor chip  utilizes a method of using fast parallel compression and decompression technology as outlined in U.S. Pat. No. 6,208,273 and patent application Ser. Nos. 09\/239,658, and 09\/491,343 as referenced above. In the embodiment of , the fast parallel compression and decompression operations are accomplished by mounting the compactor chip  preferably into a memory device or module, such as an industry standard DIMM, SIMM or SODIMM or RIMM module. The C-DIMM  thus may comprise an industry standard memory module including the compression\/decompression chip . The memory module is then compression\/decompression enabled and may be plugged into the system memory subsystem  of the computing system.","In the preferred embodiment, the compactor chip  acts as a compression and decompression co-processor. The DME  operates to transfer data to\/from the compactor chip  and control the compactor chip  under the direction of the application or C-DIMM driver and other software modules.","As indicated in , under direction of the C-DIMM software modules, the DME  operates to perform transfers to compress inactive pages  and store the inactive pages in memory in a compressed format. This may be accomplished by a DME read of the non-compressed inactive page  followed by a DME write to the C-DIMM memory aperture by the CPU. The C-DIMM memory aperture refers to a memory space, preferably on a C-DIMM memory module, wherein data in the C-DIMM memory aperture may be operated on by the compactor chip . The compactor chip  may operate to compress and\/or decompress data in the C-DIMM memory operation. In response to the page write into the C-DIMM aperture, data is preferably compressed by the compactor chip  and stored in a C-DIMM read aperture. Then the DME  may read the compressed data from the C-DIMM read aperture and write the compressed page to the compressed cache  memory area. In this case, the VMM  may believe that the data was transferred to disk whereas in reality the data was compressed and stored in the CC  in system memory.","When the virtual memory manager (VMM)  requests a page from disk, the C-DIMM software modules may first examine the CC  for resident compressed pages. If the compressed page is resident within the CC , the C-DIMM software driver  may instruct the DME  to read the inactive compressed page from the CC  and write the inactive compressed page to the compactor chip  for decompression. The DME  may then read the decompressed page (new active page) from the compactor chip  and write the new active page into the area designated by the VMM . To the VMM  and file system , this process looks like a standard disk transfer of a page that had previously been swapped to disk and is read back to the active area of system memory. However, this was actually a fast decompression and read from the CC to the designated active page  memory area. Thus, the use of the DME , the compactor chip  and the allocation of the compressed cache  enables orders of magnitude faster response time when reading inactive, cached pages into the main memory  active page  memory area.","In alternate embodiments, the compactor chip may be \u201cin-line\u201d with the data transfer such that data is compressed or decompressed as it traverses between the CPU subsystem  and the memory subsystem . The fast parallel compression and decompression operations described above make the \u201cin-line\u201d embodiments feasible, as the compression and decompression of data are performed in \u201creal-time.\u201d In one embodiment with an inline compactor chip, substantially all data transfers from the CPU subsystem  to the memory subsystem  are compressed by the compactor chip, and substantially all data transfers from the memory subsystem  to the CPU subsystem  are decompressed.","FIG. \u2014A Computer System with a Codec Engine in the Memory Controller",{"@attributes":{"id":"p-0099","num":"0102"},"figref":["FIG. 8","FIG. 6","FIG. 8","FIGS. 3B"],"b":["250","250","211","4","5"]},"As noted above, the system memory  may be partitioned by the system software into active pages , inactive pages , and compressed pages , each of which make up components of the total system memory.","As shown, the chipset logic, e.g., the system memory controller  includes both the DME  and the codec engine, shown as compactor chip . In the preferred embodiment, as noted above, the codec engine  includes parallel compression and decompression technology as outlined in U.S. Pat. No. 6,208,273 and patent application Ser. Nos. 09\/239,658, and 09\/491,343 as referenced above. In the embodiment of , the fast parallel compression and decompression operations are accomplished by mounting the compactor chip  into the memory controller  or other chipset logic.","In this embodiment, the compactor chip  acts as a compression and decompression co-processor. The DME  operates to transfer data to\/from the compactor chip , and transfer data between the compactor chip  and the system memory . The DME operates to control the compactor chip  under the direction of the application or C-DIMM driver and other software modules.","In one embodiment, the DME  and the compactor chip  are tightly integrated as a single unit. Thus the DME may be a \u201ccompression enhanced\u201d DME for performing data compression, data decompression, and data movement operations.","As indicated in , under direction of software executing on the CPU , the DME  operates to perform transfers to compress inactive pages  and store the inactive pages in memory in a compressed format. This may be accomplished by a DME read of the non-compressed inactive page , followed by the compactor chip  compressing and\/or decompressing the received data. Then the DME  may transfer the compressed data from the compactor chip  to the compressed cache  memory area. In this case, the VMM  may believe that the data was transferred to disk whereas in reality the data was compressed and stored in the CC  in system memory.","When the virtual memory manager (VMM)  requests a page from disk, the software program executing on the CPU  may first examine the CC  for resident compressed pages. If the compressed page is resident within the CC , the software driver may instruct the DME  to read the inactive compressed page from the CC , provide the inactive compressed page to the compactor chip  for decompression, and then transfer the decompressed page (new active page) from the compactor chip  into the area designated by the VMM . To the VMM  and file system , this process looks like a standard disk transfer of a page that had previously been swapped to disk and is read back to the active area of system memory. However, this was actually a fast decompression and read from the CC to the designated active page  memory area. Thus, the use of the DME , the compactor chip  and the allocation of the compressed cache  enables orders of magnitude faster response time when reading inactive, cached pages into the main memory  active page  memory area.","In alternate embodiments, as noted above, the compactor chip may be \u201cin-line\u201d with the data transfer such that data is compressed or decompressed as it traverses between the CPU subsystem  and the memory subsystem , e.g., as data traverses between the memory controller  and system memory . The fast parallel compression and decompression operations described above make the \u201cin-line\u201d embodiments feasible, as the compression and decompression of data are performed in \u201creal-time.\u201d In one embodiment with an in-line compactor chip, substantially all data transfers from the CPU subsystem  to the memory subsystem  are compressed by the compactor chip , and substantially all data transfers from the memory subsystem  to the CPU subsystem  are decompressed.","FIG. \u2014A Software Stack with Compressed Cache and Compressed Disk Drivers",{"@attributes":{"id":"p-0107","num":"0110"},"figref":["FIG. 9","FIG. 9"],"b":["600","250","250"]},"The virtual memory manager (VMM) , the file system (FS) , and the disk drivers (DD)  make up the conventional prior art operating system stack  for control of the memory subsystem  and the disk subsystem . In one embodiment, to enable the codec engine , e.g., C-DIMM , for maximum system performance, four blocks are added to the operating system software stack: the file system filter (FSF) , the compressed cache manager (CCM) , the compressed disk manager (CDM)  and the DME device driver (DDD) . The FSF , the CCM , the CDM , and the DDD  may work together to control data moved by the DME . Together these software modules introduce a second level of abstraction to the file system for manipulation of compressed pages in memory and on disk. In addition, alternate embodiments may include control of the DME  for compressed page transfers across networks and other communication mediums, or may include use with other functional units  which implement algorithms such as data encryption and decryption or other types of in-memory system acceleration.","The software stack may be similar in some operation as the software stack described in the patent applications referenced above. However, in the present embodiment, the CCM is operable to work with physical addresses, not logical addresses. Thus the DME  can operate directly with physical memory addresses, avoiding PTE look-ups.","FIG. \u2014Software and Hardware Interfaces for the C-DIMM Component Modules",{"@attributes":{"id":"p-0110","num":"0113"},"figref":["FIG. 10","FIG. 10"],"b":["100","200","300","600","9","620","220","230","200","620","300","300","220","230","720","620","240"]},"The system may dynamically determine the amount of compressed cache  memory that is needed and may dynamically adjust the allocation size of the compressed cache . This dynamic adjustment algorithm may use a history of the page swapping operation over a short period under direction of the file system  software. The system may use a novel page snooping algorithm, performed by the compressed cache manager  block, which examines the number of I\/O store and restore requests as a function of time. Thus, when the compressed cache manager (CCM)  sees that pages are not being stored as inactive pages  (less I\/O subsystem activity), or onto the disk subsystem , then the CCM  software may not allocate system memory to compressed cache pages . In the above dynamic allocation algorithm, the compactor chip  may not be used as it is assumed that the application fits into the memory subsystem  without the need for virtual storage onto disk. As more applications are instantiated, or current applications grow, requiring more memory than available from the memory subsystem , the CCM \/ may dynamically require the file system to allocate additional memory for the compressed cache . Thus, pages are moved by the DME  under direction of the file system  and the compressed cache manager  between the inactive uncompressed page  memory area and the inactive compressed page  memory area. This procedure may be used during the reallocation of active pages  to inactive pages  where such pages are targeted for disk storage within the disk subsystem . Thus, for active pages that are targeted by the VMM  for storage to the disk subsystem  by the file system software , the system uses the compressed cache manager \/, the DME device driver , the DME , and the codec engine , e.g., C-DIMM  hardware, to compress and store such pages into the local compressed cache  instead of into the disk subsystem .","In addition, the compressed cache allocation may use a set of novel cache algorithms to optimize compressed storage for the most active of the stale pages normally swapped to disk. In other words, based on the algorithm of one embodiment, pages that show a history of reuse may be compressed and stored in the compressed cache , while pages that show little history of reuse may be compressed and swapped to the compressed page partition in the disk subsystem . Thus, as the compressed cache  memory becomes full, the dynamic algorithm of the compressed cache manager  tags compressed pages according to a novel least recently used, lazy replacement LRU\/LZU algorithm and retires low utilization compressed pages into the disk subsystem . In an alternate embodiment, the compressed pages that are stored onto the disk subsystem  may not be stored in a compressed partition but still may be compressed for faster file transfer and bus I\/O bandwidth improvements.","In order to accomplish storage of compressed pages to disk, the system uses another unique filter, the compressed disk manager . The compressed disk manager  is a software module that may be used like a secondary File Allocation Table (FAT) specifically designed for compressed pages. Thus, the system may retire compressed pages located in the compressed cache  buffer to the disk subsystem  for storage. In addition, if the compressed disk manager  is not installed, the compressed cache manager  may call the codec driver  to instruct the DME  to decompress a stale page prior to storage in the disk subsystem . Thus, the performance enhancement when using the compressed disk manager  has clear advantages over prior art disk compression technologies.","One embodiment of the present invention also may decompress pages into the system memory subsystem  active page region  from either the compressed cache  memory area or directly from the disk subsystem . This process also may require the requests from the operating system software's virtual memory manager  for a page of data that the VMM  thinks resides in the disk subsystem . When a retired page is requested to be reissued to the active page  area of the memory subsystem , the compressed cache manager \/ searches its compressed cache  buffer allocation tables in order to see if the requested page is resident in the system memory  subsystem, or if the requested page may need to be restored from the disk subsystem .","When the compressed page is identified by the CCM \/ as being local to the system memory subsystem , the process of decompression and page write to the active page area  of memory begins. This may be accomplished in a number of steps. First, the compressed cache manager  may translate the page address from the file system  into an address pointer into the compressed cache  memory area. Second, the codec driver  is invoked by the CCM  to configure command blocks to be executed by the DME . The DME  may execute these command blocks to read the compressed page from the compressed cache  memory area and begin the decompression process. Third, once the decompression of the compressed page is complete, the DME  may move the decompressed page to the active page  region of the memory subsystem . The CCM  is then notified that the page decompression and move process has completed by the DME , or by the DME device driver . The CCM \/ then finishes the operation by notification to the file system  and finally the VMM  is notified that the page is in active page  region and is ready for process by the resident application software. Additionally, multiple pages can be strung together such that the above steps are concatenated to streamline the process steps.","In one embodiment, when a compressed page is identified as requiring decompression to the active page area, the system identifies or anticipates other pages that may require decompression in the future, using a type of pre-fetch mechanism.","When the compressed page is not identified by the CCM \/ as being resident to the local compressed cache  region, then the compressed page may be read and restored to the active page  region of the system memory subsystem  from the disk subsystem . This process may require a call to the compressed disk manager  if the page was stored in compressed format on the disk subsystem . If the compressed disk manager  is installed in the system and the page is located in the compressed disk partition, the compressed disk manager software  may translate the disk sector address to the compressed disk partition sector address. Once the compressed disk sector address is determined by a FAT2 (Compressed File Allocation Table) lookup, the DME  is configured to read the compressed data from the disk subsystem  into compressed cache  memory region. To accomplish the initial move, the compressed disk manager  module may request from the disk drivers  a disk controller  operation to retrieve the proper page or pages from the disk subsystem . When disk access of compressed pages has finished by indication from the disk drivers  to the compressed disk manager , the decompression operation of the compressed page may be invoked by a call from the compressed disk manager  to the DME device driver , and then configuration of command blocks for the DME . The DME device driver  may initiate the decompression process by prompting the DME  to move the compressed page through the compactor chip  located, e.g., on the C-DIMM  device. The DME  reads the coherent decompressed page and then writes that page into the active page  area of the memory subsystem . Typically, the CPU cache is updated by the read, and application execution can begin immediately. In an alternate embodiment, the write of the decompressed page back to system memory  may happen later, restoring the coherent data into the active page  region of the system memory subsystem . Once the active page is restored to the area allocated by the file system  software, the VMM  is notified that the active page is now ready for application operation.","If the compressed disk manager  is not installed in the system and the page is located in a normal non-compressed disk partition, the system need not invoke the codec driver  or the C-DIMM device  and may restore the non-compressed page(s) directly into the active page  region of the memory subsystem , e.g., by using the DME . The operation of the VMM  and the file system  are similar to that described previously when the compressed disk manager  was invoked, except that the compressed cache manager \/ is preferably not invoked to restore the non-compressed page from disk to the active page  region of the memory subsystem .","In addition, the same process can be used for network systems where compressed pages are transferred between servers connected on a local area or wide area network. In this alternate embodiment, pages are compressed by the compactor chip under direction of a network driver filter that is similar to the Compressed Disk Partition Manager , except that these pages are targeted for sending to remote client or application servers. Compressed data pages can also be sent to other client computers where data can be decompressed by either the compactor chip  or a similar derivative, or alternatively by software plug-in modules previously installed in the client end computers.","In addition, the system is not limited to the use of file filters as indicated in the preferred software embodiment, but may have specific operations defined and embedded into the operating system software kernel. Thus, operation may be enabled by specific instructions and routines embedded directly into the kernel of the operating system. Also, the codec driver , or the operating system, may have a proprietary callable application specific interface which can be used directly by applications such as database software, CAD tools, and any other application programs when specific compression and decompress tasks are required.","In one embodiment, an Application Programming Interface (API) may be provided with the compressed cache  that allows applications running on a system utilizing virtual memory, a compressed cache  and CCM to request and be notified of an effective size of memory, thus allowing the application to make memory tuning decisions. Effective space may include uncompressed virtual memory plus the effective space provided by the compressed cache. The compressed cache, for example, may be allocated 1000 pages of physical memory. If the compressed cache provides a 2:1 compression ratio, then the compressed cache provides 2000 pages of effective space. If there are 1000 pages of uncompressed virtual memory available, then the API may notify the application that there are 3000 pages of memory available. In one embodiment, if an application request the amount of physical memory available (as opposed to a virtual memory amount), then the API preferably notifies the requesting application of the true amount of physical memory available. For example, in a system with 64 MB of physical memory, if VM and the CC were allocated a total of 54 MB of physical memory, then the API would return 10 MB as the amount of physical memory available.","FIG. \u2014DME\/Codec Interface",{"@attributes":{"id":"p-0122","num":"0125"},"figref":"FIG. 11","b":["262","204","204","262","204","262","250","252","254","252","250","254","250","252","254","250","250","204","252","250","254"]},"As noted above, the DME  may include a single DME channel  or multiple DME channels . The DME channel  may include a split transaction interface to the memory controller . The DME  also allows out-of-order cache line accesses. The DME  also preferably supports scatter\/gather compressed DMA transfers, wherein a source\/destination page may comprise one or more linked blocks. The blocks may be linked whereby the physical address to the next block is embedded in each block. In one embodiment, an unlimited number of blocks can be linked together. The DME  may support block sizes from 128 bytes to 4 Kb.","The DME channel  may be operable to concurrently transfer a source page from coherent memory into the input buffer  and transfer a destination page from the output buffer  to coherent memory. The depth of the buffers  and  may be optimized for the memory controller .","Software executing on the CPU  operates to initiate DME transfers. This software may be either software drivers, the operating system, the VMM, or other software. The software executing on the CPU  operates to initiate DME transfers by first setting up one or more Command Blocks in coherent memory. Command Blocks describe or specify a DME operation and are described below. The software executing on the CPU  then initiates the DMA transfer by writing the first Command Block address into the DME channel . The DME channel  then performs the transfers specified by the one or more Command Blocks. These transfers may involve compressing and\/or decompressing data in system memory, or compressing and\/or decompressing data transferred between various devices in the system. When the DME channel  completes the transfers, i.e., has executed all of the Command Blocks, then the DME channel  updates the Command Block status.","The transfers performed by the DME channel  may be coherent transfers, i.e., transfers which may require snooping by the CPU subsystem. In one embodiment, the memory controller  handles coherency of DMA operations. The memory controller  may also operate to return data from the memory or host bus on a snoop hit.","The codec engine  may be a lossless half-duplex compression\/decompression engine. The codec engine  may operate to compress\/decompress one page at a time. The codec engine  may provide a decompression rate up to 2.1 GB\/sec at 133 MHz, and a compression rate up to 532 MB\/sec at 133 MHz. Thus the codec engine  may only require approximately 7.5 microseconds to compress a 4 Kb page at 133 MHz. The codec engine  may include a scaleable design, which can be optimized for one or more of bandwidth and compression ratio. In the preferred embodiment, the codec engine  is implemented on a chip or ASIC with approximately 200 k gates, while providing 532 MB\/sec compression\/decompression and a 256 byte history buffer or history window (also called a history table).","FIG. \u2014Compression and Decompression Operations Using a Data Movement Engine (DME) ",{"@attributes":{"id":"p-0128","num":"0131"},"figref":["FIG. 12","FIG. 12"],"b":["204","100","204","204","211","100","218","204","211","204","204","218"]},"The functional unit  may couple to the DME . As noted above, the functional unit  may comprise a codec (compression\/decompression) engine which performs compression and decompression operations. For example, in one embodiment the functional unit  is a parallel compression\/decompression codec engine. In another embodiment, as noted above, the functional unit  may be an encryption\/decryption engine for encrypting\/decrypting data. The functional unit  may be comprised in the memory system , e.g., may be comprised on a memory module. In one example where the functional unit  is a codec engine  comprised on a memory module, the memory module comprising the codec engine  is referred to herein as a C-DIMM. In another embodiment, the functional unit  may be comprised in the memory controller .","In the description of  that follows, it is presumed that the functional unit  is a codec engine  that is comprised on a memory module (C-DIMM) of the system memory . As noted above, the software programs managing the DME  and codec engine  may be any of various types, such as the operating system, VMM, device drivers, or combinations thereof. For simplicity, the description below presumes that the software programs managing the DME  and codec engine  include a DME device driver (DDD).","As shown, at step  the processor  may generate one or more command blocks . In one embodiment, the DME device driver  executing on the processor  operates to cause generation of the command blocks .  shows two command blocks A and B. As shown, the command blocks  may be generated as a linked-list format, i.e., may be chained together, as shown. The processor  may generate one or more command blocks , e.g., may generate a greater number of command blocks  as desired.","Each of the command blocks  may specify a date of movement operation (e.g., one or more data movement operations) to perform either a data compression operation, and\/or a data decompression operation. Where the operation specified by the command block is a compression operation, then in one embodiment the command block may comprise:","1) the start address for the page to be compressed;","2) the size of the page to be compressed;","3) the start address of the destination in main memory  where the compressed data is to be stored;","4) a compression\/decompression Status Byte (or bytes); and","5) further status information.","The start address for the page to be compressed comprises the start or beginning address of the page in system memory which is desired to be compressed.","The size of the page to be compressed may be the standard operating system page size, e.g., either 4 Kb or 8 Kb. This page size may be set to a default value. It is also noted that this page size may also be set on non-page granularity boundaries, as desired.","The destination start address for the compressed data indicates the beginning address for the destination in system memory  where the compressed data is to be stored in main memory .","The compression\/decompression Status Byte indicates the status of the compression operation being performed, e.g., whether this compression is complete, incomplete, uncompressible, etc. The Status Byte may also indicate an address to the next compressed block on the free list. Thus, where the resultant compressed data is being transferred to multiple different destination locations, these destination blocks may be linked or chained using the Status Byte.","The further status information may indicate other types of compression status, i.e., whether the operation is a compression operation or decompression operation, whether to clear the history window or history buffer, the compression threshold, etc. This other status information may also contain a pointer to the next command block, i.e., the next command block in the linked list.","In one embodiment, the DME device driver  executing on the processor  operates to generate command blocks  in the system memory . In another embodiment, the DME device driver  executing on the processor  causes the command blocks  to be generated and transferred to a local memory or command block buffer comprised in the memory controller  and coupled closely to the DME . In another embodiment, the command blocks  are written directly to buffers in the DME .","The software executing on the processor  then causes the DME  at step  to begin executing the command blocks . The software executing on the processor  may write a control register in the DME  to cause the DME  to begin compressing a page pointed to by a respective command block. In one embodiment, the software provides a start address or pointer to the first command block A in the linked list.","The DME  may use the start address of the first command block  to read the first command block from system memory  at step  and begin executing the first command block . As noted above, each command block  may contain a pointer to a subsequent command block in a linked list fashion. Thus, the DME  can read and execute the first command block A followed by reading an executing the second command block B, etc. until all the command blocks in the linked list have been executed. The execution of one command block is described below.","In response to reading the command block and execution of the command block , the DME  may perform the following operations. As shown at A, the DME  may read the page that is desired to be compressed from system memory , referred to herein as data source . Thus, the DME  uses the start address for the page to be compressed which is contained in the current command block  and performs a read of this page at A and then performs a write of this page to the functional unit , i.e., the codec engine A. Thus, the DME  moves a portion of the page from system memory to the input data register or input buffer  of the codec engine A. The codec engine A then begins compressing the data and in turn generates compressed data in its output buffer .","The DME  may then read the compressed data from the output buffer  of the codec engine  at C and then write this compressed data to one or more destination memory locations A, B and C in the system memory. Here, the DME  uses the start address for the compressed data, i.e., the destination start addresses to write the data to the destination system memory. In one embodiment, the DME  is a scatter-gather data movement engine (e.g., scatter-gather DMA controller). In this embodiment, the command block may comprise a plurality of destination start addresses for different destination locations in system memory A, B and C. In this embodiment, the DME  may operate to write the compressed data to different noncontiguous data destinations or locations in the system memory as represented by A, B and C.","In performing the write operation in D, the DME  may read a CODEC Status Register. The Status Register may comprise various fields, including Compression\/Decompression Status, Bytes Available, and Error indicators, etc. The DME  may read the Bytes Available bytes from the codec engine's output data buffer  and write them to a compressed data area in main memory. If the bytes to be written exceed the Block Size bytes (possibly minus linked list pointer bytes, then the DMA Controller may stop writing before it overwrites the Link List Pointer contained in the destination memory block; evaluate the Link List Pointer; begin writing at the new location This may involve non-coherent reads from the codec engine's output buffer , and coherent writes to main memory.","After the data has been written to a destination, the DME  may update the respective command block with status information, shown as step . The DME may update the Address Offset Counter with the number of bytes written. If the Status is complete, the Link List Pointer in the current memory block is updated with null pointer to indicate that this is the last block of the linked list entity. The command block may also be updated with the number of blocks written. The coherent update informs the software cache lock that compression is complete. The Block Count may also be updated. This may be used by software to update a monitor of the linked list. The software may observe the Command Block write by its coherent cache lock. If the codec Status was incomplete, then the DME may reads the codec Status Register and may attempt to repeat the read write process.","After the DME  updates the respective command block with status information, the DME  may begin processing another command block. The processor  (e.g., the DDD or CCM) may then later check the status information.","It is noted that, after step , the processor  may perform other activities independent of the DME . In one embodiment, the processor  periodically polls for the status completion in step . In another embodiment, an interrupt mechanism is used to alert the processor  that a respective command block has completed.","It is noted that decompression operations proceed in a similar manner to that described above, except that compressed data is read in A, decompressed, and uncompressed data is written in D. Various other status checks and updates may also be modified.","Coherent Transfers and Snooping","In one embodiment, the read of the page to be compressed from main memory is a coherent read. In other words, the page being read from the data source  in system memory in step A is a coherent read, and thus the address of this read operation is provided to the processor  during the read for snoop purposes. In addition, any of the data movement operations A-D may comprise coherent reads or writes.","An L or L cache comprised on the processor  may snoop the read address to determine if updated data corresponding to this address is stored in either of the L or L caches in the processor . If a snoop hit occurs, meaning that the cache comprised in the processor contains updated data corresponding to the source address, then the processor  may halt this read cycle being performed by the DME engine, and perform a writeback of the updated data to the system memory. The DME read operation may then proceed with reading the new data. In one embodiment, the DME engine may \u201csnarf\u201d or obtain the data that is being written back by the processor cache as the data is provided through the memory controller, so the DME engine  is not required to re-read this data once it has been written to system memory.","Streaming of Data","In one embodiment, the DME operation of reading the source data, writing the data to the input buffer  of the codec engine , reading the data from the output buffer  of the codec engine , and then writing the compressed data to a destination address, may comprise four data movement operations. In one embodiment, each of these data movement operations may comprise one or more individual data reads or writes, i.e., the data may be streamed from the source data and provided in respective portions to the codec engine, or data may be read from the output buffer of the codec engine as the data becomes available and multiple reads may be performed in C and corresponding multiple writes D may be performed to the destination address. In one embodiment, the command block includes four coherent status bits for each of the data movement operations A, B, C and D. In one embodiment, the default mode is that each of the data movement operations A, B, C and D is a coherent read or write operation wherein the address of the respective operation is provided to the processor  for snooping purposes. Various bits may be set in respective command blocks to designate certain data movement operations as non-coherent wherein snooping operations are not required.","In one embodiment, the DME  may operate to read the data from the data source  in A as a streaming process. For example, in one embodiment the DME and\/or memory controller may have an input buffer sufficiently large to store an entire page of memory. In this embodiment, the DME may read an entire page of memory first in the data movement A and then provide the entire page of data in a write operation B to the functional unit . However, in this embodiment, the codec engine A cannot begin compressing the data until the entire block of data has been read in step A and at least a; portion of this data has been provided to the input buffer in movement operation B. Thus, in one embodiment the DME performs a data read streaming operation in A wherein the DME engine  reads a first portion of the page, e.g., 1 kb, \u00bd kb, etc. and then writes this data in step B to the input buffer of the codec A. Thus, the compression engine A can begin compressing the data received at its input buffer  while the DME engine  is concurrently acquiring more data from the data source  in step A and then writing more data in step B to the input buffer. In a similar manner, the DME  may read portions of data from the output buffer of the codec A in step C as the compressed output data is available and write this data to destination addresses in step D. This may allow concurrency of operations between steps A and B and also between steps C and D.","In one embodiment, where the DME includes multiple physical busses or channels between the main memory and the functional unit , the DME  may operate to read data from the data source  in system memory on a first memory bus channel concurrently with the DME  writing previously read data to the codec A in step B on a second memory channel. The DME  may further be operable to write output compressed data that has been previously received by the DME  from the output buffer of the codec A to data destinations in the system memory on a third memory channel. It is noted that where there is only a single memory channel to the codec A, only one of steps B and C can operate at any one time although they may operate in a time multiplexed fashion. In another embodiment, the system may include multiple memory channels, e.g., a first memory channel between the DME  and the input buffer of the codec A and a second memory channel between the DME  and the output buffer of the codec A.","Another embodiment of  includes two DME engines A and B. The DME engine A may be used for transferring data to the input buffer of the codec engine , e.g., for reading data from the system memory and writing the data to the input buffer of the codec engine . The DME engine B may be used for reading data from the output buffer of the codec engine , and then writing the received data to a destination. These DME engines A and B may operate concurrently. This is illustrated in FIG. A.","FIG. \u2014Preferred Embodiment of Data Movement Engine (DME)",{"@attributes":{"id":"p-0159","num":"0000"},"ul":{"@attributes":{"id":"ul0004","list-style":"none"},"li":{"@attributes":{"id":"ul0004-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0005","list-style":"none"},"li":["Fetch Start Block\n        \n        ","Deliver Command to CODEC","Deliver data to CODEC\n        \n        ","Deliver CRC or Checksum to CODEC","Fetch command status from CODEC\n        \n        ","Check error codes from CODEC\n        \n        "]}}}},{"@attributes":{"id":"p-0160","num":"0180"},"figref":"FIG. 14"},"A process, executing on a computer system including a Compactor Chip, a device driver for controlling the DME , and a Compressed Cache Manager (CCM), may generate a request to transfer data resident in system memory to nonvolatile memory, such as a disk drive. In one embodiment, the Compactor Chip may reside on a C-DIMM installed as system memory, and the device driver may be a DME device driver. Alternatively, the Compactor Chip  may reside on a memory controller  in the system. In one embodiment, the process generating the request to transfer data may be a Virtual Memory Manager (VMM) or I\/O manager, and the request to transfer may be generated as the result of one of several VMM functions. Examples of VMM functions in a system that may produce a request to transfer data may include, but are not limited to: invalidation of \u201cclean\u201d data that is selected for invalidation using an LRU or other selection scheme, and: writeback of \u201cdirty\u201d data from system memory to nonvolatile memory. Clean data is data that is resident in system memory that has not been touched and\/or modified by a process since being originally loaded into system memory or since last being synchronized with the original data on the nonvolatile memory. Dirty data is data that has been touched and\/or modified by a process since being originally loaded into system memory or since last being synchronized with the original data on the nonvolatile memory. In a system with virtual memory and a VMM, the data may reside in pages (typically 4096 bytes). Pages may be divided into sectors (typically 256 or 512 bytes). Thus, the VMM may generate a request that one or more pages resident in system memory and being managed by the VMM be moved to nonvolatile memory.","In step , the request to move one or more pages of data from system memory to nonvolatile memory may be received by the CCM. In step , the CCM may direct or configure the DME  to transfer of the one or more pages to the compactor chip  to be compressed. Step  is expanded into steps and . In step , the DME  may read the one or more pages from system memory, and may write the one or more pages to the compactor chip  in step . The compactor chip  receives the one or more pages and compresses the pages in step . The compressed pages are stored in a compressed cache (CC) in system memory in step . The storing of the compressed pages in step  is expanded into steps and . In step , the DME  may read the compressed pages from the compactor chip . In step , the DME  may write the compressed pages to the compressed cache in system memory .","FIG. \u2014Compressing and Storing Data to Disk",{"@attributes":{"id":"p-0163","num":"0183"},"figref":"FIG. 15","b":"14"},"In step , the request to move one or more pages of data from system memory to nonvolatile memory may be received by the CCM. In step , the CCM may direct or configure the DME  to transfer the one or more pages to the compactor chip  to be compressed. Step  is expanded into steps and . In step , the DME  may read the one or more pages from system memory, and may write the one or more pages to the compactor chip  in step . The compactor chip  receives the one or more pages and compresses the pages in step . The compressed pages may be stored in a compressed cache (CC) in system memory and\/or to nonvolatile memory such as a disk drive in step . The storing of the compressed pages in step  is expanded into steps through . In step , the DME  may read the compressed pages from the compactor chip . In step , the DME  may write the compressed pages to the compressed cache in system memory . In step , the CCM may also write the compressed pages to the nonvolatile memory such as a disk drive.","Alternatively to being implemented as software executed on a CPU, the CCM may be implemented in hardware. In one embodiment, the CCM may be implemented as embedded hardware logic on the DME , or in the Compactor Chip .","FIG. \u2014The Processing of a Swap Read Request",{"@attributes":{"id":"p-0166","num":"0186"},"figref":"FIG. 16"},"In step , the operating system (OS) may generate a read request for one or more pages from the disk subsystem. In step , the Compressed Cache Manager (CCM) may receive the request generated by the OS in step . In one embodiment, the CCM may be installed in a stack of I\/O drivers, and the read request may be passed to the CCM from a higher-level driver as part of the processing of the read request. In another embodiment, the CCM may be implemented directly as part of the OS.","In step , the CCM may translate the logical address from the read request into a specific address for the Compressed Cache (CC). The CCM may then perform a CC lookup to determine if the requested pages are resident in the CC. In step , if the requested pages are resident in the CC, then the method proceeds to step . In step , if the requested pages are not resident in the CC, then the pages are on disk, and processing may proceed to step .","In step , if the requested pages are not stored compressed in the compressed disk, then the pages may be read by the DME  from the disk and transferred to the active page region of system memory in step . If the requested pages are stored in the compressed disk, then the Compressed Disk Manager (CDM) may translate the logical address from the read request into the device address for the compressed disk in step . The CDM may then configure the DME  to transfer the located compressed pages from the compressed disk on the disk subsystem into a temporary buffer. Preferably, the temporary buffer is in system memory. Processing then may proceed to step . Note that, in embodiments without a compressed disk and compressed disk manager, processing may proceed directly from step  to step , and steps - may be absent.","In step , the CCM may request the DME device driver to configure the DME for decompression of the compressed pages. In response, the DME device driver may configure one or more command blocks. In step , the DME  reads the compressed pages and writes them to memory on the Compactor Chip . The DME  may read the compressed pages from the compressed cache if the pages are there, or from the temporary buffer, if the compressed pages were on the compressed disk. The DME  may then instruct the Compactor Chip  to decompress the compressed pages in step , preferably using a parallel decompression method. The Compactor Chip  may then decompress the compressed pages in step . In step , the DME  may then move the decompressed pages from the Compactor Chip  to the active page region of system memory. In step , the DME  may notify the OS that the requested pages are in the active page region, e.g., by updating an appropriate command block.","Note that the parallel decompression of the compressed data described in  occurs in real-time (i.e., processing does not proceed until the decompression is completed), but due to the speed at which the parallel decompression is performed by the Compactor Chip , very little latency in data transfer for the read request is introduced by the decompression. Because the compressed cache allows for more data to be cached in system memory rather than being swapped or written back to disk, and because the data transfer time from disk to memory is about three orders of magnitude longer than from memory to CPU cache memory, any latency introduced by the decompression of the data is greatly outweighed by the performance benefits of reading the data from the compressed cache rather than from the disk subsystem.","Use of the DME with Conventional Memory Controllers","As described above, the DME  may be integrated into a memory controller. In one embodiment, the DME  is integrated into a conventional memory controller, wherein the memory controller was not designed with the compression\/decompression operations of the DME  in mind.","In the case where the DME  is included in a conventional memory controller, various issues may arise due to the operation of conventional memory controllers. For example, conventional memory controllers include a write queue which is used for posting writes to the system memory. In this write posting operation, data may be written to a write queue of a memory controller and the memory controller may only at some later time submit or perform these writes to the system memory. However, for the purposes of the DME operation with the functional unit , especially when the functional unit is a codec engine , it is important that the memory controller deliver all the data to the system memory or the functional unit  substantially immediately.","A device such as a processor or any bus master which writes data to the memory actually writes the data to the system memory controller, and the data is written to the system memory controller in a certain order. However, a memory controller which includes write queues, e.g., write posting queues, may typically operate to write the data to the system memory at a current or later time and may write the order to the system memory in various orders. Thus, conventional memory controllers are designed where the memory controller is only required to provide the data to the system memory at some time in the future and may provide it regardless of order. However, for a system as described herein which includes a functional unit  contained in the system memory, it is important that the data arrive completely to the functional unit  before the functional unit  processes the respective data. This is because the functional unit will consume the data sequentially.","This is especially true in a compression\/decompression operation or in an encryption\/decryption operation. For example, in a data compression operation, the codec engine  must compress the data sequentially, i.e., must process the data in order or sequentially. This is also true with respect to encryption\/decryption, wherein encryption or decryption of data must be performed on the data sequentially and cannot be performed on out of order data.","Therefore, in embodiments of the system described herein used with conventional memory controllers that use write queues and wrote posting techniques, the system may include one or more methods to ensure that data written to the memory controller  is completely provided to the functional unit . This guarantees that the functional unit  can process the data sequentially to guarantee proper operation.","In one embodiment, the DME  operates to provide additional write data at the end of a write block to attempt to ensure that the write queues of the memory controller are completely flushed to the functional unit . For example, assume the memory block is a 4 Kbyte page and the 4 Kb page is being written by the DME  to the functional unit . Also assume that the write queues of the memory controller are, e.g., two cache lines. In this case, the DME  will write the 4 Kb page, and at the end of this 4 Kb page write will also write an additional 2 cache lines of \u201cdummy data\u201d. The additional 2 cache lines of dummy data are written to attempt to flush the write queue of the memory controller of any of the 4 Kb of data. This ensures that all of the data that is intended for the functional unit  is actually received by the functional unit  during the write operation, and not at some later time.","There are various scenarios involved with accounting for the problem of write queue operation in a conventional memory controller.","In one scenario, the DME  and\/or one or more of the software drivers knows ahead of time the size or depth of the write queues in the memory controller. In this embodiment, the DME  can be configured to write the exact amount or number of cache lines of data to guarantee that the write queues of the memory controller are flushed during writes to the functional unit .","In a second scenario, the depth of the write queues in the memory controller are not known. In this case, the DME  may iteratively perform additional cache line writes of dummy data followed by a testing or check of status of the memory queue to see if all of the write data has been flushed from the memory controller write queues out to the functional unit . One problem with this operation is that it is undesirable for the DME to write more data to the write queue, and inadvertently have some of this dummy data actually flushed to the functional unit . Thus, in this embodiment, the DME  preferably writes the smallest amount of data, e.g, 1 cache line to the memory controller write queue, and tests to see if all data has been flushed. If not, the memory controller writes an additional cache line and performs a further test, etc., until it determines that all of the \u201creal\u201d data that is desired to be provided to the functional unit  is actually provided to the functional unit . This second embodiment is more expensive in terms of time cycles and\/or bandwidth.","In a third scenario, the DME  and\/or the software drivers do not know ahead of time the depth of the write queues in the memory controller . However, in this embodiment one or more of the software drivers or the DME  operate to adaptively learn the depth of the memory controller write queues and then store this information for use later by the DME . Thus, for example, on system boot during the driver initialization routine, the driver may test the write queues of the memory controller and determine the exact size or depth of the write queues, and then may program a register in the DME  to configure this size values. After the DME  is configured, the DME  can then write the exact or appropriate amount of cache lines to the memory controller write queue to guarantee that the write queues are flushed during each write to the functional unit .","In a fourth scenario, where the DME  is comprised in the memory controller , the DME  may be operable to provide a message to the memory controller requesting or commanding that the memory controller flush its write queues. The DME  may also be operable to provide a message to the memory controller directing the memory controller to disable write posting and use of its write queues.","In a fifth scenario, the DME  may operate to perform writes to the same address space as is being used by the write queues. This may operate to flush the write queues.","In a sixth scenario, where the DME  may or may not be in the memory controller, the DME  may perform a read operation to the memory controller of a different address space to cause the memory controller to flush its write queues. If the memory controller  is performing cache like functions, i.e., is combining writes and storing them in a cache like memory within the memory controller, then the DME  may operate to perform a read of a different address space. These reads of the different address space cause the \u201ccache\u201d in the memory controller to fill up with other data, and thus the LRU\/LZU replacement algorithm used in the memory controller would ultimately cause the desired cache lines to be flushed from the memory controller .","In another embodiment, the memory controller may be designed specifically to accommodate the DME  and its interaction with a functional unit  such as a codec engine . Thus, the memory controller  may be designed to accommodate the operation of the DME and may be designed with knowledge of the functional unit . In this embodiment, the DME  may be operable to provide a signal or indication to the memory controller, e.g., by setting a bit in a register, which causes the memory controller to flush its write queues. In another embodiment, the DME  may be able to selectively turn off write posting or use of data stored in the write queues to guarantee that all data from the memory controller is written through to the functional unit. The operation of the DME  providing a signal or setting a bit in the memory controller  to flush the write queues may be referred to as a \u201csynchronization command\u201d, a \u201cfence command\u201d or a \u201cflush command\u201d.","DME Error Reporting","In one embodiment, the DME  includes the ability to handle errors that are reported from the functional unit . The errors may be reported from the functional unit  through status information at completion or during a command being executed by the DME on the functional unit . Types of errors that may be reported by the functional unit  include data transfer errors or functional data errors, such as a decompression stream that could not properly be decompressed. The DME  may operate to notify the software through an update in the command block that the error occurred. Alternatively, the DME  may retry the command completely by re-executing the entire command functional block to the functional unit , thereby trying to avoid the error in an additional try. The DME  may include a counter that is incremented each time this retry is performed. Once the counter reaches an agreed to point, the DME  may abandon that command and report back through the command block to the software that the command was unexecutable.","Functional Unit Protocol Configuration","As described herein, the functional unit  may include an established predefined protocol wherein the DME  communicates with the functional unit  using this protocol. However, future versions of the functional unit  may include a slightly modified or different protocol. Thus, in one embodiment, the DME  is programmable though software to account for two or more possible protocols of various function units . In one embodiment, during initialization, the software may detect the type of the functional unit  present in the system. The software may then program the DME  accordingly to ensure that the DME  can communicate properly with the respective version of the functional unit  comprised in the system.","With respect to the command block, in one embodiment the command block includes a header which points to subsequent command blocks in the linked list. The DME  may include a pointer that points to the command block header, instead of pointing to the first link listed block of data. Thus the DME could use the header pointer which points to the first block as a location to store compressed data.","Compression Ratio Sampling","The DME  may include a mechanism to very quickly determine the compression ratio of a particular page. The system may support a special command block which is not involved with two memory copies. In other words, the purpose of the special command block is not to move the data from some source to some destination. Instead, the purpose is to just read the source data, write it into the codec, compress it, and use this transaction to determine the compression ratio.","The DME  may also optionally handle \u201cno bloat support\u201d. In the event that the compressed results are larger than the original results, the DME  is responsible for transferring the uncompressed data directly to the destination, but as link listed blocks. The DME  may also include a mechanism to support a minimum compression threshold. In the event that the codec  does not yield a minimum compression ratio, then DME would detect the status and choose not to complete the transfer. Instead, the DME  would notify the command block that the operation failed and that the operation failed due to not meeting the compression threshold requirements.","Another type of command block op code would be direct DME support where data could be moved directly from a gather source to a scatter destination, thus bypassing the codec. The DME  could also optionally provide zero fill support. Zero fill support is a special command block that allows the DME to zero fill a page. In this case the command block would only mention a destination location and the DME would respond by filling that destination with a specific pattern.","Miscellaneous Notes","The user of linked lists to specify command blocks provide numerous capabilities.","First, linked lists allow OS (e.g., NT) operations to scale when a first processor (or first thread) wants to perform a command, and a second processor (or second thread) wants to initiate a transfer, the second processor must wait until the first operation is complete. Now the second processor (or second thread) can simply add an additional command block to the link list and then perform other operations, allowing the first processor (first thread) to proceed. The addition of another command block to the linked list allows the DME to perform the data movement(s) at a future point in time.","Link listing command blocks also allows a driver or application to perform a number of compressed transfers. For example, if it is desired to compress an arbitrary sized buffer that is much larger than a basic 4 Kb compression block, linked listed command blocks allow the driver to perform a larger logical compression. The command block may include mechanisms to decide whether to reset the history window or not. Optionally the software could chose to string a number of command blocks, and hence a number of compression operations, and optionally not reset the history buffer between each link listed command, hence creating a logical larger compression. The larger logical compression may provide improved compression ratios.","An optional mechanism can be added to the DME in the event that there are multiple functional units  that could be used at any time. The command block may only indicate the type of function that needs to be performed. The DME could chose one of N functional units  and have a mechanism to atomically reserve use of the selected functional unit . Today this atomic use of a DME and functional unit is done as a software mechanism. However, in an alternate embodiment, a more intelligent DME may be operable to negotiate for and obtain the use of a particular functional unit .","As described in the parent application, an aperture function may be used to enable a device to control the functional unit . The aperture may be an aperture in the system memory address space, wherein reads or writes to this aperture are understood to be for the purpose of controlling the functional unit . In one embodiment, the aperture function may be part of the DME. When the DME  accesses the functional unit , meaning the DME  is either writing data into the functional unit  or reading data from the functional unit , the DME may include a mechanism for instructing the memory controller to bypass caches, write queues or cache like structures, to bypass chip kill capabilities, 16 byte ECC capabilities, or instruct the memory controller to provide a bus turn around cycle in the event that the functional unit  is sharing a chip select or bus with other devices such as memory.","The following are features that may be used in various embodiments of the invention:",{"@attributes":{"id":"p-0197","num":"0000"},"ul":{"@attributes":{"id":"ul0011","list-style":"none"},"li":{"@attributes":{"id":"ul0011-0001","num":"0000"},"ul":{"@attributes":{"id":"ul0012","list-style":"none"},"li":["Configure\/support capabilities","Null terminated or circular link-listed coherent command blocks\n        \n        ","Scatter-gather","Null terminated link-listed data blocks using physical addresses","Selective coherent transfers","Two memory copies","One or more DMA channels per DME\n        \n        ","Multiple DMEs for MP","Auto-hysteresis support to input buffer [optional]","Data ready support from output buffer","Error\/warning notification to command blocks","Auto-retry support [optional]","Couple to memory module based CODECs, motherboard CODECs, memory controller integrated CODECs, etc.","DME can be in memory controller, processor or I\/O bridge","Interrupt mechanism [optional]","Dual memory controller considerations. Inter-controller transfers [optional]","PCI registers","Touch DMA engine to execute Command Blocks","DMA channels may contain buffers to perform complete reads instead of streaming data [optional]","Only use appropriate number of link-listed destination blocks","Extra chip selects to allow DME to directly access CODECs (there may be no memory behind the CODEC)","8B ECC, 32b CRC, checksum, etc.","Handling two or more CODECs (over read) [optional]","Use it for third party DMA [optional]","Use pointer if compressed page fits in pointer [optional]","Compression ratio sampling [optional]","No bloat support [optional]","Compression threshold support [optional]","SDR, DDR and future-specifics CODEC support","Direct DMA support (bypass CODEC) [optional]","Zero fill support [optional]","Bypass cache or cache-like behavior","Bypass deep store queues","Bypass chip kill","Bypass 16B ECC","Add bus turnaround cycle","Initiate command to CODEC","Arbitrary transfer size","Mechanism for one of N DMEs to atomically reserve and use functional unit\n\nRegister Description\n\nCODEC Enable(s)\n","Written by BIOS to identify number of CODECs in operation","Each independent CODEC will have its own enable\n        \n        ","Written by software to initiate action\n        \n        ","Each independent CODEC has its own control register\n\nCommand Status Register\n","Used by DMA Controller to access in-process compression\/decompression","Each independent CODEC will have a pointer to its Status Register\n        \n        ","Used to set aside a memory space, e.g., 256 kb in logical RAM memory for CODEC control, data, and status.","Each independent CODEC will have an associated 64 bit aperture pointer for physical access by the DMA Controller\n\nPhysical Aperture Pointer Register\n","Physical Address in main memory for CODEC\n        \n        ","Pointer for DMA Controller to Command Block","One for each independent CODEC\n\nBlock Size Descriptor\n","Size of blocks used for holding compressed data","128 to 4 k","Managed as a set of linked list entities","Link list pointers are the last \u201cn\u201d bytes of the block\n\nPointer Size Descriptor\n","Number of bytes \u201cn\u201d used for the link list pointer (e.g. 4 or 8)\n\nAddress Offset Counter(s)\n","Counter used to track number of bytes written by CODEC","For large pages indicates offset into page being written\n\nBlock Count(s)\n","Number of (link list) blocks written during compression","Updated by DMA Controller at end of compression","The next address may be designed so that the software does not have to map and walk the linked list during clean. Just want to update some boundary pointers.\n\nUse of Methods described in U.S. patent application Ser. No. 09\/550,380\n"]}}}},"As noted above, the present application is a continuation-in-part of U.S. patent application Ser. No. 09\/550,380 titled \u201cParallel Compression\/Decompression System and Method for Implementation of In-Memory Compressed Cache Improving Storage Density and Access Speed for Industry Standard Memory Subsystems and In-Line Memory Modules\u201d filed on Apr. 14, 2000. This application is also incorporated by reference in its entirety as though fully and completely set forth herein. Any of the methods of application Ser. No. 09\/550,380 may be used in the system described herein for managing the functional unit  (e.g., codec engine ) and\/or compressed cache .","Parallel Compression\/decompression Engine","The compactor chip may include parallel data compression and decompression engines, designed for the reduction of data bandwidth and storage requirements and for compressing\/decompressing data at a high rate. The parallel compression\/decompression engine may be included in any of various devices, including a memory controller; memory modules; a processor or CPU; peripheral devices, such as a network interface card, modem, IDSN terminal adapter, ATM adapter, etc.; and network devices, such as routers, hubs, switches, bridges, etc., among others. In the present embodiment, the parallel compression and decompression engine may be included on a Compactor Chip  comprised on a memory module as described above. As noted above, in one embodiment, the Compactor Chip may be integrated on a DIMM, wherein a DIMM with integrated Compactor Chip may be referred to as a C-DIMM. A data movement engine (DME)  may be included in the system for interfacing with the Compactor Chip . A DME driver may be provided to enable programs, drivers, and other software executing on a system comprising a Compactor Chip to utilize the parallel compression and decompression engines on the Compactor Chip. In one embodiment, the driver may be referred to as the DME device driver (DDD).","When the Compactor Chip  is included in a system, data transfers on the system may be in either two formats: compressed or normal (non-compressed). Compressed data from system I\/O peripherals such as the nonvolatile memory, floppy drive, or local area network (LAN) may be decompressed on the Compactor Chip  and stored into memory or compressed on the Compactor Chip  and saved in memory (volatile or nonvolatile) in compressed format. Data may be saved in either a normal or compressed format, retrieved from the memory for CPU usage in a normal or compressed format, or transmitted and stored on a medium in normal or compressed format.","The Compactor Chip  preferably includes parallel compression and decompression engines designed to process stream data at more than a single byte or symbol (character) at one time. These parallel compression and decompression engines modify a single stream dictionary based (or history table based) data compression method, such as that described by Lempel and Ziv, to provide a scalable, high bandwidth compression and decompression operation. The parallel compression method examines a plurality of symbols in parallel, thus providing greatly increased compression performance.","Parallel Compression","The parallel data compression engine and method included on the Compactor Chip operate to perform parallel compression of data. In one embodiment, the parallel compression method first involves receiving uncompressed data, wherein the uncompressed data comprises a plurality of symbols. The method also may maintain a history table comprising entries, wherein each entry comprises at least one symbol. The method may operate to compare a plurality of symbols with entries in the history table in a parallel fashion, wherein this comparison produces compare results. The method may then determine match information for each of the plurality of symbols based on the compare results. The step of determining match information may involve determining zero or more matches of the plurality of symbols with each entry in the history table. The method then outputs compressed data in response to the match information.","In one embodiment, the method maintains a current count of prior matches which occurred when previous symbols were compared with entries in the history table. The method may also maintain a count flag for each entry in the history table. In this embodiment, the match information is determined for each of the plurality of symbols based on the current count, the count flags and the compare results.","The step of determining match information may involve determining a contiguous match based on the current count and the compare results, as well as determining if the contiguous match has stopped matching. If the contiguous match has stopped matching, then the method updates the current count according to the compare results, and compressed data is output corresponding to the contiguous match. The step of determining match information may also include resetting the count and count flags if the compare results indicate a contiguous match did not match one of the plurality of symbols. The count and count flags for all entries may be reset based on the number of the plurality of symbols that did not match in the contiguous match.","For a contiguous match, the output compressed data may comprise a count value and an entry pointer. The entry pointer points to the entry in the history table that produced the contiguous match, and the count value indicates a number of matching symbols in the contiguous match. The count value may be output as an encoded value, wherein more often occurring counts are encoded with fewer bits than less often occurring counts. For non-matching symbols that do not match any entry in the history table, the non-matching symbols may be output as the compressed data.","The above steps may be repeated one or more times until no more data is available. When no more data is available, compressed data may be output for any remaining match in the history table.","The Compactor Chip may be used to perform parallel compression, operating on a plurality of symbols at a time. In one embodiment, the parallel compression method accounts for symbol matches comprised entirely within a given plurality of symbols, referred to as the \u201cspecial case\u201d. Here presume that the plurality of symbols includes a first symbol, a last symbol, and one or more middle symbols. The step of determining match information includes detecting if at least one contiguous match occurs with one or more respective contiguous middle symbols, and the one or more respective contiguous middle symbols are not involved in a match with either the symbol before or after the respective contiguous middle symbols. If this condition is detected, then the method selects the one or more largest non-overlapping contiguous matches involving the middle symbols. In this instance, compressed data is output for each of the selected matches involving the middle symbols.","The compression circuit on the Compactor Chip may include an input for receiving uncompressed data, a history table, a plurality of comparators, a memory, match information logic, and an output for outputting compressed data. The input receives uncompressed data that comprises a plurality of symbols. The history table comprises a plurality of entries, wherein each entry comprises at least one symbol. The plurality of comparators are coupled to the history table and operate to compare a plurality of symbols with each entry in the history table in a parallel fashion, wherein the plurality of comparators produce compare results. The memory maintains a current count of prior matches that occurred when previous symbols were compared with entries in the history table. The memory may also maintain a count flag or value for each entry in the history table. The match information logic is coupled to the plurality of comparators and the memory and operates to determine match information for each of the plurality of symbols based on the current count, count flags and the compare results. The output is coupled to the match information logic for outputting compressed data in response to the match information.","Parallel Decompression","The parallel decompression engine and method implemented on a Compactor Chip operate to decompress input compressed data in one or more decompression cycles, with a plurality of codes (tokens) typically being decompressed in each cycle in parallel. A parallel decompression engine may include an input for receiving compressed data, a history table (also referred to as a history window), and a plurality of decoders for examining and decoding a plurality of codes (tokens) from the compressed data in parallel in a series of decompression cycles. A code or token may represent one or more compressed symbols or one uncompressed symbol. The parallel decompression engine may also include preliminary select generation logic for generating a plurality of preliminary selects in parallel. A preliminary select may point to an uncompressed symbol in the history window, an uncompressed symbol from a token in the current decompression cycle, or a symbol being decompressed in the current decompression cycle. The parallel decompression engine may also include final select generation logic for resolving preliminary selects and generating a plurality of final selects in parallel. Each of the plurality of final selects points either to an uncompressed symbol in the history window or to an uncompressed symbol from a token in the current decompression cycle. The parallel decompression engine may also include uncompressed data output logic for generating the uncompressed data from the uncompressed symbols pointed to by the plurality of final selects, and for storing the symbols decompressed in this cycle in the history window. The decompression engine may also include an output for outputting the uncompressed data produced in the decompression cycles.","The decompression engine may be divided into a series of stages. The decoders may be included in a first stage. The preliminary select generation logic may be included in a second stage. The final select generation logic may be included in a third stage. The output logic may be included in a fourth stage.","Decompression of compressed data may begin in the decompression engine when the decompression engine receives a compressed input stream. The compressed input stream may then be decompressed in parallel in one or more decode (or decompression) cycles, resulting in a decompressed output stream.","In a decompression cycle, a plurality of tokens from the compressed data stream may be selected for the decompression cycle and loaded in the decompression engine, where N is the total number of decoders. The tokens may be selected continuously beginning with the first token in the input data stream. A section may be extracted from the compressed data stream to serve as input data for a decompression cycle, and the tokens may be extracted from the extracted section. For example, a section of four bytes (32 bits) may be extracted. A token may be selected from an input section of the input data stream for the decompression cycle if there is a decoder available, and if a complete token is included in the remaining bits of the input section. If any of the above conditions fails, then the decompression cycle continues, and the token that failed one of the conditions is the first token to be loaded in the next decompression cycle.","As the tokens for the decompression cycle are selected, the tokens are passed to the decoders for decoding. One decoder may process one token in a decompression cycle. The decoders may decode the input tokens into start counts, indexes, index valid flags, and data valid flags, with one copy of each from each decoder being passed to the next stage for each of the output bytes to be generated in the decompression cycle. The original input data bytes are passed from the decoders for later possible selection as output data. A data byte is valid only if the token being decoded on the decoder represents a byte that was stored in the token in uncompressed format by the compression engine that created the compressed data. In this case, the uncompressed byte is passed in the data byte for the decoder, the data byte valid bit for the decoder is set, and the index valid bit for the decoder is cleared.","Next, the information generated by the decoders is used to generate preliminary selects for the output bytes. Overflow bits are also generated for each preliminary select. The preliminary selects and overflow bits are passed to the next stage, where the overflow bits are inspected for each of the preliminary selects. If the overflow bit of a preliminary select is not set, then the contents of the preliminary select point to one of the entries in the history window if the index valid bit is set for the output byte, or to one of the data bytes if the data byte valid bit is set for the output byte. Preliminary selects whose overflow bits are not set are passed as final selects without modification. If the overflow bit is set, then the contents of the preliminary select are examined to determine which of the other preliminary selects is generating the data this preliminary select refers to. The contents of the correct preliminary select are then replicated on this preliminary select, and the modified preliminary select is passed as a final select.","The final selects are used to extract the uncompressed symbols. The final selects may point to either symbols in the history window or to data bytes passed from the decoders. The uncompressed symbols are extracted and added to the uncompressed output symbols. A data valid flag may be used for each of the output data symbols to signal if this output symbol is valid in this decompression cycle. The uncompressed output data may then be appended to the output data stream and written into the history window.","Although the system and method of the present invention has been described in connection with the preferred embodiment, it is not intended to be limited to the specific form set forth herein, but on the contrary, it is intended to cover such alternatives, modifications, and equivalents, as can be reasonably included within the spirit and scope of the invention as defined by the appended claims."],"BRFSUM":[{},{}],"brief-description-of-drawings":[{},{}],"description-of-drawings":{"heading":"BRIEF DESCRIPTION OF THE DRAWINGS","p":["A better understanding of the present invention can be obtained when the following detailed description of the preferred embodiment is considered in conjunction with the following drawings, in which:",{"@attributes":{"id":"p-0040","num":"0043"},"figref":"FIG. 1"},{"@attributes":{"id":"p-0041","num":"0044"},"figref":"FIG. 2"},{"@attributes":{"id":"p-0042","num":"0045"},"figref":"FIGS. 3A","b":["3","4","4"]},{"@attributes":{"id":"p-0043","num":"0046"},"figref":"FIGS. 5A and 5B"},{"@attributes":{"id":"p-0044","num":"0047"},"figref":"FIG. 6"},{"@attributes":{"id":"p-0045","num":"0048"},"figref":["FIG. 7","FIG. 6"]},{"@attributes":{"id":"p-0046","num":"0049"},"figref":["FIG. 8","FIG. 6"]},{"@attributes":{"id":"p-0047","num":"0050"},"figref":"FIG. 9"},{"@attributes":{"id":"p-0048","num":"0051"},"figref":"FIG. 10"},{"@attributes":{"id":"p-0049","num":"0052"},"figref":"FIG. 11"},{"@attributes":{"id":"p-0050","num":"0053"},"figref":"FIG. 12"},{"@attributes":{"id":"p-0051","num":"0054"},"figref":"FIG. 12A"},{"@attributes":{"id":"p-0052","num":"0055"},"figref":"FIG. 13"},{"@attributes":{"id":"p-0053","num":"0056"},"figref":"FIG. 14"},{"@attributes":{"id":"p-0054","num":"0057"},"figref":"FIG. 15"},{"@attributes":{"id":"p-0055","num":"0058"},"figref":"FIG. 16"}]},"DETDESC":[{},{}]}
